reinforcement learning learning rate control chang tao gang computer control engineering nankai university tianjin china research beijing china 1 changxu wgzwp 2 taoqin liu abstract stochastic gradient descent sgd update model parameter adding local gradient time learning rate step widely used model training machine learning algorithm neural network observed model trained sgd sensitive learning rate good learning rate problem speciﬁc pose algorithm automatically learn learning rate using neural network based od deep reinforcement learning rl ticular train policy network called actor cide learning rate step training value network called critic give feedback quality decision goodness learning rate outputted actor tor made introduction auxiliary actor critic network help main network achieve ter performance experiment different datasets network architecture show approach lead better convergence sgd designed competitor 1 introduction facing large scale training data stochastic learning stochastic gradient descent sgd usually much faster batch learning often result better model observation sgd method performance highly sensitive choice learning rate lecun et 2012 clearly setting static learning rate whole training process insufﬁcient since intuitively learning rate decrease model becomes close local optimum training go time maclaurin et 2015 although some empirical suggestion guide adjust learning rate time training still difﬁcult task ﬁnd good policy adjust learning rate given good policy problem speciﬁc depend implementation detail machine learning algorithm one usually need try many time adjust learning rate manually accumulate knowledge problem however human involvement often need domain knowledge target problem ﬁcient difﬁcult scale different problem thus natural question arises learn adjust ing rate exactly focus work aim learn learning rate sgd based machine learning ml gorithms without rule tures examining current practice learning rate two observation first learning rate control sequential decision process set initial learning rate beginning step cide whether change learning rate change based current model loss training data hand maybe history training process suggested orr uller 2003 one method ing ideal learning rate decrease learning rate weight vector oscillates increase weight vector follows relatively steady direction second although step some immediate reward loss decrement obtained taking action care performance ﬁnal model found ml algorithm consider two different learning rate control cies ﬁrst one lead fast loss decrease ning get saturated stuck local minimum quickly second one start slower loss decrease sults much smaller ﬁnal loss obviously second policy better prefer reward reward combining two observation not difﬁcult see problem ﬁnding good policy learning rate fall scope reinforcement learning rl sutton barto 1998 inspired recent ce rl sequential decision problem work leverage rl technique try learn learning rate sgd based method propose algorithm learn learning rate within framework sutton 1984 sutton et 1999 barto et 1983 silver et 2014 widely used rl actor network trained take action cides learning rate current step critic network trained give feedback actor network performance help actor network adjust perform better future reduce oscillation training take gradient disagreement among training ples account feeding different training sample actor network critic network learning rate 31 may 2017 couraged small gradient oscillate sistent suggestion ideal learning rate strategy orr uller 2003 series experiment different datasets network architecture validate effectiveness proposed algorithm learning rate control main contribution paper include propose use based auxiliary network learn control learning rate ml algorithm ml algorithm achieve better gence reward exploited approach rather only immediate reward decrease loss one step expected total decrease loss ture step modeled critic network actor make decision learning rate 2 related work review some related work section improved gradient method focus improve gradient based ml algorithm automatic learning learning rate different approach proposed improve gradient method especially deep neural network since sgd solely rely given example example compare gradient model update step tends unstable take many step converge solve problem momentum sgd jacob 1988 posed accelerate sgd using recent gradient rmsprop tieleman hinton 2012 utilizes magnitude cent gradient normalize gradient always keep moving average root mean squared gradient divide current gradient adagrad duchi et 2011 adapts learning rate performs larger update infrequent smaller update quent parameter adadelta zeiler 2012 extends adagrad reducing aggressive monotonically decreasing ing rate instead accumulating past squared gradient adadelta restricts window accumulated past gradient some ﬁxed size adam kingma ba 2014 computes learning rate using estimate ﬁrst second moment gradient combine tages adagrad rmsprop senior et 2013 sutton 1992 darken moody 1990 focus predeﬁning update rule adjust learning rate training limitation method additional free parameter need set manually schaul et 2013 proposes method choose good learning rate sgd relies square norm expectation gradient expectation square norm gradient method much constrained several assumption met another recent work daniel et 2016 investigates eral feature us relative entropy policy search method controller select step size sgd rmsprop reinforcement learning since proposed algorithm based rl technique give brief introduction rl ease description algorithm next section reinforcement learning sutton 1988 concerned agent act stochastic environment sequentially choosing action sequence time step order maximize cumulative reward rl state st encodes agent observation environment time step policy function π st determines agent action take state function q function qπ st usually used denote cumulative reward taking action state st following policy π afterwards many rl algorithm proposed sutton barto 1998 watkins dayan 1992 many rl rithms sutton 1984 sutton et 1999 barto et 1983 silver et 2014 described framework algorithm learns policy tion value function simultaneously interactively policy structure known actor used select action estimated value function known critic criticizes action made actor recently deep reinforcement learning us deep neural network policy function value function shown promise various main including atari game mnih et 2015 go silver et 2016 machine translation bahdanau et 2016 image recognition xu et 2015 etc 3 method section present algorithm automate learning rate control sgd based machine learning algorithm optimizee actor network 𝑠𝑡 first layer 𝛼𝑡 action critic network automatic learning rate controller 𝑠i 𝑡 first layer 𝑄 𝛼i 𝑡 reward 𝜋𝜃𝑠𝑡 𝛼𝑡 𝑄𝜑𝑠𝑡 𝛼𝑡 𝜒 w𝑡 𝑋 last layer last layer figure 1 framework proposed automatic learning rate controller many machine learning task need train model parameter ω minimizing loss function f deﬁned set x training example arg min ω fω x 1 standard approach loss function minimization dient descent sequentially update parameter ing gradient step step ωt 2 learning rate step local dient f ωt one step whole batch training data mini batch example random sample observed performance sgd based method quite sensitive choice loss tion unfortunately f usually respect parameter w many ml algorithm especially deep neural network aim learn learning rate troller using rl technique automatically control figure 1 illustrates automatic learning rate controller adopts framework rl basic idea step given current model ωt training sample x actor network used take action ing rate used update model ωt critic network used estimate goodness tion actor network updated using estimated goodness critic network updated imizing temporal difference td sutton barto 1990 error describe detail algorithm following subsection actor network actor network called policy network rl play key role algorithm determines learning rate control policy primary ml based current model training data maybe historical mation training process note ωt could huge dimension one widely used image recognition model vggnet simonyan serman 2014 ha 140 million parameter actor network take parameter input computational complexity would dominate complexity primary algorithm unfordable therefore propose use function χ process yield compact vector st input actor network following practice rl call χ state function take ωt training data x input st χ ωt x 3 actor network πθ parameterized θ yield action πθ st 4 action continuous value determined update model primary algorithm equation note actor network ha parameter need learn output good action learn actor network need know evaluate goodness actor network critic network exactly play role two learning algorithm call one learning rate adjust primary ml algorithm one optimizes learning rate primary one ondary ml algorithm algorithm 1 algorithm learning rate learning input training step training set x loss function f state function χ discount factor γ size mθ actor network size mϕ critic network reset frequency model e 1 initialize model parameter ω policy parameter θ actor network value parameter ϕ critic network 2 1 3 sample xi 4 extract state vector st χ ωt xi 5 network selects action 6 computes learning rate πθ st 7 model parameter ω 8 compute xi 9 update ω ωt xi 10 critic network minimizing square error estimation label 11 rt f xi xi 12 extract state vector χ xi 13 compute qϕ πθ qϕ st 14 compute δt according equation 7 δt rt γqϕ πθ st 15 compute gradient critic network according equation 8 st 16 mod mϕ 0 17 update ϕ 1 mϕ 18 end 19 update actor network 20 sample xj j n j 21 extract state vector j χ xj 22 compute j πθ j 23 compute gradient actor network according equation 9 j j j 24 mod mθ 0 25 update θ 1 mθ 26 end 27 mod e 0 set end 28 end 29 return ω θ ϕ critic network recall goal ﬁnd good policy learning rate control ensure good model learnt eventually primary ml algorithm purpose actor network need output good action state st ﬁnally low training loss f achieved rl q function qπ often used denote long term reward pair following policy π take future action problem qπ st indicates accumulative decrement training loss starting step deﬁne immediate reward step one step 0 0 2000 4000 6000 8000 10000 12000 sgd adam adagrad rmsprop daniel et al vsgd method 0 2000 4000 6000 8000 10000 12000 sgd adam adagrad rmsprop daniel et al vsgd method b figure 2 result mnist training loss b test loss represents number mini batch represents loss value loss decrement rt f 5 accumulative value rt π policy π step total discounted reward step rt π σt sk ak γ 0 1 discount factor considering state action able problem critic network us parametric tion qϕ parameter ϕ approximate q value function qπ training actor critic network critic network ha parameter ϕ dated step using td learning precisely critic trained minimizing square error estimation qϕ st target yt yt rt γqϕ 6 td error deﬁned δt yt st rt γqϕ πθ st 7 weight update rule follows deterministic algorithm gradient critic network st 8 policy parameter θ actor network updated ensuring output action largest q value state st arg maxa qϕ st mathematically 9 algorithm overall algorithm learning rate learning shown algorithm step sample example line 3 extract current state vector line 4 compute learning rate using actor network line 6 update model line compute td error line update critic work line sample another example line 20 update actor network line would like make some discussion algorithm first current algorithm simplicity consider using only one example model update easy alize mini batch random example second one may notice use one example xi model critic network update different ple xj actor network update reduce oscillation training pose gradient direction current example mini batch example quite different others stage training process intuitively step model changed lot ﬁt example consequently resulting oscillation training shown experiment aforementioned one principle ideal learning rate control decrease gradient vector oscillates crease gradient vector follows relatively steady direction therefore try alleviate problem trolling learning rate according gradient disagreement feeding different example actor critic work likely critic network ﬁnd gradient direction example fed actor network inconsistent training example thus criticize large learning rate suggested actor network precisely update ω based xi learning rate suggested actor network training target actor network maximize output critic work xj big gradient disagreement xi xj update ω affected actor decision would cause critic output xj small sate effect actor network forced predict small learning rate big gradient disagreement situation 4 experiment conducted set experiment test performance learning rate learning algorithm compared several baseline method report experimental result section experimental setup speciﬁed algorithm experiment low given stochastic training common practice deep learning algorithm also ated step mini batch 1 0 20000 40000 60000 80000 100000 sgd adam adagrad rmsprop daniel et al vsgd method 1 0 20000 40000 60000 80000 100000 sgd adam adagrad rmsprop daniel et al vsgd method b figure 3 result training loss b test loss number mini batch represents loss value experiment state st χ ωt xi deﬁned average loss learning model ωt input mini batch actor network speciﬁed long term memory lstm network 20 unit layer considering good learning rate step depends correlate learning rate previous step lstm well suited model sequence dependence critic network speciﬁed simple ral network one hidden layer 10 hidden unit adam default setting toolbox used train learning rate learner experiment compared method several mainstream sgd algorithm including sgd adam kingma ba 2014 adagrad duchi et 2011 rmsprop tieleman hinton 2012 also compare method recent work daniel et 2016 work identiﬁes eral feature use rl method tive entropy policy search learn learning rate controller another baseline method vsgd schaul et 2013 2 automatically adjusts learning rate minimize expected error try compute learning rate update optimizing expected loss next update ing 1 square norm expectation gradient 2 expectation square norm gradient experimental result verify effectiveness method different datasets model structure experiment conducted two widely used image classiﬁcation datasets mnist lecun et 1998 krizhevsky hinton 2009 simplicity primary ml algorithm adopted cnn model setting tensorﬂow abadi et 2015 torial whose source code found ples algorithm dataset tried following learning rate report best performance algorithm learning rate algorithm need some parameter set decay coefﬁcients adam used default ting toolbox benchmark proposed method ﬁve independent run averaged reported following experiment trained baseline model convergence author providing source code result mnist mnist dataset handwritten digit classiﬁcation task example dataset 28 28 black white image containing digit 0 1 9 cnn model used primary ml algorithm consist two tional layer followed pooling layer ﬁnally fully connected layer training image test image dataset scaled pixel value range inputting algorithm mini batch contains 50 randomly sampled image figure 2 show result algorithm baseline method including curve training loss test loss following observation although loss algorithm doe not decrease fast beginning algorithm achieves best performance end one may expect algorithm signiﬁcantly faster convergence speed beginning considering algorithm learns learning rate cnn model baseline method only learn cnn model choose learning rate per some predeﬁne rule however not case since method target future reward rather immediate reward make decision lead better formance long term loss curve approach smooth ble others carefully design algorithm feed different sample actor work critic network discussed reduce oscillation training result dataset consisting 60000 natural 32 32 rgb image 10 class image training test used cnn 2 convolutional layer followed layer 2 fully connected layer inputting image cnn subtracted mean computed training set image figure 3 show result algorithm 10 including curve training loss test loss convergence speed method similar method sgd adam adagrad adadelta rmsprop method sgd adam adagrad adadelta rmsprop b method sgd adam adagrad adadelta rmsprop c figure 4 trajectory produced different algorithm three random regression problem ax represent value two dimension contour outline area target value target value gradually decreasing orange area blue area arrow represents one iteration algorithm whose tail tip correspond preceding subsequent iteration respectively 0 5 10 15 20 25 30 training step loss test loss method test loss sgd train loss method train loss sgd gradient disagreement gradient disagreement method gradient disagreement sgd figure 5 gradient disagreement training loss test loss sgd method regression lem baseline ﬁnal performance method best among compared algorithm analysis order verify intuitive explanation ing gradient disagreement method make learning process primary ml algorithm stable ducted another experiment experiment gate relationship gradient disagreement loss training process simple sion problem quantify gradient disagreement using euclidean distance gradient current batch data overall gradient figure 5 show gradient disagreement training loss test loss sgd method observe tion among ﬁgure discussed section feeding different sample actor critic network method would encourage learning rate small gradient disagreement large oscillation training process would relieved easy see ﬁgure test loss method stable big gradient disagreement loss sgd oscillates along gradient disagreement leading slow speed convergence test loss sgd may increase gradient disagreement increase overall test loss decline monotonous ﬁgure therefore need feed different training data actor network critic network ensure performance algorithm get deeper insight visualized optimization ce method figure 4 ﬁnd method get convergence fewer step tion trajectory relatively smooth compared od 5 conclusion future work work studied control learning rate gradient based machine learning method proposed algorithm help main network achieve better performance experiment two image classiﬁcation task shown method 1 successfully adjust learning rate different datasets cnn model structure leading better convergence 2 reduce oscillation training future work explore following tions work applied algorithm trol learning rate sgd apply ant sgd method focused learning ing rate model parameter study learn individual learning rate parameter considered learning learning rate using rl technique consider learning hyperparameters dependent dropout rate deep neural network reference abadi et 2015 martın abadi ashish agarwal paul barham et al tensorﬂow machine learning heterogeneous system software available tensorﬂow org 1 2015 bahdanau et 2016 dzmitry bahdanau philemon brakel kelvin xu anirudh goyal ryan lowe joelle pineau aaron courville yoshua bengio critic algorithm sequence prediction arxiv preprint 2016 barto et 1983 andrew g barto richard sutton charles w anderson neuronlike adaptive element solve difﬁcult learning control problem ieee action system man cybernetics 5 1983 daniel et 2016 christian daniel jonathan taylor sebastian nowozin learning step size controller bust neural network training thirtieth aaai conference artiﬁcial intelligence 2016 darken moody 1990 christian darken john moody fast adaptive clustering some ical result neural network 1990 ijcnn international joint conference page ieee 1990 duchi et 2011 john duchi elad hazan yoram singer adaptive subgradient method online learning stochastic optimization journal machine learning research 12 jul 2011 jacob 1988 robert jacob increased rate gence learning rate adaptation neural network 1 4 1988 kingma ba 2014 diederik kingma jimmy ba adam method stochastic optimization arxiv preprint 2014 krizhevsky hinton 2009 alex krizhevsky frey hinton learning multiple layer feature tiny image 2009 lecun et 1998 yann lecun eon bottou yoshua bengio patrick haffner learning plied document recognition proceeding ieee 86 11 1998 lecun et 2012 yann lecun eon bottou genevieve b orr uller cient backprop neural network trick trade page springer 2012 maclaurin et 2015 dougal maclaurin david naud ryan p adam hyperparameter optimization reversible learning proceeding international conference machine ing 2015 mnih et 2015 volodymyr mnih koray kavukcuoglu et al control deep reinforcement learning nature 518 7540 2015 orr uller 2003 genevieve b orr uller neural network trick trade springer 2003 schaul et 2013 tom schaul sixin zhang yann lecun no pesky learning rate icml 3 351 2013 senior et 2013 andrew senior georg heigold ke yang et al empirical study learning rate deep neural network speech recognition 2013 ieee international conference acoustic speech signal processing page ieee 2013 silver et 2014 david silver guy lever nicolas heess deterministic policy gradient algorithm 2014 silver et 2016 david silver aja huang et al ing game go deep neural network tree search nature 529 7587 2016 simonyan zisserman 2014 simonyan serman deep convolutional network image recognition corr 2014 sutton barto 1990 richard sutton andrew g barto model pavlovian ment page 1990 sutton barto 1998 richard sutton andrew g barto reinforcement learning introduction ume mit press cambridge 1998 sutton et 1999 richard sutton david mcallester satinder p singh yishay mansour et al policy gradient method reinforcement learning function imation nip volume 99 page 1999 sutton 1984 richard stuart sutton temporal credit signment reinforcement learning 1984 sutton 1988 richard sutton learning predict method temporal difference machine learning 3 1 1988 sutton 1992 richard sutton adapting bias ent descent incremental version aaai page 1992 tensorﬂowexamples tensorﬂowexamples tensorﬂow example tieleman hinton 2012 tijmen tieleman geoffrey hinton lecture divide gradient running average recent magnitude coursera neural network machine learning 4 2 2012 watkins dayan 1992 christopher jch watkins peter dayan machine learning 8 292 1992 xu et 2015 kelvin xu jimmy ba et al show attend tell neural image caption generation visual tention arxiv preprint 2 3 2015 zeiler 2012 matthew zeiler adadelta adaptive learning rate method arxiv preprint 2012