6 jul 2008 rollout sampling approximate policy iteration christos dimitrakakis michail lagoudakis november 10 2021 abstract several researcher recently investigated connection reinforcement learning classiﬁcation motivated proposal approximate policy iteration scheme without value function focus policy representation using classiﬁers address policy ing supervised learning problem paper proposes variant improved policy iteration scheme address core sampling problem evaluating policy simulation dit machine resulting algorithm oﬀers comparable performance previous algorithm achieved however signiﬁcantly le putational eﬀort order magnitude improvement demonstrated experimentally two standard reinforcement learning domain inverted pendulum 1 introduction supervised reinforcement learning two learning paradigm researched mostly independently recent study gated use supervised learning method reinforcement learning either value function lagoudakis parr riedmiller 2005 policy resentation lagoudakis parr fern et al 2004 langford zadrozny 2005 initial result shown policy approximately resented using either classiﬁers combination binary ﬁers rexakis lagoudakis 2008 therefore possible incorporate classiﬁcation algorithm within inner loop several reinforcement learning algorithm lagoudakis parr fern et al 2004 viewpoint low quantiﬁcation performance reinforcement learning algorithm term performance classiﬁcation algorithm langford zadrozny 2005 variety promising combination become possible synergy heretofore limited practical algorithm work build work lagoudakis parr lagoudakis parr suggested approximate policy iteration algorithm learning good policy represented classiﬁer avoiding representation any kind value function iteration new produced using 1 training data obtained extensive simulation rollouts previous policy generative model process rollouts aim identifying better action choice subset state order form set data training classiﬁer representing improved policy similar algorithm wa proposed fern et al fern et al 2004 around time key diﬀerences two algorithm related type learning problem suitable choice underlying classiﬁer type exact form classiﬁer training nevertheless main idea producing training data using rollouts iterating policy remain even though study look carefully distribution training state state space major limitation remains large amount sampling employed training state hinted lagoudakis 2003 however great improvement could achieved sophisticated management rollout sampling paper suggests managing rollout sampling procedure within algorithm goal obtaining comparable training set fore policy similar quality signiﬁcantly le eﬀort term number rollouts computation eﬀort done viewing ting akin bandit problem rollout state state sampled using rollouts algorithm bandit problem upper dence bound auer et al 2002 successive elimination et al 2006 allow optimal allocation resource rollouts trial state contribution suitably adapt bandit technique rollout management b suggest improved statistical test identifying early high conﬁdence state dominating action return obtain order magnitude improvement original algorithm term eﬀort needed collect training data classiﬁer make resulting algorithm attractive practitioner need address large problem remainder paper organized follows section 2 provides necessary background section 3 review original algorithm based subsequently approach presented detail section finally tion 5 includes experimental result obtained learning domain 2 preliminary markov decision process mdp p r γ state space process ﬁnite set action p markovian transition model p denotes probability transition state taking action state r reward function r expected reward taking action state γ 0 1 discount factor future reward initial state distribution deterministic policy π mdp mapping π state action π denotes action choice state value v π state policy π expected total discounted reward process begin state decision 2 step made according policy π v π e x γtr π st st goal decision maker ﬁnd optimal policy maximizes expected total discounted reward initial state distribution arg max π v π every mdp exists least one optimal istic policy policy iteration pi howard 1960 eﬃcient method deriving optimal policy generates sequence πk gradually improving policy terminates no change policy πk πk optimal policy improvement achieved computing v πi analytically solving linear bellman equation action value qπi r γ x p v πi determining improved policy arg max qπi policy iteration typically terminates small number step however relies knowledge full mdp model exact computation tation value function policy exact representation policy approximate policy iteration api family method suggested address curse dimensionality huge growth complexity problem grows api value function policy represented approximately some compact form iterative improvement process remains apparently guarantee monotonic ment optimality convergence compromised api may never converge however practice reach good policy only iteration reinforcement learning learner interacts process typically observes state immediate reward every step however p r not accessible goal gradually learn optimal policy interaction process step interaction learner observes current state chooses action observes resulting next state reward received many case assumed learner ha ability reset process any arbitrary state amount access generative model process simulator learner draw arbitrarily many time next state reward r performing any given action any given state several algorithm proposed learning good even optimal policy sutton barto 1998 3 3 rollout classiﬁcation policy iteration rollout classiﬁcation policy iteration rcpi algorithm lagoudakis parr lagoudakis 2003 belongs api family focus direct icy learning representation bypassing need explicit value function key idea rcpi cast problem policy learning classiﬁcation problem thinking state example action class label any terministic policy thought classiﬁer map state action therefore policy rcpi represented approximately generic class classiﬁers assign state example action class problem ﬁnding good policy equivalent problem ﬁnding classiﬁer map state good action goodness action measured term contribution long term goal agent value function qπ context ﬁxed policy π provides measure action maximizes qπ state good action whereas any action smaller value qπ bad one training set could easily formed qπ value action available subset state estimation technique rollouts provides way rately estimating qπ any given pair without requiring explicit representation value function rollout amount simulating trajectory process beginning state choosing action ﬁrst step choosing action according policy π thereafter certain horizon observed total discounted reward averaged number rollouts yield estimate thus using suﬃcient amount rollouts possible form valid training set improved policy any base policy speciﬁcally denote sequence collected reward simulated trajectory r 0 1 2 rollout estimate ˆ qπ k true value function qπ observed total discounted reward averaged k trajectory ˆ qπ k k k x qπ qπ x γtr suﬃcient amount rollouts large create improved policy π any state without requiring model mdp algorithm 1 describes rcpi beginning any initial policy training set subset state sr formed querying rollout procedure value action state purpose identifying best action bad action action said dominating empirical value signiﬁcantly greater action rcpi measured statistical sense using pairwise factor estimation error notice training set contains positive negative example state clear domination found new classiﬁer trained using example yield approximate representation improved policy previous one cycle repeated termination condition met given 4 algorithm 1 rollout classiﬁcation policy iteration input rollout state sr initial policy trajectory k horizon discount factor γ default uniformly random repeat π trainingset estimate qπ using k rollouts length end dominating action trainingset trainingset trainingset trainingset end end trainclassifier trainingset π return π approximate nature policy iteration termination condition not rely convergence single optimal policy rather terminates performance new policy measured via simulation doe not exceed previous policy rcpi algorithm ha yielded promising result several learning main however stated also lagoudakis lagoudakis 2003 sensitive distribution state sr state space reason suggested draw state future state distribution improved policy distribution also suggested fern et al fern et al 2004 yield better result resolve any potential mismatch training testing distribution classiﬁer however main drawback still excessive computational cost due need lengthy repeated rollouts reach good level accuracy iments rcpi ha observed eﬀort wasted state action value diﬀerences either ﬁne require one use prohibitive number rollouts identify signiﬁcant eﬀort also wasted sampling state dominating action could easily identiﬁed without exhausting rollouts allocated paper propose rollout sampling method remove performance 5 algorithm 2 samplestate input state policy π horizon discount factor γ r simulate qπ r x 1 r simulate x π x qπ qπ γtr x end end return qπ 4 rollout sampling policy iteration excessive sampling cost mentioned reduced careful agement resource scheme suggested rcpi also used fern et al fern et al 2004 somewhat ıve number rollouts allocated state subset sr k rollouts dedicated single action exhausted moving next action intuitively sired outcome domination single action some state conﬁdently determined early no need exhaust rollouts available state training data could stored state could removed pool without examination similarly conﬁdently determine action indiﬀerent some state simply reject without wasting any rollouts rejected state could replaced fresh one might yield meaningful result idea lead following tion examine state subset sr collectively some interleaved manner choosing time single state focus allocating rollouts only needed similar resource allocation setting context reinforcement learning bandit problem therein learner faced choice n dit one unknown reward function task allocate play discover bandit highest expected reward without wasting many resource either cumulative reward number play taking inspiration problem view set rollout state bandit state corresponds single pulling lever corresponds sampling corresponding state sampling state mean perform single rollout action state shown algorithm minimum amount information precise deﬁnition task depends speciﬁc problem formulation beyond scope article 6 request single thus problem transformed variant classic bandit problem several method proposed various version problem could potentially used context paper focus three simple counting upper ﬁdence bound auer et al 2002 successive elimination et al 2006 goal point collect good training data classiﬁer little computational eﬀort possible quantify notion ness training data term three guarantee state sampled only needed produce training data without wasting rollouts b high probability discovered action label training data dicate dominating action c training data cover state space suﬃciently produce good representation entire policy look one objective turn rollout management mentioned previously algorithm maintains pool state sr sampling performed paper state drawn uniformly random distribution cover state space evenly however sophisticated distribution may also used order allocate rollouts wisely need decide state sample every step also need determine criterion decide stop sampling state add new state pool ﬁnally stop sampling completely general form state selection rule algorithm arg max u u represents utility associated sampling state sented algorithm use one following variant count succe u u p 1 c u p ln 1 c c counter recording number time state ha sampled total number state sample ˆ empirical counterpart marginal diﬀerence qπ value state deﬁned π π qπ possible also manage sampling action within state preliminary experiment showed managing action sampling alone saved little eﬀort compared managing state sampling currently working managing sampling level 7 π maximizes qπ state π arg max qπ similarly empirical diﬀerence ˆ deﬁned term empirical q value ˆ qπ k ˆ π π ˆ qπ k ˆ π action maximizes ˆ qπ k state ˆ π arg max ˆ qπ k k c some ﬁxed independent count variant simple counting criterion state ha sampled least ha higher priority sampled next since stop sampling state soon suﬃciently good estimate criterion result le sampling compared rcpi continues sampling even estimate deemed suﬃciently good succe variant us criterion count sample state feature additional mechanism removing apparently hopeless state based successive elimination algorithm algorithm 3 et al 2006 expect criterion useful problem many state action indiﬀerent however might also result continual rejection state state sampled eﬀectively limiting amount state space covered ﬁnal gathered example variant based ucb algorithm auer et al 2002 give higher priority state high empirical diﬀerence high certainty diﬀerence thus state take priority two reason firstly sampled le secondly likely result acceptance quickly variant based original algorithm auer auer et al 2002 us shrinking error bound calculating upper dence interval since setting stop sampling state diﬀerence action suﬃciently large similar simple counting process continues however intuitively focus state likely result positive identiﬁcation dominating action quickly towards end case new state added pool soon state ha removed sr ha constant size criterion selecting example described following section case multiple equivalent maximizing action easily handled generalising set action manner fern et al fern et al 2006 discus only single best action case simplify exposition 8 statistical signiﬁcance sampling state proceeds according one rule step state identiﬁed good removed state pool added training data prevent wasted sampling order terminate sampling accept state good rely following lemma lemma hoeﬀding inequality let x random variable x x observed value xn x ˆ xn 1 n pn xi p ˆ xn x ǫ p ˆ xn x consider two random variable x true mean x pirical mean ˆ xn ˆ yn well random variable representing diﬀerence true mean x empirical mean ˆ xn yn follows lemma p ˆ ǫ 2 1 consider applying determining best action any state taken c sample every action previously let ˆ π empirically optimal action state any ˆ π set x qπ ˆ π qπ correspondingly ˆ xn ˆ yn obtain p ˆ qπ ˆ π qπ ˆ π ǫ 2 2 corollary any state following condition hold ˆ v u u 2 ln δ 3 probability incorrectly identifying π bounded proof set ǫ equal right hand side 3 obtain p qπ ˆ π qπ ˆ π v u u 2 ln δ 4 course wanted continuously shrink probability error could continue sampling state 9 incorrectly identifying π implies exists some qπ ˆ π qπ ˆ qπ ˆ π qπ however due stopping condition ˆ qπ ˆ π qπ 2 ln order make mistake concerning ordering two action estimation error must larger right side 3 thus probability also bounded given number action ˆ π application union bound implies total probability making mistake state must bounded summary every time sampled c ˆ change whenever stopping condition 3 satisﬁed state safely removed sr high probability 1 current empirical diﬀerence value not change sign sampling conﬁdently resulting action label indeed dominating finally note practice might not able obtain full trajectory case estimate true value function replaced version state space coverage policy improvement step algorithm terminates ceeded collecting nmax example performed mmax rollouts initially nmax order make sure training data not restricted static subset sr every time state characterized good removed sr add new state sr drawn some ﬁxed distribution dr serf source rollout state simplest choice dr would uniform distribution state space however choice possible especially domain knowledge structure good policy known ticated choice dr diﬃcult problem not investigate ha conjectured good choice future state distribution improved policy learned lagoudakis parr fern et al 2004 also toyed idea rejecting state seem hopeless produce training data replacing fresh state sampled some distribution succe rule incorporates rejection criterion default et al 2006 variant rejection adopted reject state u ln suit particularly well complete algorithm called rollout sampling policy iteration rspi described detail algorithm call selectstate refers one original rcpi algorithm employed pairwise choice ﬂawed since assumes normal distribution error whereas hoeﬀding bound simply assumes variable bounded 10 four selection rule described note call succe might also eliminate some state sr replacing fresh one drawn algorithm 3 rollout sampling policy iteration input distribution dr initial policy horizon discount factor γ max data nmax max sample mmax probability δ number rollout state n boolean tion range b default random n 0 0 sr r default ˆ qπ 0 ˆ 0 u 0 c 0 repeat π trainingset n selectstate sr ˆ c qπ samplestate π γ update ˆ qπ ˆ u using qπ c c 1 1 ˆ 2 2 ln δ n n 1 trainingset trainingset ˆ π trainingset trainingset ˆ π sr sr sr sr end rejection u ln sr sr sr sr end end end end trainclassifier trainingset π return π 5 experiment demonstrate performance proposed algorithm practice set basis comparison rcpi present experimental result two standard reinforcement learning domain namely inverted pendulum mountain car domain tried several setting various 11 parameter related state sampling however kept learning parameter classiﬁer constant used new statistical test even rcpi ﬁlter inﬂuence case measured performance resulting policy eﬀort needed derive term number sample section describe learning domain exact evaluation method used result described section inverted pendulum inverted pendulum problem balance pendulum unknown length mass upright position applying force cart attached three action allowed left force lf right force rf no force nf applying respectively uniform noise 10 added chosen action due noise problem return any single pair stochastic even though only employing deterministic policy not case would needed single sample state state space continuous consists vertical angle θ angular velocity θ pendulum transition governed nonlinear dynamic system wang et al 1996 depend current state current control u θ g sin θ θ 2 sin co θ u θ g gravity constant g mass pendulum kg mass cart kg l length pendulum l α simulation step second control input changed only beginning time step kept constant duration reward 0 given long angle pendulum doe not exceed absolute value pendulum horizontal line angle greater signal end episode reward penalty discount factor process set force q value function lie 0 set 0 problem problem drive underpowered car bottom valley two mountain top mountain right car not powerful enough climb any hill directly bottom valley even full throttle must build some momentum climbing ﬁrst left moving away goal right three action allowed forward throttle ft reverse throttle rt no throttle nt 0 original speciﬁcation assumes deterministic transition model make problem little challenging added noise three action uniform noise added chosen action eﬀect 12 due noise problem return stochastic thus necessitating use multiple sample state state space problem continuous consists position x velocity x car along horizontal axis transition governed simpliﬁed nonlinear dynamic system sutton barto 1998 depend current state x x current noisy control u x 1 boundx x x 1 x 1 bound x x co boundx function keep x within bound x keep x within car hit left bound position x velocity x set zero problem penalty given step long position car right bound soon car position hit right bound episode end successfully reward 0 given discount factor process set choosing problem trickier since without any condition value function lie 0 however diﬀerence q value any state doe not vary much practice state policy combination initial action doe not alter ﬁnal reward reason used 1 evaluation preliminary investigation selected perceptron 10 hidden unit classiﬁer representing policy stochastic gradient descent learning rate 25 iteration training note only one numerous choice main problem wa devise experiment determine tional eﬀort would required method ﬁnd optimal policy practice meant method would simulate process manual tuning practitioner would perform order discover optimal solution usual practice perform grid search space multiple run per grid point assuming experimenter perform number run parallel use number solution found certain number sample taken method practical metric sample complexity algorithm speciﬁcally tested proposed state selection method count succe rspi rcpi problem method used following set mmax nmax 10 20 50 100 200 δ pendulum δ performed 5 run diﬀerent random seed combination total 375 run per method run resulting policy wa tested quality policy could balance pendulum exploratory run appeared particularly hard obtain any sample car problem δ used instead 13 0 20 40 60 80 100 120 140 160 180 100 1000 10000 cumulative number successful run number sample rcpi cnt succe figure 1 cumulative distribution successful run least 1000 step balancing pendulum domain least 1000 step policy could drive car goal 75 step starting position considered successful practically optimal report cumulative distribution successful policy found number sample rollouts used method summed run formally x number sample along horizontal axis plot measure f x µ πi πi successful mi horizontal axis show least number sample required obtain number successful run shown vertical axis eﬀectively ﬁgures show number sample x required obtain f x policy experimenter wa fortuitous enough select appropriate detail figure 1 show result pendulum problem count succe method approximately total number successful run clearly dominates 4000 sample per run already obtained 180 successful policy point ha six time chance producing successful policy compared rcpi contrary rcpi only managed produce le half total number policy ﬁrst method importantly none run produced any successful policy fewer 2000 sample point method already making signiﬁcant progress perhaps worthwhile noting point form rcpi plot due fact wa always terminating sampling rollouts exhausted method may also terminate whenever nmax good sample obtained due reason plot might terminate earlier stage 14 similarly figure 2 show result problem time consider run le 75 step taken reach goal successful clear proposed method perform better rcpi higher chance producing good policy fewer sample exhibit advantage method however diﬀerences method slightly ﬁner domain interesting note result not sensitive actual value fact usually able obtain good policy quite large value mountain car domain hand one working limited budget rollouts small value δ might make convergence impossible since not enough rollouts available obtain best action necessary conﬁdence similar thing occurs large noticed initial experiment mountain car set 0 perhaps predictably important parameter appeared nmax certain threshold no good policy could found any algorithm general occurred total number good state end iteration classiﬁer able create improved policy course δ nmax large no guarantee performance policy improvement not bound probability state use correct action label however doe not appear problem practice posit two factor may explain firstly relatively low stochasticity problem environment policy deterministic single sample would enough determine optimal action state secondly smoothing inﬂuence classiﬁer may suﬃcient policy improvement even some portion state sampled incorrect label computational time doe not give meaningful measurement setting time taken trajectory depends many step pas episode terminates some problem problem ﬁnite horizon cutoﬀfor rollout estimate may constant others length time varies quality policy pendulum domain policy run longer improve opposite occurs mountain car problem reason decided only report result computational complexity would ﬁnally like note experiment additional rejection replacement state failed produce improvement however method might use environment action guishable state 6 discussion proposed approach deliver equally good policy produced rcpi signiﬁcantly le eﬀort problem order magnitude reduction number rollouts performed thus 15 0 20 40 60 80 100 120 160 100 1000 10000 100000 cumulative number successful run number sample rcpi cnt succe 140 figure 2 cumulative distribution successful run le 75 step reach goal domain computational eﬀort thus conclude selective sampling approach make rollout algorithm much practical especially since similar approach already demonstrated eﬀectiveness ning domain kocsis ari 2006 however some practical obstacle remain particular choice δ nmax not easy determine priori especially choice classiﬁer need taken account well example classiﬁer may not tolerate large δ support vector machine unfortunately point choice only done via laborious experimentation even since original algorithm suﬀered problem experimenter least assured not much time spent optimal solution found result show currently bandit algorithm variant employed state rollout selection used heuristic manner however companion paper dimitrakakis lagoudakis 2008 analyzed whole policy iteration process proved style bound progress count method guaranteed make certain assumption underlying mdp model hope extend work future order produce algorithm ically tuned task furthermore plan address rollout sampling state action level focus attention sophisticated state sampling distribution exploiting sampled state no clear ative positive action example drawn possibly developing variant upper bound tree algorithm kocsis ari 2006 mentary research route would integrate sampling procedure ﬁtting 16 algorithm use single trajectory antos et al 2008 summary presented approximate policy iteration scheme reinforcement learning relies classiﬁcation technology policy sentation learning clever management resource obtaining data belief synergy two learning paradigm ha still lot reveal machine learning researcher acknowledgement would like thank reviewer providing valuable feedback rina mitrokotsa additional proofreading work wa partially supported project european international gration grant awarded michail lagoudakis reference antos csaba ari emi munos learning policy minimization based ﬁtted policy iteration single sample path machine learning 71 1 apr doi 10 auer fischer analysis multiarmed bandit problem machine learning journal 47 christos dimitrakakis michail lagoudakis algorithm bound approximate policy iteration presented european workshop reinforcement learning eyal shie mannor yishay mansour action elimination stopping condition bandit reinforcement learning problem journal machine learning research issn fern yoon givan approximate policy iteration policy language bias advance neural information processing system 16 3 fern yoon givan approximate policy iteration policy guage bias solving relational markov decision process journal artiﬁcial intelligence research ronald howard dynamic programming markov process mit press cambridge massachusetts levente kocsis csaba ari bandit based planning proceeding european conference machine learning 2006 17 lagoudakis parr policy iteration journal chine learning research 4 6 michail lagoudakis eﬃcient approximate policy iteration method sequential decision making reinforcement learning phd thesis ment computer science duke university may michail lagoudakis ronald parr reinforcement learning tion leveraging modern classiﬁers proceeding international conference machine learning icml page washington dc usa august john langford bianca zadrozny relating reinforcement learning mance classiﬁcation performance proceeding tional conference machine learning icml page bonn many isbn doi ioannis rexakis michail lagoudakis policy representation presented european workshop reinforcement learning riedmiller neural ﬁtted q experience data eﬃcient neural reinforcement learning method european conference machine learning page richard sutton andrew barto reinforcement learning introduction mit press cambridge massachusetts hua wang kazuo tanaka michael griﬃn approach fuzzy control nonlinear system stability design issue ieee transaction fuzzy system 4 1 1996 18