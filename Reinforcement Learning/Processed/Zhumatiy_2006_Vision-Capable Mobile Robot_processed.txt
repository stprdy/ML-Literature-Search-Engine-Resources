arxiv 7 mar 2006 technical report metric state space reinforcement learning mobile robot zhumatiy gomez hutter idsia galleria 2 switzerland viktor tino marcus juergen march 2006 abstract address problem autonomously learning controller capable mobile robot extend mccallum 1995 ory algorithm allow general metric trajectory demonstrate feasibility approach successfully running rithm real mobile robot algorithm novel unique explores environment learns directly mobile robot without using computer model intermediate step b doe not require manual discretization sensor input space c work piecewise tinuous perceptual space cope partial observability together allows learning much le experience compared previous method keywords reinforcement learning mobile robot aﬃliated tu munich boltzmannstr 3 85748 garching unchen germany 1 1 introduction realization fully autonomous robot require algorithm learn direct experience obtained visual input vision system provide rich source information pwc structure ceptual space video image implied typical mobile robot environment not compatible current reinforcement learning approach environment characterized region smooth continuity separated tinuities represent boundary physical object sudden appearance disappearance object visual ﬁeld two broad approach used adapt existing algorithm real world environment 1 discretizing state space ﬁxed 20 adaptive 15 16 grid 2 using function approximator 10 5 radial basis function rbfs 1 cmac 22 memory 4 3 17 19 fixed discrete grid introduce artiﬁcial discontinuity adaptive one scale exponentially state space dimensionality neural network implement relatively smooth global function not capable approximating discontinuity rbfs cmacs like ﬁxed grid method require knowledge appropriate local scale method use neighborhood explicitly stored experience generalize new experience method suitable purpose implement local model principle approximate pwc tions typically fall short using ﬁxed neighborhood radius assume uniform sampling density state space ﬁxed radius prevents approximator clearly identifying discontinuity point side discontinuity averaged together thereby blurring location instead use ﬁxed number k neighbor eﬀect using variable radius imator ha arbitrary resolution near important state space boundary needed accurately model local dynamic use approach appropriate metric needed determine stored instance provide relevant information deciding given situation 6 apart pwc structure perceptual space robot learning rithm must also cope fact instantaneous sensory reading alone rarely provide suﬃcient information robot determine localization problem action best take some form memory needed integrate successive input identify underlying environment state otherwise only partially observable paper present algorithm called piecewise continuous nearest quence memory extends mccallum algorithm discrete partially observable state space nearest sequence memory nsm 12 general pwc case like nsm store data collect environment us continuous metric history allows used real robot environment without prior discretization perceptual space important priority work minimizing amount priori edge structure environment available learner cally artiﬁcial learning conducted simulation resulting policy transfered real robot building accurate model real environment intensive only really achievable simple sensor used unlike vision overly simpliﬁed model make policy transfer cult 14 reason stipulate robot must learn directly real world furthermore since gathering data real world costly rithm capable eﬃcient autonomous exploration robot perceptual state space without knowing amount exploration required diﬀerent part state space normally case even advanced approach exploration discrete 2 8 even metric 6 state space next section introduces section 3 present experiment robot navigation section 4 discus result future direction research 2 nearest sequence ory presenting algorithm ﬁrst brieﬂy review underlying learning nism describe nearest sequence memory extends discrete pomdps form basis basic idea originally formulated ﬁnite discrete state space incrementally estimate value pair based reward received environment agent previous estimate update rule st 1 qt st α γ max qt qt st estimate time state action α learning rate γ discount parameter requires number state st ﬁnite completely able unfortunately due sensory limitation robot not direct access complete state information instead receive only observation ot set possible observation typically much smaller set state causing perceptual aliasing robot unable behave optimally state requiring diﬀerent action look order use similar method general tions some mechanism required estimate underlying environmental state stream incoming observation idea using history servations recover underlying state form core nsm algorithm described next nearest sequence memory nsm try overcome perceptual aliasing taining chronologically ordered list history interaction agent environment basic idea disambiguate aliased state searching history ﬁnd previous experience sequence closely match recent situation time step agent store experience triple ot rt current action observation reward appending history previous experience called observation order choose action time agent ﬁnds possible action k observation state history similar current situation mccallum 12 deﬁnes similarity length common history n ht 0 0 0 ot rt 1 n ht 1 1 ot rt 1 count number contiguous experience triple two observation state match exactly starting going back time rewrite original n functionally equivalent general form using distance µ ht ht accommodate metric introduce next section k observation state ht possible action time form neighborhood nht used compute corresponding action q ht 1 x ht q ht 2 q ht local estimate q ht pair occurred time action ha selected according action b highest value updated q hi 1 q hi β ri γ max q ht b 3 nsm ha demonstrated simulation ha never run real robot using history resolve perceptual aliasing still requires considerable human gramming eﬀort produce reasonable discretization sensor following avoid issue discretization selecting appropriate metric continuous observation space piecewise continuous nsm distance measure used nsm equation 1 wa designed discrete state space continuous perceptual space substitute symbol h mccallum original notation avoid confusion accepted deﬁnition state observation sequence not correspond process state µ not metric robot must learn metric inadequate since likely triple ot rt diﬀerent µ ht always equal therefore accommodate continuous state replace equation 1 following discounted metric µ ht min x 4 λ metric take exponentially discounted average clidean distance observation sequence note unlike equation 1 metric ignores action reward distance action sequence not considered no elegant way combine discrete action uous observation primary concern robotics perspective provide metric allows robot localize based observation reward value also excluded enable robot continue using metric select action even reinforcement signal no longer available some initial training period algorithm 1 present pseudocode function randz b randr c produce uniformly distributed random number b c respectively ǫ determines greediness policy algorithm diﬀers importantly nsm using discounted metric line 8 way exploratory action policy chosen line 12 exploratory action action whose neighborhood ha highest average distance current action least information policy induces ha called balanced wandering 7 endogenous update only updated interaction real environment learning slow since update occur robot control frequency rate agent take action one way fully exploit information gathered environment perform update stored history normal update refer update endogenous originate within learning agent unlike normal exogenous update triggered real event outside agent learning agent selects random time update ht according equation 3 maximum next state computed using equation 2 see line algorithm 1 approach similar dyna architecture 21 history act kind model unlike dyna model doe not generate new experience rather already history manner similar experience replay 9 3 experiment robot navigation demonstrate mobile robot task csem robotics smarteasetm robot must use video input identify navigate target object avoiding obstacle wall camera provides only partial view ronment task requires robot use history observation remember ha last saw target target move view experimental setup experiment conducted meter walled arena shown ﬁgure robot equipped two ultrasound distance sensor one facing forward one backward vision system based axis 2100 network camera mounted top robot diameter cylindrical chassis learning wa conducted series trial robot obstacle target blue teapot placed random location arena beginning trial robot take sensor reading sends via wireless camera image vision computer sonar reading learning computer vision computer extract coordinate target visual ﬁeld calculating centroid cp pixel target color see pass learning computer along predicate p indicating whether target visible p false learning computer merges x p forward backward sonar reading f b form input observation vector x p f b x normalized f b normalized selects one 8 action turn left right either move forward backward either approximately action set wa chosen allow algorithm adapt scale environment 18 selected action sent robot robot executes action cycle repeat robot reach goal goal moved new location new trial begin entire interval sensory reading action execution second marily due camera network delay accommodate relatively low control frequency maximum velocity robot limited 10 dead time action learning computer conduct many endogenous update time permit parameter us policy algorithm 1 line 13 ǫ set mean 30 time robot selects exploratory action appropriate number nearest neighbor k used select action depends upon noisiness environment lower noise smaller k chosen amount noise sensor found learning wa fastest common practice toy reinforcement learning task discrete maze use minimal reinforcement agent rewarded only reach goal formulation useful test algorithm simulation real robot sparse delayed reward forestalls learning agent wander long period time without reward ﬁnally happening upon goal accident often speciﬁc domain knowledge incorporated reward function provide intermediate reward facilitates learning robotic domain exploration costly 11 reward function use sum two ponents one robstacle rtarget r max min f b z robstacle p 500 cp z rtarget 5 rtarget largest robot near goal looking directly towards smaller target visible middle ﬁeld view even smaller target visible not center reach minimum target not visible robstacle negative robot close some obstacle except obstacle target visible robot important note coeﬃcients equation 5 speciﬁc robot not environment represent calibration robot hardware used result taking 1500 3000 action robot learns avoid wall reduce speed approaching wall look around goal go goal whenever see much faster compared neural network based learner 5 4000 episode required resulting 100 000 action solve simpler task target wa always within perceptual ﬁeld robot neither need virtual model environment manual quantization state space like 14 knowledge result fastest term learning speed use least quantization eﬀort compared method date though unable compare result directly hardware used competing approach beginning learning corner pose serious diﬃculty causing robot get stuck receive negative reinforcement close wall robot accidentally turn towards target quickly lose track learning progress robot able recover usually within one action exploratory action cause turn away loose sight target discounted metric allows robot use history observation state remember seen target recent past figure 1 show learned policy task since robot state space not coordinate ﬂoor case rl textbook example changing position obstacle target doe not impede robot performance figure 2 show learning term immediate average reward typical sequence trial lasting total approximately 70 minute dashed vertical line two graph indicate beginning new trial learning progress robot able generalize past experience quickly ﬁnd goal ﬁrst two trial robot start accumulate reward rapidly third fourth trial completed little deliberation figure 3 illustrates two successful trial 4 discussion developed algorithm mobile robot learning fully implemented actual robot use metric state space allows algorithm work weaker requirement eﬃcient compared previous work continuous reinforcement learning 3 17 19 using metric instead discrete grid considerable relaxation mer task since obviates need guess correct scale region state space advance algorithm explores environment learns directly mobile robot without using computer model termediate step work piecewise continuous perceptual space cope partial observability metric used paper worked well experiment erful approach would allow algorithm select appropriate metric given environment task automatically choose metric criterion deﬁned determines set priori equiprobable metric µn ﬁts given history experimentation better useful criterion could example generalization criterion used mccallum algorithm 13 decide whether state split current algorithm us discrete action convenient way group observation state action space continuous algorithm lack natural way generalize action metric action space µa could used within neighborhood delimited current metric agent could randomly sample possible action query point ht obtain sampled action computing neighbor within future work explore avenue acknowledgment work partially supported csem robotics alpnach reference 1 anderson restarting hanson cowan giles editor advance neural information processing system 5 page san mateo ca morgan kaufmann 2 brafman tennenholtz general polynomial time rithm reinforcement learning mach learn 2003 3 doya reinforcement learning continuous time space neural putation 12 1 2000 4 forbes andre practical reinforcement learning continuous main technical report university california berkeley 2000 5 iida sugisaka shibata application inforcement learning real mobile robot ccd camera proc arob int l symp artiﬁcial life robotics page 2003 6 kakade kearns langford exploration metric state space machine learning proceeding twentieth international conference icml 2003 august 2003 washington dc usa aaai press 2003 7 kearns singh reinforcement learning polynomial time proc international conf machine learning page morgan kaufmann san francisco ca 1998 8 kearns singh reinforcement learning mial time machine learning 49 2002 9 lin reactive agent based reinforcement learning ning teaching machine learning 8 3 1992 10 lin mitchell memory approach reinforcement learning domain technical report carnegie mellon university school computer science may 1992 11 mataric reward function accelerated learning machine learning proceeding annual conference page 1994 12 mccallum state identiﬁcation reinforcement learning tesauro touretzky leen editor advance neural mation processing system volume 7 page mit press 1995 13 mccallum learning use selective attention memory sequential task maes mataric meyer pollack wilson editor animal animats 4 proceeding fourth international conference simulation adaptive behavior cambridge page mit press bradford book 1996 14 minato asada environmental change adaptation mobile robot navigation journal robotics society japan 18 5 2000 15 moore algorithm variable resolution reinforcement learning multidimensional cowan tesauro alspector editor advance neural information processing system ume 6 page morgan kaufmann publisher 1994 16 pareigis adaptive choice grid time reinforcement learning nip 97 proceeding 1997 conference advance neural information processing system 10 page cambridge usa mit press 17 santamaria sutton ram experiment reinforcement learning problem continuous state action space adapt 6 2 1997 18 schoknecht riedmiller learning control multiple time scale kaynak alpaydin oja xu editor artiﬁcial neural network neural information processing 2003 joint tional conference 2003 istanbul turkey june 2003 proceeding volume 2714 lecture note computer science springer 2003 19 smart kaelbling practical reinforcement learning continuous space proc international conf machine learning page morgan kaufmann san francisco ca 2000 20 sutton barto reinforcement learning introduction cambridge mit press 1998 21 sutton first result dyna integrated architecture ing planning reacting proceeding aaai spring symposium planning uncertain unpredictable changing environment 1990 22 sutton generalization reinforcement learning successful example using sparse coarse coding touretzky mozer selmo editor advance neural information processing system 8 page cambridge mit press algorithm 1 nsm 1 2 ht initialize history 3 loop 4 5 ht ot rt get store action observation reward 6 7 ha split history subset ha containing observation state action wa taken 8 nht kµ ht ht ﬁnd neighbor ht ha using metric µ equation 4 9 q ht 1 ht p ht q h case nht 10 end 11 b q ht compute action est value 12 e 1 p ht µ h ht compute best exploration action e 13 randr ǫ 14 perform e select exploratory tion 15 else 16 perform b select greedy action 17 end 18 n 19 1 20 q ht q ht q update value 21 end 22 q ht ht b initialize neighbor estimate 23 end loop x b f figure 1 learned control policy row show diﬀerent situation environment along corresponding learned policy top row robot positioned directly front target object cross camera image mark detected pixel target color circle indicates assumed tion towards target policy situation shown term visual coordinate only camera view coordinate high dimensional policy shown point policy graph indicates arrow direction robot move circle shown image point visual ﬁeld left arrow mean move left no still instance case robot move forward circle lie part policy arrow bottom row robot almost touching target policy shown term subspace spanned two ultrasound distance sensor found fore aft robot distance robot nearest obstacle front behind robot back obstacle way forward clear upper left corner policy graph tends go forward way forward obstructed nothing behind robot lower right corner robot tends turn move backward 1800 200 100 200 400 600 800 1000 1200 1600 1400 1800 600 1000 1200 1400 1600 0 average reinforcement reinforcement 0 0 500 200 800 0 400 2000 1500 1000 b figure 2 learning performance plot show reward robot receives learning b plot show reward averaged previous within trial dashed line indicate beginning new trial target moved new location b figure 3 robot arena learned trajectory picture show two typical learning scenario robot obstacle target blue plastic teapot initial location robot must learn ﬁnd target using limited visual ﬁeld video camera move avoiding obstacle robot start trial facing away target turning right navigating around obstacle goal along way encounter two diﬃculties shown rectangular highlight caused ﬁrst going narrow passage gap right wall front sonar return strong signal compared immediately surrounding wall specularly reﬂected b robot unoccluded arena learning trial robot drive almost directly target turning see held dark wall gap momentarily