reinforcement imitation learning diverse visuomotor skill yuke ziyu josh andrei tom serkan saran ano raia nando de nicolas science department stanford university usa london uk propose deep reinforcement ing method leverage small amount demonstration data assist reinforcement learning agent apply approach robotic manipulation task train visuomotor policy map directly rgb camera input joint velocity demonstrate approach solve wide variety visuomotor task engineering scripted controller would laborious experiment reinforcement imitation agent achieves signiﬁcantly better performance agent trained reinforcement learning imitation learning alone also illustrate policy trained large visual dynamic variation achieve preliminary success transfer brief visual description work viewed video introduction recent advance deep reinforcement learning rl performed well several challenging domain video game 29 go 46 robotics rl combination powerful function approximators neural network provides general framework designing sophisticated controller would hard handcraft erwise reinforcement learning method long history robotics control typically used dimensional movement representation 4 20 last year seen growing number successful tions deep rl robotic manipulation using levine et al 23 yahya et al 52 levine et al 24 technique chebotar et al 3 gu et al 9 popov et al 35 simulation real hardware nevertheless learning visuomotor controller manipulation task using free rl technique remains challenging problem developing rl agent robotics requires overcoming eral signiﬁcant challenge policy robotics must transform partial observation noisy sensor camera coordinated activity many degree dom time realistic task often come rich dynamic vary along multiple dimension visual appearance position shape etc posing signiﬁcant ization challenge method difﬁculties handling complex dynamic large variation directly training method real robotics hardware daunting due high sample complexity difﬁculty work wa done yuke zhu yukez worked summer intern deepmind motion controller collecting demonstration physic engine training simulation real environment running real robot fig 1 proposal principled robot learning pipeline used motion controller collect human demonstration task reinforcement imitation learning model leveraged demonstration facilitate learning ulated physical engine performed transfer deploy learned visuomotor policy real robot rl training compounded safety consideration well difﬁculty accessing information state environment position object deﬁne reward function finally even simulation perfect state information large amount training data available exploration signiﬁcant challenge especially method partly due often dimensional continuous action space also due difﬁculty designing suitable reward function paper present deep rl method solve variety robotic manipulation task directly pixel input key insight 1 reduce difﬁculty exploration continuous domain leveraging handful human demonstration 2 leverage several new technique exploit privileged information training only accelerate stabilize learning visuomotor policy task 3 improve generalization increasing diversity training condition result policy work well signiﬁcant variation system dynamic object appearance task length etc furthermore demonstrate promising preliminary result two task policy trained simulation achieve transfer real robot evaluate method six manipulation task ing stacking pouring etc set task includes stage task require full joint 27 may 2018 velocity control directly pixel controller need able handle signiﬁcant shape appearance variation address challenge method combine imitation learning reinforcement learning uniﬁed training framework approach utilizes demonstration data two way ﬁrst us hybrid reward combine task reward imitation reward based generative versarial imitation learning 15 aid exploration still allowing ﬁnal controller outperform human demonstrator task second us demonstration trajectory construct curriculum state along initialize episode training enables agent learn later stage task earlier training facilitating solving long task result approach solves six task neither reinforcement learning imitation learning baseline solve alone sidestep constraint training real hardware embrace paradigm ha recently shown promising result 17 39 47 use physic engine rl algorithm simulate parallel copy robot arm perform million plex physical interaction environment eliminating practical concern robot safety system reset furthermore training exploit privileged information true system state several new technique including learning policy value separate modality gail discriminator auxiliary task visual module technique stabilize speed policy learning without imposing any constraint system test time finally diversify training condition visual appearance object geometry system dynamic improves generalization respect different task condition well transfer simulation reality use model algorithm only small modiﬁcations training setup learn visuomotor controller six diverse robot arm manipulation task illustrated fig 1 instantiates visuomotor learning pipeline going collecting human demonstration learning simulation back deployment via policy transfer ii related work reinforcement learning method extensively used policy representation ment primitive solve variety control problem simulation reality three class rl algorithm currently dominant continuous control problem guided policy search method gps levine koltun 22 based method deterministic policy gradient dpg silver et al 45 lillicrap et al 26 heess et al 12 normalized advantage function naf gu et al 10 algorithm based policy gradient algorithm trust region policy optimization trpo proximal policy optimization ppo trpo 42 ppo 43 hold peal due robustness hyperparameter setting well scalability 14 lack sample efﬁciency make unsuitable training directly robotics hardware gps 22 ha used levine et al 23 yahya et al 52 chebotar et al 3 learn visuomotor policy directly real robotics hardware network pretraining phase gupta et al 11 kumar et al 21 use gps learning controller robotic hand model method employed gu et al 9 use naf learn door opening task directly robot popov et al 35 demonstrate solve stacking problem efﬁciently using distributed variant dpg idea using data collection training visuomotor controller ha focus levine et al 24 pinto gupta 33 train convolutional network predict grasp success diverse set object using large dataset thousand grasp attempt collected multiple robot setting alternative strategy dealing data demand train simulation transfer learned controller real hardware augment training synthetic data rusu et al 40 learn simple visuomotor policy jaco robot arm transfer reality using progressive network 39 viereck et al 50 minimize reality gap relying depth tobin et al 47 use visual variation learn robust object detector transfer reality james et al 17 combine randomization supervised learning bousmalis et al 2 augment training simulated data learn grasp prediction diverse shape suitable cost function exploration strategy control problem challenging design demonstration long played important role demonstration used initialize policy design cost function guide exploration augment training data combination cost function derived demonstration either via tracking objective gupta et al 11 via inverse rl boularias et al 1 finn et al 6 case via adversarial learning 15 expert action expert policy available behavioral cloning used rahmatizadeh et al 36 james et al 17 duan et al 5 alternatively expert trajectory used additional training data algorithm dpg vecerik et al 49 method require observation action space aligned robot demonstration recently method third person imitation proposed sermanet et al 44 liu et al 27 finn et al 7 concurrently work several paper presented result manipulation task rajeswaran et al 37 nair et al 30 use human demonstration aid exploration nair et al 30 extends ddpgfd algorithm 49 learn block stacking task arm simulation rajeswaran et al 37 use demonstration form behavioral cloning data augmentation learn several complex manipulation task case controller observe state feature method inherently require aligned state action space demonstration pixel observation proprioceptive feature feature cnn mlp lstm lstm joint velocity value function state prediction auxiliary task mlp deep visuomotor policy mlp discriminator score gail discriminator mlp vφ fig 2 model overview core model deep visuomotor policy take camera observation proprioceptive feature input produce next joint velocity contrast method learns visuomotor policy without reliance demonstrator action thus utilize demonstration raw demonstrator action unknown generated different body pinto et al 34 peng et al 32 address transfer simulation reality focusing randomizing visual appearance robot ic respectively peng et al transfer policy operating state feature position controlled fetch robotics arm pinto et al consider different task using visual input position control rent work introduced subset technique model employ work developed independently concurrent work integrates several new technique one coherent method experimental result demonstrate good performance come synergy combined technique iii model goal learn visuomotor policy deep ral network robot manipulation task policy take rgb camera observation proprioceptive ture vector describes joint position angular velocity two sensory modality also available real robot allowing u train simulation subsequently transfer learned policy robot without modiﬁcations fig 2 provides overview model deep visuomotor policy encodes pixel observation convolutional network cnn proprioceptive feature multilayer perceptron mlp feature two module concatenated passed recurrent long short term memory lstm layer producing joint velocity control command whole network trained start brief review basic generative adversarial imitation learning gail proximal policy optimization ppo model extends upon two method visuomotor skill background gail ppo imitation learning il problem learning behavior policy mimicking set demonstration assume human demonstration provided dataset action pair si ai some il method cast problem one supervised learning behavior cloning method use maximum likelihood train ized policy πθ state space action space arg maxθ p n log πθ behavior cloning approach work effectively strations abundant 38 however robot demonstration costly collect aim method learn handful demonstration gail 15 us demonstration data efﬁciently allowing agent interact environment learn experience similar generative adversarial network gans 8 gail employ two network policy network πθ discriminator network dψ 0 1 us objective function similar gans min θ max ψ eπe log dψ eπθ log 1 1 πe denotes expert policy generated stration trajectory objective encourages policy πθ occupancy measure close expert policy work train πθ policy gradient method maximize discounted sum reward function rgail st 1 st clipped max value continuous domain trust region method greatly stabilize policy training gail wa originally presented combination trpo 42 updating policy recently ppo 43 ha proposed simple scalable proximation trpo ppo only relies gradient easily implemented recurrent network distributed setting 14 ppo implement approximate trust region limit change policy per iteration achieved via regularization term based leibler kl divergence strength adjusted dynamically depending actual change policy past iteration reinforcement imitation learning model 1 hybrid reward shaping reward popular mean facilitating exploration although reward shaping effective also lead suboptimal solution 31 hence design task reward sparse piecewise constant function based different stage respective task example deﬁne three stage block stacking task including reaching lifting stacking reward change only occurs task transit one stage another practice ﬁnd deﬁning sparse reward easier handcrafting dense shaping reward le prone producing suboptimal behavior training agent continuous domain sparse piecewise constant reward challenging inspired reward augmentation described li et al 25 merel et al 28 provide additional guidance via hybrid reward function combine imitation reward rgail task reward rtask r st λrgail st rtask st λ 0 1 2 maximizing hybrid reward interpreted multaneous reinforcement imitation learning imitation reward encourages policy generate trajectory closer demonstration trajectory task reward courage policy achieve high return task setting λ either 0 1 reduces method standard rl gail setup experiment balanced contribution two reward agent solve task neither gail rl solve alone ﬁnal agent achieve higher return human demonstration owing exposure task reward 2 leveraging physical state simulation physic simulator employ training expose full state system even though privileged information available real system take advantage training policy simulation propose four technique leveraging physical state simulation stabilize accelerate learning 1 use curriculum derived demonstration state 2 use privileged information value function baseline 3 use feature discriminator 4 auxiliary task elaborate four technique follows demonstration curriculum problem exploration continuous domain exacerbated long duration realistic task previous work indicates shaping distribution start state towards state optimal policy tends visit greatly improve policy learning 18 35 alter start state distribution demonstration state build curriculum contains cluster state different stage task instance deﬁne three cluster pouring task including reaching mug grasping mug pouring training probability ϵ start episode random initial state probability 1 uniformly select cluster initialize episode demonstration state cluster possible since simulated system fully characterized physical state learning value function state ppo us able value function vφ estimate advantage required compute policy gradient training ppo worker executes policy k step us discounted sum reward value advantage function estimator ˆ pk st γ discount factor policy gradient relies value function reduce variance beneﬁcial accelerate learning value function rather using pixel input similar policy network take advantage physical state position velocity object robot arm train value vφ smaller multilayer perceptron ﬁnd training policy value two different modality stabilizes training reduces oscillation agent performance technique ha also proposed concurrently pinto et al 34 discriminator value tion exploit availability physical state gail discriminator provide task speciﬁc feature input ﬁnd representation absolute relative position object provide salient relevant signal discriminator state robot arm contrast lead discriminator focus irrelevant aspect behavior controller detrimental training policy inspired information hiding strategy used locomotion domain 13 28 discriminator only take feature input masking information construction representation requires certain amount domain knowledge task ﬁnd relative position object displacement gripper object usually provide informative characterization task empirically ﬁnd model not sensitive particular choice feature long carry sufﬁcient information provide detailed description appendix state prediction auxiliary task auxiliary task shown effective improving learning efﬁciency ﬁnal performance deep rl method 16 cilitate learning visuomotor policy add state prediction layer top cnn module predict location object camera observation use layer regress coordinate object task minimizing loss predicted object location auxiliary task not required model learn good visuomotor policy however adding additional supervision often accelerate training cnn module 3 policy transfer perform policy transfer experiment kinova jaco robot arm lation wa manually adjusted roughly match appearance dynamic laboratory setup kinect camera wa visually calibrated match position orientation pouring liquid block lifting order fulfillment clearing table box block stacking clearing table block block lifting real block stacking real fig 3 visualization six manipulation task experiment left column show rgb image six task simulated environment image correspond actual pixel observation input visuomotor policy right column show two task color block real robot simulated camera simulation dynamic parameter manually adjusted match dynamic real arm instead using professional calibration equipment approach policy transfer relies domain domization camera position orientation 17 47 contrast some previous work trained policy not rely any object position information intermediate goal rather learn mapping raw pixel input joint velocity addition improve robustness controller latency effect real robot also tune policy subjecting action dropping detailed description available appendix iv experiment demonstrate approach offer ﬂexible framework visuomotor policy learning end uate performance six manipulation task illustrated fig provide additional qualitative result video environment setup use kinova jaco arm ha 9 degree freedom six arm joint three actuated ﬁngers robot arm interacts diverse set object tabletop visuomotor policy control robot setting joint velocity mands producing continuous velocity range 1 proprioceptive feature consist position angular velocity arm joint ﬁngers visual observation scene provided via suitably positioned rgb camera oceptive feature camera observation available simulation real environment thus enabling policy transfer physical environment simulated mujoco physic simulator 48 use large variety object ranging basic geometric shape procedurally generated object built ensemble primitive shape increase sity object randomizing various physical property including dimension color mass friction etc collect demonstration using spacenavigator motion controller allows u operate robot arm position troller gather 30 episode demonstration task including observation action physical state episode take le minute complete demonstrating task done within half hour robot arm manipulation task fig 3 show six manipulation task experiment ﬁrst column show six task simulated ments second column show setup block lifting stacking task see obvious visual discrepancy task simulation reality six task exhibit learning challenge varying degree ﬁrst three task use simple colored block make easy replicate similar setup real robot study policy transfer block lifting stacking task sec block lifting goal grasp lift randomized block allowing u evaluate model robustness vary several random factor including robot arm dynamic friction armature lighting condition camera pose background color well property block episode start new conﬁguration random factor uniformly drawn preset range block stacking goal stack one block top block together block lifting task evaluated transfer experiment clearing table block task requires lifting two block tabletop one solution stack block lift together task requires longer time dexterous controller introducing signiﬁcant challenge exploration next three task involve large variety procedurally generated shape making difﬁcult recreate real block lifting b block stacking c clearing table block clearing table box e pouring liquid f order fulﬁllment fig 4 learning efﬁciency reinforcement imitation model baseline plot averaged 5 run different random seed policy use network architecture hyperparameters except λ environment use examine model ability generalize across object variation long complex task clearing table box goal clear tabletop ha box toy car one strategy grasp toy put box lift box box toy car randomly generated episode pouring liquid modeling reasoning deformable object ﬂuids challenge robotics community 41 design pouring task use many small sphere simulate liquid goal pour liquid one mug container task particularly challenging due dexterity required even human struggled demonstrate task motion controller extensive practice order fulﬁllment task randomly place variable number procedurally generated toy plane car table goal place plane green box car red box task requires policy generalize abstract level need recognize object category perform successful grasp diverse shape handle task variable length quantitative evaluation full model solve six task only occasional failure using policy network training algorithm ﬁxed set hyperparameters contrary neither reinforcement imitation alone solve task compare full model three baseline respond pure rl pure gail rl demonstration curriculum baseline use setup full model except set λ 0 rl λ 1 gail model us balanced contribution hybrid reward λ third baseline training episode start random initial state rather resetting demonstration state standard rl setup report mean episode return function number training iteration fig full model achieves highest return six task only case baseline model par full model block lifting task rl baseline full model achieved similar level performance hypothesize due short length lifting task random exploration provide sufﬁcient learning signal without aid demonstration ﬁve task full model outperforms reinforcement learning imitation learning baseline large margin demonstrating effectiveness combining reinforcement imitation learning complex task comparing two variant rl without using demonstration curriculum see pronounced effect altering start state distribution see rl scratch lead slow learning progress initiating episode along demonstration jectories enables agent train state different stage task result greatly reduces burden exploration improves learning efﬁciency also report mean episode return human demonstration iteration million 100 200 300 400 500 600 average episode return full model no auxiliary task no rnn policy no action discriminator no discriminator mask gail demo curriculum rl demo curriculum learning value pixel ablation study model component b model sensitivity λ value fig 5 model analysis stacking task left investigate impact performance removing individual component full model right investigate model sensitivity hyperparameter λ moderate contribution reinforcement imitation ﬁgures demonstration motion controller imperfect especially pouring see video trained agent exceed performance human operator two ﬁndings noteworthy first rl agent learns faster full model clearing block task full model eventually outperforms full model discovers novel strategy different strategy employed human operator see video case tation gave contradictory signal eventually reinforcement learning guided policy towards better strategy second pouring liquid only task gail outperforms rl counterpart imitation effectively shape agent behavior towards demonstration trajectory 51 viable solution pouring task controller generates behavior complete task domain sufﬁcient variation however controller trained only small number demonstration struggle handle complex dynamic generalize appropriately novel instance task hypothesize baseline rl agent outperforms gail agent ﬁve task perform ablation study block stacking task understand impact different component model fig trained agent number conﬁgurations single modiﬁcation full model see ablation cluster two group agent learn stack average return greater 400 agent only learn lift average return 200 300 result indicate hybrid reward learning value function state centred feature discriminator play integral role learning good policy using only rl gail reward learning value function pixel providing full arm state discriminator input no discriminator mask result inferior performance contrast optional ponents include recurrent policy core lstm use state prediction auxiliary task whether include action discriminator input result suggests model learn visuomotor policy without pretraining phase need auxiliary task opposed previous work visuomotor learning 3 23 52 furthermore work gail discriminator only ha access demonstration state without accompanying demonstrator action therefore potentially use demonstration lected different body underlying control unknown different robot actuator examine model sensitivity λ value eq see fig model work well broad range λ value provide balanced mix rl gail reward policy transfer result ass robustness policy evaluate transfer no additional training real jaco arm setup wa roughly matched simulation environment including camera position robot kinematics approximate object size color execute trained policy network robot count number successful trial lifting stacking task arm position randomly initialized target block placed number repeatable start conﬁgurations task transfer lifting policy ha success rate 64 25 trial split 5 block conﬁgurations stacking policy ha success rate 35 20 trial split 2 block conﬁgurations 80 stacking trajectory however contain ful lifting behavior 100 contains successful reaching behavior impractical conduct fair comparison previous work 17 47 50 implemented different task different conﬁgurations work closest setup progressive network 39 40 ha demonstrated block reaching behavior rl policy jaco arm work not demonstrate any lifting stacking behavior method ha achieved reaching behavior 100 success rate qualitatively policy notably robust even failed attempt stacking policy repeatedly chase block get successful grasp trying stack see video detailed description result refer appendix several aspect system mismatch constrained policy attaining better performance real robot although sim real domain similar still sizable reality gap make transfer challenging example simulated block rigid object employed setup foam block deform bounce unpredictably furthermore neural network policy sensitive subtle discrepancy simulated rendering real camera frame nonetheless preliminary success achieved policy offer good starting point future work leverage small amount experience enable better transfer discussion paper described general deep reinforcement learning method learning policy operate rgb camera image form manipulation using joint velocity control method combine use demonstration via generative adversarial imitation learning 15 rl achieve effective learning difﬁcult task robust generalization approach only requires small number demonstration trajectory 30 per task experiment additionally approach work state trajectory without demonstrator action combined use only demonstration seen discriminator simplify increase ﬂexibility data collection facilitate generalization beyond condition seen demonstration demonstration could potentially lected different body human demonstrator via motion capture demonstration collected via operation simulated arm le thirty minute per task method integrates several new technique leverage ﬂexibility scalability afforded simulation access privileged information use rl algorithm experimental result demonstrated effectiveness complex manipulation task simulation achieved preliminary success transfer real hardware trained policy policy network training algorithm hyperparameters approach make some use speciﬁc information especially choice centric feature discriminator rl reward practice found speciﬁcation feature intuitive method wa reasonably robust speciﬁc choice thus striking favorable balance need limited prior knowledge generality solution learned complex task order fulﬁll potential deep rl robotics essential confront full variability including diversity object appearance system dynamic task semantics etc therefore focused learning controller could handle signiﬁcant task variation along multiple dimension improve policy ability alize increased diversity training condition parameterized procedurally generated object randomized system dynamic ha resulted policy exhibit robustness large variation simulation well some domain discrepancy simulation real world simulation center method training lation circumvents several practical challenge deep rl robotics access state information reward iﬁcation high sample complexity safety consideration training simulation also allows u use simulation state facilitate stabilize training providing state information value function experiment ha important learning good visuomotor policy however even though method utilizes privileged information training ultimately produce policy only rely vision proprioceptive information arm thus deployed real hardware executing policy real robot reveals remains sizable domain gap simulation real hardware transfer affected visual discrepancy well difference arm dynamic physical property environment lead certain level performance degradation running simulation policy real robot still experiment exempliﬁed transfer achieve initial success rl trained policy performing velocity control vi conclusion shown combining reinforcement imitation learning considerably improves ability train system capable solving challenging dexterous manipulation task pixel method implement three stage pipeline robot skill learning ﬁrst collected small amount demonstration data simplify exploration problem second relied physical simulation perform distributed robot training third performed transfer deployment future work seek improve sample efﬁciency learning method leverage experience close reality gap policy transfer acknowledgment author would like thank yuval tassa jonathan scholz thomas orl jonathan hunt many league deepmind helpful discussion feedback reference 1 abdeslam boularias jens kober jan peter relative entropy inverse reinforcement learning aistats page 2011 2 konstantinos bousmalis alex irpan paul wohlhart yunfei bai matthew kelcey mrinal kalakrishnan laura julian ibarz peter pastor kurt konolige sergey levine vincent vanhoucke using simulation domain adaptation improve efﬁciency deep robotic grasping arxiv preprint 2017 3 yevgen chebotar mrinal kalakrishnan ali yahya adrian li stefan schaal sergey levine path integral guided policy search icra 2017 4 marc peter deisenroth gerhard neumann jan peter et al survey policy search robotics foundation trend robotics 2 2013 5 yan duan marcin andrychowicz bradly stadie jonathan ho jonas schneider ilya sutskever pieter abbeel wojciech zaremba imitation learning arxiv preprint 2017 6 chelsea finn sergey levine pieter abbeel guided cost learning deep inverse optimal control via policy optimization icml page 2016 7 chelsea finn tianhe yu tianhao zhang pieter abbeel sergey levine visual imitation learning via arxiv preprint 2017 8 ian goodfellow jean mehdi mirza bing xu david sherjil ozair aaron courville yoshua bengio generative adversarial net nip page 2014 9 shixiang gu ethan holly timothy lillicrap sergey levine deep reinforcement learning robotic manipulation arxiv preprint 2016 10 shixiang gu tim lillicrap ilya sutskever sergey levine continuous deep acceleration icml 2016 11 abhishek gupta clemens eppner sergey levine pieter abbeel learning dexterous manipulation soft robotic hand human demonstration arxiv preprint 2016 12 nicolas heess gregory wayne david silver tim icrap tom erez yuval tassa learning continuous control policy stochastic value gradient nip page 2015 13 nicolas heess greg wayne yuval tassa timothy licrap martin riedmiller david silver learning transfer modulated locomotor controller arxiv preprint 2016 14 nicolas heess srinivasan sriram jay lemmon josh merel greg wayne yuval tassa tom erez ziyu wang ali eslami martin riedmiller et al emergence motion behaviour rich environment arxiv preprint 2017 15 jonathan ho stefano ermon generative adversarial imitation learning nip page 2016 16 max jaderberg volodymyr mnih wojciech marian czarnecki tom schaul joel z leibo david ver koray kavukcuoglu reinforcement ing unsupervised auxiliary task arxiv preprint 2016 17 stephen james andrew davison edward john transferring visuomotor control lation real world task arxiv preprint 2017 18 sham kakade john langford approximately mal approximate reinforcement learning icml 2002 19 diederik kingma jimmy ba adam method stochastic optimization arxiv preprint 2014 20 jens kober jan peter reinforcement learning robotics survey reinforcement learning page springer 2012 21 vikash kumar abhishek gupta emanuel todorov sergey levine learning dexterous manipulation cies experience imitation arxiv preprint 2016 22 sergey levine vladlen koltun guided policy search icml page 2013 23 sergey levine chelsea finn trevor darrell pieter abbeel training deep visuomotor policy arxiv preprint 2015 24 sergey levine peter pastor alex krizhevsky deirdre quillen learning coordination robotic grasping deep learning data collection arxiv preprint 2016 25 yunzhu li jiaming song stefano ermon inferring latent structure human raw visual input arxiv preprint 2017 26 timothy p lillicrap jonathan j hunt alexander pritzel nicolas heess tom erez yuval tassa david silver daan wierstra continuous control deep ment learning iclr 2016 27 yuxuan liu abhishek gupta pieter abbeel sergey levine imitation observation learning imitate behavior raw video via context translation arxiv preprint 2017 28 josh merel yuval tassa dhruva tb sriram vasan jay lemmon ziyu wang greg wayne nicolas heess learning human behavior tion capture adversarial imitation arxiv preprint 2017 29 volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc g bellemare alex graf martin riedmiller andreas k fidjeland georg ostrovski et al control deep forcement learning nature 518 7540 2015 30 ashvin nair bob mcgrew marcin andrychowicz jciech zaremba pieter abbeel overcoming ploration reinforcement learning demonstration arxiv preprint 2017 31 andrew ng daishi harada stuart j russell policy invariance reward transformation theory application reward shaping icml page 287 1999 32 xue bin peng marcin andrychowicz wojciech zaremba pieter abbeel transfer robotic control dynamic randomization arxiv preprint october 2017 33 lerrel pinto abhinav gupta supersizing supervision learning grasp try 700 robot hour arxiv preprint 2015 34 lerrel pinto marcin andrychowicz peter welinder jciech zaremba pieter abbeel asymmetric actor critic robot learning arxiv 2017 35 ivaylo popov nicolas heess timothy lillicrap roland hafner gabriel matej vecerik thomas lampe yuval tassa tom erez tin riedmiller deep reinforcement learning dexterous manipulation arxiv preprint 2017 36 rouhollah rahmatizadeh pooya abolghasemi lau oni sergey levine task manipulation inexpensive robot using learning demonstration arxiv preprint 2017 37 aravind rajeswaran vikash kumar abhishek gupta john schulman emanuel todorov sergey levine learning complex dexterous manipulation deep inforcement learning demonstration arxiv preprint 2017 38 ephane ross geoffrey j gordon drew bagnell reduction imitation learning structured prediction online learning aistats page 2011 39 andrei rusu neil rabinowitz guillaume desjardins hubert soyer james kirkpatrick koray kavukcuoglu razvan pascanu raia hadsell progressive neural network arxiv preprint 2016 40 andrei rusu matej vecerik thomas orl nicolas heess razvan pascanu raia hadsell robot learning pixel progressive net arxiv preprint 2016 41 connor schenck dieter fox reasoning liquid via simulation arxiv preprint 2017 42 john schulman sergey levine pieter abbeel michael jordan philipp moritz trust region policy tion icml page 2015 43 john schulman filip wolski prafulla dhariwal alec radford oleg klimov proximal policy optimization algorithm arxiv preprint 2017 44 pierre sermanet corey lynch jasmine hsu sergey levine network learning observation arxiv preprint 2017 45 david silver guy lever nicolas heess thomas degris daan wierstra martin riedmiller deterministic policy gradient algorithm icml 2014 46 david silver aja huang chris j maddison arthur guez laurent sifre george van den driessche julian schrittwieser ioannis antonoglou veda vam marc lanctot et al mastering game go deep neural network tree search nature 529 7587 2016 47 joshua tobin rachel fong alex ray jonas schneider wojciech zaremba pieter abbeel domain ization transferring deep neural network lation real world arxiv preprint 2017 48 emanuel todorov tom erez yuval tassa mujoco physic engine control iros page 2012 49 matej vecerik todd hester jonathan scholz fumin wang olivier pietquin bilal piot nicolas heess thomas orl thomas lampe martin miller leveraging demonstration deep ment learning robotics problem sparse reward arxiv preprint 2017 50 ulrich viereck andreas ten pa kate saenko robert platt learning visuomotor controller real world robotic grasping using easily simulated depth image arxiv preprint 2017 51 ziyu wang josh merel scott reed greg wayne nando de freitas nicolas heess robust imitation diverse behavior nip 2017 52 ali yahya adrian li mrinal kalakrishnan yevgen chebotar sergey levine collective robot forcement learning distributed asynchronous guided policy search arxiv preprint appendix experiment detail policy network take pixel observation proprioceptive feature input pixel observation rgb image size 64 64 used kinect xbox one real environment proprioceptive feature describes joint position velocity kinova jaco joint position represented sin co angle joint joint coordinate joint velocity represented scalar angular velocity result proprioceptive feature contains position velocity six arm joint position three ﬁngers exclude ﬁnger velocity due noisy sensory reading real robot collecting demonstration use dof spacenavigator motion command end effector complete task used adam 19 train neural network parameter set learning rate policy value respectively discriminator auxiliary task pixel observation encoded layer convolutional network use 2 convolutional layer followed layer 128 hidden unit ﬁrst convolutional layer ha 16 8 8 ﬁlters stride 4 second 32 ﬁlters stride add recurrent layer 100 lstm unit policy value output policy output mean standard deviation conditional gaussian distribution joint velocity initial policy standard deviation set exp clearing table block task exp ﬁve task auxiliary head policy contains separate mlp sitting top convolutional network ﬁrst two layer mlp ha 200 100 hidden unit respectively third layer predicts auxiliary output finally discriminator simple mlp 100 64 hidden unit ﬁrst two layer third layer producing log probability network use tanh nonlinearities trained visuomotor policy using distributed ppo algorithm 14 synchronous gradient update 256 cpu worker worker run policy complete entire episode parameter update computed set constant episode length task based difﬁculty longest 1000 time step 50 second clearing table block order fulﬁllment task set k 50 number time step computing return truncated backpropagation time train lstm unit worker collect batch data point performs 50 parameter update policy value network 5 discriminator 5 auxiliary prediction network 1 2 3 compact table block lifting success rate different position lr ul ur c represent position lower left lower right upper left upper right center respectively lr ul ur c no action dropping action dropping table ii success rate block stacking agent action dropping different starting position left right indicate position support block upon ization left right stacking success rate lifting success rate appendix b detail better facilitate transfer lower frequency sample observation pixel observation only observed rate despite fact controller run similarly proprioceptive feature observed rate addition observation delay also apply domain variation gaussian noise standard deviation added proprioceptive feature uniform integer noise range 5 added pixel independently pixel value outside range 0 255 clipped also vary randomly shade grey jaco arm color table top well location orientation light source see fig 6 case block lifting vary addition dynamic arm speciﬁcally dynamically change friction damping armature gain parameter robot arm simulation enhance agent robustness action dropping analysis indicates real robot often delay execution action amount delay also varies signiﬁcantly ha adverse effect performance agent physical robot since agent performance depends timely execution action better facilitate transfer real robot trained agent simulation subjecting random chance dropping action speciﬁcally action emitted agent ha 50 chance executed immediately case action ﬂagged last executed action current action not executed last executed action executed using procedure agent block lifting block stacking 2 million iteration demonstrate effectiveness action dropping compare agent real robot task block lifting without action dropping baseline agent lift 48 percent time using action dropping agent succeeded 64 percent time complete set result please see table table ii fig 6 tile show representative range diversity seen variation color lighting background etc appendix c task detail use ﬁxed episode length task mined amount time skilled human demonstrator complete task episode terminates maximum number agent step performed robot arm operates control frequency mean time step take second segment sequence stage represent agent progress task instance block stacking task characterized three stage including reaching block lifting block stacking block deﬁne function underlying physical state determine stage state way cluster demonstration state according corresponding stage cluster used reset training episode demonstration curriculum technique proposed sec deﬁnition stage also give rise convenient way specifying reward function without shaping reward deﬁne piecewise constant reward function task assign constant reward state belong stage detail stage reward function auxiliary task feature six task experiment block lifting episode last 100 time step deﬁne three stage reward parenthesis initial 0 reaching block lifting block auxiliary task predict coordinate color block feature consists relative position gripper block block stacking episode last 500 time step deﬁne four stage reward initial 0 reaching orange block lifting orange block stacking orange block onto pink block auxiliary task predict coordinate two block feature consists relative position gripper two block respectively clearing table block episode last 1000 time step deﬁne ﬁve stage reward initial 0 reaching orange block lifting orange block stacking orange block onto pink block lifting block ground auxiliary task predict coordinate two block feature consists position two block well relative position gripper two block respectively clearing table box episode last 500 time step deﬁne ﬁve stage reward initial 0 reaching toy grasping toy putting toy box lifting box auxiliary task predict coordinate toy box feature consists position toy box well relative position gripper two object respectively pouring liquid episode last 500 time step deﬁne three stage reward initial 0 grasping mug pouring n number small sphere container auxiliary task predict coordinate mug feature consists position mug relative position gripper mug relative position mug container order fulﬁllment episode last 1000 time step number object varies 1 4 across episode deﬁne ﬁve stage correspond number toy box immediate reward corresponds number toy placed correct box number toy plane green box toy car red box handle variable number object only represent object nearest gripper auxiliary task feature auxiliary task predict coordinate nearest plane nearest car gripper feature consists relative position gripper two nearest object