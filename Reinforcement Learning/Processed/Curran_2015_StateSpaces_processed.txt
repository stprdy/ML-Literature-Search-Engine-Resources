using pca efﬁciently represent state space william curran curranw oregon state university corvallis oregon tim brys timbrys vrije universiteit brussel brussels belgium matthew taylor taylorm washington state university pullman washington william smart oregon state university corvallis oregon abstract reinforcement learning algorithm need deal exponential growth state tions exploring optimal control space known curse dimensionality projecting agent state onto manifold represent state space smaller efﬁcient representation using representation learning agent converge good policy much faster test approach mario benchmarking domain using dimensionality reduction mario learning converges much faster good policy critical projecting onto manifold ignoring important data paper explore convergence performance ﬁnd learning 4 dimension stead 9 improve performance past learning full dimensional space faster convergence rate introduction learning high dimensional space necessary difﬁcult robotic application state action space robotics become large continuous scale exponentially number joint lead curse dimensionality address issue researcher developed transfer learning pan yang 2010 learning demonstration argall et 2009 approach transfer learning reduces tional complexity learning simple domain transferring knowledge plex domain transfer learning three key research question transfer transfer transfer question difﬁcult answer completely domain dependent source domain target domain loosely related straight forward transfer learning doe not work lead worse performance pan yang 2010 1 3 jun 2015 transfer learning also requires computable mapping source domain target domain always possible learning demonstration lfd method speed convergence time bootstrapping ing demonstration argall et 2009 lfd learns policy using example demonstration provided human robotic teacher method extract pair strations bootstrap learning however demonstration must consistent accurately represent solving task method also solve speciﬁc complex task rather solve general control argall et 2009 work focus core problem dealing large state space approach issue projecting full state space onto manifold use principal component analysis ﬁnd transform use learning iteration way perform learning only space lead critical projecting onto manifold throwing low variance yet potentially important data however learning converge good yet suboptimal policy much faster paper explore convergence performance organize rest paper follows section 2 describes related work ality reduction machine learning reinforcement learning section 3 describe work pca dimensionality reduction reinforcement learning section 4 introduces mario benchmarking domain learning approach experimental result section 5 followed conclusion related work section 6 7 related work motivate approach introduce previous work performed ﬁeld dimensionality reduction dimensionality reduction previous work dimensionality reduction focus reducing space classiﬁcation tion approximation pca effective many machine learning data mining application extract feature large data set pec turk pentland 1991 rather using pca feature extraction large data set use pca reduce dimensionality state space learning swinehart abbott 2005 used similar approach function approximation neural network ﬁnd reduce convergence time random walk learning reducing dimension parameter space liu mahadevan 2011 also use dimensionality reduction compute policy low dimensional subspace liu computes low dimensional subspace space random projection also reduce convergence time continuous state space space viable many scenario robot need perform complex manipulation task complex manipulation need high arm manipulator example robot ha two 7 dof arm learning position velocity acceleration control lead 2 21 dimensional state space per arm learning space computationally intractable without optimization technique learn state space algorithm ﬁrst computes transform state space space perform computation need trajectory across representative set agent state space use any dimensionality reduction technique learn transform work use principal component analysis pca shlens 2005 pca identiﬁes pattern data reduces dimension dataset minimal loss information doe computing transform convert correlated data linearly lated data transformation ensures ﬁrst principal component ha largest possible variance additional component ha largest possible variance uncorrelated ous component essentially pca represents much demonstrated state space possible lower dimension transform given xw 1 x demonstrated data w p p matrix whose column eigenvectors xt x p number principle component case number dimension transform any arbitrary dimension k choose k eigenvectors w largest eigenvalue form p k dimensional matrix wk tk xwk 2 use reinforcement learning learn trajectory new manifold learning space learning iteration project state x dimensional space k xk w k x 3 compute action using chosen learning algorithm execute action simulation simulation calculates new state given executed action project state space perform learning update figure 1 learning smaller space reinforcement learning algorithm converge must faster however pca not represent variance demonstration therefore given inﬁnite time converged learning performance always worse learning full space lead critical projecting onto manifold throwing low variance yet possibly important data yet learning still converge good yet suboptimal policy much faster mario benchmark problem mario benchmark problem karakovskiy togelius 2012 based inﬁnite mario bros public reimplementation original 80 game super mario bros r task mario need collect many point possible done killing enemy 10 devouring mushroom 58 ﬁreﬂower 64 grabbing coin 16 ﬁnding hidden block 24 ﬁnishing level 1024 getting hurt creature dying action available mario correspond button ne controller left right no direction jump 3 figure 1 flowchart jump mario take one action group taneously resulting 12 distinct combined super action state space mario quite complex mario observes exact location enemy screen type also observes information mode small big ﬁre lastly ha gridlike receptive ﬁeld cell indicates type object brick coin mushroom goomba enemy screenshot shown figure reinforcement learning agent mario inspired liao brys previous work brys et 2014 liao et 2012 use q λ tabular state representation part state space agent considers consists variable mario able jump 0 mario ground 0 mario able shoot ﬁreballs 0 mario current direction 8 direction standing still 0 enemy closeby within one gridcell 8 direction 28 enemy midrange within one three gridcells 8 direction 0 whether obstacle four vertical gridcells front mario 24 closest enemy position within grid surrounding mario 1 absent enemy 0 make possible state 12 action state size state space not problem computationally sparsely visited majority small set state liao et 2012 previous work ha investigated using demonstration mario domain speed ment learning albeit different way shaping reward using demonstration brys et 2015 learning reward function using inverse reinforcement learning lee et 2014 4 figure 2 screenshot mario experiment run every learning episode procedurally generated level based random seed 0 106 difﬁculty also randomly select mode mario start small large ﬁre episode making agent learn play mario way help avoid overﬁtting speciﬁc level make generally applicable mario agent result always averaged 100 different trial result preliminary analysis calculated principle component mario domain see dimension pca weighed highest learning jump ground current tion feature heavily represented ﬁrst principal component intuitive state change frequently throughout game mario feature also fundamental skill required play game mario pca also associated feature related enemy within close proximity mario entirely last principal component feature only important speciﬁc scenario mario need quickly react many nearby enemy only one enemy nearby also represented closest enemy x closest enemy feature analysis legitimizes approach mario benchmarking domain demonstrates initially learn using fundamental skill required play mario show learn skill quickly skill represented higher principal component better strict optimization speciﬁc scenario analyzing principal component demonstration sanity check well validation richness demonstration using approach mario domain result expected projecting state manifold le 4 learning algorithm converged quickly bad policy however using manifold 4 dimension greater converged quickly much better policy promising result although dimension may still converge suboptimal policy 5000 episode since learning wa poor ﬁrst two manifold show u jump ground feature not informative enough alone learn effective mario policy yet projecting onto 3 4 dimensional manifold see large increase policy performance manifold 5 figure 3 emphasis feature relative principal component feature jump ground current direction shoot closest enemy obstacle represented intuitive feature important basic skill mario remaining feature important only ﬁne tuning policy discussion strength approach simplicity generality combined fast convergence sample domain pca ran sample use transforms given pca learning cycle whenever compute new state simple matrix operation add no additional computational cost preliminary work show learning efﬁciently performed low dimensional space increased convergence rate some fundamental issue algorithm assumes state important possible state unnecessary learning high variance also stop learning converge space ensures given inﬁnite time higher dimensional space converge better performance discus solution problem section 7 using iterative learning approach future work many area future work approach take next step take would perform additional analysis current result experiment include varying amount training data given pca use random demonstration data see quality quantity data affect learning major focus future work improve upon converged performance dimensional state representation work grzes kudenko ha shown mixed resolution function approximation work well complex domain initially learned le sive function approximation provide early guidance learning learned using expressive function approximation learn high quality policy leverage similar 6 figure 4 result varying manifold line bold experiment performed better equal learning full state dimension greater 5 performed similarly full state space not included clarity error bar 100 statistical run idea propose future work learn convergence n dimensional space shown work convergence time low transfer knowledge n 1 dimensional space hypothesize learning technique converge faster learning entirely full dimensional space reference brenna argall sonia chernova manuela veloso brett browning survey robot learning demonstration robot auton 57 5 may issn doi 10 url tim brys anna harutyunyan peter vrancx matthew e taylor daniel kudenko ann reinforcement learning problem reward shaping international joint conference neural network ijcnn page ieee tim brys anna harutyunyan halit bener suay sonia chernova matthew taylor ann reinforcement learning demonstration shaping proceeding international joint conference artiﬁcial intelligence ijcai grzes kudenko reinforcement learning reward shaping mixed resolution function approximation nternational journal agent technology system ijats 1 2 7 sergey karakovskiy julian togelius mario ai benchmark competition tional intelligence ai game ieee transaction 4 1 geoffrey lee min luo fabio zambetta xiaodong li learning super mario controller example human play evolutionary computation cec 2014 ieee congress page ieee yizheng liao kun yi zhe yang ﬁnal report reinforcement learning play mario technical report stanford university usa bo liu sridhar mahadevan compressive reinforcement learning oblique random tions sinno jialin pan qiang yang survey transfer learning knowledge data engineering ieee transaction 22 10 oct issn doi jonathon shlens tutorial principal component analysis system neurobiology laboratory salk institute biological study christian swinehart abbott dimensional reduction learning turk pentland face recognition using eigenfaces computer vision pattern recognition proceeding cvpr ieee computer society conference page 591 jun doi 8