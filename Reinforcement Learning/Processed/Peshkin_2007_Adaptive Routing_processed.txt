arxiv 28 mar 2007 reinforcement learning adaptive routing leonid peshkin pesha virginia savova savova mit artiﬁcial intelligence lab john hopkins university cambridge 02139 baltimore md 21218 abstract reinforcement learning mean learning mapping observation based feedback environment ing viewed browsing set policy evaluating trial interaction environment present application gradient ascent algorithm reinforcement learning plex domain packet routing network tion compare performance algorithm routing method benchmark problem introduction successful telecommunication requires eﬃcient source allocation achieved developing adaptive control policy reinforcement learning rl 10 17 present natural framework development policy trial error process interaction environment work apply rl algorithm network routing eﬀective network routing mean selecting optimal communication path modeled agent rl problem sense learning optimal control network routing could thought learning some traditional rl episodic task like maze searching pole balancing repeating trial many time parallel interaction among trial interpretation individual router agent make routing decision according individual policy parameter policy justed according some measure global mance network control determined local observation node not any information regarding topology network position initialization node well learning algorithm follows identical every node independent structure network no notion orientation space mantics action approach allows u update local policy avoiding necessity centralized control global knowledge network structure only global information required learning gorithm network utility expressed reward nal distributed epoch dependent average routing time learning system biologically plausible could thought ral network neuron only performs simple computation based locally available quantity ii domain test algorithm domain adopted boyan littman 4 discrete time tor communication network various topology dynamic structure communication network abstract representation system internet transport network consists geneous set node edge ing link see figure 1 node linked called neighbor link may active tive node origin ﬁnal destination packet serve router packet periodically introduced network uniformly random node origin destination travel destination node hopping mediate node no packet generated destined node origin sending packet link cur cost could thought time transition added cost waiting queue ticular node order access router computational queue delay cost assumed uniform throughout network experiment set unit cost level network traﬃc determined number packet network packet reach destination removed packet ha traveling around network long time also removed hopeless case multiple ets line node fifo ﬁrst ﬁrst queue limited size node must forward top packet fifo queue one neighbor terminology rl network represents environment whose state determined number relative position node status link tween dynamic packet tion handled packet status local link form node observation node agent ha choice action decides send packet according policy policy computed rithm stochastic opposed deterministic sends packet bound destination ent link according some distribution policy sidered experiment doe not determine whether not accept packet admission control many packet accept neighbor packet assigned priority node update parameter policy based reward reward come form nal distributed network acknowledgment packet packet ha reached ﬁnal destination reward depends total delivery time packet measure performance algorithm average delivery time packet system ha settled policy ordinate ax ﬁgure 2 apply policy shaping explicitly penalizing loop route packet assumed carry some element routing history addition obvious destination origin information include time packet wa generated time packet last received attention some router trace recently visited node number hop performed far case packet detected spent much time network failing reach destination packet carded network penalized accordingly thus deﬁning factor simulation weather ber hop performed packet total number node network iii algorithmic detail williams introduced notion policy search via gradient ascent reinforcement learning force algorithm 18 19 wa generalized broader class error criterion baird moore 1 2 general idea adjust parameter direction empirically estimated gradient aggregate ward assume standard markov decision process mdp setup 10 let u consider case single agent teracting partially observable mdp pomdp agent policy µ reactive policy represented lookup table value θoa action pair policy deﬁnes probability action given past history continuous diﬀerentiable function set parameter θ according softmax rule ξ temperature parameter µ θ pr θ exp p exp rule assures any destination any link available node sometimes chosen some small probability dependent temperature denote ht set possible experience quences h 1 1 r 1 r length order specify some element part history h time τ write ple r τ h τ h τ th reward action history also use hτ denote ﬁx sequence h truncated time τ hτ def 1 1 r 1 τ τ r τ τ value following policy µ parameter θ pected cumulative discounted factor γ 0 1 reward written v θ x γt x pr h θ r h could calculate derivative v θ θoa would possible exact gradient ascent value v making update α v θ some step size let u analyze derivative weight θoa θ h r h γt p pr h θ r h pt pr τ h θ however spirit reinforcement learning sume no knowledge world model would allow agent calculate pr must retreat tic gradient ascent instead sample tion history interacting environment calculate trial estimate gradient cumulating quantity γtr h pt µ θ particular policy architecture readily translated gradient ascent algorithm anteed converge local optimum v θ chosen policy encoding get µ θ 0 ξµ θ 1 ξ 1 θ applying algorithm network connected troller basically constitutes algorithm routing distributed gradient ascent policy search gap compare performance distributed gap algorithm three others follows best static routing scheme based shortest path counting link single unit routing cost include gorithm provides basis current industry routing heuristic 3 8 bestload performs routing according shortest path taking account queue size node close retical optimum among deterministic routing algorithm even though actual best possible routing scheme quire not simply computing shortest path based network load also analyzing load change time according routing decision since calculating shortest path every single step simulation would prohibitively costly term computational resource implemented bestload readjusting routing policy only notable change load network consider 50 successfully delivered packet constitute notable load change finally distributed rl algorithm applied speciﬁcally main littman boyan 4 algorithm stochastic performs policy search terministic value search algorithm note mentation network routing simulation based software littman boyan used test even result simulation best network diﬀer slightly littman boyan due certain modiﬁcations traﬃc eling convention instance consider packet delivered ready removal only ha passed queue destination node accessed computational resource not merely packet successfully routed destination node immediate neighbor original simulation undertake comparison gap aforementioned algorithm one important caveat gap algorithm explores class stochastic cies method pick deterministic routing policy consequently natural expect gap superior certain type network topology load optimal policy stochastic later show experiment conﬁrm expectation implement distributed gap pomdp ticular represent router pomdp state contains size queue destination packet state link environment state transition function law dynamic work traﬃc observation consists destination packet action corresponds sending packet link adjacent node ﬁnally reward signal average number packet delivered per unit time agent using gap rl algorithm move parameterization value gradient average reward ha shown 14 tion distributed gap cause system whole converge local optimum stationarity tions algorithm essentially one described chapter 3 developed chapter 5 peshkin tation 13 policy initialized two diﬀerent way domly based shortest path tried ization random policy uniformly chosen rameter space initialization result sensitive learning rate high learning rate often cause network stick local optimum combined policy space poor performance low learning rate result slow convergence constitutes high low learning rate depends speciﬁcs work not ﬁnd any satisfactory heuristic set obviously feature average number hop necessary deliver packet optimal policy well learning speed crucially depend particular characteristic network number node connectivity modularity consideration led u diﬀerent way tializing controller namely begin computing shortest path set controller route traﬃc shortest path occasionally ing packet explore alternative link call routing experiment ǫ set believe parameter would not qualitatively change outcome experiment since only ﬂuences exploratory behaviour beginning exploration capacity algorithm regulated diﬀerent way well temperature learning rate simply kept constant consideration simplicity maintaining controller ability adjust change network link failure however experiment indicate schedule reducing learning rate key initial period learning would improve performance alternatively would interesting explore diﬀerent learning rate routing parameter one hand encoding topological feature iv empirical result compared routing algorithm several work various number node degree nectivity modularity including lata telephone network network gap algorithm performed comparably better routing gorithms illustrate principal diﬀerences behavior algorithm key advantage tributed gap concentrate analysis two routing problem network diﬀer single link location figure present irregular grid network topology used boyan littman 4 ments network consists two well connected ponents bottleneck traﬃc falling two ing link resulting dependence network mance load depicted ﬁgure graph represent performance policy ha converged eraged ﬁve run tested network load ranging compare result 20 30 32 25 26 19 18 24 12 13 14 8 7 6 0 1 2 3 4 5 11 10 9 15 16 17 23 22 21 28 29 27 35 34 33 31 30 32 25 26 18 24 12 7 0 1 2 3 4 5 11 10 9 15 16 17 23 22 21 28 29 27 35 34 33 31 20 14 13 6 8 19 fig 1 left original network right modified network favoring stochastic policy tained littman boyan load corresponds value parameter poisson arrival process average number packet injected per time unit network topology gap slightly inferior algorithm lower load doe least well bestload higher load outperforming best slightly inferior performance low load due exploratory behaviour gap some fraction packet always sent random link illustrate diﬀerence algorithm explicitly altered network moving one link connecting node 32 33 connecting node 20 27 illustrated ﬁgure since node 20 obviously represents bottleneck ration optimal routing policy bound tic resulting dependence network performance load presented ﬁgure gap clearly perior algorithm high load even form bestload ha global information choosing policy bound deterministic policy notice deterministic algorithm get frustrated much lower load network conﬁguration previous one since perspective bridge highly connected component get twice thinner compare left right figure 2 gap algorithm successfully adapts change network conﬁguration increased load ferred route left part network right becomes evenly split two bridge node using link 20 algorithm ha pay penalty making extra hop compared link 20 size queue node 21 grows penalty becomes negligible compared waiting time exploratory behavior help gap discover link go adjust policy accordingly experimented giving router bit memory ﬁnite state controller 13 found doe not improve performance slows learning somewhat related work application machine learning technique domain telecommunication rapidly growing area bulk problem ﬁt category resource allocation bandwidth allocation network routing call admission control cac power management rl appears promising attacking problem arately simultaneously marbach mihatsch tsitsiklis 11 applied algorithm address resource allocation within communication network tackling routing call admission control adopt decompositional approach representing network consisting link process tial reward unfortunately empirical result even small network 4 16 node show little advantage heuristic technique om 7 introduces another rl strategy based decomposition called predictive gain scheduling control problem admission control decomposed prediction call arrival rate precomputation control policy poisson call arrival process approach result faster ing without performance loss online convergence rate increase 50 time simulated link capacity 24 generally speaking algorithm extensively investigated policy search one domain communication learning algorithm arrived promising result 1 2 3 4 5 8 11 14 17 20 load average delivery time best bestload gap 1 2 3 5 8 11 14 17 20 load average delivery time best bestload gap fig 2 performance routing algorithm original network left modified network right boyan littman 4 algorithm prof superior technique based shortest path robust respect dynamic variation simulation variety network topology including irregular grid lata phone network regulates number node packet ha traverse possibility congestion wolpert tumer frank 20 construct formalism collective intelligence coin neural net applied internet traﬃc routing approach involves automatically initializing updating local utility function individual rl agent node global utility observed local dynamic simulation outperforms full knowledge shortest path algorithm sample network seven node coin network ploy method similar spirit research presented rely distributed rl algorithm verge local optimum without endowing agent node explicit knowledge network topology however coin diﬀers form approach requiring tion preliminary structure network dividing neighborhood share local utility function encourage cooperation contrast node network update algorithm rectly global reward work presented paper focus packet routing using policy search resembles work tao baxter weaver 12 apply algorithm induce cooperation among node packet switched network order minimize age packet delay algorithm performs well several network type take many ten million trial converge network node applying reinforcement learning communication ten involves optimizing performance respect tiple criterion recent discussion challenging issue see shelton 15 context wireless munication wa addressed brown 5 considers problem ﬁnding power management policy simultaneously maximizes revenue earned ing communication minimizing battery usage problem deﬁned stochastic shortest path counted inﬁnite horizon discount factor varies model power loss approach resulted signiﬁcant 50 improvement power usage gelenbe et al 9 also compute reward weighted combination probability packet loss packet delay packet agent trolling routing ﬂow control cognitive packet network split packet three type smart dumb acknowledgment small number smart packet learn eﬃcient way ing network dumb packet simply follow route taken smart packet acknowledgment packet travel inverse route smart packet provide source routing information dumb packet division smart dumb packet explicit representation dilemma smart packet allow network adapt structural change dumb packet exploit relative stability tween change promising result obtained simulation network 100 node physical network 6 computer subramanian druschel chen 16 adopt proach ant colony similar spirit individual host network keep routing table associated cost sending packet host router ha traverse expensive table periodically dated ant whose function ass cost traversing link host ant rected probabilistically along available path form host along way cost associated travel host use information alter routing table according update rule two type ant regular ant use routing table host alter probability directed along certain path number trial regular ant mission start using route function allow host table converge correct cost ﬁgure case network stable uniform ant take any path equal probability one continue exploring network assure cessful adaptation change link status link cost vi discussion admittedly simulation network routing process presented far realistic tic model could include factor network regard link bandwidth routing node buﬀer size limit collision packet packet dering constraint various cost associated say ticular link chosen commercial versus government subnetworks minimal quality service requirement introducing priority individual packet brings yet another set optimization issue however learning algorithm applied show promise handling tive telecommunication protocol several obvious way develop research incorporating domain knowledge controller structure one direction would involve classifying node network routing packet hierarchical fashion one step line employing learning algorithm routing network work network node dynamically introduced terminated system well existing active node moving loosing some nections establishing new one realistic assumption physical variation network slower traﬃc routing evolution adaptive ing protocol deﬁnitely outperform any heuristic routine currently pursuing line research reference 1 baird reinforcement learning gradient scent phd thesis carnegie mellon university pittsburgh pa 15213 1999 2 baird moore gradient descent general reinforcement learning advance neural information processing system volume mit press 1999 3 bellman dynamic programming princeton university press princeton new jersey 1957 4 boyan littman packet routing dynamically changing network reinforcement learning approach vances neural information processing system volume 7 page 1994 5 brown low power wireless communication via ment learning advance neural information processing system volume 12 page 1999 6 brown tong singh optimizing sion control ensuring quality service multimedia network via reinforcement learning advance neural information processing system volume 12 page 1999 7 carlstrom reinforcement learning admission trol routing phd thesis uppsala university uppsala sweden may 2000 8 dijkstra note two problem connection graph numerical mathematics 1959 9 gelenbe ricardo lent design analysis tive packet network performance evaluation page 10 kaelbling littman moore ment learning survey journal ai research 1996 11 marbach mihatsch schulte tsitsiklis reinforcement learning call admission control routing integrated service network advance neural mation processing system volume 11 1998 12 nigel tao weaver policy gradient approach network routing proceeding eighteenth international conference machine learning 2001 13 peshkin reinforcement learning policy search phd thesis brown university providence ri 02912 preparation 14 peshkin kim meuleau kaelbling learning cooperate via policy search sixteenth ence uncertainty artiﬁcial intelligence page san francisco ca morgan kaufmann 15 shelton importance sampling reinforcement ing multiple objective phd thesis mit 2001 16 subramanian druschel chen ant forcement learning case study routing dynamic work proceeding fifteenth international joint conference artiﬁcial intelligence volume 2 page 839 1997 17 sutton barto reinforcement learning introduction mit press cambridge massachusetts 1998 18 williams class algorithm reinforcement learning neural network proceeding ieee first international conference neural network san diego california 1987 19 williams simple statistical rithms connectionist reinforcement learning machine learning 8 3 1992 20 wolpert tumer frank using collective intelligence route internet traﬃc advance neural information processing page denver 1998