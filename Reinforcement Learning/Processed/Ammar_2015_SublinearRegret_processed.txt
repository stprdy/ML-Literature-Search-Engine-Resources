safe policy search lifelong reinforcement learning sublinear regret haitham bou ammar haithamb rasul tutunov tutunov eric eaton eeaton university pennsylvania computer information science department philadelphia pa 19104 usa abstract lifelong reinforcement learning provides promising framework developing versatile agent accumulate knowledge lifetime experience rapidly learn new task building upon prior knowledge ever current lifelong learning method exhibit regret amount experience increase include limitation lead suboptimal unsafe control policy address issue develop lifelong policy dient learner operates adversarial ting learn multiple task online ing safety constraint learned policy demonstrate ﬁrst time sublinear regret lifelong policy search validate rithm several benchmark dynamical system application quadrotor control introduction reinforcement learning rl busoniu et 2010 sutton barto 1998 often requires substantial experience fore achieving acceptable performance individual trol problem one major contributor issue assumption typical rl method learn scratch new task setting learning performance directly correlated quality acquired sample unfortunately amount experience necessary performance increase tially task degree freedom inhibiting plication rl control problem data limited supply transfer learning cantly improve model performance new task reusing previous learned knowledge training taylor stone 2009 gheshlaghi azar et 2013 lazaric 2011 ferrante et 2008 bou ammar et 2012 task learning mtl explores another notion edge transfer task model trained proceeding 32 nd international conference machine learning lille france jmlr w cp volume right 2015 author ously share knowledge joint learning ce wilson et 2007 zhang et 2008 lifelong learning setting thrun sullivan b framed online mtl lem agent acquire knowledge incrementally learning multiple task consecutively lifetime recently based work ruvolo eaton 2013 vised lifelong learning bou ammar et al 2014 oped lifelong learner policy gradient rl ensure efﬁcient learning consecutive task work ploy taylor expansion around ters locally optimal task without fer assumption simpliﬁes mtl objective weighted quadratic form online learning since based learning technique lead rameters far globally optimal consequently ce method rl highly depends icy initialization must lead tory meaningful update also since objective function average loss task method exhibit regret form r r total number round setting addition method may produce control policy unsafe behavior capable causing damage agent environment catastrophic failure critical issue robotic control unsafe control cies lead physical damage user injury lem caused using optimization shared knowledge transfer process may lead uninformative unbounded policy paper address issue proposing ﬁrst safe lifelong learner policy gradient rl operating adversarial framework approach rapidly learns performance safe control policy based agent viously learned knowledge safety constraint task accumulating knowledge multiple consecutive task optimize overall performance theoretically alyze regret exhibited algorithm showing linear dependency form r r round thus outperforming current method evaluate proach empirically set dynamical system 21 may 2015 safe policy search lifelong reinforcement learning sublinear regret background reinforcement learning rl agent sequentially chooses action minimize expected cost problem formalized markov cision process mdps u p c x potentially inﬁnite state space u set possible action p x u x 0 1 state transition probability describing system ic c x u x cost function measuring agent performance γ 0 1 discount tor time step agent state xm must choose action um transitioning new state um yielding cost c um xm sequence pair form trajectory τ possibly inﬁnite horizon policy π x u 0 1 speciﬁes probability distribution pair π represents probability selecting tion u state goal rl ﬁnd optimal policy minimizes total expected cost policy search method shown success solving problem robotic control kober peter 2011 peter schaal sutton et 2000 method represent policy πα using vector α control parameter optimal policy found determining parameter mize expected average cost l α n x pα τ k c τ k 1 n total number trajectory pα k c k probability cost trajectory τ k pα τ k x k 0 p x k k u k πα u k k 2 c τ k 1 x c x k u k x k 3 initial state distribution x 0 1 dle constrained version policy search timality not only corresponds minimizing total pected cost also ensuring policy satisﬁes safety constraint constraint vary tions example corresponding maximum joint torque prohibited physical position online learning regret analysis paper employ special form regret tion game brieﬂy review regret imization game triple f k empty decision set f set move adversary contains bounded convex function rn r r total number round game proceeds round round j 1 r agent chooses prediction θj environment adversary chooses loss function lj end round loss function lj revealed agent decision θj revealed environment paper handle case agent may observe entire loss function lj feedback exploit making decision goal minimize cumulative regret pr lj θj hpr lj u analyzing regret method use variant deﬁnition handle lifelong rl case rr r x ltj θj r x ltj u ltj denotes loss task round framework adopt variant regret tion called follow regularized leader mizes regret two step first unconstrained solution θ determined see sect solving unconstrained optimization accumulated loss observed far given θ constrained solution determined learning projection constraint set via bregman projection see et al 2013 safe lifelong policy search adopt lifelong learning framework agent learns multiple rl task consecutively providing portunity transfer knowledge task improve learning let denote set task element mdp any time learner may face any previously seen task must strive maximize performance across task goal learn optimal policy 1 task policy task parameterized addition task equipped safety constraint ensure able policy behavior atαt bt representing allowed policy combination precise form constraint depends application domain formulation support constraint joint torque acceleration position etc round j learner observes set ntj tory n τ 1 tj τ ntj tj task tj trajectory ha length mtj support knowledge transfer task assume task policy ters αtj round j written linear nation shared latent basis l coefﬁcient vector stj therefore αtj lstj column l represents chunk transferrable knowledge task construction ha used successfully previous safe policy search lifelong reinforcement learning sublinear regret learning work kumar e iii 2012 volo eaton 2013 bou ammar et 2014 extending previous work ensure shared knowledge repository informative incorporating bounding straints frobenius norm consequently optimization problem observing r round min l r x ηtjltj f f 4 atjαtj λmin λmax p q constraint ηtj design weighting ir tr denotes set task observed far round r collection coefﬁcients h sth th 0 otherwise 1 loss function ltj αtj eq 4 corresponds icy gradient learner task tj deﬁned eq 1 cal policy gradient method kober peter 2011 sutton et 2000 maximize lower bound expected cost ltj derived taking logarithm applying jensen inequality log ltj log ntj x p tj αtj τ k tj c tj τ k tj 5 ntj e mtj x log h παtj u k tj x k tj ntj therefore goal minimize following objective er r x ntj ntj x mtj x log h παtj u k tj x k tj 6 f f atjαtj λmin λmax online formulation optimization problem mapped dard online learning framework unrolling l vector θ vec l vec choosing θ pdk ωj θ θ ηtjltj θ write safe lifelong policy search problem eq 6 arg min ωr θ 7 k set allowable policy given safety constraint note loss task tj describe later set η later sect 5 obtain regret bound leave variable generality written bilinear product θ ltj θ ntj ntj x mtj x log π tj θlθstj u k tj x k tj θl θd θd θdk θstj θ see problem eq 7 equivalent eq 6 noting r round ωr pr ηtjltj θ θ online learning method solve eq 7 two step first determine unconstrained solution k see sect given derive constrained solution ˆ learning projection projωr k straint set k amount minimizing bregman divergence ωr θ see sect complete approach given algorithm 1 available software implementation author website unconstrained policy solution although eq 6 not jointly convex l separably convex policy distribution consequently follow alternating optimization proach ﬁrst computing l holding ﬁxed updating given acquired detail process two popular pg learner ereinforce williams 1992 enac peter schaal derivation update rule found appendix update governed learning rate β λ decay time β λ chosen using method discussed boyd vandenberghe 2004 experiment adopt simple yet effective strategy β λ 0 c step 1 updating l holding ﬁxed latent repository updated according lβ l ereinforce lβ lβ sβ l enac learning rate ηβ l l inverse fisher information matrix peter schaal special case gaussian policy update l sect linearize loss around constrained lution previous round increase stability ensure vergence given linear loss sufﬁces solve bregman divergence regularizer reducing computational cost safe policy search lifelong reinforcement learning sublinear regret derived closed form l vl zl r x ηtj tj ntj x x vec φst tj tj vl x j ηtj tj ntj x x vec u k tj φst tj tj covariance gaussian policy task tj φ φ x k tj denotes state feature step 2 updating given ﬁxed basis l cient matrix updated tj tj tj er l ereinforce tj tj lβ sβ er l enac learning rate ηλ gaussian policy update stj stj vstj zstj x ηtj tj ntj x x ltφφtl vtj x ηtj tj ntj x x u k tj ltφ constrained policy solution obtained unconstrained solution satisﬁes eq 7 lead policy eters unsafe region derive constrained solution ensure safe policy learn projection projωr k constraint set ˆ arg min bωr k θ 8 bωr k θ bregman divergence ωr bωr k θ ωr θ θ θ solving eq 8 computationally expensive since ωr θ includes sum back original round remedy problem ensure stability approach antee constrained solution observed task lie within bounded region linearize loss function ltr θ around constrained solution previous round ˆ θr ltr ˆ u ˆ ftr ˆ θr ˆ u 9 ˆ ftr ˆ θr θ ˆ θr ltr θ ˆ θr θ ˆ θr ˆ θr ˆ u u 1 given linear form rewrite tion problem eq 8 ˆ arg min k θ 10 consequently determining safe policy lifelong policy search reinforcement learning amount solving min l f f st l atjlstj llt llt solve optimization problem start verting inequality constraint equality constraint introducing slack variable ctj also tee slack variable bounded incorporating 1 min l c f f lt st atjlstj btj ctj 0 llt llt formulation learning projωr k amount solving cone program program learning l section determines constrained projection shared basis l given ﬁxed show l acquired efﬁciently since step relaxed solving program llt boyd vandenberghe 2004 formulate program note trace lt k x l k x l 2 v u u k x l r 2 2 v u u k x 2 l f q trace llt constraint set recognize st tjlt tj tjltlstj tjatj atj tj safe policy search lifelong reinforcement learning sublinear regret algorithm 1 safe online lifelong policy search 1 input total number round r weighting factor η r regularization parameter straints p q number latent basis vector 2 zero k l diagk ζ p 3 j 1 r 4 tj update ij 5 compute unconstrained solution sect 6 fix c update l sect 7 use updated l derive c sect 8 end 9 output l since spectrum spectrum write min x l f p trace x st tjxstj tjatj x x x ltl cone program learning task projection determined l acquire update c solving cone program boyd berghe 2004 following form min stj ctj r x 2 r x st tj ˆ θr stj atjlstj btj ctj 0 2 max theoretical guarantee section quantiﬁes performance approach providing formal analysis regret r round show safe lifelong reinforcement learner exhibit sublinear regret total number round formally prove following theorem theorem 1 sublinear regret r round ing ηtj η 1 r l ˆ diagk ζ diagk diagonal matrix among k column l p ˆ safe lifelong forcement learner exhibit sublinear regret form r x ltj ˆ θj u r any u proof roadmap remainder section completes proof theorem 1 detail given pendix assume linear loss task strained case accordance sect although linear loss policy search rl restrictive given single operating point discussed previously remedy problem generalizing case linear loss linearization operating point resultant optimization problem bound regret need bound dual euclidean norm euclidean norm gradient loss function prove theorem 1 bounding 1 task tj gradient loss sect 2 linearized loss respect l sect bounding tj gradient loss start stating essential lemma theorem 1 due space constraint proof lemma available supplementary material bound gradient loss function ltj θ round r gaussian assumption assume policy task tj gaussian action set u bounded umax feature set φmax lemma assume task tj policy round r given π tj αtj u k tj k tj ˆ θr n αt tj ˆ θr φ x k tj σtj state x k tj action u k tj ltj ntj ntj x mtj x log h π tj αtj u k tj k tj gradient αtj ltj ˆ θr satisﬁes αtj ltj ˆ θr 2 mtj tj umax max tk 2 cmax φmax φmax trajectory task umax max k n u k tj φmax k n φ x k tj 2 bounding linearized loss discussed previously linearize loss task tr around constraint solution previous round ˆ θr acquire regret bound theorem 1 next step bound dual norm ˆ ftr ˆ θr 2 ˆ ftr ˆ θr 2 eq 9 easily seen ˆ ftr ˆ θr 2 ltr θ ˆ θr z constant θ ˆ θr 2 z lemma 2 11 θ ˆ θr 2 ˆ θr 2 z lemma 3 note derivation form policy distribution could derived similar manner work focus gaussian policy since cover broad spectrum application safe policy search lifelong reinforcement learning sublinear regret since ltr θ ˆ θr bounded δltr see sect 2 next step bound θ ˆ θr 2 lemma norm gradient loss function evaluated ˆ θr satisﬁes θ ˆ θr 2 2 ltr θ ˆ θr 2 2 q max tk 2 2 2 max 1 ﬁnalize bound ˆ ftr ˆ θr 2 needed deriving regret must derive lemma norm constraint solution round r 2 bounded 2 1 1 max tk 2 2 cmax number unique task observed far given previous two lemma prove bound ˆ ftr ˆ θr 2 lemma norm linearizing term ltr θ around ˆ θr ˆ ftr ˆ θr 2 bounded ˆ ftr ˆ θr 2 θ ˆ θr 2 ltr θ ˆ θr 12 r 1 r δltr δltr constant ltr θ ˆ θr r 1 tj umax max tk 2 cmax φmax φmax p p r max n 2 2 max p qd r p q p 1 max tk 2 2 cmax completing proof sublinear regret given lemma previous section rive sublinear regret bound given theorem using result developed et al 2013 easy see θj ηtj ˆ ftj ˆ θj convexity regularizer obtain ˆ θj ˆ ˆ ˆ θj e 1 2 ˆ θj 2 2 ˆ θj 2 ˆ ftj ˆ θj 2 therefore any u r x ηtj ltj ˆ θj u r x ηtj ˆ ftj ˆ θj 2 2 u ˆ assuming ηtj η derive r x ltj ˆ θj u r x ˆ ftj ˆ θj 2 2 η u ˆ following lemma ﬁnalizes proof theorem 1 lemma r round ηtj η 1 r any u pr ltj ˆ θj u r proof eq 12 follows ˆ ftj ˆ θr 2 2 r 1 r 2 r r 8 1 r qd 1 max n cmax r 1 r 2 tj since ˆ ftj ˆ θr 2 2 r 1 r max n 2 cmax given u r r constant r x ltj ˆ θj u r x r 1 η qd r ˆ initializing l initialize l ˆ diagk ζ p ˆ ensure invertibility safe policy search lifelong reinforcement learning sublinear regret l constraint met lead r x ltj ˆ θj u r x r η qd r choosing ηtj η r acquire sublinear regret ﬁnalizing statement theorem 1 r x ltj ˆ θj u r r qd r r r r r experimental validation validate empirical performance method applied safe online pg algorithm learn multiple secutive control task three dynamical system ure 1 generate multiple task varied ization system yielding set control task domain varying dynamic optimal control policy system vary widely only minor change system parameter providing substantial versity among task within single domain figure dynamical system used experiment simple mass system left b middle c quadrotor manned aerial vehicle right simple mass spring damper simple mass sm system characterized three parameter spring stant k damping constant mass kg system state given position x x mass varies according linear force f goal train policy controlling mass speciﬁc state gref cart pole cp ha used extensively benchmark evaluating rl method busoniu et 2010 cp dynamic characterized cart mass mc kg pole mass mp kg pole length meter damping parameter state given cart position x velocity x well pole angle θ angular velocity goal train policy control pole upright position experimental protocol generated 10 task domain varying tem parameter ensure variety task diverse timal policy including highly chaotic ic difﬁcult control ran experiment total r round varying 150 simple mass 10 000 quadrotor train l well updating pg model round j learner observed task tj 50 trajectory 150 step updated l stj dimensionality k latent space wa chosen independently domain via 3 task learning step size task domain wa determined line search gathering 10 trajectory length used enac standard pg algorithm base learner compared approach standard pg enac bou ammar et 2014 ing constrained unconstrained variant algorithm also varied number iteration ternating optimization 10 100 evaluate effect inner iteration performance shown figure 2 two mtl algorithm approach policy parameter task tj initialized using learned basis αtj lstj conﬁgured described bou ammar et al 2014 ensuring fair comparison standard pg learner provided additional trajectory order sure fair comparison described experiment policy constraint generated set constraint bt task restricted policy parameter safe region shown figure 2 c 2 also tested different value constraint l varying p q 10 approach showed robustness broad range yielding similar average cost performance result benchmark system figure 2 report result benchmark simple mass system figure 2 2 b depicts performance learned policy lifelong learning ting consecutive unconstrained task averaged 10 system 100 different initial condition result demonstrate approach capable forming standard pg wa provided 50 additional trajectory iteration ensure fair comparison term initial mance learning speed ﬁgures also show performance method increase given alternating iteration ﬁtting l evaluated ability method respect safety constraint shown figure 2 c 2 thicker black line ﬁgure depict allowable safe region policy space enable online learning task tj wa observed round shared basis l coefﬁcients stj updated using alternating optimization plotted change policy safe policy search lifelong reinforcement learning sublinear regret round 0 50 100 150 average cost 0 500 1000 1500 2000 2500 standard pg safe online 10 iteration safe online 50 iteration safe online 100 iteration simple mass round 0 1000 2000 3000 average cost 0 1000 2000 3000 4000 5000 6000 standard pg safe online 10 iteration safe online 50 iteration safe online 100 iteration b cart pole 0 1 1 2 3 standard pg safe pg 50 iteration safe pg 100 iteration optimal policy initial policy safe region c trajectory simple mass 100 120 140 160 180 200 20 25 30 35 standard pg safe pg 50 iteration safe pg 100 iteration initial policy optimal policy safe region trajectory cart pole figure result benchmark simple mass system figure b depict performance lifelong learning scenario consecutive unconstrained task showing approach outperforms standard pg figure c examine ability method abide safety constraint sample constrained task depicting two dimension policy space v demonstrating approach abides constraint dashed black region rameter vector per iteration αtj lstj method demonstrating approach abides safety constraint standard pg violate since only solve unconstrained mization problem addition ﬁgures show creasing number alternating iteration method cause take direct path optimal solution application quadrotor control also applied approach challenging main quadrotor control dynamic tor system figure 1 inﬂuenced inertial constant around b b b thrust factor inﬂuencing rotor speed affect overall variation system state length rod supporting rotor though overall state system described vector focus stability sider only six quadrotor tem ha action space goal control four rotational velocity wi 4 tor stabilize system ensure realistic dynamic used simulated model described bouabdallah 2007 voos bou ammar 2010 ha veriﬁed used control physical quadrotors generated 10 different quadrotor system varying inertia around x used linear quadratic regulator described bouabdallah 2007 initialize policy learning testing phase followed similar experimental procedure discussed update model figure 3 show performance unconstrained tion compared standard pg approach clearly outperforms standard pg initial performance learning speed also evaluated constrained task similar manner showing approach capable respecting straints since policy space higher dimensional not visualize well benchmark system instead report number iteration take approach round 0 2000 4000 6000 8000 10000 average cost 0 2000 4000 6000 8000 10000 12000 standard pg safe online 10 iteration safe online 50 iteration safe online 100 iteration figure performance quadrotor control 0 150 300 450 600 simple mass cart pole quadrotor safe pg standard pg 1 95 100 309 320 510 545 1 1 number observation reach safe policy figure average number task observation acquiring policy parameter abide constraint showing approach immediately project policy safe region project policy safe region figure 4 show approach requires only one observation task acquire safe policy substantially lower standard pg require 545 510 observation respectively quadrotor scenario conclusion described ﬁrst lifelong pg learner provides linear regret r r total round addition approach support safety constraint learned policy essential robust learning real application framework formalizes lifelong learning online mtl limited resource enables safe transfer sharing policy parameter latent knowledge base efﬁciently updated time safe policy search lifelong reinforcement learning sublinear regret reference yasin peter bartlett varun kanade yevgeny seldin csaba ari online learning markov decision process adversarially chosen transition probability distribution advance neural information processing system 26 haitham bou ammar karl tuyls matthew taylor kurt driessen gerhard wei reinforcement learning transfer via sparse coding proceeding national conference autonomous agent gent system aamas haitham bou ammar eric eaton paul ruvolo matthew taylor online learning policy gradient method proceeding international ference machine learning icml samir bouabdallah design control quadrotors application autonomous flying phd thesis ecole polytechnique erale de lausanne stephen boyd lieven vandenberghe convex tion cambridge university press new york ny lucian busoniu robert babuska bart de schutter damien ernst reinforcement learning dynamic programming using function approximators crc press boca raton fl eliseo ferrante alessandro lazaric marcello restelli transfer task representation reinforcement ing using function ceedings international joint conference autonomous agent multiagent system aamas mohammad gheshlaghi azar alessandro lazaric emma brunskill sequential transfer bandit ﬁnite set model advance neural formation processing system 26 roger horn roy mathias ities associated positive semideﬁnite matrix ear algebra application jens kober jan peter policy search motor primitive robotics machine learning 84 abhishek kumar hal e iii learning task grouping overlap learning proceeding international conference machine learning icml alessandro lazaric transfer reinforcement learning framework survey wiering van terlo editor reinforcement learning state art springer jan peter stefan schaal reinforcement learning tor skill policy gradient neural network jan peter stefan schaal natural computing 71 paul ruvolo eric eaton ella efﬁcient lifelong learning algorithm proceeding tional conference machine learning icml richard sutton andrew barto introduction reinforcement learning mit press cambridge richard sutton david mcallester satinder singh yishay mansour policy gradient method ment learning function approximation advance neural information processing system 12 matthew taylor peter stone transfer learning reinforcement learning domain survey journal machine learning research sebastian thrun joseph sullivan discovering ture multiple learning task tc algorithm ceedings international conference chine learning icml sebastian thrun joseph sullivan learning le data experiment lifelong learning seminar gest holger voos haitham bou ammar nonlinear tracking landing controller quadrotor aerial robot proceeding ieee system control ronald williams simple statistical algorithm connectionist reinforcement learning machine learning 8 aaron wilson alan fern soumya ray prasad palli reinforcement learning hierarchical bayesian approach proceeding tional conference machine learning icml jian zhang zoubin ghahramani yiming yang flexible latent variable model learning machine learning 73 3 safe policy search lifelong reinforcement learning sublinear regret update equation derivation appendix derive update equation l special case gaussian policy please note derivation easily extended policy form higher dimensional action space task tj policy π tj αtj u k tj k tj given π tj αtj u k tj k tj 1 q tj exp tj u k tj φ x k tj therefore safe lifelong reinforcement learning optimization objective written er l r x ηtj tjntj ntj x mtj x u k tj φ x k tj f f 13 arrive update equation need derive eq 13 respect l update equation l starting derivative er l respect shared repository l write l r x ηtj tjntj ntj x mtj x u k tj φ x k tj f f r x ηtj tjntj ntj x mtj x u k tj φ x k tj φ x k tj st tj acquire minimum set zero r x ηtj tjntj ntj x mtj x u k tj φ x k tj φ x k tj st tj 0 r x ηtj tjntj ntj x mtj x st tjltφ x k tj φ x k tj st tj r x ηtj tjntj ntj x mtj x u k tj φ x k tj st tj noting st tjltφ x k tj write r x ηtj tjntj ntj x mtj x φ x k tj st tjφt x k tj lstj r x ηtj tjntj ntj x mtj x u k tj φ x k tj st tj 14 solve eq 14 introduce standard vec operator leading vec r x ηtj tjntj ntj x mtj x φ x k tj st tjφt x k tj lstj vec r x ηtj tjntj ntj x mtj x u k tj φ x k tj st tj r x ηtj tjntj ntj x mtj x vec φ x k tj st tj vec φt x k tj lstj l r x ηtj tjntj ntj x mtj x vec u k tj φ x k tj st tj safe policy search lifelong reinforcement learning sublinear regret knowing given set matrix b x vec axb vec x write r x ηtj tjntj ntj x mtj x vec φ x k tj st tj st tj x k tj vec l l r x ηtj tjntj ntj x mtj x vec u k tj φ x k tj st tj choosing zl pr ηtj ntj tj pntj pmtj vec φ x k tj st tj φ x k tj tj vl pr ηtj ntj tj pntj pmtj vec u k tj φ x k tj st tj update l l vl update equation derive update equation respect similar approach l followed derivative er l respect computed task observed far er l r x ηtj tjntj ntj x mtj x u k tj φ x k tj f f x ηtj tjntj ntj x mtj x u k tj φ x k tj ltφ x k tj using similar analysis previous section choosing zstj x ηtj tj ntj x mtj x ltφ x k tj φt x k tj l vstj x ηtj tj ntj x mtj x u k tj ltφ x k tj update stj stj vstj proof theoretical guarantee appendix prove claim lemma main paper leading sublinear regret theorem 1 lemma assume policy task tj round r given π tj αtj u k tj k tj ˆ θr n αt tj ˆ θrφ x k tj σtj x k tj u k tj xtj utj representing state action space respectively gradient ltj ˆ θr ltj ntj pntj pmtj log h π tj αtj u k tj k tj satisﬁes ltj ˆ θr 2 tj umax max tk 2 cmax φmax φmax umax maxk n u k tj φmax maxk n φ x k tj 2 trajectory task proof proof lemma provided collection claim start following claim given π tj αtj u k k ˆ θr n αt tj ˆ θrφ x k tj σtj x k tj u k tj ltj ntj pntj pmtj log h π tj αtj u k tj k tj ltj ˆ θr 2 satisﬁes ltj ˆ θr 2 tj umax αtj ˆ θr 2 φmax φmax 15 safe policy search lifelong reinforcement learning sublinear regret proof since π tj αtj u k tj k tj ˆ θr n αt tj ˆ θrφ x k tj σtj write log π tj αtj u k tj k tj ˆ θr hq tj 1 tj u k tj tj ˆ θrφ x k tj therefore ltj ˆ θr ntj ntj x mtj x 1 tj u k tj tj ˆ θrφ x k tj φ x k tj ltj ˆ θr 2 tj max k u k tj tj ˆ θr φ x k tj φ x k tj 2 tj max k n u k tj φ x k tj 2 max k αt tj ˆ θrφ x k tj φ x k tj 2 tj max k n u k tj max k n φ x k tj 2 max k αtj ˆ θr φ x k tj max k n φ x k tj 2 denoting maxk n u k tj umax maxk n φ x k tj 2 φmax trajectory task write ltj ˆ θr 2 tj umax max k αtj ˆ θr φ x k tj φmax using inequality horn mathias 1990 upper bound maxk αtj ˆ θr φ x k tj max k αtj ˆ θr φ x k tj k αtj ˆ θr 2 φ x k tj 2 k αtj ˆ θr 2 φmax αtj ˆ θr 2 φmax finalizing statement claim overall bound norm gradient ltj αtj written ltj ˆ θr 2 tj umax αtj ˆ θr 2 φmax φmax 16 claim norm gradient loss function satisﬁes ltj ˆ θr 2 tj umax max cmax φmax φmax proof mentioned previously consider linearization loss function ltj around constraint solution previous round ˆ θr since ˆ θr satisﬁes atkαtk btk hence write atkαtk ctk btk tk btk tk tkatk tk left therefore tk 2 tk 2 cmax safe policy search lifelong reinforcement learning sublinear regret combining result eq 16 arrive ltj 2 tj umax max tk 2 cmax φmax φmax previous result ﬁnalizes statement lemma bounding gradient loss function term safety constraint lemma norm gradient loss function evaluated ˆ θr satisﬁes θ ˆ θr 2 2 ltj θ ˆ θr 2 2 q max tj 2 2 btj 2 2 max 1 proof derivative ltj θ ˆ θr written θ ˆ θr lt tj θ ˆ θr 1 tj ˆ θr tj ˆ θr lt tj θ ˆ θr 1 tj ˆ θr tj ˆ θr lt tj θ ˆ θr ˆ θr 0 0 lt tj θ ˆ θr 0 θ ˆ θr lt tj θ ˆ θr θd ˆ θr θdk ˆ θr θ ˆ θr 2 2 ltj αtj ˆ θr 2 2 stj ˆ θr 2 2 l ˆ θr 2 f result lemma 1 bound ltj θ ˆ θr 2 2 target bound stj ˆ θr 2 2 l ˆ θr 2 bounding stj ˆ θr 2 2 ˆ f considering constraint atjlstj ctj btj task tj realize stj tj therefore stj ˆ θr 2 tj 2 2 tj 2 btj 2 ctj 2 17 noting 2 lt 2 2 lt 2 2 lt f 2 safe policy search lifelong reinforcement learning sublinear regret relate need bound 2 term denoting spectrum ltl spec λk 0 spect λk λk λk hence 2 max n spec λmin ltl noticing spec spec recognize 2 λmin therefore 2 p 18 plugging result eq 18 eq 17 arrive stj ˆ θr 2 p l ˆ θr f max tk 2 cmax 19 finally since ˆ θr satisﬁes constraint note l ˆ θr 2 f consequently θ ˆ θr 2 2 ltj θ ˆ θr 2 2 q max tk 2 2 2 max 1 lemma norm constraint solution round r 2 bounded 2 1 1 max tk 2 2 cmax cardinality representing number different task observed proof noting ˆ θr h θdk z l ˆ θr z ˆ θr z ˆ θr z 0 unobserved task easy see 2 l ˆ θr 2 f max stk ˆ θr 2 2 max tk 2 2 cmax 1 1 max tk 2 2 cmax lemma norm linearizing term ltj θ around ˆ θr ˆ ftj ˆ θr 2 bounded ˆ ftj ˆ θr 2 θ ˆ θr 2 1 ltj θ ˆ θr r 1 r δltj δltj constant ltj θ ˆ θr r 1 tj umax max tk 2 cmax φmax φmax p p r max n 2 2 max p qd r p q p 1 1 max tk 2 2 cmax safe policy search lifelong reinforcement learning sublinear regret proof previously shown ˆ ftj ˆ θr 2 θ ˆ θr 2 ltj ˆ θr θ ˆ θr 2 ˆ θr using previously derived lemma ˆ ftj ˆ θr 2 follows θ ˆ θr 2 2 ltj θ ˆ θr 2 2 q max tk 2 2 2 max 1 θ ˆ θr 2 ltj θ ˆ θr 2 p p r max n 2 2 max p qd 1 tj umax max tk 2 cmax φmax φmax p p r max n 2 2 max p qd ˆ θr 2 2 max 1 1 max tk 2 2 cmax ˆ θr 2 p q p 1 1 max tk 2 2 cmax therefore ˆ ftj ˆ θr θ ˆ θr 2 1 ˆ θr 2 ltj θ ˆ θr 20 r 1 r δltj δltj constant ltj θ ˆ θr r 1 tj umax max tk 2 cmax φmax φmax p p r max n 2 2 max p qd r p q p 1 1 max tk 2 2 cmax theorem 1 sublinear regret restated main paper r round choosing ηtj η 1 r l ˆ diagk ζ diagk diagonal matrix among k column l p ˆ any u algorithm exhibit sublinear regret form r x ltj ˆ θr u r proof given ingredient previous section next derive sublinear regret result ﬁnalize statement theorem first easy see θj ηtj ˆ ftj ˆ θj strong convexity regularizer obtain ˆ θj ˆ ˆ ˆ θj e 1 2 ˆ θj 2 2 seen ˆ θj 2 ˆ ftj ˆ θj 2 safe policy search lifelong reinforcement learning sublinear regret finally any u r x ηtj ltj ˆ θj u r x ηtj ˆ ftj ˆ θj 2 u ˆ assuming ηtj η derive r x ltj ˆ θj u r x ˆ ftj ˆ θj 2 η u ˆ following lemma ﬁnalizes statement theorem lemma round ηtj η 1 r algorithm exhibit any u sublinear regret form r x ltj ˆ θj u r proof easy see ˆ ftj ˆ θr 2 2 r 1 r 2 r r 1 r 2 max tj r 8 1 r qd 8 1 r qd max n cmax since total number task available write ˆ ftj ˆ θr 2 2 r 1 r n 2 cmax easy see u r r constant lead r x ltj ˆ θj u r x r η qd r ˆ initializing l initialize l ˆ diagk ζ p ˆ ensures invertability l constraint met lead u r x ltj ˆ θj u r x r η qd r choosing ηtj η r acquire sublinear regret ﬁnalizing statement theorem r x ltj ˆ θj u r r qd r r r r r r constant