arxiv 28 mar 2006 technical report asymptotic learnability reinforcement problem arbitrary dependence daniil ryabko marcus hutter idsia galleria 2 daniil marcus 28 march 2006 abstract address problem reinforcement learning observation may exhibit arbitrary form stochastic dependence past observation action task agent attain best possible asymptotic reward true generating environment unknown belongs known countable family environment ﬁnd some suﬃcient condition class environment agent exists attains best asymptotic reward any environment class analyze tight condition relate diﬀerent probabilistic assumption known reinforcement learning related ﬁelds markov decision process mixing condition keywords reinforcement learning asymptotic average value policy non markov decision process work wa supported swiss nsf grant 1 1 introduction many learning problem like learning drive car playing game modelled agent π interacts environment µ sionally rewarded behavior interested agent perform well sense high reward also called value v µ π agent π environment µ known pure computational problem determine optimal agent πµ argmaxπv µ π far le clear optimal agent mean µ unknown reasonable objective single policy π high value simultaneously many environment formalize call criterion later learning approach reactive world reinforcement learning sequential decision theory adaptive control theory active expert advice theory ing problem overlap diﬀerent core focus reinforcement learning algorithm developed learn µ directly value temporal diﬀerence learning computationally eﬃcient ha slow asymptotic antees only eﬀectively small observable mdps others faster guarantee ﬁnite state mdps algorithm optimal any ﬁnite connected pomdp apparently largest class ronments considered sequential decision theory agent maximizes v ξ π considered ξ mixture environment ν c class environment contains true environment policy arbitrary class c provided c allows adaptive control theory considers simple ai spective special system linear quadratic loss function time allow computationally data eﬃcient solution action expert advice construct agent called master performs nearly well best agent best expert hindsight some class expert any environment important special case passive sequence prediction arbitrary unknown environment not aﬀect environment comparably easy diﬃculty active learning problem identiﬁed least countable class trap environment initially agent doe not know µ ha asymptotically forgiven taking initial wrong action class ergodic mdps guarantee any action history every state visited new aim paper characterize general possible class c behaviour possible general pomdps need characterize class environment forgive instance exact state recovery unnecessarily strong suﬃcient able recover high reward whatever state many real world problem no information available state environment pomdps 2 environment may exhibit long history dependency rather trying model environment mdp try identify condition suﬃcient learning towards aim propose consider only environment any arbitrary ﬁnite sequence action best value still achievable performance criterion asymptotic average reward thus consider environment exists policy whose asymptotic average reward exists asymptotic average reward any policy moreover property hold any ﬁnite sequence action ha taken no trap yet property not suﬃcient identifying optimal behavior require any sequence k action possible return optimal level reward k step condition formulated probabilistic form environment posse property called strongly show any countable class environment exists policy achieves best possible value any environment class class also show strong certain sense necessary also consider example environment posse strong particular any ergodic mdp easily shown property condition implies also demonstrated finally provide construction allowing build example environment not isomorphic ﬁnite pomdp thus demonstrating class environment quite general important argument class environment seek policy countable although class ments uncountable ﬁnd set condition necessary suﬃcient learning not rely countability class yet open problem however computational perspective countable class suﬃciently large class computable probability measure countable content paper organized follows section 2 introduces necessary tation agent framework section 3 deﬁne explain notion central paper section 4 present theorem policy class environment illustrates applicability theorem providing example strongly ments section 5 discus necessity condition main theorem section 6 provides some discussion result outlook future research formal proof main theorem given appendix section 4 contains only intuitive explanation 3 2 notation deﬁnitions essentially follow notation string probability use letter k l n n natural number denote cardinality set write x set ﬁnite string some alphabet x x set inﬁnite sequence string x length ℓ x n write xn xt abbreviate xk n x n finally deﬁne xk n provided element x added assume sequence sampled true probability measure µ p n n µ n denote expectation µ e function f x n r e f e f n p nµ n f n use probability expectation respect measure make notation explicit eν expectation respect measure called singular exists set agent framework general enough allow modelling nearly any kind telligent system cycle k agent performs action output result observation ok reward rk followed cycle assume action space observation space reward space r ﬁnite 0 rmax abbreviate zk xk rkok agent identiﬁed probabilistic policy given history z k probability agent π act yk cycle k deﬁnition π k thereafter environment µ provides probabilistic reward rk tion ok probability agent perceives xk deﬁnition µ kyk note policy environment allowed depend complete history not make any mdp pomdp assumption talk state environment only observation policy environment pair π µ generates sequence zπµ 1 zπµ 2 mathematically history zπµ 1 k random variable probability p zπµ 1 k k π µ π k µ kyk since value optimizing policy always chosen deterministic no real need consider probabilistic policy henceforth consider deterministic policy assume true unknown environment ν generic environment 3 setup environment ν policy p deﬁne random variable lower upper average value v ν p lim sup 1 mrpν 1 v ν p lim inf 1 mrpν 1 4 exists constant v v ν p v ν p v say limiting average value exists denote v ν p v environment ν explorable exists policy pν v ν pν exists v ν p ν pν probability 1 every policy case deﬁne v ν ν pν policy p set environment c v ν p ν every ν deﬁnition 1 environment explorable environment ν strongly exist sequence number rν 0 rmax two function dν k ε ϕν n ε 1 nrν 1 ν dν k ε k n ε every ﬁxed ε every k every history z k exists policy k ν p k k dν k ε nε z k n ε 1 first condition mean strong law large number ward hold uniformly history z k number rν thought expected reward optimal policy furthermore any bad sequence k action possible knowing environment recover k reward loss recover mean reach level reward obtained optimal policy beginning wa taking only optimal action suppose person ha made k possibly suboptimal action realized true environment wa act optimally suppose person b wa beginning taking only optimal action want compare performance b ﬁrst n step step environment strongly value stable catch b except k gain number rν thought expected reward b catch b reward loss dν k ε probability ϕν n ε latter doe not depend past action observation law large number hold uniformly next section presenting main theorem consider example family stable environment 4 main result section present main result along informal explanation proof illustrate applicability result example class environment theorem 2 any countable class c strongly environment exists policy 5 formal proof given appendix give some intuitive justiﬁcation suppose environment c deterministic construct optimizing policy p follows let νt ﬁrst environment algorithm assumes true environment νt try get optimal value some small called exploitation part succeeds doe some exploration follows pick ﬁrst environment νe ha higher average asymptotic value νt v νe v νt try get value acting optimally νe not get close value νe not true environment next environment picked exploration call exploration successive attempt exploit environment diﬀers current hypothesis true environment ha higher average reward switch exploitation νt exploit v νt ε switch νe time trying get vνe happen only ﬁnite number time true environment νt since v νt v νe thus exploration either νt νe found inconsistent current history νe next environment νe v νe v νt picked exploration νt ﬁrst consistent environment picked exploitation denoted νt turn happen only ﬁnite number time true environment ν picked νt algorithm still continues exploration attempt always keep within optimal value ensured k k probabilistic case somewhat complicated since not say whether environment consistent current history instead test environment consistency follows let ξ mixture environment observe together some ﬁxed policy environment µ sidered measure moreover shown any ﬁxed policy ratio ν z n ξ z n bounded away zero ν true environment µ tends zero ν singular µ fact singularity probabilistic analogue inconsistency exploration part algorithm ensures least one environment νt νe singular ν current history cession test ν z n ξ z n αs used exclude environment consideration next proposition provides some condition mixing rate ﬁcient not intend provide sharp condition mixing rate rather illustrate relation mixing condition say stochastic process hk k n satisﬁes strong condition coeﬃcients α k see sup n sup hn p b b p c k σ stand generated random variable ets loosely speaking mixing coeﬃcients α reﬂect speed process forgets past 6 proposition 3 mixing condition suppose explorable environment ν exist sequence number rν function k 1 nrν 1 n ν k k z k exists policy p sequence rpν satisﬁes strong condition coeﬃcients α k 1 some ε 0 rν k k z k k any ν proof using union bound obtain p k k k nε k rpν k k p rpν k rpν k nε ﬁrst term equal 0 assumption second term ε shown summable using sequence uniformly bounded mean random variable ri satisfying strong condition following bound hold true any integer 1 p nε cqα n some constant c case set ε po mdps applicability theorem 2 proposition 3 illustrated po mdps note policy uncountable class ﬁnite ergodic mdps pomdps known aim present section show weaker requirement requirement model also illustrate applicability result call µ stationary markov decision process mdp probability perceiving xk given history z kyk only depends yk case xk called state x state space mdp µ called ergodic exists policy every state visited inﬁnitely often probability mdp stationary policy form markov chain environment called ﬁnite partially observable mdp pomdp sequence random variable sk taking value ﬁnite space called state space xk depends only sk yk independent k given sk abusing notation sequence k called underlying markov chain pomdp called ergodic exists policy underlying markov chain visit state inﬁnitely often probability particular any ergodic pomdp ν satisﬁes strong condition coeﬃcients decaying exponentially fast case set h ν ri h ν ri yi r thus any pomdp ν use proposition 3 k ε constant function show ν strongly 7 corollary 4 suppose pomdp ν ergodic exists set h ν ν ri r ﬁnite state space underlying markov chain ν strongly however illustrative obtain result mdps directly slightly stronger form proposition 5 any ergodic mdp ν strongly environment proof let k ε denote µ true environment let z k current history let current state observation xk environment x set possible state observe mdp optimal policy depends only current state moreover policy optimal any history let pµ policy let rµ expected reward pµ step let l b n ergodicity µ exists policy p el b ﬁnite doe not depend k policy p need get state b one state visited optimal policy act according pµ let f n nrmax logn p rµ k k nε p e k k nε p l b f n sup p e k n nε n n p l b f n sup p e k k nε n xk last term deviation reward attained optimal policy expectation clearly term bounded exponentially example function k ε constant ϕ n ε decay ponentially fast suggests class environment stretch beyond ﬁnite po mdps illustrate guess construction follows example environment inﬁnitely armed bandit next present construction environment not modelled ﬁnite pomdps consider following environment countable family ζi n arm source generating reward 0 1 say empty observation some probability δi reward action space consists three action g u get next reward current arm ζi agent use action beginning current arm 8 agent move arm follows move one arm using action u move ﬁrst environment using action reward action u clearly ν pomdp countably inﬁnite number state underlying markov chain general not isomorphic ﬁnite pomdp claim environment ν constructed proof let δ nδi clearly v ν probability 1 any policy policy p knowing probability δi achieves v ν p ν p v ν easily constructed indeed ﬁnd sequence j j n j ij satisfying policy p carefully exploit one one arm ζj staying arm long enough ensure average reward close expected reward εj probability εj quickly tends 0 switching arm ha negligible impact average reward thus ν shown explorable moreover policy p sketched made independent observation reward furthermore one modify policy p possibly allowing exploit arm longer time step some j j number current arm step thus any history z k one need k action one action u enough action catch 1 shown hold k ε k ri expected reward p step since p independent reward rpν independent rate ϕ n ε exponential construction also allow action bring agent step number current environment ζ according some possibly randomized function thus changing function dν k ε possibly making ε close desirable linear 5 necessity turn question tight condition strong following proposition show requirement k ε k 1 not relaxed proposition 7 necessity k ε k exists countable family deterministic explorable environment c any ν any sequence action k exists policy p rν n rpν k n rν reward attained optimal policy pν beginning wa acting optimally 9 any policy p exists environment ν v ν p v ν clearly environment class c satisﬁes value stability tions ϕ n ε except k ε k proof two possible action b three possible reward ri no observation construct environment follows yi ri 1 yi b ri any let ni denote number action taken step ni j yj 0 construct environment νs follows ri any ri b 2 longest consecutive sequence action b taken ha length greater ni ni otherwise ri b suppose exists policy p v νi p νi 0 let true environment assumption exists n yi b ri 0 yi ri 1 implies v p also easy show uniformity convergence 1 not dropped deﬁnition allow function ϕ n ε depend additionally past history z k theorem 2 doe not hold shown example constructed proof proposition 7 letting k ε instead allowing ϕ n ε z k take value 0 1 according number action taken achieving behaviour example provided last proof finally show requirement class c learnt able not easily withdrawn indeed consider following simple class environment environment called passive observation reward independent action sequence prediction task perhaps only reasonable class passive environment task agent get reward 1 yi reward 0 otherwise clearly any deterministic passive environment ν strongly dν k ε ϕν n ε rν 1 obviously class deterministic passive environment not countable since every policy p environment p errs exactly step claim class deterministic passive environment not learned 6 discussion proposed set condition environment called any countable class environment admits policy 10 wa also shown condition certain sense tight class environment includes ergodic mdps certain class ﬁnite pomdps passive environment provably environment cept allows characterize environment class proving typically much easier proving directly considered only countable environment class computational perspective class suﬃciently large class computable bility measure countable hand countability excludes continuously parameterized family like ergodic mdps common statistical practice perhaps main open problem ﬁnd condition requirement countability class lifted ideally would like some necessary suﬃcient condition class environment satisfy condition admits policy another question concern uniformity forgetfulness environment currently deﬁnition 1 function ϕ n ε history z k action history k history x k probably possible diﬀerentiate two type forgetfulness one action one perception particular any countable class passive environment perception independent action learnable suggesting uniform forgetfulness perception may not necessary proof theorem 2 policy p constructed follows step two police pt exploit pe explores policy p either take action according pt p z z according pe p z z speciﬁed policy p ha deﬁned step k environment µ endowed policy considered measure zk assume meaning use environment measure zk µ z algorithm denotes number current step sequence let jt let also environment ν ﬁnd sequence real number εν n εν n n εν n let ı n numbering ν ha inﬁnitely many index 1 deﬁne measure ξ follows ξ z x wνν z wν any number p νwν wν 0 ν 11 deﬁne step let ν ν z ξ z deﬁne νt set νt ﬁrst environment index greater ı jt case impossible empty increment deﬁne try increment jt deﬁne νe set νe ﬁrst environment index greater ı je v νe v νt νe z k 0 environment exists otherwise proceed one step according pt try increment je consistency step deﬁne νt deﬁne νt increment iterate inﬁnite loop thus incremented only νt not empty start inﬁnite loop increment let δ v νt let ε n ε δ set let prepare exploration increment index h incremented next attempt exploring νe attempt least h step length let pt νt set let ih current step find ih v νt 2 find 1 rνt νt 3 find 4 find 1 mdνe 1 mdνt 1 mdνt ih 5 moreover always possible ﬁnd k max 1 k k δ 6 iterate step exploration set pe py n νe iterate h step according p pe iterate either following condition break rνe k k k 12 ii iii νe observe either ii necessarily broken some step νt excluded inﬁnite loop iterated exploration νe not redeﬁne νe iterate inﬁnite loop νt νe still return prepare exploration otherwise loop iterated either νt νe changed end inﬁnite loop algorithm let u show probability 1 exploration part iterated only ﬁnite number time row νt νe suppose contrary suppose some probability exploration part iterated inﬁnitely often νt νe observe 1 implies break not greater ϕνe hence lemma event break inﬁnitely often ha probability 0 νe suppose hold almost every time ii broken except ﬁnite number time use 2 3 5 6 show probability least νt 1 1 νt using lemma k obtain event ii break inﬁnitely often ha probability 0 νt thus least one environment νt νe singular respect true environment ν given described policy current history denote environment known see measure µ ν mutually singular µ xn ν xn thus z ν z 7 observe deﬁnition ξ ν z ξ z bounded hence using 7 see z ξ z since αs not changed exploration phase implies some step excluded according consistency condition contradicts assumption thus exploration part iterated only ﬁnite number time row νt νe observe incremented only ﬁnite number time since z ξ z bounded away 0 either true environment ν any ment c equivalent ν current history latter follows fact ξ z ν z submartingale bounded expectation hence submartingale convergence theorem see converges 1 13 let u show some step ν environment equivalent always selected νt consider environment νt some step v νt v ν νt excluded since any optimal νt sequence action policy measure ν νt singular v νt v ν νe equal ν some point happens suﬃcient number time νt excluded exploration part algorithm decremented ν included finally v νt ν either optimal value v ν asymptotically attained policy pt algorithm pνt suboptimal ν 1 rpνt 1 v inﬁnitely often some ε ha probability 0 νt consequently νt excluded thus exploration part ensures environment not equivalent ν index smaller ı ν removed some step νt equal environment equivalent true environment shown exploration part ενt n finally using argument lemma deﬁnition k show exploration prepare exploration part algorithm average value within ενt n v νt provided true environment equivalent νt reference bosq nonparametric statistic stochastic process springer 1996 brafman tennenholtz general polynomial time algorithm reinforcement learning proc international joint conference artiﬁcial intelligence page 1999 lugosi prediction learning game bridge university press preparation csiszar shield note information theory statistic foundation trend communication information theory 2004 pucci de farias megiddo combine expert novice advice action impact environment sebastian thrun lawrence saul bernhard olkopf editor advance neural information processing system mit press cambridge 2004 doob stochastic process john wiley son new york 1953 kakade mansour reinforcement learning pomdps without reset ijcai page 2005 14 hutter poland prediction expert advice following perturbed leader general weight proc international conf algorithmic learning theory alt 04 volume 3244 lnai page padova springer berlin hutter policy general ronments based proc annual conference computational learning theory colt 2002 lecture note cial intelligence page sydney australia july springer hutter optimality universal bayesian prediction general loss alphabet journal machine learning research 2003 hutter universal artiﬁcial intelligence sequential decision based algorithmic probability springer berlin 2005 300 page kumar varaiya stochastic system estimation tiﬁcation adaptive control prentice hall englewood cliﬀs nj 1986 poland hutter defensive universal learning expert proc international conf algorithmic learning theory alt 05 volume 3734 lnai page singapore springer berlin poland hutter universal learning repeated matrix game annual machine learning conference belgium netherlands benelearn 06 ghent 2006 russell norvig artiﬁcial intelligence modern approach englewood cliﬀs 1995 sutton barto reinforcement learning introduction bridge mit press 1998 15