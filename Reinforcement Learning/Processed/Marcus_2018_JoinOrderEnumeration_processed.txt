deep reinforcement learning join order enumeration ryan marcus brandeis university ryan olga papaemmanouil brandeis university olga abstract join order selection play signiﬁcant role query performance however modern query optimizers typically employ static join enumeration algorithm not receive any feedback quality resulting plan hence optimizers often peatedly choose bad plan not mechanism learning mistake paper argue existing deep ment learning technique applied address challenge technique powered cial neural network automatically improve cision making incorporating feedback success failure towards goal present rejoin join enumerator present preliminary result indicating rejoin match outperform postgresql optimizer term plan quality join enumeration ciency 1 introduction identifying good join ordering relational query one lem database system 16 8 12 6 since selected join ordering drastic impact query performance 11 one challenge join ordering selection enumerating set candidate ordering identifying one searching larger candidate space increase odds ﬁnding ordering cost spending time query optimization join order enumerator thus seek simultaneously imize number plan enumerated ﬁnal cost chosen plan traditional database engine employ variety join enumeration strategy example system r 16 us dynamic programming ﬁnd deep join tree lowest cost postgres 1 greedily selects pair relation tree built many commercial product 4 clude exhaustive enumeration approach allow dba control size candidate plan set constraining structurally plan only cutting oﬀenumeration some elapsed time unfortunately heuristic solution often miss good execution plan importantly tional query optimizers rely static strategy hence not learn previous experience tional system plan query execute query plan forget ever optimized query lack feedback query optimizer may select bad plan repeatedly never learning previous bad good choice paper share vision based optimizer leverage information viously processed query aiming learn optimize future one eﬀectively ing better query plan eﬃciently spending le time optimization introduce novel proach query optimization based deep reinforcement learning drl 5 process machine learns task continuous feedback help artiﬁcial neural network argue existing deep reinforcement learning niques leveraged provide better query plan using le optimization time 1 12 mar 2018 ﬁrst step towards goal present join join order enumerator tirely driven deep reinforcement learning next section describe rejoin learning work section 2 provide promising preliminary result section 3 show rejoin outperform postgresql term eﬀectiveness eﬃciency join enumeration process 2 rejoin enumerator next present deep reinforcement learning join order enumerator call rejoin join enumeration rejoin assumes traditional approach query optimization used many modern dbms 10 1 speciﬁcally given sql query input join order ator search subspace possible join ordering cheapest ordering according cost model selected execution enumeration doe not perform index selection join operator tion etc task left component dbms optimizer join ordering captured binary tree leaf node represents base relation figure 1 show three possible join tree relation b c reinforcement learning reinforcement learning assumes 5 agent interacts ment follows environment tell agent current state st set potential action agent take agent selects action environment give agent reward rt higher reward desirable along new state new action set process repeat agent selects enough action reach terminal state no action available mark end episode new episode begin agent goal maximize reward receives episode learning experience ous action state reward achieved balancing exploration new strategy exploitation current knowledge c b b c b c e l e c f r b c w h e r e figure 1 three diﬀerent join ordering framework overview next formulate join order enumeration process reinforcement ing problem query sent optimizer rejoin represents episode rejoin learns multiple episode continuously ing query sent state represent subtrees binary join tree addition tion query join selection action represent combining two subtrees together single tree note subtree represent either input relation join subtrees episode end input relation joined terminal state point rejoin assigns reward ﬁnal output join ordering based optimizer cost model ﬁnal join ordering dispatched optimizer perform operator selection index selection ﬁnal physical plan executed dbms framework rejoin shown ure formally given query q accessing relation rn deﬁne initial state episode q rn state expressed state vector state vector fed neural network 14 produce probability distribution potential action action set ai any state every unique ordered pair gers 1 inclusive ai 1 1 action x represents joining xth yth element si together action new join selected sent back vironment transition new state state selecting action x si si x si si x new state fed neural network reward every state partial ordering zero predicate not change episode exclude notation 2 state layer hidden layer action layer sql vectorization initial state action selection reinforcement learning join order enumerator experience reward execution engine terminal state operator selection environment figure 2 rejoin framework reward action arriving terminal state sf complete ordering reciprocal cost join tree represented sf 1 riodically agent us experience tweak weight neural network aiming earn larger reward example figure 3 show potential episode query involving four relation b c initial state b c action set contains one element ordered pair relation 1 4 represents joining 2 3 represents joining b agent chooses action 1 3 ing choice join next state b agent next chooses action 2 3 representing choice join b next state b point agent ha only two possible choice 1 2 2 1 supposing agent lects action 1 2 next state c b represents terminal state point agent would receive reward based cost model evaluation ﬁnal join ordering state vector rejoin us vector representation state capture information join ordering binary tree structure predicate next outline simple vectorization strategy capture information strates reinforcement learning strategy eﬀective even simple input data tree structure capture tree structure data encode binary subtree join ordering decided far x row vector v size n n total number relation database value vi zero ith relation not x equal 1 h x otherwise h x height relation ri subtree x distance root example figure 3 ﬁrst row tree vector second last state b corresponds third column ﬁrst row ha value 1 2 corresponding c height 2 subtree second column ﬁrst row ha value zero since relation b not included subtree join predicate capture critical information join predicate create binary metric matrix episode value mi j one join predicate connecting ith jth relation zero otherwise ple representation capture feasible tions figure 3 show example matrix value 1 predicate value 0 no join predicate connecting b selection predicate selection predicate tor vector k total number attribute database total ber attribute across relation ith value one ith attribute ha selection predicate given query zero otherwise doe veal attribute not used ﬁlter tuples example figure 3 value corresponding one predicate 100 reinforcement learning policy gradient framework relies policy gradient method 19 one particular subset inforcement learning policy gradient reinforcement learning agent select action based ized policy πθ θ vector represents policy parameter given state st 3 tion set policy πθ output score action context score combining two join subtrees action selected using various method 5 reinforcement learning aim optimize icy πθ episode identify policy rameters θ optimizes expected reward jπ θ however reward jπ θ typically not feasible precisely compute hence policy gradient od search optimal policy parameter θ constructing estimator e gradient reward e θ θ given estimate e gradient ascent method tune initial random parameter θ incrementing parameter θi small value gradient θ positive positive gradient indicates larger value θi increase reward decrementing parameter θi small value gradient negative policy gradient deep learning policy gradient deep learning method 15 represent icy πθ neural network θ network weight thus enabling eﬃcient diﬀerentiation πθ 14 figure 2 show policy network used rejoin vectorized representation current state fed state layer value transformed sent ﬁrst hidden layer ﬁrst hidden layer transforms pass data second hidden layer pass data nal action layer neuron action layer resents one potential action output normalized form probability distribution policy πθ si ai selects action sampling probability distribution balance exploration exploitation 19 policy gradient θ estimated using sample previous episode query time episode completed join ordering given query selected rejoin agent record new tion θ r θ represents policy parameter used episode ﬁnal cost reward r received given set experience multiple episode x various advanced technique used estimate action 2 3 1 2 b c 3 action 1 2 1 2 c b final action 1 3 1 2 3 b c 4 c b e l e c f r b c w h e r e b n c n c n b 2 1 0 0 join predicate vector 1 2 b 1 b 2 0 0 0 1 b c 0 1 1 0 b 1 0 0 0 c 1 0 0 1 0 0 1 0 column predicate vector b c 1 0 0 0 b 0 1 0 0 c 0 0 1 0 0 0 0 1 b c 0 0 b 0 1 0 0 0 0 0 1 b c 0 0 b 0 0 b c b tree vector state figure 3 two possible join order selection episode 0 100 200 300 400 500 600 700 800 900 0 2000 4000 6000 8000 10000 12000 14000 query plan cost relative postgresql number episode query rejoin postgres figure 4 rejoin convergence dient e θ expected reward 18 15 3 preliminary result present preliminary experiment cate rejoin generate join ordering cost latency good often better one generated postgresql 1 optimizer experiment based join order benchmark job set query used previous assessment query optimizers 11 benchmark includes set 113 query instance 33 query plate imdb dataset created virtual machine dataset 3 query join 4 17 tions two largest relation contain row rejoin trained 103 query tested 10 query testing query set includes 4 0 10000 20000 30000 40000 50000 optimizer cost query 700000 750000 800000 850000 postgres quickpick rejoin cost join ordering 0 20 40 latency improvement query quickpick rejoin b latency generated plan 40 50 60 70 80 90 100 110 120 130 140 150 4 5 6 7 8 9 10 11 12 14 17 planning time relation postgresql rejoin update rejoin no update c optimization time figure 5 eﬀectiveness eﬃciency result instance one randomly selected query template template 1 addition six randomly lected query total database size wa primary foreign key indexed using postgresql 1 virtual machine 2 core ram maximum shared buﬀer pool size ured postgresql execute join ordering ated rejoin instead using join merator 2 rejoin us proximal policy optimization ppo algorithm 15 13 drl technique used two hidden layer 128 rectiﬁed linear unit relus 7 learning convergence evaluate learning convergence ran rejoin algorithm edly selecting random query training set start episode result shown figure show number query episode rejoin agent ha learned far show cost generated plan relative plan generated postgresql optimizer value 200 represents plan double cost plan selected gresql optimizer rejoin start no tion thus initially perform poorly number observed episode query increase performance algorithm improves around observed query rejoin begin ﬁnd plan lower predicted cost postgresql mizer query average cost plan generated rejoin 80 cost plan erated postgresql demonstrates enough training rejoin learn produce tive join ordering join enumeration eﬀectiveness evaluate eﬀectiveness join ordering produced join ﬁrst trained system query randomly selected 103 training query process took 3 hour used erated model produce join ordering 10 test query test query used verged model generate join ordering not update model model not get add any information test query perience set recorded cost according postgresql cost model plan resulting test join ordering well execution time compare eﬀectiveness rejoin postgresql well quickpick 17 tically sample 100 join ordering selects join ordering given dbms cost model result plan optimizer cost ﬁrst evaluated join ings produced rejoin enumerator based cost model assessment cost plan generated postgresql default enumeration ce quickpick rejoin 10 test query shown figure query xy axis refers instance template x job benchmark average rejoin produced join ordering resulted query plan 20 cheaper postgresql optimizer worst case rejoin produced cost only 2 higher 5 postgresql optimizer query show rejoin wa able learn generalizable join order enumeration strategy outperforms match cost join ordering produced postgresql optimizer relatively poorer formance quickpick demonstrates rejoin good performance not due random chance query latency figure show latency executed query plan created quickpick join relative performance plan selected postgresql optimizer test query executed 10 time cold cache graph show minimum maximum median latency improvement every case plan produced rejoin join ordering outperform match plan produced postgresql hence rejoin produce plan lower execution time not lower cost according cost model relatively poorer performance quickpick strates rejoin not simply guessing random join ordering join enumeration eﬃciency voiced opinion neural network machine learning general require operation expensive include database nals demonstrate approach like join actually decrease query planning time ure show average total query optimization time 103 query training set grouped number relation joined include planning time rejoin without icy update planning latency postgresql expected relation query resulted higher mization time join ordering need sidered ran cost model however rejoin apply model time linear number relation round two subtrees joined complete join ordering produced result postgresql query optimization time increase query optimization time rejoin relatively ﬂat policy update overhead additional overhead performing policy update using ppo relatively small however join model suﬃciently converged policy update skipped reduce query planning time additional 10 30 achieving even shorter query planning time 4 open challenge ongoing work simple reinforcement learning approach join enumeration indicates room ment space applying deep reinforcement learning algorithm query optimization problem overall believe rejoin open exciting new research path some highlight next latency optimization cost model depend cardinality estimate often would desirable use actual latency execution plan opposed cost model tion reward signal rejoin us cost model proxy query performance enables u quickly train algorithm large ber episode executing query plan especially poor one generated early episode would overly currently ing technique 9 bootstrap learning process ﬁrst observing expert system postgres query optimizer mimicking improving mimicked strategy optimization rejoin only handle join order selection requires optimizer lect operator choose index coalesce predicate etc one could begin expanding rejoin handle concern modifying action space clude decision 6 reference 1 postgresql database 2 postgresql documentation controlling planner explicit join clause 3 premade vm replication 4 sql server 5 arulkumaran et al brief survey deep reinforcement learning ieee signal processing 17 6 babcock et al towards robust query optimizer sigmod 05 7 glorot et al deep sparse rectiﬁer neural network pmlr 11 8 graefe et al volcano optimizer generator icde 93 9 hester et al deep demonstration aaai 18 10 lamb et al vertica analytic database 7 year later vldb 12 11 lei et al good query optimizers really vldb 15 12 ono et al measuring complexity join enumeration vldb 90 13 schaarschmidt et al tensorforce tensorflow library applied reinforcement learning 14 schmidhuber deep learning neural network nn 15 15 schulman et al proximal policy optimization algorithm arxiv 17 16 selinger et al access path selection relational database management system sigmod 89 17 waas et al join order selection good enough easy british national conference database 10 18 wang et al sample eﬃcient experience replay iclr 17 19 williams simple statistical algorithm connectionist reinforcement ing machine learning 92 7