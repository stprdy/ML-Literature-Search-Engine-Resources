1 mechanism called eligibility propagation proposed speed time hopping technique used faster reinforcement learning simulation eligibility propagation provides time hopping similar ability eligibility trace provide conventional reinforcement learning propagates value one state temporal predecessor using state transition graph experiment simulated biped crawling robot confirm eligibility propagation accelerates learning process 3 time introduction einforcement learning rl algorithm 16 address problem learning select optimal action limited feedback usually form scalar reinforcement function environment available general rl algorithm like 17 sarsa td λ 15 proved converge globally optimal solution certain assumption 1 17 flexible not require model environment shown effective solving variety rl task flexibility however come certain cost rl algorithm require extremely long training cope large state space problem many different approach proposed speeding rl process one possible technique use function approximation 8 order reduce effect curse dimensionality unfortunately using function approximation creates instability problem used learning significant achieved demonstration goal task available 3 apprenticeship learning 7 although risk running dangerous exploration policy real world 10 successful implementation apprenticeship learning aerobatic helicopter flight 11 another possible technique speeding rl use some form hierarchical decomposition problem 4 prominent example maxq value function manuscript submitted march 31 work wa supported part japanese ministry education culture sport science technology mext kormushev dong hirota department computational intelligence system science tokyo institute technology yokohama japan phone fax petar tou hirota nomoto industrial design center mitsubishi electric corporation tokyo japan decomposition 2 hybrid method using apprenticeship learning hierarchical decomposition successfully applied quadruped locomotion 14 18 unfortunately decomposition target task not always possible sometimes may impose additional burden user rl algorithm rl algorithm efficient state space exploration 6 us active exploration policy visit state whose transition dynamic still inaccurately modeled running directly real world might lead dangerous exploration behavior instead executing rl algorithm real world simulation commonly used approach ha two main advantage speed safety depending complexity simulation run many time faster experiment also time needed set maintain simulation experiment far le compared experiment second advantage safety also important especially rl agent expensive equipment fragile robot dangerous one chemical plant whether full potential computer simulation ha utilized rl however open question new trend rl suggests might not case example two technique proposed recently better utilize potential computer simulation rl time manipulation 12 time hopping 13 share concept using simulation time tool speeding learning process first technique called time manipulation suggests backward time manipulation inside simulation significantly speed learning process improve state space exploration applied rl problem balancing problem time manipulation ha shown increase speed convergence 260 12 paper focus second technique called time hopping applied successfully continuous optimization problem unlike time manipulation technique only perform backward time manipulation time hopping technique make arbitrary hop state traverse rapidly throughout entire state space ha shown accelerate learning process 7 time some problem 13 time hopping posse mechanism trigger time manipulation event make prediction possible future reward select promising time hopping target paper proposes additional mechanism called eligibility propagation added time hopping eligibility propagation speed time hopping reinforcement learning petar kormushev kohei nomoto fangyan dong kaoru hirota r 2 technique order provide similar ability eligibility trace provide conventional rl eligibility trace easy implement conventional rl method sequential time transition case time hopping due nature number obstacle overcome following section ii make brief overview time hopping technique component section iii explains important not trivial implement some form eligibility trace time hopping proposes eligibility propagation mechanism section iv present result experimental evaluation eligibility propagation benchmark problem biped crawling robot ii overview time hopping basic time hopping time hopping algorithmic technique allows maintaining higher learning rate simulation environment hopping appropriately selected state 13 example let u consider formal definition rl problem given markov decision process mdp fig state transition ha probability associated state 1 represents situation environment common learned quickly frequency state 1 visited highest state number increase probability corresponding state becomes lower state 4 represents rarest situation therefore unlikely well explored learned fig example mdp uneven state probability distribution time hopping create shortcut time shown dashed line otherwise distant state state connected path allows even state 4 learned easily applied mdp time hopping creates shortcut time making hop direct state transition distant state inside mdp hopping state make easier learned time help avoid unnecessary repetition already state 13 process completely transparent underlying rl algorithm component time hopping applied conventional rl algorithm time hopping consists 3 component 1 hopping trigger decides hopping start 2 target selection decides hop 3 hopping performs actual hopping flowchart fig 2 show 3 component time hopping connected interact rl algorithm time hopping trigger activated target state time selected considering many relevant property state probability visit frequency level exploration connectivity state number state transition etc target state time selected hopping performed includes setting rl agent simulation environment proper state time preserving acquired knowledge agent fig time hopping technique applied conventional rl algorithm lower group marked dashed line contains conventional rl algorithm main loop time hopping component upper group integrated iii eligibility propagation role eligibility trace eligibility trace one basic mechanism temporal credit assignment reinforcement learning 16 eligibility trace temporary record occurrence event visiting state taking action learning update occurs eligibility trace used assign credit blame received reward appropriate state action example popular td λ algorithm λ refers use eligibility trace almost any td method sarsa combined eligibility trace obtain general method may learn efficiently important implement some form eligibility trace time hopping well order speed convergence eligibility trace usually easy implement conventional rl method case time hopping however due nature not trivial since arbitrary hop state allowed impossible directly apply linear eligibility trace instead propose different mechanism called eligibility propagation select action execute action no yes rl initialization hopping trigger target selection hopping get reward change state ti im e h ho op pp pi ng g r rl l al lg go ri th hm ai n l lo oo op p 1 2 3 1 2 3 4 start shortcut time created time hopping 3 eligibility propagation mechanism time hopping guaranteed converge rl algorithm used 13 learned policy independent policy followed learning mean exploration policy doe not converge optimal policy fact time hopping deliberately try avoid convergence policy order maintain high learning rate minimize exploration redundancy pose major requirement any potential ha able learn independent state transition spread sparsely throughout state space proposed solution construct oriented graph represents state transition associated action reward use data structure propagate learning update way time hopping work graph might disconnected consisting many separate connected component regardless actual order time hopping visit state oriented graph contains record correct chronological sequence state transition example state transition considered state st state information state transition independent happened happen allows efficiently collect separate piece information obtained randomized hopping process uniformly using graph structure oriented graph available used propagate state value update opposite direction state transition edge way propagation logically flow backwards time state st temporal predecessor state propagation stop value update become sufficiently small mechanism illustrated fig fig eligibility propagation mechanism applied oriented graph state transition summary explicit definition proposed mechanism follows eligibility propagation algorithmic mechanism time hopping efficiently collect represent propagate information state transition us state transition graph propagation algorithm propagate state value one state temporal predecessor state concrete implementation mechanism within time hopping technique given following section implementation eligibility propagation proposed implementation eligibility propagation called reverse graph propagation value propagated inside graph reverse opposite direction state transition direction process similar propagation bfs search algorithm order give specific implementation description used underlying rl algorithm following proposed eligibility propagation mechanism construct ordered set queue state transition called propagationqueue initialize current state transition 1 way 1 propagationqueue 1 take first state transition 1 propagationqueue remove queue let max q current maximum state st max max q q 2 transition state st state done executing action reward r received update making state transition 1 using update rule 1 max q r q γ 3 let max q new maximum state st calculated using formula 2 max max q q ε 4 construct set immediate predecessor state transition state st 1 1 transition graph 5 append end propagationqueue propagationqueue go step 2 stop decision whether propagation necessary made step propagation continues one step backwards time only significant difference old maximum new one according formula 4 formula based fact max q might different max q exactly 3 4 possible case transition 1 wa one highest value state st new bigger value need propagated backwards predecessor state predecessor state next state st reward action eligibility propagation current state 4 transition 1 wa one highest value not any value reduced propagation new maximum value belongs different transition necessary transition 1 wa not one highest value became one value need propagation only case propagation not necessary transition 1 wa not one highest value still not one update case max q equal max q formula 4 correctly detects skip propagation previous 3 case propagation performed provided significant change value determined ε parameter ε smaller algorithm tends propagate value change ε bigger tends propagate only biggest change step backwards skipping any minor update depth propagation also depends discount factor γ bigger γ deeper propagation reward accumulation stimulated still due exponential attenuation future reward γ discount factor prevents propagation going far reduces overall computational cost fig eligibility propagation integrated component technique described eligibility propagation mechanism encapsulated single component integrated time hopping technique shown fig called immediately state transition take place order propagate any potential change time hopping step occurs iv application eligibility propagation biped crawling robot order evaluate efficiency proposed eligibility propagation mechanism experiment simulated biped crawling robot conducted goal learning process find crawling motion maximum speed reward function task defined horizontal displacement robot every action experimental environment dedicated experimental software system called time hopping environment wa developed purpose evaluation general view environment shown fig ha physic simulation engine implementation time hopping technique useful visualization module simulation learning data state transition graph importantly prototype implementation eligibility propagation mechanism facilitate analysis algorithm behavior display detailed information current state previous state transition visual view simulation allows runtime modification important parameter algorithm simulation manual automatic control time hopping technique well visualization accumulated data form chart fig general view time hopping environment physic engine running biped crawling robot simulation description crawling robot experiment conducted physical simulation biped crawling robot robot ha 2 limb 2 segment total 4 degree freedom dof every dof independent rest ha 3 possible action time step move clockwise move stand still fig 6 show typical learned crawling sequence robot visualized simulation environment constructed task fig crawling robot 2 limb 2 segment total 4 dof nine different state crawling robot shown typical learned crawling sequence select action execute action no yes rl initialization hopping trigger target selection hopping eligibility propagation get reward change state ti im ho op pp pi ng g r rl l al lg go ri th hm ai n l lo oo op p 1 2 3 4 5 possible action dof robot combined assuming move time independently produce action space size 34 1 80 exclude possibility dof standing still using appropriate discretization joint angle 9 upper limb 13 lower limb state space becomes divided 9 x 13 2 13689 state better analysis crawling motion limb ha colored differently only skeleton robot displayed description experimental method conducted experiment divided 3 group experiment using conventional experiment using only time hopping technique applied described 13 experiment using time hopping eligibility propagation implementation used time hopping component shown table experiment three group conducted exactly way using rl parameter incl discount factor γ learning rate α action selection method parameter initial state robot simulation environment parameter also equal robot training continues fixed number step 45000 achieved crawling speed recorded fixed checkpoint training process repeated 10 time result averaged order ensure statistical significance evaluation eligibility propagation evaluation eligibility propagation done using 3 main experiment first experiment learning speed conventional time hopping time hopping eligibility propagation compared based best solution found fastest achieved crawling speed number training step comparison result shown fig show duration training needed 3 algorithm achieve certain crawling speed achieved speed displayed percentage globally optimal solution result show time hopping eligibility propagation much faster time hopping alone turn much faster conventional fig comparison conventional time hopping time hopping eligibility propagation based best solution achieved relative duration training achieved crawling speed measured percentage globally optimal solution fastest possible crawling speed robot compared time hopping alone eligibility propagation achieves significant learning process example 80 crawl learned only 5000 step eligibility propagation used time hopping alone need around 20000 step learn case eligibility propagation need 4 time fewer training step achieve result becomes even higher number training step increase example time hopping eligibility propagation reach 90 solution 12000 step time hopping alone need 50000 step compared conventional eligibility propagation achieves even higher example need only 4000 step achieve 70 solution conventional need 36000 step learn case eligibility propagation 9 time faster time hopping alone also outperforms conventional factor 3 case 12000 step 36000 step second experiment real computational time conventional time hopping time hopping eligibility propagation compared actual execution time necessary 3 algorithm reach certain crawling speed measured comparison result shown fig fig comparison conventional time hopping time hopping eligibility propagation based real computational time algorithm required reach certain quality solution certain crawling speed table implementation used time hopping component component name implementation used 1 hopping trigger gamma pruning 2 target selection lasso target selection 3 hopping basic hopping 4 eligibility propagation reverse graph propagation 6 result show time hopping eligibility propagation achieves 99 maximum possible speed almost 3 time faster time hopping alone 4 time faster conventional significant learning process achieved despite additional computational overhead maintaining transition graph reason improved based precise future reward prediction confirmed third experiment goal third experiment provide insight state exploration distribution order explain result previous two experiment conventional time hopping time hopping eligibility propagation compared based maximum achieved explored state 45000 training step sorted decreasing order represent distribution within explored state space fig 9 show comparison result fig comparison conventional time hopping time hopping eligibility propagation show sorted sequence maximum explored state 45000 step training time hopping eligibility propagation ha managed find much higher maximum explored state conventional ha explored state ha found lower firstly result show time hopping eligibility propagation ha managed find significantly higher maximum explored state compared conventional time hopping reason eligibility propagation manages propagate well state value update among explored state therefore raising maximum secondly result show time hopping time hopping eligibility propagation explored much fewer state conventional reason component time hopping focus exploration time hopping promising branch avoids unnecessary exploration conventional doe not mechanism therefore explores state find lower also time hopping eligibility propagation ha explored slightly fewer state time hopping alone reason algorithm concentrate exploration promising part state space only eligibility propagation manages propagate well among explored state improves accuracy future reward estimation performed component time hopping turn detects better unpromising branch exploration trigger time hopping step avoid purposeful exploration better propagation acquired state information help eligibility propagation make best every single exploration step important advantage proposed mechanism especially simulation involved computationally expensive case eligibility propagation save real computational time reducing number normal transition simulation step favor time hopping step conclusion eligibility propagation mechanism proposed provide time hopping similar ability eligibility trace provide conventional rl operation time hopping completely change normal sequential state transition rather randomized hopping behavior throughout state space pose challenge efficiently collect represent propagate knowledge action reward state transition since using sequential eligibility trace impossible eligibility propagation us transition graph obtain predecessor state updated state way propagation logically flow backwards time one state temporal predecessor state proposed mechanism implemented fourth component time hopping technique maintains clear separation 4 time hopping component make straightforward experiment alternative component implementation biggest advantage eligibility propagation speed learning process time hopping 3 time due improved ability based precise future reward prediction turn increase exploration efficiency better avoiding unpromising branch selecting appropriate hopping target conducted experiment biped crawling robot also show achieved using significantly fewer training step result becomes even higher simulation computationally expensive due purposeful exploration property make eligibility propagation suitable speeding complex learning task require costly simulation another advantage proposed implementation eligibility propagation no parameter tuning necessary learning make mechanism easy use finally important drawback proposed technique need additional memory store transition graph data word achieved using memory reference 1 dayan sejnowski td λ converges probability 1 mach vol 14 no 3 pp 1994 2 dietterich hierarchical reinforcement learning maxq value function decomposition artif intell vol 13 pp 2000 7 3 coates abbeel ng learning control multiple demonstration icml vol 25 2008 4 barto mahadevan recent advance hierarchical reinforcement learning discrete event dynamic system vol 13 pp 2003 5 humphrys action selection method using reinforcement learning phd thesis university cambridge june 1997 6 kearns singh reinforcement learning polynomial time machine learning 2002 7 ng reinforcement learning apprenticeship learning robotic control lecture note computer science vol 4264 pp 2006 8 precup sutton dasgupta learning function approximation proceeding eighteenth conference machine learning icml 2001 ed kaufmann 2001 9 price boutilier accelerating reinforcement learning implicit imitation journal artificial intelligence research vol 19 pp 2003 10 abbeel ng exploration apprenticeship learning reinforcement learning icml 2005 11 abbeel coates quigley ng application reinforcement learning aerobatic helicopter flight nip vol 19 2007 12 kormushev nomoto dong hirota time manipulation technique speeding reinforcement learning simulation international journal cybernetics information technology vol 8 no 1 pp 2008 13 kormushev nomoto dong hirota time hopping technique faster reinforcement learning simulation ieee transaction system man cybernetics part b submitted 2009 14 kolter rodgers ng control architecture quadruped locomotion rough terrain ieee international conference robotics automation 2008 15 sutton learning predict method temporal difference mach vol 3 pp 1988 16 sutton barto reinforcement learning introduction cambridge mit press 1998 17 watkins dayan mach vol 8 pp 1992 18 kolter abbeel ng hierarchical apprenticeship learning application quadruped locomotion neural information processing system vol 20 2007