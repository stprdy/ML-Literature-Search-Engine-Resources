10 apr 2010 epj manuscript no inserted editor memory model multiagent reinforcement learning mean ﬁeld approximation dynamic ihor shigeru 1 university aizu tsuruga city fukushima japan 2 prokhorov general physic institute russian academy science vavilov str 38 moscow 119991 russia received date revised version date abstract continuous time model multiagent system governed reinforcement learning free memory developed agent assumed act independently one another optimizing choice possible action via search gain awareness action value agent accumulate memory reward obtained taking speciﬁc action moment time contribution reward past agent current perception action value described integral operator kernel finally fractional diﬀerential equation governing system dynamic obtained agent considered interact one another implicitly via reward one agent depending choice agent pairwise interaction model adopted describe eﬀect speciﬁc example system interaction two agent three agent system type analyzed detail including stability analysis numerical simulation memory demonstrated cause complex dynamic system hand particular shown simultaneously two mode system instability undergoing subcritical supercritical bifurcation latter one exhibiting anomalous oscillation amplitude period growing time besides instability onset via supercritical mode may regarded altruism three agent system instability dynamic found rather irregular composed alternate fragment oscillation diﬀerent property pac dynamic social system system obeying scaling law decision theory game theory system 1 introduction last decade application physical notion mathematical formalism statistical physic describing economic social system attracted much tention scientiﬁc community see 1 ciency approach ha demonstrated ular modeling cooperative motion vehicle traﬃc ﬂow pedestrian ensemble group animal cial behavior 2 dynamic stock market opinion formation culture language evolution 5 agent reinforcement learning problem one ing technique modeling evolution adaptation complex system human factor play essential role problem wa studied mainly within scope artiﬁcial intelligence review see 6 nevertheless recently concept statistical physic combined notion reinforcement learning simulate dynamic minority game ialub b kanemoto evolutionary game 11 adaptive competition market 12 well establish relationship reinforcement learning replicator model lation biology latter particular made possible analyze complex behavior including onset dynamical chaos agent ensemble using technique dynamical system noted majority model similar one listed constructed capture istic feature social system dynamic invoke notion concept inherited directly statistical physic however generally speaking agent model imitating behavior human posse also feature inapplicable physical object least anomalous standpoint physic one anomalous feature impact system history implemented particular via eﬀects man memory learning process well evaluation previous event decision making current moment time comparing social system object classical physic note latter one no ory following sense position 2 ihor lubashevsky shigeru kanemoto memory model multiagent reinforcement learning tie particle forming closed system known some moment time chosen arbitrary ther dynamic completely predictable least formally randomness arises level reduced description even case dominating approximation vian random process without memory given state system some moment time probabilistic dynamic completely determined broadly speaking present paper intend focus possible history eﬀects social system whose dynamic imitated multiagent reinforcement learning well discus mathematical notion relevant description general argument human perception event evaluation convince u make use model describe impact system history inforcement learning model impact event happened past time weighted current moment time some function k showing decrease time diﬀerence fact let u consider two event inﬂuencing decision similar manner enables u compare assessing current situation one two event happened one day current date whereas happened one week ago regard substantially diﬀerent time respect contribution perception present uation contrast ﬁrst event occurred one month one day ago second event occurred one month one week ago draw no real distinction tween time occurrence word time lag two event ble time scale separating present moment impact regarded diﬀerent magnitude contrary time lag much le passed time event considered simultaneous exactly behavior common dependence k certain exponent z model partly justiﬁed observed term memory eﬀects foraging primate insect conclusion explicit relationship foraging memory property 25 human memory retrieval also characterized pattern 26 addition fact composed many distinct system review see 27 besides stock market man factor doubtless great concern exhibit similar eﬀects particular time correlation volatility return characterized decay see speciﬁc purpose present paper analyze memory introduced tiagent reinforcement learning well consider characteristic eﬀects dynamic multiagent system agent interaction similar game system agent share common ment time one take some action disturbs environment causing response agent learning process enables agent memory follow variation ronment optimizing action scissors model determining agent interaction wa sen two reason first model one simplest example interaction rock beat pair scissors scissors beat sheet paper paper beat rock ordering three object according dominance get competitive cycle responsible variety phenomenon social ological system see 30 reference therein second dynamic found rather common polymorphic population ular jamaican cryptic coral reef community 31 yeast population 32 population marine isopod 33 34 microbial laboratory community phic group lizard ruﬀs gouldian ﬁnches 41 fact merit detailed discussion game speciation trimorphism population wherein specie interaction referred biological game 42 caused basic property frequency dependent selection crucial maintaining diversity review see 43 frequency dependent selection fgs rate oﬀspring generation pending relative volume given phenotype implemented community via various mechanism mechanism classiﬁed two group itive negative fds according whether cause dependence increasing decreasing tively mixture positive negative fds act system become destabilized oscillate 43 keeping mind construction note learning innate behavior one main mechanism responsible positive fds aﬀecting 43 example served rapid evolution dispersal movement viduals natal previous breeding site new one 44 requires presence heritable genetic ations trait aﬀecting dispersal behavior strong selection acting trait see review 45 particular vertebrate dispersal often sidered plastic trait low heritability signiﬁcant heritability sal propensity probability dispersal habitat patch ha directly found collared catcher 46 learning eﬃcient requires enhanced memory especially concern specie bird mammal cache food later use bird carry regular seasonal migration returning breeding wintering ground even stopover site example clark nutcracker posse memory cache site spanning month 47 migratory garden warbler ory particular feeding site persists least 12 month 48 worthwhile note also investigation ihor lubashevsky shigeru kanemoto memory model multiagent reinforcement learning 3 relationship spatial use strategy medial dorsal cortical volume male lizard criterion specie memory capacity 49 male specie occur three morphs using ent spatial niche large territory holder small territory holder nonterritory holder home range smallest size according found result larger used territory larger medial dorsal cal volume analysis geographic variation tionary history lizard mating strategy 50 ha thrown light basic mechanism governing speciation polymorphic form within population starting material new specie case scissors fds causing cycle variation tains required polymorphism 50 one hand found pattern ha turned contain also phic dimorphic population resemble morphs trimorphic population resemblance suggests cycle morphs destabilized ing rise irreversible loss ﬁxation certain morphs thus rapid phenotype evolution hand phylogenetic analysis revealed trimorphism wa maintained million year even across speciation event thereby interaction specie trimorphic population seems cause evolutionary stable cyclic variation morphs undergo repeated breakdown event 50 addition alyzing clade dung beetle 55 ha strated insect assumed dimorphic form trimorphic population cyclic phenotype variation via two threshold mechanism model developed demonstrate term memory multiagent reinforcement learning governed interaction responsible complex dynamic particular relatively regular oscillation alternated tions large amplitude period growing time system admits instability onset via subcritical well supercritical bifurcation diﬀerent mode perturbation 2 agent memory reinforcement learning continuous time description dynamic let u consider collection n agent ai 1 2 n individually take one action set x x act pendently one another preference action x given agent determined agent perception value qa x gained current moment time exploring action x previously within approximation agent assumed take new action time moment tn n n 0 1 2 time step probability choosing action x given agent time pa x eβqa x x eβqa 1 quantity characterizes perception old agent evaluating action initial time 0 n 0 agent no information action value condition qa x 0 hold every agent action otherwise initial condition qa x x describes agent preliminary opinion action value numerical simulation described dition wa used quantity x set equal some random number disturb system rium induce transient process system dynamic governed learning agent aimed ﬁnding appropriate action via search following make use simple integrator algorithm reinforcement learning see first assumes agent accumulate local reward received one step raise awareness value possible action ond bounded capacity agent memory event past separated present time scale exceeding certain value practically not tribute awareness gained agent rent time third according expression 1 agent explores often action vicinity action optimal current point view struct value possible action properly weight local reward diﬀerently depending imity given action optimal one ing also supported fact people overweight event underweight event mary description detailing possible outcome respective likelihood option review see allow feature introduce coeﬃcient wa x w pa x 3 weighting reward gained given agent taking action x time depending current perception action value following spirit prospect theory 60 update rule frequency imum heuristic 6 weight coeﬃcient 3 assumed function current probability pa x agent taking action ansatz w p 1 widely met literature however w p rather weak 4 ihor lubashevsky shigeru kanemoto memory model multiagent reinforcement learning agent memory long enough reinforcement ing mechanism consideration give rise nash equilibrium instability not due agent action appendix caused higher rate reward accumulation action higher ability present paper focus multiagent reinforcement learning whose complex dynamic caused mutual interaction agent possessing ular memory standpoint instability may treated malfunction lyzed algorithm possible inﬂuence studied phenomenon eliminated keeping mind result obtained appendix adopt ansatz wa x 1 pa x 4 match simplest model free bilities nash equilibrium not caused agent interaction describes accumulation knowledge action value proceeding uniformly age possible action taking account aforementioned feature following version equation qa x pa x tn ra qa x tn qa x tn 5 applied update agent awareness time ments tn ﬁrst term expression 5 hand side describes accumulation knowledge action xa taken agent time tn δxxa kronecker delta payoﬀfunction ra describes reward normalized unit time agent ai gain action x provided agent aa taken action xa xn cofactor x weight contribution action second term caused agent memory loss follows inequality 6 assumed hold beforehand virtue inequality 6 function qa x reach some saturation large number system date need executed word every agent gain awareness action value many trial thus explores explicitly implicitly many option agent action enables u ﬁrst average tion 5 possible point conﬁguration space x xn xi assuming probability particular conﬁguration x occurring time determined expression p x n pi xi 7 second treat action value qa x uous function time decomposition 7 due adopted assumption mutual independence agent taking action way appendix update rule 5 reduced following diﬀerential equation dqa x dt ra x qa x 8 qa x qa x x qa 9 ra x x xa ra x x xa ra 10 reward rate gained agent taking action x measured relative value averaged possible action follows conﬁne consideration dynamic quantity qa x instead qa x two reason one fact probability pa x agent choice equally treated direct function qa x indeed substituting 9 1 get pa x eβqa x x eβqa 11 possibility eliminate strong neous growth action value consideration doe not aﬀect system dynamic ha no deﬁnite physical interpretation quantity qa x meet equality x qa x 0 12 any agent moment time follows form deﬁnition 9 noted expression 10 actually iﬁes some autonomous operator ra qn ping quantity qi onto reward rate ra x ra x x qn x 13 hold also rather arbitrary form w p dependence appendix fact expression 8 ihor lubashevsky shigeru kanemoto memory model multiagent reinforcement learning 5 autonomous nonlinear equation form complete description multiagent system hand provided payoﬀfunction ra known agent memory characterized time scale clarify latter statement let u consider integral representation equation 8 memory model notion initial condition using method variation parameter ential equation 8 reduced following volterra integral equation qa x z ra x qa x 14 function ra x given expression 13 volterra equation 14 interpreted plicit formulation agent memory model ized time scale former term hand side 14 speciﬁes accumulation agent knowledge action value time interval whereas latter one determines evolution knowledge gained past fact dealing whole history system replace expression 14 corresponding integral semiaxis qa x z ra x 15 using property exponential function 16 introduce notion initial condition setting qa x z ra x 17 converting 15 back 14 framework model similar event within time span tribute agent perception equivalently tion k exp kernel integral operator 14 weighting current contribution event happened past agent memory described another kernel k not equal exponential function qa x z k ra x 18 property corresponding equality 16 doe not hold notion initial condition becomes cable general case however appealing example ecological system discussed introduction see notion tial condition meaning independent speciﬁc memory model habit oﬀspring tifactorial nature originate particular mixture parental environmental contribution parental eﬀects include transmission preferable strategy behavior reﬂecting ness gained previously parent environmental tor responsible accumulating information personal experience inﬂuence grows increasing age discussion phenomenon connection diversity formation found review 63 therefore mathematical description lative eﬀect natal awareness personal experience specie adaptation time birth appropriate point introducing relevant initial condition present paper construct initial tions analyzed process reinforcement learning presuming certain speciﬁc time moment process initiated keeping mind communication let u adopt ing three assumption learning process agent memory first within suﬃciently long time interval agent remember time event happened contribution current moment time weighted kernel k exponent 0 γ latter inequality due fact one hand agent preference really cumulative eﬀect previous reward integral z z γ γ 19 ha diverge formally hand kernel k must decreasing function estimate integral 19 regarded tain capacity agent memory relating action value qa x ra x mean reward ra x gained agent time second temporal scale larger agent not rank event according occurrence time ﬁx event memory described replacement z k ra x ra x z k 20 scale integral z 21 converge lower limit addition tion memory capacity order estimate hold latter case kernel k 6 ihor lubashevsky shigeru kanemoto memory model multiagent reinforcement learning factor due continuity function k summarizing two assumption state kernel k memory model exhibit following asymptotic behavior k τ k certain microscopic time scale τ ha duced kernel k must dimensionless quantity present construction let u make use function 64 rabotnov function 65 k τ eγ γ 23 construct crossover given asymptotics eγ γ z x zk γ k 1 γ 24 function two parameter γ z gamma function limit γ event within time scale contribute equivalently agent perception current time kernel 23 take exponential form k ure 1 illustrates behavior kernel 23 third initial time agent no personal knowledge value action x rely only awareness gained previously some way certain analogy situation described within ond assumption only fact some event happened essential whereas occurrence time not known quantify deal only some quantity qa x aggregating information whole make possible measure contribution personal experience unit let u introduce eﬀective reward rate ra x related quantity qa x expression qa x ra x z k 25 time go contribution evolves ra x z k kb qa x 26 fig plot illustrating excepted approximation 23 kernel k v time diﬀerence upper panel crossover asymptotics 22 lower panel function kb r k r k 27 using function one parameter eγ z deﬁned via series 64 eγ z x zk γ γk 1 28 show directly dt eγ 1 γ eγ γ thus k τ γ eγ therefore formula 27 rewritten kb eγ 29 provided kernel k given expression 23 ihor lubashevsky shigeru kanemoto memory model multiagent reinforcement learning 7 combing together three assumption write desired integral volterra equation governing gent reinforcement learning memory following form qa x τ z eγ γ h ra x eγ qa x 30 former term equation speciﬁes accumulation agent knowledge action value gained via reinforcement learning whereas ter one describes evolution contribution noted relative ematical construction discussed within algorithm reinforcement ing 57 concluding given section underline introduced notion initial condition implies existence certain special point agent life moment agent start activity ﬁrst time thus no direct experience taking speciﬁc action contrast memory model ﬁxed time scale enables one impose initial condition system dynamic any moment time governing equation obtained integral equation 30 converted diﬀerential equation fractional time derivative ing fractional calculus namely known relationship cauchy type problem fractional ential equation volterra integral equation 64 enables u reduce 30 following equation cb dγ x τ x 1 qa x 31 side caputo fractional derivative order γ deﬁned expression cb dγ x 1 γ 1 z γ dqa x 32 equation 31 subjected initial tion 2 strictly condition qa x x 33 quantity x x x given forehand expression 31 desired governing tion mean ﬁeld theory analyzed multiagent reinforcement learning memory also point memory description multiagent reinforcement learning no longer reduced replicator equation population biology reduction hold governing equation reinforcement learning ha ﬁrst order time derivative pairwise agent interaction complete construction model hand need specify interaction agent termined payoﬀfunction ra let u conﬁne consideration pairwise approximation agent interaction 66 particular biodiversity eﬀects largely described term pairwise interaction among specie however interaction seem necessary describing complex hierarchical community model pairwise agent action payoﬀfunction ra written 71 ra ρx x 34 keeping mind formula 10 determining reward rate ra x well identity x pa x 1 worthwhile rewrite expression 34 way eliminates term not contributing ra x combine similar term one another namely let u make use following replacement ρx ρx ρy e x e e e e e notation e 1 x e 1 2 x introduced case expression 10 becomes ra x ρx x x 35 without loss generality follows sume equality ρy e 0 e 0 e 0 36 hold beforehand 3 model present section consider simple system type demonstrate free memory give rise complex dynamic caused instability mode diﬀerent time scale ciﬁc model constructed mimic particular some 8 ihor lubashevsky shigeru kanemoto memory model multiagent reinforcement learning fig diagram illustrating agent interaction type characteristic feature mating behavior trimorphic population lizard see 72 erences therein general review role sexual selection maintaining biodiversity found 73 population female preference impact quency dependent selection essential change female age female lizard exist two distinct morphs however describing female choice based multiple male trait three type female singled 74 male express three morphs exhibit alternative strategy intrasexual competition referred ical game see introduction recently mate choice governed learning accumulation individual perience ha attracted theoretical attention particular optimal choice disturbed varying vironment 75 phenomenon mutual learning 76 review ing model used notion also presented cited paper context model consideration regarded example illustrating dynamic male mate choice aﬀected female intrasexual petition model let u focus two system comprising two agent three agent set possible action consisting three element action considered equivalent ery agent therefore set ρx 0 plicit pairwise interaction agent one another determined two factor one ture reward gained pair agent taking diﬀerent action reward assumed determined payoﬀmatrix 0 1 0 1 1 0 illustrated fig according trix example agent take action agent take action ﬁrst agent ceives beneﬁt whereas second one loses value factor reward redistribution example agent take action xi specify latter factor ascribe agent individual power 0 ηa 1 assign example agent reward 37 respectively interaction constant 0 symmetrical respect permutation dice summarizing assumption following cepted renormalization payoﬀfunction ra sec write interaction matrix form b 2 1 2 1 1 2 38 addition time scale characterizing ability agent memory introduced sec set equal inﬁnity order study eﬀects condition governing equation 31 come τ dγ x x p p 39 equation posse stationary solution qeq x 0 40 every agent action x match nash equilibrium attained action equivalent value thus p eq x follows equation 39 analyzed spect stability system dynamic ment possible instability studied numerically addition single aﬀects due memory solely analysis ﬁned case identical agent setting g ǫ any pair agent linear stability analysis given multiagent system two source equilibrium perturbation one deviation initial value x qeq x random variation reward rate reﬂecting probabilistic nature agent choice since initial condition imposed system only certain speciﬁc ment time two type perturbation need considered separately present paper concern deterministic tion agent choice dynamic equilibrium turbations introduced via initial condition 33 section explicitly analyze system stability respect ﬁrst type perturbation second type perturbation considered appendix ihor lubashevsky shigeru kanemoto memory model multiagent reinforcement learning 9 eigenfunctions caputo fractional derivative operator 32 meeting cauchy initial condition type 33 written term function one parameter namely eγ λ γ λ corresponding eigenvalue complex number general case follows identity 64 cb dγ λ γ λeγ λ γ 41 known asymptotic behavior tion eγ z order 0 γ 1 64 enables u represent asymptotics eigenfunctions eγ λtγ 1 γ e 1 tγ argument eigenvalue λ lie interval λ eγ λtγ 1 λγ 1 tγ 1 λ according expression 42 instability occurs governing equation 39 admits eigenvalue meeting inequality λ case virtue perturbation growth exponential asymptotics matching stable system dynamic describes decay perturbation therefore linearizing equation 39 near stationary point 40 seek solution form qa x θx aeγ λ γ 43 θx some constant way eigenvalue problem equation 39 reduced ﬁnding value h matrix b 0 b ρ b ρ 0 b 0 b ρ b ρ b ρ 0 b ρ b ρ b ρ 0 44 system two three agent respectively notation b ρ 2 1 2 1 1 2 45 stand matrix 38 case consideration eigenvalue h λ related via expression λ 1 3 hgτ 46 addition virtue 12 corresponding tor meet equality x θx h 0 47 fig instability diagram analyzed system tical agent every agent using wolfram mathematica 7 found desired collection eigenvalue n 3 ǫ 3 two agent system n 3 2 ǫ 3 three agent system ﬁrst two eigenvalue doubly degenerate whence follows two system become unstable arg ǫ 3 inequality γ 2 π arctan 3 ǫ 49 hold figure 3 depicts condition demonstrated appendix b found condition system instability hold also case librium perturbation caused random ﬂuctuations reward rate complex system behavior found given deterministic model dissipation not change drastically inﬂuence small tions reward rate numerical simulation result algorithm assumption adopted previous section governing equation 39 rewritten mensionless form using replacement β qa x x characteristic time scale τp system namics τp τ βρτ 50 10 ihor lubashevsky shigeru kanemoto memory model multiagent reinforcement learning term take form cb dγ b 51 example two agent system vector denote following collection variable component vector related pa x eqa x 3 x eqa 52 virtue 11 side equation 51 function vector whose derivative respect nents vector bounded any value enabled u make use explicit algorithm numerical simulation system dynamic cussion point see namely governing equation 51 wa solved numerically using explicit algorithm second order 79 based following discretization equation 51 qn x ω γ k b γ n b f 2 2 p 2 p 53 index denote time moment tn n 2 3 corresponding quantity taken whereas index 2 3 labeling system sideration omitted sake simplicity ωγ k coeﬃcients entering deﬁnition fractional derivative speciﬁed example via lowing recursive formula ω γ 0 1 ω γ k 1 γ k ω γ 54 k 1 2 coeﬃcient b γ n x ω γ k 55 value ﬁrst step iteration wa lated b f p 56 initial value meeting equality 12 wa set domly initiate system perturbation near nash librium 40 instability mode given paper conﬁne discussing iou mode system instability found numerically let u ﬁrst present result simulation two agent system figure 4 depicts two mode b dynamic gotten varying initial tions shown curve obtained parameter ǫ memory exponent γ bility diagram fig 3 point lie inside instability region near boundary given magnitude parameter ǫ critical value memory exponent γc mode related stable limit cycle phase space arising mismatch action two agent able fig 4 lower row mode wa found arise stability region also γ γc particular γ figure 3 characterizes system ity only respect inﬁnitesimal perturbation rather perturbation ﬁnite initial amplitude result demonstrate u mode system stability undergoes subcritical bifurcation ory exponent γ increase periodic oscillation found subcritical region γ γc rather similar form shown fig only memory exponent γ go away critical value γc ǫ come close certain boundary absolute stability γs ǫ oscillation exhibit complex behavior particular fig 5 left column depicts steady state oscillation tained γ corresponding trajectory system motion ﬁlls uniformly certain neighborhood previous limit cycle system motion regarded oscillation mode shown fig 4 whose amplitude undergoes some time variation failed ﬁnd steady state oscillation γ ǫ perturbation induced initial random condition faded allows u estimate ary absolute instability γs ǫ given value parameter mode system instability arises memory exponent γ increase attractor seems become rather complex form instantly without smooth transformation quasicircular line phase space second mode b related appearance oscillatory instability undergoing supercritical cation arising only instability region γ γc turn even close proximity stability boundary simpliﬁed model 51 seems not able describe possible steady state dynamic type b instability found time pattern exhibit agent preference taking only one action become stronger stronger time go doe duration choice fig 4 right column also seen ﬁgure mode b match chronized behavior two agent likely oscillation stabilized temporal scale order capacity agent memory any case feature worthy individual analysis claim least characteristic time scale oscillation caused instability onset within mode b diﬀer dramatically ihor lubashevsky shigeru kanemoto memory model multiagent reinforcement learning 11 fig two mode dynamic found two agent system upper row visualizes dynamic agent ternary phase portrait trajectory middle row show corresponding time pattern agent action chosen exemplify typical characteristic system dynamic lower row visualizes correlation action two agent term relationship probability choice action present data obtained ǫ γ critical value memory exponent equal γc given magnitude parameter memory exponent γ go away bility boundary γc inward instability region mode becomes dominant whereas mode b loos bility demonstrated fig 5 right column izing example transient process ity development initial stage classiﬁed mode b ﬁnal stage convert mode besides shown pattern rather plex structure enables u presume mode system instability least metastable found two mode b interpreted phenomenon selﬁsh istic behavior agent term used given model agent act dently one another indeed within mode two agent alternately play role defector taking mostly action beating action chosen agent clearly demonstrated fig 4 lower row panel oppositely within mode b agent cooperate taking mainly action behavior really altruistic reward gained defector exceeds reward gotten sharing action factor used system parameter truism seems due memory failed ﬁnd mode b 12 ihor lubashevsky shigeru kanemoto memory model multiagent reinforcement learning fig ternary phase portrait trajectory corresponding time pattern probability oscillation within mode subcritical supercritical region instability onset presented data obtained two agent system parameter ǫ fig ternary phase portrait 6000 dot individually trajectory ing time pattern visualizing typical feature instability development tree agent system presented data obtained ǫ two value γ γ memory exponent ihor lubashevsky shigeru kanemoto memory model multiagent reinforcement learning 13 simulating actually system however agent memory characterized single temporal scale sec possible mechanism responsible altruistic behavior far understood well review corresponding phenomenon recently proposed model found least altruism doe not readily evolve illustrated evolution eration prisoner dilemma game 82 given model altruistic behavior match agent choice one action longer longer time learning process note similar dynamic wa found eling cyclic variation system altruism conspicuous tag altruist inherited via diﬀerent mechanism 83 figure 6 depicts typical detail instability opment three agent system case action agent destroys mode system dynamic becomes irregular however seen ﬁgure near instability boundary γc ǫ mode b nevertheless survive emerge suﬃciently long transient process instability development repeated several time sult corresponding phase portrait form dot exhibit some attraction system dynamic towards origin visualized certain origin neighborhood dot accumulation outside narrow neighborhood instability boundary system dynamic becomes irregular corresponding phase portrait shown γ match rather uniform dot distribution phase space related time pattern theless demonstrates fact system time time return origin remains ity time interval determined instability crement result three agent system enable u claim proposed multiagent model reinforcement learning memory describes some anomalous mechanism forcing system return periodically origin reside neighborhood remarkable time spite origin unstable point mechanism could possible planation observed evolutionary stable cyclic tions trimorphic population altered regularly formation dimorphic monomorphic population see introduction 4 conclusion model multiagent reinforcement learning free memory ha constructed development wa partly stimulated attempt elucidate way describing memory eﬀects system living play crucial role responsible term phenomenon observed stock market aging etc model detail including agent transitive interaction type ﬂect characteristic feature trimorphic population tracted much attention last decade text speciation reinforcement learning model assumes agent accumulate reward gained taking some action get awareness value tion act independently one another interaction arises implicitly via reward one agent depending action agent probability ing action related gained awareness via exponential function boltzmann model reward cumulation described integral operator kernel mean ﬁeld approximation ﬁnal governing equation fractional time derivative structed memory pose question notion initial condition system certain approximation allowing ha found key point relate initial condition time moment agent start activity thereby only awareness inherited some way case erning fractional diﬀerential equation shown caputo type dynamic system comprising two three identical agent interaction analyzed detail first system stability studied alytically instability development tigated numerically particular ha found longer memory easily nash equilibrium loos stability agreement model induced dynamic uncertainty 16 roughly speaking agent weak memory not recognize presence agent treat ﬂuence random ﬂuctuations environment two agent system numerical analysis instability ha demonstrated existence two mode signiﬁcantly diﬀerent property one mode match limit cycle system phase space stable riodic oscillation probability agent choice certain mismatch agent behavior mode dergoes subcritical bifurcation dominant system parameter lie inside instability region far enough boundary mode undergoing supercritical bifurcation describes oscillation agent preference whose amplitude period grow uously time least within simpliﬁed model used numerical simulation oscillation could bilized limit capacity agent memory however worthy individual investigation latter mode match synchronized behavior two agent regarded altruism studied transient process enable u assume mode system instability however seem metastable phenomenon due memory least actually model however memory described gle scale only ﬁrst instability mode ha found undergoes supercritical bifurcation three agent system interaction agent stroys ﬁrst mode system dynamic becomes irregular however near instability threshold ond mode survive emerge rather long complex transient process initial condition 14 ihor lubashevsky shigeru kanemoto memory model multiagent reinforcement learning randomly generated numerical simulation dynamic three agent system exhibit anomalous attraction equilibrium point namely periodically return equilibrium point unstable resides vicinity remarkable time interval behavior resembles observed dynamic trimorphic tions evolutionary stable cycle variation alternated breakdown event giving rise dimorphic phic population obtained result enable u also presume describe observed complex phenomenon speciation relevant model deal not only proportion specie also some variable quantifying process specie individual learning adapting variation environment well population structure author appreciate support rfbr grant 00736 well research support sity aizu mean ﬁeld approximation let u consider generalized form update rule 5 qa x w pa x tn qa x tn qa x tn 57 weight coeﬃcient w pa x tn assumed ﬁrst certain smooth function choice ability pa x tn system update step tn termined collection expression 57 agent index action index x run possible value independently necker delta δx xa reﬂects choice action xa agent time moment tn according adopted assumption 6 tie qa x not change substantially scale der therefore consider composite step system update comprising k elementary step 1 write following equation qa x w pa x tn x xa δxxa ra qa x tn qa x tn 58 symbol cak stand set action cak 1 2 k xkn 59 xij action taken agent aj ementary step system update row table 59 represents set xa xa action responding time standpoint agent updated value qa x determined speciﬁc realization set cak strictly speaking random quantity however time capacity agent memory high enough choose number elementary step suﬃciently large any agent explore many option choice well choice agent case sum entering equation 58 may replaced sum running possible realization set xa xa weight speciﬁed probability p xa pa xa 60 noted exactly point adopted assumption mutual independence agent action ha taken account replacement x xa x xa xa p xa 61 convert equation 58 following qa x w pa x tn pa x tn x xa ra tn qa x tn qa x tn treating quantity qa x tn continuous tions time get dqa x dt w pa x pa x x xa ra qa x desired mean ﬁeld approximation date rule 57 keeping mind reasoning presented sec convert quantity qa x quantity qa x showing relative variation qa x qa x x qa 63 term equation read dqa x dt ra x qa x 64 ihor lubashevsky shigeru kanemoto memory model multiagent reinforcement learning 15 relative reward rate given expression ra x w pa x pa x x xa ra x w pa x pa x x xa ra 65 virtue relationship 11 expression 65 ative reward rate ra x autonomous function quantity x x qn x thereby lection equation 64 form closed autonomous model reinforcement learning consideration complete derivation equation 8 need specify w p w p let u analyze family atze w p p 66 nonnegative exponent ν allows sible overweighting event agent not interfere one another action payoﬀfunction ra ra x any agent doe not depend choice agent thereby relative reward rate ra x list argument tains only quantity qa x let u consider detail situation set possible action consists only two action x addition equivalent ra ra using lationship 11 system equation 64 given agent reduced equation dq dt r 2 sinh 1 βq cosh βq 67 containing solely variable q qa must value q 0 matching nash equilibrium stationary point equation 67 however limit large value parameter nash equilibrium lose stability ν namely case inequality 1 βrt 2 68 hold equation 67 admits three stationary point point q 0 unstable two new point matching stable dynamic given system vicinity figure 7 illustrates situation ν instability innate feature given algorithm learning process doe not lead hidden nash equilibrium q 0 not due some external perturbation induced example inﬂuence agent fact given agent exploring fig illustration instability mechanism ment learning governed equation 67 often option seem preferable faster accumulates rewords eliminate mechanism nash equilibrium instability analyzed model adopt ansatz 66 ν case equation 64 expression 65 immediately lead u governing equation 8 express 10 b equilibrium perturbation caused random ﬂuctuations reward rate system stability order analyze impact random ﬂuctuations reward rate system dynamic return integral equation 30 split term ra x two part ra x rr x rf x 69 former one previously autonomous function action value rr x speciﬁed expression 10 latter summand represents small random ﬂuctuations reward rate dealing small perturbation system dynamic may consider diﬀerent component pattern rf x well variation δqa x action value induced component separately therefore let u conﬁne case random ﬂuctuations rf x cated inside some internal interval consider time scale since no perturbation qa x occur time ment equation 30 δqa x read δqa x τ z eγ γ h δrr x eγ γ h z x 70 16 ihor lubashevsky shigeru kanemoto memory model multiagent reinforcement learning δrr x regular component reward rate linearized respect small variation qa x near equilibrium point qeq x known relationship cauchy type problem factional diﬀerential equation ra integral equation 64 enables u state asymptotics perturbation δqa x obeys lowing fractional diﬀerential equation b dγ x τ x 1 δqa x 71 virtue 70 side liouville fractional derivative order γ deﬁned expression b dγ x 1 γ 1 dt z γ qa x 72 solution δqa x equation 71 meet certain initial integral condition framework analysis replaced requirement lim x c 73 c some constant 64 eigenfunction fractional derivative meet requirement 73 posse eigenvalue λ function 64 eλz γ 1 eγ γ λ γ 74 asymptotic behavior eλt γ λ γ e 1 λ eλt γ 1 1 λ obtained result enable u state ing first virtue 42 75 instability onset caused perturbation initial condition dom ﬂuctuations reward rate match value λ corresponding fractional derivative longing region complex plane second equation 31 71 governing dynamic turbations induced mechanism actually form within replacement cb dγ dγ thereby instability condition 49 found sec scribe also instability onset induced random ﬂuctuations reward rate reference econophysics sociophysics trands adn perspective edited chakrabarti chakraborti terjee verlag gmbh rgaa heim 2006 helbing rev mod phys 73 1067 2001 mantegna stanley introduction physic correlation complexity finance bridge university press 2000 ed easterling unexpected return understanding ular stock market cycle cypress house fort bragg 2005 castellano fortunato loreto rev mod phys 81 591 2009 soniu ska de schutter prehensive survey reinforcement learning ieee transaction system men cybernetics part c application review 38 2008 garrahan moro sherrington phys rev e 62 2000 challet marsili zecchina phys rev let 84 1824 2000 marsili challet phys rev e 64 056138 2001 de martino eur phys b 35 143 2003 panait tuyls luke mech learn 9 423 2008 cavagna garrahan giardina ton phys rev let 83 4429 1999 borgers sarin econ th 77 1 1997 fudenberg levine theory learning game mit press 1998 sato crutchﬁeld phys rev e 67 015206 2003 sato akiyama crutchﬁeld physica 210 21 2005 galstyan continuous strategy replicator dynamic learning lubashevsky plawinska physic system motivation interdisciplinary branch science print lubashevsky plawinska mathematical malism physic system motivation garber primatol 19 203 1989 gibeault macdonald primate 41 147 2000 erhart overdorﬀ folium primatol 79 185 2008 johnson ecology 72 1408 1991 arquez hill barthell pham doty well kansa entomol soc 81 315 2008 koganezawa hara hayakawa shimada theor biol 260 353 2009 rhodes turvey physica 385 255 2007 squire neurobiology learning memory 82 171 2004 wang yamasaki havlin stanley phys rev e 73 026117 2006 yamasaki muchnik havlin bunde stanley proc natl acad sci 102 9424 2005 hauert phys 73 405 2005 ihor lubashevsky shigeru kanemoto memory model multiagent reinforcement learning 17 bus jackson nat 113 223 1979 paquin adam nature 306 368 1983 shuster wade nature 350 606 1991 shuster wade anim behav 41 1071 1991 kerr riley feldman hannan nature 418 171 2002 kirkup riley nature 428 412 2004 sinervo lively nature 380 240 1996 sinervo zamudio heredity 92 198 2001 lank smith hanotte burke cooke nature 378 411 1995 widemo anim behav 56 329 1998 pryke griﬃth proc soc b 273 949 2006 maynard smith evolution theory game cambridge cambridge univ 1982 sinervo calsbeek annu rev ecol evol syst 37 581 2006 greenwood harvey annu rev ecol syst 13 1 1982 ronce annu rev ecol syst 38 231 2007 doligez gustafsson art proc soc b 276 2829 2009 balda kamil anim behav 44 761 1992 gwinner proc natl acad sci 100 5863 2003 ladage riggs sinervo sudov anim behav 78 91 2009 corl davis kuchta sinervo proc natl acad sci 107 4254 2010 smith nature 195 60 1962 proc natl acad sci 83 1388 1986 sinervo svensson heredity 89 329 2002 gray mckinnon trend ecol evol 22 71 2007 rowland emlen science 323 773 2009 bush mosteller stochastic model ing new york wiley 1955 fu anderson exper psyc general 135 184 2006 hau pleskac kiefer hertwig behav making 21 493 2008 hertwig erev trend cognitive science 13 517 2009 tversky kahneman journal risk tainty 5 297 1992 dufty clobert møller trend ecology evolution 17 190 2002 uller trend ecology evolution 23 432 2008 badyaev uller phil trans soc b 364 1169 2009 kilbas srivastava trujillo theory application fractional diﬀerential equation sevier amsterdam 2006 podlubny fractional diﬀerential equation academic press san diego 1999 kok vlassis machine learning research 7 1789 2006 kirwan uscher finn collins porqueddu helgadottir shaug brophy coran ottir gado elgersma fothergill golinski grieu gustavsson oglind iliadis jørgensen kadziuliene karyotis lunnan malengier maltoni meyer nyfeler parente smit thumm connolly journal ecology 95 530 2007 kirwan connolly finn brophy uscher nyfeler ecology 90 2032 2009 perkins holmes weltzin journal vegetation science 18 685 2007 engel weltzin plant ecol 195 77 2008 sinervo genetica 417 2001 bleay sinervo behav ecol 18 304 2007 schuett tregenza dall biological view published online 18 2009 lancaster hipsley sinervo behav ecol 20 993 2009 collins mcnamara ramsey behav ecol 17 799 2006 fawcett bleay behav ecol 20 68 2009 deithelm ford ford weilbeer put appl math 186 482 2006 gaﬁychuk datsko appl math comput 198 251 2008 galeone garrappa comput appl math 228 548 2009 lehmann keller evolut biol 19 1365 2006 west gardner science 327 1341 2010 trivers rev biol 46 35 1971 jansen van baalen nature 440 663 2006