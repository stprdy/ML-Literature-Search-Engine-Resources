29 may 2016 jmlr workshop conference proceeding vol 2016 reinforcement learning pomdps using spectral method kamyar azizzadenesheli kazizzad university california irvine alessandro lazaric institut national de recherche en informatique et en automatique inria animashree university california irvine abstract propose new reinforcement learning algorithm partially observable markov decision ce pomdp based spectral decomposition method spectral method previously employed consistent learning passive latent variable model hidden markov model pomdps challenging since learner interacts environment possibly change future observation process devise learning algorithm running episode episode employ spectral technique learn pomdp parameter trajectory generated ﬁxed policy end episode optimization oracle return optimal memoryless planning policy maximizes expected reward based estimated pomdp model prove regret bound respect mal memoryless policy efﬁcient scaling respect dimensionality observation action space keywords spectral method method moment partially observable markov decision ce latent variable model upper conﬁdence reinforcement learning introduction reinforcement learning rl effective approach solve problem sequential making uncertainty rl agent learn maximize reward using ence obtained direct interaction stochastic environment bertsekas tsitsiklis 1996 sutton barto 1998 since environment initially unknown agent ha balance tween exploring environment estimate structure exploiting estimate compute policy maximizes reward result designing rl algorithm requires three different element 1 estimator environment structure 2 planning algorithm pute optimal policy estimated environment lavalle 2006 3 strategy make trade exploration exploitation minimize regret difference performance exact optimal policy reward accumulated agent time azizzadenesheli supported part nsf career award onr award lazaric supported part grant cper de data advanced data science technology cristal centre de recherche en informatique et automatique de lille french national research agency anr project anandkumar supported part microsoft faculty fellowship nsf career award onr award aro yip award afosr yip c azizzadenesheli lazaric anandkumar azizzadenesheli lazaric anandkumar rl literature assumes environment modeled markov decision ce mdp markovian state evolution fully observed number exploitation strategy shown strong performance guarantee mdps either term regret sample complexity see sect review however assumption full observability state evolution often violated practice agent may only noisy observation true state environment noisy sensor robotics case appropriate use mdp pomdp sondik 1971 model many challenge arise designing rl algorithm pomdps unlike mdps tion problem element 1 involves identifying parameter latent variable model lvm mdp agent directly observes stochastic state transition estimation generative model straightforward via empirical estimator hand pomdp transition reward model must inferred noisy observation markovian state evolution hidden planning problem element 2 computing optimal policy pomdp known parameter papadimitriou tsitsiklis 1987 requires ing augmented mdp built continuous belief space distribution hidden state pomdp finally integrating estimation planning egy element 3 guarantee no strategy currently known see sect summary result main contribution paper follows propose new rl algorithm pomdps incorporates spectral parameter estimation within work ii analyze regret bound assuming access optimization oracle provides best memoryless planning policy end learning episode iii prove order optimal regret efﬁcient scaling dimension thereby providing ﬁrst guaranteed rl algorithm wide class pomdps estimation pomdp carried via spectral method involve tion certain moment tensor computed data learning algorithm interleaved optimization planning policy using strategy inspired ucrl method mdps ortner auer 2007 jaksch et 2010 resulting algorithm called spectral method reinforcement learning run episode variable length agent follows ﬁxed policy enough data collected update current policy according estimate pomdp parameter accuracy throughout paper focus estimation aspect algorithm assume access planning oracle class memoryless policy policy directly mapping observation distribution action theoretical result prove following learning result full detail see thm 3 sect theorem informal result learning pomdp parameter let pomdp x state observation action r reward x characterized density ft fo fr deﬁning state transition observation reward model given assumption common many work bandit rl literature see ari 2011 linear bandit chen et al 2013 combinatorial bandit focus exploitation strategy rather optimization problem 2 reinforcement learning pomdps using spectral method sequence observation action reward generated executing memoryless policy action chosen n time exists spectral method return estimate b ft b fo b fr suitable assumption pomdp policy number sample satisfy fo r n fr r n ft n high probability any state x any action result show consistency estimated pomdp parameter also provides explicit conﬁdence interval employing learning result ucrl framework prove following bound regret regn optimal memoryless policy full detail see thm 4 sect theorem informal result regret bound let pomdp x state tions action r reward diameter deﬁned max x min π e τ π largest mean passage time any two pair pomdp using memoryless policy π mapping observation action run n step using conﬁdence interval thm 3 suitable assumption pomdp space policy number sample regn ay rn high probability result show despite complexity estimating pomdp parameter noisy observation hidden state regret similar case mdps regret ucrl scale e dmdpx regret since e n match lower bound mdps another interesting aspect diameter pomdp natural extension mdp case dmdp measure mean passage time using policy policy ping state action pomdps policy not deﬁned state rather observation naturally translates deﬁnition diameter detail dependent term bound discussed sect derived regret bound respect best memoryless stochastic policy given pomdp indeed general pomdp optimal policy need not memoryless however ﬁnding optimal policy uncomputable inﬁnite horizon regret minimization madani 1998 instead memoryless policy shown good performance practice see section related work moreover class contextual mdp special class pomdps optimal policy also memoryless krishnamurthy et 2016 3 azizzadenesheli lazaric anandkumar analysis learning algorithm learning result thm 3 based spectral tensor decomposition method previously used consistent estimation wide class lvms anandkumar et 2014 contrast traditional learning method em dempster et 1977 no consistency guarantee may converge local optimum arbitrarily bad spectral method previously employed sequence modeling hmms anandkumar et 2014 representing multiview model application pomdps not trivial fact unlike hmm consecutive observation pomdp no longer conditionally independent conditioned hidden state middle view decision action depends observation limiting memoryless policy control range dependence conditioning action show obtain conditionally independent view result starting sample collected along trajectory generated ﬁxed policy construct model use tensor decomposition method action separately estimate parameter pomdp deﬁne conﬁdence interval proof follows similar step previous work spectral method hmms anandkumar et 2014 extend concentration inequality dependent random ables matrix valued function combining result kontorovich et al 2008 matrix azuma inequality tropp 2012 allows u remove usual assumption sample generated stationary distribution current policy particularly portant case since policy change episode avoid discarding initial sample waiting corresponding markov chain converged phase condition pomdp ha observation state x follows dard condition apply spectral method corresponds considering pomdps underlying mdp deﬁned number state space produce large number noisy observation common application system atrash pineau 2006 png et 2012 medical tions hauskrecht fraser 2000 also show assumption relaxed result applied wider family pomdps analysis strategy applies popular conﬁdence interval estimated pomdp compute optimal policy optimistic pomdp admissible set optimistic choice provides smooth combination exploration encouraged conﬁdence interval larger conﬁdence interval favor uniform exploration exploitation estimate pomdp parameter algorithmic integration rather simple analysis not trivial spectral method not use sample generated different policy length episode fully tuned guarantee estimator improve episode furthermore analysis requires redeﬁning notion diameter pomdp addition carefully bound various turbation term order obtain efﬁcient scaling term dimensionality factor finally appendix f report preliminary synthetic experiment demonstrate periority method existing rl method ucrl mdps principle ha successfully used wide number problem ranging bandit auer et 2002 linear contextual bandit et 2011 linear quadratic trol ari 2011 reinforcement learning ortner auer 2007 jaksch et 2010 4 reinforcement learning pomdps using spectral method also purely exploratory method random sampling randomly chooses action independent observation converges much faster better solution lutions relying mdp assumption directly work high dimensional observation space perform poorly fact even worse random sampling policy baseline contrast method aim ﬁnd lower dimensional latent space derive policy allows ucrl ﬁnd much better memoryless policy vanishing regret worth noting general slight change learning set one come new algorithm learn different pomdp model slightly upper conﬁdence bound moreover applying memoryless policy collecting sufﬁcient number sample model parameter learned well one planing belief space get memory dependent policy therefore improve performance even related work last decade mdp ha widely studied kearns singh 2002 brafman tennenholtz 2003 bartlett tewari 2009 jaksch et 2010 different setting even large state space mdp classical approach not scalable kocsis ari 2006 duce mdp planning tree one viable approach ﬁnd optimal policy addition special class mdps markov jump afﬁne model action space continuous baltaoglu et 2016 proposes order optimal learning policy rl mdps ha widely studied design effective strategy pomdps still relatively unexplored ross et al 2007 poupart vlassis 2008 propose integrate problem estimating belief state bayesian rl approach distribution possible mdps updated time proposed rithms bayesian inference done accurately step pomdp sampled posterior corresponding optimal policy executed resulting method implicitly balance exploration exploitation no theoretical guarantee provided regret algorithmic complexity requires introduction approximation scheme inference planning step alternative approach adapt algorithm case pomdps perkins 2002 proposes approach estimation show convergence locally optimal oryless policy algorithm ha advantage computationally efﬁcient local optimal policy may arbitrarily suboptimal thus suffer linear regret alternative approach solve pomdps use policy search method avoid mating value function directly optimize performance searching given policy space usually contains memoryless policy see ng jordan 2000 baxter bartlett 2001 poupart boutilier 2003 bagnell et 2004 beside practical success ofﬂine problem policy search ha successfully integrated efﬁcient technique shown achieve small regret et 2013 2014 nonetheless performance method severely constrained choice policy space may not contain policy good performance another approach solve pomdps proposed guo et 2016 work agent randomly chooses action independent tions reward agent executes random policy collect sufﬁcient number sample estimate model parameter given collected information author propose probably approximately correct pac framework rl pomdp setting show polynomial sample 5 azizzadenesheli lazaric anandkumar complexity learning model parameter learning phase deﬁnes induced hidden markov model applies random policy capture different aspect model planing phase given estimated model parameter compute optimum policy far word proposed algorithm explores environment sufﬁciently enough exploit exploration come optimal policy given estimated model contrast method considers rl pomdps episodic learning framework matrix decomposition method previously used general setting dictive state representation psrs boot et 2011 reconstruct structure cal system despite generality psrs proposed model relies strong assumption dynamic system doe not any theoretical guarantee performance gheshlaghi azar et al 2013 used spectral tensor decomposition method bandit framework identify hidden generative model sequence bandit problem showed may drastically reduce regret recently hamilton et 2014 introduced compressed psr cpsr method reduce computation cost psr exploiting advantage sionality reduction incremental matrix decomposition compressed sensing work take idea considering powerful tensor decomposition technique krishnamurthy et al 2016 recently analyzed problem learning proved sample complexity bound polynomial capacity policy space number state horizon objective minimize regret ﬁnite horizon instead consider inﬁnite horizon problem open question analyze modify spectral ucrl algorithm ﬁnite horizon problem stated earlier contextual mdps special class pomdps memoryless policy optimal assume sample drawn contextual mdp handle much general class pomdps minimize regret respect best memoryless policy given pomdp finally related problem considered ortner et al 2014 series possible resentations based observation history available agent only one actually markov strategy adopted shown achieve regret paper focus learning problem consider access optimization oracle compute optimal memoryless policy problem planning general pomdps intractable ﬁnite horizon papadimitriou tsitsiklis 1987 putable inﬁnite horizon madani 1998 many exact approximate heuristic method proposed compute optimal policy see spaan 2012 recent survey alternative approach consider memoryless policy directly map observation ﬁnite history action littman 1994 singh et 1994 li et 2011 deterministic policy may perform poorly stochastic memoryless policy shown many domain barto et 1983 loch singh 1998 williams singh 1998 even optimal speciﬁc case contextual mdps krishnamurthy et 2016 although computing optimal stochastic memoryless policy still littman 1994 several method shown converge cies polynomial complexity some condition pomdp jaakkola et 1995 li et 2011 work employ memoryless policy prove regret bound forcement learning pomdps work suggest focusing memoryless policy may not restrictive limitation practice 6 reinforcement learning pomdps using spectral method xt yt rt figure 1 graphical model pomdp memoryless policy paper organization paper organized follows sect 2 introduces notation summarized also table sect 6 technical assumption concerning pomdp space memoryless policy consider sect 3 introduces spectral method estimation pomdp parameter together thm sect 4 outline integrate spectral method strategy prove regret bound thm sect 5 draw conclusion discus possible direction future investigation proof reported appendix together preliminary empirical result showing effectiveness proposed method preliminary pomdp tuple r ft fr x ﬁnite state space cardinality x ﬁnite action space cardinality ﬁnite observation space cardinality r ﬁnite reward space cardinality r largest reward rmax notation convenience use vector notation element r r indicator vector entry equal 0 except 1 position corresponding speciﬁc element set en refers element use j x index state k l action r reward n observation finally ft denotes transition density ft probability transition given pair x fr reward density fr probability receiving reward r corresponding value indicator vector r given pair x fo observation density fo probability receiving observation corresponding indicator vector given state whenever convenient use tensor form density function ti j l p l ft l p fo γi l p r l fr l γ also denote l ﬁber vector rx obtained ﬁxing arrival state j action l transition matrix state using action graphical model associated pomdp illustrated fig 1 7 azizzadenesheli lazaric anandkumar focus stochastic memoryless policy map observation action any icy π denote fπ density function denote p set stochastic memoryless policy probability explore action p π min min fπ πmin acting according policy π pomdp deﬁnes markov chain characterized transition density ft π x x fπ fo ft stationary distribution ωπ state ωπ x p ft π ωπ expected average reward performance policy π η π x x ωπ x rπ x rπ x expected reward executing policy π state x deﬁned rπ x x x fo fπ r x r x p r rfr expected reward pair x best stochastic memoryless policy p arg max η π denote η average throughout paper assume access optimization oracle returning optimal policy p any pomdp need following assumption pomdp assumption 1 ergodicity any policy π corresponding markov chain ft π godic ωπ x 0 state x characterize markov chain generated policy any ergodic markov chain stationary distribution ωπ let distribution state reached policy π step starting initial state inverse mixing time ρmix π chain deﬁned ρmix π sup metric kontorovich et al 2014 show any ergodic markov chain mixing time bounded ρmix π π π 1 π geometric ergodicity 0 π 1 contraction coefﬁcient markov chain generated policy π use rather recall fact restrict attention p actual optimal policy pomdp general constructed 8 reinforcement learning pomdps using spectral method assumption 2 full observation matrix full column rank deﬁne assumption guarantee distribution fo state x column matrix not result linear combination distribution state show later sufﬁcient condition recover fo since make state distinguishable observation also implies notice pomdps often used opposite scenario x application robotics imprecise sensor prevents distinguishing different state hand many domain number observation may much larger set state deﬁne dynamic system typical example case spoken dialogue system atrash pineau 2006 png et 2012 observation sequence word uttered user much larger state conversation actual meaning user intended communicate similar scenario found medical application hauskrecht fraser 2000 state patient sick healthy produce huge body different random observation problem crucial able reconstruct underlying small state space actual dynamic system observation assumption 3 invertible any action transition matrix ible similar previous assumption mean any action distribution ft not obtained linear combination distribution state sufﬁcient condition able recover transition tensor asm 2 3 strictly related assumption introduced anandkumar et al 2014 tensor method hmms sect 4 discus partially relaxed learning parameter pomdp section introduce novel spectral method estimate pomdp parameter ft fo fr stochastic policy π used generate trajectory yn rn n step need following assumption together asm 1 guarantee state action constantly visited assumption 4 policy set policy π belongs similar case hmms key element apply spectral method construct model hidden state despite similarity spectral method developed hmm anandkumar et al 2014 not directly employed fact hmms state transition observation only depend current state hand pomdps probability transition state not only depends x also action since action chosen according memoryless policy π based current observation creates indirect dependency observation make model intricate 9 azizzadenesheli lazaric anandkumar model estimate pomdp parameter action l separately let 2 n step l construct three view yt rt contain observable element seen fig 1 three view provide some information hidden state xt observation trigger action inﬂuence transition xt careful analysis graph dependency show conditionally xt view independent instance let u consider yt two random variable clearly dependent since yt inﬂuences action trigger transition emits observation nonetheless sufﬁcient condition action l break dependency make yt independent similar argument hold element view used recover latent variable xt formally encode triple vector v l 1 view v l 1 e whenever k en em suitable mapping index 1 r index k n action observation reward similarly proceed v l 2 v l 3 introduce three view matrix v l ν ν 1 2 3 associated action l deﬁned v l 1 v l 2 v l 3 v l 1 p l 1 v l 1 n k p en em v l 2 p l 2 l v l 2 p l v l 3 p l 3 l v l 3 p l following denote µ l ν v l ν ith column matrix v l ν any ν 1 2 3 notice asm 2 asm 3 imply view matrix full column rank result construct model relates spectral decomposition second third moment modiﬁed view column third view matrix proposition 1 thm anandkumar et 2014 let k l ν e v l ν l relation matrix view ν deﬁne modiﬁed version ﬁrst second view e v l 1 k l k l l 1 e v l 2 k l k l l 2 1 second third moment modiﬁed view spectral decomposition l 2 e v l 1 v l 2 x x ω l π µ l 3 l 3 2 l 3 e e v l 1 v l 2 l 3 x x ω l π µ l 3 l 3 l 3 3 tensor product ω l π p x l state stationary distribution π conditioned action l selected policy π 10 reinforcement learning pomdps using spectral method notice asm 1 4 ω l π always bounded away zero given l 2 l 3 recover column third view µ l 3 directly applying standard spectral decomposition method anandkumar et al 2012 need recover view v l 3 deﬁnition modiﬁed view eq 1 µ l 3 e l k l k l l k l k l l 1 µ l 3 e l k l k l l k l k l l 2 4 thus sufﬁcient invert pseudo invert two equation obtain column ﬁrst second view matrix process could done any order could ﬁrst estimate second view applying suitable symmetrization step eq 1 recovering ﬁrst third view reversing similar equation eq hand not repeat symmetrization step multiple time estimate view independently without inverting eq 4 fact estimate returned spectral method consistent suitable permutation index state doe not pose any problem computing one single view estimated two view independently permutation may different thus making impossible use recovering pomdp parameter hand estimating ﬁrst one view recovering others inverting eq 4 guarantee consistency labeling hidden state recovery pomdp parameter view v l ν 3 computed l 2 l 3 derive ft fo fr particular parameter pomdp obtained manipulating second third view illustrated following lemma lemma 2 given view v l 2 v l 3 any state x action l pomdp parameter obtained follows any reward r reward density fr l x v l 2 5 any observation observation density f l r x v l 2 fπ ρ l 6 ρ l r x x v l 2 fπ 1 p finally second mode transition tensor obtained l v l 3 7 matrix observation ft l 11 azizzadenesheli lazaric anandkumar algorithm 1 estimation pomdp parameter routine tensordecomposition refers spectral tensor decomposition method anandkumar et al 2012 input policy density fπ number state x trajectory yn rn variable estimated second third view b v l 2 b v l 3 any action l estimated observation reward transition model b fo b fr b ft l 1 set l n l n l l construct view v l 1 v l 2 yt rt v l 3 any l compute covariance matrix b k l b k l b k l b k l ν 1 n l x l v l ν l ν 1 2 3 compute modiﬁed view e v l 1 b k l b k l e v l 2 b k l b k l l 2 any l compute second third moment c l 2 1 n l x e v l 1 v l 2 c l 3 1 n l x e v l 1 v l 2 l 3 compute b v l 3 tensordecomposition c l 2 c l 3 compute b µ l 2 b k l b k l µ l 3 any x compute b f l py b v l 2 any x r compute ρ l pr py v l 2 fπ any x n compute b f l pr v l 2 n fπ ρ l any x n end compute bound b l set arg minl b l b fo b f construct matrix b n j b fo reorder column matrix b v l 2 b v l 3 matrix l match 4 x l compute l b b v l 3 end return b fr b ft b fo br bt bo previous statement use f l denote observation model recovered view related action exact case f l identical moving empirical version lead different estimate one action view used compute among select estimate better accuracy column l corresponds column 12 reinforcement learning pomdps using spectral method empirical estimate pomdp parameter practice l 2 l 3 not available need estimated sample given trajectory n step obtained executing policy π let l 2 n l set step action l played collect triple yt rt any l construct corresponding view v l 1 v l 2 v l 3 symmetrize view using empirical estimate covariance matrix build empirical version eq 2 3 using n l l sample thus obtaining c l 2 1 n l x e v l 1 v l 2 c l 3 1 n l x e v l 1 v l 2 l 3 8 given resulting c l 2 c l 3 apply spectral tensor decomposition method recover empirical estimate third view b v l 3 invert eq 4 using estimated covariance matrix obtain b v l 2 finally estimate b fo b ft b fr obtained plugging estimated view b vν process described lemma spectral method indeed recover factor matrix permutation hidden state case since separately carry spectral decomposition different action recover permuted factor matrix since observation matrix common action use align decomposition let deﬁne min x actually minimum separability level matrix estimation error column matrix le one come permutation issue matching column ol matrix condition reﬂected condition number sample action ha larger some number overall method summarized alg empirical estimate pomdp parameter enjoy following guarantee theorem 3 learning parameter let b fo b ft b fr estimated pomdp model using trajectory n step denote σ l ν σx k l ν smallest singular value covariance matrix kν ν 1 2 3 σmin v l ν smallest singular value view matrix v l ν strictly positive asm 2 asm 3 deﬁne ω l min ω l π x strictly positive asm 1 any action l number sample n l satisﬁes condition n l 4 σ l 2 oy r λ l g π 2 π ω l min min min v l ν 2 θ l log 2 ay r δ 9 θ l deﬁned eq 275 g π θ π geometric ergodicity contraction ﬁcients corresponding markov chain induced π any δ 0 1 any state not report explicit deﬁnition θ l contains exactly quantity ω l min already present part condition eq 9 13 azizzadenesheli lazaric anandkumar x action l f l l co λ l r log n l 10 fr l l l r cr λ l r log n l 11 ft l l l ct λ l log n l 12 probability 1 2 ay r aδ randomness transition observation policy co cr ct numerical constant λ l σmin π l min l ω l min min min v l ν 13 finally denote b fo accurate estimate observation model estimate b f arg b l denote bo corresponding bound remark 1 consistency dimensionality previous error decrease rate e p n l showing consistency spectral method action repeatedly tried time estimate converge true parameter pomdp contrast method typically get stuck local maximum return biased estimator thus preventing deriving conﬁdence interval bound eq 10 11 12 b fo b fr b ft depend x r number action only appear probability statement bound eq 12 b ft worse bound b fr b fo eq 10 11 factor seems unavoidable since b fr b fo result manipulation matrix v l 2 r column estimating b ft requires working v l 2 v l 3 addition come upper bound b ft complicated bound derivation needed ha one step frobenious norm norm transformation derivation procedure b ft complicated compared b fo b fr add term x ﬁnal bound appendix c remark 2 pomdp parameter policy π previous bound several term depend structure pomdp policy π used collect sample λ l capture main term full matrix asm 2 3 smallest singular value inﬂuence accuracy inversion construction modiﬁed view eq 1 tation second view third using eq similarly presence σmin justiﬁed used recover transition tensor eq finally dependency smallest singular value min v l ν due tensor decomposition method see app j detail 14 reinforcement learning pomdps using spectral method speciﬁc feature bound not depend state number time ha explored indeed inverse dependency ω l min condition n l eq 9 implies state j poorly visited empirical estimate any state may negatively affected striking contrast fully observable case accuracy estimating reward model state action l simply depends number time pair ha explored even some state never explored difference intrinsic partial observable nature pomdp reconstruct information state reward transition observation model only indirect observation result order accurate estimate pomdp structure need rely policy π ergodicity corresponding markov chain guarantee whole state space covered asm 1 markov chain ft π ergodic any π since no assumption made fact sample generated π sampled stationary distribution condition n l depends fast chain converge ωπ characterized parameter g π θ π policy deterministic some action would not explored thus leading inaccurate estimation see dependency fπ eq 6 inverse dependency πmin deﬁned p account amount exploration assigned every action determines accuracy estimate furthermore notice also singular value σ l σ l depend distribution view turn partially determined policy notice ﬁrst two term basically bound spectral method applied hmm song et 2013 dependency πmin speciﬁc pomdp case hand analysis hmms usually no dependency parameter g θ sample assumed drawn stationary distribution chain removing assumption required developing novel result tensor decomposition process using extension matrix concentration inequality case markov chain not yet stationary distribution overall analysis reported app worth note kontorovich et al 2013 without stationary assumption proposes new method learn transition matrix hmm model given factor matrix provides theoretical bound estimation error spectral ucrl interesting aspect estimation process illustrated previous section applied sample collected using any policy π set result integrated any strategy policy change time attempt minimizing regret algorithm algorithm illustrated alg 2 result integration spectral method structure similar ucrl jaksch et 2010 designed optimize learning process split episode increasing length beginning episode k 1 ﬁrst episode used initialize variable estimated pomdp c k x r b f k b f k r b f k computed using spectral method 15 azizzadenesheli lazaric anandkumar algorithm 2 algorithm input conﬁdence variable number sample n k l estimated observation reward transition model b f k b f k r b f k initialize 1 initial state δ 6 k 1 n compute estimated pomdp c k alg 1 using n k l sample per action compute set admissible pomdps k using bound thm 3 compute optimistic policy e π k arg max max k η π set v k l 0 action l v k l k l execute π k obtain reward rt observe next observation set 1 end store n l v l sample action l set k k 1 end alg unlike ucrl not use sample past episode fact distribution view depends policy used generate sample result whenever policy change spectral method using only sample collected speciﬁc policy nonetheless exploit fact spectral method applied action separately episode k action l use sample coming past episode returned largest number sample action let v k l number sample obtained episode k action l denote n k l k v l largest number sample available past episode action separately feed spectral method compute estimated pomdp c k beginning episode given estimated pomdp c k result thm 3 construct set k admissible pomdps f r e ft e fr e transition reward observation model belong conﬁdence interval f k fo any state construction guarantee true pomdp included k high probability following optimism face uncertainty principle used ucrl compute optimal memoryless policy corresponding optimistic pomdp within k formally e π k arg max max k η π 14 intuitively speaking optimistic policy implicitly balance exploration exploitation large conﬁdence interval suggest c k poorly estimated exploration needed instead computation optimal policy within p optimistic model may not trivial nonetheless ﬁrst notice given horizon n policy need recomputed log n time number episode furthermore optimization oracle η π given pomdp available sufﬁcient domly sample multiple pomdps k computationally cheap operation ﬁnd corresponding best policy return best among enough pomdps sampled additional regret caused approximately optimistic procedure bounded e n 16 reinforcement learning pomdps using spectral method performing purely explorative policy still exploit current estimate construct set admissible pomdps selects policy maximizes performance η π pomdps k choice using optimistic pomdp guarantee e π k explores often action corresponding large conﬁdence interval thus contributing improve estimate time computing optimistic policy e π k executed number sample one action doubled v k l k l stopping criterion avoids switching policy often guarantee episode terminated enough sample collected compute new better policy process repeated episode expect optimistic policy get progressively closer best policy estimate pomdp get accurate regret analysis study regret best policy general may not optimal πmin usually set small value oftentimes optimal memoryless policy stochastic may actually contained given horizon n step regret deﬁned regn n x rt 15 rt random reward obtained time according reward model fr state traversed policy performed episode actual pomdp restate similar mdp case complexity learning pomdp partially determined diameter deﬁned max x min e τ π 16 corresponds expected passing time state x state starting action terminating action following effective memoryless policy π main difference diameter underlying mdp see jaksch et al 2010 considers distance pair using memoryless policy instead policy stating main result introduce version parameter acterizing thm let min min ω l min min min v l ν worst smallest zero singular value view action l acting according policy π let min min σmin k l π worst smallest singular value covariance matrix k l π ﬁrst third view action l acting according policy similarly deﬁne also introduce ωmin min min x min ω l π x n max max max 4 oy r λ l π 2 π 2 θ l log 2 2 ay r δ 17 sufﬁcient number sample statement thm 3 hold any action any policy θ l also model related parameter deﬁned eq prove following result 17 azizzadenesheli lazaric anandkumar theorem 4 regret bound consider pomdp x state action observation r reward characterized diameter observation matrix smallest singular value σx consider policy space p worst smallest value resp worst smallest probability reach state ωmin run n step conﬁdence interval thm 3 used δ 6 constructing plausible pomdps f asm 1 2 3 suffers total regret regn rmax λ ay rn log 18 probability 1 numerical constant λ equivalent eq 13 deﬁned λ σmin 19 remark 1 comparison mdps ucrl could run directly underlying mdp state directly observable would obtain regret jaksch et 2010 regn p log n dmdp max x min π e τ π high probability ﬁrst notice regret order e n mdp pomdp bound mean despite complexity pomdps ha dependency number step mdps ha vanishing regret furthermore pendency known minimax optimal diameter general larger mdp counterpart dmdp since take account fact memoryless policy only work observation not efﬁcient policy moving one state another although no lower bound available learning pomdps believe dependency unavoidable since strictly related partial observable nature pomdps remark 2 dependency pomdp parameter dependency number action mdps pomdps hand moving pomdps naturally brings dimensionality observation reward model x r respectively bound dependency r directly inherited bound thm term indeed result two term x ﬁrst term mdps second come fact transition tensor derived eq finally term λ eq 18 summarizes series term depend policy space p pomdp structure term directly inherited spectral decomposition method used core discussed sect 3 due partial observability state fact unobservable state need visited often enough able compute accurate estimate observation reward transition model 18 reinforcement learning pomdps using spectral method remark 3 computability conﬁdence interval common assumption dimensionality x hidden state space known well number action observation reward not often case term λ l appearing thm 3 actually available doe not pose any problem descriptive bound thm 3 ally need compute bound b l b l r b l explicitly construct conﬁdence interval situation relatively common many algorithm require computing conﬁdence interval containing range random variable parameter butions case variable practice value often replaced parameter tuned hand set much smaller value theoretical one result run term λ l replaced ﬁxed parameter notice any inaccurate choice setting λ l would mostly translate bigger multiplicative constant ﬁnal regret bound similar bound smaller probability general computing conﬁdence bound hard problem even simpler case markov chain hsu et al 2015 therefore ﬁnding upper conﬁdence bound pomdp challenging not know mixing property mentioned another parameter needed compute per conﬁdence bound λ l described practice one replace coefﬁcient λ l some constant cause bigger multiplicative constant ﬁnal regret bound alternatively one estimate λ l data case add lower order term regret decay 1 n remark 4 relaxation assumption thm 3 4 rely observation matrix ry full column rank asm 2 discussed sect 2 may not veriﬁed some pomdps number state larger number observation x le possible correctly estimate pomdp parameter not full exploiting additional information coming reward action taken step particular use triple redeﬁne third view v l 3 v l 3 p v l 3 l v l 3 n k p en em l replace asm 2 assumption view matrix v l 3 full sically requires reward jointly observation informative enough struct hidden state change doe not affect way observation reward model recovered lemma 2 only depend second view v l 2 tion transition tensor need write third view v l 3 v l 3 v l 3 n k x x p en em l j p l x x p j k p en p j p l fπ x x fr k fo ft l 19 azizzadenesheli lazaric anandkumar factorized three component deﬁnition v l 3 used graphical model pomdp consider dependency introduce auxiliary matrix w w j w n k j fπ fr k fo contain known value any state action l restate deﬁnition third view w l v l 3 20 allows computing transition model l w v l 3 w inverse change deﬁnition third view allows signiﬁcant relaxation original assumption come cost potentially worsening bound b ft thm fact shown ft l l max ct ay r λ xa log n 21 beside dependency multiplication r r due fact v l 3 larger matrix bound transition triggered action l scale number sample least visited action due fact matrix w involves not only action computing transition model action well result any action poorly visited w not accurately estimated some part may negatively affect quality estimation transition model directly propagates regret analysis since require action repeatedly visited enough immediate effect introduction different notion diameter let τ l π mean passage time two step action l chosen according policy π deﬁne dratio max τ l π τ l π 22 diameter ratio deﬁnes ratio maximum mean passing time ing action choosing minimum mentioned order accurate estimate ft action need repeatedly explored dratio small action executed frequently enough large least one action executed not many others finally obtain regn λ p rdration log 1 ﬁrst sight bound clearly worse case stronger assumption notice λ contains smallest singular value newly deﬁned view particular v l 3 larger also covariance matrix kν bigger larger singular value could signiﬁcantly alleviate inverse dependency result relaxing asm 2 may not necessarily worsen ﬁnal bound since bigger diameter may compensated better dependency term leave complete comparison two conﬁgurations without asm 2 future work 20 reinforcement learning pomdps using spectral method conclusion introduced novel rl algorithm pomdps relies spectral method tently identify parameter pomdp optimistic approach solution problem resulting algorithm derive conﬁdence interval parameter minimax optimal bound regret work open several interesting direction future development 1 not accumulate sample episode since thm 3 requires sample drawn ﬁxed policy doe not negative impact regret bound open question apply spectral method sample together still preserve theoretical guarantee 2 memoryless policy may perform well some domain important extend current approach policy 3 pomdp special case predictive state representation psr model littman et al 2001 allows representing sophisticated dynamical system given spectral method developed paper natural extension apply general psr model integrate algorithm achieve bounded regret 21 azizzadenesheli lazaric anandkumar table notation pomdp notation sect 2 e indicator vector pomdp model x x x j state space cardinality element index n observation space cardinality indicator element index l k action space cardinality element index r r r r rmax reward space cardinality element indicator element index largest value ft transition density state x state given action transition tensor fo observation density indicator given state x observation matrix fr γ reward density indicator r given pair reward tensor π fπ π policy policy density action given observation indicator policy matrix πmin p smallest element policy matrix set stochastic memoryless policy fπ markov chain transition density policy π pomdp transition density ft ωπ ω l π stationary distribution state given policy π conditional action l η π expected average reward policy π pomdp best expected average reward policy p pomdp estimation notation sect 3 ν 1 2 3 index view v l ν v l ν νth view view matrix time given l k l ν σ l ν covariance matrix view ν smallest singular value given action l l 2 l 3 second third order moment view given middle action l b f l b f l r b f l estimate observation reward transition density action l n n l total number sample number sample action l co cr ct numerical constant bo br bt upper conﬁdence bound error estimated fo fr ft sect 4 regn cumulative regret pomdp diameter k index episode b f k b f k r b f k c k estimated parameter pomdp episode k k set plausible pomdps episode k v k l number sample action l episode k n k l maximum number sample action l episode k e π k optimistic policy executed episode k n min number sample meet condition thm 3 any policy any action σν worst smallest singular value covariance k l ν any policy action ωmin smallest stationary probability action state policy 22 reinforcement learning pomdps using spectral method reference yasin csaba ari regret bound adaptive control linear quadratic system colt page yasin avid al csaba ari improved algorithm linear stochastic bandit advance neural information processing system 24 nip page animashree anandkumar daniel hsu sham kakade method moment mixture model hidden markov model arxiv preprint animashree anandkumar rong ge daniel hsu sham kakade matus telgarsky tensor decomposition learning latent variable model journal machine learning research 15 1 atrash pineau efﬁcient planning tracking pomdps large observation space aaai workshop statistical empirical approach spoken dialogue system peter auer nicol paul fischer analysis multiarmed bandit problem machine learning 47 peter auer thomas jaksch ronald ortner regret bound reinforcement learning advance neural information processing system page bagnell sham kakade jeff schneider andrew ng policy search dynamic programming thrun saul olkopf editor advance neural information processing system 16 page mit press sevi baltaoglu lang tong qing zhao online learning optimization markov jump afﬁne model arxiv preprint peter bartlett ambuj tewari regal regularization based algorithm reinforcement learning weakly communicating mdps proceeding annual conference uncertainty artiﬁcial intelligence barto sutton anderson neuronlike adaptive element solve difﬁcult learning control problem system man cybernetics ieee transaction 5 sept issn doi jonathan baxter peter bartlett estimation artif int 15 1 november issn bertsekas tsitsiklis programming athena scientiﬁc byron boot sajid siddiqi geoffrey j gordon closing loop predictive state representation international journal robotics research 30 7 ronen brafman moshe tennenholtz general polynomial time algorithm optimal reinforcement learning journal machine learning research 2003 23 azizzadenesheli lazaric anandkumar wei chen yajun wang yang yuan combinatorial bandit general framework application sanjoy dasgupta david mcallester editor proceeding international conference machine learning volume 28 page jmlr workshop conference proceeding arthur p dempster nan laird donald b rubin maximum likelihood incomplete data via em algorithm journal royal statistical society series b methodological page lazaric brunskill regret bound reinforcement learning policy advice proceeding european conference machine learning ecml 13 lazaric brunskill stochastic optimization locally smooth function correlated bandit feedback proceeding international conference machine learning icml 14 mohammad gheshlaghi azar alessandro lazaric emma brunskill sequential transfer bandit ﬁnite set model burges bottou welling mani weinberger editor advance neural information processing system 26 page curran associate zhaohan daniel guo shayan doroudi emma brunskill pac rl algorithm episodic pomdps proceeding international conference artiﬁcial intelligence tic page william hamilton mahdi milani fard joelle pineau efﬁcient learning planning compressed predictive state journal machine learning research 15 1 milo hauskrecht hamish fraser planning treatment ischemic heart disease partially observable markov decision process artiﬁcial intelligence medicine 18 3 244 issn daniel j hsu aryeh kontorovich csaba ari mixing time estimation reversible markov chain single sample path advance neural information processing system page tommi jaakkola satinder singh michael jordan reinforcement learning algorithm partially observable markov decision problem advance neural information processing system 7 page mit press thomas jaksch ronald ortner peter auer regret bound reinforcement learning mach learn august issn michael kearns satinder singh reinforcement learning polynomial time chine learning 49 levente kocsis csaba ari bandit based planning machine learning ecml 2006 page springer 2006 24 reinforcement learning pomdps using spectral method aryeh kontorovich boaz nadler roi wei learning hmms arxiv preprint aryeh kontorovich roi wei et al uniform chernoff equality markov chain related process journal applied probability 51 4 1113 leonid aryeh kontorovich kavita ramanan et al concentration inequality dependent dom variable via martingale method annals probability 36 6 akshay krishnamurthy alekh agarwal john langford reinforcement learning rich observation arxiv preprint steven lavalle planning algorithm cambridge university press yanjie li baoqun yin hongsheng xi finding optimal memoryless policy pomdps expected average reward criterion european journal operational research 211 3 567 michael littman memoryless policy theoretical limitation practical result ings third international conference simulation adaptive behavior animal animats 3 animal animats 3 page cambridge usa mit press isbn michael littman richard sutton satinder singh predictive representation state advance neural information processing system 14 page mit press john loch satinder p singh using eligibility trace ﬁnd best memoryless policy partially observable markov decision process icml page omid madani computability partially observable markov decision ce fall symposium planning pomdps orlando fl lingsheng meng bing zheng optimal perturbation bound inverse frobenius norm linear algebra application 432 4 andrew ng michael jordan pegasus policy search method large mdps pomdps proceeding sixteenth conference uncertainty artiﬁcial intelligence uai 00 page san francisco ca usa morgan kaufmann publisher isbn p ortner r auer logarithmic online regret bound undiscounted reinforcement learning advance neural information processing system ronald ortner maillard daniil ryabko selecting approximate state representation reinforcement learning peter auer alexander clark thomas mann sandra zilles editor algorithmic learning theory volume 8776 lecture note computer science page springer international publishing isbn 25 azizzadenesheli lazaric anandkumar christos papadimitriou john tsitsiklis complexity markov decision process math oper 12 3 august issn theodore perkins reinforcement learning pomdps based action value stochastic optimization proceeding eighteenth national conference artiﬁcial intelligence fourteenth conference innovative application artiﬁcial intelligence 2002 page aaai press shaowei png pineau building adaptive dialogue system via pomdps selected topic signal processing ieee journal 6 8 dec issn doi poupart vlassis bayesian reinforcement learning partially observable domain international symposium artiﬁcial intelligence mathematics isaim pascal poupart craig boutilier bounded ﬁnite state controller sebastian thrun lawrence saul bernhard olkopf editor nip page mit press stephane ross brahim joelle pineau pomdps advance neural information processing system page satinder p singh tommi jaakkola michael jordan learning without partially observable markovian decision process icml page citeseer sondik optimal control partially observable markov process phd thesis stanford university le song animashree anandkumar bo dai bo xie nonparametric estimation latent variable model arxiv preprint matthijs spaan partially observable markov decision process marco wiering martijn van otterlo editor reinforcement learning volume 12 adaptation learning tion page springer berlin heidelberg isbn richard sutton andrew g barto introduction reinforcement learning mit press joel tropp tail bound sum random matrix foundation computational mathematics 12 4 christopher jch watkins peter dayan machine learning 8 john williams satinder singh experimental result learning stochastic memoryless policy partially observable markov decision process michael kearns sara solla david cohn editor nip page mit press 1998 26 reinforcement learning pomdps using spectral method appendix organization appendix thm 6 regret prop 1 learning symmetrization recovery view tensor decomp tensor estimation conc inequality thm 1 lemma 1 thm 2 lemma 2 thm 7 thm 8 figure 2 organization proof ﬁrst report proof main result paper section b c e postpone technical tool used derive section right preliminary empirical result sect particular main lemma theorem paper organized fig furthermore summarize additional notation used throughout appendix lowing table l concentration matrix η l j mixing coefﬁcient p l translator element sequence sample given middle action l actual sequence number quadruple consequence state random variable given second action l quadruple consequence state given second action l sequence j sequence j triple consequence view random variable given second action l triple consequence observation given second action l sequence j sequence j tensor matrix vi ni 1 p tensor linear operator deﬁned follows ip element vp ip x p p vp jp ip 27 azizzadenesheli lazaric anandkumar appendix proof lemma 2 proof proceeds construction first notice element second view written v l 2 v l 2 p l p l p l fr l used independence observation reward result summing observation recover reward model fr l x v l 2 23 any combination state x action l order compute observation model elaborate deﬁnition v l 2 v l 2 v l 2 p p p p l fπ fo fr l p since policy fπ known divide previous term fπ sum observation reward obtain denominator previous expression r x x v l 2 fπ 1 p let ρ l computed observation model f l r x v l 2 fπ ρ l 24 28 reinforcement learning pomdps using spectral method repeating procedure give full observation model f l left transition tensor need resort third view v l 3 written v l 3 v l 3 x x p l j p l x x p j p l x x fo ft l 25 used graphical model pomdp introduce dependency since policy fπ known observation model obtained second view eq 6 possible recover transition model recall observation matrix n j fo restate eq 25 l v l 3 26 l second mode transition tensor since term known ﬁnally obtain l v l 3 repeating state action give full transition model ft appendix proof thm 3 proof build upon previous result hmm anandkumar et al 2012 song et al 2013 thm 10 appendix following statement hold assumption sample drawn stationary distribution induced policy π pomdp ft π proving thm 4 consider additional error coming fact sample not necessarily drawn ft π denote singular value matrix recall covariance matrix k l ν rank x asm 2 denote σ l ν σx k l ν smallest singular value ν 1 2 3 adapting result song et al 2013 following performance guarantee spectral method applied recover column third view lemma 5 let b µ l 3 3 b ω l π estimated third view conditional distribution computed state using spectral method sect 3 using n l sample let ω l min ω l π x number sample n l 29 azizzadenesheli lazaric anandkumar n l g π 2 π ω l min min min v l ν 2 log 2 δ θ l 27 θ l max 1 3 c 2 3 1 ω l min 1 3 4 2 l min min min v l ν 28 numerical constant dimension ﬁrst second view thm 16 any δ 0 1 7 b v l 3 v l 3 2 probability 1 randomness transition observation policy l g π 4 2 2 ω l min 1 2 1 π log 2 δ n ǫm ω l min 29 e ǫm l 2 π 2 π r log 2 δ n l ω l min 1 2 min σmin v l ν 3 π 2 π min min v l ν ω l min log 2 δ n l notice although not explicit notation l depends policy π term ω l min proof proceed simplifying expression l rewriting condition n l eq 27 obtain log 2 δ n l ω l min min min v l ν g π 2 π 2 precisely statement phrased exists suitable permutation label state due fact spectral method not recover exact identity state properly relabel estimate accurate not make explicit permutation order simplify notation readability result notice l doe not depend speciﬁc state column 30 reinforcement learning pomdps using spectral method substituting bound factor log ar δ l second term eq 29 obtain e ǫm l 2 π 2 π r log 2 δ n l ω l min 1 2 min σmin v l ν 3 π 2 π min min v l ν ω l min log 2 δ n l lead ﬁnal statement trivial bound remaining term previous bound doe hold ﬁrst second view computed pendently suitable symmetrization step discussed section 3 lead inconsistent state index result compute view inverting eq deriving bound accuracy corresponding estimate introduce two proposition useful later proposition 6 fix ς ς point 2 ra let ξ random vector p ξ ei ςi 1 let ξn n copy ξ ˆ ς 1 n n p j ξj empirical average ς r log n probability 1 proof see lemma anandkumar et al 2012 proposition 7 let b k l empirical estimate k l obtained using n l sample n l σ l 2 30 k l b k l q log n l σ l q log n l 2 σ l log 1 δ n l probability 1 proof since k l e v l 3 l 1 view vector entry matrix indeed probability number 0 1 sum element 1 ςi 0 p ςi 1 31 azizzadenesheli lazaric anandkumar matrix sum result apply proposition 6 k l obtain l k l log n l 31 probability 1 statement follows applying lemma anandkumar et al 2012 previous proposition hold k l well σ l replacing σ l ready state prove accuracy estimate second view similar bound hold ﬁrst view lemma 8 let b v l 2 second view estimated inverting eq 4 using estimated covariance trice k v l 3 n l satisﬁes condition eq 27 eq 30 probability 1 b v l 2 v l 2 2 l 21 σ l l proof any state action l obtain second view inverting eq 4 computing v l 2 k l k l v l 3 derive conﬁdence bound empirical version µ l 2 proceed ﬁrst upper bounding error b v l 2 v l 2 2 l k l k l v l 3 l k l b k l v l 3 l k l b v l 3 v l 3 error l k l bounded direct application proposition 6 see also eq 31 directly use proposition 7 bound second term lemma 5 third term obtain b v l 2 v l 2 2 3 σ l log 1 δ n l l σ l l σ l used k l l l v l 3 since bound used hold probability 1 ﬁnal statement valid probability least 1 ready derive bound thm proof proof thm 3 ﬁrst recall estimate b fr b fo b ft obtained working second third view only illustrated sect 3 32 reinforcement learning pomdps using spectral method step 1 bound fr using empirical version eq 5 reward model state action l computed b fr l x b v l 2 error bounded fr l l r x b fr l l r x x b v l 2 x v l 2 r x x b v l 2 v l 2 r r x x b v l 2 v l 2 r b v l 2 v l 2 2 use any vector v applying lemma 8 obtain fr l l cr σ l ω l min 3 2 min min v l ν r log ar δ n l cr numerical constant step 2 bound ρ l proceed bounding error estimate term ρ l computed b ρ l r x x b v l 2 fπ used estimate observation model similarly bound fr l ρ l r x x v l 2 b v l 2 fπ 1 π l min v l 2 b v l 2 1 r π l min v l 2 b v l 2 2 r σ l l min ǫρ l 32 π l min fπ smallest probability taking action according policy π 33 azizzadenesheli lazaric anandkumar step 3 bound fo observation model state action l recovered plugging estimate eq 5 obtain b f l r x b v l 2 fπ b ρ l dependency l due fact use view computed action result estimation error bounded follows x b f l x r x 1 fπ b v l 2 b ρ l v l 2 ρ l 1 π l min x r x ρ l b v l 2 v l 2 v l 2 l ρ l b ρ l ρ l 1 π l min x r x b v l 2 v l 2 b ρ l ρ l ρ l b ρ l ρ l x r x v l 2 1 π l min r b ρ l b v l 2 v l 2 2 ρ l ρ l b ρ l ρ l r x v l 2 b 1 π l min r b ρ l ǫρ l b ρ l ρ l c 1 π l min 21 σ l ǫρ l used fact only summing r element instead whole r dimensionality vector v l 2 b use lemma 5 8 c fact l p similar ρ l recalling deﬁnition ǫρ l lemma 5 lemma 8 obtain f l 62 π l min 2 l l co π l min l ω l min 3 2 min min v l ν r log ar δ n l co numerical constant mentioned sect 3 since obtain one estimate per action end deﬁne b fo estimate smallest conﬁdence interval b fo b f arg min b f l b l 34 reinforcement learning pomdps using spectral method whose corresponding error bound fo min co π l min l ω l min 3 2 min min v l ν r log ar δ n l column estimated l matrix different permutation state matrix different column ordering let assume number sample action way satisﬁes b l 4 one exactly match matrix l propagate order matrix v l 2 v l 3 condition b l 4 represented follow n l oy r λ l step 4 bound ft derivation bound b ft complex since tion b ft obtained solution linear system equation eq 26 any state action l compute b l b b v l 3 33 b obtained plugging estimate b ﬁrst recall following general result matrix instance case let w c w any pair matrix c w w e suitable error matrix e meng zheng 2010 w 5 2 max w 34 spectral norm since lemma 8 provides bound error column v l 2 action bound error ρ l already developed step 2 bound norm estimation error column b x min b l 35 focus maximum eq 34 need bound spectral norm estimated σx b σx b singular value matrix b whose perturbation bounded since matrix ha rank x asm 2 σx b 1 σx 1 σx 1 σx 1 σx recall b fo corresponds estimate b f l tightest bound b l 35 azizzadenesheli lazaric anandkumar ready bound estimation error transition tensor deﬁnition eq 33 any state 1 x error bounded l ti l 3 v l 3 l 3 lemma 5 bound error column v l 3 thus v l 3 l 3 v l 3 l 3 l using bound eq 34 denoting l 3 σmax v l 3 obtain ti 5 2 σx 1 σx σmax v l 3 18 l 1 σx 1 σx 2 σx 1 σx σmax v l 3 18 l finally using bound eq 35 bounding σmax v l 3 l ti 4 σx x min b l 18 l ct σx π l min l ω l min 3 2 min min v l ν r log n l thus leading ﬁnal statement since require bound hold simultaneously action probability ﬁnal statement 1 notice sake readability ﬁnal expression reported theorem use denominator error transition model bound error report statement probability 1 change logarithmic term bound accordingly appendix proof theorem 4 proof proof theorem 4 proof similar ucrl jaksch et al 2010 step ha carefully adapted speciﬁc case pomdps estimated model obtained spectral method obtained l 3 l 3 x since sum column v l 3 one 36 reinforcement learning pomdps using spectral method step 1 regret decomposition ﬁrst rewrite regret making explicit regret accumulated episode remove phase regn k x x k xt e πk yt rmaxψ k x x k xt e πk yt rmaxkψ rt xt e πk yt random reward observed taking action prescribed timistic policy e πk depending observation triggered state xt introduce time step k k k l k lt l k x l k xt x l counter v k k v k l k l v k x l k x l recall n k l denotes number sample action l available beginning episode k used compute optimistic policy e πk ﬁrst remove randomness observed reward hoeffding inequality p k k x k rt xt e πk yt x x l v k x l r x l v k log 1 δ 2 n k l l probability taken reward model fr observation model fo r x l expected reward pair x recalling deﬁnition optimistic pomdp f k arg k η π f k e π k e η k applying previous bound regret deﬁnition obtain regn k x x x x v k x l e η k r x l z k p n log rmaxkψ high probability last term follows jensen inequality fact p k v k step 2 condition n l reported thm 3 conﬁdence interval valid only action l 1 enough sample available result need compute many episode condition eq 9 satisﬁed high probability ﬁrst roughly simplify condition introducing ω l min ω l π x n max max 4 σ l 2 oy r λ l 2 ω l min 2 min min v l ν θ l log 2 ar δ 37 azizzadenesheli lazaric anandkumar θ l max 1 3 c 2 3 1 ω l min 1 3 4 2 l min min min v l ν 36 recall beginning episode k pomdp estimated using n k l largest number sample collected action l any episode prior k n k l k v l thus ﬁrst study many sample likely collected any action l any episode length let τ l π mean passage time two step action l chosen according policy π deﬁne τ l τ l π e l l l l random variable represent passing time two step action l chosen according policy π markov inequality probability take l take action l divide episode length v l interval length l within interval probability observe sample action l thus average total l sample thus obtain number sample action l p v l v l v log l point derive lower bound length episode guarantee desired number sample collected solve v l v log l obtain condition q l log q l log l n simpliﬁed v l n log 37 thus need ﬁnd suitable number episode e k exists episode e k v satisﬁes condition eq since episode terminated action l v k l selected twice number sample available beginning episode n k l episode k wa episode past k least step c max n number action ac episode wa least one episode action reached sample forced episode least long condition eq 37 need turn give e k v ﬁnally implies e k v sufﬁcient condition number episode needed guarantee action selected enough condition thm 3 satisﬁed 38 reinforcement learning pomdps using spectral method left measuring regret accumulated ﬁrst e k episode e x x k xt e πk yt e x v k e x e k v 1 38 ﬁrst step maximize regret rmax use rough upper bound length episode length doubled episode ﬁnally use e step 3 failing conﬁdence interval even ﬁrst e k episode conﬁdence interval used construct set pomdps k may not correct implies true pomdp not contained k bound regret case failing conﬁdence interval thm rfail k x x k xt e πk yt k k x v k k n x denotes set admissible pomdps according sample available time recall step 2 number step needed statement thm 3 valid 4 v 1 n large enough bound regret rfail x n x n n x left bounding last term ﬁrst notice redeﬁne conﬁdence interval thm 3 substituting term log log obtain any time instant statement hold probability 1 since n x n z dt n n set k any time step probability 1 result regret due failing conﬁdence bound bounded n probability 1 step 4 reward model focus regret k k k contained f mk decompose two term k x x x v k x l e η k r k x l z x x x v k x l e r k x l r x l z b 39 azizzadenesheli lazaric anandkumar e r k expected reward used optimistic pomdp f k start bounding second term only depends size conﬁdence interval estimating reward model pomdp b x x x v k x l max e r k l r l x v k l max e r k x l r x l x v k l b k l r b k l r corresponds term b l r thm 3 computed using n k l sample collected episode k l arg k v l step 5 transition observation model proceed studying ﬁrst term compare optimal average reward optimistic model f k optimistic reward collected state traversed policy e π k true pomdp ﬁrst recall poisson equation any pomdp any policy π action value function qπ x r satisﬁes qπ x r x x ft x fπ qπ 39 x x ft x fπ qπ x fπ p fo fπ term r ft depend speciﬁc pomdp deﬁne function qπ x l qπ x l qπ x l min x l qπ x l x l qπ x l 2 centered version qπ x l order characterize q introduce notion diameter speciﬁc pomdps family policy considered problem max x x l min e π x l π x l random time take move state x ﬁrst taking action l following policy π reaching state performing action important feature diameter used upper bound range function qπ computed using policy derived eq 14 optimistic model proof fact similar case diameter mdps ﬁrst recall deﬁnition optimistic policy e π k arg max max k η π 40 40 reinforcement learning pomdps using spectral method mk optimistic model joint choice policy model seen pomdp f augmented action space considered taking action state x corresponds basic action choice transition reward observation model k denote p k corresponding augmented policy space using p set admissible pomdps k result any augmented policy e executed f obtain transition reward observation equivalent executing standard policy e π speciﬁc pomdp f k viceversa result computing e π k corresponding optimistic model f k equivalent choosing optimal policy pomdp f since true pomdp diameter k diameter augmented pomdp f furthermore show optimal policy ha range bounded let u assume exists pair x qe π k f k x π k f k easy construct policy different e π k achieves better already know deﬁnition diameter exists policy moving x step average optimal policy followed only rmaxd reward could missed thus difference function x not larger rmaxd 1 thus contradicting assumption result obtain max x qe π k f k x x qe π k f k x 41 thus qe π k f k x x l 1 2 replacing q q poisson equation optimistic pomdp characterized transition model e f k observation model policy e πk take action according distribution e f k e π k obtain x e f k l x e f k e π k qe π k f k π k f k x l x e f k l x e f k e π k qe π k f k x ft l x fe π k qe π k f k x ft l x fe π k qe π k f k π k f k x l x x e f k l e f k e π k l fe π k qe π k f k z c x ft l x fe π k qe π k f k π k f k x l z ζ k x l 41 azizzadenesheli lazaric anandkumar term c expanded c x x e f k l l e f k e π k l e f k e π k π k qe π k f k x e f k l l z x ft l x e f k e π k π k z π k f k used fact p e f k e π k ﬁrst term directly apply bound thm 3 eq 12 obtain f k l l xb k l error estimating observation model x x e f k k e π k x e f k k plugging back two bound c together bound qe π k f k x l obtain c xb k l b k rmax 1 2 term regret thus bounded xb k l b k rmax 1 2 ζ k x l step 6 residual error bound cumulative sum term ζ k x l episode k x x x v k x l ζ k x l x k x ft lt x fe π k qe π k f k π k f k xt lt 42 reinforcement learning pomdps using spectral method introduce term qe π k f k obtain two different term x x x v k x l ζ k x l x k x ft lt x fe π k qe π k f k π k f k qe π k f k π k f k xt lt x k x ft lt x fe π k qe π k f k π k f k z yt use fact range qe π k f k bounded diameter notice e xt yt 0 thus yt martingale difference sequence use azuma inequality bound cumulative sum fact n x yt q log n probability 1 result bound total sum term ζ k x x x v k x l ζ k x l n x yt rmaxkd q log n rmaxkd step 7 regret bound regret k x v k l 2 b k l r xb k l b k rmax 1 2 recalling result thm 3 bound ﬁrst term previous expression k 1 p log n cr ct x v k l λ k l 1 n k l since number sample n k l collected previous episode doubled current episode k n k l k l obtain k 1 q v k log n cr ct max 1 λ k l step 8 bringing together recollect previous term number episode needed use thm 3 step 2 regret case failing conﬁdence interval step 3 43 azizzadenesheli lazaric anandkumar regret step 7 result regn p n log n z step 1 v 1 z step 2 n z step 3 q log n rmaxkd z step 6 k x e k last term bounded k x e k 1 λ p log n cr ct λ mink l λ k l deﬁned statement theorem since k random number need provide use similar argument step given stopping criterion episode every step length episode doubled result k episode inequality n k x v k x result obtain upper bound k bringing bound together obtain ﬁnal statement probability 1 appendix proof remark 2 section 4 ﬁrst prove bound transition tensor requires step 4 proof thm proof step 4 bound ft derivation bound b ft complex since distribution b ft obtained solution linear system equation like eq 7 any state action l compute l w v l 3 derive transition tensor follows b l c w b v l 3 42 c w obtained plugging estimate b fo b fr policy fπ ﬁrst recall following general result matrix instance case let 44 reinforcement learning pomdps using spectral method w c w any pair matrix c w w e suitable error matrix e meng zheng 2010 w 5 2 max w 43 spectral norm deﬁnition w v l 2 w j w n k j fπ fr k fo v l 2 v l 2 ρ l fπ fo fr l clear any column j w result stacking matrix v l 2 action properly ρ l w v 1 2 ρ j 1 v l 2 ρ j l v 2 ρ j relationship hold c w b v l 2 since lemma 8 eq 32 provide bound error column v l 2 action bound error ρ l already developed step 2 bound norm estimation error column w c w c w w 2 x l r x b v l 2 b ρ l v l 2 ρ l following similar step step 3 summand bounded b v l 2 b ρ l v l 2 ρ l b v l 2 v l 2 1 ρ l 1 b ρ l v l 2 error bounded c w w v u u x b v l 2 v l 2 2 v u u x 1 ρ l 1 b ρ l r x v l 2 2 v u u x b v l 2 v l 2 2 v u u x 1 ρ l 1 b ρ l v u u x l σ l 2 v u u x ρ l x l σ l ǫρ l r x l σ l l min 45 azizzadenesheli lazaric anandkumar bound spectral norm error estimating w w w xy r x l σ l l min 44 focus maximum eq 34 need bound spectral norm estimated w σx c w σx c w singular value matrix c w whose perturbation bounded w since matrix w rank x matrix asm 2 w σx c w 1 σx w 1 w σx w 1 σx w 1 w σx w ready bound estimation error transition tensor deﬁnition eq 33 any state 1 x error bounded l ti w l 3 v l 3 l 3 w lemma 5 bound error column v l 3 thus v l 3 l 3 v l 3 l 3 l using bound eq 34 denoting l 3 σmax v l 3 obtain l ti 5 2 w σx w 1 w σx w σmax v l 3 18 l 1 σx w 1 w σx w 2 σx w 1 w σx w σmax v l 3 w 18 l using bound eq 44 σmax v l 3 x obtain l ti 4 σx w 40 r x l σ l l min 18 l r max 1 log thus leading ﬁnal statement bound conﬁdence transition tensor move analyzing new estimator transition tensor affect regret algorithm proof exactly thm 4 except step proof 46 reinforcement learning pomdps using spectral method regret regret bounded k x v k l b k l r k l b k rmax 1 2 term treated except cumulative regret due transition model estimation error deﬁne pk e pa v k l k l rmax 1 give x k rmax 1 x v k l x max c k r λ k log n v k let τ l π mean passage time two step action l chosen according policy π restate ration dπ ratio dπ ratio τ l π τ l π dratio dratio max dπ ratio need following lemma directly follows inequality lemma 9 markovian inequality probability l π action l not visited le 1 episode k p n v k l 2 v k l π r v k log 1 δ hand p n v k l v k l π r v k log 1 δ let ct max c k σmax v k 3 λ k 3 2 rrmax r log 1 δ 1 x k x q v k l v u u u u u v k l min v k z 47 azizzadenesheli lazaric anandkumar lemma 9 v k l min v k v k 2 min τ l π q v k log 1 δ 1 2 v k 2 max τ l π q v k log 1 δ ratio ratio max τ l π log 1 δ v k 4 max τ l π log 1 δ v k 16 max τ l π 2 log 1 δ v k probability 1 ﬁrst term dominates term thus ratio probability least 1 thus ﬁnally obtain k x e x v k l k l rmax 1 3 rrmax r log 1 δ 1 p 2 2 probability least 1 probability least 1 regret bounded ﬁnal statement appendix experiment illustrate performance method simple synthetic environment follows pomdp structure x 2 4 2 r 4 rmax ﬁnd spectral learning method converges quickly true model parameter seen fig f estimation transition tensor take longer compared estimation observation matrix reward tensor observation reward matrix ﬁrst estimated tensor composition transition tensor estimated subsequently additional manipulation moreover transition tensor ha parameter since tensor involving observed hidden action state observation reward matrix involve fewer parameter planning given pomdp model parameter ﬁnd memoryless policy using ple alternating minimization heuristic alternate update policy tionary distribution ﬁnd practice converge good solution regret bound shown fig f compare following policy 1 baseline random policy simply selects random action without looking observed data 2 auer et al 2009 attempt ﬁt mdp model observed data run ucrl policy 3 watkins dayan 1992 method update policy based ﬁnd method converges much faster competing method moreover converges much better policy note policy perform poorly even worse baseline far policy mdp policy try ﬁt data high dimensional observed space therefore poor convergence rate hand spectral method efﬁciently ﬁnds 48 reinforcement learning pomdps using spectral method 1 2 3 4 5 6 7 8 9 10 0 episode average deviation true parameter transition tensor observation matrix reward tensor learning spectral method 0 1000 2000 3000 4000 5000 6000 7000 2 average reward number trial random policy b regret performance figure 3 accuracy estimated model parameter tensor decomposition see h eq 12 b comparison method attempt ﬁt mdp model ucrl policy ǫ random policy correct low dimensional hidden space quickly therefore able converge efﬁcient policy 49 azizzadenesheli lazaric anandkumar appendix concentration bound concentration function hmm provide concentration bound any matrix valued function φ sample drawn hmm extends result scalar function kontorovich et al 2014 any ergodic markov chain let consider ω stationary distribution distribution state time given initial state let deﬁne inverse mixing time ρmix follows ρmix sup kontorovich et al 2014 show measure bounded ρmix 1 geometric ergodicity 0 1 contraction coefﬁcient markov chain let yn yn denote sequence observation hmm let xn xn n denote sequence hidden state consider matrix valued function φ yn said respect spectral norm sup yn yn norm respect hamming metric yn any two sequence sample observation theorem 10 hmm concentration bound consider hidden markov model ﬁnite sequence n sample yi observation ﬁnite observation set yn arbitrary initial state distribution any matrix valued function φ yn φ yn 1 1 3 2 1 r log δ probability least g geometric ergodicity constant corresponding markov chain e φ yn expectation sample hmm initial distribution sponds stationary distribution proof appendix h theorem 11 pomdp concentration bound consider partially observable markov decision process ﬁnite sequence n l sample l 1 2 n l tions ﬁnite observation set yn l arbitrary initial state distribution any 50 reinforcement learning pomdps using spectral method matrix valued function φl function φl yn l φl yn l 2 1 1 3 2 1 r log δ probability least g geometric ergodicity constant corresponding markov chain e φ yn l expectation sample pomdp middle action l initial distribution corresponds stationary distribution proof appendix h appendix proof thm 10 11 proof based result tropp 2012 kontorovich et al 2008 kontorovich et al 2014 minor modiﬁcations applying following inequality g 1 log δ n n 1 1 1 3 2 1 r log δ bring sketch proof thm 12 give upper conﬁdence bound φ expectation initial distribution used next step ﬁnding upper bound difference e φ arbitrary initial distribution estat φ initial distribution equal stationary distribution clear kontorovich et al 2014 quantity upper bounded p upper bounded g appendix concentration bound theorem 12 matrix azuma consider hidden markov model ﬁnite sequence n sample si observation given arbitrary initial state distribution c matrix valued tion φ sn 1 dimension φ 1 1 r log δ probability least 1 e φ given initial distribution sample proof thm tropp 2012 present upper conﬁdence bound summation matrix random variable consider ﬁnite sequence matrix ψi variance parameter upper bound p ψi ψi x ψi ψi 2 r log δ probability least 1 function φ let deﬁne martingale difference function φ input random variable 51 azizzadenesheli lazaric anandkumar arbitrary initial distribution state mdi φ si 1 e 1 1 sj sub set sample position sequence one summation set random variable give e 1 φ φ sn 1 φ e φ expectation initial state distribution remaining part ﬁnding σ upper bound p mdi φ si 1 2 possible sequence let deﬁne mdi φ maxsi 1 mdi φ si 1 kontorovich et al 2008 easy show φ c function upper bounded chi n kontorovich et al 2014 shown hi n upper bounded gθ n case φ symmetric matrix reduced constant 8 reduced result thm 12 extended situation distribution next state depends current state current observation even complicated model like policy pomdp theorem 13 concentration bound consider ﬁnite sequence multiple view drawn memory le policy pomdp common middle action corresponding covariance trix v l ν l ν 1 2 3 ν simplicity let consider one set ν one speciﬁc middle action n sample drawn deﬁne random variable φi 1 n l h e hp v l ν l si 1 hp v l ν l 1 ii dimension dν dν ν 1 2 3 dimension along ν view x φi 2 1 n l x h v l ν l 1 n l e x v l ν l 2 g π 1 π δ n l probability least 1 tensor case 1 n l h e hp v l ν l l si 1 hp v l ν l l 1 ii ν any permutation set 1 2 3 1 n l x h v l ν l l 1 n l e x v l ν l l 2 g π 1 π δ n l probability least 1 proof simplicity let proof ﬁrst claim thm 13 proof second claim would followed procedure proof thm 13 needed bring together result tropp 2012 kontorovich et al 2008 thm 10 12 modify thm tropp 2012 present following upper conﬁdence bound 1 n l x h v l ν l 1 n l e x v l ν l 2 r 8 e σν p air 2 log dν δ 52 reinforcement learning pomdps using spectral method probability least 1 1 n l x h v l ν l l 1 n l e x v l ν l l 2 r 8 e σν triple 2 log δ probability least 1 needed show e σi p air 2 g π 2 n π 2 e σi triple 2 g π 2 n π 2 function φ sn 1 sn 1 collection possible sn length martingale difference deﬁned follows mdi φ si 1 e 1 1 mdi φ maxsi 1 mdi φ si 1 upper bound e σν p air follows e σν p air 2 n x u 2 2 ut ﬁxed sequence matrix follows mdt possible mdt bound triple tensor derived matricizing martingale difference next step upper bound p u 2 let deﬁne new set variable given action middle action ai l following set variable collection yp l ap l rp l yp l rp l yp l triple middle action equal l p l corresponding position original sequence let deﬁne variable consequence four hidden state xp l xp l xp l xp l variable corresponding set consecutive triple view quadruple hidden state note time deﬁne mixing coefﬁcients η l j 1 p bn l j 1 1 l bn l j 1 1 l tv tv total variation distance distribution η l j sup 1 η l ij 1 p 1 1 l p 1 1 nonzero possible input variable l 53 azizzadenesheli lazaric anandkumar l j 1 j η l j j 0 otherwise hn l 1 η l η l n martingale difference upper bound e σν p air enough upper bound pn u 2 2 directly upper bound pn 2 possible sequence sample result kontorovich et al 2008 show upper bounded pn l φ 2 φ c addition obvious class moment function element 0 1 c upper bounded 1 n l purpose paper remaining show upper bound hn pn l n lemma 14 function hn upper bounded g π π pn chn 2 π π 2 π n π 2 proof mentioned hn l 1 η l η l n needed ﬁnd upper bound 1 η l η l n η l ij 1 1 2 x bn l j bn l j bn l j 1 1 l bn l j bn l j 1 1 l ﬁrst part p bn l j bn l j 1 1 l x 1 sn l j p bn l j bn l j 1 1 sn l j sn l j 1 1 l 54 reinforcement learning pomdps using spectral method let assume simplicity hidden state not overlap state p bn l j bn l j 1 1 l x 1 sn l j p bn l j bn l j 1 1 1 1 sn l j sn l j p 1 1 sn l j sn l j 1 p 1 1 l x 1 sn l j p bn l j bn l j l j sn l j p 1 1 1 1 p 1 1 sn l j sn l j 1 p 1 1 l representation η l ij 1 1 2 x bn l j x 1 sn l j p bn l j bn l j l j sn l j p 1 1 sn l j sn l j p 1 1 1 1 p p 1 1 l p p 1 1 l η l ij 1 2 x x 1 p bn l j bn l j l j sn l j p 1 1 sn l j sn l j p 1 1 1 1 p p 1 1 l p p 1 1 l η l ij 1 2 x xp j x 1 p 1 1 p xp j l p 1 1 1 1 q q v l p v p 1 1 l p v p 1 1 l 55 azizzadenesheli lazaric anandkumar η l ij 1 2 x xp j x p xp j l h l 2 x xp j x xp l p xp j l x xp l xp l xp l h l 2 x xp j x xp l p xp j l h xp l l h v l x 1 p 1 1 p 1 1 1 1 q v l h xp l l x xp l xp l xp l h l consequence η l ij 1 1 2 j 1 p j p xp j l lemma kontorovich et al 2008 kontorovich et al 2014 p x h x l 0 1 2 h 1 clear η l ij 1 also η l ij bounded 1 2 1 g π θ π p j l l verify p x h 0 1 2 h 1 x xp l h xp l l x xp l x xp l xp l xp l h l x h l x 1 p 1 1 p 1 1 1 1 q l x 1 p 1 1 p 1 1 1 1 p p 1 1 l p p 1 1 l ﬁrst part parenthesis x 1 p 1 1 p 1 1 1 1 p p 1 1 l 1 second one show condition lemma kontorovich et al 2008 p x h x l 0 1 2 h 1 met presented proof case state case overlapped situation proof pretty much similar case 56 reinforcement learning pomdps using spectral method time upper bound l hn 1 n l x η l ij max x j η l ij π n l x θ π p l π n l x θ π g π 1 π n l 1 π g π 1 π deﬁne estat expectation initial distribution equal stationary distribution generally tensor decomposition interested 1 n l x h v l ν l 1 n l estat x v l ν l 2 1 n l x h v l ν l l 1 n l estat x v l ν l l 2 instead 1 n l x h v l ν l 1 n l e x v l ν l 2 1 n l x h v l ν l l 1 n l e x v l ν l l 2 derived thm come upper conﬁdence bound mentioned interesting deviation needed derive upper bound deviation expectation arbitrary initial state distribution expectation stationary distribution initial state simplicity let derive bound second order moment 1 n l en x v l ν l 1 n l estat x v l ν l 2 bound ǫi deviation stationary distribution markov chain follows ǫ g π θ π result 1 n l e x v l ν l 1 n l estat x v l ν l 2 g π n l 1 π negligible compared e 1 57 azizzadenesheli lazaric anandkumar corollary 15 result hold pure hmm model tensor case ν any permutation set 1 2 3 1 n l x h vν n e x vν 2 g 1 δ n l probability least 1 1 n x h vν n e x vν 2 g 1 δ n probability least 1 deviation bound follows 1 n e x vν n estat x vν 2 g n 1 proof kontorovich et al 2008 kontorovich et al 2014 shown hmm model value hn bounded g mean corresponding tingale difference bounded cg consequence hmm φ bounded n 2 appendix whitening symmetrization bound theorem 16 whitening symmetrization bound pick any hmm model k hidden state representation factor matrix ﬁnite observation set dimension corresponds representation number sample arbitrary initial state distribution satisﬁes n ωmin mini k ai 2 log 2 δ max 1 3 c 2 3 ω 1 3 min 4 2 mini k ai some constant c tensor symmetrizing whitening low order polynomial tion complexity robust power method anandkumar et al 2012 yield whitened component view µk probability least 1 b µj 58 reinforcement learning pomdps using spectral method j 1 k permutation ǫm 2 r log 2 δ n ω 1 2 min mini σk ai 3 r log 2 δ n 3 mini σk ai min therefore ai j b ai 2 1 2 3 j 1 k permutation g 4 2 4 ωmin 1 2 1 log 2 δ n ωmin proof appendix appendix whitening symmetrization bound proof proof thm 16 appendix upper conﬁdence bound deviation empirical pair matrix sor original one derived shown song et al 2013 anandkumar et al 2014 model factor three view model k hidden state derive factor matrix applying tensor decomposition method one efﬁcient way show tensor decomposition method needed ﬁrst symmetrize initial raw empirical tensor whiten get orthogonal symmetric tensor well known orthogonal symmetric tensor unique eigenvalue eigenvectors obtained thorough power method anandkumar et al 2014 without loss generality let assume interested derivation done view permuting assume tensor e triple raw cross correlation view matrix rotation matrix rotating second third view ﬁrst view mean result symmetric tensor anandkumar et al 2014 rotation matrix follow e e e e deﬁne second order moment e symmetrized version let w linear transformation w w 59 azizzadenesheli lazaric anandkumar k identity matrix matrix w 2 uλv gular value decomposition well known result tensor w symmetric orthogonal tensor ready power iteration compute unique 1 k come upper conﬁdence bound b 2 column factor matrix needed aggregate different source error deviation due empirical average error derived symmetrizing error whitening error obtain upper bound aggregated error let apply following proof technique clear matrix c c w 1 c b w let assume matrix b b singular value decomposition c w 1 b w easy show f c 2 f c 2 f c 2 f w 2 ǫm ǫm f f f c c c 2 c c c c 2 c c c 2 mean ǫm 2 c 2 c 2 c 2 c c c 2 let assume singular value decomposition matrix w 1 fact 1 2 1 2 square root matrix able learn factor matrix show 1 mini σk aidiag ω 1 2 1 ω 1 2 min mini σk ai 1 2 3 clear say c 2 c wi 2 2 ω 1 2 min mini σk ai 1 2 3 2 c 2 c 2 c 2 2 2 c 2 ω 1 2 min mini σk ai 3 bound second term ǫm c c c 2 1 3 diag ω 1 c wi wi 2 diag ω 1 c wi wi 2 diag ω 1 f wi bd 1 2 2 diag ω 1 f wi 2 1 2 2 60 reinforcement learning pomdps using spectral method diag ω 1 f wi 2 control 1 2 let e e f c fk restriction singular value e e 2 c 2 f c 1 2 2 1 2 1 2 2 2 c w 1 2 45 c w 1 e ec 2 c 2 c 2 2 c 2 4 c 2 ω 1 2 min mini σk ai 2 46 conclusion shown ǫm 2 2 c 2 ω 1 2 min mini σk ai 3 ω 1 2 min mini σk ai 2 3 47 c 2 appendix following hold 2 2 1 1 log 2 δ n 2 1 1 1 2 1 2 δ n probability least 1 followed ǫm 2 r log 2 δ n ω 1 2 min mini σk ai 3 r log 2 δ n 3 mini σk ai min probability least 1 result hold required c 2 anandkumar et al 2012 ǫm ﬁrst requirement n ω 1 2 min mini σk ai 2 2 log 2 δ 61 azizzadenesheli lazaric anandkumar second requirement ǫm k hold enough term eq 47 upper bounded c k some constant c k 2 r log 2 δ n ω 1 2 min mini σk ai 3 n 2 c ω 1 2 min mini σk ai 3 2 k log 2 δ second part c k r log 2 δ n 3 mini σk ai min n 1 6 c 1 3 mini σk ai 3 min 2 log 2 δ mean enough n ωmin mini k ai 2 max 2 δ max 1 3 c 2 1 3 min 4 log 2 δ 2 mini k ai reduced n ωmin mini k ai 2 log 2 δ max 1 3 c 2 3 ω 1 3 min 4 2 mini k ai anandkumar et al 2014 shown ǫm c c c robust power method anandkumar et al 2012 decomposes tensor come set b λi orthogonal b µi k x b λib 2 62 reinforcement learning pomdps using spectral method µi λib µi 2 λi 48 veriﬁed proof order simplify notation following use µ µi ω ωi ζ similar term estimated quantity mentioned bound ζµ ζb µ 2 ζ µ µ b ζ b µ 2 l take square left hand side obtain ζ µ µ b ζ b µ 2 2 2 b ζ 2 ζ ζ x b µ µ b µ 2 x b µ µ b µ 2 last step used inequality thus obtain equation 2 ζ ζ l solving obtain q ζ ζ 2 ζ use bound eq 48 fact since b µ probability distribution obtain q ζ ǫm ζ p 25 ζ plugging original notation previous expression obtain ﬁnal statement finally since ζ ω l π ω l π probability thus result permutation 63 azizzadenesheli lazaric anandkumar lemma 17 upper bound µi follow c 2 4 2 2 ωmin 1 2 1 log 2 δ n ωmin 49 proof shown anandkumar et al 2012 reconstruct column view process needed shown column recovered f w f w f w simplicity let investigate third view process two view third view c 2 f w 3 w 3 2 c w 3 2 λiµi λib µi 2 clear λiµi λib µi 2 ωmin 1 2 c w 3 1 ωmin f w 3 w 3 2 bd 1 2 c w 3 2 2 last inequality inspired eq c 2 2 ωmin 2 ωmin 1 2 therefore c 2 4 2 2 ωmin 1 2 1 log 2 δ n ωmin 50 64