9 dec 2008 reinforcement learning genetic policy sharing jake clark university dated october 22 2018 abstract eﬀects policy sharing agent dynamical system ha not studied extensively simulate system agent optimizing task using reinforcement learning study eﬀects diﬀerent population density policy sharing demonstrate ing policy decrease time reach asymptotic behavior result improved asymptotic behavior introduction human society thought system many interacting intelligent agent learning key learning completely independently ineﬀective facilitation knowledge people share formation allowing human better accomplish task goal enhancement task performance communication observed eybee colony 1 well bacterial colony 2 modeled system agent ment learning system clear sharing information improve performance learning tems sharing information ha shown speed task optimization reinforcement learning based agent simulation though not eﬀect asymptotic performance learning task 3 4 eﬀect not necessarily general intelligent system diﬀerent type information shared agent paper only consider policy one agent absorbs superior policy another agent order investigate eﬀects sharing tion optimization time asymptotic ior system implement robust learning rithm ensure learning algorithm keep evolution environment following investigate eﬀects changing population sity system well probability agent share information see tem display behavior display dependence information sharing asymptotic behavior often time reinforcement learning problem studied applying method reinforcement learning use presented result understand reinforcement learning extended improved ment learning sharing information result also display new type eﬀect asymptotic ior speciﬁcally sharing fact change address jellowitz asymptotic behavior intelligent system reinforcement learning overview brieﬂy cover reinforcement learning go depth only primary algorithm used lations information algorithm mentioned well aspect tutorial found elaborated ref 5 6 reinforcement rl learning subﬁeld machine learning concerned ﬁnding best set action agent environment long term reward environment maximized agent learner decision maker environment agent interacts everything agent immediate control time agent take action presented new situation environment call situation agent state though trial error agent gradually discovers best set action take certain state agent system interact sequence crete time step time step agent receives representation environment state st set possible state not confuse entire system environment environment merely agent local observation nevertheless agent take action st st set possible action state next time step 1 agent receives reward vironment well new state see ﬁgure agent decision governed policy map state action π agent aim ﬁnd policy maximizes reward key aspect reinforcement learning algorithm rl designed tion system composed agent ronment markovian ha markov 2 fig 1 rl ﬂowchart 5 erty principle no reason not tain any type information instance ory rl primary focus decision making process not designing state signal accordingly want state signal compact reﬂect immediate sensory information though system may not markovian least approximation markovian system good enough order rl algorithm work properly policy π set probability distribution specifying probability agent take certain action given state probability given π p x π return step rt deﬁned rt x episode duration number time step system take reach terminal state meaning system ha achieved intermediate ﬁnal goal γ 0 1 discount parameter discount parameter speciﬁes present consideration past event episode value function policy π given v π eπ st value function tell u expected return given state tell u good state state higher value function preferable expect higher reward similar value function deﬁne function policy π qπ eπ st function tell u good take certain action certain state using logic value function easy see v π x qπ optimizing performance reinforcement learning tem corresponds maximizing value value function accordingly aim ﬁnd optimal value function optimal function v v max π v π max π qπ thus need some method determining policy better others comparing value function π π following deﬁne optimal policy note not necessarily unique policy improvement method involved ﬁnding optimal policy fairly straightforward applies generally diﬀerent method optimization step e e e e policy evaluation policy improvement respectively method policy provement relevant π 1 arg q arg q softmax π eq p eq temperature parameter τ speciﬁes ness decision ql approximate function according q st q st α n γ max q st 1 α learning rate γ count parameter 7 like discount parameter want α 0 1 order policy evaluated 3 ql converge discrete number time step proven ref 8 x αk x k 2 order criterion equation 2 met deﬁne αk 1 k k number time pair ha visited decided use method ensure accuracy approximation value function rather method ﬁnding accurate value approach latter method see ref 9 implementation ql found algorithm algorithm 1 true π st observe α st α st 1 α st q st q st α st γ maxa q st iteration iteration 1 iteration max iteration terminal state update π softmax iteration 0 end end ii system system investigate enclosed dimensional square arena containing circular density ρ corresponds surface area bot divided total area arena bot ha sors covering n equal wedge circumference sensor numbered 0 1 n simulation ﬁx n some object touching agent some angle φ agent orientation nth sensor activated n given n state n sensor oﬀ thus deﬁne state x µn 1 nth sensor activated µn 0 not according representation possible state 0 1 illustration found ﬁgure bot smooth hard disk collide wall bot completely inelastically simulation tialized placing agent arena diﬀerent random initial orientation none overlap fig 2 illustration two touching agent one left state 4 whereas agent state one agent broadcast policy would receive broadcast ha ﬁxed set action rotate one ﬁxed evenly spaced move forward straight line universal constant speed along orientation θ thus n respectively next learning iteration occurs agent reach minal orientation done rotating towards new gle experience collision occurrence called event event occurs agent cast policy probability p adjacent agent touching original agent circumference ﬁgure 2 algorithm 2 display policy sharing work b set touching broadcast policy policy sharing rithm genetic evaluates ﬁtness policy based value function proceeds ıve lutionary selection algorithm 2 policy sharing algorithm random p b πa v b v qb qa πb πa end end end not confuse traditional notion state used rl state measured discrete interval ing time step learning iteration agent 1 oriented angle θ corresponds ing angle θ n 1 2 n 4 small arena 150 150 ρ b large arena 200 200 ρ fig 3 image small large arena used simulation moving no moving state associated agent next state determined reading sensor event simulation run two square arena l length side arena r radius large arena ha 20 whereas small arena ha large arena hold 50 agent small arena hold 25 agent some image diﬀerent setup found ﬁgure agent task travel greatest distance represent task specifying reward agent receive action kdat c k dat distance traveled result action c parameter discourage action small k parameter make sure reward traveling not drown good behavior corresponds short convergence time quick learning higher average speed longer distance per unit time focusing one learning algorithm conduct primary simulation using single ing algorithm though interesting behavior not ited best algorithm one achieves objective greatest extent choose one duce number variable decided choose algorithm ing density parameter varied want safe not buried behavior contained algorithm struggle optimize policy typical son learning algorithm considered displayed ure behavior displayed observed across ent density clear softmax ql algorithm quick adapting environment simulation γ τ 0 500 1000 1500 2000 2500 3000 3500 0 100 200 300 400 500 average distance traveled per bot time large arena master random mcε qlε qlb fig 4 comparison learning algorithm low arena density ε corresponds using policy uation whereas b corresponds softmax policy ation no policy sharing 0 500 1000 1500 2000 2500 3000 3500 4000 time 0 1000 2000 3000 4000 5000 6000 average distance per bot slope threshold density p data fit fig 5 vertical red line speciﬁes point time system converged sharing probability case iii result already seen ql algorithm converges convergence show constant average locity per bot displayed ﬁgure 4 agent policy become static criterion use mine system average distance per bot becomes linear time point found ﬁtting tail curve ing ﬁt line backwards time point ﬁt curve deviate data threshold system display asymptotic behavior see ﬁgure hypothesis lower arena density system le cated accordingly agent experience ilar situation often optimize quickly also see eﬀects sharing prominently higher density interaction per 5 bot per time step also expect general some sharing speed convergence however much sharing might slow system act greedily beginning optimizing immediate reward impairing eﬀorts exploration something not guess fastest convergence lie along sharing probability space also unsure eﬀect sharing probability p asymptotic behavior though source mentioned earlier lead u believe sharing doe not aﬀect behavior simulation result discussion ﬁgure 6 see lower density sharing doe not strong eﬀect convergence time also observe general lower density converge quicker observe similar behavior smaller arena curiously take time smaller arena converge see ﬁgure regardless general trend seem apply system time convergence 0 1 pshare density 0 500 1000 1500 2000 2500 fig 6 simulation result large arena notice fastest convergence time correspond highest sharing probability since curious asymptotic performance investigate asymptotic average velocity ample ﬁgure ﬁgure 9 appears no ference low sharing probability high ing probability doe appear diﬀerence no sharing independent any amount sharing order get closer look ratio refer ﬁgure observe heavy discrepancy independent system asymptotic behavior any sharing system density increase agent ﬁnding local optimal policy eventually make some sort preference towards resolve certain instance turn left turn right encountering collision preference become approximately permanent ter policy well established preference cause any given agent collective butt head way agent prefers instance 0 500 1000 1500 2000 2500 0 time density convergence time fig 7 closer look some sharing probability ure some unﬁtting spike due artifact gence time calculation see ﬁgure 5 instance data ran closely parallel tail linear ﬁt long duration time time convergence 0 1 pshare density 0 500 1000 1500 2000 2500 fig 8 simulation result smaller arena notice ilar behavior large arena ﬁgure counterclockwise vortex versus clockwise one agent forced swap policy collective converges single preference exact behavior displayed ﬁgure make no mistake dent agent policy optimize independent reinforcement learning diﬃcult agent coordinate iv conclusion see time convergence depends arena size arena density sharing ity smaller arena experienced slower convergence time lower density experienced faster convergence time higher sharing probability experienced faster convergence time sharing probability ing fastest convergence time appears regime 1 not somewhere 0 1 expected perhaps interesting result simulation 6 convergent average speed 0 1 pshare density 0 1 2 3 4 5 6 7 fig 9 simulation result large arena displaying asymptotic performance collective notice slightly darker band no sharing left rest behavior uniform 1 0 sharing density ratio convergent average velocity per bot large arena small arena fig 10 ratio sharing terminal speed independent terminal speed dependency average convergent policy large arena 0 3 6 9 12 15 state 0 1 2 3 action 0 1 p fig 11 low share probability p high share bility display collective adapting preference p 0 see average policy level ing agent not coordinate contrast ref 3 4 ﬁnd sharing cies signiﬁcantly aﬀect asymptotic behavior system especially higher density system velops agent develops preference towards resolving diﬀerent state preference become manent agent ha well established policy any sharing p 0 system run long period time agent adopt uniform preference collective becomes coordinated speciﬁed task performed better sharing case independent case improvement show prominently greater density appears independent arena size genetic policy sharing used outlined rithm 2 no mean best algorithm variant might prove perform better averaging policy value function rather erasing past tion agent accumulated regardless result explained quite plainly demonstrated asymptotic behavior rl system improved policy sharing mechanism future work ﬂuctuations system not discussed aspect system could yield interesting ior example emergent cooperative behavior system increase decrease ﬂuctuations performance measure total distance traveled per agent respect time one could also study eﬀects robustness learning algorithm context system ref 10 maozu guo et al ﬁnd learning rithm based simulated annealing performs well would interesting see simulated annealing learning algorithm performs well ql see system behaves varying parameter respect two learning algorithm additional alternative algorithm include varying sharing algorithm introducing inhomogeneity ing algorithm furthermore one introduce mogeneity agent example varying agent radius mediating algorithm introduced nate agent algorithm based rl some subﬁeld machine learning eﬀects might introduce some ing behavior nature system lends well spacial diﬀusion simulation done increasing arena size placing many agent dense group arena center addition studying spacial fusion one study diﬀusion knowledge among collective one way done placing master agent among dummy agent tracking knowledge diﬀuses diﬀusion aﬀects collective task performance 7 acknowledgment thanks go benny brown helping get project oﬀthe ground also jim crutchﬁeld would also like thank ping xuan helpful discussion research wa conducted part network dynamic program complexity science center university california davis wa supported nsf reu program 2008 versity california davis appendix running simulation order run simulation execute ﬁle primary directory some help included well readme ﬁle contact any tions regarding simulation output ing process simulation source code available 1 sherman visscher nature 419 920 2002 issn 2 philisophical transaction royal society london 361 1283 2003 3 tan icml 1993 pp 4 panait tuyls aamas 07 proceeding international joint conference autonomous agent multiagent system acm new york ny usa 2007 pp isbn 5 sutton barto reinforcement ing introduction mit press cambridge usa 1998 isbn 6 humphrys thesis university cambridge 1997 7 watkins thesis university bridge 1989 8 watkins dayan machine learning 8 279 1992 9 mansour mach learn 5 1 2004 issn 10 guo liu malec ieee transaction system man cybernetics part b 34 2140 2004