fully convolutional attention network recognition xiao liu tian xia jiang wang yi yang feng zhou yuanqing lin baidu research xiatian linyuanqing abstract recognition challenging due subtle local difference versus large tions pose key address problem localize discriminative part extract tures however part annotation pensive acquire moreover hard deï¬ne part many class work introduces fully convolutional attention network fcans ment learning framework optimally glimpse local criminative region adaptive different main compared previous method approach joy three advantage 1 ment learning procedure requires no expensive part tions 2 architecture speed training testing 3 greedy reward strategy erates convergence learning demonstrate effectiveness method extensive experiment four challenging benchmark datasets cluding stanford dog stanford car introduction recognition refers task guishing category bird specie 1 dog breed 2 car model 3 ï¬‚ower category 4 food dish 5 etc great potential rivaling man expert ha shown tremendous application real world ranging 6 7 education 8 9 although great success ha achieved recognition last year 10 11 12 13 grained recognition still face two challenge first difï¬cult gather large amount labeled data call expert specialized domain knowledge addition difference class subtle criminative feature often not based global shape appearance variation contained local part pattern instance shown fig 1 rhinoceros auklet correctly classified parakeet auklet conventional cnn fully convolutional attention network figure conventional cnn approach left ï¬nds difï¬culty ferentiating similar category subtle local ations rhinoceros auklet parakeet auklet trast proposed fully convolutional attention network right able automatically efï¬ciently localize part bird eye beak given only weakly supervised class label eye texture beak shape crucial differentiate parakeet auklet rhinoceros auklet end main body previous research ha cused devising discriminative feature detecting aligning object part nevertheless conventional method 14 15 utilize manually deï¬ned part localize region head bird recognition relying manually deï¬ned part ha several drawback 1 precise part annotation usually pensive acquire 2 strongly supervised model might fail some part occluded 3 some category difï¬cult manually ï¬ne part example difï¬cult deï¬ne part food recognition suggested fig 2 4 importantly no clue manually deï¬ned part optimal recognition task overcome problem propose visual tion framework called fully convolutional attention work fcans recognition without part 0 21 mar 2017 hooded oriole bmw kungpao chicken scott oriole yuxiang rousi bird b car c food figure recognition often involves part localization localizing head breast distinguish bird b localizing brand classify car make relatively easy deï¬ne part structured object like bird car however hard deï¬ne rigid part unstructured class c food may solved attention model annotation given only image label framework lizes reinforcement learning simultaneously localize ject part classify object within scene itively framework simulates human visual system proceeds object recognition via series glimpse ject part glimpse strives ï¬nd criminative location differentiate object category given previous observation similar previous visual attention model 16 17 employ reinforce gorithm training 18 action tion glimpse state image tions previous glimpse reward measure classiï¬cation correctness whole framework trained only image classiï¬cation loss thus requiring no manual part annotation visual attention approach demonstrated perform well recognition without requiring manually labeled object part 17 compared previous reinforcement visual attention framework 16 17 fcans enjoy ter computational efï¬ciency well higher classiï¬cation accuracy recognition concretely proposed framework improves attention model three way computational efï¬ciency previous framework run convolutional neural network individually image crop computationally sive training testing contrast method feature map computed fully convolutional neural network 12 11 glimpse way similar 19 make training prediction computationally efï¬cient fully convolutional neural work architecture feature sharing technique multiple part localization testing model able simultaneously locate multiple part adaptive size previous framework 16 17 generally only locate one part iteration faster training convergence instead assigning delayed reward end attention iteration vious method 16 17 apply new greedy reward strategy every step attention crucial convergence speed training racy prediction result proposed approach improves nition accuracy previous reinforcement learning based method 16 17 computationally cient method also achieve competitive result method multiple datasets related work recognition ha extensively studied recent year 5 9 20 21 22 3 2 14 4 review three relevant direction section representation learning since seminal work alexnet 10 nessing transition feature convolutional neural network tation learning 11 12 13 current recognition algorithm also based deep cnn representation distinguish subtle ence 23 24 25 branson et al 15 claim ing layer layer feature learns discriminative representation nition lin et al 25 propose bilinear architecture model local pairwise feature interaction recognition convolutional feature two el combined translation invariant manner qian et al 26 propose metric learning framework learn distance metric pull data point class close push data point different class far apart wang et al 27 combine object tection approach sampling scheme tract robust discriminative feature car classiï¬cation parallel effort method combine representation learning part tection uniï¬ed framework part model since 70 early cognitive research study 28 ha shown recognition based comparing 1 appearance detail object part drawing ration fact various pose normalization od 29 30 31 14 32 proposed focus important region however method strongly supervised one heavily relying manually part modeled poselet 33 dpm 34 due limit recent effort spent ically discover critical part weaker setting stance berg et al 35 use data mining technique learn set intermediate feature differentiate two class based appearance particular part yang et al 36 propose template model discover common ric pattern object part statistic pattern similarly gavves et al 37 chai et al 38 segment image align image segment unsupervised fashion aligned image segment lized feature extraction separately recently simon rodner 39 propose neural activation constellation proach able learn part model unsupervised manner compared method however od require tedious tuning individual nents attention model one main drawback model need strong motivation part deï¬nition either hand method may lack many object food dish 40 hand several work introduce model localization instance mnih et al 16 present recurrent neural network model ject detection adaptively selecting sequence tion region extract appearance representation region since model trained reinforcement learning technique learn policy ba et al 41 extend 16 successfully achieve good result challenging recognition task despite remarkable contribution theory current attention model still suffer several drawback practice first only result small performance provement instance sermanet et al 17 tend 41 recognition only achieve mean accuracy percentage 3 glimpse stanford dog dataset result googlenet baseline 12 second computational burden high lating feature glimpse 17 requires forwarding googlenet three time leading slow training testing exceptional work spatial transformer work 42 build differentiable attention nism doe not need reinforcement learning training alternative approach show reinforcement learning still effective efï¬cient improving recognition fully convolutional attention network fig 3 illustrates architecture fully tional attention network fcans three main ponents feature network attention network classiï¬cation network feature map extraction feature network tains fully convolutional network extract feature input image subsequent attention crop feature map shared part attention classiï¬cation experiment adopt one popular cnn architecture 11 googlenet 12 resnet 13 basis fully lutional network imagenet dataset 27 target dataset testing image attention crop resized canonical size feature extraction similar 16 hence amount computation performs controlled pendently input image size training although cropping local image region achieve good performance requires u perform multiple forward backward pass deep tional network one batch time complexity feature extraction depends number part ber attention region sampled part practice thus extract feature map original image multiple scale across time step feature part obtained selecting corresponding region convolutional feature map receptive ï¬eld selected gion size part result only need run forward pas one training batch fully convolutional part attention attention work localizes multiple part generating multiple part score map basis convolutional feature map score map generated using two stacked convolutional er one spatial softmax layer ï¬rst convolutional layer us 64 3 3 kernel second one us one 3 3 kernel output conï¬dence map spatial softmax layer convert conï¬dence map probability testing model selects attention region highest probability part location ing training model sample attention region multiple time according probability map process applied ï¬xed number time step multiple part location time step generates location ular part detail step following section classiï¬cation classiï¬cation work contains convolutional network part well whole image classiï¬cation network part fully convolutional layer followed softmax layer different part might different size local age region cropped around part location according size ï¬nal prediction score average 2 time time input crop cnn training part en classiï¬ca feature extrac te ng input en crop cnn en figure architecture fcans framework example attention network ï¬nds two part different size blue region yellow region upper part show architecture testing lower part show architecture training testing crop corresponding part patch high resolution image classiï¬cation training volutional feature attention network classiï¬cation note testing compute part attention simultaneously make model computationally efï¬cient traditional recurrent attention model prediction score individual classiï¬ers order discriminate subtle visual difference local image region cropped high resolution model entire attention problem formulated markov decision process mdp time step mdp fcans work agent perform action based observation receives reward work action corresponds location attention region observation input image crop tention region reward measure quality classiï¬cation using attention region target learning ï¬nd optimal decision policy generate action observation characterized parameter fcans maximize sum expected reward across time step deï¬ne input image x feature work parameterized Î¸f computes feature map Ï† x Î¸f attention network output attention cation lt location lt Î¸t l Ï€ policy attention selection parameterized Î¸l Î¸t l time step classiï¬cation ponent crop image region location lt extract new feature Ï† lt predicts classiï¬cation score st classiï¬cation network parameterized Î¸c Î¸t c computes ï¬nal classiï¬cation score st erage prediction score time st 1 x sÏ„ Ï† lÏ„ Î¸Ï„ c 1 note fcans Î¸l Î¸c different set parameter Î¸t l Î¸t c different time step only eters feature network Î¸f shared across time step different original recurrent tion model 16 41 parameter shared across multiple time step reward rt step sures output st match ground truth label training since no annotation indicate select attention region attention differentiable function adopt reinforcement learning learn network parameter given set training image ground truth label xn yn jointly optimize three component 3 maximize following objective function max Î¸ j Î¸ max Î¸f Î¸l Î¸c r Î¸f Î¸l Î¸f Î¸c 2 Î¸ Î¸f Î¸l Î¸c parameter feature network attention network classiï¬cation work respectively l Î¸f Î¸c 1 nt n x x lt n xn yn Î¸f Î¸c 3 average classiï¬cation loss n ing sample time step r Î¸f Î¸l 1 nt n x x eÎ¸ rt n 4 average expected reward n training sample time step eÎ¸ rt n x lt n Ï€ lt Î¸f Î¸t l rt n 5 expected reward selected attention region sample Î¸t l parameter tention network Ï€ lt Î¸f Î¸t l Ï€ lt xn Î¸t l probability selecting lt n attention region ward function rt n crucial developing efï¬cient ing algorithm describe design reward tion following section reward strategy straightforward reward strategy measure ity attention region selection policy whole ing ï¬nal classiï¬cation result rt n 1 yn arg maxy p n 0 otherwise although mdp reward strategy learn recurrent way 16 confuses effect selected region different time step might lead problem vergence difï¬culty consider alternative reward strategy namely greedy reward rt n 1 1 arg maxy p n 1 1 arg maxy p n n n 0 otherwise 6 lt n classiï¬cation loss sample step image classiï¬ed correctly ï¬rst step attention network immediately receives reward time step reward corresponding attention network only image classiï¬ed correctly classiï¬cation loss decrease regard last time step otherwise attention network receives zero reward since tion network immediately receives reward image correctly classiï¬ed current attention region convergence training much easier classification result reward attention region confidence map convolution feature ğœ™ğ‘¥ ğœƒ ğ‘™ ğ‘™ ğ‘™ ğ‘ ğ‘™ ğœƒ ğ‘ ğ‘™ ğœƒ ğ‘ ğ‘™ ğœƒ ğœ‹ğœ™ ğœƒ ğœ‹ğœ™ ğœ‹ğœ™ ğœƒ ğ‘Ÿ ğ‘Ÿ ğ‘Ÿ forwarding ğœ™ğ‘¥ ğœƒ ğ‘™ ğ‘™ ğ‘ ğ‘™ ğ‘ ğ‘™ ğ‘ ğ‘™ ğœ‹ğœ™ ğœ‹ğœ™ ğœ‹ğœ™ ğ‘Ÿ ğ‘Ÿ classification result reward attention region confidence map convolution feature b figure forward b process training attention network mdps dashed line indicate sampling procedure optimization difï¬cult directly compute gradient eÎ¸ rt n Î¸ requires evaluating exponentially many possible part location training hence employ reinforce algorithm approximate gradient monte carlo way 43 rt n k k x Ï€ lt xn Î¸t l rt nk 7 lt nk lt xn Î¸t l sampled according multinomial distribution parameterized output ï¬dence map attention network forward process training attention network mdps shown fig given basis convolutional feature map Ï† x input attention network output conï¬dence map Ï€ Ï† Î¸t l different time step Ï€ Ï† Î¸t l form multinomial distribution location attention region lt sampled distribution sampling procedure repeated k time use classiï¬cation network get reward rt 4 gradient Î¸f Î¸c obtained classiï¬cation work gradient Î¸f Î¸l calculated using icy gradient shown equation notice reward 0 ignore sample implementation detail training although jointly training tire model possible develop algorithm sake training speed ï¬rst step initialize cnn model extract basis tional feature map attention classiï¬cation second step ï¬x cache basis convolutional ture map ï¬rst step train attention work separately third step ï¬x cache lected attention region second step ï¬nal classiï¬cation model feature caching peated feature calculating avoided notice volutional neural network attention ï¬nal siï¬cation different though initialized similarly described repeat step several time convergence approximation training although compute feature map obtain feature high resolution region crop could still image resolution large thus employ approximated feature extraction method similar 19 only compute feature map input image one scale lutional feature part obtained selecting corresponding region convolutional feature map whole image receptive ï¬eld selected region size part erates training attention network note since adopt training rcnn approximation only utilized attention work training ï¬nal classiï¬cation network still trained given feature extracted cropped high olution image discussion attention component inspired recurrent visual attention model 16 however instead building recurrent attention network share parameter ferent time step model us multiple convolutional network different parameter model temporal effect testing attention network work like independent part detector share basis age feature even possible combine attention network single convolutional network compute part attention simultaneously make inference much faster dataset class train test bbox part stanford dog 2 120 stanford car 3 196 1 200 5 101 table statistic four benchmark datasets experiment conduct extensive experiment four benchmark datasets including 1 stanford dog 2 stanford car 3 5 table 1 show statistic four datasets experimental setup use 13 feature extraction ï¬rst resize image 512 512 tion randomly cropped 448 448 patch input image put 2048 16 16 feature map use feature map train attention network ï¬nd two part ï¬rst part selects 4 4 region feature map corresponding 128 128 patch resized image second one selects 8 8 region corresponding 256 256 patch resized image crop two result attention patch resize 512 512 train prediction model ï¬nal classiï¬cation stage model trained using rmsprop batch size 512 90 epoch initial learning rate multiplied every 30 epoch implementation based caffe 44 computational time stanford dog dataset fcans take 3 hour train single tesla gpu signiï¬cantly faster conventional recurrent attention model 17 take 30 hour converge implementation convolutional feature requiring additional training time model image resolution testing time cost attention selection negligible compared feature calculation time compared recurrent attention model 17 take testing method faster comparison compare framework previous method summarize result table 3 table recognition accuracy comparable method 25 42 24 without using bounding box testing 5 accuracy acc box zhang et al 32 branson et al 15 simon et al 39 krause et al 22 lin et al 25 jaderberg et al 42 kong et al 24 model table comparison related work dataset testing ground truth box part stanford dog accuracy acc box gavves et al 37 simon rodner 39 sermanet et al 17 zhang et al 45 krause et al 40 model table comparison related work stanford dog dataset stanford car accuracy acc box chai et al 46 gosselin et al 47 girshick et al 48 lin et al 25 wang et al 49 krause et al 22 model table comparison related work stanford car dataset method accuracy acc box bossard et al 5 myers et al 50 model table experimental result dataset stanford dog stanford car model also competitive example obtain accuracy stanford car test set bounding box testing far best result note baseline method sermanet et al 17 us reinforcement learning based recurrent attention model similar approach method improves 12 stanford dog suggesting fcans tive framework recognition method dog car bird food finetune baseline random region center region attention region table experimental comparison effect attention method dog car bird food finetune baseline one attention only one attention two attention attention table experimental comparison number attention ablation study effect attention since approach roughly three time full image two attention region expensive single model testing conduct two tional baseline demonstrate superiority one random region experiment augment baseline single image model two random cropped region second baseline center region ment crop two center region image size two crop experiment size part attention model table 6 rizes result costing amount testing time attention network clearly outperform random gion center region model number attention table 7 summarizes result number attention affect ï¬nal cation accuracy take stanford dog example baseline achieves racy combining one 8 8 attention region diction result original image improves signiï¬cantly combining one 8 8 region one 4 4 region original image together improves result ï¬nd adding two attention 3 attention only improves performance slightly expense computation hence throughout periments ï¬x number attention two reward strategy table 8 illustrates effectiveness training reward strategy compared ditional reward setting only assigns reward attention iteration greedy reward strategy work niï¬cantly better hypothesize greedy reward help reinforcement learning quickly converge criminative 6 approach recurrent attention implementation stanford car stanford dog input attention attention input attention attention figure qualitative comparison method left recurrent attention 17 right different datasets left plot ï¬rst two attention region regenerated fcan corresponds 4 4 8 8 attention region respectively lighter color indicates higher score right also show ï¬rst two selected region 17 using implementation method dog car bird food baseline reward greedy reward table experimental comparison reward strategy baseline reward strategy only assigns reward attention iteration qualitative result qualitatively compare attention region selected model recurrent attention model 17 fig model contain attention mechanism ply reinforcement learning train focus local criminative region observe model different attention correspond different image region attention region generated 17 focus only one gion attention map also diverse tion map 17 illustrates attention model outperforms previous reinforcement learning based tention work conclusion paper present fully convolutional attention network fcans recognition fully convolutional architecture model much faster previous reinforcement learning based visual attention model training testing conduct sive experiment four different benchmark datasets show competitive performance method 7 reference 1 wah branson welinder perona belongie dataset 2011 0 5 2 khosla jayadevaprakash yao li novel dataset image categorization ford dog proc cvpr workshop visual categorization fgvc vol 2 2011 0 1 5 3 krause stark deng object resentations categorization proceeding ieee international conference computer vision workshop 2013 pp 0 1 5 4 nilsback zisserman automated ï¬‚ower ï¬cation large number class computer vision graphic image processing icvgip 08 sixth dian conference ieee 2008 pp 0 1 5 bossard guillaumin van gool mining discriminative component random forest european conference computer vision springer 2014 pp 0 1 5 6 6 bell bala learning visual similarity product design convolutional neural network acm tions graphic tog vol 34 no 4 98 2015 0 7 hadi xufeng svetlana alexander tamara buy matching street clothing tos online shop computer vision iccv 2015 ieee international conference vol 8 2015 0 8 kumar belhumeur biswas jacob kress lopez soares leafsnap computer vision system automatic plant specie identiï¬cation computer springer 2012 pp 516 0 9 berg liu woo lee alexander jacob belhumeur birdsnap visual categorization bird proceeding ieee conference computer vision pattern recognition 2014 pp 0 1 10 krizhevsky sutskever hinton imagenet classiï¬cation deep convolutional neural network advance neural information processing system 2012 pp 0 1 11 simonyan zisserman deep convolutional network image recognition arxiv preprint 2014 0 1 2 12 szegedy liu jia sermanet reed anguelov erhan vanhoucke rabinovich going deeper convolution proceeding ieee conference computer vision pattern nition 2015 pp 0 1 2 13 zhang ren sun deep residual learning image recognition proceeding ieee ence computer vision pattern recognition 2016 pp 0 1 2 5 14 liu kanazawa jacob belhumeur dog breed classiï¬cation using part localization european conference computer vision springer 2012 pp 185 0 1 2 15 branson van horn belongie perona bird specie categorization using pose normalized deep tional net arxiv preprint 2014 0 1 6 16 mnih heess graf et recurrent model visual attention advance neural information ing system 2014 pp 1 2 3 4 5 17 sermanet frome real attention grained categorization arxiv preprint 2014 1 2 5 6 7 18 williams simple statistical rithms connectionist reinforcement learning machine learning vol 8 no pp 1992 1 19 girshick fast proceeding ieee national conference computer vision 2015 pp 1448 1 5 20 cui zhou lin belongie gorization dataset bootstrapping using deep metric ing human loop proceeding ieee conference computer vision pattern recognition 2016 pp 1 21 huang xu tao zhang cnn visual categorization proceeding ieee conference computer vision pattern tion 2016 pp 1 22 krause jin yang recognition without part annotation proceeding ieee conference computer vision pattern tion 2015 pp 1 6 23 gao beijbom zhang darrell compact bilinear pooling proceeding ieee conference computer vision pattern recognition 2016 pp 326 1 24 kong fowlkes bilinear ing classiï¬cation arxiv preprint 2016 1 5 6 25 lin roychowdhury maji bilinear cnn model visual recognition proceeding ieee international conference computer vision 2015 pp 1 5 6 26 qian jin zhu lin visual categorization via metric learning ings ieee conference computer vision pattern recognition 2015 pp 1 27 wang yang chen lin pling image classiï¬cation arxiv preprint 2014 1 2 28 rosch mervis gray johnson basic object natural category nitive psychology vol 8 no 3 pp 1976 1 8 29 farrell oza zhang morariu darrell davis birdlets subordinate categorization ing volumetric primitive appearance computer vision iccv 2011 ieee international ference ieee 2011 pp 2 30 zhang farrell iandola darrell deformable part descriptor recognition attribute prediction proceeding ieee international ference computer vision 2013 pp 2 31 zhang paluri ranzato darrell bourdev panda pose aligned network deep attribute modeling proceeding ieee conference computer vision pattern recognition 2014 pp 2 32 zhang donahue girshick darrell based category detection pean conference computer vision springer 2014 pp 2 6 33 bourdev malik poselets body part detector trained using human pose annotation computer sion 2009 ieee international conference ieee 2009 pp 2 34 felzenszwalb girshick mcallester manan object detection discriminatively trained based model ieee transaction pattern analysis machine intelligence vol 32 no 9 pp 2010 2 35 berg belhumeur poof feature categorization face veriï¬cation attribute estimation proceeding ieee conference computer vision pattern recognition 2013 pp 962 2 36 yang bo wang shapiro unsupervised template learning object recognition vances neural information processing system 2012 pp 2 37 gavves fernando snoek smeulders tuytelaars categorization alignment proceeding ieee international conference computer vision 2013 pp 2 6 38 chai lempitsky zisserman bicos method image classiï¬cation puter vision iccv 2011 ieee international conference ieee 2011 pp 2 39 simon rodner neural activation constellation unsupervised part model discovery convolutional work proceeding ieee international ence computer vision 2015 pp 2 6 40 krause sapp howard zhou toshev duerig philbin unreasonable fectiveness noisy data recognition ropean conference computer vision springer 2016 pp 2 6 41 ba mnih kavukcuoglu multiple ject recognition visual attention arxiv preprint 2014 2 3 42 jaderberg simonyan zisserman et spatial transformer network advance neural information processing system 2015 pp 2 5 6 43 sutton mcallester singh mansour et policy gradient method reinforcement learning function nip vol 99 1999 pp 4 44 jia shelhamer donahue karayev long shick guadarrama darrell caffe tional architecture fast feature embedding ings acm international conference dia acm 2014 pp 5 45 zhang wei wu cai lu nguyen weakly supervised image tion arxiv preprint arxiv vol 1504 7 2015 6 46 chai lempitsky zisserman symbiotic mentation part localization tion proceeding ieee international conference computer vision 2013 pp 6 47 gosselin murray egou perronnin visiting ï¬sher vector classiï¬cation tern recognition letter vol 49 pp 2014 6 48 girshick donahue darrell malik rich ture hierarchy accurate object detection semantic segmentation proceeding ieee conference computer vision pattern recognition 2014 pp 6 49 wang choi morariu davis mining criminative triplet patch classiï¬cation proceeding ieee conference computer vision pattern recognition 2016 pp 6 50 meyers johnston rathod korattikara ban silberman guadarrama papandreou huang murphy towards automated bile vision food diary proceeding ieee national conference computer vision 2015 pp 1241 6 9