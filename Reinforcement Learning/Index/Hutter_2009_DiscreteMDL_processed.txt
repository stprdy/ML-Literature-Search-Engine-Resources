25 sep 2009 discrete mdl predicts total variation marcus hutter rsise anu sml nicta canberra act 0200 australia marcus september 2009 abstract minimum description length mdl principle selects model ha shortest code data plus model show countable class model mdl prediction close true distribution strong sense result completely general no independence ergodicity arity identiﬁability assumption model class need made formally show any countable class model tions selected mdl map asymptotically predict merge true measure class total variation distance implication main like forecasting discriminative learning reinforcement learning discussed content 1 introduction 1 2 fact insight problem 4 3 notation main result 6 4 proof finite model class 8 5 proof countable model class 10 6 implication 12 7 variation 14 reference 14 keywords minimum description length countable model class total variation distance sequence prediction discriminative learning reinforcement learning 1 introduction minimum description length mdl principle recommends use among peting model one allows compress better compression regularity ha detected hence better 1 prediction mdl principle regarded formalization ockham razor say select simplest model consistent data multistep lookahead sequential prediction consider sequential diction problem observed sequence x xℓ ℓ predict z observe x cal prediction concerned h 1 lookahead 1 h total prediction h paper consider last hardest case infamous problem category black raven paradox ing observed ℓblack raven likelihood raven black computer science problem inﬁnite horizon reinforcement learning predicting inﬁnite future necessary evaluating policy see section 6 application discrete mdl bayes let countable class sequence x sorted k qi say containing unknown true sampling distribution main result arbitrary measurable space x keep thing simple introduction let u illustrate mdl ﬁnite x case deﬁne qi x data sequence x possible code x logp x bit using huﬀman coding since x sampled p code optimal shortest among preﬁx code since not know p could select lead shortest code observed data order able reconstruct x code need know q ha chosen also need code q take k q bit hence x coded x q bit mdl selects model minimizer mdlx arg min q x k q given x true predictive probability z p p xz x since p unknown use mdlx mdlx xz x substitute main concern close latter former measure distance two predictive distribution dh p x h p 1 h easy see dh monotone increasing twice total variation distance tvd deﬁned 3 mdl closely related bayesian prediction comparison existing result bayes interesting bayesians use bayes prediction bayes x p x wq bayesian mixture prior weight wq 0 p natural choice q 2 result following result shown e dh p ℓ p e dh p ℓ p p p surely ℓ x 2 expectation e p left statement h dh almost surely including some form convergence rate bayes ha proven mdl proof adapted far asymptotics concerned right result much stronger require sophisticated proof technique bayes result follows proof mdl primary novel contribution paper precisely arbitrary measurable x total variation distance another general consistency result presented consistency shown only probability predictive implication result unclear stronger almost sure result alluded given reference contains only result sequence not generalize arbitrary class existing result discrete mdl far le satisfactory elegant bayesian prediction tvd motivation result hold completely arbitrary countable model class no independence ergodicity stationarity identiﬁability sumption need made bulk previous result mdl continuous model class much ha shown class independent identically distributed dom variable many result naturally generalize ergodic sequence like markov instance asymptotic consistency ha shown many application violating tions some presented section one often hear exaggerated claim unlike bayes mdl used even true distribution p not indeed used question wether any good some result supporting claim p closure similar result exist bayes essentially p need least close some mdl work interesting environment not even close data pervasive includes prediction lem like weather forecasting stock market prediction indeed also perfect example process much green house gas massive volcanic eruption asteroid impact another world war could change irreversibly life also not ergodic one inattentive second car irreversible consequence also stationarity easily violated scenario environment contains learning agent stationary relevant learning phase extensive game reinforcement learning classical example often assumed true distribution uniquely identiﬁed totically environment asymptotic distinguishability depend 3 realized observation prevent prior reduction partitioning even principally possible practically burdensome presence approximate symmetry indeed problem primary reason considering predictive mdl mdl might never identify true distribution main result show sequentially selected model become predictively indistinguishable countability severest restriction result le countable case useful problem class md qθ rd say reduced countable class pd result hold pd bayes nml estimate md alternatively dmd could reduced countable class considering only computable parameter essentially interesting model class contain countable topologically dense subset certain circumstance mdl still work parameter alternatively one may simply reject parameter philosophical ground finally niques countable case might aid proving general result continuous possibly along line content paper organized follows section 2 provide some insight mdl bayes work restricted setting break general countable circumvent problem formal development start section 3 introduces notation main result proof ﬁnite presented section 4 denumerable section section 6 show result applied sequence prediction classiﬁcation regression discriminative learning reinforcement learning section 7 discus some mdl variation 2 fact insight problem starting formal development describe mdl bayes work some restricted setting break general countable circumvent problem deterministic environment mdl reduces learning elimination four result 2 easily understood consistency mdl source also intelligible general mdl may no longer converge true model give idea model identiﬁcation concentrate predictive performance deterministic mdl elimination learning countable class deterministic sorted k qi say easy see mdl work q model one inﬁnite sequence xq 1 q xq given true observation 1 ℓso far mdl selects simplest q consistent xp 1 ℓand predicts xq potentially q becomes forever inconsistent only prediction wa wrong assume true model p qm since 4 elimination occurs order increasing index qm never make any error mdl make prediction error indeed described classical gold style learning elimination 1 h prediction xq may wrong only xq cause h wrong prediction error revealed note time ℓonly xp ℓis revealed hence total number error bounded bound instance attained class consisting true sequence switch 1 0 observed one wrong prediction get eventually revealed hence wrong qi get eventually eliminated p get eventually selected show number error ﬁnite no bound number error term only possible instance take n time step reveal prediction wrong n chosen arbitrarily large deterministic bayes majority learning bayesian learning time closely related diﬀerent mdl bayes predicts weighted average model rather single one deterministic class bayes similar prediction majority consider model consistent true observation xp 1 ℓ total weight w take weighted majority prediction decision loss bayesian prediction would randomize making wrong prediction mean q contributing least half total weight w get eliminated since p qm never get eliminated wp error hence number error bounded p probabilistic bayesian prediction proper also easy see expected number error bounded p one show bound essentially sharp qi deﬁned digit comma binary expansion reasoning mdl case h 1 multiply bound h get correct prediction eventually no explicit bound anymore comparison ﬂavor result carry some extent probabilistic case abstract level even line reasoning carry although deeply buried sophisticated mathematical analysis latter special deterministic case illustrates complex probabilistic case instance see bayes make only error mdl make error carry probabilistic case also multiplier h 1 h lack explicit bound cf bound 2 reader invited reveal relation not explicitly mentioned diﬀerences follows probabilistic case true p general not identiﬁed anymore bayesian bound trivially follows old classical merging opinion result corresponding mdl bound prove paper diﬃcult obtain consistency mdl source class 5 law large number applied random variable zt log p xt xt implies 1 ℓ pℓ p log p either kl divergence zero case only p logp ℓ ℓ asymptotically mdl doe not select countable reﬁnement argument show mdl eventually selects p reasoning extended ergodic essentially not beyond see limitation come present some troubling example trouble maker instance let p bernoulli process let probability θt still assuming independence suitably converging oscillating inﬁnitely often larger smaller limit sequence θt one show log p converges oscillates around k q p distribution mdl doe not converge not even wrong distribution one idea solve problem partition two distribution partition only asymptotically indistinguishable like p q ask mdl only identify partition approach not succeed generally whatever particular criterion used following reason let p 0 let p q asymptotically indistinguishable p remainder sequence let p q asymptotically distinguishable distribution diﬀerent bernoulli show ergodic source like one asymptotic distinguishability depends drawn sequence ﬁrst observation lead totally diﬀerent future predictive mdl avoids trouble bayesian posterior doe not need verge single true distribution order prediction work something similar mdl time still select single distribution give idea identifying single distribution asymptotically measure predictive success accept inﬁnite oscillation approach taken paper 3 notation main result formal development start section need probability measure ﬁlters inﬁnite sequence conditional probability density total variation distance concept merging opinion order formally state main result measure sequence let ω x f p space inﬁnite sequence natural ﬁltration product f probability measure let inﬁnite sequence sampled true measure except mentioned otherwise probability statement expectation refer p almost surely probability 1 short 1 let ℓbe ﬁrst ℓsymbols ω 6 countable x probability inﬁnite sequence start x p x p x conditional distribution event given x p x x exists probability measure q ω deﬁne q x q analogously general x considered end section convergence total variation p said absolutely continuous relative q written p q 0 implies p 0 p q said mutually singular written iﬀthere exists p q total variation distance tvd q p given x deﬁned p sup q 3 q said predict p tvd merge p p ℓ x note particular implies stronger predictive convergence q ℓ ℓ any not necessarily equal ω famous blackwell dubins convergence result state p absolutely continuous relative q only q merges p p p ℓ x bayesian prediction result immediately utilized bayesian tion let countable ﬁnite inﬁnite class probability measure bayes p wq wq 0 p model assumption p hold obviously p hence bayes merges p p p unlike many bayesian convergence consistency theorem no independence ergodicity stationarity identiﬁability assumption model class need made good convergence rate weaker dh also shown analogous result mdl follows theorem 1 mdl prediction let countable class probability sures x unknown true sampling distribution no dence ergodicity stationarity identiﬁability assumption need made let mdlx arg min q x k q x q measure selected mdl time ℓgiven predictive butions mdlx converge p sense p mdlx ℓ x 7 k q usually interpreted deﬁned length some preﬁx code q case p q k q q chosen complexity bayes rule pr q x x maximum posteriori estimate mapx pr hence theorem also applies map proof theorem surprisingly subtle complex compared ogous bayesian case one reason mdlx x not measure x arbitrary arbitrary x deﬁnitions subtle casual reader satisﬁed countable x skip paragraph consider even generally xt let bt subset xt let fℓ x ℓ smallest containing let ω x f p probability space let pℓbe marginal distribution x ℓ fℓ pℓ predictive distribution p ℓ ℓ version conditional distribution future given past ℓ implicitly deﬁned r p ℓ ℓ dpℓ ℓ similarly deﬁne qℓand qℓfor see detail let measure ωsuch q absolutely continuous see relative q instance bayes ha property deﬁne density derivative qℓ ℓ round bracket measure qℓ square bracket relative mℓ important note essential quantity particular mdlx independent particular choice therefore plainly speak even countable x counting measure qℓ qℓ x coincide q q x following drop sup superscript ℓ since always clear argument note carathodory extension theorem q x uniquely deﬁnes q 4 proof finite model class ﬁrst prove theorem 1 ﬁnite model class need following deﬁnition lemma deﬁnition 2 relation q p any probability measure q p let lebesgue decomposition q relative p absolutely continuous measure qr singular measure g ω q ℓ ℓ version nikodym derivative qr r ag dp ω q ℓ ℓ ω g ω ω ω p ℓ x 8 lebesgue decomposition exists unique resentation derivative limit local density found ℓ ω ℓ ℓ constitute two martingale sequence converge qr implies limit zr derivative indeed doob martingale convergence theorem used prove theorem implies zr g uniquely deﬁned ﬁnite lemma 3 generalized merging opinion any q p following hold p only p ii p implies p ω iii p ω generalizes ii say q x x converges almost surely strictly positive value only p absolutely continuous relative q ii say almost sure positive limit q x x implies q merges iii say even p still p almost every sequence ha positive limit q x x proof recall deﬁnition 2 assume p p 0 implies q r ag dp 0 since g 0 assumption p therefore p assume p choose b p b 1 q b qr r dp 0 implies 0 b p implies p hence p ii p implies p ω 1 celebrated result result follows iii generalizes p 0 reduces ii case p 1 trivial therefore assume 0 p 1 consider measure p conditioned b assume q using r dp 0 get 0 qr r ag dp r dp since g 0 outside implies p p p b p b hence p ii implies p p probability since p also get p together implies 0 p p p p ω claim follows p ω p ω p p p 1 p 1 p p ω 1 intuition behind proof theorem 1 follows mdl cally not select q q x x hence q potentially selected mdl ω hence ω ω p technical 9 diﬃculties ﬁnite eligible q depend sequence ω inﬁnite deal converging infer p proof theorem 1 ﬁnite recall deﬁnition 2 let gq q ωq refer some qm set sequence ω some gq some undeﬁned ha zero hence ignored fix some sequence gq ω deﬁned let mω gq ω mdlx arg min lq x lq x q x k q consider diﬀerence lq x x q x p x k q p gq ω k q p hence ℓq lq x lp x since ﬁnite implies lq x lp x max ℓq q therefore since p mdlx safely ignore q focus q let ωq since p lemma 3 iii also assume ω q gq ω 0 ω q ω ωq p implies p sup p inequality hold ℓ limit hold since ﬁnite since set ω excluded consideration ha measure zero p prof theorem ﬁnite 5 proof countable model class proof previous section crucially exploited ﬁniteness want prove probability mdl asymptotically selects complex q small following lemma establishes probability mdl selects speciﬁc complex q inﬁnitely often small lemma 4 mdl avoids complex probability measure q any q p p q x x inﬁnitly often 10 proof p q x p x p lim q x p x b ce lim ℓ q x p x c 1 ce lim ℓ q x p x c lim ℓ e q x p x e c true deﬁnition limit superior lim b markov inequality c exploit fact limit q x x exists us fatou lemma e obvious suﬃciently complex q lemma 4 implies lq x lp x since convergence q not apply lemma inﬁnitely many complex q directly need lump one proof theorem 1 countable let ordered somehow increasing order complexity k q p qn choose some large let f set complex show probability mdl selects inﬁnitely often complex q small p mdlx inﬁnitely often p mdlx p lq x x p sup qi x p x p qi p q x p x δ p b δ p c ﬁrst three relation follow immediately deﬁnition various tities bound crucial lumping step first bound sup qi x p x qi x qi x p x qi δ q x p x δ x qi q x 1 δ x qi x qi not single measure ωand hence diﬃcult deal q proper probability measure sense step reduces mdl bayes apply lemma 4 b single measure bound c hold suﬃciently large mε p since δ show sequence mdl estimate ℓ ℓ qm probability least 1 hence already proven theorem 1 ﬁnite implies p probability least since convergence hold every ε 0 hold 11 6 implication due generality theorem 1 applied many problem class illustrate some immediate implication theorem 1 forecasting classiﬁcation regression discriminative learning reinforcement learning forecasting classical online sequence prediction concerned predicting sequence ℓfor forecasting farther future possible predicting some h 0 one show 0 see 1 3 hence theorem 1 implies good asymptotic prediction oﬄine learning concerned training predictor ℓfor ﬁxed selling using predictor learning theorem 1 show enough training data prediction good classiﬁcation regression classiﬁcation discrete x regression tinuous x sample set pair yℓ xℓ functional relationship x f conditional probability p shall learned reason apparent swapped usual role x dot indicate x x ℓand assume also follows some distribution start countable model class joint distribution q x contains true joint distribution p x main sult implies mdld x converges true distribution p x indeed sample assumed need invoke general result discriminative learning instead learning generative joint tion p x requires model assumption input tively learn p directly without any assumption not even simply treat oracle q deﬁne x q apply main result leading not yet useful since never known completely conditionally independent write q ℓ q x q x q q taking limit get q q generic property satisﬁed causal process future yt l doe not inﬂuence past observation hence class conditionally independent distribution get since x given not identically distributed classical mdl consistency result source not apply following corollary formalizes ﬁndings corollary 5 discriminative mdl let class discriminative causal distribution q q q x ℓand ℓ 12 regression classiﬁcation typical example assume countable let q discriminative mdl sure time ℓgiven x supa ℓ x p almost surely every sequence ﬁnite conditionally independent x intuitive reason work follows appears ﬁnitely often play asymptotically no role appears inﬁnitely often p learned inﬁnite deterministic result also intelligible every might appear only probing enough function value xt yt allows identify function reinforcement learning rl agent framework agent interacts environment cycle time agent chooses action yt based past experience x past action probability π ty say lead new perception xt probability µ say cycle start let p xy π ty joint interaction probability make no markov stationarity ergodicity assumption µ may pomdps beyond corollary 6 mdl ﬁxed π class vironments let qi qi qℓ p joint corollary follows immediately previous corollary fact qi causal p 1 jointly x reinforcement learning perception xt ot rt consists some regular observation ot reward goal ﬁnd policy maximizes accrued reward long run previous corollary implies corollary 7 mdl value function convergence let vp xy ep future reward sum true value π similarly vqi xy qi mdl value converges true value xy xy any policy proof corollary follows general inequality ep f f sup p inserting f p using corollary since value function probe inﬁnite future really made use convergence result total variation corollary 7 show mdl approximates 13 true value asymptotically arbitrarily well result weaker may appear following policy maximizes estimated mdl value often not good idea since policy doe not explore properly nevertheless reassuring result 7 variation mdl general principle model selection uniquely deﬁned cedure instance crude reﬁned mdl related mml principle static dynamic hybrid way using mdl tion variation setup could deﬁned lookahead prediction product prediction mdli ℓ ℓ mdlx mdli mdli xz x incremental mdl version mdlx mdli static sense allows dynamic hybrid version due incremental nature mdli likely ha better predictive property mdlx conveniently deﬁnes single measure x inconveniently one reason using mdl computationally simpler bayes class mdps mdlx still mdp hence tractable mdli like bayes nightmare deal acknowledgement thanks go peter sunehag useful discussion reference amini habrard ralaivola usunier editor learning data theory algorithm practice lnidd 09 bled slovenia 2009 barron logically smooth density estimation phd thesis stanford versity 1985 barron cover minimum complexity density estimation ieee transaction information theory 1991 blackwell dubins merging opinion increasing information annals mathematical statistic 1962 lugosi prediction learning game cambridge university press 2006 doob stochastic process wiley new york 1953 14 unwald minimum description length principle mit press cambridge 2007 hutter convergence loss bound bayesian sequence prediction ieee transaction information theory 49 8 2003 hutter universal artiﬁcial intelligence sequential decision based algorithmic probability springer berlin 2005 300 page hutter universal prediction bayesian conﬁrmation theoretical puter science 384 1 2007 jebara machine learning discriminative generative springer 2003 kalai lehrer weak strong merging opinion journal mathematical economics 1994 long servedio simon discriminative learning succeed generative learning fails information processing letter 103 4 2007 maher probability capture logic scientiﬁc conﬁrmation cock editor contemporary debate philosophy science chapter 3 page blackwell publishing 2004 poland hutter asymptotics discrete mdl online prediction ieee transaction information theory 51 11 2005 russell norvig artiﬁcial intelligence modern approach hall englewood cliﬀs nj edition 2003 ryabko characterizing predictable class process proc ference uncertainty artiﬁcial intelligence uai 09 montreal 2009 sutton barto reinforcement learning introduction mit press cambridge 1998 wallace statistical inductive inference minimum message length springer berlin 2005 weinberg rosenschein multiagent learning stationary environment proc international joint conf autonomous agent multi agent system aamas 04 page 2004 15