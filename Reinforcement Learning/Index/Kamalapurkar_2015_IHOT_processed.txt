1 jun 2015 1 reinforcement learning approximate optimal tracking rushikesh kamalapurkar lindsey andrew patrick walter warren dixon paper provides approximate online adaptive solution optimal tracking problem nonlinear system unknown drift dynamic reinforcement learning used relax persistence excitation condition forcement learning implemented using concurrent based system identiﬁer simulate experience evaluating bellman error unexplored area state space tracking desired trajectory convergence developed policy neighborhood optimal policy established via stability analysis simulation result demonstrate effectiveness developed technique index learning optimal control driven control nonlinear control system identiﬁcation introduction past decade reinforcement learning rl technique effectively utilized obtain online approximate solution optimal control problem system ﬁnite space stationary environment cf 1 2 however progress system continuous action space ha slow due various technical challenge cf 3 4 various implementation learning strategy solve deterministic optimal regulation problem found result 5 16 ofﬂine online approach solve tracking problem proposed result 17 22 result 18 21 23 solve optimal tracking lem linear nonlinear system online persistence excitation pe error state used establish convergence general impossible guarantee pe priori hence probing signal designed using trial error added controller ensure pe however probing signal not considered stability analysis paper objective employ rl design online approximate optimal tracking controller uncertain nonlinear system relaxed ﬁnite excitation condition rushikesh kamalapurkar lindsey andrew patrick walter warren dixon department mechanical aerospace engineering university florida gainesville fl usa email rkamalapurkar wdixon research supported part nsf award number 1161260 1217908 onr grant number contract afrl mathematical modeling optimization institute any opinion ﬁndings conclusion recommendation expressed material author not necessarily reﬂect view sponsoring agency submitted special issue new development neural network structure signal processing autonomous decision adaptive control rl system continuous state action space realized via value function approximation value function corresponding optimal control problem approximated using parametric universal approximator control policy generally derived approximate value function hence obtaining good approximation value function critical stability system trajectory tracking problem value function depends explicitly time since universal function approximators approximate function arbitrary accuracy only compact domain value function optimal tracking problem not approximated arbitrary accuracy 17 23 desired trajectory expressed output autonomous dynamical system value function expressed stationary function state desired trajectory hence universal function approximators employed approximate value function arbitrary accuracy using system state augmented desired trajectory training input cf 17 21 23 technical challenge associated tonomous nature trajectory tracking problem dressed author previous work 23 established matching condition desired trajectory optimal trajectory tracking problem reformulated stationary optimal control problem since value function associated stationary optimal trol problem approximated using traditional function approximation technique aforementioned reformulation 23 requires tation tracking controller depends system model hence development 23 requires exact model knowledge obtaining accurate estimate desired controller injecting resulting estimation error stability analysis major technical challenge extending work 23 uncertain system paper preliminary work 24 concurrent learning cl system identiﬁer used estimate desired controller rl used simulate experience evaluating bellman error unexplored area state space 24 27 error actual controller estimate included stability analysis formulating equation term actual controller effectiveness developed technique demonstrated via numerical simulation main contribution work include 1 2 imate model inversion using system identiﬁer approximate desired controller presence uncertainty drift dynamic 2 implementation rl relax pe condition ﬁnite excitation condition 3 simulation result demonstrate tion optimal policy without added exploration signal ii problem formulation exact solution consider control afﬁne system described tial equation x f x g x u x denotes state u denotes control input f rn g rn locally lipschitz continuous function denote drift dynamic control effectiveness control objective optimally track desired trajectory xd facilitate subsequent control development error signal e deﬁned e since control input required system track desired trajectory general not identically zero optimal control problem formulated term quadratic cost function containing e u always result inﬁnite cost address issue alternative cost function formulated term tracking error mismatch actual control signal desired control 17 21 23 following assumption facilitate determination desired control assumption 1 23 function g bounded matrix g x ha full column rank x function rn deﬁned gt bounded locally lipschitz assumption 2 23 desired trajectory bounded known positive constant exists locally lipschitz function hd rn xd hd xd g xd xd hd xd xd hd xd xd based assumption 1 2 control policy ud rn required system track desired trajectory xd expressed ud xd hd xd fd xd xd error actual control signal desired state control signal deﬁned µ xd using µ system dynamic expressed autonomous form ζ f ζ g ζ µ 1 concatenated state ζ deﬁned ζ et xt function f g deﬁned f ζ e xd ut xd gt e xd ht notational brevity unless otherwise speciﬁed domain function assumed furthermore suppressed equation deﬁnitions example trajectory x deﬁned abuse notation x unless otherwise iﬁed equation form f h g x interpreted f h g x g ζ e xd control error µ treated hereafter design variable control objective solve optimal regulation problem online simultaneously synthesize utilize control signal µ online minimize cost functional j ζ µ ˆ r ζ τ µ τ dτ dynamic constraint ζ f ζ g ζ µ tracking desired trajectory r local cost deﬁned r ζ µ e µt rµ r positive deﬁnite symmetric matrix constant q rn continuous positive deﬁnite function assuming optimal policy exists optimal policy characterized term value function v deﬁned v ζ min µ τ ˆ r φµ τ ζ µ τ dτ u rm action space notation φµ denotes trajectory ζ f ζ g ζ µ control signal µ initial condition initial time assuming minimizing policy exists v continuously differentiable solution optimal policy obtained 28 ζ ζ ζ optimal policy optimal value function satisfy hjb equation 28 ζv ζ f ζ ζ ζ ζ ζ ζ 2 initial condition v 0 0 function q deﬁned q xt q e xd remark assumption 1 2 eliminated discounted cost optimal tracking problem considered instead total cost problem considered article discounted cost tracking problem considers value function form v ζ min u τ ˆ eκ r φu τ ζ u τ dτ κ r 0 constant discount factor control effort u minimized instead control error µ 3 control effort required system perfectly track desired trajectory generally nonzero even initial system state desired trajectory hence general optimal value function discounted cost problem doe not satisfy v 0 online rl technique generally analyzed using optimal value function candidate lyapunov function since optimal value function discounted cost problem doe not evaluate zero origin not used candidate lyapunov function hence analyzing stability discounted cost optimal controller learning phase complex iii bellman error since solution hjb generally infeasible obtain approximate solution sought approximate solution optimal value function v replaced parametric estimate ˆ v ζ ˆ wc optimal policy parametric estimate ˆ µ ζ ˆ wa ˆ wc ˆ wa denote vector estimate ideal parameter objective critic learn parameter ˆ wc objective actor learn parameter ˆ wa substituting estimate ˆ v ˆ µ v hjb equation respectively yield residual error δ rl rl called deﬁned δ ζ ˆ wc ˆ wa q ζ ˆ µt ζ ˆ wa rˆ µ ζ ˆ wa ˆ v ζ ˆ wc f ζ g ζ ˆ µ ζ ˆ wa 3 speciﬁcally solve optimal control problem critic aim ﬁnd set parameter ˆ wc actor aim ﬁnd set parameter ˆ wa δ ζ ˆ wc ˆ wa ˆ u ζ ˆ wa ζ ˆ v ζ ˆ wa ζ since exact basis value function approximation generally not available approximate set parameter minimizes sought particular ensure uniform approximation value function policy compact operating domain c desirable ﬁnd parameter minimize error e rl rl deﬁned e ˆ wc ˆ wa δ ζ ˆ wc ˆ wa computation error e computation control signal u require knowledge system drift dynamic two prevalent approach employed render control design robust uncertainty system drift dynamic integral rl cf 15 29 state derivative estimation cf 12 23 integral rl exploit fact 0 3 ha equivalent integral form δint ˆ wc ˆ wa ˆ v φˆ µ ˆ wc ˆ r φˆ µ τ ˆ µ φˆ µ τ ˆ wa dτ v φˆ µ ˆ wc since integral form doe not require model knowledge policy designed based δint implemented without knowledge state derivative technique exploit fact 3 expressed δd ζ ζ ˆ wa ˆ wc ˆ v ζ ˆ wc ζ q ζ ˆ µt ζ ˆ wa rˆ µ ζ ˆ wa hence estimate computed without model knowledge estimate derivative ζ available adaptive derivative estimator 30 could used estimate ζ online integral form inherently dependent state trajectory since adaptive derivative estimator approximate derivative only along trajectory derivative technique also dependent state trajectory hence technique 12 15 23 29 only evaluated along system trajectory thus error e approximated instantaneous integral error ˆ e ˆ φˆ µ τ ˆ wc ˆ wa dτ intuitively ˆ e approximate e operating domain state trajectory φˆ µ need visit many point operating domain possible intuition formalized fact technique 12 15 23 29 31 require pe achieve convergence pe condition relaxed 15 ﬁnite excitation condition using integral rl along experience replay evaluation δint interpreted gained experience experience stored history stack repeatedly used learning algorithm improve data efﬁciency paper different approach used improve data ﬁciency dynamic system identiﬁer developed generate parametric estimate ˆ f ζ ˆ θ drift dynamic f ˆ θ denotes estimate matrix unknown parameter given ˆ f ˆ v ˆ µ estimate evaluated any ζ using ˆ f experience simulated extrapolating unexplored point operating domain hence identiﬁer developed ˆ f approach f exponentially fast learning law optimal policy utilize simulated experience along experience gained stored along state trajectory parametric approximators used approximate f convergence ˆ f f implied convergence parameter unknown ideal value well known adaptive system identiﬁers require pe achieve parameter 4 convergence relax pe condition cf 24 27 system identiﬁer us recorded data learning developed following section iv system identification any compact set c function f represented using neural network nn f x θt σf ǫθ x xt θ denote constant unknown nn weight σf rp denotes bounded nn basis function ǫθ rn denotes function reconstruction error p denotes number nn neuron using universal function approximation property single layer nns given constant matrix row σf form proper basis exist constant ideal weight θ known constant θ ǫθ θ x x θ denotes frobenius norm 32 using estimate ˆ θ weight matrix θ function f approximated function ˆ f deﬁned ˆ f ζ ˆ θ θt σθ ζ 4 σθ deﬁned σθ ζ σf 1 et xt estimator online ﬁcation drift dynamic developed ˆ x ˆ θt σθ ζ g x u x 5 x x k positive constant learning gain assumption 3 26 history stack containing recorded action pair xj uj along numerically computed state derivative xj satisﬁes λmin x σfjσt fj σθ 0 xj available priori σfj xt j known positive constant xj f xj g xj uj λmin denotes minimum weight estimate ˆ θ updated using following based update law ˆ x σfj xj θtσfj 6 priori availability history stack used ease exposition not necessary provided system state exciting ﬁnite time interval versus traditional approach history stack also recorded online controller developed 23 used time interval history stack recorded controller developed result used thereafter use two different controller result switched system one switching event since only one switching event stability switched system follows stability individual subsystem kθ constant positive cl gain γθ constant diagonal positive deﬁnite tion gain matrix using 4 3 approximated ˆ δ ζ ˆ θ ˆ wc ˆ wa q ζ ˆ µt ζ ˆ wa rˆ µ ζ ˆ wa ˆ v ζ ˆ wa fθ ζ ˆ θ ζ g ζ ˆ µ ζ ˆ wa 7 fθ ζ ˆ θ θt σθ ζ x xd ˆ θt σθ xd 0 ζ g e xd xd hd hd value function approximation since v function state ζ mization problem stated section ii intractable obtain minimization problem optimal value function represented any compact operating domain c using nn v ζ w σ ζ ǫ ζ w denotes vector unknown nn weight σ rl denotes bounded nn basis function ǫ denotes function reconstruction error l denotes number nn neuron using universal function approximation property single layer nns any compact set c exist constant ideal weight w known positive constant w ǫ ζ ζ 32 nn representation optimal policy obtained ζ ζ ζ w ζ 8 using estimate ˆ wc ˆ wa ideal weight w timal value function optimal policy approximated ˆ v ζ ˆ wc w c σ ζ ˆ µ ζ ˆ wa ζ ζ ˆ wa 9 optimal control problem thus reformulated need ﬁnd set weight ˆ wc ˆ wa online minimize error ˆ eˆ θ ˆ wc ˆ wa ˆ δ ζ ˆ θ ˆ wc ˆ wa given ˆ θ simultaneously improving ˆ θ using 6 ensuring stability system using control law u ˆ µ ζ ˆ wa ˆ ud ζ ˆ θ 10 ˆ ud ζ ˆ θ hd θt σθd σθd xt error ud ˆ ud included stability analysis based fact 5 error trajectory generated system e f x g x u xd controller 10 identical error trajectory generated system ζ f ζ ζ µ control law µ ˆ µ ζ ˆ wa θt σθd ǫθd 11 ǫθd xd vi simulation experience since computation supremum ˆ eˆ θ intractable general simulation experience implemented imizing squared sum ﬁnitely many point state space following assumption facilitates aforementioned approximation assumption 4 24 exists ﬁnite set point ζi 1 n constant c 0 c n inf λmin n x ωiωt ρi ρi νωt γωi ωi ζi fθ ζi ˆ θ ζi g ζi ˆ µ ζi ˆ wa using assumption 4 simulation experience mented weight update law ˆ wc ρ ˆ δt n γ n x ωi ρi ˆ δti 12 γ βγ γ 1 13 ˆ wa ˆ wa wc ˆ wa σ ˆ waωt n x σi ˆ waωt ˆ wc 14 ω ζ fθ ζ ˆ θ ζ g ζ ˆ µ ζ ˆ wa γ gain matrix γ denotes positive saturation constant β denotes constant forgetting factor denote constant itive adaptation gain 1 denotes indicator function set gσ ζ g ζ ζ ζ ρ νωt γω ν positive normalization constant 12 14 subsequent development any function ξ ζ notation ξi deﬁned ξi ζi instantaneous ˆ δt ˆ δti given ˆ δt ˆ δ ζ ˆ wc ˆ wa ˆ θ ˆ δti ˆ δ ζi ˆ wc ˆ wa ˆ θ vii stability analysis state penalty function q positive deﬁnite optimal value function v positive deﬁnite serf lyapunov function concatenated system optimal control policy hence v used cf 11 12 29 candidate lyapunov function loop system policy ˆ function q hence function v positive semideﬁnite hence function v not valid candidate lyapunov function however result 23 used show nonautonomous form optimal value function denoted v rn r deﬁned v e v e xd positive deﬁnite decrescent hence v 0 0 r exist class k function v r v r v e 15 e facilitate stability analysis candidate lyapunov function rn selected x θ 1 2 xt x 1 θt θ θ 16 θ θ tr denotes trace matrix using 5 6 following bound time derivative established θ 2 f ǫθ kθdθ θ f 17 dθ x x concatenated state z deﬁned z et w c w xt vec θ candidate lyapunov function deﬁned vl z e 1 2 w c wc 1 2 w wa θ x 18 vec denotes vectorization operator deﬁned 16 saturated update law 13 ensures exist positive constant γ γ γ 19 using 16 bound 19 15 fact tr θt θ θ vec θ θ vec θ candidate lyapunov function 18 bounded vl z 20 z vl r r vl r class k function notational brevity dependence function f g σ ǫ σθ ǫθ g system state suppressed hereafter facilitate stability analysis approximate 6 7 expressed term weight estimation error ˆ δt wc θ 1 4 w gσ wa 21 θ ζ θ ǫθ given any compact set χ containing open ball radius ρ centered origin positive constant ι deﬁned ι 3 w νγ w 4 2 3 w σg kθdθ 2 2 ǫθd 1 1 w ǫθd 22 gr gǫ let vl r class k function vl 2 8 wc 2 6 wa 2 k 4 kθσθ 6 vec θ 2 23 sufﬁcient gain condition used subsequent theorem 1 l ι ρ 24 3 2 w 25 3 3 2 26 22 26 any function rl l notation denotes σg sufﬁcient condition 24 requires set χ large enough based constant since nn approximation error depend compact set χ general ﬁxed number nn neuron constant ι increase size set however ﬁxed set χ constant ι reduced reducing function reconstruction error increasing number nn neuron increasing learning gain provided σθ large enough hence sufﬁcient number nn neuron extrapolation point required satisfy condition 24 theorem provided assumption hold control gain number nn neuron extrapolation point selected based 24 26 controller 10 along weight update law 12 14 identiﬁer 5 along weight update law 6 ensure system state remain bounded tracking error ultimately bounded control policy ˆ µ converges borhood around optimal control policy proof using 1 fact v e v ζ candidate lyapunov function 18 vl f w c ˆ 2 w c wc w ˆ wa 27 using 2 8 9 11 expression 27 bounded vl ζ w c ˆ 2 w c w ˆ wa 1 2 gσ wa w θt σθd θt σθd 1 1 w ǫθd ǫθd 28 using update law 12 14 bound 17 21 expression 28 bounded vl ζ n x w c n ωiωt ρi wc θ 2 f w wa w c ω ρ w θ w c ω ρ w wc w w 1 w c ω ρ w gσ wa n x w c n ωi ρi w θi n x 1 4 w c n ωi ρi w gσi wa w c n n x ωi ρi w σ ˆ waωt n x σi ˆ waωt ˆ wc ǫθ kθdθ θ f 1 2 gσ wa w θt σθd θt σθd 1 1 w ǫθd ǫθd segregation term completion square use young inequality yield vl ζ 4 wc 2 3 wa 2 2 3 θ 2 f 4 2 w wc 2 7 3 wa 2 1 2 wa 2 3 w νγ w 4 2 3 w σg kθdθ 2 2 1 1 w ǫθd ǫθd 29 z provided sufﬁcient condition 25 26 satisﬁed expression 29 yield vl l ι 30 using 20 24 30 theorem 33 invoked conclude every trajectory z satisfying ρ bounded satisﬁes lim l ι viii conclusion based implementation based rl developed obtain approximate online tion inﬁnite horizon optimal tracking problem nonlinear system desired state controller used facilitate formulation feasible optimal control problem system state augmented desired trajectory facilitate formulation tionary optimal control problem system identiﬁer developed remove dependence desired state controller system drift dynamic facilitate simulation experience via extrapolation simulation result provided demonstrate effectiveness developed technique similar pe condition online optimal control literature assumption 4 not general anteed priori however assumption 4 heuristically met oversampling selecting n furthermore unlike pe satisfaction assumption 4 monitored online hence algorithm employed preserve rank selecting new point minimum singular value fall certain threshold provided minimum singular value doe not decrease switch trajectory resulting switched system shown uniformly bounded using common lyapunov function formulation sufﬁcient condition assumption 4 veriﬁed priori topic future research reference 1 sutton barto reinforcement learning introduction cambridge usa mit press 1998 2 bertsekas dynamic programming optimal control athena scientiﬁc 2007 3 mehta meyn pontryagin minimum ple proc ieee conf decis control 2009 pp 3598 4 deisenroth efﬁcient reinforcement learning using gaussian ce kit scientiﬁc publishing 2010 5 doya reinforcement learning continuous time space neural vol 12 no 1 pp 2000 6 lewis huang policy iteration equation feedback control input saturation ieee trans autom control vol 51 no 12 pp 1995 dec 2006 7 padhi unnikrishnan wang balakrishnan single network adaptive critic snac architecture optimal control si class nonlinear system neural vol 19 no 10 pp 2006 8 lewis linear hjb solution using approximate dynamic programming gence proof ieee trans syst man cybern part b vol 38 pp 2008 9 chen jagannathan generalized formulation neural network control afﬁne nonlinear time system ieee trans neural vol 19 no 1 pp 2008 10 dierks thumati jagannathan optimal control unknown afﬁne nonlinear system using neural work proof convergence neural vol 22 no pp 2009 11 vamvoudakis lewis online algorithm solve inﬁnite horizon optimal control problem automatica vol 46 no 5 pp 2010 12 bhasin kamalapurkar johnson vamvoudakis lewis dixon novel architecture imate optimal control uncertain nonlinear system automatica vol 49 no 1 pp 2013 13 zhang liu luo wang adaptive dynamic ming control algorithm stability ser communication control engineering london 2013 14 liu wei policy iteration adaptive dynamic programming algorithm nonlinear system ieee trans neural netw learn vol 25 no 3 pp mar 2014 15 modares lewis integral forcement learning experience replay adaptive optimal control system matica vol 50 no 1 pp 2014 16 yang liu wang reinforcement learning adaptive optimal control unknown nonlinear system input constraint int control vol 87 no 3 pp 2014 17 zhang wei luo novel optimal tracking control scheme class nonlinear system via greedy hdp iteration algorithm ieee trans syst man cybern part b vol 38 no 4 pp 2008 18 dierks jagannathan optimal tracking control afﬁne nonlinear system unknown internal dynamic proc ieee conf decis control 2009 pp 19 zhang cui zhang luo robust imate optimal tracking control unknown general nonlinear system using adaptive dynamic programming method ieee trans neural vol 22 no 12 pp 2011 20 wei liu optimal tracking control scheme nonlinear system approximation error advance neural network isnn 2013 ser lecture note computer science guo hou zeng ed springer berlin heidelberg 2013 vol 7952 pp 21 kiumarsi lewis modares karimpour reinforcement optimal tracking control linear system unknown dynamic automatica 2014 22 qin zhang luo online optimal tracking control linear system unknown dynamic using tive dynamic programming international journal control vol 87 no 5 pp 2014 23 kamalapurkar dinh bhasin dixon approximate optimal trajectory tracking nonlinear system automatica vol 51 pp january 2015 24 kamalapurkar andrew walter dixon based reinforcement learning approximate optimal tracking proc ieee conf decis control 2014 pp 8 25 chowdhary concurrent learning adaptive control convergence without persistencey excitation dissertation georgia institute technology december 2010 26 chowdhary johnson theory validation adaptive controller guid control vol 34 no 2 pp march 2011 27 chowdhary yucelen mühlegg johnson concurrent learning adaptive control linear system exponentially convergent bound int adapt control signal vol 27 no 4 pp 301 2013 28 kirk optimal control theory introduction dover 2004 29 lewis vrabie syrmos optimal control ed wiley 2012 30 bhasin kamalapurkar dinh dixon robust state derivative estimation nonlinear system ieee trans autom control vol 58 no 1 pp 2013 31 modares lewis optimal tracking control nonlinear system using integral ment learning automatica vol 50 no 7 pp 1780 1792 2014 32 lewis neural network control robot manipulator nonlinear system crc press 1999 33 khalil nonlinear system ed upper saddle river nj usa prentice hall 2002