machine learning physical science giuseppe carleo center computational quantum physic flatiron institute 162 avenue new york ny 10010 ignacio cirac fur quantenoptik 1 garching germany kyle cranmer center cosmology particle physic center data science new york university 726 broadway new york ny 10003 usa laurent daudet lighton 2 rue de la bourse paris france maria schuld university durban 4000 south africa national institute theoretical physic durban 4000 south africa xanadu quantum computing 777 bay street toronto canada naftali tishby hebrew university jerusalem edmond safra campus jerusalem 91904 israel leslie department chemistry new york university new york ny 10003 usa lenka zdeborová institut de physique théorique université paris saclay cnrs cea machine learning encompasses broad range algorithm modeling tool used vast array data processing task ha entered scientiﬁc discipline recent year review selective way recent research interface machine learning physical science includes conceptual development chine learning ml motivated physical insight application machine learning technique several domain physic two ﬁelds giving basic notion machine learning method principle describe amples statistical physic used understand method ml move 6 dec 2019 2 describe application ml method particle physic cosmology quantum many body physic quantum computing chemical material physic also highlight research development novel computing architecture aimed ating ml section describe recent success well methodology challenge content introduction 3 concept machine learning 4 supervised learning neural network 4 unsupervised learning generative modelling 6 reinforcement learning 6 ii statistical physic 7 historical note 7 theoretical puzzle deep learning 7 statistical physic unsupervised learning 8 contribution understanding basic unsupervised method 8 restricted boltzmann machine 9 modern unsupervised generative modelling 9 statistical physic supervised learning 10 perceptron glms 10 physic result neural network 10 information bottleneck 11 landscape glassiness deep learning 11 application ml statistical physic 12 outlook challenge 12 iii particle physic cosmology 13 role simulation 13 classiﬁcation regression particle physic 14 jet physic 15 neutrino physic 15 robustness systematic uncertainty 16 triggering 16 theoretical particle physic 17 classiﬁcation regression cosmology 17 photometric redshift 17 gravitational lens ﬁnding parameter estimation 18 example 18 inverse problem inference 19 inference 20 example particle physic 20 example cosmology 21 generative model 22 outlook challenge 22 iv quantum matter 22 quantum state 23 representation theory 24 learning data 24 variational learning 25 speed simulation 25 classifying quantum phase 26 synthetic data 26 experimental data 27 tensor network machine learning 27 outlook challenge 28 quantum computing 29 quantum state tomography 29 controlling preparing qubits 30 error correction 31 vi chemistry material 31 energy force based atomic environment 32 potential free energy surface 32 material property 33 3 electron density density functional theory 34 data set generation 34 outlook challenge 35 vii ai acceleration classical quantum hardware 36 beyond von neumann architecture 36 neural network running light 36 revealing feature data 36 machine learning 37 outlook challenge 38 viii conclusion outlook 38 acknowledgement 38 reference 39 introduction past decade ha seen prodigious rise learning ml based technique impacting many area industry including autonomous driving nance manufacturing energy harvesting ml largely perceived one main disruptive nologies age much computer 1980 1990 general goal ml recognize pattern data inform way unseen problem treated example highly complex system car vast amount data coming sensor turned decision control car computer ha learned recognize pattern danger success ml recent time ha marked ﬁrst signiﬁcant improvement some existing technology example ﬁeld image tion large extent advance constituted ﬁrst demonstration impact ml method specialized task recently application ditionally inaccessible automated software successfully enabled particular deep learning nology demonstration reinforcement learning technique game playing example ha deep impact perception whole ﬁeld wa moving step closer expected general artiﬁcial intelligence parallel rise ml technique industrial plication scientist increasingly become interested potential ml fundamental research physic no exception some extent not surprising since ml physic share some method well goal two discipline concerned process gathering analyzing data design model predict behaviour complex system however ﬁelds prominently diﬀer author gcarleo corresponding author way fundamental goal realized one hand physicist want understand mechanism nature proud using knowledge intelligence intuition inform model hand machine learning mostly doe site model agnostic machine provides intelligence extracting data although ten powerful resulting model notoriously known opaque understanding data tern machine learning tool physic therefore welcomed enthusiastically some eyed suspicion others diﬃcult deny produce surprisingly good result some case review attempt providing coherent lected account diverse intersection ml physic speciﬁcally look ample spectrum ﬁelds ranging statistical quantum physic high energy cosmology ml recently made prominent appearance discus potential application challenge intelligent data mining technique diﬀerent context start review ﬁeld statistical physic section ii interaction machine learning ha long history drawing method physic provide better understanding problem machine learning turn wheel direction using machine learning physic section iii treat progress ﬁelds physic cosmology section iv review ml idea helping understand mystery quantum system section v brieﬂy explore promise machine learning within quantum computation section vi highlight some amazing advance computational chemistry material design due ml application section vii discus some vances instrumentation leading potentially ware adapted perform machine learning task conclude outlook section viii 4 concept machine learning purpose review brieﬂy explain some fundamental term concept used machine learning reading recommend source some targeted especially physic audience historical overview development ﬁeld recommend ref lecun et 2015 schmidhuber 2014 excellent recent introduction machine learning physicist ref mehta et 2018 includes notebook tical demonstration useful online resource florian marquardt course machine learning cists 1 useful textbook written machine learning researcher christopher bishop standard textbook bishop 2006 well goodfellow et 2016 focus theory foundation deep learning cover many aspect research ety online tutorial lecture useful get basic overview get started topic learn theoretical progress made tical physic neural network ommend rather accessible book statistical ic learning engel van den broeck 2001 learning detail replica method use puter science information theory machine learning would recommend book nishimori nishimori 2001 recent statistical physic ogy textbook mézard montanari excellent reference mézard montanari 2009 get basic idea type problem chine learning able tackle useful deﬁned three large class learning problem supervised learning unsupervised learning reinforcement learning also allow u state basic terminology ing basic equipment expose some basic tool machine learning supervised learning neural network supervised learning given set n sample data let u denote one sample xµ µ 1 something concrete mind xµ could instance photograph animal p number pixel sample xµ given label yµ commonly label could encode instance specie animal photograph goal supervised learning ﬁnd function f new sample xnew presented without label output function f xnew approximates well label data 1 see set xµ yµ n called training set order test resulting function f one usually split available data sample training set used learn function test set evaluate performance let u describe training procedure monly used ﬁnd suitable function commonly function expressed term set parameter called weight w leading fw one structs loss function l fw xµ yµ sample µ idea loss small fw xµ yµ close vice versa average loss training set called empirical risk r fw pn l fw xµ yµ training procedure weight w adjusted order minimize empirical risk training error measure well minimization achieved notion error important generalization error related performance predicting label ynew data sample xnew not seen training set application mon practice build test set randomly picking fraction available data perform training using remaining fraction training set note part literature generalization error diﬀerence performance test set one training set algorithm commonly used minimize empirical risk function weight based dient descent respect weight mean weight iteratively adjusted direction gradient empirical risk wt fw 1 rate γ performed called ing rate commonly used successful variant gradient descent stochastic gradient descent sgd full empirical risk function r placed contribution sample subset sample called small single sample physic term sgd algorithm often compared langevin dynamic ﬁnite temperature langevin dynamic zero perature gradient descent positive temperature introduces thermal noise certain way similar noise arising sgd diﬀerent others many variant sgd algorithm used practice initialization weight change performance practice choice learning rate variety regularization term weight decay penalizing weight tend converge large absolute value choice right version algorithm important many heuristic rule thumb certainly theoretical insight question would desirable one typical example task supervised learning classiﬁcation label yµ take value 5 discrete set accuracy sured fraction time learned function siﬁes data point correctly another example gression goal learn tion accuracy typically measured term error true label learned estimate example would learning input label vector dimension larger one many method supervised learning many variant one basic supervised learning method widely known used linear gression function fw x parameterized form fw xµ xµw w data live high dimensional space number sample not much larger dimension indispensable use regularized form linear regression called ridge regression tikhonov regularization ridge sion formally equivalent assuming weight w gaussian prior generalized form linear regression parameterization fw xµ g xµw g some output channel function also often used property described section another popular way regularization based rating example n classiﬁcation task separate category divided clear gap wide possible idea stand behind deﬁnition support vector machine method rather powerful generalization ridge regression kernel ridge regression kernel ridge regression closely related gaussian process sion support vector machine method often bined kernel method still method many application especially number available sample not large another classical supervised learning method based decision tree decision tree used go observation data sample represented branch conclusion item target value represented leaf best known application decision tree physical science data analysis particle accelerator discussed sec supervised learning method stand behind machine learning revolution past decade neural network ffnn also sometimes called perceptrons also relevant method purpose review shall describe brieﬂy fully nected neural network function fw xµ terized follows fw xµ g l w l g 2 w 2 g 1 w 1 xµ 2 w w 1 w l l w p rl matrix weight ri 1 called width hidden layer function g 1 activation function act wise vector note input activation function often slightly generic aﬃne transforms output previous layer simply matrix multiplication including bias number layer l called network depth neural network depth larger some small integer called deep neural network subsequently machine learning based deep neural network called deep learning theory neural network tell u without hidden layer l 1 corresponding generalized linear regression set function proximated way limited minsky papert 1969 hand already one hidden layer l 2 wide enough large enough function g 1 general class function well approximated principle benko 1989 theory however not tell u optimal set parameter activation function width layer depth der learning w 1 w l tractable eﬃciently know empirical success past decade many task interest tractable deep neural network using gradient descent sgd algorithm deep neural network derivative respect weight computed using chain rule leading celebrated algorithm take care eﬃciently scheduling operation required compute gradient goodfellow et 2016 important powerful variant deep forward neural network convolutional neural network goodfellow et 2016 input hidden unit obtained via ﬁlter plied small part input space ﬁlter shifted diﬀerent position corresponding diﬀerent hidden unit convolutional neural network implement invariance translation particular suitable analysis image compared fully connected ral network layer convolutional neural network ha much smaller number parameter tice advantageous learning algorithm many type variance convolutional neural work among mention residual neural network resnets use shortcut jump some er next neural network called recurrent neural network rnn put unit feed back input next time step rnns result thus given set weight also whole temporal sequence state due intrinsically dynamical nature rnn larly suitable learning temporal data set speech language time series many type variant rnns one caused excitement past decade arguably 6 long memory lstm network hochreiter schmidhuber 1997 lstms deep variant task speech processing music composition natural language processing unsupervised learning generative modelling unsupervised learning class learning problem input data obtained supervised learning no label available goal learning recover some underlying possibly structure dataset typical example vised learning data clustering data point assigned group way every group ha some common property unsupervised learning one often seek probability distribution generates sample statistically similar observed data sample often referred generative modelling some case ity distribution written explicit form plicitly implicitly parameterized generative model internally contain latent variable source domness number latent variable much smaller dimensionality data speak dimensionality reduction one path towards vised learning search value latent variable maximize likelihood observed data range application likelihood associated observed data not known computing self intractable case some generative model discussed oﬀer alternative free path section also discus called abc method type ference turn useful many context arising physical science basic method unsupervised learning include cipal component analysis variant cover some theoretical insight method tained using physic section physically appealing method unsupervised learning called boltzmann machine bm bm basically inverse ising model data sample seen sample boltzmann distribution teracting ising model goal learn value interaction magnetic ﬁelds likelihood probability boltzmann measure observed data large restricted boltzmann machine rbm particular case bm two kind variable visible unit see input data hidden unit interact eﬀective coupling interaction case only visible hidden unit adjusted order likelihood observed data large given appealing interpretation term physical model application bm rbms widespread several physic domain discussed section neat idea perform unsupervised learning yet able use method algorithm oped supervised learning toencoder neural network ha input data input also output aim reproduce data typically going trough tleneck sense some intermediate layer small width compared dimensionality data idea autoencoder ing ﬁnd succinct representation data still keep salient feature sample tional autoencoders vae kingma welling 2013 rezende et 2014 combine variational inference autoencoders provide deep generative model data trained unsupervised fashion approach unsupervised learning worth mentioning adversarial generative network gans goodfellow et 2014 gans attracted substantial attention past year constitute another fruitful way take advantage progress made supervised learning unsupervised one gans typical use two neural network one called generator another called tor generator network used generate output random input designed put look like observed sample discriminator network used discriminate true data ples sample generated generator network discriminator aiming best possible accuracy classiﬁcation task whereas generator network adjusted make accuracy discriminator smallest possible gans currently art system many application image processing interesting method model distribution clude normalizing ﬂows autoregressive model advantage tractable likelihood trained via maximum likelihood larochelle murray 2011 papamakarios et 2017 uria et 2016 hybrid supervised learning unsupervised learning important application include supervised learning only some label available active learning label acquired lected set data point certain cost reinforcement learning reinforcement learning sutton barto 2018 area machine learning artiﬁcial agent take action environment goal maximizing reward action change state ment some way agent typically observes some information state environment corresponding reward based observation 7 agent decides next action reﬁning strategy action choose order maximize ing reward type learning designed case only way learn property environment interact key concept forcement learning exploitation good strategy found far exploration order ﬁnd yet better strategy also note reinforcement learning intimately related ﬁeld theory control especially optimal control theory one main type reinforcement learning plied many work learning based value matrix q assigns quality given action environment given state value function q iteratively reﬁned cent advanced application set state action large impossible even store whole matrix case deep neural network used represent function succinct manner give rise deep recent example success inforcement learning computer program alphago alphago zero ﬁrst time history reached performance traditional board game go another well known use reinforcement learning locomotion robot ii statistical physic historical note machine learning tool physic research relatively new phenomenon fertilization two discipline date back much especially statistical physicist made important contribution theoretical understanding ing term statistical unmistakably suggests connection statistical mechanic learning theory started statistical learning amples took logic rule based ai mid two seminal paper marked transformation valiant theory learnable valiant 1984 opened way rigorous statistical learning ai hopﬁeld neural network model associative ory hopﬁeld 1982 sparked rich application concept spin glass theory neural network model wa marked memory capacity culation hopﬁeld model amit gutfreund sompolinsky amit et 1985 following work much tighter application learning model wa made seminal work elizabeth gardner applied replica trick gardner 1987 1988 calculate umes weight space simple neural network supervised unsupervised learning model gardner method enabled explicitly calculate ing curve typical training generalization error function number training example speciﬁc one neural network gyi tishby 1990 seung et sompolinsky et 1990 analytic statistical physic tions demonstrated learning dynamic hibit much richer behavior predicted case distribution free pac bound pac stand ably approximately correct valiant 1984 ular learning exhibit phase transition poor good generalization györgyi 1990 rich ing dynamic curve appear many machine learning problem wa shown various model see recent review zdeborová krzakala 2016 statistical physic learning reached peak early rather minor inﬂuence learning practitioner theorist focused general generalization bound characterized dimension rademacher complexity hypothesis class theoretical puzzle deep learning machine learning new millennium wa marked much larger scale learning problem size moved hundred million mensionality training data size number adjustable parameter wa dramatically strated return large scale neural network model many hidden layer known deep neural network deep neural network essentially convolution ral network proposed already much larger scale input big clean training data trick hack network started beat many diﬀerent pattern recognition machine learning competition roughly 2010 amazing performance deep learning trained old stochastic gradient descent sgd propagation algorithm took everyone surprise one puzzle existing learning theory based generalization bound unable explain phenomenal success ing theory doe not predict deep network adjustable way higher number training sample good generalization property lack theory wa coined classical article zhang et 2016 author show numerically neural network used classiﬁcation able classify perfectly randomly generated label case ing learning theory doe not provide any useful bound 8 generalization error yet practice observe good generalization deep neural network trained true label continuing open question not good understanding learning problem putationally tractable particularly important since point view computational complexity theory learning problem encounter worst case another open question central current deep learning concern choice architecture far guided lot combined impressive rience researcher time tions ml spreading many domain ﬁeld call systematic approach current basic question minimal number sample need order able learn given task good precision entirely open time current literature deep ing ﬂourishing interesting numerical observation experiment call explanation physic audience situation could perhaps compared fundamental physic quantum mechanic wa developed ﬁeld wa full unexplained experiment evading ing theoretical understanding clearly perfect time some physic idea study neural network resurrect revisit some current question direction machine learning given long history work done neural work statistical physic not aim complete review direction research focus lective way recent contribution originating physic opinion important impact rent theory learning machine learning purpose review also putting aside large volume work done statistical physic recurrent neural network biological application mind statistical physic unsupervised learning contribution understanding basic unsupervised method one basic tool unsupervised learning across science method based composition observed data matrix data ing principal component analysis pca independent component analysis ica matrix completion method example class mathematical language matrix position problem stated follows observe n ples data xi 1 noting x n p matrix data idea ing decomposition method assumes x some function x written noisy version rank r matrix r r rank much lower dimensionality number sample therefore name ticularly challenging yet relevant interesting regime dimensionality p comparable number sample n level noise large way perfect estimation signal not possible turn matrix estimation noisy regime modelled tistical physic model spin glass vector variable special planted conﬁguration found concretely model deﬁned student scenario teacher generates dimensional latent variable 1 n taken given probability distribution pu latent variable j j 1 p taken given probability distribution pv teacher generates component data matrix x some given conditional probability distribution pout j goal student cover latent variable precisely ble knowledge x distribution pout pu pv spin glass theory used obtain rather plete understanding model matrix estimation limit p n α ω 1 r ω 1 one compute replica method best error estimation student possibly achieve done decade ago some special choice r pout pu pv barkai sompolinsky 1994 biehl mietzner 1993 watkin nadal 1994 portance early work physic acknowledged some landmark paper subject tic see johnstone lu 2009 however lack mathematical rigor limited understanding algorithmic tractability caused impact work machine learning statistic remain limited resurrection interest statistical physic proach matrix decomposition came study stochastic block model detection sparse network problem community detection wa studied heuristically gorithmically extensively statistical physic view see fortunato 2010 however exact tion understanding algorithmic limitation stochastic block model came spin glass theory decelle et b work computed rigorously asymptotically optimal performance delimited sharply region parameter formance reached belief propagation bp gorithm yedidia et 2003 second order phase sitions appearing model separate phase clustering not performed better random 9 guessing region done eﬃciently bp first order phase transition one spinodal line separate region clustering impossible possible not doable bp rithm easy bp algorithm ref decelle et b also conjectured bp rithm not able reach optimal performance large instance model no polynomial algorithm work attracted large amount work mathematics statistic machine ing computer science community statistical physic understanding stochastic block model conjecture belief propagation algorithm optimal among polynomial one spired discovery new class spectral algorithm sparse data matrix x sparse kala et spectral algorithm basic tool data analysis ng et 2002 von luxburg 2007 based singular value decomposition matrix x function yet sparse matrix x spectrum known leading singular value localized singular vector unrelated latent ing structure robust spectral method obtained linearizing belief propagation thus obtaining called matrix krzakala et variant spectral method based algorithmic interpretation hessian bethe free energy also originated physic saade et 2014 line inspired research merging mainstream statistic machine learning largely thanks recent progress understanding algorithmic limitation due analysis approximate message passing amp algorithm bolthausen 2014 deshpande nari 2014 javanmard montanari 2013 matsushita tanaka 2013 rangan fletcher 2012 rank matrix estimation generalization equation thouless et 1977 well known physic literature spin glass b progress proving many corresponding result mathematically rigorous way some inﬂuential paper direction related matrix estimation barbier et 2016 et 2018 deshpande montanari 2014 lelarge miolane 2016 proof replica formula optimal performance restricted boltzmann machine boltzmann machine particular restricted mann machine another method unsupervised learning often used machine learning apparent name method strong relation statistical physic indeed boltzmann machine often called inverse ising model physic erature used extensively range area cent review physic boltzmann machine see nguyen et 2017 concerning restricted boltzmann machine number study physic clarifying chine work structure learn model random restricted boltzmann machine weight imposed random sparse not learned studied cocco et 2018 tubiana monasson 2017 rather remarkably range tentials hidden unit work unveiled even single layer rbm able represent compositional structure insight work recently used model protein family sequence mation tubiana et 2018 analytical study learning process rbm commonly done using contrastive divergence algorithm based gibbs sampling hinton 2002 challenging first step studied decelle et 2017 beginning learning process dynamic linearized another ing direction coming statistical physic replace gibbs sampling contrastive divergence training algorithm equation thouless et 1977 ha done gabrié et 2015 tramel et 2018 training wa shown competitive application approach discussed rbm random weight lation hopﬁeld model wa clariﬁed barra et 2018 mézard 2017 modern unsupervised generative modelling dawn deep learning brought exciting tions unsupervised learning physic friendly overview some classical recent concept wang 2018 linear activation function closely related pca variational autoencoders vae kingma welling 2013 rezende et 2014 variant much closer physicist mind set autoencoder represented via graphical model trained using prior latent variable tional inference vae single hidden layer closely related widely used technique signal ing dictionary learning sparse coding nary learning problem ha studied statistical physic technique kabashima et 2016 krzakala et sakata kabashima 2013 generative adversarial network gans powerful set idea emerged work goodfellow et 2014 aiming generate sample image hotel bedroom type ing set study gans starting appear work solvable model gans 10 wang et 2018 intriguing generalization earlier statistical physic work online learning ceptrons also want point reader attention toregressive generative model larochelle murray 2011 papamakarios et 2017 uria et 2016 main interest autoregressive model stem fact family explicit probabilistic el direct unbiased sampling possible application model realized statistical wu et 2018 quantum physic lem sharir et 2019 statistical physic supervised learning perceptron glms arguably basic method supervised ing linear regression one aim ﬁnd vector coeﬃcients w scalar product data point xiw corresponds observed predicate often solved least square method 2 minimized bayesian guage least square method corresponds ing gaussian additive noise ξ yi xiw ξi high dimensional setting almost always indispensable use regularization weight common ridge regularization corresponds bayesian pretation gaussian prior weight abilistic thinking generalized assuming eral prior pw generic noise represented conditional probability distribution pout resulting model called generalized linear regression generalized linear model glm many problem interest data analysis learning sented glm instance sparse regression simply requires pw ha large weight zero ceptron threshold κ output ha special form pout z κ δ z δ 1 language neural network glm represents single layer no hidden variable fully connected network generic channel pout traditional theory statistic not readily applicable regime limited data dimension p number sample n grow large tio α remains ﬁxed basic question doe best achievable generalization error depend number sample remain open yet regime related question great interest understanding well setting glm seems uisite understand involved deep learning method statistical physic approach used obtain ciﬁc result glm considering data random independent identically distributed iid matrix modelling label created setting teacher generates vector weight w wj j 1 teacher us vector data matrix x produce label taken pout student know x pw pout posed learn rule teacher us ideally learn already setting random put data provides interesting insight mic tractability problem number sample change line work wa pioneered elisabeth ner gardner derrida 1989 actively studied physic past special case pout pw see györgyi tishby 1990 seung et sompolinsky et 1990 replica method used compute mutual information x model related free energy physic one deduce optimal estimation error vector well mal generalization error remarkable recent progress wa made barbier et 2019 ha proven replica method yield correct result glm random input generic pout pw combining result analysis proximate message passing algorithm javanmard montanari 2013 one deduce case amp algorithm able reach optimal performance region not amp algorithm tured best polynomial algorithm case model could thus used practitioner understand far optimality general purpose algorithm case only ited number sample available physic result neural network statistical physic analysis learning tion property deep neural network challenging task progress made several complementary direction one inﬂuential direction involved study ear deep neural network linear neural network not expressive power represent generic function learning dynamic gradient descent algorithm bear strong resemblance learning namics network time namics learning deep linear neural network described via closed form solution saxe et 2013 learning dynamic linear neural network also able reproduce range fact generalization observed numerically network see advani saxe 2017 another special case ha analyzed great 11 detail called committee machine review see engel van den broeck 2001 committee machine neural network learning random input data only ﬁrst layer weight learned subsequent one ﬁxed theory restricted limit number hidden neuron k 1 mensionality input p number sample n diverge α 1 stochastic gradient descent aka online learning saad solla b optimal alization error analyzed closed form case schwarze 1993 recently replica analysis timal generalization property ha established orously aubin et 2018 key feature tee machine display specialization phase transition number sample small optimal error achieved every hidden unit eﬀectively plementing simple regression only number hidden unit exceeds specialization threshold diﬀerent hidden unit learn diﬀerent weight resulting improvement generalization error another esting observation committee machine hard phase good generalization achievable not tractably get larger number hidden unit grows committee chine wa also used analyzed consequence neural network goldt et b another remarkable limit neural network wa analysed recent series work mei et 2018 rotskoﬀand 2018 work network analysed limit number hidden unit large dimensionality put kept ﬁxed limit weight interact only weakly leading term mean ﬁeld lution tracked via ordinary diﬀerential tion analogous studied glassy system dean 1996 related diﬀerent treatment limit hidden layer large based tion dynamic around initial condition leading relation gaussian process kernel method see jacot et 2018 lee et 2018 information bottleneck information bottleneck tishby et 2000 another concept stemming statistical physic ha ﬂuential quest understanding theory hind success deep learning theory formation bottleneck deep learning tishby 2017 tishby zaslavsky 2015 aim tify notion layer neural network ing oﬀbetween keeping enough information input output label predicted forgetting much unnecessary information sible order keep learned representation concise one interesting consequence tion theoretic analysis traditional capacity expressivity dimension network vc dimension replaced exponent mutual formation input compressed den layer representation implies every bit representation compression equivalent doubling training data impact generalization error analysis tishby 2017 also suggests representation compression achieved stochastic gradient descent sgd diﬀusion irrelevant dimension problem according compression achieved any unit nonlinearity reducing snr irrelevant dimension layer layer diﬀusion weight ing prediction insight time converge good generalization scale like negative number layer theory also predicts nection hidden layer bifurcation phase transition information bottleneck resentations mutual information internal tations intrinsically hard compute directly large neural network none prediction depend explicit estimation mutual information value related line work statistical physic aim vide reliable scalable approximation model mutual information tractable mutual mation computed exactly linear network saxe et 2018 reliably approximated model neural network learning matrix weight close enough rotationally invariant exploited within replica theory order compute desired mutual information gabrié et 2018 landscape glassiness deep learning training deep neural network usually done via stochastic gradient descent sgd landscape loss function statistical physic ha long experience study complex energy landscape relation dynamical behaviour gradient scent algorithm closely related langevin namics often considered physic some inspired work choromanska et 2015 became ular somewhat naive exploring analogy interesting insight relation glassy namics learning deep neural network sented et 2018 particular role making landscape look le glassy highlighted contrasted 12 parametrized network another intriguing line work relates learning neural network property landscape explored baldassi et 2016 2015 work based realization simple model binary perceptron learning dynamic end part ha many conﬁgurations go suggest learning favour wide part space weight argues might explain algorithm attracted wide local minimum generalization property improve teresting theory variant tic gradient descent algorithm suggested chaudhari et 2016 application ml statistical physic researcher theoretical physic encounter deep neural network early layer ing represent input data ﬁner scale later layer immediately think tion group used physic order extract scopic behaviour microscopic rule analogy wa explored instance bény 2013 mehta schwab 2014 analogy renormalization group principle component analysis reported bradde bialek 2017 natural idea use neural network order learn new renormalization scheme first attempt direction appeared ringel 2018 li wang 2018 however remains shown whether lead new physical discovery model not well understood previously phase transition boundary diﬀerent phase matter usually determined using order parameter some system not priori clear determine proper order parameter ral idea neural network may able learn appropriate order parameter locate phase sition without priori physical knowledge idea wa explored carrasquilla melko 2017 ingstar melko 2018 tanaka tomiya van nieuwenburg et 2017 range model ing conﬁgurations sampled uniformly model interest obtained using monte carlo simulation ferent phase diﬀerent temperature using pervised learning order classify conﬁgurations phase extrapolating conﬁgurations not used training set plausibly lead determination phase transition studied model general guiding principle used large number application analyze synthetic experimental data speciﬁc case context tum physic detailed section detailed understanding limitation method term identifying previously unknown der parameter well understanding whether reliably distinguish true thermodynamic phase transition mere yet clariﬁed experiment presented ising model mehta et 2018 provide some preliminary thought direction some underlying mechanism cussed kashiwa et 2019 kernel based learning method learning phase frustrated magnetic rial easily interpretable able identify complex order parameter introduced studied greitemann et 2019 liu et 2019 disordered glassy solid identiﬁcations order parameter particularly challenging also studied particular nussinov et 2016 ronhovde et 2011 use network clustering od identify spatial structure glass cubuk et 2015 learn identify structural ﬂow defect schoenholz et 2017 argues tify parameter capture history dependence disordered system ongoing eﬀort go beyond limitation supervised learning classify phase identify phase transition several direction towards unsupervised ing begin explored instance wetzel 2017 ising xy model wang zhai 2017 2018 frustrated spin system work tiniani et 2019 explores direction identifying phase simple compression underlying urations machine learning also provides exciting set tool study predict control dynamical system instance pathak et 2018 2017 used recurrent neural network called echo state network voir computer jaeger haas 2004 predict trajectory chaotic dynamical system el used weather prediction author reddy et 2016 2018 used reinforcement learning teach autonomous glider literally soar like bird using thermal atmosphere outlook challenge described method statistical physic quite powerful dealing data set model largest diﬀerence traditional ing theory theory coming statistical physic later often based toy tive model data lead solvable model sense quantity interest achievable error computed closed form including constant term contrast aim mainstream learning theory aim provide worst case bound error general assumption setting data structure architecture two approach 13 complementary ideally meet future understand key condition practical case close worse case right model realistic data function next challenge statistical physic approach formulate solve model some kind universality class real setting interest ing reproduce important aspect haviour observed practical application neural network need model input data no longer iid vector instance output generative neural network gabrié et 2018 perceptual manifold chung et 2018 teacher network producing label supervised setting need model suitably tion structure data label need ﬁnd analyze stochastic gradient descent algorithm relevant variant promising work direction rely dynamic ﬁeld theory glass mannelli et 2018 2019 need generalize existing methodology layer network extensive width hidden layer going back direction using machine learning physic full potential ml research linear dynamical system statistical physic yet uncovered mentioned work certainly provide exciting appetizer iii particle physic cosmology diverse portfolio planned ments well poised explore universe unimaginably small world fundamental particle awe inspiring scale universe experiment like large hadron collider lhc large synoptic survey telescope lsst deliver enormous amount data compared prediction speciﬁc oretical model area well established ical model serve null hypothesis standard model particle physic λcdm cosmology includes cold dark matter cosmological constant interestingly alternate hypothesis considered formulated theoretical framework namely quantum ﬁeld theory general relativity despite sharp theoretical tool challenge still daunting expected deviation null expected incredibly tiny revealing subtle eﬀects requires robust treatment complex experimental apparatus complicating statistical inference prediction data not come simple equation instead plex computer simulation machine learning making wave particle physic cosmology oﬀers suit technique confront challenge new perspective motivates bold new strategy excitement span cal experimental aspect ﬁelds includes application immediate impact well prospect transformational change longer term role simulation important aspect use machine learning particle physic cosmology use computer simulation generate sample labeled training data xµ yµ n example target refers particle type particular scattering process ter appearing fundamental theory often speciﬁed directly simulation code ulation directly sample x case simulation not directly conditioned provides sample x z z latent variable describe happened inside simulation not observable actual experiment target label computed latent variable via function z labeled training data xµ zµ n also created simulation use ﬁdelity simulation generate labeled training data ha not only key early success supervised learning area also focus research addressing shortcoming approach particle physicist developed suite ﬁdelity simulation hierarchically composed describe interaction across huge range length scale component simulation include feynman diagrammatic perturbative expansion quantum ﬁeld theory phenomenological model complex pattern radiation detailed model interaction ticles matter detector resulting simulation ha high ﬁdelity simulation ha free parameter tuned number residual tainties simulation must taken account analysis task similarly cosmologist simulate evolution universe diﬀerent length scale using general ativity relevant eﬀects matter radiation becomes increasingly important ing structure formation rich array mations made speciﬁc setting provide enormous speedup compared computationally pensive simulation billion massive object interact gravitationally become prohibitively expensive feedback eﬀects cluded cosmological simulation generally involve istic evolution stochastic initial condition due mordial quantum ﬂuctuations simulation expensive relatively tions cover large volume 14 statistically isotropic homogeneous large scale contrast particle physic simulation stochastic throughout initial scattering interaction detector simulation collider experiment run commodity hardware parallel manner physic goal quire enormous number simulated collision critical role simulation ﬁelds much recent research machine learning related simulation one way another goal recent work develop technique data eﬃcient incorporating domain knowledge directly machine learning model incorporate uncertainty simulation training procedure develop weakly supervised procedure applied real data not rely tion develop anomaly detection algorithm ﬁnd anomalous feature data without simulation speciﬁc signal hypothesis improve tuning simulation reweight adjust simulated data better match real data use machine learning model residual simulation real data learn fast neural network surrogate tion used quickly generate synthetic data develop approximate inference technique make eﬃciently use simulation learn fast neural network surrogate used directly statistical inference classiﬁcation regression particle physic machine learning technique used decade experimental particle physic aid particle identiﬁcation event selection seen classiﬁcation task machine learning ha also used reconstruction seen regression task supervised learning used train predictive model based large number labeled training sample xµ yµ n x denotes input data target label case particle identiﬁcation put feature x characterize localized energy deposit detector label refers one particle specie electron photon pion struction task type sensor data x used target label refers energy tum particle responsible energy deposit algorithm applied bulk data processing lhc data event selection refers task selecting small subset collision relevant geted analysis task instance search higgs boson supersymmetry dark matter data alysts must select small subset lhc data consistent feature hypothetical nal process typically event selection ments also satisﬁed background ce mimic feature signal either due experimental limitation fundamental quantum chanical eﬀects search simplest form reduce comparing number event data satisfy requirement prediction only null hypothesis nate hypothesis thus eﬀective event tion requirement rejecting background process accept signal process powerful sulting statistical analysis within physic machine learning classiﬁcation technique traditionally referred multivariate analysis emphasize contrast traditional technique based simple thresholding cut applied carefully lected engineered feature early simple ral network commonly used task ral network largely displaced boosted decision tree bdts classiﬁcation sion task decade breiman et 1984 freund schapire 1997 roe et 2005 starting around 2014 technique based deep learning emerged demonstrated signiﬁcantly powerful several application recent review history see ref guest et 2018 radovic et 2018 deep learning wa ﬁrst used task targeting hypothesized particle theory beyond standard model not only boosted sion tree also not require engineered feature achieve impressive performance baldi et 2014 work network wa deep perceptron trained large ing set using simpliﬁed detector setup shortly idea parametrized classiﬁer wa introduced concept binary classiﬁer wa extended situation 1 signal hypothesis lifted composite hypothesis parameterized ously instance term mass hypothesized particle baldi et 15 jet physic copious interaction hadron collider lhc produce high energy quark gluon ﬁnal state quark gluon radiate quark gluon eventually combine neutral composite particle due phenomenon ﬁnement resulting collimated spray meson baryon strike detector collectively referred jet developing useful characterization ture jet theoretically robust used test prediction quantum ic qcd ha active area particle physic research decade furthermore many scenario physic beyond standard model predict tion particle decay two jet unstable particle produced large mentum resulting jet boosted jet overlap single fat jet nontrivial structure classifying boosted fat jet much copiously produced jet standard model process involving quark gluon area signiﬁcantly improve physic reach lhc generally identifying progenitor jet cation task often referred jet tagging shortly ﬁrst application deep learning event selection deep convolutional network used purpose jet tagging detector data lends tion baldi et de oliveira et 2016 machine learning technique used within ticle physic decade practice ha always stricted input feature x ﬁxed dimensionality one challenge jet physic natural tation data term particle number particle associated jet varies ﬁrst tion recurrent neural network particle physic wa context ﬂavor tagging guest et 2016 recently ha explosion research use diﬀerent network architecture including recurrent network operating sequence tree graph see ref larkoski et 2017 recent review jet physic includes hybrid approach leverage domain knowledge design architecture example motivated technique natural language processing recursive network designed ate created class jet tering algorithm louppe et similarly work developed motivated invariance permutation particle presented network stability detail radiation pattern cles komiske et 2019 recently isons diﬀerent approach speciﬁc benchmark problem organized kasieczka et 2019 addition classiﬁcation regression machine learning technique used density tion modeling smooth spectrum analytical form not well motivated simulation ha niﬁcant uncertainty frate et 2017 work also allows one model alternative signal hypothesis diﬀuse prior instead speciﬁc concrete physical model abstractly gaussian process work ing used model intensity inhomogeneous son point process scenario found particle physic astrophysics cosmology one teresting aspect line work gaussian process kernel constructed using compositional rule correspond clearly causal model cists intuitively use describe observation aid interpretability duvenaud et 2013 neutrino physic neutrino interact feebly matter thus experiment require large detector volume achieve appreciable interaction rate diﬀerent type action whether come diﬀerent specie trinos background cosmic ray process leave diﬀerent pattern localized energy deposit detector ume detector volume homogeneous vates use convolutional neural network ﬁrst application deep convolutional network analysis data particle physic ment wa context noνa experiment us scintillating mineral oil interaction noνa lead production light imaged two diﬀerent vantage point noνa developed tional network simultaneously processed two image aurisano et 2016 network improves eﬃciency true positive rate selecting electron neutrino 40 purity network ha used search appearance electron trinos hypothetical sterile neutrino similarly microboone experiment detects no created fermilab us 170 ton time projection chamber charged particle ionize liquid argon ionization electron drift ume three wire plane resulting data processed represented image nantly populated noise only sparsely lated legitimate energy deposit microboone collaboration used fasterrcnn ren et 2015 identify localize neutrino interaction bounding box acciarri et 2017 success important future neutrino experiment based time projection chamber deep underground neutrino experiment dune addition relatively low energy neutrino duced accelerator facility machine learning ha also used study neutrino cube observatory located south pole 16 ular convolutional graph neural network applied signal classiﬁcation problem ter approach detector array modeled graph vertex sensor edge learned tion sensor spatial coordinate graph ral network wa found outperform method well classical tional neural network choma et 2018 robustness systematic uncertainty experimental particle physicist keenly aware simulation incredibly accurate not perfect result community ha developed number strategy falling roughly two broad class ﬁrst involves incorporating eﬀect simulation used training involves either propagating underlying source uncertainty calibration detector response quark gluon position proton impact correction perturbation theory etc simulation analysis chain source uncertainty nuisance parameter ν included resulting statistical model p ν parameterized nuisance parameter addition hood function data augmented term p ν representing uncertainty source tainty case penalized maximum likelihood analysis context machine learning classiﬁers regressors typically trained using data generated nominal simulation ν yielding predictive model f treating predictive model ﬁxed possible propagate uncertainty ν f using model p ν p ν however statistical analysis based approach not optimal since predictive model wa not trained taking account uncertainty machine learning literature situation often referred covariate shift two domain resented training distribution target distribution various technique domain tation exist train classiﬁers robust change tend restricted binary main ν train target address problem adversarial training technique wa developed tends domain adaptation domain parametrized ν louppe et 2016 adversarial proach encourages network learn pivotal tity p f x ν independent ν alently p f x p f x p ν adversarial approach ha also used context algorithmic fairness one desire train classiﬁers sor independent decorrelated ciﬁc continuous attribute observable quantity instance jet physic one often would like jet ger independent jet invariant mass min et 2017 previously diﬀerent algorithm called uboost wa developed achieve similar goal boosted decision tree rogozhnikov et 2015 stevens williams 2013 second general strategy used within particle physic cope systematic ulation avoid using simulation modeling distribution p follows let r denote index various subset data satisfying responding selection requirement various strategy developed relate distribution data control region p r 0 tions region interest p r 1 tionships also involve simulation art approach base relationship aspect simulation considered robust simplest ample estimating distribution p r 1 speciﬁc process identifying subset data r 0 dominated p 0 extreme situation limited applicability recently weakly supervised technique developed only involve identifying region only class proportion known assuming relative probability p not linearly dent komiske et metodiev et 2017 technique also assume distribution p r independent r reasonable some text questionable others approach ha used train jet tagger discriminate quark gluon area ﬁdelity simulation no longer adequate assumption method reasonable approach major development machine learning particle physic though limited set problem example approach not plicable one target category corresponds hypothetical particle may not exist present data triggering enormous amount data must collected lider experiment lhc ena targeted exceedingly rare bulk collision involve phenomenon previously studied characterized data volume ciated full data stream impractically large result collider experiment use reduction system referred trigger trigger make critical decision event keep future analysis event discard la cm experiment retain only 1 every event machine learning technique used various degree system essentially 17 particle identiﬁcation classiﬁcation task appears context though computational demand performance term false positive negative diﬀerent environment lhcb experiment ha leader using chine learning technique trigger roughly 70 data selected lhc trigger selected machine learning algorithm initially experiment used boosted decision tree purpose gligorov williams 2013 wa later replaced trixnet algorithm developed yandex likhomanenko et 2015 trigger system often use specialized hardware ﬁrmware gate array fpgas recently tool developed streamline compilation machine learning model fpgas target requirement triggering system duarte et 2018 tsaris et 2018 theoretical particle physic bulk machine learning particle physic cosmology focused analysis observational data also example using machine learning tool theoretical physic instance machine learning ha used characterize landscape string theory cariﬁo et 2017 identify phase transition quantum chromodynamics qcd pang et 2018 study dence hashimoto et b some work closely connected use machine learning tool condensed matter quantum physic speciﬁcally deep learning ha used context lattice qcd lqcd exploratory work direction deep neural network used predict parameter qcd lagrangian lattice urations shanahan et 2018 needed number approach aim improve eﬃciency computationally tensive lqcd calculation problem wa setup regression task one challenge relatively training example additionally machine learning technique used reduce correlation time markov chain albergo et 2019 tanaka tomiya order solve task training example important age known local gauge symmetry lattice data data augmentation not scalable solution given richness symmetry instead author performed feature engineering imposed gauge symmetry translational invariance approach proved eﬀective would able consider richer class network ariant covariant symmetry data approach discussed sec continuation work supported argon leadership computing facility new system rora capable 1 exaﬂops speciﬁcally aiming problem combine traditional high formance computing modern machine learning niques classiﬁcation regression cosmology photometric redshift due expansion universe distant minous object redshifted relation fundamental component observational mology precise redshift estimate obtained spectroscopy however spectroscopic veys expensive time consuming photometric survey based broadband photometry imaging color band give coarse approximation tral energy distribution photometric redshift refers regression task estimating redshift metric data case ground truth training data come precise spectroscopic survey traditional approach photometric redshift based template ﬁtting method benítez 2000 mer et 2008 feldmann et 2006 decade cosmologist also used machine learning method based neural network boosted decision tree photometric redshift carrasco kind ner 2013 collister lahav 2004 firth et 2003 one interesting aspect body work eﬀort ha placed go beyond point estimate redshift various approach exist determine uncertainty redshift estimate obtain terior distribution training data not generated ulation still concern distribution training data may not representative bution data model applied type covariate shift result various selection fects spectroscopic survey subtlety photometric survey dark energy survey ered number approach established validation process evaluate critically bonnett et 2016 recently ha work use erarchical model build additional causal structure model robust diﬀerences language machine learning new model aid transfer learning domain adaptation chical model also aim combine interpretability traditional template ﬁtting approach ﬂexibility machine learning model leistedt et 2018 18 gravitational lens ﬁnding parameter estimation one striking eﬀects general relativity gravitatioanl lensing massive foreground object warp image background object strong gravitational lensing occurs example sive foreground galaxy nearly coincident sky background source event powerful probe dark matter distribution massive galaxy provide valuable cosmological constraint ever system rare thus scalable able lens ﬁnding system essential cope large survey lsst euclid wfirst simple feedfoward convolutional residual neural network resnets applied supervised ﬁcation problem estrada et 2007 lanusse et 2018 marshall et 2009 setting ing data came simulation using pic pipeline image cosmological strong lensing li et 2016 strong lensing lenspop collett 2015 mock lsst observing identiﬁed terizing lensing object maximum likelihood estimation computationally intensive timization task recently convolutional network used quickly estimate parameter gular isothermal ellipsoid density proﬁle commonly used model strong lensing system hezaveh et 2017 example addition example ground truth object relatively unambiguous approach cosmologist also leveraging machine learning infer quantity involve servable latent process parameter mental cosmological model example convolutional network trained predict fundamental cosmological parameter based dark matter spatial distribution bakhsh et 2017 see fig 1 work network trained using computationally intensive simulation evolution dark matter universe assuming speciﬁc value 10 parameter standard λcdm cosmology model real application technique visible matter one would need model bias variance visible tracer respect underlying dark matter bution order close gap convolutional network trained learn fast mapping dark matter visible galaxy zhang et 2019 lowing simulation accuracy computational cost one challenge work common application solid state physic lattice ﬁeld theory many body quantum system simulation computationally expensive thus relatively statistically independent tions large simulation xµ deep learning tends require large labeled training various type subsampling data augmentation approach explored ameliorate situation alternative approach subsampling backdrop provides stochastic gradient loss function even individual sample introducing stochastic masking backpropagation pipeline golkar cranmer 2018 inference fundamental cosmological model also appears classiﬁcation setting particular iﬁed gravity model massive neutrino mimic prediction observables predicted standard λcdm model degeneracy exist restricting xµ statistic broken incorporating statistic rich representation weak lensing signal ticular author peel et 2018 constructed novel representation wavelet decomposition weak lensing signal input convolutional network resulting approach wa able discriminate previously degenerate model 83 accuracy deep learning ha also used estimate mass galaxy cluster largest gravitationally bound structure universe powerful logical probe much mass galaxy cluster come form dark matter not directly observable galaxy cluster mass estimated via gravitational lensing observation cluster medium dynamical analysis cluster galaxy ﬁrst use machine learning dynamical cluster mass estimate wa performed using support distribution machine póczos et 2012 simulation ntampaka et 2015 2016 number network algorithm cluding gaussian process regression kernel ridge sion support vector machine gradient boosted tree gressors others applied problem using macsis simulation henson et 2016 training data simulation go beyond simulation incorporates impact various astrophysical process allows opment realistic processing pipeline plied observational data need accurate automated mass estimation pipeline motivated large survey eboss desi erosita pol euclid author found compared traditional relation mass ratio using machine learning technique reduced factor 4 armitage et 2019 recently lutional neural network used mitigate tematics virial scaling relation improving dynamical mass estimate ho et 2019 tional neural network also used estimate cluster mass synthetic mock observation 19 puter science carnegie mellon university 5000 forbes pittsburgh pa 15213 usa enter cosmology department physic carnegie mellon university carnegie 5000 forbes 213 usa abstract allenge century ccurately estimate cosmological universe major approach g cosmological parameter scale matter distribution xy survey provide mean map structure three mation galaxy location arized single function scale galaxy correlation function show possible estimate logical parameter directly matter paper present deep convolutional network c representation well result obtained using posed distribution regression ng machine learning technique able sometimes point estimate using al model open way parameter universe acy n ha brought u tool method e universe far greater detail u deeply probe fundamental gy suite cosmological 33 rd international conference machine rk ny usa jmlr w cp volume author figure dark matter distribution three cube produced using different set parameter cube divided small cube training prediction note although cube ﬁgure produced using different cosmological eters constrained sampled set effect not visually cernible servations allow u make serious inroad derstanding universe including cosmic crowave background cmb planck collaboration et 2015 hinshaw et 2013 supernova perlmutter et 1999 riess et 1998 large scale structure galaxy galaxy cluster cole et 2005 anderson et 2014 parkinson et 2012 particular large scale structure involves measuring position property bright source great volume sky amount information overwhelming modern method machine learning statistic play creasingly important role modern cosmology ample common method compare large scale ture observation theory compare compressed figure 1 dark matter distribution three cube produced using diﬀerent set parameter cube divided small cube training prediction note although cube ﬁgure produced using diﬀerent cosmological parameter constrained sampled set eﬀect not visually discernible reproduced ravanbakhsh et 2017 galaxy cluster author ﬁnd scatter predicted mass reduced compared traditional ray luminosity based method ntampaka et 2018 inverse problem inference stressed repeatedly particle physic mology characterized well motivated forward simulation forward simulation ther intrinsically stochastic case bilistic decay interaction found particle physic simulation deterministic case gravitational lensing gravitational tions however even deterministic physic simulator usually followed probabilistic description observation based poisson count model strumental noise case one consider ulation implicitly deﬁning distribution p x x refers observed data z unobserved latent variable take random value inside simulation parameter forward model coeﬃcients lagrangian 10 parameter λcdm cosmology many scientiﬁc task acterized inverse problem one wish infer z x simplest case considered classiﬁcation take categorical value regression point estimate ˆ x x ˆ z x x useful scientiﬁc plication often require uncertainty estimate many case solution inverse problem sense small change x lead large change estimate implies estimator high variance some case forward model equivalent linear operator maximum lihood estimate ˆ ymle x ˆ zmle x expressed matrix inversion case instability inverse related matrix forward model poorly conditioned maximum hood estimate may unbiased tends high ance penalized maximum likelihood ridge regression tikhonov regularization gaussian process sion closely related approach within particle physic type problem often referred unfolding case one often ested distribution some kinematic property collision prior detector eﬀects x sent smeared version quantity folding detector eﬀects similarly estimating parton sity function describe quark gluon inside proton cast inverse problem sort ball et 2015 forte et 2002 recently neural work gaussian process sophisticated physically inspired kernel applied problem bozson et 2018 frate et 2017 context cosmology example inverse problem denoise laser interferometer servatory ligo time series underlying waveform gravitational wave shen et 2019 generative adversarial network gans even used context inverse problem used noise recover image galaxy beyond naive volution limit schawinski et 2017 another ple involves estimating image background object prior gravitationally lensed foreground ject case describing physically motivated prior background object diﬃcult recently recurrent 20 inference machine putzky welling 2017 introduced way implicitly learn prior verse problem successfully applied strong gravitational lensing morningstar et 2018 2019 ambitious approach inverse problem volves providing detailed probabilistic characterization given frequentist paradigm one would aim characterize likelihood function l p x bayesian formalism one would wish terize posterior p x x p analogous situation happens inference latent ables z given particle physic cosmology approach statistical inference based detailed modeling likelihood markov chain monte carlo mcmc et 2013 hamiltonian monte carlo variational ence jain et 2018 lang et 2016 regier et 2018 however approach require likelihood function tractable inference somewhat surprisingly probability density lihood p x implicitly deﬁned ulator often intractable symbolically probability density written p r p x dz z latent variable simulation latent space simulation enormous highly structured integral not performed analytically simulation single collision lhc z may hundred million component practice simulation often based monte carlo technique generate sample xµ zµ x density estimated challenge x diﬃcult accurately estimate density example naive based approach not scale high dimension kernel density estimation technique only thy around adding challenge distribution large dynamic range interesting physic often sits tail butions intractability likelihood implicitly deﬁned simulation foundational problem not only particle physic cosmology many area science well including epidemiology netics ha motivated development inference algorithm only require ability generate sample simulation forward mode one prominent technique approximate bayesian computation abc abc one performs bayesian ference using mcmc rejection sampling approach likelihood approximated bility p ρ x x ϵ x observed data conditioned ρ x some distance metric tween x output simulator ϵ tolerance parameter ϵ one recovers exact bayesian inference however eﬃciency dure vanishes one challenge abc larly x speciﬁcation distance measure ρ x maintains reasonable ceptance eﬃciency without degrading quality inference beaumont et 2002 marin et 2012 joram et 2003 sisson fan 2011 sisson et 2007 approach estimating likelihood quite similar traditional practice particle physic using histogram kernel density estimation imate ˆ p case domain knowledge required identify useful summary order reduce dimensionality data interesting sion abc technique utilizes universal probabilistic programming particular technique known ence compilation sophisticated form importance sampling neural network control random number generation probabilistic program bias simulation produce output closer served x le et 2017 term abc often used synonymously general term inference however number approach involve learning approximate likelihood likelihood ratio used surrogate intractable likelihood ratio example neural density estimation autoregressive model normalizing ﬂows larochelle murray 2011 papamakarios et 2017 rezende mohamed 2015 used purpose scale higher dimensional data cranmer louppe 2016 papamakarios et 2018 alternatively training classiﬁer discriminate x x used estimate hood ratio ˆ r used inference either frequentist bayesian paradigm brehmer et cranmer et 2015 herman et 2019 example particle physic thousand published result within particle physic including discovery higgs boson involve tical inference based surrogate likelihood ˆ p structed density estimation technique applied synthetic datasets generated simulation typically restricted mary statistic no feature ber event observed term inference relatively new core methodology experimental particle physic recently suite inference 21 simulation machine learning inference x z latexit latexit latexit latexit r x latexit latexit yfnf latexit yfnf latexit kqlq x latexit bmhp latexit whfd latexit whfd latexit gutw latexit acmhicpvlt latexit acmhicpvlt cfelahnftcgllmillywscqnom cevdbvmgnupiidsjazxgmyyry sbmmhergriwykdodxbktvpkh latexit acmhicpvlt cfelahnftcgllmillywscqnom cevdbvmgnupiidsjazxgmyyry sbmmhergriwykdodxbktvpkh latexit acmhicpvlt eluggayhlrrjdosienvfcjrck arg min g l g latexit acq latexit acq latexit acq latexit acq ˆ r latexit latexit latexit latexit latexit latexit latexit latexit latexit latexit latexit latexit atexit parameter latent observable augmented data approximate likelihood ratio figure 2 schematic machine learning based approach inference simulation provides training data neural network subsequently used surrogate intractable likelihood inference reproduced brehmer et niques based neural network developed applied model physic beyond dard model expressed term eﬀective ﬁeld theory eft brehmer et b eft provide tematic expansion theory around standard model parametrized coeﬃcients quantum mechanical operator play role setting one interesting observation work even though likelihood likelihood ratio tractable joint likelihood ratio r x joint score x log p x tractable used augment training data see fig 2 dramatically improve sample eﬃciency technique brehmer et addition inference compilation technique ha applied inference decay eﬀort required developing probabilistic programming protocol integrated ing simulation code sherpa baydin et 2018 casado et 2017 approach provides bayesian inference latent ables p x deep interpretability terior corresponds distribution complete trace simulation allowing any aspect ulation inspected probabilistically another technique inference wa motivated challenge particle physic known adversarial variational optimization avo louppe et avo parallel generative adversarial network generative model no longer neural network instead simulation instead optimizing parameter network goal optimize parameter simulation generated data match target data distribution main challenge unlike neural network scientiﬁc simulator not diﬀerentiable get around problem variational optimization technique used provides diﬀerentiable surrogate loss function technique investigated tuning parameter simulation computationally intensive task bayesian optimization ha also recently used ilten et 2017 example cosmology within cosmology early us abc include straining thick disk formation scenario milky way robin et 2014 inference rate morphological transformation galaxy high shift cameron pettitt 2012 aimed track hubble parameter evolution type ia supernova measurement experience motivated opment tool cosmoabc streamline plication methodology cosmological tions ishida et 2015 recently inference method based machine learning also developed motivated experience cosmology confront lenges abc observation x data compression strategy wa developed learns summary statistic maximize fisher tion parameter alsing et 2018 charnock et 2018 learned summary statistic mate suﬃcient statistic implicit likelihood small neighborhood some nominal ﬁducial eter value approach closely connected brehmer et recently approach extended learn summary statistic robust systematic uncertainty alsing wandelt 2019 22 generative model active area machine learning research involves using unsupervised learning train generative model produce distribution match some empirical distribution includes generative adversarial work gans goodfellow et 2014 variational toencoders vaes kingma welling 2013 rezende et 2014 autoregressive model model based normalizing ﬂows larochelle murray 2011 pamakarios et 2017 rezende mohamed 2015 interestingly issue motivates free inference intractability density implicitly deﬁned simulator also appears generative sarial network gans density gan tractable gans would trained via standard mum likelihood density intractable trick wa needed trick introduce sary discriminator network used classify sample generative model sample taken target distribution discriminator fectively estimating likelihood ratio two distribution provides direct connection approach inference based ﬁers cranmer louppe 2016 operationally model play similar role ditional scientiﬁc simulator though traditional tion code also provide causal model underlying data generation process grounded physical principle however traditional scientiﬁc simulator often slow distribution interest emerge level microphysical description example ing collision lhc involves physic ionization scintillation similarly simulation cosmology involve gravitational interaction among mous number massive object may also include complex feedback process involve radiation star formation etc therefore learning fast approximation simulation great value within particle physic early work direction included gans energy deposit particle calorimeter paganini et b studied atlas collaboration atlas tion 2018 cosmology generative model used learn simulation cosmological structure formation rodríguez et 2018 interesting brid approach deep neural network wa used predict structure formation universe residual fast physical simulation based linear perturbation theory et 2018 case simulation not way exist impractical nevertheless generative model data valuable purpose calibration illustrative example rection come ravanbakhsh et 2016 see fig author point next generation mological survey weak gravitational lensing rely accurate measurement apparent shape distant galaxy however shape measurement method require precise calibration meet accuracy requirement science analysis calibration process lenging requires large set high quality galaxy image expensive collect therefore gan enables implicit generalization ric bootstrap outlook challenge particle physic cosmology long tory utilizing machine learning method scope topic machine learning applied ha grown signiﬁcantly machine learning seen key strategy confronting challenge graded lhc albertsson et 2018 apollinari et 2015 inﬂuencing strategy future experiment cosmology particle physic ntampaka et 2019 one area particular ha gathered great deal attention lhc challenge identifying track left charged particle environment farrell et 2018 ha focus recent kaggle lenge almost area machine learning plied physic problem desire incorporate domain knowledge form hierarchical structure compositional structure geometrical structure metries known exist data generation process recently ha spate work machine learning community tion bronstein et 2017 cohen welling 2016 hen et 2018 cohen et 2019 kondor 2018 kondor et 2018 kondor trivedi 2018 ments followed closely physicist ready incorporated contemporary research area iv quantum matter intrinsic probabilistic nature quantum ic make physical system realm eﬀectively inﬁnite source big data appealing ground ml application paradigmatic example probabilistic nature measurement process quantum physic measuring position r tron orbiting around nucleus only mately inferred measurement inﬁnitely cise classical measurement device only used record outcome speciﬁc observation tron position ultimately complete characterization measurement process given wave function 23 2 fig 2 sample dataset versus generated sample using conditional generative adversarial network section iii synthetic image 128 colored image inverted produced conditioning set feature 2 0 1 pair observed generated image column correspond value detail feature see willett et al 2013 instance selected unavailable model training image conditioned statistic interest brightness size galaxy allow u thesize calibration datasets speciﬁc galaxy population object exhibiting realistic morphology related work machine learning literature regier et al use convex combination smooth spiral template unconditioned generative model galaxy image regier et al propose using vae following section give brief background image generation calibration signiﬁcance ern cosmology review current approach deep conditional generative model introduce new technique problem setting section ii iii section iv ass quality generated image comparing conditional distribution shape morphology parameter simulated real galaxy ﬁnd good agreement weak gravitational lensing weak regime gravitational lensing distortion background galaxy image modeled anisotropic shear noted γ whose amplitude orientation depend matter distribution observer distant galaxy shear affect particular apparent ellipticity galaxy denoted measuring weak lensing effect made possible assumption background galaxy randomly oriented ensemble average shape would average zero absence lensing apparent ellipticity e used noisy unbiased estimator shear ﬁeld γ e e cosmological current approach address problem cosmology literature ﬁt analytic parametric light proﬁles deﬁned size intensity ellipticity steepness parameter observed galaxy followed simple modelling distribution ﬁtted parameter function quantity interest galaxy brightness modelling usually simply involves ﬁtting linear dependence mean standard deviation gaussian distribution see hoekstra et al 2016 appendix however simple parametric model galaxy light proﬁles not complex morphology needed calibration task only currently available alternative realistic galaxy morphology needed use training set image input simulation pipeline involves subsampling training set match distribution size redshift brightness target galaxy simulation leaving only relatively small number object reused several hundred time simulate large survey see jarvis et al 2016 section analysis involves computing measured ellipticity galaxy different distance correlation function compared theoretical diction order constrain cosmological model shed light nature dark energy however measuring galaxy ellipticity ensemble average used cosmological analysis unbiased extremely challenging task fig 1 illustrates main step involved acquisition science image weakly sheared galaxy image undergo additional distortion essentially blurring go mosphere telescope optic acquired imaging sensor pixelates noisy image ﬁgure illustrates cosmological shear clearly subdominant effect ﬁnal image need disentangled subsequent blurring atmosphere telescope option blurring point spread function psf directly measured using star point source shown top fig image acquired shape measurement algorithm used estimate ellipticity galaxy ing psf however despite best effort weak lensing community nearly two decade current shape measurement algorithm still susceptible bias inferred shear measurement bias commonly modeled term additive multiplicative bias parameter c deﬁned e e 1 γ c 1 γ true shear depending shape ment method used c depend factor psf level noise image generally intrinsic property galaxy population like size ellipticity distribution etc calibration bias achieved using image simulation closely mimicking real observation given survey using galaxy image distorted known shear thus allowing measurement bias parameter eq 1 image simulation pipeline galsim package rowe et al 2015 use forward modeling tions reproducing step image acquisition figure 3 sample dataset versus generated sample using conditional generative adversarial network synthetic image colored image inverted produced conditioning set feature 0 1 37 pair observed generated image column correspond value reproduced ravanbakhsh et 2016 ψ r whose square modulus ultimately deﬁnes ability p r r observing electron given position space case single electron theoretical prediction experimental inference p r eﬃciently performed situation becomes dramatically complex case many quantum particle example probability observing position n electron p rn intrinsically function seldom exactly determined n much larger ten nential hardness estimating p rn rect consequence estimating body amplitude ψ rn commonly referred quantum problem quantum problem manifest variety case chieﬂy include theoretical modeling simulation complex quantum system al molecule only approximate solution often available important manifestation quantum problem include standing analysis experimental outcome cially relation complex phase matter following discus some ml application focused alleviating some challenging theoretical perimental problem posed quantum problem quantum state quantum state nqs tation term cial neural network anns carleo troyer 2017 commonly adopted choice parameterize function amplitude neural network ψ r g l w l g 2 w 2 g 1 w 1 r 3 similar notation introduced eq 2 early work mostly concentrated shallow work notably restricted boltzmann machine rbm smolensky 1986 rbms hidden unit without bias visible unit mally correspond ffnns depth l 2 tivations g 1 x log cosh x g 2 x exp x important diﬀerence respect rbm application unsupervised learning probability distribution used nqs rbm state typically taken weight carleo troyer 2017 deeper architecture consistently ied introduced recent work example nqs based convolutional deep network choo et 2018 saito 2018 sharir et 2019 see fig 4 schematic example tion use deep ffnn network apart tical success deep learning industrial application also come general theoretical argument quantum physic example ha shown deep nqs sustain entanglement eﬃciently rbm state levine et 2019 liu et extension nqs representation concern tion mixed state described density matrix rather pure context possible deﬁne rbm parametrizations density matrix torlai melko 2018 one speciﬁc challenge emerging tum domain imposing physical symmetry nqs representation case periodic arrangement matter spatial symmetry imposed using volutional architecture similar used age classiﬁcation task choo et 2018 saito 2018 sharir et 2019 selecting state ent symmetry sector ha also demonstrated choo et 2018 spatial symmetry analogous counterpart ml application satisfying involved quantum symmetry often need deep ing ann architecture notable case sense exchange symmetry boson amount imposing tionally invariant respect exchange particle 24 heisenberg convolutional complex deep b sum channel 12 10 8 6 4 2 σ σ σ latexit stvuioh figure 4 top example shallow convolutional neural network used represent system spin particle square lattice bottom ters convolutional rbm found ational learning heisenberg model adapted carleo troyer 2017 index model ha adopted benchmark ann bosonic architecture result obtained saito 2017 2018 saito kato 2017 teng 2018 lenging symmetry however certainly fermionic one case nqs representation need code antisymmetry ing two particle position example lead minus sign case diﬀerent approach plored mostly expanding existing variational ansatz fermion symmetric rbm ing antisymmetric correlator part ha used study interacting lattice fermion mura et 2017 approach tackled fermionic symmetry problem using backﬂow mation slater determinant luo clark 2018 directly working ﬁrst quantization han et situation fermion certainly ing ml approach moment owing ciﬁc nature symmetry application side nqs representation used along three main diﬀerent research line representation theory active area research concern general sive power nqs also compared family variational state theoretical activity tation property nqs seek understand large deep neural network describing esting interacting quantum system connection ﬁrst numerical result obtained rbm state entanglement ha soon identiﬁed possible didate expressive power nqs rbm state example eﬃciently support scaling deng et number variational parameter scaling only polynomially system size rection language tensor network ha ticularly helpful clarifying some property nqs chen et pastori et 2018 ily nqs based rbm state ha shown equivalent certain family variational state known clark 2018 glasser et question determining large spective class quantum state belonging nqs form eq 3 computationally eﬃcient tensor network however still open exact representation several intriguing phase matter including topological state stabilizer code deng et glasser et huang moore 2017 kaubruegger et 2018 lu et 2018 zheng et 2018 also obtained closed rbm form not surprisingly given shallow depth rbm architecture also expected limitation general ground speciﬁcally not general possible write possible physical state term compact rbm state gao duan 2017 order lift intrinsic limitation rbms ciently describe large family physical state necessary introduce deep boltzmann machine dbm two hidden layer gao duan 2017 similar network construction introduced also sible theoretical framework alternative standard representation quantum mechanic leo et 2018 learning data parallel activity understanding cal property nqs family study ﬁeld concerned problem understanding hard practice learn quantum state numerical data realized using either synthetic data example coming numerical simulation directly experiment line research ha explored vised learning setting understand well nqs represent state not easily expressed closed analytic form ann goal train nqs network represent close possible tain target state amplitude eﬃciently computed approach ha successfully used learn fermionic frustrated bosonic hamiltonians cai liu 2018 represent teresting study case since structure target pose challenge dard activation function used ffnn along line supervised approach proposed learn random matrix product state shallow nqs borin abanin 2019 alized nqs including computationally treatable dbm form pastori et 2018 latter case study revealed eﬃcient strategy perform learning former case hardness learning some 25 random mp ha showed present lated hardness originates entanglement structure random mp however unclear related hardness nqs optimization scape intrinsic limitation shallow nqs besides supervised learning given quantum state approach nqs largely trated unsupervised approach framework only measurement some target state density matrix available goal reconstruct full state nqs form using measurement simplest setting one given data set ments r 1 r distributed according born rule prescription p r r p r structed case positive inite only measurement certain basis provided reconstructing p r standard vised learning approach enough reconstruct available information underlying quantum state approach example ha demonstrated stoquastic hamiltonians torlai et 2018 using generative model approach based deep vae generative model ha also demonstrated case family sample quantum state rocchetto et 2018 eﬀect network depth ha shown beneﬁcial compression general setting problem struct general quantum state either pure mixed using measurement single basis quantum number especially necessary construct also complex phase quantum state problem corresponds problem quantum information known quantum state raphy speciﬁc nqs approach troduced carrasquilla et 2019 torlai et 2018 torlai melko 2018 discussed detail dedicated section also connection ml technique used task variational learning finally one main application nqs representation context variational imations quantum problem goal approach example approximately solve schrödinger equation using nqs tation case problem ﬁnding ground state given quantum nian h formulated variational term lem learning nqs weight w minimizing e w w w w w achieved using learning scheme based variational monte carlo mization carleo troyer 2017 within family application no external data representative tum state given thus typically demand larger computational burden supervised unsupervised learning scheme nqs experiment variety spin choo et 2018 deng et glasser et liang et 2018 bosonic choo et 2018 saito 2017 2018 saito kato 2017 fermionic han et luo clark 2018 nomura et 2017 model shown result competitive existing proaches obtained some case improvement existing variational result demonstrated notably lattice model carleo troyer 2017 luo clark 2018 nomura et 2017 topological phase matter glasser et kaubruegger et 2018 nqs application concern solution schrödinger equation carleo troyer 2017 czischek et 2018 fabiani mentink 2019 schmitt heyl 2018 application one us variational principle dirac frenkel dirac 1930 frenkel 1934 learn mal time evolution network weight ably generalized also open dissipative quantum tems variational solution lindblad equation realized hartmann carleo 2019 nagy savona 2019 vicentini et 2019 yoshioka hamazaki 2019 great majority variational application discussed learning scheme used typically technique standard sgd approach stochastic reconﬁguration sr approach becca sorella 2017 sorella 1998 generalization case carleo et 2012 proven particularly suitable variational learning nqs sr scheme seen quantum analogous method learning probability butions amari 1998 build intrinsic try associated parameter recently eﬀort use deeper expressive network initially adopted learning scheme building technique sistently used kochkov clark 2018 sharir et 2019 constitute two diﬀerent philosophy proaching problem one hand early cation focused small network learned curate expensive training technique hand later approach focused deeper network cheaper also le learning technique combining two philosophy computationally cient way one open challenge ﬁeld speed simulation use ml method realm tum problem extends well beyond 26 network representation quantum state ful technique study interacting model tum monte carlo qmc approach method stochastically compute property quantum system mapping eﬀective classical model ample mean representation practical issue often resulting mapping providing eﬃcient sampling scheme space path integral perturbation series etc quire careful tuning often ing sampler representation therefore particularly challenging problem vised ml method however adopted tool monte carlo sampling classical quantum application several approach tion proposed leverage ability supervised learning well approximate target tribution sampled underlying monte carlo scheme relatively simple generative model used early application cal system huang wang 2017 liu et monte carlo technique generalized also fermionic system chen et liu et nagai et 2017 overall ha found approach eﬀective reducing autocorrelation time especially compared lie le eﬀective markov chain monte carlo local update recently generative ml model adopted sampling ciﬁc task notably wu et 2018 used deep toregressive model may enable eﬃcient pling hard classical problem spin glass problem ﬁnding eﬃcient sampling scheme underlying classical model transformed problem ﬁnding eﬃcient corresponding sive deep network representation approach ha also generalized quantum case sharir et 2019 autoregressive representation introduced representation automatically normalized allows bypass markov chain monte carlo variational learning discussed exact large family bosonic spin tems qmc technique typically incur severe sign problem dealing several interesting fermionic model well frustrated spin hamiltonians case tempting use ml approach attempt rect indirect reduction sign problem only ﬁrst stage family application ha used infer information fermionic phase den information green function broecker et similarly ml technique help reduce burden subtle manifestation sign problem namical property quantum model particular problem reconstructing spectral function correlation imaginary time also ﬁeld ml used alternative tional technique perform cal continuation qmc data arsenault et 2017 fournier et 2018 yoon et 2018 classifying quantum phase challenge posed complexity quantum state manifest many form speciﬁcally several elusive phase quantum matter often hard characterize pinpoint cal simulation experiment reason ml scheme identify phase matter become ularly popular context quantum phase following review some speciﬁc application quantum domain general discussion identifying phase phase transition found synthetic data following early development phase cation supervised approach carrasquilla melko 2017 van nieuwenburg et 2017 wang 2016 many study since focused analyzing phase matter synthetic data mostly simulation quantum system not attempt vide exhaustive review many study appeared direction highlight two large family lem largely served benchmark new ml tool ﬁeld ﬁrst challenging test bench phase classiﬁcation scheme case quantum localization elusive phase matter showing tic ﬁngerprints not necessarily emerging traditional der parameter see example alet laﬂorencie 2018 recent review topic first study direction focused training strategy ing hamiltonian entanglement spectrum hsu et 2018 huembeli et schindler et 2017 venderley et 2018 zhang et 2019 work demonstrated ability eﬀectively learn mbl phase transition relatively small system cessible exact diagonalization technique study instead focused identifying signature directly experimentally relevant quantity tably dynamic local quantity doggen et 2018 van nieuwenburg et 2018 latter scheme appear present ing application experiment former used tool identify existence pected phase presence correlated disorder hsu 27 et 2018 another challenging class problem found analyzing topological phase matter largely considered test ml scheme cause phase typically characterized local order parameter turn order parameter hard learn popular classiﬁcation scheme used image speciﬁc issue already present analyzing classical model featuring logical phase transition example presence transition learning scheme trained raw monte carlo conﬁgurations not eﬀective beach et 2018 hu et 2017 problem vented devising training strategy using feature broecker et cristoforetti et 2017 wang zhai 2017 wetzel 2017 instead raw monte carlo sample feature typically rely some portant assumption nature phase transition looked thus diminishing tiveness looking new phase matter deeper quantum world ha research activity along direction learning supervised fashion topological invariant neural network used example classify family topological hamiltonians using input discretized cients either real ohtsuki ohtsuki 2016 2017 momentum space sun et 2018 zhang et case found neural network able reproduce already known beforehand topological invariant winding number berry curvature context cal matter large extent challenging case band model case common approach deﬁne set carefully engineered feature used top raw data one well known example case quantum loop topography zhang kim 2017 trained local operator computed single shot sampled walker example done variational monte carlo ha shown speciﬁc choice local feature able distinguish strongly teracting fraction chern insulator also tum spin liquid zhang et 2017 similar eﬀorts realized classify exotic phase ter including magnetic skyrmion phase iakovlev et 2018 dynamical state antiskyrmion dynamic ritzmann et 2018 despite progress seen far along many rection described fair say topological phase matter especially interacting system stitute one main challenge phase tion some good progress ha already made huembeli et scheurer 2018 future research need address issue ﬁnding training scheme not relying data feature experimental data beyond extensive study data numerical ulations supervised scheme found way also tool analyze experimental data quantum tems atom experiment supervised ing tool used map topological phase particle well onset mott insulating phase ﬁnite optical trap rem et 2018 speciﬁc case phase already known identiﬁable approach ever technique combining theoretical knowledge experimental data hold potential genuine scientiﬁc discovery example ml enable scientiﬁc discovery interesting case experimental data ha tributed one many available equally likely priory theoretical model experimental tion hand not easily interpreted typically esting case emerge example order eter complex only implicitly known function experimental outcome tion ml approach used powerful tool eﬀectively learn underlying trait given theory provide possibly unbiased classiﬁcation mental data case incommensurate phase superconductors scanning tunneling microscopy image reveal complex patter hard decipher using conventional analysis tool using supervised approach context recent work zhang et ha shown possible infer nature spatial ordering system also see fig similar idea ha also used another totypical interacting quantum system fermion hubbard model implemented atom periments optical lattice case reference model provide snapshot thermal density matrix supervised learning ion outcome study bohrdt et 2018 experimental result good conﬁdence compatible one theory proposed case geometric string theory charge carrier last two experimental application described outcome supervised approach large extent highly hard predict ori basis information hand inner bias induced choice theory classiﬁed however one current limitation kind approach face tensor network machine learning research topic reviewed far mainly cerned use ml idea tool study 28 16 figure 5 example machine learning approach ﬁcation experimental image scanning tunneling croscopy superconductors image classiﬁed according prediction distinct type riodic spatial modulation reproduced zhang et lem realm quantum physic plementary philosophy interesting research direction ﬁeld explores inverse direction vestigating idea quantum physic inspire devise new powerful ml tool central development representation quantum state successful variational family wave function urally emerging representation quantum state verstraete et 2008 tensor work serve practical conceptual tool ml task supervised vised setting approach build idea providing learning scheme network structure alternative conventionally adopted stochastic learning scheme ffnn network example trix product state mp representation simulation interacting tum system white 1992 perform classiﬁcation task liu et 2018 novikov et 2016 stoudenmire schwab 2016 also recently adopted explicit generative model supervised learning han et stokes illa 2019 worth mentioning related tensor decomposition developed text applied mathematics used ml pose acar yener 2009 anandkumar et 2014 decomposition oseledets 2011 formally equivalent mp representation introduced parallel tool perform various machine ing task gorodetsky et 2019 izmailov et 2017 novikov et 2016 network closely related mp also explored modeling guo et 2018 eﬀort increasing amount entanglement encoded tensor decomposition recent work concentrated tions alternative mp form one notable ple use tree tensor network cal structure hackbusch kühn 2009 shi et 2006 applied classiﬁcation liu et stoudenmire 2018 generative eling cheng et 2019 task good success example use entangled plaquette state changlani et 2009 gendiar nishino 2002 zacapo et 2009 string bond state schuch et 2008 showing sizable improvement tion task mp state glasser et theoretical side deep connection tween tensor network complexity measure tum entanglement entropy used understand possible inspire successful network design ml purpose network formalism ha proven powerful interpreting deep learning lens renormalization group concept pioneering work direction ha connected mera tensor network state vidal 2007 cal bayesian network bény 2013 later analysis convolutional arithmetic circuit cohen et 2016 family convolutional network product linearity introduced convenient model bridge tensor decomposition ffnn architecture beside conceptual relevance connection help clarify role inductive bias modern monly adopted neural network levine et 2017 outlook challenge application ml quantum problem seen progress past year touching diverse selection topic ranging merical simulation data analysis potential ml technique ha already surfaced context already showing improved performance respect existing technique selected problem large extent ever real power ml technique domain ha only partially demonstrated several open problem remain addressed context variational study nqs ample origin empirical success obtained far diﬀerent kind neural network quantum state not equally well understood family variational state like tensor network key open lenges remain also representation simulation fermionic system eﬃcient representation still found representation ml purpose 29 well network like used nqs play important role bridge ﬁeld back arena computer science challenge future research direction consist eﬀectively interfacing community retaining interest generality physic tool concern ml approach experimental data ﬁeld largely still infancy only application demonstrated far stark contrast ﬁelds astrophysics ml approach matured stage often used standard tool data analysis moving towards achieving goal quantum domain demand closer collaboration theoretical experimental eﬀorts well deeper understanding speciﬁc problem ml make substantial diﬀerence overall given relatively short time span application ml approach quantum matter emerged however good reason believe challenge energetically some coming year quantum computing quantum computing us quantum system process information popular framework based quantum computing nielsen chuang 2002 quantum algorithm describes evolution initial state quantum system n system called qubits ﬁnal state discrete formation quantum gate gate usually act only small number qubits sequence gate deﬁnes computation intersection machine learning quantum computing ha become active research area last couple year contains variety way merge two discipline see also dunjko briegel 2018 review quantum machine learning asks quantum computer enhance speed innovate machine learning biamonte et 2017 ciliberto et 2018 schuld petruccione see also section vii v quantum learning theory highlight retical aspect learning quantum framework arunachalam de wolf 2017 section concerned third angle namely machine learning help u build study quantum computer angle includes topic ranging use intelligent data mining method ﬁnd physical regime material used qubits kalantre et 2019 veriﬁcation tum device agresti et 2019 learning design quantum algorithm bang et 2014 wecker et 2016 facilitating classical simulation quantum cuits jónsson et 2018 automated design tum experiment krenn et 2016 melnikov et 2018 learning extract relevant information measurement seif et 2018 focus three general problem related tum computing targeted range ml method problem reconstructing en ing quantum state via measurement problem preparing quantum state via quantum control problem maintaining information stored state quantum error correction ﬁrst lem known quantum state tomography pecially useful understand improve upon itations current quantum hardware quantum control quantum error correction solve related problem however usually former refers lutions latter us algorithmic solution problem executing computational protocol quantum system similar discipline review machine learning ha shown promising result area longer run likely enter toolbox quantum computing used method quantum state tomography general goal quantum state tomography qst reconstruct density matrix unknown tum state experimentally available ments qst central tool several ﬁelds quantum information quantum technology general often used way ass quality limitation experimental platform resource needed perform full qst however extremely manding number required measurement scale exponentially number degree freedom see paris rehacek 2004 review topic haah et 2017 donnell wright 2016 discussion hardness learning state tomography ml tool identiﬁed already several year ago tool improve upon cost full qst ing some special structure density matrix pressed sensing gross et 2010 one prominent proach problem allowing reduce number required measurement rd log 2 density matrix rank r dimension ful experimental realization technique ha example implemented state tóth et 2010 system trapped ion ofrío et 2017 methodology side full qst ha recently seen development deep learning approach example using supervised approach based neural network output full density matrix input possible measurement 30 outcome xu xu 2018 problem choosing optimal measurement basis qst ha also cently addressed using based approach optimizes prior distribution target sity matrix using bayes rule quek et 2018 general ml approach full qst serve viable tool alleviate measurement requirement not however provide improvement intrinsic exponential scaling qst exponential barrier typically overcome only situation quantum state assumed some speciﬁc regularity property tomography based paremeterizations density trix ha important ﬁrst step direction allowing tomography large tum system lanyon et 2017 ml approach qst emerged recent time viable alternative especially highly gled state speciﬁcally assuming nqs form see eq 3 case pure state qst reformulated unsupervised ml learning task scheme trieve phase case pure state ha demonstrated torlai et 2018 application complex phase retrieved upon reconstruction several probability density associated measurement ce diﬀerent basis overall approach ha lowed demonstrate qst highly entangled state 100 qubits unfeasible full qst technique tomography approach suitably generalized case mixed state introducing parameterizations density matrix based either puriﬁed nqs torlai melko 2018 deep normalizing ﬂows mer et 2019 former approach ha also demonstrated experimentally rydberg atom lai et 2019 interesting alternative nqs representation tomographic purpose ha also cently suggested carrasquilla et 2019 based parameterizing density matrix directly term valued measure povm operator approach therefore ha important advantage directly learning measurement process ha demonstrated scale well rather large mixed state possible inconvenient approach density matrix only implicitly deﬁned term generative model opposed explicit tions found approach approach qst explored use quantum state parameterized local hamiltonians xin et 2018 intriguing sibility bypassing qst directly measure quantum entanglement gray et 2018 extension complex problem quantum process tomography also promising banchi et 2018 scalability approach larger system still present challenge finally problem learning quantum state experimental measurement ha also profound tions understanding complexity tum system framework pac ity quantum state aaronson 2007 tally demonstrated rocchetto et 2017 shadow tomography approach aaronson 2017 showed even linearly sized training set vide suﬃcient information succeed certain quantum learning task guarantee come computational restriction learning ﬁcient only special class state rocchetto 2018 controlling preparing qubits central task quantum control following given evolution u θ depends parameter θ map initial quantum state θ u θ parameter overlap distance prepared state target state θ facilitate analytic study space possible control intervention often discretized u θ u st becomes sequence step st example control ﬁeld could applied only two diﬀerent strength goal ﬁnd optimal strategy st 1 bring initial state close possible target state using only discrete action setup directly generalizes reinforcement learning framework sutton barto 2018 agent pick move list allowed control ventions two ﬁeld strength applied quantum state qubit framework ha proven competitive method various ting state preparation body quantum system interacting qubits bukov et 2018 use strong periodic oscillation prepare state bukov 2018 recent study comparing deep reinforcement learning traditional optimization method stochastic gradient descent preparation gle qubit state show learning advantage action space naturally discretized suﬃciently small zhang et 2019 picture becomes increasingly complex slightly realistic setting example control noisy niu et 2018 interesting twist trol problem ha also tackled predicting future noise using recurrent neural network analysis time series past noise using prediction ticipated future noise corrected mavadia et 2017 altogether diﬀerent approach state preparation machine learning try ﬁnd optimal strategy evaporative cooling create condensate 31 wigley et 2016 online optimization egy based bayesian optimization frazier 2018 jones et 1998 gaussian process used statistical model capture relationship trol parameter quality condensate strategy discovered machine learning model low cooling protocol us 10 time fewer tions pure optimization technique interesting feature contrary common reputation machine learning gaussian process allows mine control parameter important others another angle captured approach learn sequence optical instrument order pare highly entangled photonic quantum state nikov et 2018 error correction one major challenge building universal quantum computer error correction any putation error introduced physical imperfection hardware classical computer allow simple error correction based duplicating mation theorem quantum mechanic requires complex solution proposal surface code prescribes encode one ical qubit topological state several physical qubits measurement physical qubits reveal footprint chain error event called syndrome decoder map syndrome error sequence known corrected applying error sequence without aﬀecting logical qubits store actual quantum information roughly stated art quantum error correction therefore predict error syndrome task naturally ﬁts framework machine learning past year various model plied quantum error correction ranging vised unsupervised reinforcement learning detail application became increasingly complex one ﬁrst proposal deploys boltzmann machine trained data set pair error syndrome speciﬁes probability p error syndrome used draw sample desired distribution p torlai melko 2017 simple recipe show performance certain kind error comparable common benchmark relation tween syndrome error likewise learned neural network krastanov jiang 2017 maskara et 2019 varsamopoulos et 2017 ever strategy suﬀer scalability issue space possible decoder explodes data acquisition becomes issue recently neural network combined concept renormalization group address problem varsamopoulos et 2018 signiﬁcance diﬀerent neural network ha studied varsamopoulos et 2019 besides scalability important problem quantum error correction syndrome measurement cedure could also introduce error since involves plying small quantum circuit setting increase problem complexity essential real application noise identiﬁcation error mitigated repeated cycle syndrome measurement consider additional time dimension recurrent ral network architecture proposed baireuther et 2018 another avenue consider decoding reinforcement learning problem sweke et 2018 agent choose consecutive operation acting physical qubits opposed logical qubits rect syndrome get rewarded sequence corrected error much machine learning error correction focus surface code represent logical qubit physical qubits according some set scheme ment agent also set agnostic code one could say learn code along decoding strategy ha done quantum memory system quantum state supposed stored rather manipulated nautrup et 2018 well feedback control framework protects qubits decoherence fösel et 2018 finally beyond traditional reinforcement learning novel gy projective simulation used combat noise tiersch et 2015 summary machine learning quantum error correction problem several layer complexity realistic application requires rather complex learning framework nevertheless natural candidate machine learning especially ment learning vi chemistry material machine learning approach applied dict energy property molecule solid popularity application increasing matically quantum nature atomic interaction make energy evaluation computationally expensive ml method particularly useful many calculation required recent year expanding application ml chemistry rial research include predicting structure related molecule calculating energy surface based molecular dynamic md simulation identifying structure desired material property creating learned density functionals type problem input descriptor must account diﬀerences atomic 32 environment compact way much current work using ml atomistic modeling based early work describing local atomic environment metry function input neural work behler parrinello 2007 representing atomic potential using gaussian process regression method bartók et 2010 using sorted interatomic tances weighted nuclear charge coulomb matrix molecular descriptor rupp et 2012 continuing development suitable structural tations reviewed behler 2016 discussion ml chemical system general including learning relationship found review butler et al 2018 additional focus enabled theoretical chemistry reviewed rupp et al 2018 section present recent ples ml application chemical physic energy force based atomic environment one primary us ml chemistry rial research predict relative energy series related system typically compare diﬀerent structure atomic composition cation aim determine structure likely observed experimentally identify molecule may synthesizable drug candidate example supervised learning ml method employ iou quantum chemistry calculation label molecular representation xµ corresponding energy yµ generate training test data set xµ yµ n quantum chemistry application nn method great success predicting relative energy wide range system including constitutional isomer conﬁgurations molecule using symmetry function describe local atomic neighborhood atom behler 2016 many success area derived type decomposition molecular energy element represented using separate nn behler parrinello 2007 see fig 6 example 1 deep nn potential successfully trained return density functional theory dft energy any molecule 8 heavy atom h c n smith et 2017 work atomic coordinate training set selected using normal mode sampling include some vibrational perturbation along optimized ometries another example general nn ular atomic system deep potential ular dynamic dpmd method speciﬁcally created run md simulation trained energy bulk simulation zhang et rather ply include interaction via total energy system another approach wa inspired body expansion used standard computational physic case adding layer allow interaction nns improved molecular energy diction lubber et 2018 example use invariant representation atomic environment thanks incorporation symmetry function nn input some application ing molecular reaction material phase mations atomic representation must also ous diﬀerentiable smooth overlap atomic position soap kernel address ments including similarity metric atomic environment bartók et 2013 recent work serve symmetry alternate molecular representation address problem diﬀerent way capitalize known molecular symmetry coulomb matrix put bonding rigid dynamic symmetry incorporated improve coverage training data conﬁgurational space chmiela et 2018 work also includes force training allowing md simulation level coupled cluster lations small molecule would traditionally intractable molecular symmetry also learned shown determining local environment descriptor make use convolution scribe atomic interaction schütt et 2018 development atom environment descriptor compact unique diﬀerentiable certainly tate new us ml model study molecule material however machine learning ha also applied way closely integrated conventional approach easily incorporated ing code example atomic charge assignment patible classical force ﬁelds learned without need run new quantum mechanical calculation new molecule interest sifain et 2018 addition condensed phase simulation molecular specie require accurate intermolecular tials diﬃcult parameterize end local nn potential combined motivated coulomb van der waals butions describe larger molecular system yao et 2018 local ml description also successfully combined expansion method allow application ml potential larger system strated water cluster nguyen et 2018 tively intermolecular interaction ﬁtted set ml model trained monomer create transferable model dimer cluster bereau et 2018 potential free energy surface machine learning method also employed scribe free energy surface rather learning 33 figure 6 several representation currently used describe molecular system ml model including atomic nates symmetry function encoding local bonding environment input neural network reproduced gastegger et 2017 b nuclear potential approximated sum gaussian function input kernel ridge regression model electron density modiﬁed brockherde et 2017 tential energy molecular conformation directly described alternate approach learn free energy surface system function collective variable global steinhardt order parameter local dihedral angle set atom compact ml representation free energy surface fe using nn allows improved sampling high dimensional space calculating observables depend ensemble conformers example learned fe sampled predict isothermal compressibility solid xenon pressure expected nmr spin j coupling peptide schneider et 2017 small nn representing fe also trained tively using data point generated adaptive sampling sidky whitmer 2018 promising approach highlight beneﬁt using smooth sentation full conﬁgurational space using ml model generate new training data use fe representation crease important determine limit accuracy small nn use model starting point larger network ml tectures relevant minimum identiﬁed fe next challenge understand process take system one basin another ample developing markov state model describe formational change requires dimensionality reduction translate molecular coordinate global reaction coordinate space end power deep ing autoencoder method ha nessed identify slowly changing collective variable peptide folding example wehmeyer noé 2018 variational approach ha also used identify important kinetic process protein ing simulation provides framework unifying coordinate transformation fe surface exploration mardt et 2018 promising alternate approach use ml sample conformational distribution rectly boltzmann generator sample rium distribution collective variable space sequently provide set state represent tribution state fe noé et 2019 furthermore long history ﬁnding relationship minimum complex energy landscape may also useful learn understand ml model hibit general success relationship method idea currently used describe molecular system corresponding reviewed ballard et 2017 going forward many tool developed physicist explore quantify feature energy landscape may helpful creating new algorithm eﬃciently optimize model weight training see also related discussion sec area interdisciplinary research promise yield method useful machine learning physic ﬁelds material property using learned interatomic potential based local vironments ha also aﬀorded improvement tion material property matching experimental data typically requires sampling ensemble possible conﬁgurations come considerable cost using large simulation cell conventional method recently structure material property phous silicon predicted using molecular dynamic md ml potential trained density functional theory dft calculation only small simulation cell deringer et 2018 related application using ml potential model phase change crystalline amorphous region material gete amorphous carbon reviewed sosso et al 2018 generating potential suﬃciently accurate describe phase change relative energy defect atomistic terial scale quite diﬃcult however recent success silicon property indicates ml method challenge bartók et 2018 ideally experimental measurement could also corporated ml method aim dict material property however reported result 34 often limited material no counter example training process addition noisy data coupled lack precise structural formation needed input ml model organic molecular crystal challenge come prediction nmr chemical shift sensitive local environment using gaussian process regression framework trained value known structure paruzzo et 2018 ing calculated value experimental result prior training ml model enabled validation dicted pharmaceutical crystal structure intriguing direction include identiﬁcation structurally similar material via clustering using convex hull construction determine many predicted structure stable certain thermodynamic constraint anelli et 2018 using descriptor construction vex hull ha applied identify crystalline ice phase wa shown cluster thousand structure fer only proton disorder stacking fault engel et 2018 see fig 7 method based combination supervised unsupervised technique certainly promise fruitful research area future particular remains exciting challenge identify predict even suggest material exhibit particular desired property electron density density functional theory many example density functional ory calculation used source training data ﬁtting machine learning also playing role creating new density functionals machine ing natural choice situation dft not knowledge functional form exact solution beneﬁt approach ing density functional wa illustrated approximating kinetic energy functional electron distribution potential well snyder et 2012 use standard based dft code derivative ml functional must also used ﬁnd priate ground state electron distribution using kernel ridge regression without modiﬁcation lead noisy derivative projecting resulting energy back onto learned space using pca resolve sue li et 2015 approach learning potential ha also strated system nagai et 2018 case ml method make direct use derivative erated nn training step also possible bypass functional derivative entirely using ml generate appropriate ground state electron density corresponds nuclear tential brockherde et 2017 shown fig 6 b furthermore work demonstrated energy molecular system also learned electron sities input enabling reactive md simulation proton transfer event based dft energy ingly approximate electron density sum density isolated atom ha also successfully employed input predicting molecular energy eickenberg et 2018 related approach odic crystalline solid used local electron density embedded atom method train bayesian ml el return total system energy schmidt et 2018 since total energy extensive property scalable nn model based summation local electron density ha also developed run large tions porous graphene sheet mill et 2019 success ha become clear given sity functional machine learning oﬀers new way learn electron density corresponding system energy many approach improving proximate functionals use today rely imposing constraint far including type restriction method ha met only partial success example requiring ml functional fulﬁll one constraint scaling law improves overall formance manner hollingsworth et 2018 obtaining accurate derivative larly molecule conformational change still open question ml functionals potential not explicitly trained goal bereau et 2018 snyder et 2012 data set generation application machine learning parison various method requires standardized data set quantum chemistry include molecule data set ramakrishnan et 2014 benchmark data set composed subset small molecule peptide data set entry optimized using computational method smith et 2018 chemistry material research computational data often expensive generate selection ing data point must carefully considered put output representation also inform choice data inspection molecular energy data set showed importance choosing input data structure convey conformer change faber et 2017 addition dense sampling chemical composition space not always sary example initial ani training set 20 lion molecule could replaced million ing point selected using active learning method 35 figure 7 clustering thousand possible ice structure based descriptor identiﬁes observed form group similar structure together reproduced engel et 2018 added poorly predicted molecular example training cycle smith et 2018 alternate sampling approach also used eﬃciently build training set range active learning od estimate error multiple nn evaluation new molecule gastegger et 2017 generating new atomic conﬁgurations based md simulation using model zhang et esting insight theoretical aspect active learning wa presented seung et work area needed identify atomic composition conﬁgurations important diﬀerentiating candidate structure nn shown generate accurate energy amount data required prevent prohibitively expensive many case speciﬁc task predicting anharmonic contribution brational frequency small molecule formaldehye gaussian process method accurate used fewer point nn although point need selected carefully kamath et 2018 balancing computational cost data generation ease model training model evaluation time continues important consideration choosing appropriate ml method application outlook challenge going forward ml model beneﬁt ing method practice developed problem physic some idea already explored exploiting input data symmetry molecular conﬁgurations still many nities improve model training eﬃciency ization some promising challenging area include applying method exploration dimensional landscape optimization identifying include boundary behavior scaling law ml architecture 36 put data format connect directly imental data future ml method account uncertainty error calculation measured property avoid prove transferability model vii ai acceleration classical quantum hardware area physic contribute chine learning mean tool theoretical investigation problem novel ware platform may help expensive information cessing pipeline extend number crunching ities cpu gpus also known ai accelerator physic research ha oﬀer variety device could potentially enhance machine learning beyond von neumann architecture speak computer usually think versal digital computer based electrical circuit boolean logic von neumann paradigm modern computing any physical tem interpreted way process information namely mapping input parameter mental setup measurement result output way thinking close idea analog ing ha seems ambs 2010 berg 2005 dwarfed digital cousin application context machine ing however computation executed analog computing device found new surge interest hardware used emulate full model inspired chip ambrogio et 2018 outsource only subroutine computation done gate array fpgas integrated circuit asics fast linear algebra computation jouppi et 2017 markidis et 2018 following present selected example various research direction investigate hardware platform physic lab optic ic quantum computer become novel kind ai accelerator neural network running light processing information optic natural pealing alternative least complement computer fast made massively parallel requires low power consumption optical connects already widespread carry information short long distance light interference property also leveraged order provide advanced processing case machine learning one perk some standard building block tic lab striking resemblance way mation processed neural network killoran et 2018 lin et 2018 shen et 2017 insight no mean new lu et 1989 example large bulk optic experiment ic network interferometer interferometer passive optical element made beam splitter phase shifter clements et 2016 reck et 1994 consider amplitude light mode ing signal interferometer eﬀectively applies unitary transformation input see figure 8 left fying damping amplitude understood applying diagonal matrix consequently mean singular value decomposition ampliﬁer sandwiched two interferometer implement arbitrary matrix multiplication data encoded optical plitudes adding operation ally hardest precisely control lab turn device emulator standard neural network layer lin et 2018 shen et 2017 speed light interesting question ask use tum instead classical light example imagine information encoded quadrature tromagnetic ﬁeld quadrature much like sition momentum quantum particle two commuting operator describe light quantum system exchange setup quantum optic component squeezer displacers get neural network encoded quantum property light killoran et 2018 using multiple layer choosing nonlinear operation component optical kerr admittedly still experimental challenge optical setup becomes universal tum computer run any computation quantum computer perform true quantum neural network variation quantum cal neural net example information encoded discrete rather property light steinbrecher et 2018 investigation quantum device mean machine learning example whether pattern data easier recognized begun revealing feature data one doe not implement full machine ing model physical hardware outsource 37 figure 8 illustration method discussed text optical component interferometer ampliﬁers emulate neural network map input x ϕ wx w learnable weight matrix ϕ nonlinear activation using quantum optic component displacement squeezing one encode information quantum property light turn neural net universal quantum computer random embedding optical processing unit data encoded laser beam spatial light modulator dmd diﬀusive medium generates random feature quantum computer used compute distance data point quantum kernel ﬁrst part quantum algorithm us routine sx embed data hilbert space second part reveals inner product embedded vector kernel processed standard kernel method support vector machine single component example highlight second application data preprocessing feature extraction includes mapping data another space either compressed blown case revealing feature machine learning algorithm one approach data compression expansion physical device leverage statistical nature many machine learning algorithm multiple light tering generate ness needed random embeddings see figure 8 top right nutshell multiplication set vector random matrix approximately johnson lindenstrauss 1984 used dimensionality reduction data compression spirit compressed sensing donoho 2006 eﬃcient nearest neighbor search ity sensitive hashing also used sionality expansion limit large sion approximates kernel saade et 2016 device built optic coherent laser source commercial light modulators cmos sensor scattering material see machine learning application range transfer learning deep neural network time series analysis feedback loop implementing network dong et 2018 detection keriven et 2018 data device already outperform cpu gpus speed power consumption machine learning fair amount eﬀort ﬁeld quantum machine learning ﬁeld investigates intersection tum information intelligent data mining biamonte et 2017 schuld petruccione go application quantum hardware ing task et 2017 noisy quantum nisq device not only hoped enhance machine learning application term speed may lead entirely new rithms inspired quantum physic already mentioned one example quantum neural network emulate classical neural net go beyond model fall larger class tional parametrized quantum machine learning rithms mcclean et 2016 mitarai et 2018 idea make quantum algorithm thereby device implementing quantum computing operation depend parameter θ trained data measurement trained device represent new put artiﬁcially generated data sample erative model classiﬁcations supervised classiﬁer another idea use quantum computer 38 hance learning inspired kernel method hofmann et 2008 see figure 8 bottom right associating parameter quantum algorithm input data sample x one eﬀectively embeds x tum state x vector hilbert space havlicek et 2018 schuld killoran 2018 simple interference routine measure overlap two quantum state prepared way overlap inner product vector hilbert space machine literature known kernel distance measure two data point result quantum computer compute rather exotic kernel may classically intractable active area search ﬁnd interesting quantum kernel machine learning task beyond quantum kernel variational circuit quantum machine learning present many idea use quantum hardware ai accelerator ple sampler training inference graphical model adachi henderson 2015 benedetti et 2017 linear algebra computation lloyd et 2014 another interesting branch research gate quantum device directly analyze data produced quantum experiment without making detour measurement cong et 2018 exploration major challenge still severe tions nisq device reduce ical experiment hardware demonstration theoretical analysis remains riously diﬃcult machine learning outlook challenge example demonstrate way physic research contribute machine learning namely investigating new hardware platform execute tiresome computation standard von neumann gy struggle keep pace moore law open number opportunity novel computing paradigm simplest embodiment take form specialized accelerator device plugged onto standard server accessed custom apis future search focus hardware ities innovation machine learning adapted programming language well er optimized distribution computing task hybrid server 2 many quantum machine learning algorithm based linear gebra acceleration recently shown make unfounded claim exponential speedup tang 2018 compared classical algorithm analysing datasets strong sampling access however still interesting context even constant speedup make diﬀerence viii conclusion outlook number overarching theme become apparent ter reviewing way machine learning used ha enhanced diﬀerent discipline physic first clear interest machine ing technique suddenly surged recent year true even area statistical physic energy physic connection machine learning technique ha long history seeing research move exploratory eﬀorts toy model towards use real experimental data also seeing evolution understanding limitation approach situation performance justiﬁed theoretically healthy critical ment potential power limitation chine learning includes analysis od break distinctly not good physicist notoriously hungry detailed derstanding method work machine learning incorporated physicist box reasonable expect physicist may shed light some notoriously diﬃcult question chine learning facing speciﬁcally physicist ready contributing issue interpretability niques validate guarantee result ple way chose various parameter neural network architecture one direction physic community ha much learn machine learning community culture practice sharing code developing benchmark datasets thermore physic would well emulate practice developing portable implementation key method ideally involvement fessional software engineer picture emerges level activity enthusiasm surrounding ﬁrst success story interaction machine learning physical science merely infancy ticipate exciting result stemming play machine learning physical science acknowledgement research wa supported part tional science foundation grant no nsf 1420073 u army research oﬃce number well erc european union horizon 2020 research innovation programme grant agreement smile additionally would like thank support moore sloan foundation kavli institute theoretical physic ucsb institute 39 vanced study finally would like thank michele ceriotti yoav levine andrea rocchetto mile mire ryan sweke reference aaronson 2007 proceeding royal society mathematical physical engineering science 463 2088 aaronson 2017 arxiv acar yener 2009 ieee transaction edge data engineering 21 1 acciarri et al microboone 2017 jinst 12 03 adachi henderson 2015 arxiv preprint advani saxe 2017 arxiv preprint agresti viggianiello flamini spagnolo crespi osellame wiebe sciarrino 2019 physical review x 9 1 albergo kanwar shanahan 2019 phys rev 3 034515 albertsson et al 2018 proceeding international workshop advanced computing analysis niques physic research acat 2017 seattle wa usa august 2017 phys conf ser 1085 2 022008 alet laﬂorencie 2018 comptes rendus physique quantum simulation simulation quantique 19 6 alsing wandelt 2019 alsing wandelt feeney 2018 mon not roy astron soc 477 3 2874 amari 1998 neural computation 10 2 ambrogio narayanan tsai shelby boybat nolfo sidler giordano bodini farinha et al 2018 nature 558 7708 ambs 2010 advance optical technology amit gutfreund sompolinsky 1985 ical review 32 2 anandkumar ge hsu kakade garsky 2014 journal machine learning research 15 anelli engel pickard ceriotti 2018 phys rev mater 2 apollinari brüning nakamoto rossi 2015 cern yellow report 5 1 armitage kay barnes 2019 mnras 484 1526 arsenault neuberg hannah millis 2017 inverse problem 33 11 arunachalam de wolf 2017 acm sigact news 48 2 atlas collaboration 2018 deep generative model fast shower simulation atlas tech cern geneva aubin maillard krzakala macris zdeborová et al 2018 advance neural information processing system pp preprint aurisano radovic rocco himmel messier niner pawloski psihas sousa vahle 2016 jinst 11 09 baireuther brien tarasinski beenakker 2018 quantum 2 sagun geiger spigler arous cammarota lecun wyart biroli 2018 icml 2018 arxiv preprint baldassi borgs chayes ingrosso lucibello saglietti zecchina 2016 proceeding national academy science 113 48 baldassi ingrosso lucibello saglietti zecchina 2015 physical review letter 115 12 baldi bauer eng sadowski whiteson physical review 93 9 baldi cranmer faucett sadowski whiteson eur phys 5 235 baldi sadowski whiteson 2014 nature mun 5 4308 ball et al nnpdf 2015 jhep 04 040 ballard da martiniani mehta sagun stevenson wale 2017 phys chem chem phys 19 20 banchi grant rocchetto severini 2018 new journal physic 20 12 bang ryu yoo pawłowski lee 2014 new journal physic 16 7 barbier dia macris krzakala lesieur zdeborová 2016 advance neural information processing system pp barbier krzakala macris miolane borová 2019 proceeding national academy ences 116 12 barkai sompolinsky 1994 physical review e 50 3 barra genovese sollich tantari 2018 physical review e 97 2 bartók kermode bernstein csányi 2018 physical review x 8 4 bartók kondor csányi 2013 phys rev b 87 bartók payne kondor csányi 2010 phys rev lett 104 13 baydin heinrich bhimji louppe shao prabhat cranmer wood 2018 beach golubeva melko 2018 ical review b 97 4 beaumont zhang balding 2002 netics 162 4 becca sorella 2017 quantum monte carlo approach correlated system cambridge university press cambridge united kingdom new york ny behler j 2016 chem phys 145 17 behler parrinello 2007 phys rev lett 98 14 583 40 benedetti biswas ortiz 2017 physical review x 7 4 benítez 2000 astrophys j 536 571 arxiv bény 2013 iclr 2013 arxiv preprint bereau distasio tkatchenko von lilienfeld 2018 chem phys 148 24 biamonte wittek pancotti rebentrost wiebe lloyd 2017 nature 549 7671 biehl mietzner 1993 epl europhysics ters 24 5 bishop 2006 pattern recognition machine ing springer bohrdt chiu ji xu greif greiner demler grusdt knap 2018 bolthausen 2014 communication mathematical physic 325 1 bonnett et al de 2016 phys rev 4 042005 borin abanin 2019 mat physic bozson cowan spanò 2018 bradde bialek 2017 journal statistical physic 167 brammer van dokkum coppi 2008 astrophys j 686 1503 brehmer cranmer louppe pavez phys rev 5 052004 brehmer cranmer louppe pavez phys rev lett 121 11 111801 ph brehmer louppe pavez cranmer breiman friedman olshen stone 1984 brockherde vogt li tuckerman burke müller 2017 nat commun 8 1 broecker assaad trebst broecker carrasquilla melko trebst scientiﬁc report 7 1 bronstein bruna lecun szlam vandergheynst 2017 ieee sig proc mag 34 4 18 bukov 2018 physical review b 98 22 bukov day sels weinberg polkovnikov mehta 2018 physical review x 8 3 butler davy cartwright isayev walsh 2018 nature 559 cai liu 2018 physical review b 97 3 cameron pettitt 2012 mnras 425 44 cariﬁo halverson krioukov nelson 2017 jhep 09 157 carleo becca schiro fabrizio 2012 scientiﬁc report 2 carleo nomura imada 2018 nature munications 9 1 carleo troyer 2017 science 355 6325 carrasco kind brunner 2013 mnras 432 1483 carrasquilla melko 2017 nature physic 13 5 carrasquilla torlai melko aolita 2019 nature machine intelligence 1 3 casado et al 2017 changlani kinder umrigar chan 2009 physical review b 80 24 charnock lavaux wandelt 2018 phys rev 8 083004 chaudhari choromanska soatto lecun dassi borgs chayes sagun zecchina 2016 iclr 2017 arxiv preprint chen xu liu batrouni scalettar meng physical review b 98 4 chen cheng xie wang xiang physical review b 97 8 cheng wang xiang zhang 2019 physic stat chmiela sauceda müller tkatchenko 2018 nat commun 9 choma monti gerhardt palczewski ronaghi prabhat bhimji bronstein klein bruna 2018 arxiv choo carleo regnault neupert 2018 physical review letter 121 16 choromanska henaﬀ mathieu arous lecun 2015 artiﬁcial intelligence statistic pp chung lee sompolinsky 2018 physical review x 8 3 ciliberto herbster ialongo pontil chetto severini wossnig 2018 proceeding royal society mathematical physical neering science 474 2209 clark 2018 journal physic mathematical theoretical 51 13 clements humphreys metcalf kolthammer walmsley 2016 optica 3 12 cocco monasson posani rosay tubiana 2018 physica statistical mechanic tions 504 cohen sharir shashua 2016 nual conference learning theory proceeding chine learning research vol 49 edited feldman rakhlin shamir pmlr columbia university new york new york usa pp cohen welling 2016 international conference machine learning pp cohen geiger köhler welling 2018 proceeding international ence learning representation iclr arxiv preprint cohen weiler kicanaoglu welling 2019 arxiv krzakala perkins zdeborová 2018 advance mathematics 333 collett 2015 astrophysical journal 811 1 collister lahav 2004 publication astronomical society paciﬁc 116 345 arxiv cong choi lukin 2018 arxiv preprint 41 cranmer golkar pappadopulo 2019 physic physic physic ph stat arxiv cranmer louppe 2016 brief idea cranmer pavez louppe 2015 cristoforetti jurman nardelli furlanello 2017 physic cubuk schoenholz rieser malone rottler durian kaxiras liu 2015 physical review letter 114 10 cybenko 1989 mathematics control signal tems 2 4 czischek gärttner gasenzer 2018 physical review b 98 2 dean 1996 journal physic mathematical general 29 24 decelle fissore furtlehner 2017 epl rophysics letter 119 6 decelle krzakala moore zdeborová physical review e 84 6 decelle krzakala moore zdeborová physical review letter 107 6 deng li da sarma physical view b 96 19 deng li da sarma physical view x 7 2 deringer bernstein bartók cliﬀe kerber marbella grey elliott csányi 2018 phys chem lett 9 11 deshpande montanari 2014 2014 ieee ternational symposium information theory dirac 1930 mathematical proceeding cambridge philosophical society 26 03 doggen schindler tikhonov mirlin neupert polyakov gornyi 2018 physical review b 98 17 dong gigan krzakala wainrib 2018 2018 ieee statistical signal processing workshop ssp ieee pp donoho 2006 ieee transaction information ory 52 4 duarte et al 2018 jinst 13 07 dunjko briegel 2018 report progress physic 81 7 duvenaud lloyd grosse tenenbaum zoubin 2013 international conference machine learning pp eickenberg exarchakis hirn mallat thiry 2018 chem phys 148 24 engel van den broeck 2001 statistical ic learning cambridge university press engel anelli ceriotti pickard need 2018 nat commun 9 estrada annis diehl hall la lin makler merritt scarpine lam tucker 2007 astrophys j 660 1176 faber hutchison huang gilmer holz dahl vinyals kearnes riley von lilienfeld 2017 chem theory comput 13 11 fabiani mentink 2019 scipost physic 7 1 farrell et al 2018 international workshop necting dot 2018 seattle washington usa march 2018 feldmann carollo porciani lilly pak taniguchi le fèvre renzini ille ajiki aussel contini mccracken mobasher murayama sander sasaki lata scodeggio shioya silverman takahashi thompson zamorani 2006 mnras 372 565 arxiv firth lahav somerville 2003 mnras 339 1195 arxiv hogg lang goodman 2013 125 306 forte garrido latorre piccione 2002 jhep 05 062 arxiv fortunato 2010 physic report 486 fournier wang yazyev wu 2018 physic physic frate cranmer kalia whiteson 2017 frazier 2018 arxiv preprint frenkel 1934 wave mechanic advanced general theory international series monograph nuclear energy reactor design physic no 2 clarendon press freund schapire 1997 comput syst sci 55 1 fösel tighineanu wei marquardt 2018 physical review x 8 3 gabrié manoel luneau macris kala zdeborová et al 2018 advance neural information processing system pp preprint gabrié tramel krzakala 2015 vances neural information processing system pp gao duan 2017 nature communication 8 1 gardner 1987 epl europhysics letter 4 4 gardner 1988 journal physic mathematical general 21 1 gardner derrida 1989 journal physic mathematical general 22 12 gastegger behler marquetand 2017 chem sci 8 10 gendiar nishino 2002 physical review e 65 4 glasser pancotti august rodriguez cirac physical review x 8 1 glasser pancotti cirac arxiv preprint gligorov williams 2013 jinst 8 goldt advani saxe krzakala borová arxiv preprint goldt advani saxe krzakala borová arxiv preprint golkar cranmer 2018 42 goodfellow bengio courville 2016 deep learning mit press goodfellow mirza xu farley ozair courville bengio 2014 vances neural information processing system pp gorodetsky karaman marzouk 2019 puter method applied mechanic engineering 347 gray banchi bayat bose 2018 physical review letter 121 15 greitemann liu pollet et al 2019 physical view b 99 6 gross liu flammia becker eisert 2010 physical review letter 105 15 guest collado baldi hsu urban whiteson 2016 phys rev 11 112002 guest cranmer whiteson 2018 ann rev nucl part sci 68 161 guo jie lu poletti 2018 physical view e 98 4 györgyi 1990 physical review 41 12 györgyi tishby 1990 theumann kobrele editor neural network spin glass haah harrow ji wu yu 2017 ieee transaction information theory 63 9 hackbusch kühn 2009 journal fourier ysis application 15 5 han zhang e physic han wang fan wang zhang physical review x 8 3 hartmann carleo 2019 physic hashimoto sugishita tanaka tomiya phys rev 98 hashimoto sugishita tanaka tomiya phys rev 4 046019 havlicek córcoles temme harrow chow gambetta 2018 arxiv preprint li feng ho ravanbakhsh chen póczos 2018 arxiv henson kay barnes mccarthy schaye jenkins 2016 monthly notice royal astronomical society 465 1 213 herman begy louppe 2019 arxiv hezaveh perreault levasseur marshall 2017 nature 548 555 hinton 2002 neural computation 14 8 ho rau ntampaka farahi trac poczos 2019 hochreiter schmidhuber 1997 neural tion 9 8 hofmann schölkopf smola 2008 annals statistic hollingsworth baker burke 2018 chem phys 148 24 hopﬁeld j 1982 proceeding national academy science 79 8 hsu li deng da sarma 2018 physical review letter 121 24 hu singh scalettar 2017 physical review e 95 6 huang wang 2017 physical review b 95 3 huang moore 2017 mat huembeli dauphin wittek physical review b 97 13 huembeli dauphin wittek gogolin physic arxiv iakovlev sotnikov mazurenko 2018 physical review b 98 17 ilten williams yang 2017 jinst 12 04 ishida vitenti cisewski de souza trindade cameron busti coin collaboration 2015 astronomy computing 13 1 izmailov novikov kropotov 2017 c stat jacot gabriel hongler 2018 advance neural information processing system pp jaeger haas 2004 science 304 5667 jain srijith desai 2018 javanmard montanari 2013 information inference journal ima 2 2 johnson lindenstrauss 1984 contemporary mathematics 26 johnstone lu 2009 journal ican statistical association 104 486 jones schonlau welch 1998 journal global optimization 13 4 jouppi young patil patterson agrawal bajwa bates bhatia boden borchers et al 2017 computer architecture isca 2017 annual international symposium ieee pp jónsson bauer carleo 2018 physic physic physic kabashima krzakala mézard sakata zdeborová 2016 ieee transaction information theory 62 7 kalantre zwolak ragole wu merman stewart taylor 2019 npj tum information 5 1 kamath krems rington jr manzhos 2018 chem phys 148 24 kashiwa kikuchi tomiya 2019 progress theoretical experimental physic 2019 8 kasieczka plehn butter debnath fairbairn fedorko gay gouskos komiske lei et al 2019 kaubruegger pastori budich 2018 ical review b 97 19 195136 43 keriven garreau poli 2018 arxiv preprint killoran bromley arrazola schuld quesada lloyd 2018 quantum neural network kingma welling 2013 arxiv preprint ringel 2018 nature physic 14 6 kochkov clark 2018 physic physic komiske metodiev nachman schwartz phys rev 1 011502 komiske metodiev thaler jhep 04 013 komiske metodiev thaler 2019 jhep 01 121 kondor 2018 corr kondor lin trivedi 2018 neurips 2018 kondor trivedi 2018 international ence machine learning pp krastanov jiang 2017 scientiﬁc report 7 1 krenn malik fickler lapkiewicz zeilinger 2016 physical review letter 116 9 krzakala mézard zdeborová formation theory proceeding isit 2013 ieee tional symposium ieee pp krzakala moore mossel neeman sly borová zhang proceeding national academy science 110 52 lang hogg mykytyn 2016 tractor probabilistic astronomical source detection ment astrophysics source code library lanusse li collett li bakhsh mandelbaum póczos 2018 mnras 473 3895 lanyon maier holzäpfel baumgratz hempel jurcevic dhand buyskikh daley cramer plenio blatt roos 2017 nature physic advance online tion larkoski moult nachman 2017 larochelle murray 2011 proceeding fourteenth international conference artiﬁcial gence statistic pp le baydin wood 2017 artiﬁcial telligence statistic pp lecun bengio hinton 2015 nature 521 7553 lee bahri novak schoenholz pennington 2018 icrl 2018 leistedt hogg wechsler derose 2018 arxiv lelarge miolane 2016 probability theory related field levine sharir cohen shashua 2019 physical review letter 122 6 levine yakira cohen shashua 2017 iclr 2018 arxiv li snyder pelaschier huang ranjan duncan rupp müller burke 2015 int quantum chem 116 11 li gladders rangel florian bleem heitmann habib fasel 2016 astrophysical journal 828 1 li wang 2018 phys rev lett 121 liang liu lin guo zhang 2018 physical review b 98 10 likhomanenko ilten khairullin rogozhnikov ustyuzhanin williams 2015 proceeding international conference computing high ergy nuclear physic chep 2015 okinawa japan april 2015 phys conf ser 664 8 082025 lin rivenson yardimci veli luo rahi ozcan 2018 science 361 6406 liu ran wittek peng garcía su lewenstein physic physic physic stat arxiv liu qi meng fu physical review b 95 4 liu shen qi meng fu physical review b 95 24 liu greitemann pollet et al 2019 physical view b 99 10 liu zhang lewenstein ran 2018 physic stat arxiv lloyd mohseni rebentrost 2014 nature physic 10 louppe cho becot cranmer louppe herman cranmer louppe kagan cranmer 2016 lu gao duan 2018 physic arxiv lu wu xu francis 1989 applied optic 28 22 lubber smith barros 2018 chem phys 148 24 lundberg 2005 ieee control system 25 3 luo clark 2018 mat physic physic arxiv mannelli biroli cammarota krzakala urbani zdeborová 2018 arxiv preprint mannelli krzakala urbani zdeborová 2019 arxiv preprint mardt pasquali wu noé 2018 nat commun 9 marin pudlo robert ryder 2012 statistic computing marjoram molitor plagnol tavaré 2003 proceeding national academy science 100 26 markidis der chien laure peng vetter 2018 ieee international parallel distributed processing symposium workshop ipdpsw 44 marshall hogg moustakas nacht bradač schrabback blandford 2009 astrophysical journal 694 2 martiniani chaikin levine 2019 phys rev x 9 maskara kubica connor 2019 physical review 99 5 matsushita tanaka 2013 advance neural information processing system pp mavadia frey sastrawan dona cuk 2017 nature communication 8 mcclean romero babbush guzik 2016 new journal physic 18 2 mehta bukov wang day richardson fisher schwab 2018 arxiv preprint mehta schwab 2014 arxiv preprint mei montanari nguyen 2018 arxiv preprint melnikov nautrup krenn dunjko tiersch zeilinger briegel 2018 ings national academy science 115 6 metodiev nachman thaler 2017 jhep 10 174 mézard 2017 physical review e 95 2 mézard montanari 2009 information physic computation oxford university press mezzacapo schuch boninsegni cirac 2009 new journal physic 11 8 mill ryczko luchak domurad beeler tamblyn 2019 chem sci 10 15 minsky papert 1969 perceptrons tion computational geometry mit press cambridge usa mitarai negoro kitagawa fujii 2018 arxiv preprint morningstar melko 2018 journal machine learning research 18 163 morningstar hezaveh perreault asseur blandford marshall putzky wechsler 2018 arxiv morningstar perreault levasseur veh blandford marshall putzky rueter wechsler welling 2019 arxiv nagai akashi sasaki tsuneyuki 2018 chem phys 148 24 nagai shen qi liu fu 2017 physical review b 96 16 nagy savona 2019 physic nautrup delfosse dunjko briegel friis 2018 arxiv preprint ng jordan wei 2002 advance neural information processing system pp nguyen zecchina berg 2017 advance physic 66 3 nguyen székely imbalzano behler csányi ceriotti götz paesani 2018 chem phys 148 24 nielsen chuang 2002 quantum tion quantum information van nieuwenburg bairey refael 2018 ical review b 98 6 nishimori 2001 statistical physic spin glass information processing introduction vol 111 press niu boixo smelyanskiy neven 2018 arxiv preprint noé olsson köhler wu 2019 science 365 6457 nomura darmawan yamaji imada 2017 physical review b 96 20 novikov troﬁmov oseledets 2016 arxiv ntampaka trac sutherland battaglia póczos schneider 2015 astrophys j 803 50 ntampaka trac sutherland fromenteau póczos schneider 2016 astrophys j 831 135 ntampaka zuhone eisenstein nagai vikhlinin hernquist marinacci nelson pakmor pillepich torrey vogelsberger 2018 arxiv ntampaka et al 2019 nussinov ronhovde hu chakrabarty sun mauro sahu 2016 information ence material discovery design springer pp donnell wright 2016 proceeding annual acm symposium theory puting acm pp ohtsuki ohtsuki 2016 journal physical society japan 85 12 ohtsuki ohtsuki 2017 journal physical society japan 86 4 de oliveira kagan mackey nachman schwartzman 2016 jhep 07 069 oseledets 2011 siam journal scientiﬁc computing 33 5 paganini de oliveira nachman phys rev lett 120 4 042003 paganini de oliveira nachman phys rev 1 014021 pang zhou su petersen stöcker wang 2018 nature commun 9 1 210 papamakarios murray pavlakou 2017 advance neural information processing system pp papamakarios sterratt murray 2018 arxiv paris rehacek ed 2004 quantum state mation lecture note physic berlin heidelberg paruzzo hofstetter musil de ceriotti emsley 2018 nat commun 9 pastori kaubruegger budich 2018 physic physic physic arxiv pathak hunt girvan lu ott 2018 physical review letter 120 2 024102 45 pathak lu hunt girvan ott 2017 chaos interdisciplinary journal nonlinear science 27 12 peel lalande starck pettorino merten giocoli meneghetti baldi 2018 arxiv print benedetti biswas 2017 arxiv preprint póczos xiong sutherland schneider 2012 corr putzky welling 2017 arxiv preprint quek fort ng 2018 arxiv radovic williams rousseau kagan corsi himmel aurisano terao rad 2018 nature 560 7716 ramakrishnan dral rupp von lilienfeld 2014 sci data 1 rangan fletcher 2012 information theory proceeding isit 2012 ieee international symposium ieee pp ravanbakhsh lanusse mandelbaum der poczos 2016 arxiv ravanbakhsh oliva fromenteau price ho schneider poczos 2017 arxiv reck zeilinger bernstein bertani 1994 physical review letter 73 1 reddy celani sejnowski vergassola 2016 proceeding national academy science 113 33 reddy celani sejnowski vergassola 2018 nature 562 7726 regier miller schlegel adam mcauliﬀe prabhat 2018 arxiv rem käming tarnowski asteria fläschner becker sengstock berg 2018 physic ph arxiv ren girshick sun 2015 advance neural information processing system pp rezende mohamed 2015 international ference machine learning pp rezende mohamed wierstra 2014 proceeding international conference ternational conference machine 32 jmlr org pp riofrío gross flammia monz nigg blatt eisert 2017 nature communication 8 ritzmann von malottki kim heinze sinova dupé 2018 nature electronics 1 8 robin reylé fliri czekaj robert martin 2014 astronomy astrophysics 569 rocchetto 2018 quantum information tion 18 7 8 rocchetto aaronson severini carvacho poderini agresti bentivegna sciarrino 2017 arxiv rocchetto grant strelchuk carleo erini 2018 npj quantum information 4 1 rodríguez kacprzak lucchi amara sgier fluri hofmann réfrégier 2018 computational astrophysics cosmology 5 4 scheurer 2018 arxiv roe yang zhu liu stancu mcgregor 2005 nucl instrum meth 577 arxiv physic rogozhnikov bukva gligorov ustyuzhanin williams 2015 jinst 10 03 ronhovde chakrabarty hu sahu sahu kelton mauro nussinov 2011 pean physical journal e 34 9 rotskoﬀ 2018 advance neural information processing system pp rupp von lilienfeld burke 2018 chem phys 148 24 rupp tkatchenko müller von lilienfeld 2012 phys rev lett 108 saad solla physical review letter 74 21 saad solla physical review e 52 4 saade caltagirone carron daudet drémeau gigan krzakala 2016 acoustic speech signal processing icassp 2016 ieee international conference ieee pp saade krzakala zdeborová 2014 advance neural information processing system pp saito 2017 journal physical society japan 86 9 saito 2018 journal physical society japan 87 7 saito kato 2017 journal physical society japan 87 1 sakata kabashima 2013 epl europhysics letter 103 2 saxe bansal dapello advani kolchinsky tracey cox 2018 saxe mcclelland ganguli 2013 iclr 2014 arxiv preprint schawinski zhang zhang fowler santhanam 2017 monthly notice royal nomical society letter schindler regnault neupert 2017 physical review b 95 24 schmidhuber j 2014 corr schmidt fowler elliott bristowe 2018 comput mater sci 149 schmitt heyl 2018 scipost physic 4 2 schneider dai topper tuckerman 2017 phys rev lett 119 15 schoenholz cubuk kaxiras liu 2017 proceeding national academy science 114 2 schuch wolf verstraete cirac 2008 physical review letter 100 4 040501 46 schuld killoran 2018 arxiv preprint schuld petruccione quantum computing supervised learning springer schuld petruccione supervised learning quantum computer springer schütt sauceda kindermans tkatchenko müller 2018 chem phys 148 24 schwarze 1993 journal physic mathematical general 26 21 seif landsman linke figgatt roe hafezi 2018 journal physic b atomic molecular optical physic 51 17 174006 arxiv seung sompolinsky tishby physical review 45 8 seung opper sompolinsky proceeding ﬁfth annual workshop computational learning theory acm pp shanahan trewartha detmold 2018 phys rev 9 094506 sharir levine wy carleo shashua 2019 shen george huerta zhao 2019 shen harris skirlo prabhu hochberg sun zhao larochelle englund et al 2017 nature photonics 11 7 shi duan vidal 2006 physical review 74 2 shimmin sadowski baldi weik whiteson goul søgaard 2017 phys rev 7 074034 tishby 2017 arxiv preprint sidky whitmer 2018 chem phys 148 10 sifain lubber nebgen smith lokhov isayev roitberg barros tiak 2018 phys chem lett 9 16 sisson fan 2011 mcmc chapman new 839 sisson fan tanaka 2007 proceeding national academy science 104 6 smith isayev roitberg 2017 chem sci 8 4 smith nebgen lubber isayev roitberg 2018 chem phys 148 24 smolensky 1986 chap information processing namical system foundation harmony theory mit press cambridge usa pp snyder rupp hansen müller burke 2012 phys rev lett 108 25 sompolinsky tishby seung 1990 physical review letter 65 13 sorella 1998 physical review letter 80 20 sosso deringer elliott csányi 2018 mol simulat 44 11 steinbrecher olson englund carolan 2018 quantum optical neural network stevens williams 2013 jinst 8 stokes terilla 2019 arxiv preprint stoudenmire schwab 2016 advance neural information processing system 29 edited lee sugiyama luxburg guyon nett curran associate pp stoudenmire 2018 quantum science technology 3 3 sun yi zhang shen zhai 2018 ical review b 98 8 sutton barto 2018 reinforcement ing introduction mit press sweke kesselring van nieuwenburg eisert 2018 arxiv preprint tanaka tomiya journal physical society japan 86 6 tanaka tomiya lat tang 2018 arxiv preprint teng 2018 physical review e 98 3 thouless anderson palmer 1977 philosophical magazine 35 3 tiersch ganahl briegel 2015 scientiﬁc report 5 tishby pereira bialek 2000 arxiv preprint tishby zaslavsky 2015 information theory workshop itw 2015 ieee ieee pp torlai mazzola carrasquilla troyer melko carleo 2018 nature physic 14 5 torlai melko 2017 physical review letter 119 3 torlai melko 2018 physical review letter 120 24 torlai timar van nieuwenburg levine omran keesling bernien greiner vuletić lukin melko endres 2019 physic arxiv tóth wieczorek gross krischek mer weinfurter 2010 physical review letter 105 25 tramel gabrié manoel caltagirone krzakala 2018 physical review x 8 4 tsaris et al 2018 proceeding international shop advanced computing analysis technique physic research acat 2017 seattle wa usa august 2017 phys conf ser 1085 4 tubiana cocco monasson 2018 arxiv preprint tubiana monasson 2017 physical review letter 118 13 uria côté gregor murray larochelle 2016 journal machine learning research 17 205 valiant 1984 communication acm 27 11 van nieuwenburg liu huber 2017 nature physic 13 5 varsamopoulos bertels almudever 2018 arxiv preprint varsamopoulos bertels almudever 2019 arxiv preprint varsamopoulos criger bertels 2017 tum science technology 3 1 015004 47 venderley khemani kim 2018 physical review letter 120 25 verstraete murg cirac 2008 advance physic 57 2 vicentini biella regnault ciuti 2019 physic arxiv vidal 2007 physical review letter 99 22 von luxburg u 2007 statistic computing 17 4 wang hu lu 2018 arxiv preprint wang zhai 2017 physical review b 96 14 wang zhai 2018 frontier physic 13 5 wang 2016 physical review b 94 19 wang 2018 generative model physicist watkin nadal 1994 journal physic mathematical general 27 6 wecker hastings troyer 2016 physical review 94 2 wehmeyer noé 2018 chem phys 148 24 wetzel j 2017 physical review e 96 2 white 1992 physical review letter 69 19 wigley everitt van den hengel bastian sooriyabandara mcdonald hardman quinlivan manju kuhn petersen luiten hope robin hush 2016 scientiﬁc report 6 wu wang zhang 2018 arxiv preprint xin lu cao anikeeva lu li long zeng 2018 arxiv xu xu 2018 arxiv yao herr toth mckintyre parkhill 2018 chem sci 9 8 yedidia freeman wei 2003 ing artiﬁcial intelligence new millennium 8 yoon sim han 2018 physical review b 98 24 yoshioka hamazaki 2019 physic zdeborová krzakala 2016 advance physic 65 5 zhang bengio hardt recht vinyals 2016 arxiv preprint zhang han wang car e phys rev lett 120 14 zhang lin wang car et al arxiv preprint zhang shen zhai physical review letter 120 6 zhang wang wang 2019 physical review b 99 5 zhang wang zhang sun contardo ho 2019 arxiv zhang wei asad yang wang 2019 arxiv preprint zhang kim 2017 physical review letter 118 21 zhang melko kim 2017 physical review b 96 24 zhang mesaros fujita edkins hamidian ch ng eisaki uchida davis khatami kim physic physic zheng regnault bernevig 2018