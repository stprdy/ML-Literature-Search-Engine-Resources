hyperparameter optimization machine learning algorithm theory practice li yang abdallah shami department electrical computer engineering university western ontario 1151 richmond st london canada r c l e n f article history received 13 december 2019 revised 14 may 2020 accepted 16 july 2020 available online 25 july 2020 communicated yuhua cheng keywords optimization machine learning bayesian optimization particle swarm optimization genetic algorithm grid search b r c machine learning algorithm used widely various application area ﬁt machine learning model different problem must tuned selecting best parameter conﬁguration machine learning model ha direct impact model performance often requires deep knowledge machine learning algorithm appropriate mization technique although several automatic optimization technique exist different strength drawback applied different type problem paper optimizing common machine learning model studied introduce several art optimization technique discus apply machine learning algorithm many able library framework developed optimization problem provided some open challenge optimization research also discussed paper moreover experiment conducted benchmark datasets compare performance different optimization method provide practical example optimization survey paper help industrial user data analyst researcher better develop machine learning model identifying proper conﬁgurations effectively 2020 elsevier right reserved introduction machine learning ml algorithm widely used many application domain including advertising tion system computer vision natural language processing user behavior analytics 1 generic demonstrate high performance data analytics problem ent ml algorithm suitable different type problem datasets 2 general building effective machine learning model complex process involves determining appropriate algorithm obtaining optimal model architecture tuning hp 3 two type parameter exist machine learning model one initialized updated data learning ce weight neuron neural network named model parameter named not directly estimated data learning must set training ml model deﬁne model tecture 4 parameter used either conﬁgure ml model penalty parameter c support vector machine learning rate train neural work specify algorithm used minimize loss tion activation function optimizer type neural network kernel type support vector machine 5 build optimal ml model range possibility must explored process designing ideal model architecture optimal conﬁguration named parameter tuning tuning considered key component building effective ml model especially based ml model deep neural network many 6 tuning process different among different ml algorithm due different type including categorical discrete continuous 7 manual testing traditional way tune still prevalent graduate student research although requires deep understanding used ml algorithm value setting 8 ever manual tuning ineffective many problem due certain factor including large number complex model model evaluation interaction factor inspired increased research technique automatic optimization optimization hpo 2020 elsevier right reserved address yang shami neurocomputing 415 2020 content list available sciencedirect neurocomputing journal homepage 9 main aim hpo automate tuning process make possible user apply machine learning model practical problem effectively 3 optimal model architecture ml model expected obtained hpo process some important reason applying hpo technique ml model follows 6 reduces human effort required since many ml developer spend considerable time tuning cially large datasets complex ml algorithm large number improves performance ml model many ml parameter different optimum achieve best mance different datasets problem make model research reproducible only level tuning process implemented different ml algorithm compared fairly hence using hpo method different ml algorithm also help determine suitable ml model ﬁc problem crucial select appropriate optimization technique detect optimal traditional optimization niques may unsuitable hpo problem since many hpo problem optimization lem may result local instead global optimum 10 gradient method common type traditional optimization algorithm used tune continuous parameter calculating gradient 11 example learning rate neural network optimized method compared traditional optimization method like gradient descent many optimization technique suitable hpo problem including approach sian optimization model optimization technique metaheuristics algorithm 7 apart detecting ous many algorithm also capacity effectively identify discrete categorical tional method based concept ing search space detecting combination search space ultimately selecting combination grid search g 12 approach exhaustively search optimal conﬁguration ﬁxed domain parameter random search r 13 another theoretic method randomly selects nation search space given limited execution time resource g r conﬁguration treated independently unlike g r bayesian optimization bo 14 model determine next value based previous result tested value avoids many unnecessary evaluation thus bo detect optimal parameter combination within fewer iteration g r applied different problem bo model distribution objective function using different model surrogate function including gaussian process gp random forest rf parzen estimator tpe model 15 retain conditionality variable 15 thus used optimize conditional like kernel type penalty parameter c support vector machine svm however since bo model work sequentially balance exploration unexplored area exploitation region difﬁcult parallelize training ml model often take considerable time space optimization algorithm developed tackle problem limited resource common one algorithm hyperband 16 popular optimization technique considered improved version r generates small version datasets allocates budget combination iteration hyperband parameter conﬁgurations eliminated save time resource metaheuristic algorithm set technique used solve complex large search space optimization lem hpo problem belong 17 among metaheuristic method genetic algorithm ga 18 particle swarm mization pso 19 two prevalent metaheuristic rithms used hpo problem genetic algorithm detect performing combination generation pas next generation combination identiﬁed pso algorithm particle nicates particle detect update current global optimum iteration ﬁnal optimum detected metaheuristics efﬁciently explore search space detect optimal solution hence particularly suitable hpo problem large conﬁguration space due high efﬁciency instance used deep ral network dnns large conﬁguration space multiple including activation optimizer type learning rate rate etc although using hpo algorithm tune ml model greatly improves model performance certain aspect like computational complexity still much room improvement hand since different hpo model advantage suitable problem overviewing necessary proper optimization algorithm selection term different type ml model problem paper make following contribution review common ml algorithm important parameter analyzes common hpo technique including beneﬁts drawback help apply different ml model appropriate algorithm selection practical problem survey common hpo library framework practical use discus open challenge research direction hpo research domain survey paper begin comprehensive tion common optimization technique used ml parameter tuning problem section 2 introduces main cepts mathematical optimization mization well general hpo process section 3 discus key common ml model need tuned section 4 cover various tion approach proposed tackling hpo lem section 5 analyze different hpo method discus applied ml algorithm section 6 provide introduction various public library work developed implement hpo section 7 present discus experimental result using hpo benchmark datasets hpo method comparison practical use case demonstration section 8 discus several research direction open challenge considered improve current hpo model develop new hpo approach conclude paper section 9 296 yang shami neurocomputing 415 2020 mathematical optimization optimization problem key process machine learning solve optimization problem build ml model weight parameter ized optimized optimization method objective function approach minimum value accuracy approach maximum value 20 similarly optimization method aim optimize architecture ml model ing optimal conﬁgurations section main concept mathematical optimization parameter optimization machine learning model discussed mathematical optimization mathematical optimization process ﬁnding best solution set available candidate maximize mize objective function 20 generally optimization problem classiﬁed constrained unconstrained optimization problem based whether constraint decision variable solution variable unconstrained optimization problem decision variable x take any value space real number unconstrained optimization problem denoted 21 min f x ð þ f x ð þ objective function hand optimization problem constrained optimization problem decision variable x constrained optimization problem subject certain constraint could mathematical equality inequality therefore constrained optimization problem general tion problem expressed 21 min x f x ð þ subject gi x ð þ 6 0 1 2 hj x ð þ 0 j 1 2 p x 2 x gi x ð þ 1 2 inequality constraint function hj x ð þ j 1 2 p equality constraint function x domain role constraint limit possible value mal solution certain area search space named ble region 21 thus feasible region x represented x 2 xjgi x ð þ 6 0 hj x ð þ 0 conclude optimization problem consists three major component set decision variable x objective function f x ð þ either minimized maximized set constraint allow variable take value certain range constrained optimization problem therefore goal mization task obtain set variable value minimize maximize objective function satisfying any applicable constraint regarding ml model many hpo problem certain straints like feasible domain number cluster mean well time space constraint therefore strained optimization technique hpo lem 3 optimization problem many case only local instead global optimum obtained example obtain minimum problem assuming feasible region sion variable x global minimum point 2 satisfying f ð þ 6 f x ð 2 local minimum point 2 neighborhood n satisfying f ð þ 6 f x ð 2 n 21 thus local optimum may only optimum small range instead optimal solution entire feasible region local optimum only guaranteed global optimum convex function 22 convex function function only one optimum therefore continuing search along direction objective function decrease detect global minimum value function f x ð þ convex function 2 x 2 0 1 f þ 1 ð ð þ 6 tf ð þ þ 1 ð þf ð þ x domain decision variable coefﬁcient range optimization problem convex optimization problem only objective function f x ð þ convex function sible region c convex set denoted 22 min x f x ð þ subject x 2 c hand nonconvex function multiple local optimum only one optimum global optimum ml hpo problem nonconvex optimization problem thus utilizing inappropriate optimization method may only result local instead global optimum many traditional method used solve optimization problem including gradient descent newton method conjugate gradient heuristic optimization method 20 gradient descent optimization method us negative gradient direction search direction move towards optimum however gradient descent not guarantee detect global optimum unless objective tion convex function newton method us inverse matrix hessian matrix obtain optimum newton method ha faster convergence speed gradient descent often requires time larger space gradient descent store calculate hessian matrix conjugate gradient search along conjugated direction constructed gradient known data point detect optimum conjugate gradient ha faster convergence speed gradient descent tion conjugate gradient complex unlike tional method heuristic method use empirical rule solve optimization problem instead following systematical step obtain solution heuristic method often detect approximate global optimum within iteration not guarantee detect global optimum 20 optimization problem statement design process ml model effectively searching space using optimization technique identify optimal model optimization process consists four main ponents estimator regressor classiﬁer objective function search space conﬁguration space search mization method used ﬁnd combination evaluation function compare performance ent conﬁgurations domain continuous ing rate discrete number cluster binary whether use early stopping not categorical type optimizer yang shami neurocomputing 415 2020 297 therefore classiﬁed continuous discrete categorical continuous discrete domain usually bounded practical application 12 23 hand conﬁguration space sometimes contains conditionality parameter may need used tuned depending value another called conditional parameter 10 instance svm degree polynomial kernel function only need tuned kernel type chosen polynomial simple case take unrestricted real value feasible set x valued vector space however case ml model often take value ent domain different constraint optimization problem often complex constrained optimization problem 24 instance number considered feature decision tree range 0 number feature number cluster not larger size data point additionally categorical feature often only take several certain value like limited choice activation function optimizer neural network therefore sible domain x often ha complex structure increase problem complexity 24 general optimization problem aim obtain 19 arg min f x ð þ f x ð þ objective function minimized error rate root mean squared error rmse parameter conﬁguration produce optimum value f x ð þ x take any value search space aim hpo achieve optimal model performance tuning within given budget 3 mathematical expression function f varies ing objective function chosen ml algorithm performance metric function model performance evaluated various metric like accuracy rmse false alarm rate hand practice time budget essential constraint optimizing hpo model must considered often requires massive amount time optimize objective function ml model reasonable number parameter conﬁgurations every time value tested entire ml model need retrained tion set need processed generate score reﬂects model performance main process hpo follows 10 select objective function performance metric select require tuning summarize type determine appropriate optimization technique train ml model using default ration common value baseline model start optimization process large search space feasible domain determined manual ing domain knowledge narrow search space based region tested value explore new search space necessary return conﬁguration ﬁnal solution however traditional optimization technique 25 unsuitable hpo since hpo problem different tional optimization problem following aspect 10 optimization target objective function ml model usually function fore many traditional optimization method designed solve convex differentiable optimization problem often unsuitable hpo problem since method may return local optimum instead global optimum additionally optimization target lacking smoothness make certain tional optimization model perform poorly hpo problem 26 ml model include continuous crete categorical conditional thus many traditional numerical optimization method 27 only aim tackle numerical continuous variable able hpo problem often computationally expensive train ml model dataset hpo technique sometimes use data pling obtain approximate value objective function thus effective optimization technique hpo problem able use approximate value however tion evaluation time often ignored many mization bbo model often require exact instead approximate objective function value consequently many bbo algorithm often unsuitable hpo problem limited time resource budget therefore appropriate optimization algorithm applied hpo problem identify optimal ﬁgurations ml model machine learning model boost ml model hpo ﬁrstly need ﬁnd key people need tune ﬁt ml model speciﬁc problem datasets general ml model classiﬁed supervised pervised learning algorithm based whether built model labeled unlabeled datasets 127 supervised learning algorithm set machine learning algorithm map input feature target training labeled data mainly include linear model neighbor knn support vector machine svm naíve bayes nb model deep learning dl algorithm 28 unsupervised learning algorithm used ﬁnd pattern unlabeled data divided clustering dimensionality reduction algorithm based aim clustering method mainly include based spatial clustering application noise dbscan archical clustering em two common dimensionality reduction algorithm principal component analysis pca linear discriminant analysis lda 29 moreover several ensemble learning method combine different singular model improve model formance like voting bagging adaboost paper important common ml model studied based name python library including sklearn 30 xgboost 31 kera 32 supervised learning algorithm supervised learning input x output available goal obtain optimal predictive model function f minimize cost function l f x ð þ ð þ model error estimated output label predictive model function f varies based model structure limited model architecture determined different parameter conﬁgurations domain ml model function f 298 yang shami neurocomputing 415 2020 restricted set function thus optimal predictive model f obtained 33 f arg min 1 n x n l f xi ð þ yi ð þ n number training data point xi feature tor instance yi corresponding actual output l cost function value sample many different loss function exist supervised learning rithms including square euclidean distance information gain etc 33 hand different ml rithms generate different predictive model architecture based different conﬁgurations cussed detail subsection linear model general supervised learning model classiﬁed regression classiﬁcation technique used predict tinuous discrete target variable respectively linear regression 34 typical regression model predicts target lowing equation w x ð þ þ þ þ wpxp target variable expected linear combination p input feature x xp predicted value weight vector w wp designated attribute deﬁned another attribute linear model sklearn usually no need tuned linear regression linear model performance mainly depends well problem data follows linear distribution improve original linear regression model ridge sion wa proposed 35 ridge regression imposes penalty coefﬁcients aim minimize objective function 36 2 þ x p yi wi xi ð coefﬁcient vector ularization strength larger value indicates larger amount shrinkage thus coefﬁcients also robust collinearity lasso regression 37 another linear model used estimate sparse coefﬁcients consisting linear model priori added regularization term aim minimize objective tion 36 þ x p yi wi xi ð regularization strength coefﬁcient vector therefore regularization strength crucial ridge lasso regression model logistic regression lr 38 linear model used tion problem lr cost function may different depending regularization method chosen penalization three main type regularization method lr regularization 39 therefore ﬁrst need tuned lr regularization method used penalization elasticnet none called penalty sklearn coefﬁcient c another essential ne regularization strength model addition ver type representing optimization algorithm type set lbfgs liblinear sag saga lr ver type ha correlation penalty c tional knn neighbor knn simple ml algorithm used classify data point calculating distance ent data point knn predicted class test sample set class neighbor training set belong assuming training set ð þ ð þ xn yn ð þ f g xi feature vector instance yi 2 cm f g class instance 1 2 n ð þ test instance x class denoted 40 arg max cj x x ð þ yi cj 1 2 n j 1 2 x ð þ indicator function 1 yi cj otherwise 0 nk x ð þ ﬁeld involving neighbor knn number considered nearest neighbor k crucial 41 k small model k large model require high computational time addition weighted tion used prediction also chosen uniform point weighted equally distance point weighted inverse distance depending speciﬁc problem distance metric power parameter minkowski metric also tuned result minor improvement lastly algorithm used compute nearest neighbor also chosen ball tree kd tree brute force search typically model determine appropriate algorithm setting algorithm auto sklearn 30 svm support vector machine svm 42 supervised learning algorithm used classiﬁcation regression problem svm algorithm based concept mapping data point space make linearly separable hyperplane generated classiﬁcation boundary partition data point 43 assuming n data point objective function svm 44 128 arg min w 1 n x n max 0 1 yif xi ð þ f g þ cwtw w normalization vector c penalty parameter error term important svm model kernel function f x ð þ used measure similarity two data point xi xj chosen multiple type kernel svm model therefore kernel type would vital tuned common kernel type svm include linear kernel radial basis function rbf polynomial kernel sigmoid kernel different kernel function denoted follows 45 linear kernel f x ð þ xt xj polynomial kernel f x ð þ cxt xj þ r rbf kernel f x ð þ exp x k k 2 sigmoid kernel f x ð þ tanh cxt xj þ r yang shami neurocomputing 415 2020 299 shown kernel function equation different need tuned kernel type chosen coefﬁcient c denoted gamma sklearn conditional kernel type set polynomial rbf sigmoid r speciﬁed sklearn conditional polynomial sigmoid nels moreover polynomial kernel ha additional tional representing degree polynomial kernel function support vector regression svr model another epsilon indicating distance error loss function 30 naíve bayes naíve bayes nb 46 algorithm supervised learning rithms based bayes theorem assuming n dependent feature xn target variable objective function naíve bayes denoted arg max p ð þ n p xijy ð þ p ð þ probability value p xijy ð þ posterior probability xi given value regarding different assumption distribution p xijy ð þ different type naíve bayes classiﬁers four main type nb model bernoulli nb gaussian nb multinomial nb complement nb 47 gaussian nb 48 likelihood feature assumed follow gaussian distribution p xijy ð þ 1 ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ q exp xi ly 0 b 1 c maximum likelihood method used calculate mean value ly variance normally not any parameter need tuned gaussian nb mance gaussian nb model mainly depends well dataset follows gaussian distribution multinomial nb 49 designed data based naíve bayes algorithm assuming n tures hyi distribution value target variable equal conditional probability p xijy ð þ feature value involved data point belonging class based concept relative frequency counting hy estimated smoothed version hyi 30 hyi nyi þ ny þ nyi number time feature data point belonging class ny sum nyi 0 1 2 n smoothing prior p 0 used feature not learning sample 1 called laplace smoothing 1 called lidstone smoothing complement nb 50 improved version standard multinomial nb algorithm suitable processing anced data bernoulli nb 51 requires sample feature vector data follow ate bernoulli distribution additive lidstone smoothing parameter main need tuning conclude naíve bayes algorithm opers often not need tune only need tune smoothing parameter continuous parameter model decision tree dt 52 common classiﬁcation method us model decision possible quences summarizing set classiﬁcation rule data dt ha three main component root node representing entire data multiple decision node indicating decision test split feature several leaf node senting result class 53 dt algorithm recursively split training set better feature value achieve good decision subset pruning mean removing some decision node used dt avoid since deeper tree ha make accurate decision maximum tree depth essential control complexity dt algorithm 54 many important hp tuned build tive dt model 55 firstly quality split measured setting measuring function denoted criterion sklearn gini impurity information gain two main type suring function split selection method splitter also set best choose best split random select random split number considered feature generate best split also tuned feature selection process moreover several discrete related splitting process minimum number data point split decision node obtain leaf node denoted split respectively indicating maximum number leaf node mean minimum weighted fraction total weight also tuned improve model performance 30 55 based concept dt model many ensemble algorithm proposed improve model formance combining multiple decision tree including random forest rf extra tree et extreme gradient boosting xgboost model rf 56 ensemble learning method us bagging method combine multiple decision tree rf basic dts built many subset class majority voting selected ﬁnal classiﬁcation result 129 et 57 another ensemble learning method similar rf us sample build dts randomly selects feature set addition rf optimizes split dts et randomly make split xgboost 31 popular ensemble model designed speed performance improvement us boosting gradient descent method combine basic dts xgboost next input sample new dt related result previous dts xgboost aim minimize following objective function 54 obj 1 2 x j hj þ k þ ct number leaf decision tree g h sum ﬁrst second order gradient statistic cost function c k penalty coefﬁcients since ensemble model built decision tree base learner dt el described subsection apart parameter rf et xgboost another crucial parameter tuned number decision tree combined denoted sklearn xgboost ha several additional including 58 weight mean minimum sum weight child node subsample used control sampling ratio instance feature respectively four 300 yang shami neurocomputing 415 2020 continuous gamma alpha lambda indicating minimum loss reduction split regularization term weight learning rate respectively ensemble learning algorithm apart ensemble model several generic ensemble learning method combine multiple singular ml model achieve better model performance any singular algorithm alone three common ensemble learning model ing bagging adaboost introduced subsection 59 voting 59 basic ensemble learning algorithm us majority voting rule combine singular estimator generate comprehensive estimator improved accuracy sklearn voting method set hard soft indicating whether use majority voting averaged predicted probability mine classiﬁcation result list selected single ml tor weight also tuned certain case instance higher weight assigned singular ml model voting model bootstrap aggregating 59 also named bagging train multiple base estimator different subset struct ﬁnal predictor 130 using bagging method ﬁrst consideration type number base tor ensemble denoted tor respectively indicating sample size feature size generate different subset also tuned adaboost 59 short adaptive boosting ensemble learning method train multiple base learner consecutively weak learner later learner emphasize sample previous learner ultimately ﬁnal strong learner obtained process instance retrained new instance weight adjusted subsequent classiﬁers focus difﬁcult case thereby gradually building stronger classiﬁer adaboost type base estimator set decision tree method addition maximum number mators boosting terminated learning rate shrink contribution classiﬁer also tuned achieve two parameter deep learning model deep learning dl algorithm widely applied various area like computer vision natural language processing machine translation since great success solving many type problem dl model based theory ﬁcial neural network anns common type dl architecture include deep neural network dnns feedforward neural work ffnns deep belief network dbns convolutional neural network cnns recurrent neural network rnns many 60 dl model similar since similar underlying neural network architecture pared ml model dl model beneﬁt hpo since often many require tuning ﬁrst set related construction dl model hence named model design since neural network model input layer output layer complexity deep learning model mainly depends number hidden layer number neuron layer two main build dl model 61 two set tuned according complexity datasets problem dl model need enough capacity model objective function prediction task avoiding next stage certain function type need set tuned ﬁrst function type conﬁgure loss function type chosen mainly based problem type binary binary classiﬁcation problem classiﬁcation problem rmse regression problem another important activation function type used model function set softmax tiﬁed linear unit relu sigmoid tanh softsign lastly optimizer type set stochastic gradient descent sgd adaptive moment estimation adam root mean square tion rmsprop etc 62 hand some related optimization training process dl model hence rized optimizer learning rate one important dl model 63 mine step size iteration enables objective function converge large learning rate speed learning process gradient may oscillate around local minimum value even not converge hand small learning rate converges smoothly largely increase model training time requiring training epoch appropriate learning rate enable objective function converge global minimum reasonable amount time another common rate standard larization method dl model proposed reduce proportion neuron randomly removed percentage neuron removed tuned size number epoch two dl represent number processed sample updating model number complete pass entire training set respectively 64 size affected resource requirement training process number iteration number epoch depends size training set tuned slowly ing value validation accuracy start decrease indicates hand dl model often converge within epoch following epoch may lead essary additional execution time avoided early stopping method early stopping form regularization whereby model training stop advance validation accuracy doe not increase certain number consecutive epoch number waiting epoch called early stop patience also tuned reduce model training time apart traditional dl model transfer learning tl technology obtains model data related domain transfer target task 65 fer dl model one problem another problem certain number top layer frozen only remaining layer retrained ﬁt new problem therefore number zen layer vital tune tl used unsupervised learning algorithm unsupervised learning algorithm set ml algorithm used identify unknown pattern unlabeled datasets ing algorithm two main type unsupervised learning method clustering method include dbscan em hierarchical clustering etc pca lda two dimensionality reduction algorithm 29 clustering algorithm clustering algorithm including em hierarchical clustering number cluster tant tune 66 yang shami neurocomputing 415 2020 301 algorithm 67 us k prototype indicating centroid cluster cluster data algorithm number cluster must speciﬁed mined minimizing sum squared error 68 x nk min xi uj xn ð þ data matrix uj also called centroid cluster ck mean sample cluster nk number sample point cluster ck tune crucial parameter besides method centroid initialization init could set random array slightly affect model performance addition denoting number time algorithm executed different centroid seed maximum number iteration single execution mean also slight impact model performance 30 em algorithm 69 tive algorithm used detect maximum likelihood estimation parameter gaussian mixture model clustering method us mixture gaussian distribution model data menting em method similar major parameter tuned indicating number cluster gaussian distribution additionally different od chosen constrain covariance estimated class gaussian mixture model including full covariance tied diagonal spherical 70 could also tuned including tol representing ber em iteration perform convergence threshold respectively 30 hierarchical clustering 71 method build cluster ously merging splitting cluster hierarchy cluster represented root indicates unique cluster gathering sample leaf represent cluster only one sample 71 sklearn function agglomerativeclustering common type hierarchical tering agglomerative clustering linkage criterion linkage determines distance set observation set ward complete average single indicating whether minimize variance cluster use maximum average minimum distance every two cluster tively like clustering method main number cluster however not set choose set linkage tance threshold merging cluster since determined automatically dbscan 72 clustering method mine cluster dividing data cluster sufﬁciently high density unlike clustering model number ters doe not need conﬁgured training instead dbscan ha two signiﬁcant conditional scan radius represented eps minimum number sidered neighbor point represented deﬁne cluster density together 73 dbscan work starting unvisited point detecting neighbor point within distance eps number neighbor point reach value unvisited point neighbor deﬁned cluster procedure executed recursively data point visited higher lower eps indicates higher density form cluster dimensionality reduction algorithm increasing amount collected data provides ample mation also increase problem complexity application many feature irrelevant redundant predict target variable dimensionality reduction algorithm often serve feature engineering method extract important feature eliminate insigniﬁcant redundant feature two common algorithm principal component analysis pca linear discriminant analysis lda pca lda number feature extracted represented component sklearn main tuned principal component analysis pca 74 widely used linear dimensionality reduction method pca based concept mapping original feature tures new orthogonal feature also called principal ponents pca work calculating covariance matrix data matrix obtain eigenvectors covariance matrix matrix comprises eigenvectors k feature gest eigenvalue largest variance consequently data matrix transformed new space reduced sionality singular value decomposition svd 75 popular method used obtain eigenvalue eigenvectors covariance matrix pca therefore addition svd solver type another pca tuned assigned auto full arpack randomized 30 linear discriminant analysis lda 76 another common dimensionality reduction method project feature onto discriminative direction unlike pca obtains direction largest variance principal component lda optimizes feature subspace classiﬁcation objective lda minimize variance inside class maximize variance different class projection thus projection point class close possible distance center point different class large possible similar pca number feature extracted tuned lda model additionally solver type lda also set svd svd lsqr solution eigen eigenvalue decomposition 77 lda also ha conditional shrinkage parameter shrinkage set ﬂoat value along lsqr eigen solver optimization technique algorithm babysitting babysitting also called trial error grad student descent gsd basic tuning method 8 method implemented 100 manual tuning widely used dent researcher workﬂow simple building ml model student test many possible value based experience guessing analysis evaluated result process repeated student run time often reaching deadline satisﬁed result approach requires sufﬁcient amount prior knowledge experience identify optimal value limited time manual tuning infeasible many problem due several factor like large number complex model model evaluation parameter interaction 9 factor inspired increased research technique automatic optimization 78 grid search grid search g one method explore conﬁguration space 120 g 302 yang shami neurocomputing 415 2020 sidered exhaustive search method ate combination given grid ﬁgurations 131 g work evaluating cartesian product ﬁnite set value 6 g not exploit region therefore identify global optimum following dure need performed manually 2 start large search space step size narrow search space step size based previous result conﬁgurations repeat step 2 multiple time optimum reached g easily implemented parallelized however main drawback g inefﬁciency conﬁguration space since number tions increase exponentially number grows exponential growth referred curse dimensionality 79 g assuming k parameter ha n distinct value computational ity increase exponentially rate nk 19 thus only conﬁguration space small g effective hpo method random search overcome certain limitation g random search r wa proposed 13 r similar g instead testing value search space r randomly selects number sample upper lower bound candidate value train candidate deﬁned budget exhausted theoretical basis r conﬁguration space large enough global mum least approximation detected ited budget r able explore larger search space g 13 main advantage r easily parallelized since evaluation independent unlike g r sample ﬁxed number parameter combination speciﬁed distribution improves system efﬁciency reducing probability wasting much time small performing region since number total evaluation r set ﬁxed value n optimization process start computational complexity r n ð þ 80 addition r detect global optimum optimum given enough budget 6 although r efﬁcient g large search space still large number unnecessary function evaluation since doe not exploit previously region 2 conclude main limitation r g every evaluation iteration independent previous tions thus waste massive time evaluating performing area search space issue solved optimization method like bayesian optimization us previous evaluation record determine next evaluation 14 optimization gradient descent 81 traditional optimization technique calculates gradient variable identify promising direction move towards optimum randomly ing data point technique move towards opposite tion largest gradient locate next data point therefore local optimum reached convergence local mum also global optimum convex function based algorithm time complexity nk optimizing k 82 speciﬁc machine learning algorithm gradient certain calculated gradient descent used optimize although algorithm faster convergence speed reach local optimum method section several limitation firstly only used optimize continuous type like categorical not gradient direction secondly only efﬁcient convex function local instead global optimum may reached function 2 therefore algorithm only used some case possible obtain gradient mizing learning rate neural network nn 11 still not guaranteed ml algorithm identify global optimum using optimization technique bayesian optimization bayesian optimization bo 83 iterative algorithm popularly used hpo problem unlike g r bo determines future evaluation point based result determine next conﬁguration bo us two key component surrogate model acquisition function 56 surrogate model aim ﬁt observed point objective function obtaining predictive distribution probabilistic surrogate model acquisition function determines usage different point balancing exploration exploitation exploration sample instance area not sampled exploitation sample currently promising region global optimum likely occur based posterior distribution bo model balance exploration exploitation process detect current likely optimal region avoid missing better tions unexplored area 84 basic procedure bo follows 83 build probabilistic surrogate model objective function detect optimal value surrogate model apply value real objective tion evaluate update surrogate model new result repeat step maximum number iteration reached thus bo work updating surrogate model evaluation objective function bo efﬁcient g r since detect optimal tions analyzing value running rogate model often much cheaper running entire objective function however since bayesian optimization model executed based value belong sequential method difﬁcult parallelize usually detect combination within iteration 7 common surrogate model bo include gaussian process gp 85 random forest rf 86 tree parzen estimator tpe 12 therefore three main type bo algorithm based surrogate model yang shami neurocomputing 415 2020 303 native name sequential algorithm uration smac 86 gaussian process gp standard surrogate model tive function modeling bo 83 assuming function f mean l covariance realization gp diction follow normal distribution 87 p yjx ð þ n l conﬁguration space f x ð þ evaluation result value obtaining set prediction point evaluated next selected conﬁdence interval generated model data point added sample record model new information procedure repeated termination applying size n dataset ha time complexity space complexity 88 one main limitation cubic plexity number instance limit capacity lelization 3 additionally mainly used optimize continuous variable smac random forest rf another popular surrogate function bo model objective function using ensemble regression tree bo using rf surrogate model also called smac 86 assuming gaussian model n l l mean variance regression function r x ð þ respectively 86 l 1 jbj x r x ð þ 1 jbj 1 x r x ð þ l ð þ 2 b set regression tree forest major dures smac follows 3 rf start building b regression tree constructed sampling n instance training set replacement split node selected tree maintain low computational cost minimum ber instance considered split number tree grow set certain value finally mean variance new conﬁguration estimated rf compared main advantage smac port type variable including continuous discrete gorical conditional 87 time complexity using smac ﬁt predict variance nlogn ð þ logn ð þ respectively much lower complexity 3 parzen estimator tpe 12 another mon surrogate model bo instead deﬁning predictive tribution used creates two density function l x ð þ g x ð þ act generative model domain ables 3 apply tpe observation result divided good result poor result percentile two set result modeled simple parzen dows 12 p xjy ð þ l x ð þ g x ð þ expected improvement acquisition tion reﬂected ratio two density function used determine new conﬁgurations evaluation parzen estimator organized tree structure speciﬁed conditional dependency retained therefore tpe naturally support speciﬁed conditional 87 time complexity nlogn ð þ lower complexity 3 bo method effective many hpo problem even objective function f stochastic however main drawback bo model fail achieve balance exploration exploitation might only reach local instead global optimum r doe not limitation since doe not focus any speciﬁc area additionally difﬁcult parallelize bo model since intermediate result dependent 7 optimization algorithm one major issue hpo long execution time increase larger conﬁguration space larger datasets execution time take several hour several day even 89 optimization technique common approach solve constraint limited time resource save time people use subset original dataset subset feature 90 involves evaluation combine practical application 91 evaluation relatively small subset evaluated low cost poor generalization performance evaluation relatively large subset evaluated better generalization performance higher cost evaluation optimization algorithm conﬁgurations discarded round evaluation generated subset only conﬁgurations evaluated entire training set algorithm categorized tion algorithm shown success dealing deep learning optimization problem 3 two common technique successive halving 92 hyperband 16 successive halving theoretically speaking exhaustive method able identify optimal combination evaluating given combination however many factor including limited time resource considered practical application factor called budget b overcome limitation g r improve efﬁciency successive halving algorithm proposed 92 main process using successive halving algorithm hpo follows firstly presumed n set combination evaluated budget b according uation result iteration half conﬁgurations eliminated performing half passed next iteration double budget 2 bi process repeated ﬁnal mal combination detected successive halving efﬁcient r affected number tions budget allocated conﬁguration 6 thus main concern successive halving allocate get determine whether test fewer conﬁgurations 304 yang shami neurocomputing 415 2020 higher budget test conﬁgurations lower budget 2 hyperband hyperband 16 proposed solve dilemma cessive halving algorithm dynamically choosing reasonable number conﬁgurations aim achieve number conﬁgurations n cated budget dividing total budget b n piece allocating piece conﬁguration b successive halving serf subroutine set random tions eliminate urations improve efﬁciency main step hyperband algorithm shown algorithm 1 2 algorithm 1 hyperband input bmax bmin 1 smax log bmax bmin 2 2 bmax bmin 1 0 f g 3 n determinebudget ð þ 4 c sampleconfigurations n ð þ 5 successivehalving c ð þ 6 end 7 return best conﬁguration far firstly budget constraint bmin bmax determined total number data point minimum number instance required train sensible model available budget number conﬁgurations n budget size allocated conﬁguration calculated based bmin bmax step algorithm conﬁgurations sampled based n b passed successive halving model demonstrated step successive halving algorithm discard ﬁed conﬁgurations pass performing conﬁgurations next iteration process repeated ﬁnal optimal conﬁguration identiﬁed involving successive halving searching method hyperband ha computational complexity nlogn ð þ 16 bohb bayesian optimization hyperband bohb 93 hpo technique combine bayesian optimization hyperband incorporate advantage avoiding drawback original hyperband us random search search conﬁguration space ha low efﬁciency bohb replaces r method bo achieve high performance well low execution time effectively using parallel resource optimize type parameter bohb tpe standard surrogate model bo us multidimensional kernel density estimator therefore complexity bohb also nlogn ð þ 93 ha shown bohb outperforms many mization technique tuning svm dl model 93 only limitation bohb requires evaluation set small budget representative evaluation entire training set otherwise bohb may slower gence speed standard bo model metaheuristic algorithm metaheuristic algorithm 94 set algorithm mainly inspired biological theory widely used optimization problem unlike many traditional optimization method metaheuristics capacity solve continuous optimization problem optimization algorithm poa major type metaheuristic algorithm including genetic algorithm gas evolutionary algorithm evolutionary strategy cle swarm optimization pso poa start creating updating population generation individual every tion evaluated global optimum identiﬁed 14 main difference different poa method used generate select population 17 poa easily lelized since population n individual evaluated n thread machine parallel 6 genetic algorithm particle swarm optimization two main poa hpo problem genetic algorithm genetic algorithm ga 18 one common tic algorithm based evolutionary theory individual best survival capability adaptability ment likely survive pas capability future generation next generation also inherit ents characteristic may involve better worse individual better individual likely survive capable offspring worse individual gradually pear several generation individual best ability identiﬁed global optimum 95 apply ga hpo problem chromosome individual represents decimal value actual input value evaluation every mosome ha several gene binary digit mutation operation performed gene chromosome population involves possible value within initialized range ﬁtness function characterizes evaluation metric parameter 95 since parameter value often not include optimal parameter value several operation including selection crossover mutation operation must performed chromosome identify optimum 18 chromosome selection implemented selecting chromosome good ﬁtness function value keep lation size unchanged chromosome good ﬁtness function value passed next generation higher probability generate new chromosome parent best characteristic chromosome selection ensures good teristics generation passed later generation crossover used generate new chromosome exchanging proportion gene different chromosome mutation operation also used generate new chromosome randomly altering one gene chromosome crossover mutation ations enable later generation different characteristic reduce chance missing good characteristic 3 main procedure ga follows 94 randomly initialize population chromosome gene represent entire search space value respectively evaluate performance individual current eration calculating ﬁtness function indicates objective function ml model perform selection crossover mutation operation chromosome produce new generation involving next conﬁgurations evaluated repeat step 2 3 termination condition met terminate output optimal conﬁguration yang shami neurocomputing 415 2020 305 among step population initialization step important step ga pso since provides initial guess optimal value although initialized value tively improved optimization process suitable population initialization method signiﬁcantly improve convergence speed performance poa good initial population involve individual close global optimum covering promising region not localized unpromising region search space 96 generate conﬁguration candidate initial population random initialization simply creates tial population random value given search space often used ga 97 thus ga easily implemented doe not necessitate good initialization selection mutation operation lower possibility missing global optimum hence useful data analyst doe not much experience determining potential appropriate initial search space main limitation ga algorithm introduces additional ﬁgured including ﬁtness function type population size rate mutation rate moreover ga sequential execution algorithm making difﬁcult parallelize time complexity ga 98 result sometimes ga may inefﬁcient due low convergence speed particle swarm optimization particle swarm optimization pso 99 another set tionary algorithm commonly used optimization lem pso algorithm inspired biological population exhibit individual social behavior 17 pso work enabling group particle swarm traverse search space manner 9 pso algorithm identify optimal solution cooperation information sharing among vidual particle group pso group n particle swarm 2 sn ð þ particle si represented vector si xi vi pi xi current position vi current velocity pi known best position swarm far initializing position velocity particle evaluates current position record position performance score next iteration velocity vi particle changed based previous position pi rent global optimal position p vi vi þ u 0 ð þ pi xi þ u 0 ð þ p xi u 0 u ð þ continuous uniform distribution based acceleration constant particle move based new velocity vector xi xi þ vi procedure repeated convergence nation constraint reached compared ga easier implement pso since pso doe not certain additional operation like crossover mutation ga chromosome share information entire population move uniformly toward mal region pso only information individual best particle global best particle transmitted others ﬂow information sharing entire search process follows direction current optimal solution 2 computational complexity pso algorithm nlogn ð þ 100 case convergence speed pso faster addition particle pso operate independently only need share information iteration process easily parallelized improve model efﬁciency 9 main limitation pso requires proper population initialization otherwise might only reach local instead global optimum especially discrete 101 proper population initialization requires developer prior ence using population initialization technique many tion initialization technique proposed improve performance evolutionary algorithm like based optimization algorithm 97 space transformation search method 102 involving additional population initialization technique require execution time resource applying optimization technique machine learning algorithm optimization technique analysis grid search g simple method major limitation impacted curse ality 79 thus unsuitable large number parameter moreover g often not able detect global optimum continuous parameter since requires deﬁned ﬁnite set value also unrealistic g used identify integer continuous parameter optimum limited time resource therefore compared technique g only efﬁcient small number categorical random search efﬁcient g support type practical application using r evaluate value help analyst explore large search space however since r doe not consider result may involve many unnecessary ations decrease efﬁciency 13 hyperband considered improved version r support parallel execution hyperband balance model performance resource usage efﬁcient r especially limited time resource 15 however g r hyperband major constraint treat independently not consider parameter correlation 103 thus inefﬁcient ml algorithm conditional like svm dbscan logistic regression algorithm not prevalent choice parameter optimization since only support continuous only detect local instead global optimum hpo problem 2 therefore based algorithm only used optimize certain parameter like learning rate dl model bayesian optimization model divided three different smac surrogate model bo algorithm determine next value based result reduce unnecessary evaluation improve efﬁciency mainly support uous discrete rounding doe not support conditional 14 smac able handle categorical discrete continuous conditional smac performs better many categorical conditional parameter used performs better only 306 yang shami neurocomputing 415 2020 continuous parameter 15 preserve speciﬁed conditional relationship one advantage gp innate support speciﬁed conditional parameter 14 metaheuristic algorithm including ga pso plicated many hpo algorithm often perform well complex optimization problem support type particularly efﬁcient large tion space since obtain solution even within iteration however ga pso advantage disadvantage practical use pso able port parallelization particularly suitable tinuous conditional hpo problem 19 hand ga executed sequentially making difﬁcult parallelized therefore pso often executes faster ga especially large conﬁguration space large datasets however appropriate population initialization crucial pso otherwise may verge slowly only identify local instead global optimum yet impact proper population initialization not icant ga pso 104 another limitation ga introduces additional like crossover mutation rate 18 strength limitation tion algorithm involved paper summarized table 1 apply hpo algorithm ml model since many different hpo method different use case crucial select appropriate optimization technique different ml model firstly access multiple ﬁdelities mean able deﬁne meaningful budget performance ings conﬁgurations evaluated small budget similar conﬁguration ranking full budget original dataset bohb would best choice since ha advantage bo hyperband 6 93 hand multiple ﬁdelities not applicable mean using subset original dataset subset original feature misleading noisy reﬂect performance entire dataset bohb may perform poorly higher time complexity standard bo model ing hpo algorithm would efﬁcient 93 ml algorithm classiﬁed characteristic conﬁgurations appropriate optimization rithms chosen optimize based characteristic one discrete commonly some ml algorithm like certain based clustering dimensionality reduction algorithm only one discrete need tuned knn major k number considered neighbor essential hierarchical tering em number cluster similarly ality reduction algorithm including pca lda basic number feature extracted situation bayesian optimization best choice three surrogate could tested ﬁnd best one band another good choice may fast execution speed due capacity parallelization some case people may want ml model considering le important like distance metric knn svd solver type pca ga pso could chosen situation one continuous some linear model including ridge lasso algorithm some naíve bayes algorithm involving multinomial nb bernoulli nb complement nb generally only one vital continuous tuned ridge lasso algorithm continuous alpha regularization strength three nb algorithm mentioned critical parameter also named alpha represents additive smoothing parameter term ml algorithm best choice since good optimizing small number continuous algorithm also used might only detect local optimum le effective conditional noticeable many ml algorithm conditional like svm lr dbscan lr ha three lated penalty c solver type larly dbscan ha eps must tuned conjunction svm complex since setting different kernel type separate set conditional need tuned next described section hence some hpo method not effectively optimize conditional including g r hyperband not suitable ml model conditional ml method best choice relationship among table 1 comparison common hpo algorithm n number value k number hpo method strength limitation time complexity g simple only efﬁcient categorical hp nk r efﬁcient g enable parallelization not consider previous result not efﬁcient conditional hp n ð þ based model fast convergence speed continuous hp only support continuous hp may only detect local optimum nk fast convergence speed continuous hp poor capacity parallelization not efﬁcient conditional hp smac efﬁcient type hp poor capacity parallelization nlogn ð þ efﬁcient type hp keep conditional dependency poor capacity parallelization nlogn ð þ hyperband enable parallelization not efﬁcient conditional hp require subset small budget representative nlogn ð þ bohb efﬁcient type hp enable parallelization require subset small budget representative nlogn ð þ ga efﬁcient type hp not require good initialization poor capacity parallelization pso efﬁcient type hp enable parallelization require proper initialization nlogn ð þ yang shami neurocomputing 415 2020 307 smac also good choice since also form well tuning conditional ga pso used well large conﬁguration space multiple type algorithm including dt rf et xgboost well dl algorithm like dnn cnn rnn complex type ml algorithm bed tuned since many parameter various different type ml model pso best choice since enables parallel execution improve efﬁciency particularly dl model often require massive training time some technique like ga smac also used may cost time pso since difﬁcult parallelize technique categorical category mainly ensemble learning algorithm since major gorical bagging adaboost categorical set singular ml model voting estimator indicating list ml gular model combined voting method ha another egorical voting used choose whether use hard soft voting method only consider categorical g would sufﬁcient detect suitable base machine learner hand many case need considered like bagging well adaboost quently bo algorithm would better choice optimize continuous discrete conclusion tuning ml model achieve high model performance low computational cost suitable hpo algorithm selected based property existing hpo framework tackle hpo problem many library exist apply theory practice lower threshold ml er section provide brief introduction some popular hpo library framework mainly python gramming principle behind involved optimization rithms provided section 4 sklearn sklearn 30 gridsearchcv implemented detect optimal using g algorithm value conﬁguration space evaluated program performance evaluated using instance conﬁguration space evaluated optimal combination deﬁned search space performance score returned randomizedsearchcv also provided sklearn implement r method evaluates number value parallel validation conducted effectively evaluate performance conﬁguration spearmint spearmint 83 library using bayesian optimization gaussian process surrogate model spearmint primary deﬁciency not efﬁcient categorical tional bayesopt bayesian optimization bayesopt 105 python library employed solve hpo problem using bo bayesopt us sian process surrogate model calculate objective tion based past evaluation utilizes acquisition function determine next value hyperopt hyperopt 106 hpo framework involves r optimization algorithm unlike some library only support single model hyperopt able use multiple model model hierarchical addition hyperopt parallelizable since us mongodb central database store combination 107 hyperas 108 two library apply hyperopt kera library smac smac 109 another library us bo random forest surrogate model support categorical continuous discrete variable bohb bohb framework 93 combination bayesian tion hyperband 15 overcomes one limitation band randomly generates test conﬁgurations replacing procedure bo tpe used surrogate model store model function evaluation using bohb evaluate instance achieve model performance current budget optunity optunity 79 popular hpo framework provides several optimization technique including g r pso optunity categorical converted discrete indexing discrete processed continuous rounding support type skopt skopt 110 hpo library built top 30 library implement several sequential optimization model including r method exhibit good performance small search space proper initialization gpflowopt gpflowopt 111 python library bo using gp rogate model support running gpu using ﬂow library therefore gpflowopt good choice bo used deep learning model gpu resource available talos talos 112 python package designed optimization kera model talos fully deployed 308 yang shami neurocomputing 415 2020 any kera model implemented easily without learning any new syntax several optimization technique including g r probabilistic reduction implemented using talos sherpa sherpa 113 python package used hpo problem used ml library including sklearn 30 tensorﬂow 114 kera 32 support parallel computation ha several optimization method including g r via gpyopt hyperband training pbt osprey osprey 115 python library designed optimize parameter several hpo strategy available osprey ing g r via hyperopt via gpyopt 116 optimization package employ algorithm tensorflow tains optimizers like reverse forward method library designed build access optimizers tensorflow allowing deep learning model training parameter optimization gpu ing environment hyperband hyperband 16 python package tuning parameter hyperband approach similar gridsearchcv randomizedsearchcv class named hyperbandsearchcv hyperband combined sklearn used hpo problem hyperbandsearchcv method used evaluation deap deap 117 novel evolutionary computation package python contains several evolutionary algorithm like ga pso integrates parallelization mechanism like cessing machine learning package like sklearn tpot tpot 118 python tool us genetic gramming optimize ml pipeline tpot built top sklearn easy implement tpot ml model tpotclassiﬁer principal function several additional ga must set ﬁt speciﬁc problem nevergrad nevergrad 119 python library includes wide range optimizers like pso ml nevergrad used tune type including crete continuous categorical choosing different optimizers experiment summarize content section comprehensive overview applying optimization technique ml model shown table provides summary mon ml algorithm suitable optimization method available python library thus data analyst researcher look table select suitable optimization algorithm well library practical use put theory practice several experiment ducted based table section provides experiment applying eight different hpo technique three common resentative ml algorithm two benchmark datasets ﬁrst part section experimental setup main process hpo discussed second part result utilizing ent hpo method compared analyzed sample code experiment ha published 132 illustrate ce applying optimization ml model experimental setup based step optimize discussed section several step completed actual mization experiment start firstly two standard benchmarking datasets provided sklearn library 30 namely modiﬁed national institute standard technology dataset mnist boston housing dataset selected benchmark datasets hpo method evaluation data analytics problem mnist digit recognition dataset used problem boston housing dataset contains information price house various place city boston used regression dataset predict housing price next stage ml model objective function need conﬁgured section common ml model divided ﬁve category based type among ml category one discrete conditional large conﬁguration space multiple type three common case thus three ml algorithm knn svm rf selected target model optimized since type represent three mon hpo case knn ha one important number considered nearest neighbor sample svm ha conditional like kernel type penalty parameter c rf ha multiple ferent type discussed section moreover knn svm rf applied solve classiﬁcation regression problem next step performance metric evaluation od conﬁgured experiment selected two set cross validation implemented evaluate involved hpo method two mance metric used experiment classiﬁcation el accuracy used classiﬁer performance metric proportion correctly classiﬁed data regression model mean squared error mse used regressor formance metric measure average squared difference predicted value actual value additionally computational time ct total time needed complete hpo process also used model efﬁciency metric 54 experiment optimized ml model architecture ha highest accuracy lowest mse optimal conﬁguration returned yang shami neurocomputing 415 2020 309 fairly compare different optimization algorithm framework certain constraint satisﬁed firstly compare different hpo method using parameter conﬁguration space knn only parameter optimized set range 1 20 optimization method evaluation svm rf model classiﬁcation regression problem also set conﬁguration space type problem speciﬁcs conﬁguration space ml model shown table selected parameter search space determined based concept section 3 domain knowledge manual testing 120 type ml algorithm also summarized table hand fairly compare performance metric optimization technique maximum number iteration hpo method set 50 rf svm model optimization 10 knn model optimization based manual testing domain knowledge moreover avoid impact randomness experiment repeated ten time different random seed result averaged regression problem given majority vote classiﬁcation problem section 4 ten hpo method introduced experiment eight representative hpo approach selected performance comparison including g r hyperband bohb ga pso setting fair experimental environment hpo method hpo iments implemented based step discussed section experiment conducted using python machine 6 core processor 16 gigabyte gb memory involved ml hpo algorithm evaluated using multiple table 3 conﬁguration space tested ml model ml model type search space rf classiﬁer discrete discrete discrete discrete criterion categorical gini entropy discrete svm classiﬁer c continuous kernel categorical linear poly rbf sigmoid knn classiﬁer discrete rf regressor discrete discrete discrete discrete criterion categorical mse mae discrete svm regressor c continuous kernel categorical linear poly rbf sigmoid epsilon continuous knn regressor discrete table 2 comprehensive overview common ml model suitable optimization technique available python library ml algorithm main hp optional hp hpo method library linear regression ridge lasso alpha skpot logistic regression penalty c solver smac hyperopt smac knn weight p algorithm bos hyperband skpot hyperopt smac hyperband svm c kernel epsilon svr gamma degree smac bohb hyperopt smac bohb nb alpha skpot dt criterion splitter ga pso smac bohb tpot optunity smac bohb rf et criterion splitter ga pso smac bohb tpot optunity smac bohb xgboost subsample gamma alpha lambda ga pso smac bohb tpot optunity smac bohb voting estimator voting weight g sklearn bagging g bos sklearn skpot hyperopt smac adaboost smac hyperopt smac deep learning number hidden layer unit per layer loss optimizer activation dropout rate epoch early stop patience number frozen layer transfer learning used pso bohb optunity bohb init bos hyperband skpot hyperopt smac hyperband hierarchical clustering linkage bos hyperband skpot hyperopt smac hyperband dbscan eps smac bohb hyperopt smac bohb gaussian mixture tol skpot pca bos hyperband skpot hyperopt smac hyperband lda solver shrinkage bos hyperband skpot hyperopt smac hyperband 310 yang shami neurocomputing 415 2020 python library framework introduced tion 6 including sklearn 30 skopt 110 hyperopt 106 nity 79 hyperband 16 bohb 93 tpot 118 performance comparison experiment applying eight different hpo method ml model summarized table table provide formance optimization algorithm applied rf svm knn classiﬁers evaluated mnist dataset plete optimization process table demonstrate formance hpo method applied rf svm knn regressors evaluated dataset ﬁrst step ml model default tion trained evaluated baseline model hpo algorithm implemented ml model evaluate compare accuracy classiﬁcation problem ms regression problem well computational time ct table see using default hp ration not yield best model performance ments emphasizes importance utilizing hpo method g r seen baseline model hpo lem result table shown tional time g often much higher optimization method search space size r faster g not guarantee detect conﬁgurations ml model especially rf svm model larger search space knn performance bo model much better g r computation time often higher hpo method due cubic time complexity obtain better performance metric ml model size continuous space like knn conversely hyperband often not able obtain highest accuracy lowest mse among optimization method tional time low work subset performance bohb often better others since detect optimal ﬁgurations within short computational time metaheuristics method ga pso accuracy often higher hpo method classiﬁcation problem ms often lower optimization technique however computational time often higher model especially ga doe not port parallel execution summarize simple implement g r often not detect optimal conﬁgurations table 4 performance evaluation applying hpo method rf classiﬁer mnist dataset optimization algorithm accuracy ct default hp g r hyperband bohb ga pso table 5 performance evaluation applying hpo method svm classiﬁer mnist dataset optimization algorithm accuracy ct default hp g r hyperband bohb ga pso table 6 performance evaluation applying hpo method knn classiﬁer mnist dataset optimization algorithm accuracy ct default hp g r hyperband bohb ga pso table 7 performance evaluation applying hpo method rf regressor housing dataset optimization algorithm mse ct default hp g r hyperband bohb ga pso table 8 performance evaluation applying hpo method svm regressor dataset optimization algorithm mse ct default hp g r hyperband bohb ga pso table 9 performance evaluation applying hpo method knn regressor dataset optimization algorithm mse ct default hp g r hyperband bohb ga pso yang shami neurocomputing 415 2020 311 cost much computational time ga also cost computational time many hpo method work well small conﬁguration space ga effective large conﬁguration space hyperband computatinal time low not guarantee detect global optimum ml model large conﬁguration space bohb pso often work well open issue challenge future research direction although many existing hpo algorithm practical framework some issue still need addressed several aspect domain could improved section discus open challenge current research question potential research direction future classiﬁed model complexity challenge model performance lenges summarized table 10 model complexity costly objective function evaluation evaluate performance ml model different conﬁgurations objective function must minimized evaluation depending scale data model complexity available computational resource evaluation conﬁguration may take several minute hour day even 89 additionally value certain direct impact execution time like number considered neighbor knn number basic decision tree rf number hidden layer deep neural network 121 solve problem hpo algorithm bo model reduce total number evaluation spending time choosing next evaluating point instead simply evaluating possible parameter conﬁgurations however still require much tion time due poor capacity parallelization hand although optimization method like hyperband some success dealing hpo problem limited budget still some problem not effectively solved hpo due complexity model scale datasets 6 example imagenet 122 lenge popular problem image processing domain ha not any research work efﬁciently ing imagenet challenge yet due huge scale complexity cnn model used imagenet complex search space many problem ml algorithm applied only signiﬁcant effect model mance main require ing however certain unimportant may still affect performance slightly may considered mize ml model increase dimensionality search space number conﬁgurations increase exponentially increase dimensionality search space complexity lem total objective function evaluation time also increase exponentially 7 therefore necessary reduce inﬂuence large search space execution time ing existing hpo method model performance strong anytime performance ﬁnal performance hpo technique often expensive sometimes require extreme resource especially massive datasets complex ml model one example model deep learning model since view objective function evaluation function not consider complexity ever overall budget often limited practical uations sohpo algorithm able prioritize objective function evaluation strong anytime performance indicates capacity detect optimal conﬁgurations even limited budget 93 instance efﬁcient hpo method high convergence speed would not huge difference result model convergence avoid random result even time resource limited like r method not hand condition permit adequate budget given hpo approach able identify global mal conﬁguration named strong ﬁnal mance 93 comparability hpo method optimize ml model different mization algorithm applied ml framework ent optimization technique strength drawback different case currently no single mization approach outperforms approach processing different datasets various metric parameter type 3 paper analyzed strength weakness common optimization niques based principle performance practical application topic could extended comprehensively table 10 open challenge future direction hpo research category challenge future requirement brief description model complexity costly objective function evaluation hpo method reduce evaluation time large datasets complex search space hpo method reduce execution time high dimensionality large parameter search space model performance strong anytime performance hpo method able detect optimal hp even limited budget strong ﬁnal performance hpo method able detect global optimum given sufﬁcient budget comparability exist standard set benchmark fairly evaluate compare different optimization algorithm generalization optimal hp detected hpo method generalizability build efﬁcient model unseen data randomness hpo method reduce randomness obtained result scalability hpo method scalable multiple library platform distributed ml platform continuous updating capability hpo method consider capacity detect update optimal hp combination data 312 yang shami neurocomputing 415 2020 solve problem standard set benchmark could designed agreed community better comparison different hpo algorithm example platform called coco comparing continuous optimizers 123 provides benchmark analyzes common continuous optimizers ever date not any reliable platform provides benchmark analysis common mization approach would easier people choose hpo algorithm practical application platform like coco exists hpo problem addition uniﬁed metric also improve comparability different hpo algorithm since ferent metric currently used different practical problem 6 hand based comparison different hpo algorithm way improve hpo combine existing model propose new model contain many beneﬁts possible suitable practical problem existing singular model example bohb method 93 ha some success dealing hpo problem combining bayesian mization hyperband addition future research sider model performance time budget develop hpo algorithm suit application generalization generalization another issue hpo model since parameter evaluation done ﬁnite number tions datasets optimal value detected hpo approach might not optimum data similar issue ml model occur model closely ﬁt ﬁnite number known data point unﬁt unseen data 124 tion also common concern algorithm like hyperband bohb since need extract subset sent entire dataset one solution reduce avoid use validation identify stable optimum performs best subset instead sharp optimum only form well singular validation set 6 however validation increase execution time would beneﬁcial method better deal overﬁtting improve generalization future research randomness stochastic component objective function ml algorithm thus some case optimal ﬁguration might different run randomness could due various procedure certain ml model like neural work initialization different sampled subset bagging model 89 due certain procedure hpo algorithm like crossover mutation operation addition often ﬁcult hpo method identify global optimum due fact hpo problem mainly problem many ing hpo algorithm only collect several different value caused randomness thus existing hpo model improved reduce impact ness one possible solution run hpo method multiple time select value occurs ﬁnal optimum scalability practice one main limitation many existing hpo work tightly integrated one couple machine learning library like sklearn kera restricts only work single node instead large data volume 3 tackle large datasets some distributed machine learning platform like apache systemml 125 spark mlib 126 developed however only hpo framework exist support distributed ml therefore research effort scalable hpo framework like one supporting distributed ml platform developed support library hand future practical hpo algorithm scalability efﬁciently optimize small size large size irrespective whether ous discrete categorical conditional continuous updating capability practice many datasets not stationary constantly updated adding new data deleting old data ingly optimal value combination may also change change data currently developing hpo method capacity continuously tune value data change ha not drawn much attention since researcher data analyst often not alter ml model achieving currently optimal performance 3 however since optimal value would change data change proper approach proposed achieve uous updating capability conclusion machine learning ha become primary strategy tackling problem ha widely used various cation apply ml model practical problem parameter need tuned ﬁt speciﬁc datasets however since scale produced data greatly increased manually tuning extremely computationally expensive ha become crucial optimize automatic process survey paper sively discussed research domain optimization well apply ferent ml model theory practical experiment apply optimization method ml model type ml model main concern hpo method selection summarize bohb recommended choice optimizing ml model randomly selected subset given dataset since efﬁciently optimize type otherwise bo model recommended small conﬁguration space pso usually best choice large conﬁguration space moreover some existing useful hpo tool framework open challenge potential research direction also provided highlighted practical use future research purpose hope survey paper serf useful resource ml user developer data analyst researcher use tune ml model utilizing proper hpo technique framework also hope help enhance understanding challenge still exist within hpo domain thereby advancing hpo ml cation future research credit authorship contribution statement li yang conceptualization methodology software validation formal analysis investigation data curation writing original draft visualization abdallah shami conceptualization resource writing review editing supervision project istration funding acquisition declaration competing interest author declare no known competing cial interest personal relationship could appeared inﬂuence work reported paper yang shami neurocomputing 415 2020 313 reference 1 jordan mitchell machine learning trend perspective prospect science 349 2015 2 zöller huber benchmark survey automated machine learning framework arxiv preprint 2019 3 shawi maher sakr automated machine learning open challenge arxiv preprint 2019 4 kuhn johnson applied predictive modeling springer 2013 isbn 9781461468493 5 diaz nannicini samulowitz effective algorithm hyperparameter optimization neural network ibm dev 61 2017 6 hutter kotthoff vanschoren ed automatic machine learning method system challenge springer 2019 isbn 9783030053185 7 muñoz castañeda escudero garcía carriegos effect sampling dataset hyperparameter optimization phase efﬁciency machine learning algorithm complexity 2019 2019 8 abreu automated architecture design deep neural network arxiv preprint 2019 9 steinholtz comparative study optimization algorithm tuning deep neural network thesis dept elect luleå univ 2018 10 luo review automatic selection method machine learning algorithm value netw model anal heal inf bioinf 5 2016 11 maclaurin duvenaud adam hyperparameter optimization reversible learning arxiv preprint 2015 12 bergstra bardenet bengio kégl algorithm optimization proc adv neural inf process syst 2011 13 james yoshua random search optimization mach learn 13 1 2012 14 eggensperger feurer hutter bergstra snoek hoos brown towards empirical foundation assessing bayesian optimization hyperparameters bayesopt work 2013 15 eggensperger hutter hoos efﬁcient benchmarking hyperparameter optimizers via surrogate proc natl conf artif intell 2 2015 16 li jamieson desalvo rostamizadeh talwalkar hyperband novel approach hyperparameter optimization mach learn 18 2012 17 yao et taking human learning application survey automated machine learning arxiv preprint 2018 18 lessmann stahlbock crone optimizing hyperparameters support vector machine genetic algorithm proc 2005 int conf artif intell icai 05 1 2005 19 lorenzo nalepa kawulok ramos paster particle swarm optimization selection deep neural network proc acm int conf genet evol comput 2017 20 sun cao zhu zhao survey optimization method machine learning perspective arxiv preprint 2019 21 bradley hax applied mathematical programming reading massachusetts 1977 22 bubeck convex optimization algorithm complexity found trend mach learn 8 2015 23 shahriari de freitas unbounded bayesian optimization via regularization proc artif intell 2016 24 diaz nannicini samulowitz effective algorithm hyperparameter optimization neural network ibm dev 61 2017 25 gambella ghaddar optimization model machine learning survey arxiv preprint 2019 26 spark talwalkar haas franklin jordan kraska automating model search large scale machine learning proc acm symp cloud comput 2015 27 nocedal wright numerical optimization 2006 isbn 28 caruana empirical comparison supervised learning algorithm acm int conf proc ser 148 2006 http 29 kramer machine learning evolution strategy springer international publishing cham switzerland 2016 pp 30 pedregosa et machine learning python mach learn 12 2011 31 chen xgboost scalable tree boosting system arxiv preprint 2016 32 chollet kera 2015 33 gambella ghaddar optimization model machine learning survey 2019 34 bishop pattern recognition machine learning 2006 springer isbn 35 hoerl kennard ridge regression application nonorthogonal problem technometrics 12 1970 36 melkumova shatskikh comparing ridge lasso estimator data analysis procedia eng 201 2017 37 tibshirani regression shrinkage selection via lasso stat soc ser b 58 1996 38 hosmer jr lemeshow applied logistic regression technometrics 34 1 2013 39 ogutu piepho genomic selection using regularized linear regression model ridge regression lasso elastic net extension bmc proc biomed cent 6 2012 40 keller gray fuzzy neighbor algorithm ieee trans syst man cybern 1985 41 zuo zhang wang kernel neighbor classiﬁcation pattern anal appl 11 2008 42 smola vapnik support vector regression machine adv neural inf process syst 9 1997 43 yang muresan hadjileontiadis visibility estimation algorithm intelligent transportation system ieee access 6 2018 44 zhang jin yang hauptmann modiﬁed logistic regression approximation svm application text categorization proceeding twent int conf mach learn 2 2003 45 soliman mahmoud classiﬁcation system remote sensing satellite image using support vector machine kernel function 2012 int conf informatics syst info 2012 2012 46 rish empirical study naive bayes classiﬁer ijcai 2001 work empir method artif intell 2001 47 sulzmann fürnkranz hüllermeier pairwise naive bayes classiﬁers lect note comput sci including subser lect note artif intell lect note bioinformatics 4701 lnai 2007 48 bustamante garrido soto comparing fuzzy naive bayes gaussian naive bayes decision making robocup lect note comput sci including subser lect note artif intell lect note bioinformatics 2006 4293 lna 49 kibriya frank pfahringer holmes multinomial naive bayes text categorization revisited lect note artif intell subseries lect note comput sci 3339 2004 50 rennie shih teevan karger tackling poor assumption naive bayes text classiﬁers proc twent int conf mach learn icml 2003 51 narayanan arora bhatia fast accurate sentiment classiﬁcation using enhanced naíve bayes model arxiv preprint 2013 52 rasoul david survey decision tree classiﬁer methodology ieee trans syst man cybern 21 1991 53 mania jammal hawilo shami heidari larabi brunner machine learning virtual network function placement 2019 ieee glob commun conf globecom 2019 proc 2019 54 yang moubayed hamieh shami intelligent intrusion detection system internet vehicle 2019 ieee glob commun conf globecom 2019 proc 2019 55 sander informing use hyperparameter optimization metalearning proc ieee int conf data mining icdm 2017 56 injadat salo nassif essex shami bayesian optimization machine learning algorithm towards anomaly detection 2018 ieee glob commun conf 2018 57 arjunan modi enhanced intrusion detection framework securing network layer cloud computing isea asia secur priv conf 2017 iseasp 2017 2017 doi 58 xia liu li liu boosted decision tree approach using bayesian optimization credit scoring expert syst appl 78 2017 59 dietterich ensemble method machine learning mult classif syst 2000 1857 60 yin kann yu schütze comparative study cnn rnn natural language processing arxiv preprint 2017 61 koutsoukas monaghan li huan investigating deep neural network comparison performance 314 yang shami neurocomputing 415 2020 shallow method modeling bioactivity data cheminf 9 2017 62 domhan springenberg hutter speeding automatic hyperparameter optimization deep neural network extrapolation learning curve ijcai int jt conf artif intell 2015 63 ozaki yano onishi effective hyperparameter optimization using method deep learning ipsj trans comput vi appl 9 2017 64 soon khaw chuah kanesan optimisation deep cnn architecture vehicle logo recognition iet intell transp syst 12 2018 65 han liu fan new image classiﬁcation method using cnn transfer learning web data augmentation expert syst appl 95 2018 66 di francescomarino duma federici ghidini maggi rizzi simonetto genetic algorithm hyperparameter optimization predictive business process monitoring inf syst 74 2018 http 67 moubayed injadat shami lutﬁyya student engagement level environment clustering using distance educ 34 2020 68 ding cluster structure clustering via principal component analysis lect note comput sci including subser lect note artif intell lect note bioinformatics 3056 2004 69 moon algorithm ieee signal process mag 13 6 1996 70 bermak shi chan fast robust gas identiﬁcation system using integrated gas sensor technology gaussian mixture model ieee sen j 5 2005 71 hierarchical clustering algorithm document dataset data min knowl discov 10 2005 72 khan rehman aziz fong sarasvady vishwa dbscan past present future int conf appl digit inf web technol icadiwt 2014 2014 pp 73 zhou wang li research adaptive parameter determination dbscan algorithm inf comput sci 9 2012 74 shlens tutorial principal component analysis arxiv preprint 2014 75 halko martinsson tropp finding structure randomness probabilistic algorithm constructing approximate matrix decomposition siam rev 53 2 2011 76 loog conditional linear discriminant analysis proc int conf pattern recognit 2 2006 77 howland wang park solving small sample size problem face recognition using generalized discriminant analysis pattern recognit 39 2006 78 ilievski akhtar feng shoemaker efﬁcient hyperparameter optimization deep learning algorithm using deterministic rbf surrogate aaai conf artif intell aaai 2017 2017 pp 79 claesen simm popovic moreau de moor easy hyperparameter search using optunity arxiv preprint 2014 80 witt approximation simple randomized search heuristic proceeding annual symposium theoretical aspect computer science stacs 05 stuttgart germany 2005 pp 81 bengio optimization hyperparameters neural comput 12 8 2000 82 yang amari complexity issue natural gradient descent method training multilayer perceptrons neural comput 10 8 1998 2157 83 snoek larochelle adam practical bayesian optimization machine learning algorithm adv neural inf process syst 4 2012 84 hazan klivans yuan hyperparameter optimization spectral approach arxiv preprint 2017 85 seeger gaussian process machine learning int neural syst 14 2004 86 hutter hoos sequential optimization general algorithm conﬁguration proc lion 5 2011 87 dewancker mccourt clark bayesian optimization primer 2015 url bayesian optimization 88 hensman fusi lawrence gaussian process big data arxiv preprint 2013 89 claesen de moor hyperparameter search machine learning arxiv preprint 2015 90 bottou machine learning stochastic gradient descent proceeding compstat springer 2010 pp 91 zhang xu huang chen new optimal sampling rule ﬁdelity optimization via ordinal transformation ieee int conf autom sci eng 2016 92 karnin koren somekh almost optimal exploration bandit int conf mach learn icml 2013 28 2013 93 falkner klein hutter bohb robust efﬁcient hyperparameter optimization scale int conf mach learn icml 2018 4 2018 94 gogna tayal metaheuristics review application exp theor artif intell 25 2013 95 itano de abreu de sousa extending mlp ann optimization using genetic algorithm proc int jt conf neural network 2018 96 kazimipour li qin review population initialization technique evolutionary algorithm 2014 ieee congr evol comput 2014 97 rahnamayan tizhoosh salama novel population initialization method accelerating evolutionary algorithm comput math appl 53 2007 98 lobo goldberg pelikan time complexity genetic algorithm exponentially scaled problem proc genet evol comput conf 2000 158 99 shi eberhart parameter selection particle swarm optimization evolutionary programming vii springer 1998 pp 100 yan chen novel software partitioning method based position disturbed particle swarm optimization invasive weed optimization 32 2017 101 cheng huang hutomo multiobjective pso optimizing work shift schedule constr eng manag 144 2018 102 wang wu wang dong yu chen new population initialization method based space transformation search int conf nat comput icnc 2009 5 2009 103 wang xu wang combination hyperband bayesian optimization hyperparameter optimization deep learning arxiv preprint 2018 104 cazzaniga nobile besozzi impact particle initialization pso parameter estimation case point 2015 ieee conf comput intell bioinforma comput biol cibcb 2015 2015 105 bayesopt bayesian optimization library nonlinear optimization experimental design bandit mach learn 15 2015 106 bergstra komer eliasmith yamins cox hyperopt python library model selection hyperparameter optimization comput sci discov 8 2015 107 komer bergstra eliasmith automatic hyperparameter conﬁguration proc icml workshop automl 2014 108 pumperla hyperas 2019 109 lindauer eggensperger feurer falkner biedenkapp hutter smac algorithm conﬁguration python http 110 tim head mechcoder gilles louppe et doi 111 knudde van der herten dhaene couckuyt gpﬂowopt bayesian optimization library using tensorflow arxiv preprint 2017 112 autonomio talos computer software 113 hertel sadowski collado baldi sherpa hyperparameter optimization machine learning model conf neural inf process 2018 114 abadi agarwal barham brevdo chen citro et tensorflow machine learning heterogeneous distributed system arxiv preprint 2016 115 grandgirard poinsot krespi nénon cortesero osprey hyperparameter optimization machine learning 103 2002 116 franceschi donini frasconi pontil forward reverse based hyperparameter optimization int conf mach learn icml 2017 70 2017 117 fortin de rainville gardner parizeau gagn e deap evolutionary algorithm made easy mach learn 13 2012 2175 118 olson moore tpot pipeline optimization tool automating machine learning auto mach learn 2019 119 rapin teytaud nevergrad optimization platform 2018 120 injadat moubayed nassif shami systematic ensemble model selection approach educational data mining syst 200 2020 105992 121 bishop neural network pattern recognition oxford university press 1995 122 krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural network adv neural inf process syst 25 2012 123 hansen auger mersmann tusar brockhoff coco platform comparing continuous optimizers setting arxiv preprint 2016 yang shami neurocomputing 415 2020 315 124 cawley talbot model selection subsequent selection bias performance evaluation mach learn 11 2010 125 boehm surve tatikonda et systemml declarative machine learning spark proc vldb endow 9 2016 126 meng bradley yavuz et mllib machine learning apache spark mach learn 17 1 2016 127 moubayed injadat shami lutﬁyya dns domain detection data analytics machine learning based approach 2018 ieee glob commun conf globecom 2018 128 li yang comprehensive visibility indicator algorithm adaptable speed limit control intelligent transportation system university guelph 2018 129 salo injadat moubayed nassif essex clustering enabled classiﬁcation using ensemble feature selection intrusion detection 2019 int conf comput netw commun icnc 2019 130 moubayed aqeeli shami feature selection classiﬁcation model dns detection 2020 ieee conf electr comput eng 2020 131 injadat moubayed nassif shami optimized bagging ensemble model selection educational data mining springer appl intell 2020 132 yang shami hyperparameter optimization machine learning algorithm 2020 li yang received degree computer science wuhan university science technology wuhan china 2016 masc degree neering university guelph guelph canada since 2018 ha working toward degree department electrical computer engineering western university london canada research interest include cybersecurity machine learning data analytics intelligent transportation system abdallah shami professor ece department western university ontario canada director optimized computing tions laboratory western university currently associate editor ieee transaction mobile computing ieee network ieee communication survey tutorial ha chaired key symposium ieee globecom ieee icc ieee icnc iccit wa elected chair ieee communication society technical committee communication software ieee london ontario section chair 316 yang shami neurocomputing 415 2020