1 foundation trend multimodal machine learning principle challenge open question paul pu liang amir zadeh morency machine learning department language technology institute carnegie mellon university usa multimodal machine learning vibrant research field aim design computer agent intelligent capability understanding reasoning learning integrating multiple communicative modality including linguistic acoustic visual tactile physiological message recent interest video understanding embodied autonomous agent generation multisensor fusion application domain healthcare robotics multimodal machine learning ha brought unique computational theoretical challenge machine learning community given heterogeneity data source interconnection often found modality however breadth progress multimodal research ha made difficult identify common theme open question field synthesizing broad range application domain theoretical framework historical recent perspective paper designed provide overview computational theoretical foundation multimodal machine learning start defining three key principle modality heterogeneity connection interaction driven subsequent innovation propose taxonomy six core technical challenge representation alignment reasoning generation transference quantification covering historical recent trend recent technical achievement presented lens taxonomy allowing researcher understand similarity difference across new approach end motivating several open problem future research identified taxonomy cc concept computing methodology learning artificial intelligence computer vision natural language processing additional key word phrase multimodal machine learning representation learning data heterogeneity feature interaction language vision multimedia acm reference format paul pu liang amir zadeh morency foundation trend multimodal machine learning principle challenge open question preprint 1 1 article 1 october 2022 36 page 1 introduction ha always grand goal artificial intelligence develop computer agent intelligent capability understanding reasoning learning multimodal experience data similar human perceive interact world using multiple sensory modality recent advance embodied autonomous agent 37 222 car 295 image video understanding 11 243 image video generation 210 234 multisensor fusion application domain robotics 136 170 healthcare 119 151 closer ever intelligent agent integrate learn many sensory modality author address paul pu liang pliang amir zadeh abagherz morency morency machine learning department language technology institute carnegie mellon university 5000 forbes ave burgh pa usa permission make digital hard copy part work personal classroom use granted without fee provided copy not made distributed profit commercial advantage copy bear notice full citation first page copyright component work must honored us contact 2022 copyright held preprint vol 1 no 1 article publication date october 20 feb 2023 paul pu liang amir zadeh morency 12 12 representation alignment transference generation quantification reasoning fig core research challenge multimodal learning 1 representation study represent summarize multimodal data reflect heterogeneity interconnection individual modality element 2 alignment aim identify connection interaction across element 3 reasoning aim compose knowledge multimodal evidence usually multiple inferential step task 4 generation involves learning generative process produce raw modality reflect interaction structure coherence 5 transference aim transfer knowledge modality representation 6 quantification involves empirical theoretical study better understand multimodal learning process vibrant research field multimodal machine learning brings unique challenge given heterogeneity data interconnection often found modality ha widespread application multimedia 184 affective computing 204 robotics 127 136 interaction 190 228 healthcare 40 180 however rate progress multimodal research ha made difficult identify common theme underlying historical recent work well key open question field synthesizing broad range multimodal research paper designed provide overview methodological computational theoretical foundation multimodal machine learning complement recent survey vision language 269 language reinforcement learning 161 multimedia analysis 19 interaction 114 better understand foundation multimodal machine learning begin defining three key principle driven subsequent technical challenge innovation 1 modality heterogeneous information present often show diverse quality structure representation 2 modality connected since often related share commonality 3 modality interact give rise new information used task inference building upon definition propose new taxonomy six core challenge multimodal learning representation alignment reasoning generation transference quantification see figure 1 constitute core multimodal technical challenge understudied conventional unimodal machine learning need tackled order progress field forward 1 representation learn representation reflect heterogeneity nections modality element cover approach 1 representation fusion integrating information two modality capture interaction 2 representation coordination interchanging information goal keeping number representation improving multimodal contextualization 3 sentation fission creating larger set disjoint representation reflects knowledge internal structure data clustering factorization preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning 2 alignment identify connection interaction modality element alignment challenging since may depend dependency involves ambiguous segmentation word utterance could either not exist cover 1 discrete alignment identifying connection discrete element across modality 2 continuous alignment modeling alignment tinuous modality signal ambiguous segmentation 3 contextualized representation learning better representation capturing interaction element 3 reasoning defined composing knowledge usually multiple inferential step exploit problem structure specific task reasoning involves 1 modeling structure composition occurs 2 intermediate concept composition process 3 understanding inference paradigm abstract concept 4 leveraging external knowledge study structure concept inference 4 generation involves learning generative process produce raw modality egorize subchallenges 1 summarization summarizing multimodal data reduce information content highlighting salient part input 2 translation lating one modality another keeping information content consistent connection 3 creation simultaneously generating multiple modality increase information content maintaining coherence within across modality 5 transference aim transfer knowledge modality usually help target modality may noisy limited resource transference exemplified 1 transfer adapting model task involving primary modality 2 transferring information secondary primary modality sharing representation space modality 3 model induction keeping individual unimodal model separate transferring information across model 6 quantification sixth final challenge involves empirical theoretical study better understand 1 dimension heterogeneity multimodal datasets subsequently influence modeling learning 2 presence type modality connection interaction multimodal datasets captured trained model 3 learning optimization challenge involved heterogeneous data finally conclude paper perspective multimodal learning motivating open research question identified taxonomy survey wa also presented author visual medium tutorial cvpr 2022 naacl 2022 well course multimodal machine learning advanced topic multimodal machine learning cmu reader encouraged check publicly available video recording additional reading material discussion probe motivating open research question multimodal learning 2 foundational principle multimodal research modality refers way natural phenomenon perceived expressed example modality include speech audio recorded microphone image video captured via camera force vibration captured via haptic sensor modality placed along spectrum raw abstract raw modality closely detected sensor speech recording microphone image captured camera abstract modality farther away sensor language extracted speech recording object detected image even abstract concept like sentiment intensity object category multimodal refers situation multiple modality involved research tive multimodal entail computational study heterogeneous interconnected connection interaction modality firstly modality heterogeneous information present different modality often show diverse quality structure representation secondly preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency dimension heterogeneity element distribution structure information noise relevance fig information present different modality often show diverse quality structure representation dimension heterogeneity measured via difference individual element distribution structure element well modality information noise task relevance modality not independent entity rather share connection due complementary information thirdly modality interact different way integrated task expand three foundational principle multimodal research following subsection principle 1 modality heterogeneous principle heterogeneity reflects observation information present different modality often show diverse quality structure representation heterogeneity seen spectrum two image camera capture view modulo camera wear tear closer homogeneous two different language capture meaning different depending language family slightly heterogeneous language vision even heterogeneous section present list dimension heterogeneity see figure 2 illustration dimension complementary may overlap multimodal problem likely involves heterogeneity multiple dimension 1 element representation modality typically comprised set element basic unit data not rather user chooses not broken unit 26 147 example typed text recorded via set character video recorded via set frame graph recorded via set node edge basic element present modality represent formally dimension measure heterogeneity sample space representation space modality element 2 distribution refers frequency likelihood element modality element typically follow unique distribution word linguistic corpus following zipf law classic example distribution heterogeneity refers difference frequency likelihood element different frequency recorded signal density element 3 structure natural data exhibit structure way individual element composed form entire modality 38 example image exhibit spatial structure across individual object element language hierarchically composed individual word signal exhibit temporal structure across time structure heterogeneity refers difference underlying structure 4 information measure total information content present modality subsequently information heterogeneity measure difference information content across modality could formally measured information theoretic metric 227 5 noise noise introduced several level across naturally occurring data also data recording process natural data noise includes occlusion imperfection generated data imperfect keyboard typing unclear speech data ambiguity due sensor failure 151 noise heterogeneity measure difference noise distribution across modality well difference ratio 6 relevance finally modality show different relevance toward specific task context certain modality may useful certain task others 78 task relevance describes modality used inference context relevance describes modality contextualized modality preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning 9 statistical semantic association correlation dependency causal temporal correspondence grounding relationship laptop used function connection shared information relates modality unique unique stronger weaker unconnected fig modality connection describe modality related share commonality spondences concept language image dependency across spatial temporal dimension connection studied statistical semantic perspective useful take dimension heterogeneity account studying unimodal multimodal data unimodal case specialized encoders typically designed capture unique characteristic modality 38 multimodal case modeling heterogeneity useful learning representation capturing alignment 314 key subchallenge quantifying multimodal model 150 principle 2 modality connected although modality heterogeneous often connected due shared complementary information presence shared information often contrast unique information exists solely single modality 290 modality connection describe extent dimension information shared across modality reasoning connection multimodal data helpful think statistical semantic approach see figure 3 statistical perspective connection identified distributional pattern multimodal data semantic approach define connection based domain knowledge modality share contain unique information 1 statistical association exists value one variable relate value another example two element may resulting higher frequency occurring time statistically could lead correlation degree element linearly related association perspective discovering element associated important modeling joint distribution across modality multimodal representation alignment 257 2 statistical dependence go deeper association requires understanding exact type statistical dependency two element example causal dependency one element another underlying confounder causing element present time form dependency could spatial temporal one element occurring typically statistical association estimated purely data understanding nature statistical dependence requires some knowledge element underlying relationship 188 267 3 semantic correspondence seen problem ascertaining element one modality share semantic meaning element another modality 192 ing correspondence fundamental many problem related language grounding 46 translation retrieval 203 alignment 248 4 semantic relation finally semantic relation generalize semantic correspondence instead modality element sharing exact meaning semantic relation includes attribute describing exact nature relationship two modality element semantic preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency additive contextualized asymmetric noninteracting noninteracting information mechanic response equivalence enhancement independence dominance modulation emergence redundancy fig several dimension modality interaction 1 interaction information study whether common redundant information unique information involved interaction 2 interaction mechanic study manner interaction occurs 3 interaction response study inferred task change presence multiple modality logical causal functional relation identifying semantically related connection important reasoning 26 172 principle 3 modality interact modality interaction study modality element interact give rise new information integrated together task inference note important difference modality connection interaction connection exist within multimodal data whereas interaction only arise modality integrated processed together bring new response figure 4 provide illustration some dimension interaction exist 1 interaction information investigates type connected information involved interaction interaction involves shared information common modality interaction redundant interaction one doe not solely rely shared information instead relies different ratio shared unique possibly even synergistic information 290 2 interaction mechanic functional operator involved integrating modality element task inference example interaction expressed statistically additive form 117 well semantic perspective two element interact logical causal temporal operation 268 3 interaction response study inferred response change presence multiple modality example redundant interaction say two modality create equivalence response multimodal response response either modality enhancement multimodal response display higher confidence hand interaction modulation emergence happen exist different multimodal versus unimodal response 197 core technical challenge building three core principle detailed review recent work propose new taxonomy characterize core technical challenge multimodal research representation alignment reasoning generation transference quantification table 1 summarize full taxonomy six core challenge subchallenges category corresponding approach recent example category following section describe new taxonomy detail also revisit principle heterogeneity connection interaction see pose research question inspire research six challenge preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning table table summarizes taxonomy 6 core challenge multimodal machine learning subchallenges category corresponding approach representative example believe taxonomy help catalog rapid progress field better identify open research question challenge subchallenge approach key example representation fusion abstract 117 310 raw 24 209 fusion coordination strong 75 206 partial 276 319 coordination fission 94 262 1 48 fission alignment discrete connection local 60 100 global 142 alignment continuous alignment warping 90 103 segmentation 243 contextualization joint 140 93 159 graphical 301 reasoning structure modeling hierarchical 15 temporal 297 interactive 161 discovery 200 intermediate concept attention 299 discrete symbol 13 274 language 109 317 inference paradigm logical 82 246 causal 4 189 304 external knowledge knowledge graph 86 324 commonsense 196 315 generation summarization extractive 52 270 abstractive 139 193 translation 122 135 generative 6 115 210 creation conditional decoding 63 191 321 transference transfer tuning 208 266 multitask 150 235 transfer 160 representation 118 312 generation 202 249 model induction 33 68 239 302 quantification heterogenity importance 78 195 bias 92 199 noise 163 interconnection connection 3 42 255 interaction 94 149 285 learning generalization 150 212 optimization 284 293 tradeoff 151 9 challenge 1 representation fusion coordination fission modality representation modality representation modality representation definition learning representation reflect interaction individual element across different modality fig challenge 1 aim learn representation reflect interaction individual modality element 1 fusion integrating information reduce number separate tions 2 coordination interchanging information goal keeping number representation improving multimodal contextualization 3 fission creating larger set decoupled representation reflects knowledge internal structure 3 challenge 1 representation first fundamental challenge learn representation reflect interaction individual element across different modality challenge seen learning local representation element representation using holistic feature section cover 1 representation fusion integrating information 2 modality effectively reducing number separate representation 2 representation coordination interchanging information goal keeping number representation ing multimodal contextualization 3 representation fission creating new decoupled set representation usually larger number input set reflects knowledge internal structure data clustering factorization figure 5 subchallenge representation fusion representation fusion aim learn joint representation model interaction individual element different modality effectively reducing number separate preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency fusion abstract modality homogeneous fusion raw modality fusion heterogeneous fission fission encoder encoder fusion fig categorize representation fusion approach 1 fusion abstract modality modal encoders first capture holistic representation element fusion relatively homogeneous representation 2 fusion raw modality entail representation fusion early stage perhaps directly involving heterogeneous raw modality representation categorize approach fusion abstract modality fusion raw modality figure 6 fusion abstract modality suitable unimodal encoders first applied capture holistic representation element modality entirely several building block representation fusion used learn joint representation result fusion happens abstract representation level hand fusion raw modality entail representation fusion early stage minimal preprocessing perhaps even involving raw modality fusion abstract modality begin treatment representation fusion abstract representation additive multiplicative interaction operator seen tiable building block combining information two stream data flexibly inserted almost any unimodal machine learning pipeline given unimodal data feature additive fusion seen learning new joint representation zmm ùúñ weight learned additive fusion bias term ùúñthe error term joint representation zmm directly taken prediction ÀÜ ùë¶ additive fusion resembles late ensemble fusion ÀÜ unimodal predictor 74 otherwise additive representation zmm also undergo subsequent unimodal multimodal processing 23 multiplicative interaction extend additive interaction include cross term model used extensively statistic interpreted moderation effect affecting linear relationship ùë¶ 25 overall purely additive interaction zmm seen polynomial input modality combining additive multiplicative zmm capture polynomial go beyond first interaction tensor specifically designed explicitly capture interaction across modality 310 given unimodal data tensor defined zmm outer product 28 76 tensor product higher order represent polynomial interaction higher order element 98 however computing tensor product expensive since dimension scale exponentially number modality several efficient approximation based decomposition proposed 98 158 finally multiplicative interaction mi generalize additive multiplicative operator include learnable parameter capture interaction 117 general form mi defines bilinear product zmm 1 u b w u z b trainable parameter multimodal gated unit learn representation dynamically change every input 47 284 general form written zmm ‚Ñérepresents function sigmoid activation product ‚Ñé commonly referred attention weight learned attend recent work ha explored expressive form learning attention weight using mechanism 261 neural network layer 18 47 even hard gated unit sharper attention 55 preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning strong coordination closer partial coordination correlation equivalence order hierarchy relationship fig spectrum representation coordination function strong coordination aim force strong equivalence dimension whereas partial coordination only certain dimension may coordinated capture general connection correlation order hierarchy relationship fusion raw modality entail representation fusion early stage perhaps even involving raw modality approach typically bear resemblance early sion 23 performs concatenation input data applying prediction model zmm fusing raw modality level challenging since raw modality likely exhibit dimension heterogeneity nevertheless barnum et al 24 demonstrated robustness benefit fusion early stage gadzicki et al 77 also found complex early fusion outperform abstract fusion account greater heterogeneity complex early fusion many approach rely generic encoders applicable modality convolutional layer 24 77 transformer 150 153 however complex fusion model actually learn interaction modality element not necessarily according hessel lee 94 cover fundamental analysis question quantification challenge subchallenge representation coordination representation coordination aim learn multimodal contextualized representation coordinated interconnection figure 7 contrast representation fusion nation keep number representation improves multimodal contextualization start discussion strong coordination enforces strong equivalence modality element moving partial coordination capture general connection correlation order hierarchy relationship beyond similarity strong coordination aim bring semantically corresponding modality close together coordinated space thereby enforcing strong equivalence modality element example model would encourage representation word dog image dog close semantically positive pair distance word dog image car far apart semantically negative pair 75 coordination distance typically cosine distance 174 287 loss 102 recent work ha explored representation coordination scaling contrastive learning image text pair 206 also found contrastive learning provably capture redundant information across two view 256 258 not information addition contrastive learning several approach instead learn coordinated space mapping corresponding data one modality another 69 example socher et al 236 map image embeddings word embedding space image classification similar idea used learn coordinated representation text video audio 202 well pretrained language model image feature 249 partial coordination instead strictly capturing equivalence via strong coordination partial coordination instead capture general modality connection correlation order chies relationship achieve goal partially coordinated model enforce different type constraint representation space beyond semantic similarity perhaps only certain dimension representation preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency fission fission encoder strong coordination closer partial coordination fig representation fission creates larger set decoupled representation reflects knowledge internal structure 1 fission factorizes information primarily modality multimodal information redundant modality 2 fission attempt break multimodal data individual subspace canonical correlation analysis cca computes linear projection maximizes lation two random variable enforcing dimension new representation orthogonal 254 cca model used extensively retrieval 211 signal analysis 221 emotion recognition 186 increase pressiveness cca several nonlinear extension proposed including kernel cca 134 deep cca 16 cca autoencoders 283 ordered hierarchical space another example representation coordination come image language 276 aim capture partial order language image embeddings enforce hierarchy coordinated space similar model using denotation graph wa also proposed young et al 306 denotation graph used induce partial ordering hierarchy relationship coordination order learn coordinated space capture semantic ship element beyond correspondence zhang et al 319 use structured representation text image create multimodal concept taxonomy delaherche chetouani 61 learn coordinated representation capturing hierarchical relationship alviar et al 12 apply multiscale coordination speech music using partial correlation measure finally xu et al 298 learn coordinated representation using cauchy loss strengthen robustness outlier subchallenge representation fission finally representation fission aim create new decoupled set representation usually larger number input representation set reflects knowledge internal multimodal structure data clustering independent factor variation information comparison joint coordinated representation representation fission enables careful interpretation controllability depending granularity decoupled factor method categorized fission figure 8 fission aim factorize information primarily modality multimodal information redundant modality 101 262 disentangled sentation learning aim learn mutually independent latent variable explain particular variation data 30 95 ha useful fission enforcing dence constraint multimodal latent variable 101 262 tsai et al 262 hsu glass 101 study factorized multimodal representation demonstrate tance multimodal factor towards generation prediction shi et al 231 study fission multimodal variational autoencoders using layer wu goodman 292 instead use layer representation disentanglement suitable difficult retrain disentangled model especially large pretrained multimodal model empirical function projection emap 94 approach disentanglement effect unimodal ditive contribution interaction multimodal task work arbitrary multimodal model task emap also closely related use shapley value feature preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning 11 11 challenge 2 alignment definition identifying correspondence dependency element multiple modality following structure discrete alignment continuous alignment contextualized representation discrete element connection segmentation continuous warping alignment representation fig alignment aim identify connection interaction modality element recent work ha involved 1 discrete alignment identify connection among discrete element 2 continuous alignment continuous signal ambiguous segmentation 3 contextualized representation learning capture interaction connected element disentanglement interpretation 176 also used representation disentanglement general model fission beyond factorizing only individual modality representation grained fission attempt break multimodal data individual subspace covered modality 277 clustering approach group data based semantic ity 165 integrated multimodal network representation fission prediction example hu et al 102 combine clustering representation pervised audiovisual learning chen et al 48 combine clustering contrastive learning video subspace clustering 1 approximate graph laplacians 125 gate mixture model 124 dictionary learning 126 also integrated multimodal model motivated similar goal representation fission matrix factorization technique also seen several application multimodal prediction 10 image retrieval 41 4 challenge 2 alignment second challenge identify connection interaction element multiple modality example analyzing speech gesture human subject align specific gesture spoken word utterance alignment modality challenging since may depend dependency involves ambiguous segmentation word utterance could either not exist section cover recent work multimodal alignment involving 1 discrete alignment identifying connection discrete element across modality 2 continuous alignment modeling ment continuous modality signal ambiguous segmentation 3 contextualized representation learning better multimodal representation capturing interaction element figure 9 subchallenge discrete alignment first subchallenge aim identify connection discrete element multiple modality describe recent work 1 local alignment discover connection given matching pair modality element 2 global alignment alignment must performed globally learn connection matchings figure 10 local alignment connected element particularly suitable multimodal task clear segmentation discrete element word text object bounding box image video task visual coreference resolution 131 visual referring expression recognition 58 59 retrieval 75 203 supervised data form connected modality pair contrastive learning popular approach goal match representation concept expressed different modality 23 several objective function learning aligned space varying quantity paired 43 107 unpaired 85 preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency continuous warping modality segmentation local global undirected directed fig discrete alignment identifies connection discrete element spanning 1 local alignment discover connection given matching pair 2 global alignment alignment must performed globally learn connection matchings modality element data proposed many idea enforce strong 75 152 partial 16 276 319 representation coordination also applicable local alignment several example include aligning book corresponding 323 matching referring expression visual object 169 finding similarity image region description 105 method local alignment also enabled learning shared semantic concept not purely based language also additional modality vision 107 sound 60 236 multimedia 323 useful downstream task global alignment modality pairing not available alignment must performed globally element across modality optimal transport ot approach 278 belong broader set matching algorithm potential solution since jointly optimize coordination function optimal coupling modality element posing alignment divergence minimization problem approach useful aligning multimodal representation space 142 205 alleviate computational issue several recent advance integrated neural network 54 approximated optimal transport entropy regularization 288 formulated convex relaxation efficient learning 85 subchallenge continuous alignment far one important assumption made modality element already segmented discretized certain modality display clear segmentation sentence object region image many case segmentation not readily provided continuous signal financial medical data satellite weather image data without clear semantic boundary mri image setting method based warping segmentation recently proposed continuous warping aim align two set modality element representing continuous representation space forming bridge representation space adversarial training popular approach warp one representation space another initially used domain adaptation 27 adversarial training learns representation across domain domain classifier unable identify domain feature came 8 idea extended align multimodal space 100 103 181 hsu et al 100 use adversarial training align image medical report hu et al 103 design adversarial network modal retrieval munro damen 181 design alignment adversarial alignment objective multimodal action recognition dynamic time warping dtw 133 related approach segment align time series data dtw measure similarity two sequence find optimal match time warping inserting frame aligned across segmented time boundary multimodal task necessary design similarity metric modality 17 251 dtw wa extended using cca map modality coordinated space allowing alignment dtw coordination cca different modality stream jointly 260 preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning continuous warping modality segmentation undirected fig continuous alignment tackle difficulty aligning continuous signal element tation not readily available cover related work 1 continuous warping representation space 2 modality segmentation continuous signal discrete element appropriate granularity modality segmentation involves dividing data element meaningful boundary common problem involves temporal segmentation goal discover temporal boundary across sequential data several approach temporal tation include forced alignment popular approach align discrete speech unit individual word transcript 309 malmaud et al 167 explore multimodal alignment using factored hidden markov model align asr transcript ground truth clustering approach also used group continuous data based semantic similarity 165 tion ha recently emerged important preprocessing step generalizing pretraining clear segmentation boundary discrete element video pretraining without clear segmentation boundary continuous element clustering raw video audio feature discrete set approach videobert 243 form masked pretraining raw video audio data similarly approach 210 271 cmcm 156 also utilize discretized intermediate layer obtained via vector quantization showed benefit modality alignment subchallenge contextualized representation finally contextualized representation learning aim model modality connection teractions learn better representation contextualized representation used intermediate often latent step enabling better performance number downstream task cluding speech recognition machine translation medium description visual categorize work contextualized representation 1 joint undirected alignment 2 modal directed alignment 3 alignment graph network figure 12 joint undirected alignment aim capture undirected connection across pair modality connection symmetric either direction commonly referred literature unimodal bimodal trimodal interaction 164 joint undirected alignment typically captured parameterizing model alignment layer training multimodal task alignment layer include attention weight 47 tensor product 158 310 multiplicative interaction 117 recently transformer model 273 emerged powerful encoders sequential data automatically aligning capturing complementary feature different time step building upon initial transformer model multimodal transformer proposed perform joint alignment using full modality element concatenated across sequence dimension early fusion 140 243 result modality element become jointly connected modality element similarly modeling connection using similarity kernel directed alignment relates element source modality directed ner target modality model asymmetric connection example temporal attention model use alignment latent step improve many task 297 318 attention mechanism typically directed output input resulting weight reflect soft alignment distribution input multimodal transformer perform directed alignment using attention mechanism attend one modality preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency joint undirected alignment directed alignment graphical alignment fig contextualized representation learning aim model modality connection learn better resentations recent direction include 1 joint undirected alignment capture undirected symmetric connection 2 directed alignment model asymmetric connection directed manner 3 graphical alignment generalizes sequential pattern arbitrary graph structure 11 11 challenge 3 reasoning definition combining knowledge usually multiple inferential step exploiting multimodal alignment problem structure structure modeling intermediate concept inference paradigm external knowledge word fig reasoning aim combine knowledge usually multiple inferential step exploiting problem structure reasoning involves 1 structure modeling defining learning relationship reasoning occurs 2 intermediate concept used reasoning 3 inference increasingly abstract concept evidence 4 leveraging external knowledge study structure concept inference sequence another repeating bidirectional manner result two set metric contextualized representation account possibly asymmetric connection modality 159 248 261 method useful sequential data automatically aligning capturing complementary feature different 261 multimodal pretraining ha also emerged effective way train architecture aim learning representation unlabeled multimodal data ferring specific downstream task via supervised 140 pretraining objective typically consist unimodal masked prediction crossmodal masked prediction multimodal alignment prediction 93 graphical alignment generalizes sequential pattern seen undirected directed ment arbitrary graph structure element ha several benefit since doe not require element connected allows user choose different edge function ferent connection solution subcategory typically make use graph neural network 275 recursively learn element representation contextualized element locally connected neighborhood 223 275 approach applied multimodal sequential data mtag 301 capture connection human video 289 additionally add factorizes node along speaker turn 5 challenge 3 reasoning reasoning defined combining knowledge usually multiple inferential step ing multimodal alignment problem structure categorize work towards multimodal reasoning 4 subchallenges structure modeling intermediate concept inference paradigm external knowledge figure 13 1 structure modeling involves defining learning lationships reasoning occurs 2 intermediate concept study parameterization individual multimodal concept reasoning process 3 inference paradigm learns increasingly abstract concept inferred individual multimodal evidence 4 external knowledge aim leverage database study structure concept inference preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning hierarchical temporal interactive discovery fig structure modeling aim define relationship composition occurs 1 hierarchical abstract concept defined function le abstract one 2 temporal organized across time 3 interactive state change depending step decision 4 discovered latent structure unknown instead directly inferred data optimization subchallenge structure modeling structure modeling aim capture hierarchical relationship composition occurs usually via data structure parameterizing atom relation reasoning process commonly used data structure include tree 97 graph 308 neural module 15 cover recent work modeling latent hierarchical temporal interactive structure well structure discovery latent structure unknown figure 14 hierarchical structure defines system organization abstract concept defined function le abstract one hierarchical structure present many task involving language syntax visual syntax reasoning approach typically construct graph based predefined node edge category using heterogeneous variant graph neural network capture representation structure 230 using language syntactic structure guide visual module discover specific information image 15 58 based reasoning approach applied visual commonsense reasoning 155 visual question answering 220 machine translation 305 recommendation system 250 web image search 281 social medium analysis 224 temporal structure extends notion compositionality element across time necessary modality contain temporal information video audio data explicit memory mechanism emerged popular choice accumulate multimodal information across time interaction captured storage retrieval memory rajagopalan et al 209 explore various memory representation including multimodal fusion coordination factorization insight memory 297 memory 311 also successfully applied application including question answering video captioning emotion recognition sentiment analysis interactive structure extends challenge reasoning interactive setting state reasoning agent change depending local decision made every step cally formalized sequential framework challenge lie maximizing cumulative reward despite only interacting environment action 244 tackle challenge interactive reasoning growing research field modal reinforcement learning rl ha emerged intersection language understanding embodiment visual world deep reinforcement learning robotics refer reader extensive survey paper luketina et al 161 position paper bisk et al 32 full review field luketina et al 161 separate literature rl multimodal interaction necessitated problem formulation instruction following 47 286 rl multimodal data optionally used facilitate learning reading instruction manual 185 structure discovery may challenging define structure multimodal composition without some domain knowledge given task alternative approach recent work ha also explored using differentiable strategy automatically search structure fully preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency manner one first need define candidate set reasoning atom relationship using meta approach architecture search automatically search ideal sequence composition given task 200 300 approach benefit optimization trick often used neural architecture search literature memory attention composition mac similarly search series reasoning step data approach 110 finally hu et al 104 extend predefined reasoning structure obtained language parsing andreas et al 15 instead using policy gradient automatically optimize compositional structure discrete set neural module subchallenge intermediate concept second subchallenge study parameterize individual multimodal concept within reasoning process intermediate concept usually dense vector representation standard neural architecture ha also substantial work towards interpretable attention map discrete symbol language intermediate medium reasoning attention map popular choice intermediate concept since certain extent retaining differentiability example andreas et al 15 design individual module attend combine count measure parametrized attention operation input image visual question answering xu et al 299 explore soft hard attention mechanism reasoning image captioning generation related work ha also used attention map dual attention architecture 182 stacked latent attention architecture 71 multimodal reasoning typically applied problem involving complex reasoning step clevr 120 vqa 320 discrete symbol level discretization beyond attention map involves using discrete symbol represent intermediate concept recent work learning aim integrate discrete symbol intermediate step multimodal reasoning task visual question answering 15 168 274 referring expression recognition 58 core challenge approach lie maintaining differentiability discrete symbol ha tackled via differentiable reasoning 13 226 language medium finally perhaps form intermediate concept natural language discrete word phrase medium recently zeng et al 317 explore using language intermediate medium coordinate multiple separate pretrained model manner several approach also used language phrase obtained external knowledge graph facilitate interpretable reasoning 86 324 hudson manning 109 designed neural state machine simulate execution question asked image using discrete word intermediate concept subchallenge inference paradigm third subchallenge multimodal reasoning defines way increasingly abstract concept inferred individual multimodal evidence advance local representation fusion additive multiplicative sequential fusion see full review also generally applicable goal reasoning interpretable inference process domain knowledge multimodal problem end cover recent direction explicitly modeling inference process via logical causal operator example recent trend direction logical inference differentiable reasoning ha widely used represent knowledge neural network 13 226 many approach use differentiable fuzzy logic 272 provides probabilistic interpretation logical predicate function stants ensure differentiability logical operator applied visual question answering 82 visual reasoning 13 among greatest benefit logical reasoning lie preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning ability perform interpretable compositional reasoning 111 logical framework also useful entailment 246 geometric numerical reasoning 50 field logical inductive bias crucial toward strong performance causal inference extends associational level reasoning interventional factual level 198 requires extensive knowledge world imagine counterfactual effect example yi et al 304 propose clevrer benchmark focusing four specific element reasoning video descriptive color explanatory responsible predictive happen next counterfactual beyond clevrer recent work ha also proposed causal vqa 4 counterfactual vqa 189 measure robustness vqa model controlled intervention question step towards mitigating language bias vqa model method inspired integrating causal reasoning capability neural network model also shown improve robustness reduce bias 282 subchallenge external knowledge final subchallenge study derivation knowledge study defining composition structure knowledge typically derived domain knowledge datasets alternative using domain knowledge compositional structure recent work ha also explored reasoning automatically using method widely accessible weakly supervised data outside immediate task domain multimodal knowledge graph extend classic work language symbolic knowledge graph freebase 35 dbpedia 20 yago 241 wordnet 178 semantic network containing multimodal concept node multimodal relationship edge 322 multimodal knowledge graph important enable grounding structured information visual physical world example liu et al 157 construct multimodal knowledge graph containing numerical feature image entity visual genome another example containing dense annotation object attribute relationship image text 132 multimodal knowledge base shown benefit visual question answering 294 324 knowledge base completion 201 image captioning 175 303 gui et al 86 integrates knowledge transformer automatic reasoning knowledge source refer reader comprehensive survey zhu et al 322 additional reference multimodal commonsense reasoning requires deeper knowledge potentially spanning logical causal temporal relationship concept example element causal reasoning required answer question regarding image vcr 315 sualcomet 196 work also introduced datasets video text input test temporal reasoning movieqa 252 moviefib 166 tvqa 137 benchmark multimodal commonsense typically require leveraging external knowledge knowledge base 237 pretraining paradigm datasets 159 316 6 challenge 4 generation fourth challenge involves learning generative process produce raw modality reflect interaction structure coherence summarization translation creation figure 15 three category distinguished based information change input output modality following categorization text generation 62 cover recent advance well evaluation generated content subchallenge summarization summarization aim compress data create abstract represents important relevant information within original content recent work ha explored various input modality guide text summarization image 51 video 141 audio 70 116 139 recent trend multimodal summarization include extractive abstractive approach extractive preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency 7 challenge 4 generation definition learning generative process produce raw modality reflects interaction structure coherence summarization translation creation reduction expansion maintenance information content fig learn generative process produce raw modality reflect interaction structure coherence generation involves 1 summarizing multimodal data highlight salient part 2 translating one modality another consistent modality connection 3 creating multiple modality simultaneously maintaining coherence approach aim filter word phrase unimodal element input create summary 52 116 139 beyond text output video summarization task producing compact version video visual summary encapsulating informative part 218 li et al 139 collected dataset news video article paired manually annotated summary benchmark towards multimodal summarization finally uzzaman et al 270 aim simplify complex sentence extracting multimodal summary accessibility hand abstractive approach define generative model generate summary multiple level granularity 51 143 although approach only focus generating textual summary multimodal data 193 several direction also explored generating summarized image supplement generated textual summary 51 141 subchallenge translation translation aim map one modality another respecting semantic connection information content 279 example generating descriptive caption image help improve accessibility visual content blind people 88 multimodal translation brings new difficulty involving generation structured data well evaluation recent approach classified limited retrieving training instance translate modality guarantee fidelity 72 generative model translate arbitrary instance interpolating beyond data face challenge quality diversity evaluation 128 210 266 despite challenge recent progress translation model ha yielded impressive quality generated content 210 215 234 115 213 gesture 6 187 language pose 7 speech music generation 191 subchallenge creation creation aim generate novel data could span text image audio video modality small initial example latent conditional variable conditional decoding process extremely challenging since need 1 conditional preserve cally meaningful mapping initial seed series parallel modality 2 synchronized semantically coherent across modality 3 stochastic capture many possible future generation given particular state 4 across possibly long range many modality considered target creation language generation ha explored long time 207 recent work ha explored speech sound generation using neural network 191 photorealistic image generation ha also recently become possible due advance generative modeling 123 furthermore number tempts generating abstract scene 247 computer graphic 177 talking head 321 ha some progress toward video generation 234 complete synchronized generation realistic video text audio remains challenge preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning 13 13 challenge 5 transference definition transfer knowledge modality usually help target modality may noisy limited resource transfer model induction fig transference study transfer knowledge modality usually help noisy limited primary modality via 1 transfer model trained abundant data secondary modality 2 multimodal share information across modality sharing representation 3 model induction keep individual unimodal model separate induces behavior separate model finally one biggest challenge facing multimodal generation difficulty evaluating generated content especially exist serious ethical issue fake news 29 hate speech 2 79 deepfakes 89 video 245 easily generated ideal way evaluate generated content user study costly potentially introduce subjectivity bias evaluation process 81 several automatic proxy metric proposed 14 53 none universally robust across many generation task 7 challenge 5 transference transference aim transfer knowledge modality representation knowledge learned secondary modality predicted label representation help model trained primary modality challenge particularly relevant primary modality ha limited resource lack annotated data noisy input unreliable label call challenge transference since transfer information secondary modality give rise new behavior previously unseen primary modality identify three type transference approach 1 transfer 2 multimodal 3 model induction figure 16 subchallenge transfer setting may easier collect either labeled unlabeled data secondary modality train strong supervised pretrained model model conditioned downstream task involving primary modality word line research extends unimodal transfer setting tuning inspired prior work nlp involving prefix tuning 146 prompt tuning 138 recent work ha also studied tuning pretrained language model condition visual modality example tsimpoukelli et al 266 quickly condition pretrained frozen language model image image captioning related work ha also adapted prefix tuning image captioning 49 multimodal fusion 91 summarization 307 prefix tuning simple efficient provides user only limited control information transferred representation tuning go level deeper modifying inner representation language model via contextualization modality example ziegler et al 325 includes additional layer language model layer external modality rahman et al 208 design shifting gate adapt language model layer audio visual information multitask learning aim use multiple task improve performance compared learning individual task several model perceiver 113 multimodel 121 bert 144 polyvit 153 explored possibility using unimodal encoder architecture different input across unimodal task language image video transformer architecture ha emerged popular choice due suitability serialized input text sequence token 65 image sequence patch 67 video sequence image 243 data sequence timesteps 154 also preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency several attempt build single model work well suite multimodal task including not limited highmmt 150 vatt 9 flava 235 gato 212 transfer learning research ha focused transfer within modality external information 236 296 312 liang et al 152 study transfer new modality using small amount paired unlabeled data lu et al 160 found transformer pretrained language transfer sequential modality well liang et al 150 build single multimodal model capable transferring completely new modality task recently ha also line work investigating transfer pretrained language model planning 106 interactive 145 subchallenge multimodal multimodal aim transfer information learned secondary modality target task involving primary modality sharing intermediate representation space modality approach essentially result single joint model across modality via representation aim learn either joint coordinated representation space using modality input typically involves adding secondary modality training process designing suitable representation space investigating multimodal model transfer primary modality testing example devise learns coordinated similarity space image text improve image classification 75 marino et al 171 use knowledge graph image classification via joint representation space jia et al 118 improve image classifier contrastive representation learning image noisy caption finally zadeh et al 312 showed implicit also possible without explicit objective via generation instead learns translation model primary secondary modality resulting enriched representation primary modality predict label hallucinate secondary modality containing shared information classic example category includes language modeling mapping contextualized text embeddings image 249 image classification projecting image embeddings word embeddings 236 language sentiment analysis translating language video audio 202 subchallenge model induction contrast model induction approach keep individual unimodal model across primary secondary modality separate aim induce behavior model model induction exemplified two learning algorithm trained separately view data using algorithm prediction new unlabeled example enlarge training set view 33 therefore information transferred across multiple view model prediction instead shared representation space multimodal extends jointly learning classifier multiple tie 96 guillaumin et al 87 study learning using classifier image text unlabeled image training final classifier labeled unlabeled image cheng et al 56 performs multimodal learning using preserving algorithm finally dunnmon et al 68 applies idea data programming problem weak supervision weak label derived secondary modality text used train model primary modality image another set model employ regularizer penalizes function ther modality disagree class model called useful technique control model complexity preferring hypothesis class containing model dict similarly across two view 233 sridharan kakade 239 provide guarantee preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning 16 16 challenge 6 quantification definition empirical theoretical study better understand heterogeneity interaction multimodal learning process heterogeneity interconnection learning epoch loss fig quantification empirical theoretical study design better understand 1 dimension heterogeneity 2 presence type interconnection 3 learning optimization challenge approach using framework recently similar proaches also applied multimodal feature selection 99 multimodal learning 302 video summarization 179 8 challenge 6 quantification quantification aim provide deeper empirical theoretical study multimodal model gain insight improve robustness interpretability reliability application break quantification 3 1 quantifying dimension heterogeneity subsequently influence modeling learning 2 quantifying presence type connection interaction multimodal datasets trained model 3 characterizing learning optimization challenge involved learning heterogeneous data figure 17 subchallenge dimension heterogeneity subchallenge aim understand dimension heterogeneity commonly encountered multimodal research subsequently influence modeling learning figure 18 modality information understanding information entire modality stituents important determining segment modality contributed subsequent modeling recent work categorized 1 interpretable method explicitly model modality used 195 263 313 2 explanation model 45 84 former method concept bottleneck model 129 fitting sparse linear layer 291 decision tree 280 top deep feature representation emerged promising choice latter approach visualization 84 225 232 feature attribution modality contribution 78 lime 214 shapley value 176 used highlight region modality used model modality bias unintended correlation input output could introduced data collection 31 36 modeling 80 human annotation 64 modality bias lead unexpectedly poor performance real world 219 even dangerously potential harm towards underrepresented group 92 199 example goyal et al 83 found unimodal bias language modality vqa task resulting mistake due ignoring visual information 5 subsequent work ha developed diagnostic benchmark mitigate data collection bias like vqa 83 gqa 111 242 recent work ha also found compounding social bias multimodal system 57 216 240 stemming gender bias language visual modality 39 229 may cause danger deployed 199 modality noise topology robustness study modality noise topology aim benchmark improve multimodal model perform presence data imperfection modality ha unique noise topology determines distribution noise imperfection commonly encounter example image susceptible blur shift typed text susceptible typo following keyboard position multimodal series data susceptible correlated imperfection across synchronized time step liang et al 151 collect comprehensive set targeted noisy distribution unique modality addition preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency information bias noise generalization optimization tradeoff epoch loss fig subchallenge heterogeneity quantification aim understand dimension ity commonly encountered multimodal research 1 different quantity usage modality information 2 presence modality bias 3 quantifying mitigating modality noise information bias noise generalization optimization tradeoff epoch loss connection interaction signal response inference fig quantifying modality interconnection study 1 connection discover modality element related 2 interaction understand modality element interact inference natural noise topology related work ha also explored adversarial attack 66 distribution shift 73 multimodal system ha also some progress accounting noisy missing modality modality imputation using probabilistic model 163 autoencoders 259 translation model 202 approximation 148 however run risk possible error compounding require knowing modality imperfect beforehand subchallenge modality interconnection modality connection interaction essential component multimodal model ha inspired important line work visualizing understanding nature modality interconnection datasets trained model divide recent work quantification 1 connection modality related share commonality 2 interaction modality element interact inference figure 19 connection recent work ha explored quantification modality connection visualization tool joint representation space 112 attention map 3 analysis perturbs input observes change output understand internal tions 149 189 finally specifically curated diagnostic datasets also useful understanding semantic connection winoground 255 probe vision language model compositionality paintskills 57 measure connection necessary visual reasoning interaction one common categorization interaction involves redundancy uniqueness synergy 290 redundancy describes information shared among feature uniqueness study information present only one feature synergy investigates emergence new information feature present statistical perspective measure redundancy include mutual information 22 33 contrastive learning tor 258 264 approach studied measure isolation redundancy via distance prediction logits using either feature 173 statistical distribution test input feature 21 via human annotation 217 semantic view recent work causal vqa 4 counterfactual vqa 189 seek understand interaction captured trained model measuring robustness controlled semantic edits question image finally recent work ha formalized definition interaction quantify ence trained model 238 265 parallel research emap 94 dime 162 285 multiviz 149 aim quantify interaction multimodal datasets model preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning information bias noise generalization optimization tradeoff epoch loss fig studying multimodal learning process involves understanding 1 generalization across tie task 2 optimization balanced efficient training 3 tradeoff performance robustness complexity deployment multimodal model subchallenge multimodal learning process finally need characterize learning optimization challenge involved learning heterogeneous data section cover recent work 1 generalization across modality task 2 better optimization balanced efficient training 3 balancing tradeoff performance robustness complexity deployment figure 20 generalization advance sensing technology many platform cellphone smart device car healthcare technology robot integrate much larger number sensor beyond prototypical text video audio modality 108 recent work ha studied generalization across paired modality input 152 206 unpaired scenario task defined only small subset modality 150 160 212 optimization challenge related work ha also explored optimization challenge modal learning multimodal network often prone overfitting due increased capacity different modality overfit generalize different rate training jointly single optimization strategy 284 subsequent work ha suggested empirical theoretical study joint training multimodal network may difficult ha proposed method improve optimization process via weighting approach 293 modality tradeoff deployment balance performance robustness complexity often required therefore one often need balance utility additional modality additional complexity data collection modeling 151 well increased susceptibility noise imperfection additional modality 202 formally quantify utility risk input modality balancing tradeoff reliable world usage several attempt toward formalizing semantics multimodal representation benefit transfer downstream task 147 253 264 argument also provided useful insight 33 239 9 conclusion paper defined three core principle modality heterogeneity connection interaction central multimodal machine learning research proposing taxonomy six core technical challenge representation alignment reasoning generation transference quantification covering historical recent direction despite immense opportunity afforded recent progress multimodal machine learning remain many unsolved challenge theoretical computational application perspective future direction representation theoretical empirical framework formally define three core principle heterogeneity connection interaction mathematical empirical framework enable u taxonomize dimension heterogeneity interconnection subsequently quantify presence multimodal datasets model answering fundamental question lead better understanding capability limitation current multimodal representation beyond additive multiplicative interaction preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency recent work ha successful modeling multiplicative interaction increasing order capture causal logical temporal connection interaction right type data domain knowledge necessary model interaction brain multimodal perception many core insight regarding multimodal processing gained brain human cognition including brain neural architecture 34 intrinsic multimodal property 130 mental imagery 183 nature neural signal 194 doe human brain represent different modality multisensory integration performed insight inform multimodal learning direction several challenge opportunity processing brain signal fmri multimodal learning help future analysis data collected neuroscience alignment memory interaction many current multimodal benchmark only short temporal dimension ha limited demand model accurately process sequence learn interaction capturing interaction present challenge since difficult semantically relate information occur far apart time space raise complexity issue design model perhaps memory mechanism ensure interaction captured reasoning multimodal compositionality understand reasoning process trained model especially regard combine information modality element challenge compositional generalization difficult since many composition element typically not present training possible number composition increase exponentially number element 255 best test compositionality reasoning approach enable compositional generalization transference learning aim learn representation especially large number heterogeneous data source common feature many multimodal system car iot 108 modality introduce dimension heterogeneity incur complexity challenge unimodal multimodal processing require dealing data not modality present time generation creation ethical concern complete synchronized creation realistic video text audio remains challenge furthermore recent success modality generation ha brought ethical concern regarding use example pretrained language model potentially generate text denigrating particular social group 229 toxic speech 79 sensitive pretraining data 44 future work study risk potentially amplified reduced dataset multimodal whether ethical issue specific multimodal generation quantification modality utility tradeoff selection formalize modality useful task potential reason modality harmful come formal guideline compare tradeoff subsequently select modality explainability interpretability model safely used stakeholder doctor educator policymakers need understand taxonomy multimodal phenomenon datasets trained model aim interpret evaluate whether phenomenon accurately interpreted challenge exacerbated relatively understudied modality beyond language vision modality not easy visualize finally tailor explanation possibly manner inform also core challenge understanding quantifying modality social bias well robustness imperfect noisy modality conclusion believe taxonomy help catalog future research paper better understand remaining unresolved problem multimodal machine learning preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning acknowledgement material based upon work partially supported national science foundation award 1722822 1750439 national institute health award bmw north america meta ppl partially supported facebook phd fellowship carnegie mellon university center machine learning health fellowship any opinion finding conclusion recommendation expressed material author not necessarily reflect view national science foundation national institute health bmw north america facebook carnegie mellon university center machine learning health no official endorsement inferred extremely grateful alex wilf arav agarwal catherine cheng chaitanya ahuja daniel fried dong lee jack hessel leena mathur lenore blum manuel blum martin peter wu richard chen ruslan salakhutdinov santiago benoit su min park torsten wortwein victoria lin volkan cirik hubert tsai yejin choi yiwei lyu yonatan bisk youssouf kebe helpful discussion feedback initial version paper reference 1 mahdi abavisani vishal patel deep multimodal subspace clustering network ieee journal selected topic signal processing 12 6 2018 2 abubakar abid maheen farooqi james zou persistent bias large language model proceeding 2021 conference ai ethic society 3 estelle aflalo meng du tseng yongfei liu chenfei wu nan duan vasudev lal interactive visualization tool interpreting transformer cvpr 4 vedika agarwal rakshith shetty mario fritz towards causal vqa revealing reducing spurious correlation invariant covariant semantic editing cvpr 5 aishwarya agrawal dhruv batra devi parikh analyzing behavior visual question answering model emnlp 6 chaitanya ahuja dong lee yukiko nakano morency style transfer gesture animation approach eccv springer 7 chaitanya ahuja morency natural language grounded pose forecasting ieee 8 hana ajakan pascal germain hugo larochelle fran√ßois laviolette mario marchand neural network arxiv preprint 2014 9 hassan akbari liangzhe yuan rui qian et al vatt transformer multimodal learning raw video audio text arxiv preprint 2021 10 mehmet aktukmak yasin yilmaz ismail uysal probabilistic framework incorporate type feature matrix factorization multimodal side information neurocomputing 367 2019 11 alayrac jeff donahue pauline luc antoine miech iain barr yana hasson et al flamingo visual language model learning arxiv preprint 2022 12 camila alviar rick dale akeiylah dewitt christopher kello multimodal coordination sound movement music speech discourse process 57 8 2020 13 saeed amizadeh hamid palangi alex polozov yichen huang kazuhito koishida visual reasoning disentangling visual reasoning icml pmlr 14 peter anderson basura fernando mark johnson stephen gould spice semantic propositional image caption evaluation european conference computer vision springer 15 jacob andreas marcus rohrbach trevor darrell dan klein neural module network cvpr 16 galen andrew raman arora jeff bilmes karen livescu deep canonical correlation analysis icml 17 xavier anguera jordi luque ciro gracia alignment speech recognition limited resource fifteenth annual conference international speech communication association 18 john arevalo thamar solorio manuel g√≥mez fabio gonz√°lez gated multimodal unit information fusion arxiv preprint 2017 19 pradeep k atrey anwar hossain abdulmotaleb el saddik mohan kankanhalli multimodal fusion multimedia analysis survey multimedia system 16 6 2010 20 s√∂ren auer christian bizer georgi kobilarov jens lehmann richard cyganiak zachary ives dbpedia nucleus web open data semantic web springer preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency 21 benjamin auffarth maite l√≥pez jes√∫s cerquides comparison redundancy relevance measure feature selection tissue classification ct image industrial conference data mining springer 22 balcan avrim blum ke yang expansion towards bridging theory practice neurips 17 2004 23 tadas baltru≈°aitis chaitanya ahuja morency multimodal machine learning survey taxonomy ieee tpami 41 2 2018 24 george barnum sabera j talukder yisong yue benefit early fusion multimodal representation learning neurips 2020 workshop svrhm 25 reuben baron david kenny variable distinction social psychological research conceptual strategic statistical consideration journal personality social psychology 1986 26 roland barthes macmillan 27 shai john blitzer koby crammer fernando pereira analysis representation domain adaptation neurips 19 2006 28 hedi r√©mi cadene matthieu cord nicolas thome mutan multimodal tucker fusion visual question answering iccv 29 emily bender timnit gebru angelina shmargaret shmitchell danger stochastic parrot language model big faact 30 yoshua bengio aaron courville pascal vincent representation learning review new perspective tpami 35 8 2013 31 abeba birhane vinay uday prabhu emmanuel kahembwe multimodal datasets misogyny pornography malignant stereotype arxiv preprint 2021 32 yonatan bisk ari holtzman jesse thomason jacob andreas yoshua bengio joyce chai mirella lapata angeliki lazaridou jonathan may aleksandr nisnevich et al experience ground language emnlp 33 avrim blum tom mitchell combining labeled unlabeled data colt 34 lenore blum manuel blum theory consciousness theoretical computer science perspective insight conscious turing machine proceeding national academy science 2022 35 kurt bollacker colin evans praveen paritosh tim sturge jamie taylor freebase collaboratively created graph database structuring human knowledge acm sigmod 36 tolga bolukbasi chang james zou venkatesh saligrama adam kalai man computer programmer woman homemaker debiasing word embeddings neurips 37 simon brodeur ethan perez ankesh anand florian golemo luca celotti et al home household multimodal environment nip 2017 interaction language workshop 38 michael bronstein joan bruna taco cohen petar veliƒçkoviƒá geometric deep learning grid group graph geodesic gauge arxiv preprint 2021 39 joy buolamwini timnit gebru gender shade intersectional accuracy disparity commercial gender classification conference fairness accountability transparency pmlr 40 qiong cai hao wang zhenmin li xiao liu survey multimodal smart healthcare system approach application ieee access 7 2019 41 juan c caicedo fabio gonz√°lez online matrix factorization multimodal image retrieval ican congress pattern recognition springer 42 jize cao zhe gan yu cheng licheng yu chen jingjing liu behind scene revealing secret model european conference computer vision springer 43 zhangjie cao mingsheng long jianmin wang qiang yang transitive hashing network heterogeneous multimedia retrieval proceeding aaai conference artificial intelligence 44 nicholas carlini florian tramer eric wallace matthew jagielski ariel et al extracting training data large language model usenix security 45 arjun chandrasekaran viraj prabhu deshraj yadav prithvijit chattopadhyay devi parikh explanation make vqa model predictable human emnlp 46 khyathi raghavi chandu yonatan bisk alan w black grounding grounding nlp finding association computational linguistics 2021 47 devendra singh chaplot kanthashree mysore sathyendra rama kumar pasumarthi dheeraj rajagopal ruslan salakhutdinov architecture language grounding aaai 48 brian chen andrew rouditchenko kevin duarte hilde kuehne samuel thomas angie boggust et al multimodal clustering network learning unlabeled video iccv 49 jun chen han guo kai yi boyang li mohamed elhoseiny visualgpt adaptation pretrained language model image captioning arxiv preprint 2021 preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning 50 jiaqi chen jianheng tang jinghui qin xiaodan liang lingbo liu eric xing liang lin geoqa geometric question answering benchmark towards multimodal numerical reasoning finding 51 jingqiang chen hai zhuge abstractive summarization using attentional hierarchical rnn emnlp 52 jingqiang chen hai zhuge extractive summarization using rnn 2018 international conference semantics knowledge grid skg ieee 53 lele chen guofeng cui ziyi kou haitian zheng chenliang xu comprises good video generation survey benchmark arxiv preprint 2020 54 liqun chen zhe gan yu cheng linjie li lawrence carin jingjing liu graph optimal transport alignment icml pmlr 55 minghai chen sen wang paul pu liang tadas baltru≈°aitis amir zadeh morency modal sentiment analysis fusion reinforcement learning icmi 56 yanhua cheng xin zhao rui cai zhiwei li kaiqi huang yong rui et al multimodal deep learning object recognition ijcai 57 jaemin cho abhay zala mohit bansal probing reasoning skill social bias generative transformer arxiv preprint 2022 58 volkan cirik taylor morency using syntax ground referring expression natural image aaai vol 32 59 volkan cirik taylor morency referring expression recognition dataset 360 image acl 60 volkan cirik morency taylor visual referring expression recognition system actually learn naacl 61 emilie delaherche mohamed chetouani multimodal coordination exploring relevant feature measure proceeding international workshop social signal processing 62 mingkai deng bowen tan zhengzhong liu eric xing zhiting hu compression transduction creation unified framework evaluating natural language generation emnlp 63 emily denton rob fergus stochastic video generation learned prior icml pmlr 64 laurence devillers laurence vidrascu lori lamel challenge emotion annotation machine learning based detection neural network 18 4 2005 65 jacob devlin chang kenton lee kristina toutanova bert deep bidirectional transformer language understanding 1 66 wenhao ding baiming chen bo li kim ji eun ding zhao multimodal scenario generation algorithm evaluation ieee robotics automation letter 6 2 2021 67 alexey dosovitskiy lucas beyer alexander kolesnikov dirk weissenborn xiaohua zhai et al image worth word transformer image recognition scale iclr 2021 68 jared dunnmon alexander j ratner khaled saab nishith khandwala matthew markert hersh sagreiya roger goldman et al data programming enables rapid medical machine learning pattern 2020 69 chris dyer note noise contrastive estimation negative sampling arxiv preprint 2014 70 georgios evangelopoulos athanasia zlatintsi alexandros potamianos et al multimodal saliency fusion movie summarization based aural visual textual attention ieee transaction multimedia 2013 71 haoqi fan jiatong zhou stacked latent attention multimodal reasoning cvpr 72 ali farhadi mohsen hejrati mohammad amin sadeghi peter young cyrus rashtchian julia hockenmaier david forsyth every picture tell story generating sentence image eccv springer 73 andreas foltyn jessica deuschel towards reliable multimodal stress detection distribution shift companion publication 2021 international conference multimodal interaction 74 jerome h friedman bogdan e popescu predictive learning via rule ensemble annals applied statistic 2 3 2008 75 andrea frome greg corrado jon shlens samy bengio jeff dean marc aurelio ranzato tomas mikolov devise deep embedding model advance neural information processing system 76 akira fukui dong huk park daylen yang anna rohrbach trevor darrell marcus rohrbach multimodal compact bilinear pooling visual question answering visual grounding emnlp acl 77 konrad gadzicki razieh khamsehashari christoph zetzsche early v late fusion multimodal tional neural network 2020 ieee international conference information fusion fusion ieee 78 itai gat idan schwartz alex schwing perceptual score data modality doe model perceive neurips 34 2021 79 samuel gehman suchin gururangan maarten sap yejin choi noah smith realtoxicityprompts evaluating neural toxic degeneration language model emnlp finding preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency 80 robert geirhos jacobsen claudio michaelis richard zemel wieland brendel matthias bethge felix wichmann shortcut learning deep neural network nature machine intelligence 2020 81 mor geva yoav goldberg jonathan berant modeling task annotator investigation annotator bias natural language understanding datasets 82 tejas gokhale pratyay banerjee chitta baral yezhou yang visual question answering lens logic european conference computer vision springer 83 yash goyal tejas khot douglas dhruv batra devi parikh making v vqa matter elevating role image understanding visual question answering cvpr 84 yash goyal akrit mohapatra devi parikh dhruv batra towards transparent ai system interpreting visual question answering model arxiv preprint 2016 85 edouard grave armand joulin quentin berthet unsupervised alignment embeddings wasserstein procrustes international conference artificial intelligence statistic pmlr 86 liangke gui borui wang qiuyuan huang alex hauptmann yonatan bisk jianfeng gao kat knowledge augmented transformer arxiv preprint 2021 87 matthieu guillaumin jakob verbeek cordelia schmid multimodal learning image classification 2010 ieee computer society conference computer vision pattern recognition ieee 88 danna gurari qing li abigale j stangl anhong guo chi lin kristen grauman jiebo luo jeffrey p bigham vizwiz grand challenge answering visual question blind people cvpr 89 jeffrey hancock jeremy n bailenson social impact deepfakes page 90 sanjay haresh sateesh kumar huseyin coskun shahram n syed andrey konin zeeshan zia tran learning aligning video time cvpr 91 md kamrul hasan sangwu lee wasifur rahman amir zadeh rada mihalcea morency ehsan hoque humor knowledge enriched transformer understanding multimodal humor aaai 92 lisa anne hendricks kaylee burn kate saenko trevor darrell anna rohrbach woman also snowboard overcoming bias captioning model proceeding european conference computer vision eccv 93 lisa anne hendricks john mellor rosalia schneider alayrac aida nematzadeh decoupling role data attention loss multimodal transformer arxiv preprint 2021 94 jack hessel lillian lee doe multimodal model learn interaction harder tell might think emnlp 95 irina higgins loic matthey arka pal christopher burgess xavier glorot matthew botvinick shakir mohamed alexander lerchner learning basic visual concept constrained variational framework 2016 96 ryota hinami junwei liang shin ichi satoh alexander hauptmann multimodal selecting good example webly labeled video arxiv preprint 2018 97 richang hong daqing liu xiaoyu mo xiangnan hanwang zhang learning compose reason language tree structure visual grounding ieee tpami 2019 98 ming hou jiajia tang jianhai zhang wanzeng kong qibin zhao deep multimodal multilinear fusion polynomial pooling neurips 32 2019 99 hsieh yiwei sun suhang wang vasant honavar adaptive structural unsupervised feature selection 2019 ieee international conference big knowledge icbk ieee 100 harry hsu weng willie boag matthew mcdermott peter szolovits unsupervised multimodal representation learning across medical image report arxiv preprint 2018 101 hsu james glass disentangling partitioning representation learning framework multimodal sensory data arxiv preprint 2018 102 di hu feiping nie xuelong li deep multimodal clustering unsupervised audiovisual learning cvpr 103 peng hu dezhong peng xu wang yong xiang multimodal adversarial network retrieval system 180 2019 104 ronghang hu jacob andreas marcus rohrbach trevor darrell kate saenko learning reason module network visual question answering iccv 105 ronghang hu huazhe xu marcus rohrbach jiashi feng kate saenko trevor darrell natural language object retrieval cvpr 106 wenlong huang pieter abbeel deepak pathak igor mordatch language model planner extracting actionable knowledge embodied agent arxiv preprint 2022 107 xin huang yuxin peng mingkuan yuan common representation learning hybrid transfer network proceeding international joint conference artificial intelligence 108 zhenhua huang xin xu juan ni honghao zhu cheng wang multimodal representation learning recommendation internet thing ieee internet thing journal 6 6 2019 preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning 109 drew hudson christopher manning learning abstraction neural state machine neurips 2019 110 drew hudson christopher manning compositional attention network machine reasoning arxiv preprint 2018 111 drew hudson christopher manning gqa new dataset visual reasoning compositional question answering cvpr 112 masha itkina ivanovic ransalu senanayake mykel kochenderfer marco pavone evidential cation multimodal latent space conditional variational autoencoders arxiv 2020 113 andrew jaegle felix gimeno andrew brock andrew zisserman oriol vinyals joao carreira perceiver general perception iterative attention arxiv preprint 2021 114 alejandro jaimes nicu sebe multimodal interaction survey computer vision image understanding 108 2007 115 amir jamaludin joon son chung andrew zisserman said synthesising talking face audio international journal computer vision 127 11 2019 116 anubhav jangra adam jatowt mohammad hasanuzzaman sriparna saha summary generation using joint integer linear programming european conference information retrieval springer 117 siddhant jayakumar wojciech czarnecki jacob menick jonathan schwarz jack rae simon osindero yee whye teh tim harley razvan pascanu multiplicative interaction find iclr 118 chao jia yinfei yang ye xia chen zarana parekh hieu pham et al scaling visual language representation learning noisy text supervision icml pmlr 119 alistair ew johnson tom j pollard lu shen h lehman mengling feng mohammad ghassemi et al freely accessible critical care database scientific data 3 1 2016 120 justin johnson bharath hariharan laurens van der maaten li c lawrence zitnick ross girshick clevr diagnostic dataset compositional language elementary visual reasoning cvpr 121 lukasz kaiser aidan n gomez noam shazeer ashish vaswani niki parmar llion jones jakob uszkoreit one model learn arxiv preprint 2017 122 andrej karpathy armand joulin li f deep fragment embeddings bidirectional image sentence mapping neurips 27 2014 123 tero karras samuli laine miika aittala janne hellsten jaakko lehtinen timo aila analyzing improving image quality stylegan cvpr 124 vasil khalidov florence forbes radu horaud conjugate mixture model clustering multimodal data neural computation 2011 125 aparajita khan pradipta maji approximate graph laplacians multimodal data clustering ieee tpami 43 3 2019 126 minjae kim david k han hanseok ko joint patch dictionary learning multimodal image fusion information fusion 27 2016 127 elsa kirchner stephen h fairclough frank kirchner embedded multimodal interface robotics application future trend societal implication handbook interface language processing software commercialization emerging 3 128 jing yu koh jason baldridge honglak lee yinfei yang generation grounded user attention wacv 129 pang wei koh thao nguyen yew siang tang stephen mussmann emma pierson kim percy liang concept bottleneck model icml pmlr 130 stephen kosslyn giorgio ganis william l thompson multimodal image brain ological foundation mental motor imagery 2010 131 satwik kottur jos√© mf moura devi parikh dhruv batra marcus rohrbach visual coreference resolution visual dialog using neural module network eccv 132 ranjay krishna yuke zhu oliver groth justin johnson kenji hata joshua kravitz et al visual genome connecting language vision using crowdsourced dense image annotation ijcv 123 1 2017 133 joseph b kruskal overview sequence comparison time warp string edits macromolecule siam review 25 2 1983 134 pei ling lai colin fyfe kernel nonlinear canonical correlation analysis international journal neural system 2000 135 r√©mi lebret pedro pinheiro ronan collobert image captioning icml pmlr 136 michelle lee yuke zhu krishnan srinivasan parth shah silvio savarese et al making sense vision touch learning multimodal representation task icra ieee 137 jie lei licheng yu mohit bansal tamara berg tvqa localized compositional video question answering emnlp preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency 138 brian lester ramus noah constant power scale prompt tuning emnlp 139 haoran li junnan zhu cong jiajun zhang chengqing zong summarization asynchronous collection text image audio video emnlp 140 liunian harold li mark yatskar da yin hsieh chang visualbert simple performant baseline vision language arxiv preprint 2019 141 mingzhe li xiuying chen shen gao zhangming chan dongyan zhao rui yan vmsmo learning generate multimodal summary news article arxiv preprint 2020 142 manling li ruochen xu shuohang wang luowei zhou xudong lin chenguang zhu michael zeng heng ji chang connecting text image event structure cvpr 143 manling li lingyu zhang heng ji richard j radke keep meeting summary topic abstractive meeting summarization acl 144 qing li boqing gong yin cui dan kondratyuk xianzhi du et al towards unified foundation model jointly transformer unpaired image text arxiv preprint 2021 145 shuang li xavier puig yilun du clinton wang ekin akyurek antonio torralba jacob andreas igor mordatch language model interactive arxiv preprint 2022 146 xiang lisa li percy liang optimizing continuous prompt generation 147 paul pu liang brainish formalizing multimodal language intelligence consciousness arxiv preprint 2022 148 paul pu liang zhun liu hubert tsai qibin zhao ruslan salakhutdinov morency learning representation imperfect time series data via tensor rank regularization acl 149 paul pu liang yiwei lyu gunjan chhablani nihal jain zihao deng et al multiviz towards visualizing understanding multimodal model iclr 150 paul pu liang yiwei lyu xiang fan shengtong mo dani yogatama et al highmmt towards modality task generalization representation learning arxiv preprint 2022 151 paul pu liang yiwei lyu xiang fan zetian wu yun cheng jason wu leslie yufan chen et al multibench multiscale benchmark multimodal representation learning neurips datasets benchmark track 152 paul pu liang peter wu liu ziyin morency ruslan salakhutdinov ization learning low resource modality via acm multimedia 153 valerii likhosherstov mostafa dehghani anurag arnab krzysztof marcin choromanski mario lucic yi tay adrian weller polyvit vision transformer image video audio 154 bryan lim sercan √∂ arƒ±k nicolas loeff tomas pfister temporal fusion transformer interpretable time series forecasting international journal forecasting 2021 155 bill yuchen lin xinyue chen jamin chen xiang ren kagnet graph network commonsense reasoning 156 alex liu souyoung jin lai andrew rouditchenko aude oliva james glass discrete representation learning acl 157 ye liu hui li alberto mathias niepert daniel david rosenblum mmkg knowledge graph european semantic web conference springer 158 zhun liu ying shen varun bharadhwaj lakshminarasimhan paul pu liang amirali bagher zadeh philippe morency efficient multimodal fusion factor acl 159 jiasen lu dhruv batra devi parikh stefan lee vilbert pretraining visiolinguistic tations task advance neural information processing system 160 kevin lu aditya grover pieter abbeel igor mordatch pretrained transformer universal computation engine arxiv preprint 2021 161 jelena luketina nantas nardelli gregory farquhar jakob n foerster jacob andreas edward grefenstette shimon whiteson tim rockt√§schel survey reinforcement learning informed natural language ijcai 162 yiwei lyu paul pu liang zihao deng ruslan salakhutdinov morency dime interpretation multimodal model via disentangled local explanation arxiv preprint 2022 163 mengmeng jian ren long zhao sergey tulyakov cathy wu xi peng smil multimodal learning severely missing modality arxiv preprint 2021 164 emiliano macaluso jon driver multisensory spatial interaction window onto functional integration human brain trend neuroscience 28 5 2005 165 soni madhulatha overview clustering method arxiv preprint 2012 166 tegan maharaj nicolas ballas anna rohrbach aaron courville christopher pal dataset exploration model understanding video data cvpr preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning 167 jonathan malmaud jonathan huang vivek rathod nicholas johnston andrew rabinovich kevin murphy cookin interpreting cooking video using text speech vision 168 jiayuan mao chuang gan pushmeet kohli joshua b tenenbaum jiajun wu concept learner interpreting scene word sentence natural supervision iclr 169 junhua mao jonathan huang alexander toshev oana camburu alan l yuille kevin murphy generation comprehension unambiguous object description cvpr 170 matthew marge carol nigel g ward abeer alwan yoav artzi mohit bansal et al spoken language interaction robot recommendation future research computer speech language 2022 171 kenneth marino ruslan salakhutdinov abhinav gupta know using knowledge graph image classification cvpr ieee 172 emily e marsh marilyn domas white taxonomy relationship image text journal documentation 2003 173 alessio mazzetto dylan sam andrew park eli upfal stephen bach aggregation dependent weak supervision source performance guarantee aistats 174 dalila mekhaldi multimodal document alignment towards multimedia archive proceeding multimedia information retrieval workshop sigir amsterdam netherlands 175 luke alexander rush george han training diversity image paragraph captioning proceeding 2018 conference empirical method natural language processing 176 luke merrick ankur taly explanation game explaining machine learning model using shapley value international conference machine learning knowledge extraction springer 177 ben mildenhall pratul p srinivasan matthew tancik jonathan barron ravi ramamoorthi ren ng nerf representing scene neural radiance field view synthesis european conference computer vision springer 178 george miller wordnet lexical database english commun acm 38 11 1995 179 olivier morere hanlin goh antoine veillard vijay chandrasekhar jie lin deep tations video summarization 2015 ieee international conference image processing icip ieee 180 ghulam muhammad fatima alshehri fakhri karray abdulmotaleb el saddik et al comprehensive survey multimodal medical signal fusion smart healthcare system information fusion 76 2021 181 jonathan munro dima damen domain adaptation action recognition cvpr 182 hyeonseob nam ha jeonghee kim dual attention network multimodal reasoning matching cvpr 183 bence nanay multimodal mental imagery cortex 105 2018 184 milind naphade john r smith jelena tesic chang winston hsu lyndon kennedy alexander hauptmann jon curtis concept ontology multimedia ieee multimedia 13 3 2006 185 karthik narasimhan regina barzilay tommi jaakkola grounding language transfer deep ment learning journal artificial intelligence research 63 2018 186 shahla nemati reza rohani mohammad ehsan basiri moloud abdar neil yen vladimir makarenkov hybrid latent space data fusion method multimodal emotion recognition ieee access 7 2019 187 evonne ng hanbyul joo liwen hu hao li trevor darrell angjoo kanazawa shiry ginosar learning listen modeling dyadic facial motion cvpr 188 maximilian nickel kevin murphy volker tresp evgeniy gabrilovich review relational machine learning knowledge graph proc ieee 104 1 2015 189 yulei niu kaihua tang hanwang zhang zhiwu lu hua wen counterfactual vqa look language bias cvpr 190 zeljko obrenovic dusan starcevic modeling multimodal interaction computer 2004 191 aaron oord yazhe li igor babuschkin karen simonyan oriol vinyals et al parallel wavenet fast speech synthesis icml pmlr 192 christian otto matthias springstein avishek anand ralph ewerth characterization classification semantic relation international journal multimedia information retrieval 9 2020 193 shruti palaskar jindrich libovick spandana gella florian metze multimodal abstractive summarization video arxiv preprint 2019 194 simone palazzo concetto spampinato isaak kavasidis daniela giordano joseph schmidt mubarak shah decoding brain representation multimodal learning neural activity visual feature ieee tpami 2020 195 dong huk park lisa anne hendricks zeynep akata anna rohrbach bernt schiele trevor darrell marcus rohrbach multimodal explanation justifying decision pointing evidence cvpr 196 jae sung park chandra bhagavatula roozbeh mottaghi ali farhadi yejin choi visualcomet reasoning dynamic context still image european conference computer vision springer preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency 197 sarah partan peter marler communication go multimodal science 283 5406 1999 198 judea pearl causality cambridge university press 199 alejandro pe√±a ignacio serna aythami morale julian fierrez faircvtest demo understanding bias multimodal learning testbed fair automatic recruitment icmi 200 valentin vielzeuf st√©phane pateux moez baccouche fr√©d√©ric jurie mfa multimodal fusion architecture search cvpr 201 pouya pezeshkpour liyan chen sameer singh embedding multimodal relational data knowledge base completion proceeding 2018 conference empirical method natural language processing 202 hai pham paul pu liang thomas manzini morency barnab√°s p√≥czos found translation learning robust joint representation cyclic translation modality aaai vol 33 203 bryan plummer liwei wang chris cervantes juan c caicedo julia hockenmaier svetlana lazebnik entity collecting correspondence richer model iccv 204 soujanya poria erik cambria rajiv bajpai amir hussain review affective computing unimodal analysis multimodal fusion information fusion 2017 205 shraman pramanick aniket roy vishal patel multimodal learning using optimal transport sarcasm humor detection proceeding winter conference application computer vision 206 alec radford jong wook kim chris hallacy aditya ramesh gabriel goh sandhini agarwal et al learning transferable visual model natural language supervision icml pmlr 207 alec radford jeff wu rewon child david luan dario amodei ilya sutskever language model unsupervised multitask learner 2019 208 wasifur rahman md kamrul hasan sangwu lee amirali bagher zadeh chengfeng mao morency ehsan hoque integrating multimodal information large pretrained transformer acl 209 shyam sundar rajagopalan morency tadas baltru≈°aitis roland goecke extending long memory structured learning european conference computer vision 210 aditya ramesh mikhail pavlov gabriel goh scott gray chelsea voss alec radford mark chen ilya sutskever generation icml pmlr 211 nikhil rasiwasia jose costa pereira emanuele coviello gabriel doyle gert lanckriet roger levy nuno vasconcelos new approach multimedia retrieval acmmm 212 scott reed konrad zolna emilio parisotto sergio g√≥mez colmenarejo alexander novikov gabriel mai gim√©nez yury sulsky et al one model learn deepmind technical report 2022 213 yi ren yangjun ruan xu tan tao qin sheng zhao zhou zhao liu fastspeech fast robust controllable text speech neurips 32 2019 214 marco tulio ribeiro sameer singh carlos guestrin trust explaining prediction any classifier kdd 215 robin rombach andreas blattmann dominik lorenz patrick esser bj√∂rn ommer image synthesis latent diffusion model cvpr 216 candace ross boris katz andrei barbu measuring social bias grounded vision language embeddings arxiv preprint 2020 217 natalie ruiz ronnie taib fang chen examining redundancy multimodal input proceeding australia conference interaction design activity artefact environment 218 shagan sah sourabh kulhare allison gray subhashini venugopalan emily prud hommeaux raymond ptucha semantic text summarization long video wacv ieee 219 keisuke sakaguchi ronan le bra chandra bhagavatula yejin choi winogrande adversarial winograd schema challenge scale aaai vol 34 220 raeid saqur karthik narasimhan multimodal graph network compositional generalization visual question answering neurips 33 2020 221 mehmet emre sargin y√ºcel yemez engin erzin murat tekalp audiovisual synchronization fusion using canonical correlation analysis ieee transaction multimedia 9 7 2007 222 manolis savva abhishek kadian oleksandr maksymets yili zhao erik wijmans bhavana jain julian straub jia liu vladlen koltun jitendra malik et al habitat platform embodied ai research iccv 223 franco scarselli marco gori ah chung tsoi markus hagenbuchner gabriele monfardini graph neural network model ieee transaction neural network 20 1 2008 224 manos schinas symeon papadopoulos georgios petkos yiannis kompatsiaris pericles mitkas modal event detection summarization social medium stream acm multimedia 225 ramprasaath r selvaraju michael cogswell abhishek da ramakrishna vedantam devi parikh dhruv batra visual explanation deep network via localization iccv preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning 226 luciano serafini artur avila garcez logic tensor network deep learning logical reasoning data knowledge arxiv preprint 2016 227 claude elwood shannon mathematical theory communication bell system technical journal 1948 228 rajeev sharma vladimir pavloviƒá thomas huang toward multimodal interface advance image processing understanding festschrift thomas huang world scientific 229 emily sheng chang prem natarajan nanyun peng woman worked babysitter bias language generation 230 chuan shi yitong li jiawei zhang yizhou sun yu philip survey heterogeneous information network analysis ieee transaction knowledge data engineering 29 1 2016 231 yuge shi brook paige philip torr variational autoencoders multimodal deep generative model neurips 2019 232 karen simonyan andrea vedaldi andrew zisserman deep inside convolutional network visualising image classification model saliency map arxiv preprint 2013 233 vikas sindhwani partha niyogi mikhail belkin approach learning multiple view proceeding icml workshop learning multiple view vol citeseer 234 uriel singer adam polyak thomas hayes xi yin jie songyang zhang qiyuan hu et al generation without data arxiv preprint 2022 235 amanpreet singh ronghang hu vedanuj goswami guillaume couairon et al flava foundational language vision alignment model arxiv preprint 2021 236 richard socher milind ganjoo christopher manning andrew ng learning modal transfer neurips 2013 237 dandan song siyi zhanchen sun sicheng yang lejian liao knowledge enhanced bert visual commonsense reasoning system 230 2021 107408 238 daria sorokina rich caruana mirek riedewald daniel fink detecting statistical interaction additive grove tree proceeding international conference machine learning 239 karthik sridharan sham kakade information theoretic framework learning 2008 240 tejas srinivasan yonatan bisk worst world bias compound model arxiv preprint 2021 241 fabian suchanek gjergji kasneci gerhard weikum yago core semantic knowledge www 242 alane suhr yoav artzi visual bias analysis arxiv preprint 2019 243 chen sun austin myers carl vondrick kevin murphy cordelia schmid videobert joint model video language representation learning iccv 244 richard sutton andrew g barto reinforcement learning introduction mit press 245 supasorn suwajanakorn steven seitz ira synthesizing obama learning lip sync audio acm transaction graphic tog 36 4 2017 246 riko suzuki hitomi yanaka masashi yoshikawa koji mineshima daisuke bekki multimodal logical inference system entailment arxiv preprint 2019 247 fuwen tan song feng vicente ordonez generating compositional scene textual description cvpr 248 hao tan mohit bansal lxmert learning encoder representation transformer 249 hao tan mohit bansal vokenization improving language understanding contextualized grounded supervision emnlp 250 zhulin tao yinwei wei xiang wang xiangnan xianglin huang chua mgat multimodal graph attention network recommendation information processing management 57 5 2020 102277 251 makarand tapaswi martin bauml rainer stiefelhagen aligning video scene book chapter cvpr 252 makarand tapaswi yukun zhu rainer stiefelhagen antonio torralba raquel urtasun sanja fidler movieqa understanding story movie cvpr 253 jesse thomason jivko sinapov maxwell svetlik peter stone raymond j mooney learning grounded linguistic semantics playing spy ijcai 254 bruce thompson canonical correlation analysis 2000 255 tristan thrush ryan jiang max bartolo amanpreet singh adina williams douwe kiela candace ross winoground probing vision language model compositionality cvpr 256 yonglong tian dilip krishnan phillip isola contrastive multiview coding eccv springer 257 yonglong tian chen sun ben poole dilip krishnan cordelia schmid phillip isola make good view contrastive learning neurips 33 2020 preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency 258 christopher tosh akshay krishnamurthy daniel hsu contrastive learning redundancy linear model alt 259 luan tran xiaoming liu jiayu zhou rong jin missing modality imputation via cascaded residual autoencoder cvpr 260 george trigeorgis mihalis nicolaou bj√∂rn w schuller stefanos zafeiriou deep canonical time warping simultaneous alignment representation learning sequence ieee tpami 40 5 2017 261 hubert tsai shaojie bai paul pu liang j zico kolter morency ruslan salakhutdinov multimodal transformer unaligned multimodal language sequence acl 262 hubert tsai paul pu liang amir zadeh morency ruslan salakhutdinov learning factorized multimodal representation iclr 2019 263 hubert tsai martin muqiao yang ruslan salakhutdinov morency multimodal routing improving local global interpretability multimodal language analysis emnlp 264 hubert tsai yue wu ruslan salakhutdinov morency learning perspective iclr 265 michael tsang dehua cheng yan liu detecting statistical interaction neural network weight iclr 266 maria tsimpoukelli jacob menick serkan cabi sm eslami oriol vinyals felix hill multimodal learning frozen language model neurips 34 2021 267 peter turney michael l littman learning analogy semantic relation machine learning 60 2005 268 len unsworth chris cl√©irigh multimodality reading construction meaning interaction routledge 269 shagun uppal sarthak bhagat devamanyu hazarika navonil majumder et al multimodal research vision language review current emerging trend information fusion 77 2022 270 naushad uzzaman jeffrey p bigham james f allen multimodal summarization complex sentence proceeding international conference intelligent user interface acm 271 aaron van den oord oriol vinyals et al neural discrete representation learning neurips 30 2017 272 emile van krieken erman acar frank van harmelen analyzing differentiable fuzzy logic operator artificial intelligence 2022 273 ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ≈Çukasz kaiser illia polosukhin attention need advance neural information processing system 274 ramakrishna vedantam karan desai stefan lee marcus rohrbach dhruv batra devi parikh probabilistic neural symbolic model interpretable visual question answering icml pmlr 275 petar veliƒçkoviƒá guillem cucurull arantxa casanova adriana romero pietro li√≤ yoshua bengio graph attention network iclr 276 ivan vendrov ryan kiros sanja fidler raquel urtasun image language arxiv preprint 2015 277 ren√© vidal subspace clustering ieee signal processing magazine 28 2 2011 278 c√©dric villani optimal transport old new vol springer 279 oriol vinyals alexander toshev samy bengio dumitru erhan show tell lesson learned 2015 mscoco image captioning challenge ieee tpami 2016 280 alvin wan lisa dunlap daniel ho jihan yin scott lee suzanne petryk sarah adel bargal joseph e gonzalez nbdt decision tree iclr 281 meng wang hao li dacheng tao ke lu xindong wu multimodal reranking web image search ieee transaction image processing 21 11 2012 282 tan wang jianqiang huang hanwang zhang qianru sun visual commonsense representation learning via causal inference proceeding conference computer vision pattern recognition workshop 283 weiran wang raman arora karen livescu jeff bilmes deep representation learning icml pmlr 284 weiyao wang du tran matt feiszli make training classification network hard cvpr 285 xingbo wang jianben zhihua jin et al visualizing explaining multimodal model sentiment analysis ieee transaction visualization computer graphic 2021 286 xin wang qiuyuan huang asli celikyilmaz jianfeng gao et al reinforced matching imitation learning navigation cvpr 287 j√¥natas wehrmann anderson mattjie rodrigo c barros order embeddings convolution multimodal alignment pattern recognition letter 102 2018 preprint vol 1 no 1 article publication date october foundation trend multimodal machine learning 288 xiaofan wei huibin li jian sun liming chen unsupervised domain adaptation regularized optimal transport multimodal facial expression recognition fg ieee 289 alex wilf qianli paul pu liang amir zadeh morency contrastive learning social intelligence arxiv preprint 2022 290 paul l williams randall beer nonnegative decomposition multivariate information arxiv preprint 2010 291 eric wong shibani santurkar aleksander madry leveraging sparse linear layer debuggable deep network icml 292 mike wu noah goodman multimodal generative model scalable learning neurips 31 2018 293 nan wu stanis≈Çaw jastrzƒôbski kyunghyun cho krzysztof j geras characterizing overcoming greedy nature learning deep neural network arxiv preprint 2022 294 qi wu peng wang chunhua shen anthony dick anton van den hengel ask anything visual question answering based knowledge external source cvpr 295 yi xiao felipe codevilla akhil gurram onay urfalioglu antonio l√≥pez multimodal autonomous driving ieee transaction intelligent transportation system 2020 296 chen xing negar rostamzadeh boris oreshkin pedro pinheiro adaptive learning neurips 297 caiming xiong stephen merity richard socher dynamic memory network visual textual question answering icml 298 chang xu dacheng tao chao xu intact space learning ieee tpami 2015 299 kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan salakhudinov rich zemel yoshua bengio show attend tell neural image caption generation visual attention icml 300 zhen xu david r andrew dai mufasa multimodal fusion architecture search electronic health record arxiv preprint 2021 301 jianing yang yongxin wang ruitao yi yuying zhu azaan rehman amir zadeh et al mtag attention graph unaligned human multimodal language sequence 302 yang yang wang zhan hui xiong yuan jiang comprehensive learning ijcai 303 ting yao yingwei pan yehao li tao mei exploring visual relationship image captioning eccv 304 kexin yi chuang gan yunzhu li pushmeet kohli jiajun wu antonio torralba joshua b tenenbaum clevrer collision event video representation reasoning arxiv preprint 2019 305 yongjing yin fandong meng jinsong su chulun zhou zhengyuan yang jie zhou jiebo luo novel fusion encoder neural machine translation acl 306 peter young alice lai julia hockenmaier image description visual denotation new similarity metric semantic inference event description tacl 2 2014 307 tiezheng yu wenliang dai zihan liu pascale fung vision guided generative language model multimodal abstractive summarization emnlp 308 weijiang yu jingwen zhou weihao yu xiaodan liang nong xiao heterogeneous graph learning visual commonsense reasoning neurips 309 jiahong yuan mark liberman et al speaker identification scotus corpus journal acoustical society america 2008 310 amir zadeh minghai chen soujanya poria erik cambria morency tensor fusion network multimodal sentiment analysis arxiv preprint 2017 311 amir zadeh paul pu liang navonil mazumder soujanya poria erik cambria morency memory fusion network sequential learning aaai vol 32 312 amir zadeh paul pu liang morency foundation multimodal information fusion 64 2020 313 amirali bagher zadeh paul pu liang soujanya poria erik cambria morency multimodal language analysis wild dataset interpretable dynamic fusion graph acl 314 amir r zamir alexander sax william shen leonidas j guibas jitendra malik silvio savarese taskonomy disentangling task transfer learning cvpr 315 rowan zellers yonatan bisk ali farhadi yejin choi recognition cognition visual commonsense reasoning ieee conference computer vision pattern recognition cvpr 316 rowan zellers ximing lu jack hessel youngjae yu jae sung park jize cao ali farhadi yejin choi merlot multimodal neural script knowledge model neurips 34 2021 preprint vol 1 no 1 article publication date october 2022 paul pu liang amir zadeh morency 317 andy zeng adrian wong stefan welker krzysztof choromanski federico tombari et al socratic model composing multimodal reasoning language arxiv preprint 2022 318 zeng chen chuang liao juan carlos niebles min sun leveraging video description learn video question answering aaai 319 hao zhang zhiting hu yuntian deng mrinmaya sachan zhicheng yan eric xing learning concept taxonomy data acl 320 weifeng zhang jing yu hua hu haiyang hu zengchang qin multimodal feature fusion relational reasoning attention visual question answering information fusion 55 2020 321 hao zhu huaibo huang yi li aihua zheng ran arbitrary talking face generation via attentional coherence learning ijcai 322 xiangru zhu zhixu li xiaodan wang xueyao jiang penglei sun xuwu wang et al knowledge graph construction application survey arxiv preprint 2022 323 yukun zhu ryan kiros rich zemel ruslan salakhutdinov raquel urtasun antonio torralba sanja fidler aligning book movie towards visual explanation watching movie reading book iccv 324 yuke zhu ce zhang christopher r√© li building multimodal knowledge base system answering visual query arxiv preprint 2015 325 zachary ziegler luke sebastian gehrmann alexander rush adaptation conditional language generation arxiv preprint 2019 preprint vol 1 no 1 article publication date october 2022