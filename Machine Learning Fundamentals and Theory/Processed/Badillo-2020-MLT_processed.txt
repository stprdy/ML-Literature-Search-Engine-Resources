clinical pharmacology therapeutic volume 107 number 4 april 2020 871 introduction machine learning solveig balazs fabian iakov lucy tony juliane bernhard jitao david last year machine learning ml artificial intelligence seen new wave publicity fueled huge amount data computational power well discovery improved learning algorithm however idea computer learning some abstract concept data applying yet unseen situation not new ha around least since many basic principle familiar pharmacometrics clinical pharmacology community paper want introduce foundational idea ml community reader obtain essential tool need understand publication topic although not go detail theoretical background aim point reader relevant literature put application ml molecular biology well field pharmacometrics clinical pharmacology perspective advent data availability growth computational power combined arrival novel learning method ha led number breakthrough many scientific area includes biological clinical research application range molecular image data cal however idea computer learning some stract human around least since first neural veloped even method like bayesian statistic markov chain used similar idea mind many method known pharmacometrics cal pharmacology community different naming convention left indicate machine learning terminology right usual statistic naming based tibshirani network graph model weight parameter learning fitting generalization test set performance supervised learning regression classification unsupervised learning density estimation clustering feature covariates explanatory variable main difference traditional approach lie much two distinct culture statistical modeling ha eluded nearly 2 decade ago extend definition incorporating physiological model one culture particular culture 1 involves specifying model scribe observed data culture 2 aim solve problem taking algorithmic modeling approach thus inherently leading model higher number free parameter complex interaction complexity pose challenge tion model called black box problem approach typically used pharmacometric application fall culture 1 underlying model assumed based pharmacological principle understanding drug property model usually physiologically interpretable machine learning ml approach fall culture 2 no explicit model specified computer responsible identifying association observed data model tend difficult interpret ologically however significant progress wa made year interpretability ml today many aspect black box model interpreted using proper paper aim support reader develop intuition needed understand computer learn help human identify pattern data foundational idea ml lighted not describe detail theoretical ground available ml method point interested reader article book element statistical learning 9 referred esl refer example plication molecular biology drug discovery drug development clinical pharmacology first introduce concept data point feature feature space similarity measure dive deeper two main domain machine learning namely unsupervised pervised learning touching key aspect example case unsupervised learning computer tasked identify yet known pattern data without knowledge like group class whereas case supervised learning computer tasked learn predict class value yet served data point based concept often also called model ha derived training dataset figure 1 show onomy different method described paper received october 8 2019 accepted january 15 science roche pharma research early development pred roche innovation center basel basel switzerland correspondence solveig badillo wa employed soladis group time manuscript wa written author alphabetical order author contributed equally correction added march 2020 first online publication author contribution text wa added tutorial volume 107 number 4 april 2020 872 used reference albeit nonexhaustive scenario able apply ml tool please note unsupervised method also applicable case label available data feature ml deal data datasets dataset composed multiple data point sometimes also called sample data point represents entity want analyze therefore data point represent anything like patient sample taken cancer tissue many issue related data sal affect not only ml approach any quantitative pline including pharmacometrics compile dataset one ha measured collected number feature data describe property data figure 1 taxonomy overview main machine learning ml algorithm taxonomy different method presented b overview ml method spectrum available method range simpler interpretable advanced algorithm potentially higher performance expense le interpretability position method figure qualitative practice depends number free parameter model complexity data type exact definition interpretability principal component analysis svm support vector machine tsne stochastic neighbor embedding umap uniform manifold approximation projection guide machine learning unsupervised learning supervised learning unlabelled data b goal explore labelled data goal predict clustering dimensionality regression find subgroup reduced dimensionality numerical label categorical label hierarchical based pca tsne umap k nearest neighbor naive bayes decision tree svm regression neural network regression neural network regression svm l b e r p r e n linear regression decision tree neighbor random forest kernel method deep neural network performance body size kid age yes no kid adult yes no feature classifier output label input neuron hidden neuron output neuron support vector support vector op mal hyperplane margin tutorial 15326535 2020 4 downloaded test wiley online library see term condition wiley online library rule use oa article governed applicable creative common license clinical pharmacology therapeutic volume 107 number 4 april 2020 873 point feature categorical predefined value no particular order like male female ordinal predefined value intrinsic order like disease stage numerical real value patient clinical setting could combination patient demographic disease history result blood test complex high dimensional measure like gene expression profile lar tissue single nucleotide polymorphism represent patient unique genome feature represents one dimension feature space concrete value feature particular data point place point defined place dimension space taken together value feature data point called feature vector feature collected set higher dimensionality resulting feature vector feature space obviously dimensionality increase visualization dimension feature space becomes tractable rely computer identify relevant pattern apply dimensionality reduction method explained later section dimensionality clinical pharmacologist usually familiar longitudinal data pharmacokinetic pk pharmacodynamic pd profile play central role fact model used pharmacometrics based equation justified based physiology pharmacology yield insight system similar example physical problem weather forecast air flow temperature lead certain temporal behavior system ml including time distinguished continuous variable respective algorithm remains challenging area active research several option exist clude data ml datasets either directly time point represents feature via transformation fourier transform resulting coefficient basic function considered feature alternatively recurrent neural network rnns used handle longitudinal data outlined section recurrent neural however approach limitation ml algorithm designed handle datasets hence derived feature existing data often included data product ratio tures advanced combination data transformation important preprocessing step profound effect model performance therefore always good idea use available domain knowledge expertise come relevant feature process sometimes referred feature engineering data quality play crucial role ml carefully chosen ml method visual inspection defend extreme value outlier missing data however challenging not method support data missingness data transformation could required preprocessing step case various way impute missing data performance depends dataset method ial approach imputation replace missing value feature mean across sample defined however sometimes cause also see section performance measure issue overfitting also essential scrutinize any bias data tion bias preferably sample ml unbiased random subset population practice rarely case some bias data bias fect ability model generalize beyond training dataset even test dataset share similar bias example generalization problem model supposed learn distinguish wolf husky animal characteristic eventually turn simply tify patch snow various approach mitigate bias one could completely exclude biased sample feature lar propensity score useful estimating effect therapeutic inspection feature importance provides valuable information magnitude fect recommend used checking trustworthiness ml model many clinical classification datasets unbalanced meaning one class underrepresented could pose difficulty many ml algorithm including artificial neural network gradient boosting method one way mitigate problem class respectively tweaking misclassification cost objective finally many application important define larity distance measure two data point feature space simplest distance measure would euclidean distance numerical feature vector two data point b feature n depending type data dealing many sometimes much complex distance similarity measure cosine similarity score two biological main takeaway transforming input data feature engineering may improve model missing data requires imputation bias data scrutinized unbalanced datasets require amendment model meaningful measure similarity sample defined unsupervised learning exploratory data analysis often not know true label might want examine naturally emerging pattern data purpose use unsupervised learning method like clustering frequent pattern detection dimensionality reduction focus particularly 1 b n ai 2 tutorial 15326535 2020 4 downloaded test wiley online library see term condition wiley online library rule use oa article governed applicable creative common license volume 107 number 4 april 2020 874 clustering dimensionality reduction many application molecular biology clinical practice clustering goal applying clustering method identify relevant group given dataset without predefined hypothesis property subgroup might example cohort patient particular disease might want identify subtypes represent distinct biological mechanism driving disease based molecular measure cluster subset data similar whereas point belonging different cluster multiple approach clustering use different derlying algorithm group data point advantage disadvantage needed selected carefully depending application property data one simple approach clustering number cluster identified predefined lected parameter cluster represented cluster center artificial data point represents mean dian value point assigned cluster beginning k cluster center known seed randomly placed feature space algorithm iterates two step step one assignment data point assigned cluster represented closest center step two center shift position cluster center updated based composition ters step one number iteration usually converge local optimum cluster assignment not only marginally change result process visualized figure although procedure intuitive major drawback usually clustering strongly influenced value k often not true number cluster data unknown priori rarely clear cut right wrong answer clustering cluster investigation required identify meaningful cluster challenging particularly light feature space another group method clustering method cluster represents part ture space data point dense data point belonging region feature space low density considered noise one clustering algorithm based spatial clustering application clustering doe not require predefined value setting number cluster provides reproducible result able also identify complex cluster shape like one shown figure hierarchical clustering analysis goal build hierarchy cluster esl chapter 14 one simple approach hierarchical clustering neighbor joining first pairwise distance data point dataset computed later every step iterative process two data point smallest distance grouped together result cluster structure played figure left side top heatmap branch length tree represent distance sample arrive discrete set cluster like distance old ha chosen tree cut horizontally no optimal way selecting threshold many reasonable solution may exist hierarchical clustering used alone used combination heatmaps figure visualize selected feature instance gene expression data dimensionality reduction number feature therefore dimensionality feature space high ten thousand measure per sample not only doe make data visualization challenging also analysis challenging particular analysis mensional datasets associated phenomenon known curse dimensionality 21 refers data sparsity counterintuitive geometrical property figure 2 overview result different clustering approach show result hierarchical clustering two dendrograms visualize similarity across sample also across marker measured visualization frequently used biology gene expression technology readout b show outcome classical clustering using selected value k resulting cluster usually convex every point assigned one cluster namely one represented closest center point marked x c show result clustering please note approach identify nonconvex cluster form orange cluster 0 2 4 6 8 b c tutorial 15326535 2020 4 downloaded test wiley online library see term condition wiley online library rule use oa article governed applicable creative common license clinical pharmacology therapeutic volume 107 number 4 april 2020 875 space curse dimensionality pose challenge data analysis approach including not limited ml mitigate problem dimensionality reduction method might applied dimensionality reduction aid data visualization transforming data point two mensions keeping majority variability relative tances furthermore dropping uninformative feature could improve model performance convergence time although some method like principal component analysis even veloped long term ml ha others like tributed stochastic neighbor uniform manifold approximation developed recently address complex challenge arising data analysis also powerful neural dimensionality reduction approach called toencoder detail apply dimensionality reduction biomedical data would like refer reader recent example unsupervised ml application clustering widely used analyzing data transcriptomic metabolomic proteomic periments typically hierarchical clustering would used identify main factor affecting readout well cation module high degree coregulation sequencing nonhierarchical clustering used understand cell type present sample clustering also used identify relationship among patient tissue disease even disease drug compound may also clustered based gene expression sensitivity target protein goal guiding drug discovery dimensionality reduction routinely used transcriptomic experiment usually identify outlier tential batch effect sequencing uniform manifold approximation projection stochastic neighbor embedding used data visualization subsequent dimensionality reduction also used visualize chemical cessing step improve performance ml main takeaway clustering used understand structure data grouping similar observation together clustering simple yet powerful tool however number cluster must specified advance method not require prespecified number cluster allows identification complex pattern data hierarchical clustering provides overview relationship multiple level dimensionality reduction used not only data visualization also drop uninformative feature supervised learning supervised learning problem computer fed training data observation corresponding known output ues goal learn general rule also often called model map input output possible predict output new unseen data observed input value not associated output two main category supervised learning fication output value categorical ii regression output value numeric subsequent section context model fitting supervised learning common issue overfitting introduced explain performance evaluated classification regression tool ass quality mapping put output algorithm aspect essential merit adopting ml method often center around prospect obtaining higher performance interpretability understanding different performance metric enables better uation merit proposed model opposed assumption ml solution could always outperform traditional approach dive some existing classification sion method starting shallow end interpretation model still straightforward progressing toward approach performance triumph often expense interpretability figure 1 summarizes available trum method respect performance interpretability section concludes nonexhaustive review tions supervised learning method biology particularly clinical pharmacology performance measure issue overfitting goal learning algorithm learn concept function model describes observed training data able generalize new independent data avoiding underfitting overfitting performance model evaluated method allow model assessment estimating well given model performs general model selection estimation mance different model choose adequate model some method highlighted next section model fitting model parameter estimated based observed data training set derive optimal parameter value coefficient weight distance measure model data defined minimized numerically independently metric chosen goal model fitting always estimate parameter minimizing distance also called loss function cost function two requirement model provide predicted value close served one training set otherwise say fit ha high bias model generalize beyond training set model overfits predicts well training set poorly independent test set often complex data case also talk high variance following call objective function any function optimized estimate model parameter tutorial 15326535 2020 4 downloaded test wiley online library see term condition wiley online library rule use oa article governed applicable creative common license volume 107 number 4 april 2020 876 regression case figure 3 illustrates issue underfitting overfitting context regression underfitting occur model simple feature extracted data not informative enough figure 3 left panel overfitting often occurs model complex many feature small set training example figure 3 right panel issue also often referred come expression pected prediction error including bias variance term bias indication average error model different training set discrepancy average predicted ues true mean trying predict variance reflects sensitivity model training set given point corresponds spread predicted value around mean minimize predicted error minimizing bias variance increasing model complexity crease bias increase variance build le complex model different technique exist summarized term tion principle consists modifying objective function adding penalization term influence parameter estimation regularization common one esl section different category loss function different objective function chosen measure distance observed data value predicted model some distance metric used practice associated likelihood likelihood indicates probable observe data according selected model common use likelihood find parameter make model fit optimally data maximum likelihood parameter estimate usually negative logarithm likelihood minimized considered objective function ha favorable numerical property similarly ml metric mean squared error logistic objective used find optimal parameter ass fitness model practice analytical calculation maximum likelihood minimal loss may not feasible often necessary use numerical optimization algorithm solve best parameter value gradient descent algorithm first define objective function want minimize iteratively update value parameter direction steepest decrease derivative objective tion convergence minimum distance deemed reached scenario nonconvex objective function success finding global minimum opposed landing some local minimum depend choice initial set parameter value learning rate step size iteration terion convergence reader refer ref 35 detail convex nonconvex optimization process stochastic gradient descent additional trick speed zation randomly sampling training dataset summing distance across subset training data point ing objective function general principle model selection assessment problem overfitting show model performance training set not good indicator performance new dataset detail principle model performance evaluation supervised learning setting general principle model selection follows enough data separate three validation test set training set used build different model whereas validation set subsequently used choose algorithm select hyperparameters needed model best performance validation set selected finally test set enables ass generalization error also called test error prediction error test dataset figure 3 illustration issue simple regression case data point shown blue dot model fit red line underfitting occurs linear model left panel good fit polynomial degree 4 center panel overfitting polynomial degree 20 right panel root mean squared error chosen objective function evaluating training error generalization error assessed using g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g x underfit training error generalization error polynomial fit degree 1 g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g x good fit training error generalization error polynomial fit degree 4 g g g g g g g g g g g g g g g g g g g g g g g g g g g g g g x overfit training error generalization error 2000 polynomial fit degree 20 tutorial 15326535 2020 4 downloaded test wiley online library see term condition wiley online library rule use oa article governed applicable creative common license clinical pharmacology therapeutic volume 107 number 4 april 2020 877 wa not used important note generalization error could higher expected original dataset biased see section data feature validating model fully independent test dataset method assessing generalizability model dataset small extract decent validation set example possible use technique select model hyperparameters putting aside subset data testing validation consists dividing training set k subset subset used ing last one ass performance process repeated k time k subset used validation performance score subset averaged set hyperparameters test tion procedure summarized figure choose different learning nested used indicator model complexity goodness fit pharmacometrics model selection usually based quantitative measure summarize well model fit data often penalty overfitting commonly used akaike information criterion bayesian information criterion penalize number model parameter reward goodness fit measured likelihood akaike information criterion formalized number parameter maximum likelihood contrast bayesian information criterion take account number data point model selection approach rarely used ml partly due complexity datasets associated violation distributional assumption instead approach like tion commonly used clustering performance measure model assessment regression model typically use mean squared error type average objective function compare model performance training test set classification problem common performance measure often derived confusion matrix shown figure 5 briefly described precision corresponding ratio correctly predicted itive value total number predicted positive value recall also called true positive rate tpr corresponding ratio correctly predicted positive value total number positive value dataset false positive rate fpr corresponds proportion negative value predicted incorrectly accuracy corresponding number correctly predicted value divided total number predicted value area roc curve auc receiver operating teristic roc curve show tpr recall fpr dence binary classification point roc curve located choosing different threshold classification yi positive negative class top left corner roc curve 2 ln 3 n ln figure 4 illustration general principle supervised learning case limited dataset ass generalization ability supervised learning algorithm data separated training subset used building model test subset used ass generalization error limited dataset 1 subset subset training fold test fold score 1 score 2 score k sk run model built subset performance measure computed test subset model valida run 1 run 2 run k training subset test subset final model built training subset evaluate model test set generaliza error model test average score k run select model best metric tutorial 15326535 2020 4 downloaded test wiley online library see term condition wiley online library rule use oa article governed applicable creative common license volume 107 number 4 april 2020 878 ideal case 100 positive value correctly classified tpr 1 0 positive value incorrectly predicted 0 fpr 0 ideal maximize tpr minimizing fpr larger area roc curve auc better some metric could generalized multiclass problem two different label dataset however metric mentioned tinuous respect model parameter hence parameter optimization may challenging used tive function continuous alternative widely used metric previously mentioned section model fitting tropy esl chapter 9 not only account likely prediction also prediction score prediction confidence neighbor start overview existing learning method method skip learning step completely therefore doe not lead explicit model learned training data discus later also one biggest shortcoming type learning also often referred learning particular example neighbor learning knn approach learning simply consists storing existing labeled data point training data database new yet unclassified example observed algorithm place feature space based feature value data point database compute distance euclidean distance complex one new data point order identify k closest neighbor second step examine known label knns database say chosen k nine observe seven nearest neighbor labeled class x whereas two labeled class case would assign new data point class x majority neighbor class sion simple approach would weight importance neighbor classification distance new data point despite straightforward simple prof effective classification method practice efficient come training storing data database efficient implementation computing knns exist challenge approach obvious one no learning step knns approach doe not identify feature really relevant predict class new case therefore even though ture space only 2 might really relevant classification distance computed taking 20 dimension account thus k nearest data point returned query highly influenced irrelevant feature noise see also dimensionality reduction remove some feature quence resulting classification driven noise rather real underlying pattern data aspect approach suffers challenge also clustering approach see section clustering facing often summarized curse 21 naive bayes second intuitive learning approach would like introduce naive bayes based computing simple tistics given training dataset learning step following straightforward naive application bayesian formula conditional probability order obtain classification due simplicity also often used obtain baseline sification performance involved method improve upon best explained simple example let u assume training dataset patient suffering either harmless cold influenza flu infection measured two feature patient namely fever high low no pain strong low no patient know laboratory test patient influenza infection not want learn data apply diagnose new patient no laboratory test available using naive bayes approach learning step count feature value often cur influenza cold patient group obtain probability high fever condition patient flu result learning step might seen table figure 5 confusion matrix problem confusion matrix indicates successful algorithm wa predicting label binary classification problem label take value 0 called negative 1 called positive evaluating predicted real label every data point test set belongs one four category different measure derived number 1 0 1 true tp false fn true rate tpr 0 false fp true tn speciﬁcity false rate fpr precision false rate accuracy predicted label actual label tutorial 15326535 2020 4 downloaded test wiley online library see term condition wiley online library rule use oa article governed applicable creative common license clinical pharmacology therapeutic volume 107 number 4 april 2020 879 table 1 summarizes probability feature given egory patient show whole patient population probability patient influenza infection whereas probability normal cold generated value therefore completed learning step analyzing dataset naive bayes make naive assumption feature conditionally independent one another reality rarely true advanced bayesian learning method not make assumption however assumption allows straightforward application bayesian theorem detail formula derive classifier would like refer reader ther reading material esl chapter 6 brief probability certain label flu cold new test item computed product single conditional feature probability fever pain observed data point time probability class flu cold class maximal posterior hood selected predicted class test item assuming test person unknown diagnosis influenza cold know person show high fever high level pain would compute likelihood influenza way would compute likelihood cold patient present doctor high fever strong muscular pain headache result ized posterior probability influenza infection probability normal cold therefore tient suffers likely flu cold many aspect naive bayes therefore formalizes human might learn experience decision tree random forest gradient boosting decision tree essential building block many ml gorithms used least 50 idea behind decision tree intuitive best represented visual form figure 1 depending problem cision tree leaf node class probability continuous value case regression early day ml decision tree used solve pharmacological problem dosing toxicology although usage decision tree intuitive question construct tree available data famous approach worth mentioning currently decision tree almost never used ml original form one reason fact decision tree prone overfitting nevertheless decision tree became building block two widely used approach random sion forest gradient boosting framework random decision forest gradient boosting use set ensemble trained decision tree predict come variable crucial difference gradient boosting random decision forest tree created case random forest algorithm construct hundred thousand deep decision tree strong predictor tree likely overfitted however combining output multiple tree solve overtraining problem contrary gradient boosting algorithm xgboost catboost tree shallow decision tree weak tor algorithm iteratively decrease classification error time adding tree today gradient boosting method show great performance publication ml competition even without parameter tuning usually provide excellent performance relatively low computational hand random forest usually le prone quire le parameter make random decision est attractive smaller datasets baseline method benchmarking tree ensemble method used classification task well regression case tree output averaged create smooth output function kernel method support vector machine regression kernel method specifically support vector machine svm classification support vector regression svr continuous output found application computational ogy ability robust noise work datasets found genetics transcriptomics concretely recent example svr wa used delineating cell composition bulk transcriptomics 4 p p flu 5 p p cold table 1 illustration naive bayes example learning step result flu dataset showing probability feature value given patient category feature fever pain class high low no strong low no influenza flu p flu p fever p fever p fever 0 p pain p pain p pain cold p cold p fever p fever p fever p pain p pain p pain tutorial 15326535 2020 4 downloaded test wiley online library see term condition wiley online library rule use oa article governed applicable creative common license volume 107 number 4 april 2020 880 section first offer brief overview key concept highlighting notion kernel transformation objective function lossless region regularization emphasis placed providing reasoning behind versatile method dealing multiple input effect output unknown lated span nonlinear function background similar regression method objective svr postulate function input help estimate observed output likewise svm goal find optimal decision boundary separate class name suggests core concept behind ability objectively choose subset training data called support vector support vector define model usually hyperplane some feature space achieve several notion need introduced loss function allows residual le ε considered lossless thus not part support vector factored estimate function regularization term added objective function aim searching model describe relationship tween input output variable hyperplane kept flat possible slack variable introduced allow training error termed soft margin output found outside sensitive region introducing slack variable tolerance residual term greater ε made kernel function allows u work higher dimension space feature space kernel function applied input space responds dot product feature space similarity measure computed achieved without plicitly map input data input space some feature space some mapping function concept hand capable fitting model some thickness known tube introduced sensitive loss function whereas regularization term control flatness hyperplane some feature space defined kernel function figure 6 illustrates basic concept svm kernel trick choice svr capture nonlinear target function map multivariate input output precisely kernel trick mean kernel applied set input input space equivalent ing dot product similarity measure some feature space achieved without explicitly perform ping input xi mapping function kernel tion calculated input space corresponds dot product some feature space only symmetric positive definite choice radial basis function kernel often made expanded feature space infinite dimension although radial basis function cover wide range 6 k xi xj xi φ xj 7 xi φ xj xi xj rbf 2 figure 6 illustration support vector machine svm principle illustration simple case hyperplane separate two group directly input space b illustration performing nonlinear classification implicitly mapping input feature space data point separated hyperplane input space feature space construc hyperplane implicit mapping kernel trick b op mal hyperplane margin support vector tutorial 15326535 2020 4 downloaded test wiley online library see term condition wiley online library rule use oa article governed applicable creative common license clinical pharmacology therapeutic volume 107 number 4 april 2020 881 possible effect lead harder interpretation eventual model practice selection kernel function based computational efficiency popular kernel include linear polynomial neural network background neural network constitute collection neuron edge drawing origin circuit analysis different weight applied edge connecting neuron neuron activation function applied weighed input signal generate output signal sigmoidal function often used consisting first order lowpass filter unit step function sigmoidal function ha advantage yielding bounded output continuously differentiable needed backward propagation step tune weight parameter model see step defined section recurrent neural neuron subdivided input layer hidden er output layer shown figure hidden layer perform layer abstraction needed go input layer output layer number hidden layer define whether system shallow learning system one hidden layer deep learning many hidden layer herent number hidden layer time required train model reason although core concept embedded neural network not novel one ha found resurgence application due recent advance putational power basic type known feedforward neural network information propagated input layer hidden layer finally output layer current state tem not defined any past state hence represents ryless system following illustrative example neural network described recurrent neural network long memory network gated recurrent network notable neural network scope article recommend reading convolutional neural generative recurrent neural network recurrent neural network class neural network dedicated time series datasets factor inherent sequential relationship observed data one time point another ha found success known field sequential data order time sequence signal play role namely natural language processing time series forecasting closely related field research ha found application predicting outcome electronic health record richness come inherently sequence correlation structure data recommend swift even anticipatory action taken medical rephrasing question solve modeling conundrum pharmacometrics field only starting emerge time paper wa drafted tang et al present one rare attempt use ml rnns characterize pk remifentanil compared result pharmacometrics method although nonstandard pk model used comparison generalizability result challenged tang et al make valuable contribution exemplifying rnns could used pharmacometrics basic form rnn shown figure current state time defined combination previous state system current input similar concept classical dynamic system weight edge determined far back look similar time constant contrary feedforward neural network identical weight shared across individual neuron unit block across earlier discrete time step core rnn consists input sequence defined x output sequence defined hidden system state sequence defined h well chained submodules repeated unit step needed train rnn model follows define network architecture initialize model random weight bias perform forward propagation compute estimated output calculate error output layer figure 7 neural network basic feedforward neural network b unfolding recurrent neural network c extension recurrent neural network gating unit black square represents delay one discrete time step h x h x h x h x f f f f f unfold output layer hidden layer input layer forget gate input gate output gate c b tutorial 15326535 2020 4 downloaded test wiley online library see term condition wiley online library rule use oa article governed applicable creative common license volume 107 number 4 april 2020 882 perform backward propagation update weight using optimization approach repeat step number epoch iteration loss function value deemed minimized extension vanilla rnn developed address problem unstable gradient problem vanishing gradient problem serious counterpart ity caused exploding gradient problem core due multiplication influence ical error introduced backward propagation relation error estimate parameter along layer neural network word vanishing gradient cause information need captured time point away current time thus render model weak capture valuable stored memory longer time lag le common event least one partial derivative violates requirement stability translating state matrix least one eigenvalue 1 lead exploding ent problem known problem traditional dynamic system discrete time remit used address fundamental problem described two extension rnn long memory lstm gated recurrent network gru ha many different variant development rnn research novel method serf address different problem ultimately leading development robust model example circumvent unstable gradient problem gradient clipping forcing gradient threshold ha proposed ref far widely accepted method inclusion gating unit long memory gated recurrent network lstm part larger family gated rnns retain forget information introduction gating unit specifically three gating unit included system shown figure first direct copying clearing state altogether controlled forget gate similar approach also handled input gate decide whether include current input signal part update state amount information retain previous state signal perturbation input signal learned time system need learn time dependency retaining information must also occasionally learn clear information current consequently solving vanishing exploding gradient problem finally output gate introduced although le common gating mechanism decide output signal get fed back system simpler rendition thus faster training implementation found gru grus address problem unstable dients represent new addition family rnn sion core difference lstm gru latter omits output gate us simpler reset update theory however lstm perform better information longer example supervised ml application clinical pharmacology model clinical pharmacology typically established translating physiological pharmacological principle system differential equation using imization algorithm estimate model parameter mechanistically motivated approach ha proven useful many application component drug ment program potentially due success established approach only example applying ml method ical pharmacology problem exist ryu et al trained deep neural network large curated database covering interaction order predict food interaction prescription dietary recommendation new combining datasets multiple study create large database increase potential use ml tackle broad clinical pharmacology question ml ha also used bridge drug discovery clinical development example hammann et al able predict incidence adverse event molecule chemical structure using decision tree similarly lancaster sobie plemented svms predict risk torsades de pointes vitro area personalized safety ml ha used daunhawer et al personalize safety context bilirubinemia author used lasso random forest make prediction clinical datasets furthermore reinforcement learning wa used gaweda et al personalize pharmacological anemia similar approach wa used develop closed loop system glucose control bining mathematical model glucose sensor reinforcement learning chavada et al hennig et al investigated feasibility bayesian feedback dose adjustment area personalized healthcare could greatly benefit using ml model recommend dose adjustment real time recent study control algorithm wa grated existing structural model familiar pharmacometricians resulting control system wa found outperform main takeaway supervised learning method infer model based labeled pair training dataset performance metric used ass classification gression model avoid overfitting training dataset many supervised learning method exist different interpretability performance rnn special form neural network represents namic system discrete time example application supervised learning od computational biology particularly clinical cology beginning emerge discussion tutorial introduced some fundamental method ml likely interest clinical pharmacology tutorial 15326535 2020 4 downloaded test wiley online library see term condition wiley online library rule use oa article governed applicable creative common license clinical pharmacology therapeutic volume 107 number 4 april 2020 883 pharmacometrics community brief introduction plemented range relevant reference provided context mentioning example relevant drug development conclude summarizing field ml clinical pharmacology currently situated providing outlook expect see integration field ture advanced statistical method not new tricians fact method used describe pk pd phenomenon some time example bayesian method component pharmacometric seems therefore likely statistical ml approach come established prominent cal industry pharmacometricians among take advantage method furthermore new opportunity investigate clinical question patient stratification baseline characteristic may become sible clinical pharmacology using ml approach several example ml approach applied clinical pharmacology question include integration sical modeling technique specifying structural model based mechanistic understanding ml classical pharmacometric approach based pharmacological principle reflect hypothesis generated understanding physiology drug property unlikely model completely replaced ml approach near future however datasets problem complex many unknown influence relationship exist focus terpolation fast evaluation pharmacometrics might benefit applying method going forward expect fusing understanding ml model could lead tive model future recent perspective article provides detail application ml clinical age big data many new opportunity ml clinical pharmacology example data generated able device pose new challenge linked pk data future addition access data could vide strong evidence covariates supplement control datasets bolster model trained small datasets pharmacometric approach predictive model typically established integrating structural model relevant data structural model substantially constrains solution space therefore relatively little data required fit model contrary neural network model structure not specified thus comparatively much data required building predictive model also important note still much infancy stage understanding point merger larger data novel ml method beneficial performance compared traditional method following time ries forecasting show combination classic statistical ml method produce accurate forecasting thus suggest way forward one main driver success pharmacometric approach model include thorough understanding process drug absorption tribution metabolism elimination established model highly predictive thus find wide use supporting drug development due success despite arrival ml sical pharmacometrics approach not expected decrease importance activity contrast enhanced improved knowledge insight distilled ml od model ongoing challenge member clinical pharmacology community wish use ml method inherent lence longitudinal data far many ml method rely baseline feature make prediction relatively amples longitudinal data used overall expect never universal approach modeler different field converge note many area potential synergy eling field overlap remit drug development clinical pharmacology community continue base analysis pharmacological principle gradually build new ml ements workflow strengthening model addition clinical pharmacology community able hance range question able address using ml approach funding recipient roche postdoctoral fellowship study funded roche conflict interest author declared no competing interest work 2020 author clinical pharmacology therapeutic published wiley periodical behalf american society clinical pharmacology therapeutic open access article term creative common license permit use distribution any medium provided original work properly cited use no modification adaptation made camacho collins power costello collins machine learning biological network cell 173 2018 shen wu suk deep learning medical image analysis annu rev biomed eng 19 2017 rajkomar dean j kohane machine learning medicine engl med 380 2019 kleene representation event nerve net finite automaton rand project air force santa monica ca 1951 6138 breiman statistical modeling two culture comment rejoinder author stat sci 16 2001 ribeiro singh guestrin trust explaining prediction any classifier proceeding acm sigkdd international conference knowledge discovery data mining san francisco ca august 2016 štrumbelj kononenko explaining prediction model individual prediction feature contribution knowl inf syst 41 2014 lipton mythos model interpretability acm queue 16 2018 hastie tibshirani friedman element statistical learning data mining inference prediction springer new york new york ny 2008 jerez et al missing data imputation using statistical machine learning method real breast cancer problem artif intell med 50 2010 tutorial 15326535 2020 4 downloaded test wiley online library see term condition wiley online library rule use oa article governed applicable creative common license volume 107 number 4 april 2020 884 dorogush ershov gulin catboost gradient boosting categorical feature support arxiv preprint 2018 cortes mohri riley rostamizadeh freund györfi turán zeugmann ed sample selection bias correction theory algorithmic learning theory springer berlin heidelberg 2008 lee lessler j stuart improving propensity score weighting using machine learning stat med 29 2010 newby freitas ghafourian coping unbalanced class data set oral absorption model chem inf model 53 2013 hu huang ke tsai distance function effect neighbor classification medical datasets springerplus 5 1304 2016 borozan watt ferretti integrating sequence similarity measure biological sequence classification bioinformatics 31 2015 collisson bailey chang biankin molecular subtypes pancreatic cancer nat rev gastroenterol hepatol 16 2019 lloyd least square quantization pcm ieee trans inf theory 28 1982 kriegel kröger sander zimek clustering wiley interdisc rev data min knowl discov 1 2011 ester kriegel sander j xu algorithm discovering cluster algorithm discovering cluster large spatial database noise proceeding second international conference knowledge discovery data mining aaai press portland 1996 zimek schubert kriegel survey unsupervised outlier detection numerical data stat anal data min 5 2012 pearson line plane closest fit system point space london edinburgh dublin philosophical magazine journal science 2 1901 van der maaten hinton visualizing data using mach learning 9 2008 becht et al dimensionality reduction visualizing data using umap nat biotechnol 37 2019 nguyen holmes ten quick tip effective dimensionality reduction plo comput biol 15 2019 wang machiraju huang breast cancer patient stratification using molecular regularized consensus clustering method method 67 2014 cooper bynum somers recent insight epidemiology autoimmune disease improved prevalence estimate understanding clustering disease autoimmun 33 2009 walsh rybicki symptom clustering advanced cancer support care cancer 14 2006 expression gtex consortium et al expression gtex pilot analysis multitissue gene regulation human science 348 2015 zhang berntenis roth ebeling data mining reveals network gene consensus signature vitro vivo toxicity pharmacogenomics j 14 2014 gemma et al anticancer drug clustering lung cancer based gene expression profile sensitivity database bmc cancer 6 174 2006 koch waldmann protein structure similarity clustering natural product structure guiding principle drug discovery drug discov today 10 2005 reutlinger schneider nonlinear dimensionality reduction mapping compound library drug discovery mol graph model 34 2012 ezzat wu li kwoh interaction prediction using ensemble learning dimensionality reduction method 129 2017 nesterov lecture convex optimization vol 137 springer berlin germany 2018 cawley talbot model selection subsequent selection bias performance evaluation mach learn 11 2010 mitchell machine learning new york ny 1997 belson matching prediction principle biological classification stat soc ser c appl stat 8 1959 krischer annotated bibliography decision analytic application health care oper 28 1980 shortliffe buchanan feigenbaum knowledge engineering medical decision making review based clinical decision aid proc ieee 67 1979 bach bridge decision tree approach application drug metabolism kinetic study vivo vitro toxicological pharmacological testing arch toxicol suppl 8 1985 jordan reichman versus dosing theophylline decision analysis approach evaluating theophylline blood level compliance rev respir dis 140 1989 breiman friedman olshen stone classification regression tree wadsworth int group 37 1984 quinlan induction decision tree mach learn 1 1986 breiman random forest mach learn 45 2001 segal machine learning benchmark random forest regression technical report center bioinformatics molecular biostatistics university california san francisco ca 2003 ong sonnenburg schölkopf b rätsch support vector machine kernel computational biology plo comput biol 4 2008 newman et al robust enumeration cell subset tissue expression profile nat method 12 2015 vapnik lerner pattern recognition using generalized portrait method automat rem contr 24 1963 smola schölkopf tutorial support vector regression stat comput 14 2004 mercer function positive negative type connection theory integral equation philos trans soc lond b bio sci 1909 schölkopf b smola learning kernel support vector machine regularization optimization beyond mit press cambridge 2002 krizhevsky sutskever hinton imagenet classification deep convolutional neural network advance neural information processing system 25 2012 cho al learning phrase representation using rnn statistical machine translation arxiv preprint 2014 cho van merriënboer bahdanau bengio property neural machine translation approach arxiv preprint 2014 kingma mohamed rezende welling learning deep generative model proceeding international conference neural information processing system 2 2014 choi al using recurrent neural network model early detection heart failure onset oup academic 2016 http tang cao xiao guo predication plasma concentration remifentanil based elman neural network cent south univ 20 2013 pascanu mikolov bengio difficulty training recurrent neural network international conference machine learning edinburgh june 1 2013 tutorial 15326535 2020 4 downloaded test wiley online library see term condition wiley online library rule use oa article governed applicable creative common license clinical pharmacology therapeutic volume 107 number 4 april 2020 885 bengio pascanu advance optimizing recurrent network ieee international conference acoustic speech signal processing kyoto march 2012 hochreiter schmidhuber long memory neural comput 9 1997 goodfellow bengio courville deep learning mit press cambridge 2016 chung gulcehre cho bengio empirical evaluation gated recurrent neural network sequence modeling workshop deep learning nip montreal december 2014 ryu kim lee deep learning improves prediction drugdrug drugfood interaction proc natl acad sci 115 2018 hammann gutmann vogt helma drewe prediction adverse drug reaction using decision tree modeling clin pharmacol ther 88 2010 lancaster sobie improved prediction torsades de pointes simulation dynamic machine learning algorithm clin pharmacol ther 100 2016 daunhawer et al enhanced early prediction clinically relevant neonatal hyperbilirubinemia machine learning pediatr 86 122 2019 gaweda et al individualization pharmacological anemia management using reinforcement learning neural network 18 2005 benhamou et al insulin delivery adult type 1 diabetes condition multicentre randomised controlled crossover trial lancet dig health 1 2019 chavada ghosh sandaradura maley van hal establishment threshold nephrotoxicity step towards individualized vancomycin dosing resistant staphylococcus aureus bacteremia antimicrob agent chemother 61 2017 hennig holthouse staatz comparing dosage adjustment method tobramycin paediatric adolescent patient cystic fibrosis clin pharmacokinet 54 2015 dansirikul morris tett duffull bayesian approach population pharmacokinetic modelling sirolimus br clin pharmacol 62 2006 lunn best thomas wakefield j spiegelhalter bayesian analysis population model general concept software pharmacokinet pharmacodyn 29 2002 hutchinson et al model machine deep learning take clinical pharmacology next level cpt pharmacomet syst pharmacol 8 2019 makridakis spiliotis assimakopoulos competition result finding conclusion way forward int forecast 34 2018 tutorial 15326535 2020 4 downloaded test wiley online library see term condition wiley online library rule use oa article governed applicable creative common license