electronics review survey deep learning theory architecture md zahangir alom 1 tarek taha 1 chris yakopcic 1 stefan westberg 1 paheding sidike 2 mst shamima nasrin 1 mahmudul hasan 3 brian van essen 4 abdul awwal 4 vijayan asari 1 1 department electrical computer engineering university dayton dayton oh 45469 usa 2 department earth atmospheric science saint louis university saint louis mo 63108 usa 3 comcast lab washington dc 20005 usa 4 lawrence livermore national laboratory llnl livermore ca 94550 usa correspondence received 17 january 2019 accepted 31 january 2019 published 5 march 2019 abstract recent year deep learning ha garnered tremendous success variety application domain new Ô¨Åeld machine learning ha growing rapidly ha applied traditional application domain well some new area present opportunity different method proposed based different category learning including supervised learning experimental result show performance using deep learning compared traditional machine learning approach Ô¨Åelds image processing computer vision speech recognition machine translation art medical imaging medical information processing robotics control bioinformatics natural language processing cybersecurity many others survey present brief survey advance occurred area deep learning dl starting deep neural network dnn survey go cover convolutional neural network cnn recurrent neural network rnn including long memory lstm gated recurrent unit gru ae deep belief network dbn generative adversarial network gan deep reinforcement learning drl additionally discussed recent development advanced variant dl technique based dl approach work considers paper published 2012 history deep learning began furthermore dl approach explored evaluated different application domain also included survey also included recently developed framework sdks benchmark datasets used implementing evaluating deep learning approach some survey published dl using neural network survey reinforcement learning rl however paper not discussed individual advanced technique training deep learning model recently developed method generative model keywords deep learning convolutional neural network cnn recurrent neural network rnn ae restricted boltzmann machine rbm deep belief network dbn generative adversarial network gan deep reinforcement learning drl transfer learning electronics 2019 8 292 electronics 2019 8 292 2 66 introduction since small subset artiÔ¨Åcial intelligence ai often called machine learning ml ha revolutionized several Ô¨Åelds last decade neural network nn subÔ¨Åeld ml wa subÔ¨Åeld spawned deep learning dl since inception dl ha creating ever larger disruption showing outstanding success almost every application domain figure 1 show taxonomy ai dl us either deep architecture learning hierarchical learning approach class ml developed largely 2006 onward learning procedure consisting estimating model parameter learned model algorithm perform speciÔ¨Åc task example artiÔ¨Åcial neural network ann parameter weight matrix dl hand consists several layer input output layer allows many stage information processing unit hierarchical architecture present exploited feature learning pattern classiÔ¨Åcation learning method based representation data also deÔ¨Åned representation learning 3 recent literature state dl based representation learning involves hierarchy feature concept concept deÔ¨Åned one concept deÔ¨Åned one some article dl ha described universal learning approach able solve almost kind problem different application domain word dl not task speciÔ¨Åc 4 since small subset artificial intelligence ai often called machine learning ml ha revolutionized several field last decade neural network nn subfield ml wa subfield spawned deep learning dl since inception dl ha creating ever larger disruption showing outstanding success almost every application domain figure 1 show taxonomy ai dl us either deep architecture learning hierarchical learning approach class ml developed largely 2006 onward learning procedure consisting estimating model parameter learned model algorithm perform specific task example artificial neural network ann parameter weight matrix dl hand consists several layer input output layer allows many stage information processing unit hierarchical architecture present exploited feature learning pattern classification learning method based representation data also defined representation learning 3 recent literature state dl based representation learning involves hierarchy feature concept concept defined one concept defined one some article dl ha described universal learning approach able solve almost kind problem different application domain word dl not task specific 4 type deep learning approach deep learning approach categorized follows supervised partially supervised unsupervised addition another category learning approach called reinforcement learning rl deep rl drl often discussed scope sometimes unsupervised learning approach figure 2 show pictorial diagram figure taxonomy ai ai artificial intelligence ml machine learning nn neural network dl deep learning snn spiking neural network deep supervised learning supervised learning learning technique us labeled data case supervised dl approach environment ha set input corresponding output example input xt intelligent agent predicts ùë¶ ùëì agent receive loss value ùëô ùë¶ agent iteratively modify network parameter better approximation desired output successful training agent able get correct answer question environment different supervised learning approach deep leaning including deep neural network dnn convolutional neural network cnn recurrent neural network rnn including long short term memory lstm gated recurrent unit gru network described detail respective section figure taxonomy ai ai artiÔ¨Åcial intelligence ml machine learning nn neural network dl deep learning snn spiking neural network type deep learning approach deep learning approach categorized follows supervised partially supervised unsupervised addition another category learning approach called reinforcement learning rl deep rl drl often discussed scope sometimes unsupervised learning approach figure 2 show pictorial diagram electronics 2019 8 x peer review 3 67 deep learning learning learning occurs based partially labeled datasets some case drl generative adversarial network gan used learning technique gan discussed section section 8 survey drl approach additionally rnn including lstm gru used learning well deep unsupervised learning unsupervised learning system one without presence data label case agent learns internal representation important feature discover unknown relationship structure within input data often clustering dimensionality reduction generative technique considered unsupervised learning approach several member deep learning family good clustering dimensionality reduction including ae restricted boltzmann machine rbm recently developed gan addition rnns lstm rl also used unsupervised learning many application domain section 6 7 discus rnns lstms detail deep reinforcement learning rl deep reinforcement learning learning technique use unknown environment drl began 2013 google deep mind several advanced method proposed based rl example rl environment sample input agent predict ùë¶ ùëì agent receive cost ùë¶ p unknown probability distribution environment asks agent question give noisy score answer sometimes approach called learning well many technique implemented based concept section 8 rl not straight forward loss function thus making learning harder compared traditional supervised approach fundamental difference rl supervised learning first not full access function trying optimize must query interaction second interacting environment input depends previous action depending upon problem scope space one decide type rl need applied solving task problem ha lot parameter optimized drl best way go problem ha fewer parameter optimization derivation free rl approach good example annealing cross entropy method spsa figure category deep leaning approach feature learning key difference traditional ml dl feature extracted traditional ml approach use handcrafted engineering feature applying several feature extraction algorithm apply learning algorithm additionally boosting approach figure category deep leaning approach electronics 2019 8 292 3 66 deep supervised learning supervised learning learning technique us labeled data case supervised dl approach environment ha set input corresponding output xt yt example input xt intelligent agent predicts ÀÜ yt f xt agent receive loss value l yt ÀÜ yt agent iteratively modify network parameter better approximation desired output successful training agent able get correct answer question environment different supervised learning approach deep leaning including deep neural network dnn convolutional neural network cnn recurrent neural network rnn including long short term memory lstm gated recurrent unit gru network described detail respective section deep learning learning learning occurs based partially labeled datasets some case drl generative adversarial network gan used learning technique gan discussed section section 8 survey drl approach additionally rnn including lstm gru used learning well deep unsupervised learning unsupervised learning system one without presence data label case agent learns internal representation important feature discover unknown relationship structure within input data often clustering dimensionality reduction generative technique considered unsupervised learning approach several member deep learning family good clustering dimensionality reduction including ae restricted boltzmann machine rbm recently developed gan addition rnns lstm rl also used unsupervised learning many application domain section 6 7 discus rnns lstms detail deep reinforcement learning rl deep reinforcement learning learning technique use unknown environment drl began 2013 google deep mind several advanced method proposed based rl example rl environment sample input xt agent predict ÀÜ yt f xt agent receive cost ct ÀÜ yt p unknown probability distribution environment asks agent question give noisy score answer sometimes approach called learning well many technique implemented based concept section 8 rl not straight forward loss function thus making learning harder compared traditional supervised approach fundamental difference rl supervised learning first not full access function trying optimize must query interaction second interacting environment input xt depends previous action depending upon problem scope space one decide type rl need applied solving task problem ha lot parameter optimized drl best way go problem ha fewer parameter optimization derivation free rl approach good example annealing cross entropy method spsa feature learning key difference traditional ml dl feature extracted traditional ml approach use handcrafted engineering feature applying several feature extraction algorithm apply learning algorithm additionally boosting approach often used electronics 2019 8 292 4 66 several learning algorithm applied feature single task dataset decision made according multiple outcome different algorithm hand case dl feature learned automatically represented hierarchically multiple level strong point dl traditional machine learning approach table 1 show different learning approach different learning step table different feature learning approach approach learning step input feature output traditional machine learning input feature mapping feature output representation learning input feature mapping feature output deep learning input simple feature complex feature mapping feature output apply dl dl employed several situation machine intelligence would useful see figure 3 absence human expert navigation mar human unable explain expertise speech recognition vision language understanding solution problem change time tracking weather prediction preference stock price prediction solution need adapted particular case biometrics personalization problem size vast limited reasoning capability calculation webpage rank matching ad facebook sentiment analysis electronics 2019 8 x peer review 5 67 figure accuracy imagenet classification challenge different dl model 5 0 2 4 6 8 10 12 14 16 18 2012 2013 2014 2014 2015 alexnet 7 clarifia 8 9 19 10 152 11 human erros model year experimental result cdnn pooling 18 ctc 19 dcnn 20 ensemble 21 rnn transducer 19 rnn 22 segmental rnn 23 phone error rate per percentage figure accuracy imagenet classiÔ¨Åcation challenge different dl model present dl applied almost area result approach often called universal learning approach performance dl some outstanding success Ô¨Åelds computer vision speech recognition discussed image classiÔ¨Åcation imagenet dataset one problem named large scale visual recognition challenge lsvrc cnn variant one dl branch showed electronics 2019 8 292 5 66 accuracy imagenet task following graph show success story dl technique overtime challenge figure 3 show ha achieved error rate outperformed human accuracy figure accuracy imagenet classification challenge different dl model figure phone error rate per timit continuous speech corpus dataset 23 b automatic speech recognition initial success field speech recognition popular timit dataset common data set generally used evaluation wa recognition task 24 timit continuous speech corpus contains 630 speaker eight major dialect american english speaker read 10 sentence figure 4 summarizes error rate including early result measured percent phone error rate per last 20 year bar graph clearly show recently developed dl 0 2 4 2012 2013 2014 2014 2015 alexnet 7 clarifia 8 9 19 10 152 11 human e model year 0 5 10 15 20 25 30 35 scrf 13 scrf 14 deep segmental nn 15 discriminative segmetal cascade dsc 16 dl 17 dsc pas 16 cdnn pooling 18 ctc 19 dcnn 20 ensemble 21 rnn transducer 19 rnn 22 segmental rnn 23 phone error rate per percentage figure phone error rate per timit continuous speech corpus dataset b automatic speech recognition initial success Ô¨Åeld speech recognition popular timit dataset common data set generally used evaluation wa recognition task 24 timit continuous speech corpus contains 630 speaker eight major dialect american english speaker read 10 sentence figure 4 summarizes error rate including early result measured percent phone error rate per last 20 year bar graph clearly show recently developed dl approach top graph perform better compared any previous machine learning approach timit dataset some example application shown figure 5 electronics 2019 8 x peer review 6 67 approach top graph perform better compared any previous machine learning approach timit dataset some example application shown figure 5 figure traffic forecasting using dynamic traffic flow road 1 2 3 capture spatial dependency using diffusion convolutional recurrent neural network 25 lymphocyte detection 26 semantic segmentation 27 road 1 road 2 road 3 figure trafÔ¨Åc forecasting using dynamic trafÔ¨Åc Ô¨Çow road 1 2 3 capture spatial dependency using diffusion convolutional recurrent neural network 25 electronics 2019 8 292 6 66 figure traffic forecasting using dynamic traffic flow road 1 2 3 capture spatial dependency using diffusion convolutional recurrent neural network 25 lymphocyte detection 26 semantic segmentation 27 nucleus segmentation 28 blood cell classification 29 figure example image dl applied successfully achieved performance image taken correspond ding reference dl universal learning approach dl approach sometimes called universal learning applied almost any application domain robust deep learning approach not require precisely designed feature instead optimal feature automatically learned task hand result robustness natural variation input data achieved generalization dl approach used different application different data type approach often called transfer learning addition approach helpful problem road 3 figure example image dl applied successfully achieved performance image taken corresponding reference dl universal learning approach dl approach sometimes called universal learning applied almost any application domain robust deep learning approach not require precisely designed feature instead optimal feature automatically learned task hand result robustness natural variation input data achieved generalization dl approach used different application different data type approach often called transfer learning addition approach helpful problem doe not sufÔ¨Åcient available data number literature discussed concept see section 4 scalability dl approach highly scalable microsoft invented deep network known resnet 11 network contains 1202 layer often implemented supercomputing scale big initiative lawrence livermore national laboratory llnl developing framework network like implement thousand node 24 challenge dl several challenge dl electronics 2019 8 292 7 66 big data analytics using dl scalability dl approach ability generate data important data not available learning system especially computer vision task inverse graphic energy efÔ¨Åcient technique special purpose device including mobile intelligence fpgas transfer learning learning mean learning different domain different model together dealing causality learning challenge already considered dl community firstly big data analytics challenge good survey wa conducted 2014 30 paper author explained detail dl deal different criterion including volume velocity variety veracity big data problem author also showed different advantage dl approach dealing big data problem figure 7 clearly demonstrates performance traditional ml approach show better performance lesser amount input data amount data increase beyond certain number performance traditional machine learning approach becomes steady whereas dl approach increase respect increment amount data dl approach highly scalable microsoft invented deep network known resnet 11 network contains 1202 layer often implemented supercomputing scale big initiative lawrence livermore national laboratory llnl developing framework network like implement thousand node 24 challenge dl several challenge dl big data analytics using dl scalability dl approach ability generate data important data not available learning system especially computer vision task inverse graphic energy efficient technique special purpose device including mobile intelligence fpgas transfer learning learning mean learning different domain different model together dealing causality learning challenge already considered dl community firstly big data analytics challenge good survey wa conducted 2014 30 paper author explained detail dl deal different criterion including volume velocity variety veracity big data problem author also showed different advantage dl approach dealing big data problem figure 7 clearly demonstrates performance traditional ml approach show better performance lesser amount input data amount data increase beyond certain number performance traditional machine learning approach becomes steady whereas dl approach increase respect increment amount data figure performance deep learning respect amount data secondly case solving problem solution implemented computing hpc system cluster sometimes considered cloud computing offer immense potential business computing data explodes velocity variety veracity volume getting increasingly difficult scale compute performance using server storage step increase article considered demand suggested efficient hpc heterogeneous computing system one example lawrence livermore national laboratory llnl ha developed framework called livermore big artificial neural network lbann figure performance deep learning respect amount data secondly case solving problem solution implemented computing hpc system cluster sometimes considered cloud computing offer immense potential business computing data explodes velocity variety veracity volume getting increasingly difÔ¨Åcult scale compute performance using server storage step increase article considered demand suggested efÔ¨Åcient hpc heterogeneous computing system one example lawrence livermore national laboratory llnl ha developed framework called livermore big artiÔ¨Åcial neural network lbann implementation scale dl clearly supplants issue scalability dl 24 thirdly generative model another challenge deep learning one example gan outstanding approach data generation any task generate data distribution 33 fourthly transfer learning discussed section fourthly lot research ha conducted energy efÔ¨Åcient deep learning approach respect network architecture hardwires section 10 discus issue make any uniform model solve multiple task different application domain far system concerned one article google titled one model learn 34 good example approach learn different application domain including electronics 2019 8 292 8 66 imagenet multiple translation task image captioning dataset speech recognition corpus english parsing task discussing challenge respective solution survey some technique proposed last year finally learning system causality ha presented graphical model deÔ¨Ånes one may infer causal model data recently dl based approach ha proposed solving type problem 38 however many challenging issue solved last year not possible solve efÔ¨Åciently revolution example image video captioning 39 style transferring one domain another domain using gan 40 text image synthesis 41 many 42 some survey conducted recently dl Ô¨Åeld paper survey dl revolution not address recently developed generative model called gan 33 addition discus little rl not cover recent trend drl approach case survey conducted different dl approach individually good survey based reinforcement learning approach another survey exists transfer learning 48 one survey ha conducted neural network hardware 49 however main objective work provide overall idea deep learning related Ô¨Åelds including deep supervised dnn cnn rnn unsupervised ae rbm gan sometimes gan also used learning task drl some case drl considered unsupervised approach addition considered recently developing trend Ô¨Åeld application developed based technique furthermore included framework benchmark datasets often used evaluating deep learning technique moreover name conference journal also included considered community publishing research article rest paper ha organized following way detailed survey dnns discussed section 2 section 3 discus cnn section 4 describes different advanced technique efÔ¨Åcient training dl approach section 5 discus rnns aes rbms discussed section gans application discussed section rl presented section section 9 explains transfer learning section 10 present energy efÔ¨Åcient approach hardwires dl section 11 discus deep learning framework standard development kit sdk benchmark different application domain web link given section conclusion made section 13 deep neural network history dnn brief history neural network highlighting key event shown figure computational neurobiology ha conducted signiÔ¨Åcant research constructing computational model artiÔ¨Åcial neuron artiÔ¨Åcial neuron try mimic behavior human brain fundamental component building anns basic computational element neuron called node unit receives input external source ha some internal parameter including weight bias learned training produce output unit called perceptron fundamental ann discussed reference electronics 2019 8 292 9 66 electronics 2019 8 x peer review 9 67 including weight bias learned training produce output unit called perceptron fundamental ann discussed reference figure history deep learning development anns general nns consist multilayer perceptron mlp contain one hidden layer multiple hidden unit neuron detail mlp please see reference gradient descent gradient descent approach optimization algorithm used finding local minimum objective function ha used training anns last couple decade successfully stochastic gradient descent sgd since long training time main drawback traditional gradient descent approach sgd approach used training deep neural network dnn bp dnn trained popular bp algorithm sgd case mlps easily represent nn model using computation graph directive acyclic graph representation dl use efficiently calculate gradient top bottom layer bp shown reference momentum momentum method help accelerate training process sgd approach main idea behind use moving average gradient instead using only current real value gradient express following equation mathematically Œ≥ ‚Ñ± 1 2 Œ≥ momentum ùúÇ learning rate tth round training popular approach introduced last year explained section ix scope optimization approach main advantage using momentum training prevent network getting stuck local minimum value momentum Œ≥ noted higher momentum value overshoot minimum possibly making network mcculloch pitt show neuron combined construct turing machine 50 rosenblatt show perceptron converge trying learn represented 51 minsky papert show limitation perceptron killing research neural network decade 52 backpropagation algorithm geoffrey hinton et al 53 revitalizes field neocognitron hierarchical neural network capable visual pattern recognition 54 cnns backpropagation document analysis yan lecun 55 hinton lab solves training problem dnns pressent variety deep learning algorithm increasingly emerging 1943 present figure history deep learning development anns general nns consist multilayer perceptron mlp contain one hidden layer multiple hidden unit neuron detail mlp please see reference gradient descent gradient descent approach optimization algorithm used Ô¨Ånding local minimum objective function ha used training anns last couple decade successfully stochastic gradient descent sgd since long training time main drawback traditional gradient descent approach sgd approach used training deep neural network dnn bp dnn trained popular bp algorithm sgd case mlps easily represent nn model using computation graph directive acyclic graph representation dl use efÔ¨Åciently calculate gradient top bottom layer bp shown reference momentum momentum method help accelerate training process sgd approach main idea behind use moving average gradient instead using only current real value gradient express following equation mathematically vt Œ≥ 1 Œ∏t vt 2 Œ≥ momentum Œ∑ learning rate tth round training popular approach introduced last year explained section 9 scope optimization approach main advantage using momentum training prevent network getting stuck local minimum value momentum Œ≥ noted higher momentum value overshoot minimum possibly making network unstable general Œ≥ set initial learning stabilizes increased higher 60 electronics 2019 8 292 10 66 learning rate Œ∑ learning rate important component training dnn learning rate step size considered training make training process faster however selecting value learning rate sensitive example choose larger value Œ∑ network may start diverging instead converging hand choose smaller value Œ∑ take time network converge addition may easily get stuck local minimum typical solution problem reduce learning rate training 64 three common approach used reducing learning rate training constant factored exponential decay first deÔ¨Åne constant Œ∂ applied reduce learning rate manually deÔ¨Åned step function second learning rate adjusted training following equation Œ∑t œµ 3 Œ∑t tth round learning rate initial learning rate Œ≤ decay factor value range 0 1 step function format exponential decay Œ∑t œµ 4 common practice use learning rate decay Œ≤ reduce learning rate factor 10 stage weight decay weight decay used training deep learning model regularization approach help prevent overÔ¨Åtting network model generalization regularization f Œ∏ x deÔ¨Åned 5 ÀÜ Œµ f Œ∏ x Œµ f Œ∏ x 1 œâ 6 gradient weight Œ∏ 7 general practice use value Œª smaller Œª accelerate training necessary component efficient training including data preprocessing augmentation network initialization approach batch normalization activation function regularization dropout different optimization approach discussed section 4 last decade many efÔ¨Åcient approach proposed better training deep neural network 2006 attempt taken training deep architecture failed training deep supervised neural network tended yield worse result training test error shallow one 1 2 hidden layer hinton revolutionary work dbns spearheaded change 2006 due composition many layer dnns capable representing highly varying nonlinear function compared shallow learning approach moreover dnns efÔ¨Åcient learning combination feature extraction classiÔ¨Åcation layer following section discus detail different dl approach necessary component electronics 2019 8 292 11 66 convolutional neural network cnn cnn overview network structure wa Ô¨Årst proposed fukushima 1988 54 wa not widely used however due limit computation hardware training network lecun et al 55 applied learning algorithm cnns obtained successful result handwritten digit classiÔ¨Åcation problem researcher improved cnns reported result many recognition task cnns several advantage dnns including like human visual processing system highly optimized structure processing image effective learning extracting abstraction feature max pooling layer cnns effective absorbing shape variation moreover composed sparse connection tied weight cnns signiÔ¨Åcantly fewer parameter fully connected network similar size cnns trained learning algorithm suffer le diminishing gradient problem given algorithm train whole network minimize error criterion directly cnns produce highly optimized weight figure 9 show overall architecture cnns consists two main part feature extractor classiÔ¨Åer feature extraction layer layer network receives output immediate previous layer input pass output input next layer cnn architecture consists combination three type layer convolution classiÔ¨Åcation two type layer low network convolutional layer layer even numbered layer convolution layer operation output node convolution layer grouped plane called feature mapping plane layer usually derived combination one plane previous layer node plane connected small region connected plane previous layer node convolution layer extract feature input image convolution operation input node electronics 2019 8 x peer review 11 67 cnn overview network structure wa first proposed fukushima 1988 54 wa not widely used however due limit computation hardware training network lecun et al 55 applied learning algorithm cnns obtained successful result handwritten digit classification problem researcher improved cnns reported result many recognition task cnns several advantage dnns including like human visual processing system highly optimized structure processing image effective learning extracting abstraction feature max pooling layer cnns effective absorbing shape variation moreover composed sparse connection tied weight cnns significantly fewer parameter fully connected network similar size cnns trained learning algorithm suffer le diminishing gradient problem given based algorithm train whole network minimize error criterion directly cnns produce highly optimized weight figure 9 show overall architecture cnns consists two main part feature extractor classifier feature extraction layer layer network receives output immediate previous layer input pass output input next layer cnn architecture consists combination three type layer convolution classification two type layer low network convolutional layer layer even numbered layer convolution numbered layer operation output node convolution pooling layer grouped plane called feature mapping plane layer usually derived combination one plane previous layer node plane connected small region connected plane previous layer node convolution layer extract feature input image convolution operation input node figure overall architecture convolutional neural network cnn includes input layer multiple alternating convolution layer one layer one classification layer feature derived feature propagated lower level layer feature propagate highest layer level dimension feature reduced depending size kernel convolutional operation respectively however number feature map usually increased representing better feature input image ensuring classification accuracy output last layer cnn used input fully connected network called classification layer neural network used classification layer better performance classification layer extracted feature taken input respect dimension weight matrix final figure overall architecture convolutional neural network cnn includes input layer multiple alternating convolution layer one layer one classiÔ¨Åcation layer feature derived feature propagated lower level layer feature propagate highest layer level dimension feature reduced depending size kernel convolutional operation respectively however number feature map usually increased representing better feature input image ensuring classiÔ¨Åcation accuracy output last layer cnn used input fully connected network called classiÔ¨Åcation layer neural network electronics 2019 8 292 12 66 used classiÔ¨Åcation layer better performance classiÔ¨Åcation layer extracted feature taken input respect dimension weight matrix Ô¨Ånal neural network however fully connected layer expensive term network learning parameter nowadays several new technique including average pooling global average pooling used alternative network score respective class calculated top classiÔ¨Åcation layer using layer based highest score classiÔ¨Åer give output corresponding class mathematical detail different layer cnns discussed following section convolutional layer layer feature map previous layer convolved learnable kernel output kernel go linear activation function sigmoid hyperbolic tangent softmax rectiÔ¨Åed linear identity function form output feature map output feature map combined one input feature map general xl j f iœµmj ij bl j 8 xl j output current layer previous layer output kl ij kernel present layer bl j bias current layer mj represents selection input map output map additive bias b given however input map convolved distinct kernel generate corresponding output map output map Ô¨Ånally go linear activation function sigmoid hyperbolic tangent softmax rectiÔ¨Åed linear identity function layer subsampling layer performs sampled operation input map commonly known pooling layer layer number input output feature map doe not change example n input map exactly n output map due sampling operation size dimension output map reduced depending size sampling mask example 2 2 sampling kernel used output dimension half corresponding input dimension image operation formulated xl j j 9 represents function two type operation mostly performed layer average pooling case average pooling approach function usually sum n n patch feature map previous layer selects average value hand case highest value selected n n patch feature map therefore output map dimension reduced n time some special case output map multiplied scalar some alternative layer proposed fractional layer convolution explained section classiÔ¨Åcation layer fully connected layer computes score class extracted feature convolutional layer preceding step Ô¨Ånal layer feature map represented vector scalar value passed fully connected layer fully connected neural layer used classiÔ¨Åcation layer no strict rule electronics 2019 8 292 13 66 number layer incorporated network model however case two four layer observed different architecture including lenet 55 alexnet 7 vgg net 9 fully connected layer expensive term computation alternative approach proposed last year include global average pooling layer average pooling layer help reduce number parameter network signiÔ¨Åcantly backward propagation cnns fully connected layer update following general approach fully connected neural network fcnn Ô¨Ålters convolutional layer updated performing full convolutional operation feature map convolutional layer immediate previous layer figure 10 show basic operation convolution input image electronics 2019 8 x peer review 13 67 layer average pooling layer help reduce number parameter network significantly backward propagation cnns fully connected layer update following general approach fully connected neural network fcnn filter convolutional layer updated performing full convolutional operation feature map convolutional layer immediate previous layer figure 10 show basic operation convolution input image figure feature map performing convolution pooling operation network parameter required memory cnn number computational parameter important metric measure complexity deep learning model size output feature map formulated follows ùëÜ 1 10 ùëÅ refers dimension input feature map ùêπ refers dimension filter receptive field ùëÄ refers dimension output feature map ùëÜ stand stride length padding typically applied convolution operation ensure input output feature map dimension amount padding depends size kernel equation 17 used determining number row column padding 11 ùëÉ amount padding ùêπ refers dimension kernel several criterion considered comparing model however case number network parameter total amount memory considered number parameter layer calculated based following equation 12 bias added weight equation written follows 1 13 total number parameter lathe yer represented ùëÉ total number output feature map total number input feature map channel example let assume layer ha 32 input feature map 64 output feature map filter size case total number parameter bias layer 5 5 33 64 thus amount memory need operation layer expressed 14 popular cnn architecture figure feature map performing convolution pooling operation network parameter required memory cnn number computational parameter important metric measure complexity deep learning model size output feature map formulated follows n 1 10 n refers dimension input feature map f refers dimension Ô¨Ålters receptive Ô¨Åeld refers dimension output feature map stand stride length padding typically applied convolution operation ensure input output feature map dimension amount padding depends size kernel equation 17 used determining number row column padding p f 11 p amount padding f refers dimension kernel several criterion considered comparing model however case number network parameter total amount memory considered number parameter parml lth layer calculated based following equation parml f f fml 12 bias added weight equation written follows parml f f 1 fml 13 total number parameter lth lathe yer represented pl fml total number output feature map total number input feature map channel example let assume lth layer ha 32 input feature map fml 64 output feature electronics 2019 8 292 14 66 map Ô¨Ålter size f case total number parameter bias layer parml 5 5 33 64 528 thus amount memory meml need operation lth layer expressed meml nl nl fml 14 popular cnn architecture section several popular cnn architecture examined general deep convolutional neural network made key set basic layer including convolution layer layer dense layer layer architecture typically consist stack several convolutional layer layer followed fully connected softmax layer end some example model lenet 55 alexnet 7 vgg net 9 nin 66 convolutional conv 67 alternative efÔ¨Åcient advanced architecture proposed including densenet 68 fractalnet 69 googlenet inception unit residual network 11 basic building component convolution pooling almost across architecture however some topological difference observed modern deep learning architecture many dcnn architecture alexnet 7 vgg 9 googlenet dense cnn 68 fractalnet 69 generally considered popular architecture performance different benchmark object recognition task among structure some architecture designed especially data analysis googlenet resnet whereas vgg network considered general architecture some architecture dense term connectivity densenet 68 fractal network alternative resnet model lenet 1998 although lenet wa proposed limited computation capability memory capacity made algorithm difÔ¨Åcult implement 2010 55 lecun et al 55 however proposed cnns algorithm experimented handwritten digit dataset achieve accuracy proposed cnn architecture 55 basic conÔ¨Åguration follows see figure 11 two convolution conv layer two layer two fully connected layer output layer gaussian connection total number weight multiply accumulates mac 431 k respectively computational hardware started improving capability cnns stated becoming popular effective learning approach computer vision machine learning community electronics 2019 8 x peer review 14 67 section several popular cnn architecture examined general deep convolutional neural network made key set basic layer including convolution layer layer dense layer layer architecture typically consist stack several convolutional layer layer followed fully connected softmax layer end some example model lenet 55 alexnet 7 vgg net 9 nin 66 convolutional conv 67 alternative efficient advanced architecture proposed including densenet 68 fractalnet 69 googlenet inception unit residual network 11 basic building component convolution pooling almost across architecture however some topological difference observed modern deep learning architecture many dcnn architecture alexnet 7 vgg 9 googlenet dense cnn 68 fractalnet 69 generally considered popular architecture performance different benchmark object recognition task among structure some architecture designed especially data analysis googlenet resnet whereas vgg network considered general architecture some architecture dense term connectivity densenet 68 fractal network alternative resnet model lenet 1998 although lenet wa proposed limited computation capability memory capacity made algorithm difficult implement 2010 55 lecun et al 55 however proposed cnns algorithm experimented handwritten digit dataset achieve accuracy proposed cnn architecture 5 55 basic configuration follows see figure 11 two convolution conv layer two layer two fully connected layer output layer gaussian connection total number weight multiply accumulates mac 431 k respectively computational hardware started improving capability cnns stated becoming popular effective learning approach computer vision machine learning community figure architecture lenet alexnet 2012 2012 alex krizhevesky others proposed deeper wider cnn model compared lenet difficult imagenet challenge visual object recognition called imagenet large scale visual recognition challenge ilsvrc 2012 7 alexnet achieved recognition accuracy traditional machine learning computer vision approach wa significant breakthrough field machine learning computer vision visual recognition classification task point history interest deep learning increased rapidly th hit f al n h fi 12 th fi l ti l l f figure architecture lenet alexnet 2012 2012 alex krizhevesky others proposed deeper wider cnn model compared lenet difÔ¨Åcult imagenet challenge visual object recognition called imagenet large scale visual recognition challenge ilsvrc 2012 7 alexnet achieved electronics 2019 8 292 15 66 recognition accuracy traditional machine learning computer vision approach wa signiÔ¨Åcant breakthrough Ô¨Åeld machine learning computer vision visual recognition classiÔ¨Åcation task point history interest deep learning increased rapidly architecture alexnet shown figure Ô¨Årst convolutional layer performs convolution local response normalization lrn 96 different receptive Ô¨Ålters used 11 11 size max pooling operation performed 3 3 Ô¨Ålters stride size operation performed second layer 5 5 Ô¨Ålters 3 3 Ô¨Ålters used third fourth Ô¨Åfth convolutional layer 384 384 296 feature map respectively two fully connected fc layer used dropout followed softmax layer end two network similar structure number feature map trained parallel model two new concept local response normalization lrn dropout introduced network lrn applied two different way first applying single channel feature map n n patch selected feature map normalized based neighborhood value second lrn applied across channel feature map neighborhood along third dimension single pixel location electronics 2019 8 x peer review 15 67 3 filter stride size operation performed second layer 5 5 filter 3 3 filter used third fourth fifth convolutional layer 384 384 296 feature map respectively two fully connected fc layer used dropout followed softmax layer end two network similar structure number feature map trained parallel model two new concept local response normalization lrn dropout introduced network lrn applied two different way first applying single channel feature map patch selected feature map normalized based neighborhood value second lrn applied across channel feature map neighborhood along third dimension single pixel location figure architecture alexnet convolution local response normalization lrn fully connected fc layer alexnet ha three convolution layer two fully connected layer processing imagenet dataset total number parameter alexnet calculated follows first layer input sample filter kernel mask receptive field ha size 11 stride 4 output first convolution layer according equation section calculate first layer ha 290400 neuron 364 11 363 1 bias weight parameter first convolution layer table 2 show number parameter layer million total number weight mac whole network respectively zfnet clarifai 2013 2013 matthew zeiler rob fergue 2013 ilsvrc cnn architecture wa extension alexnet network wa called zfnet 8 author name cnns expensive computationally optimum use parameter needed model complexity point view zfnet architecture improvement alexnet designed tweaking network parameter latter zfnet us kernel instead kernel significantly reduce number weight reduces number network parameter dramatically improves overall recognition accuracy network network nin model slightly different previous model couple new concept introduced 66 first concept use multilayer perception convolution convolution performed filter help add nonlinearity model help increase depth network regularized dropout concept used often bottleneck layer deep learning model figure architecture alexnet convolution local response normalization lrn fully connected fc layer alexnet ha three convolution layer two fully connected layer processing imagenet dataset total number parameter alexnet calculated follows Ô¨Årst layer input sample 224 224 3 Ô¨Ålters kernel mask receptive Ô¨Åeld ha size 11 stride 4 output Ô¨Årst convolution layer 55 55 according equation section calculate Ô¨Årst layer ha 55 55 96 neuron 364 11 3 363 1 bias weight parameter Ô¨Årst convolution layer 364 table 2 show number parameter layer million total number weight mac whole network respectively zfnet clarifai 2013 2013 matthew zeiler rob fergue 2013 ilsvrc cnn architecture wa extension alexnet network wa called zfnet 8 author name cnns expensive computationally optimum use parameter needed model complexity point view zfnet architecture improvement alexnet designed tweaking network parameter latter zfnet us 7 7 kernel instead 11 11 kernel signiÔ¨Åcantly reduce number weight reduces number network parameter dramatically improves overall recognition accuracy electronics 2019 8 292 16 66 network network nin model slightly different previous model couple new concept introduced 66 Ô¨Årst concept use multilayer perception convolution convolution performed 1 1 Ô¨Ålter help add nonlinearity model help increase depth network regularized dropout concept used often bottleneck layer deep learning model second concept use global average pooling gap alternative fully connected layer help reduce number network parameter signiÔ¨Åcantly gap change network structure signiÔ¨Åcantly applying gap large feature map generate Ô¨Ånal low dimensional feature vector without reducing dimension feature map vggnet 2014 visual geometry group vgg wa 2014 ilsvrc 9 main contribution work show depth network critical component achieve better recognition classiÔ¨Åcation accuracy cnns vgg architecture consists two convolutional layer use relu activation function following activation function single max pooling layer several fully connected layer also using relu activation function Ô¨Ånal layer model softmax layer classiÔ¨Åcation 9 convolution Ô¨Ålter size changed 3 3 Ô¨Ålter stride three 9 model proposed model 11 16 19 layer respectively vgg network model shown figure electronics 2019 8 x peer review 16 67 vggnet 2014 visual geometry group vgg wa 2014 ilsvrc 9 main contribution work show depth network critical component achieve better recognition classification accuracy cnns vgg architecture consists two convolutional layer use relu activation function following activation function single max pooling layer several fully connected layer also using relu activation function final layer model softmax layer classification 9 convolution filter size changed 3 3 filter stride three 9 model proposed model 11 16 19 layer respectively vgg network model shown figure figure basic building block vgg network convolution conv fc fully connected layer version model ended three fully connected layer however number convolution layer varied contained 8 convolution layer 13 convolution layer 16 convolution layer computational expensive model contained mac googlenet 2014 googlenet winner ilsvrc 2014 10 wa model proposed christian szegedy google objective reducing computation complexity compared traditional cnn proposed method wa incorporate inception layer variable receptive field created different kernel size receptive field created operation captured sparse correlation pattern new feature map stack figure inception layer naive version figure basic building block vgg network convolution conv fc fully connected layer version model ended three fully connected layer however number convolution layer varied contained 8 convolution layer 13 convolution layer 16 convolution layer computational expensive model contained mac googlenet 2014 googlenet winner ilsvrc 2014 10 wa model proposed christian szegedy google objective reducing computation complexity compared traditional cnn proposed method wa incorporate inception layer variable receptive Ô¨Åelds created different kernel size receptive Ô¨Åelds created operation captured sparse correlation pattern new feature map stack initial concept inception layer seen figure googlenet improved recognition accuracy using stack inception layer seen figure difference na√Øve inception layer Ô¨Ånal inception layer wa addition 1 1 convolution kernel kernel allowed dimensionality reduction computationally expensive layer googlenet consisted 22 layer total wa far greater any network later improved version network proposed 71 however number network parameter electronics 2019 8 292 17 66 googlenet used wa much lower predecessor alexnet vgg googlenet network parameter alexnet computation googlenet also mac far lower alexnet vgg convolution layer 16 convolution layer computational expensive model contained mac googlenet 2014 googlenet winner ilsvrc 2014 10 wa model proposed christian szegedy google objective reducing computation complexity compared traditional cnn proposed method wa incorporate inception layer variable receptive field created different kernel size receptive field created operation captured sparse correlation pattern new feature map stack figure inception layer naive version initial concept inception layer seen figure googlenet improved recognition accuracy using stack inception layer seen figure difference na√Øve inception layer final inception layer wa addition convolution kernel kernel allowed dimensionality reduction computationally expensive layer googlenet consisted 22 layer total wa far greater any network later improved version network proposed 71 however number network parameter googlenet used wa much lower predecessor alexnet vgg googlenet figure inception layer naive version electronics 2019 8 x peer review 17 67 network parameter alexnet computation googlenet also mac far lower alexnet vgg figure inception layer dimension reduction residual network resnet 2015 winner ilsvrc 2015 wa residual network architecture resnet 11 resnet wa developed kaiming intent designing network not suffer vanishing gradient problem predecessor resnet developed many different number layer 34 152 even popular contained 49 convolution layer 1 fully connected layer end network total number weight mac whole network respectively basic block diagram resnet architecture shown figure resnet traditional feedforward network residual connection output residual layer defined based output come previous layer defined ‚Ñ± output performing various operation convolution different size filter batch normalization bn followed activation function relu final output residualthe unit defined following equation ‚Ñ± 15 figure basic diagram residual block residual network consists several basic residual block however operation figure inception layer dimension reduction residual network resnet 2015 winner ilsvrc 2015 wa residual network architecture resnet 11 resnet wa developed kaiming intent designing network not suffer vanishing gradient problem predecessor resnet developed many different number layer 34 152 even popular contained 49 convolution layer 1 fully connected layer end network total number weight mac whole network respectively basic block diagram resnet architecture shown figure resnet traditional feedforward network residual connection output residual layer deÔ¨Åned based output l th come previous layer deÔ¨Åned f output performing various operation convolution different size Ô¨Ålters batch normalization bn followed activation function relu Ô¨Ånal output residualthe unit xl deÔ¨Åned following equation xl f 15 electronics 2019 8 x peer review 17 67 network parameter alexnet computation googlenet also mac far lower alexnet vgg figure inception layer dimension reduction residual network resnet 2015 winner ilsvrc 2015 wa residual network architecture resnet 11 resnet wa developed kaiming intent designing network not suffer vanishing gradient problem predecessor resnet developed many different number layer 34 152 even popular contained 49 convolution layer 1 fully connected layer end network total number weight mac whole network respectively basic block diagram resnet architecture shown figure resnet traditional feedforward network residual connection output residual layer defined based output come previous layer defined ‚Ñ± output performing various operation convolution different size filter batch normalization bn followed activation function relu final output residualthe unit defined following equation ‚Ñ± 15 figure basic diagram residual block residual network consists several basic residual block however operation residual block varied depending different architecture residual network 11 wider version residual network wa proposed zagoruvko el 72 another improved residual network approach known aggregated residual transformation 73 recently some variant residual model introduced based residual network architecture 76 furthermore several advanced architecture combined inception figure basic diagram residual block electronics 2019 8 292 18 66 residual network consists several basic residual block however operation residual block varied depending different architecture residual network 11 wider version residual network wa proposed zagoruvko et al 72 another improved residual network approach known aggregated residual transformation 73 recently some variant residual model introduced based residual network architecture furthermore several advanced architecture combined inception residual unit basic conceptual diagram unit shown following figure electronics 2019 8 x peer review 18 67 figure basic block diagram inception residual unit mathematically concept represented ‚Ñ± 16 symbol refers concentration operation two output filter convolution operation performed filter finally output added input block concept inception block residual connection introduced architecture 71 improved version network also proposed densely connected network densenet densenet developed gao et al 2017 68 consists densely connected cnn layer output layer connected successor layer dense block 68 therefore formed dense connectivity layer rewarding name densenet concept efficient feature reuse dramatically reduces network parameter densenet consists several dense block transition block placed two adjacent dense block conceptual diagram dense block shown figure figure dense block growth rate layer take preceding feature map input deconstructing figure 19 layer received feature map previous layer ùë•‡¨µ input ùë•‡¨µ 17 ùë•‡¨µ concatenated feature layer 0 considered single tensor performs three different consecutive operation bn 78 followed relu 70 3 3 convolution operation transaction block 1 1 l ti l ti f ith bn f b 2 2 li l thi figure basic block diagram inception residual unit mathematically concept represented xl f k 16 symbol j refers concentration operation two output 3 3 5 5 Ô¨Ålters convolution operation performed 1 1 Ô¨Ålters finally output added input block concept inception block residual connection introduced architecture 71 improved version network also proposed densely connected network densenet densenet developed gao et al 2017 68 consists densely connected cnn layer output layer connected successor layer dense block 68 therefore formed dense connectivity layer rewarding name densenet concept efÔ¨Åcient feature reuse dramatically reduces network parameter densenet consists several dense block transition block placed two adjacent dense block conceptual diagram dense block shown figure electronics 2019 8 x peer review 18 67 figure basic block diagram inception residual unit mathematically concept represented ‚Ñ± 16 symbol refers concentration operation two output filter convolution operation performed filter finally output added input block concept inception block residual connection introduced architecture 71 improved version network also proposed densely connected network densenet densenet developed gao et al 2017 68 consists densely connected cnn layer output layer connected successor layer dense block 68 therefore formed dense connectivity layer rewarding name densenet concept efficient feature reuse dramatically reduces network parameter densenet consists several dense block transition block placed two adjacent dense block conceptual diagram dense block shown figure figure dense block growth rate layer take preceding feature map input deconstructing figure 19 layer received feature map previous layer ùë•‡¨µ input ùë•‡¨µ 17 ùë•‡¨µ concatenated feature layer 0 considered single tensor performs three different consecutive operation bn 78 followed relu 70 3 3 convolution operation transaction block 1 1 figure dense block growth rate k electronics 2019 8 292 19 66 layer take preceding feature map input deconstructing figure 19 lth layer received feature map previous layer input xl hl 17 concatenated feature layer 0 l hl considered single tensor performs three different consecutive operation bn 78 followed relu 70 3 3 convolution operation transaction block 1 1 convolutional operation performed bn followed 2 2 average pooling layer new model show accuracy reasonable number network parameter object recognition task electronics 2019 8 x peer review 19 67 architecture advanced alternative architecture resnet model efficient designing large model nominal depth shorter path propagation gradient training 69 concept based another regularization approach making large network result concept help enforce speed versus accuracy tradeoff basic block diagram fractalnet shown figure figure detailed fractalnet module left fractalnet right capsulenet cnns effective methodology detecting feature object achieving good recognition performance compared handcrafted feature detector limit cnns doe not take account special relationship perspective size orientation feature example face image doe not matter placement different component nose eye mouth etc face neuron cnn wrongly active recognition face without considering special relationship orientation size imagine neuron contains likelihood property feature perspective orientation size special type neuron capsule detect face efficiently distinct information capsule network consists several layer capsule node first version capsule network capsnet consisted three layer capsule node encoding unit architecture mnist image 256 kernel applied stride 1 output 28 1 20 256 feature map output fed primary capsule layer modified convolution layer generates vector instead scalar first convolutional layer kernel applied stride 2 output dimension 20 1 6 primary capsule used kernel generates 32 group 8 neuron size figure detailed fractalnet module left fractalnet right fractalnet 2016 architecture advanced alternative architecture resnet model efÔ¨Åcient designing large model nominal depth shorter path propagation gradient training 69 concept based another regularization approach making large network result concept help enforce speed versus accuracy tradeoff basic block diagram fractalnet shown figure 19 capsulenet cnns effective methodology detecting feature object achieving good recognition performance compared handcrafted feature detector limit cnns doe not take account special relationship perspective size orientation feature example face image doe not matter placement different component nose eye mouth etc face neuron cnn wrongly active recognition face without considering special relationship orientation size imagine neuron contains likelihood property feature perspective orientation size special type neuron capsule detect face efÔ¨Åciently distinct information capsule network consists several layer capsule node Ô¨Årst version capsule network capsnet consisted three layer capsule node encoding unit architecture mnist 28 28 image 256 9 9 kernel applied stride 1 output 28 1 20 256 feature map output fed primary capsule layer modiÔ¨Åed convolution layer generates vector instead scalar Ô¨Årst convolutional layer 9 9 kernel applied stride 2 output dimension electronics 2019 8 292 20 66 20 1 6 primary capsule used 8 32 kernel generates 32 8 6 6 32 group 8 neuron 6 6 size entire encoding decoding process capsnet shown figure 20 21 respectively used layer cnn often handle translation variance even feature move still max pooling window detected capsule contains weighted sum feature previous layer therefore approach capable detecting overlapped feature important segmentation detection task electronics 2019 8 x peer review 20 67 figure capsnet encoding unit 3 layer instance class represented vector capsule digitcaps layer used calculating classification loss weight primary capsule layer digitcaps layer represented ùëä entire encoding decoding process capsnet shown figure 20 21 respectively used layer cnn often handle translation variance even feature move still max pooling window detected capsule contains weighted sum feature previous layer therefore approach capable detecting overlapped feature important segmentation detection task figure decoding unit digit reconstructed digitcaps layer representation euclidean distance used minimizing error input sample reconstructed sample sigmoid layer true label used reconstruction target training traditional cnn single cost function used evaluate overall error propagates backward training however case weight two neuron zero activation neuron not propagated neuron signal routed respect feature parameter rather one size fit cost function iterative dynamic routing agreement detail architecture please see reference 79 new cnn architecture provides accuracy handwritten digit recognition mnist however application point view architecture suitable segmentation detection task compare classification task comparison different model figure capsnet encoding unit 3 layer instance class represented vector capsule digitcaps layer used calculating classiÔ¨Åcation loss weight primary capsule layer digitcaps layer represented wij electronics 2019 8 x peer review 20 67 figure capsnet encoding unit 3 layer instance class represented vector capsule digitcaps layer used calculating classification loss weight primary capsule layer digitcaps layer represented ùëä entire encoding decoding process capsnet shown figure 20 21 respectively used layer cnn often handle translation variance even feature move still max pooling window detected capsule contains weighted sum feature previous layer therefore approach capable detecting overlapped feature important segmentation detection task figure decoding unit digit reconstructed digitcaps layer representation euclidean distance used minimizing error input sample reconstructed sample sigmoid layer true label used reconstruction target training traditional cnn single cost function used evaluate overall error propagates backward training however case weight two neuron zero activation neuron not propagated neuron signal routed respect feature parameter rather one size fit cost function iterative dynamic routing agreement detail architecture please see reference 79 new cnn architecture provides accuracy handwritten digit recognition mnist however application point view architecture suitable segmentation detection task compare classification task comparison different model comparison recently proposed model based error network parameter maximum number connection given table table error computational parameter mac different deep cnn model figure decoding unit digit reconstructed digitcaps layer representation euclidean distance used minimizing error input sample reconstructed sample sigmoid layer true label used reconstruction target training traditional cnn single cost function used evaluate overall error propagates backward training however case weight two neuron zero activation neuron not propagated neuron signal routed respect feature parameter rather one size Ô¨Åts cost function iterative dynamic routing agreement detail architecture please see reference 79 new cnn architecture provides accuracy handwritten digit recognition mnist however application point view architecture suitable segmentation detection task compare classiÔ¨Åcation task comparison different model comparison recently proposed model based error network parameter maximum number connection given table electronics 2019 8 292 21 66 table error computational parameter mac different deep cnn model method 54 alexnet 7 overfeat fast 8 9 googlenet 10 11 error input size 28 28 227 227 231 231 224 224 224 224 224 224 number conv layer 2 5 5 16 21 50 filter size 5 3 number feature map stride 1 1 number weight 26 k 16 number mac 666 g g g g number fc layer 2 3 3 3 1 1 number weight 406 k 130 124 1 1 number mac 405 k 130 124 1 total weight 431 k 61 146 138 7 total mac 724 g g g g dnn model many network architecture fast cnn 80 xception 81 popular computer vision community 2015 new model wa proposed using recurrent convolution layer named recurrent convolution neural network rcnn 82 improved version network combination two popular architecture inception network recurrent convolutional network inception convolutional recurrent neural network ircnn 83 ircnn provided better accuracy compared rcnn inception network almost identical network parameter visual phase guided cnn vip cnn proposed phase guided message passing structure pmps build connection relational component show better speed recognition accuracy 84 look based cnn 85 fast compact accurate model enabling efÔ¨Åcient inference 2016 architecture known fully convolutional network fcn wa proposed segmentation task commonly used 27 recently proposed cnn model include pixel net 86 deep network stochastic depth network ladder network additional cnn architecture model explained 90 some article published deep net really need deep some article published Ô¨Åtnet hit 94 initialization method 95 deep versus wide net 96 training dl large training set 97 graph processing 98 energy efÔ¨Åcient network architecture application cnns cnns solving graph problem learning graph data structure common problem various application data mining machine learning task dl technique made bridge machine learning data mining group efÔ¨Åcient cnn arbitrary graph processing wa proposed 2016 101 image processing computer vision model discussed applied different application domain including image classification detection segmentation localization captioning video classification many good survey dl approach image processing computer vision related task including image classification segmentation detection 102 example single image using cnn method 103 image denoising using cnn 104 photo aesthetic assessment using adaptive deep cnn 105 dcnn hyperspectral imaging segmentation 106 image registration 107 fast artistic style transfer 108 image background segmentation using dcnn 109 handwritten character recognition 110 optical image classiÔ¨Åcation 111 crop mapping using satellite imagery 112 object recognition cellular simultaneous recurrent network cnn 113 dl approach massively applied human activity recognition task achieved performance electronics 2019 8 292 22 66 compared exiting approach however model classiÔ¨Åcation segmentation detection task listed follows 1 model classiÔ¨Åcation problem according architecture classiÔ¨Åcation model input image encoded different step convolution subsampling layer Ô¨Ånally softmax approach used calculate class probability model discussed applied classiÔ¨Åcation problem however model classiÔ¨Åcation layer used feature extraction segmentation detection task list classiÔ¨Åcation model follows alexnet 55 vggnet 9 googlenet 10 resnet 11 densenet 68 fractalnet 69 capsulenet 79 ircnn 83 irrcnn 77 dcrn 120 2 model segmentation problem several semantic segmentation model proposed last year segmentation model consists two unit encoding decoding unit encoding unit convolution subsampling operation performed encode lower dimensional latent space decoding unit decodes image latent space performing deconvolution operation Ô¨Årst segmentation model fully convolutional network fcn later improved version network proposed named segnet 122 several new model proposed recently includes reÔ¨Ånenet 123 pspnet 124 deeplab 125 unet 126 127 3 model detection problem detection problem bit different compared classiÔ¨Åcation segmentation problem case model goal identify target type corresponding position model answer two question object classiÔ¨Åcation problem object regression problem achieve goal two loss calculated classiÔ¨Åcation regression unit top feature extraction module model weight updated respect loses Ô¨Årst time region based cnn rcnn proposed object detection task 128 recently some better detection approach proposed including focal loss dense object detector 129 later different improved version network proposed called faster rcnn fast rcnn mask 131 only look yolo 132 ssd single shot multibox detector 133 tissue detection pathological image 120 speech processing cnns also applied speech processing speech enhancement using multimodal deep cnn 134 audio tagging using convolutional gated recurrent network cgrn 135 cnn medical imaging litjens et al 136 provided good survey dl medical image processing including classification detection segmentation task several popular dl method developed medical image analysis instance mdnet wa developed medical diagnosis using image corresponding text description 137 cardiac segmentation using mri 138 segmentation optic disc retinal vasculature using cnn 139 brain tumor segmentation using random forest feature learned fully convolutional neural network fcnn 140 technique applied Ô¨Åeld computational pathology achieved performance advanced training technique advanced training technique component need considered carefully efÔ¨Åcient training dl approach different advanced technique apply training deep learning model better technique including input better initialization method batch normalization alternative convolutional approach advanced activation function alternative pooling technique network regularization approach better optimization method training following section discussed individual advanced training technique individually electronics 2019 8 292 23 66 preparing dataset presently different approach applied feeding data network different operation prepare dataset follows sample rescaling mean subtraction random cropping Ô¨Çipping data respect horizon vertical axis color jittering whitening many network initialization initialization deep network ha big impact overall recognition accuracy previously network initialized random weight complex task high dimensionality data training dnn becomes difÔ¨Åcult weight not symmetrical due process therefore effective initialization technique important training type dnn however many effective technique proposed last year lecun 142 bengio 143 proposed simple effective approach method weight scaled inverse square root number input neuron layer stated nl number input neuron lth layer deep network initialization approach xavier ha proposed based symmetric activation function respect hypothesis linearity approach known xavier initialization approach recently dmytro et al 95 proposed lsuv initialization approach provides good recognition accuracy several benchmark datasets including imagenet one popular initialization approach ha proposed et al 2015 144 distribution weight lth layer normal distribution mean zero variance 2 nl expressed follows wl 0 2 nl 18 batch normalization batch normalization help accelerate dl process reducing internal covariance shifting input sample mean input linearly transformed zero mean unit variance whitened input network converges faster show better regularization training ha impact overall accuracy since data whitening performed outside network no impact whitening training model case deep recurrent neural network input nth layer combination layer not raw feature input training progress effect normalization whitening reduces respectively cause vanishing gradient problem slow entire training process cause saturation better training process batch normalization applied internal layer deep neural network approach ensures faster convergence theory experiment benchmark batch normalization feature layer independently normalized mean zero variance one algorithm batch normalization given algorithm algorithm 1 batch normalization bn input value x b output yi bnŒ≥ Œ≤ xi ¬µb 1 xi mean b xi 2 variance ÀÜ xi q normalize yi Œ≥ ÀÜ xi Œ≤ Œ≤ xi scaling shifting electronics 2019 8 292 24 66 parameter Œ≥ Œ≤ used scale shift factor normalization value normalization doe not only depend layer value use normalization technique following criterion recommended consider implementation increase learning rate dropout batch normalization doe job weight regularization accelerating learning rate decay remove local response normalization lrn used shufÔ¨Çe training sample thoroughly useless distortion image training set alternative convolutional method alternative computationally efÔ¨Åcient convolutional technique reduce cost multiplication factor proposed 147 activation function traditional sigmoid tanh activation function used implementing neural network approach past decade graphical mathematical representation shown figure output bn‡Æì ‡Æí ‡µü ‡¨µ ‡≠´ mean ùúéùîÖ ‡¨∂ ‡¨µ ‡¨∂ variance ùë• ‡∂ß‡∞ôùîÖ normalize ùõæùë• ùõΩ ‡Æí scaling shifting parameter ùõæ ùõΩ used scale shift factor normalization value normalization doe not only depend layer value use normalization technique following criterion recommended consider implementation increase learning rate dropout batch normalization doe job weight regularization accelerating learning rate decay remove local response normalization lrn used shuffle training sample thoroughly useless distortion image training set alternative convolutional method alternative computationally efficient convolutional technique reduce cost multiplication factor proposed 147 activation function traditional sigmoid tanh activation function used implementing neural network approach past decade graphical mathematical representation shown figure 22 b figure activation function sigmoid function b hyperbolic transient sigmoid 1 1 19 tanh 20 popular activation function called rectified linear unit relu proposed 2010 solves vanishing gradient problem training deep learning approach basic concept simple keep value zero set negative value zero shown figure 23 64 relu activation wa first used alexnet 7 figure activation function sigmoid function b hyperbolic transient sigmoid 1 1 ex 19 tanh ex ex 20 popular activation function called rectiÔ¨Åed linear unit relu proposed 2010 solves vanishing gradient problem training deep learning approach basic concept simple keep value zero set negative value zero shown figure 23 64 relu activation wa Ô¨Årst used alexnet 7 electronics 2019 8 x peer review 25 67 figure pictorial representation rectified linear unit relu mathematically express relu follows max 0 ùë• 21 activation function play crucial role learning weight deep architecture many researcher focus much done area meanwhile several improved version relu proposed provide even better accuracy figure pictorial representation rectiÔ¨Åed linear unit relu electronics 2019 8 292 25 66 mathematically express relu follows max 0 x 21 activation function play crucial role learning weight deep architecture many researcher focus much done area meanwhile several improved version relu proposed provide even better accuracy compared relu activation function shown figure efÔ¨Åcient improved version relu activation function called parametric relu prelu proposed kaiming et al figure 25 show pictorial representation leaky relu elu activation function technique automatically learn parameter adaptively improve accuracy negligible extra computing cost 144 figure pictorial representation rectified linear unit relu mathematically express relu follows max 0 ùë• 21 activation function play crucial role learning weight deep architecture many researcher focus much done area meanwhile several improved version relu proposed provide even better accuracy compared relu activation function shown figure efficient improved version relu activation function called parametric relu prelu proposed kaiming et al figure 25 show pictorial representation leaky relu elu activation function technique automatically learn parameter adaptively improve accuracy negligible extra computing cost 144 b figure diagram leaky relu rectified linear unit b exponential linear unit elu leaky relu max ùëéùë• ùë• 22 ùëé constant value elu ùë• ùëé ùë• 0 23 recent proposal exponential linear unit activation function allowed faster accurate version dcnn structure 148 furthermore tuning negative part activation function creates leaky relu multiple exponent linear unit melu proposed recently 149 shape rectified linear activation unit proposed 2015 150 survey modern activation function wa conducted 2015 151 figure diagram leaky relu rectiÔ¨Åed linear unit b exponential linear unit elu electronics 2019 8 x peer review 26 67 figure average operation layer pooling layer present two different technique used implementation deep network pooling layer average concept average pooling layer wa used first time lenet 55 alexnet used layer instead 2012 7 conceptual diagram max pooling average pooling operation shown figure concept special pyramid pooling ha proposed et al 2014 shown figure 26 152 pyramid pooling wa proposed 2015 153 2015 benjamin proposed new architecture fractional max pooling provides classification accuracy datasets structure generalizes network considering two important property layer pooling layer first pooling layer limit generalize deep structure network paper proposed network overlapped instead layer 154 another paper ha conducted research different type pooling approach including mixed gated tree generalization pooling function 155 figure average operation leaky relu max ax x 22 constant value elu x x ex x 0 23 recent proposal exponential linear unit activation function allowed faster accurate version dcnn structure 148 furthermore tuning negative part activation function creates leaky relu multiple exponent linear unit melu proposed recently 149 shape rectiÔ¨Åed linear activation unit proposed 2015 150 survey modern activation function wa conducted 2015 151 electronics 2019 8 292 26 66 layer pooling layer present two different technique used implementation deep network pooling layer average concept average pooling layer wa used Ô¨Årst time lenet 55 alexnet used layer instead 2012 7 conceptual diagram max pooling average pooling operation shown figure concept special pyramid pooling ha proposed et al 2014 shown figure 26 152 present two different technique used implementation deep network pooling layer average concept average pooling layer wa used first time lenet 55 alexnet used layer instead 2012 7 conceptual diagram max pooling average pooling operation shown figure concept special pyramid pooling ha proposed et al 2014 shown figure 26 152 pyramid pooling wa proposed 2015 153 2015 benjamin proposed new architecture fractional max pooling provides classification accuracy datasets structure generalizes network considering two important property layer pooling layer first pooling layer limit generalize deep structure network paper proposed network overlapped instead layer 154 another paper ha conducted research different type pooling approach including mixed gated tree generalization pooling function 155 figure spatial pyramid pooling regularization approach dl different regularization approach proposed past year deep cnn simplest efficient approach called dropout wa proposed hinton 2012 156 dropout randomly selected subset activation set zero within layer 157 dropout concept shown figure figure spatial pyramid pooling pyramid pooling wa proposed 2015 153 2015 benjamin proposed new architecture fractional max pooling provides classiÔ¨Åcation accuracy datasets structure generalizes network considering two important property layer pooling layer first layer limit generalize deep structure network paper proposed network 3 3 overlapped instead 2 2 layer 154 another paper ha conducted research different type pooling approach including mixed gated tree generalization pooling function 155 regularization approach dl different regularization approach proposed past year deep cnn simplest efÔ¨Åcient approach called dropout wa proposed hinton 2012 156 dropout randomly selected subset activation set zero within layer 157 dropout concept shown figure electronics 2019 8 x peer review 27 67 figure pictorial representation concept dropout another regularization approach called drop connect case instead dropping activation subset weight within network layer set zero result layer receives randomly selected subset unit immediate previous layer 158 some regularization approach proposed well 159 optimization method dl different optimization method sgd adagrad adadelta rmsprop adam 160 some activation function improved upon case sgd wa proposed added variable momentum improved training testing accuracy h f ad h b l l l figure pictorial representation concept dropout another regularization approach called drop connect case instead dropping activation subset weight within network layer set zero result layer receives randomly selected subset unit immediate previous layer 158 some regularization approach proposed well 159 electronics 2019 8 292 27 66 optimization method dl different optimization method sgd adagrad adadelta rmsprop adam 160 some activation function improved upon case sgd wa proposed added variable momentum improved training testing accuracy case adagrad main contribution wa calculate adaptive learning rate training method summation magnitude gradient considered calculate adaptive learning rate case large number epoch summation magnitude gradient becomes large result learning rate decrease radically cause gradient approach zero quickly main drawback approach cause problem training later rmsprop wa proposed considering only magnitude gradient immediately previous iteration prevents problem adagrad provides better performance some case adam optimization approach proposed based momentum magnitude gradient calculating adaptive learning rate similar rmsprop adam ha improved overall accuracy help efÔ¨Åcient training better convergence deep learning algorithm 161 improved version adam optimization approach ha proposed recently called eve eve provides even better performance fast accurate convergence 162 recurrent neural network rnn introduction human thought persistence human throw thing away start thinking scratch second reading article understand word sentence based understanding previous word sentence traditional neural network approach including dnns cnns not deal type problem standard neural network cnn incapable due following reason first approach only handle vector input image video frame produce vector output probability different class second model operate Ô¨Åxed number computational step number layer model rnns unique allow operation sequence vector time hopÔ¨Åeld newark introduced concept 1982 idea wa described shortly 1974 163 pictorial representation shown figure electronics 2019 8 x peer review 28 67 figure structure basic recurrent neural network rnn loop different version rnn proposed jordan elman elman architecture us output hidden layer input alongside normal input hidden layer 129 hand output output unit used input input hidden layer jordan network 130 jordan contrast us input output output unit input hidden layer mathematically expressed elman network 1164 24 25 jordan network 165 26 27 figure structure basic recurrent neural network rnn loop different version rnn proposed jordan elman elman architecture us output hidden layer input alongside normal input hidden layer 129 hand output output unit used input input hidden layer jordan network 130 jordan contrast us input output output unit input hidden layer mathematically expressed elman network 164 ht œÉh whxt bh 24 electronics 2019 8 292 28 66 yt œÉy 25 jordan network 165 ht œÉh bh 26 yt œÉy 27 xt vector input ht hidden layer vector yt output vector w u weight matrix b bias vector loop allows information passed one step network next recurrent neural network thought multiple copy network network passing message successor diagram figure 29 show happens unroll loop architecture us output hidden layer input alongside normal input hidden layer 129 hand output output unit used input input hidden layer jordan network 130 jordan contrast us input output output unit input hidden layer mathematically expressed elman network 1164 24 25 jordan network 165 26 27 vector input hidden layer vector output vector w u weight matrix b bias vector loop allows information passed one step network next recurrent neural network thought multiple copy network network passing message successor diagram figure 29 show happens unroll loop figure unrolled rnns main problem rnn approach exists vanishing gradient problem first time problem solved hochreiter et al 166 deep rnn consisting 1000 subsequent layer wa implemented evaluated solve deep learning task 1993 167 several solution proposed solving vanishing gradient problem rnn approach past decade two possible effective solution problem first clip gradient scale gradient norm large secondly create better rnn model one better model wa introduced felix el 2000 named long memory lstm lstm different advanced approach proposed last year explained following section diagram lstm shown figure rnn approach allowed sequence input output general case example dl text mining building deep learning model textual data requires representation basic text unit word neural network structure hierarchically capture sequential nature text case rnns recursive neural network figure unrolled rnns main problem rnn approach exists vanishing gradient problem Ô¨Årst time problem solved hochreiter et al 166 deep rnn consisting 1000 subsequent layer wa implemented evaluated solve deep learning task 1993 167 several solution proposed solving vanishing gradient problem rnn approach past decade two possible effective solution problem Ô¨Årst clip gradient scale gradient norm large secondly create better rnn model one better model wa introduced felix et al 2000 named long memory lstm lstm different advanced approach proposed last year explained following section diagram lstm shown figure electronics 2019 8 x peer review 29 67 used language understanding 170 language modeling try predict next word set word some case sentence based previous one 171 rnns network loop allowing information persist another example rnns able connect previous information present task using previous video frame understanding present trying generate future frame well 172 figure diagram long memory lstm long memory lstm key idea lstms cell state horizontal line running top figure lstms remove add information cell state called gate input gate forget gate output gate defined œÉ 28 œÉ 29 c tanh 30 c 31 œÉ 32 33 lstm model popular temporal information processing paper include figure diagram long memory lstm rnn approach allowed sequence input output general case example dl text mining building deep learning model textual data requires representation basic text unit word neural network structure hierarchically capture sequential nature text case rnns recursive neural network used language understanding 170 language modeling try predict next word set word some case sentence based previous one 171 rnns network loop allowing information persist another example rnns able connect previous information present task using previous video frame understanding present trying generate future frame well 172 electronics 2019 8 292 29 66 long memory lstm key idea lstms cell state horizontal line running top figure lstms remove add information cell state called gate input gate forget gate ft output gate ot deÔ¨Åned ft œÉ xt bf 28 œÉ xt bi 29 e ct tan h xt bc 30 ct ft ct 31 ot œÉ xt bo 32 ht h ct 33 lstm model popular temporal information processing paper include lstm model some minor variance some discussed following section slightly modiÔ¨Åed version network peephole connection gers schimidhuber proposed 2000 168 concept peephole included almost gated model figure diagram long memory lstm long memory lstm key idea lstms cell state horizontal line running top figure lstms remove add information cell state called gate input gate forget gate output gate defined œÉ 28 œÉ 29 c tanh 30 c 31 œÉ 32 33 lstm model popular temporal information processing paper include lstm model some minor variance some discussed following section slightly modified version network peephole connection gers schimidhuber proposed 2000 168 concept peephole included almost gated model figure diagram gated recurrent unit gru gated recurrent unit gru gru also came lstms slightly variation 173 grus popular community working recurrent network main reason popularity computation cost simplicity model shown figure grus lighter version figure diagram gated recurrent unit gru gated recurrent unit gru gru also came lstms slightly variation 173 grus popular community working recurrent network main reason popularity computation cost simplicity model shown figure grus lighter version rnn approach standard lstm term topology computation cost complexity 173 technique combine forget input gate single update gate merges cell state hidden state along some change simpler model gru ha growing increasingly popular mathematically gru expressed following equation zt œÉ xt 34 rt œÉ xt 35 e ht tan h rt xt 36 ht 1 zt ht 37 question one best according different empirical study no clear evidence winner however gru requires fewer network parameter make model faster hand lstm provides better performance enough data computational power 174 variant lstm named deep lstm 175 another variant electronics 2019 8 292 30 66 bit different approach called clockwork rnn 176 important empirical evaluation different version rnn approach including lstm greff et al 2015 177 Ô¨Ånal conclusion wa lstm variant 177 another empirical evaluation conducted thousand rnn architecture including lstm gru Ô¨Ånding some worked better lstms certain task 178 convolutional lstm convlstm problem fully connected fc lstm short model handling spatiotemporal data usage full connection transaction no spatial information ha encoded internal gate convlstm tensor last two dimension spatial dimension row column convlstm determines future state certain cell grid respect input past state local neighbor achieved using convolution operation transition shown figure tanh 36 1 h 37 question one best according different empirical study no clear evidence winner however gru requires fewer network parameter make model faster hand lstm provides better performance enough data computational power 174 variant lstm named deep lstm 175 another variant bit different approach called clockwork rnn 176 important empirical evaluation different version rnn approach including lstm greff et al 2015 177 final conclusion wa lstm variant 177 another empirical evaluation conducted thousand rnn architecture including lstm gru finding some worked better lstms certain task 178 convolutional lstm convlstm problem fully connected fc lstm short model handling spatiotemporal data usage full connection transaction no spatial information ha encoded internal gate convlstm tensor last two dimension spatial dimension row column convlstm determines future state certain cell grid respect input past state local neighbor achieved using convolution operation transition shown figure figure pictorial diagram convlstm convlstm providing good performance temporal data analysis video datasets 172 mathematically convlstm expressed follows represents convolution operation denotes hadamard product œÉ 38 œÉ 39 tanh 40 ‡∑© 41 œÉ 42 43 variant architecture rnn respective application figure pictorial diagram convlstm convlstm providing good performance temporal data analysis video datasets 172 mathematically convlstm expressed follows represents convolution operation hadamard product œÉ wxi xt whi whi bi 38 ft œÉ wxf xt whf whf bf 39 e ct tan h wxc xt whc bc 40 ct ft ct 41 ot œÉ wxo xt bo 42 ht ot h ct 43 variant architecture rnn respective application incorporate attention mechanism rnns used case word sentence encoding powerful word embedding technique predictive nn raw text input approach used different Ô¨Åelds application including unsupervised learning word relationship learning different word ability abstract higher meaning word based similarity sentence modeling language understanding many different word embedding approach proposed past year used solve difÔ¨Åcult task provide performance including machine translation language modeling image video captioning time series data analysis application point view rnns solve different type problem need different architecture rnns shown figure figure 33 input vector represented electronics 2019 8 292 31 66 green rnn state represented blue orange represents output vector structure described one one standard mode classiÔ¨Åcation without rnn image classiÔ¨Åcation problem shown figure many one sequence input single output sentiment analysis input set sentence word output positive negative expression shown figure one many system take input produce sequence output image captioning problem input single image output set word context shown figure many many sequence input output machine translation machine take sequence word english translates sequence word french shown figure many many sequence sequence learning video classiÔ¨Åcation problem take video frame input wish label frame video shown figure ability abstract higher meaning word based similarity sentence modeling language understanding many different word embedding approach proposed past year used solve difficult task provide art performance including machine translation language modeling image video captioning time series data analysis application point view rnns solve different type problem need different architecture rnns shown figure figure 33 input vector represented green rnn state represented blue orange represents output vector structure described one one standard mode classification without rnn image classification problem shown figure 33 many one sequence input single output sentiment analysis input set sentence word output positive negative expression shown figure one many system take input produce sequence output image captioning problem input single image output set word context shown figure many many sequence input output machine translation machine take sequence word english translates sequence word french shown figure many many sequence sequence learning video classification problem take video frame input wish label frame video shown figure b c e figure different structure rnn respect application one one b many one c one many many many e many many figure different structure rnn respect application one one b many one c one many many many e many many model rnn different model proposed using rnn approach Ô¨Årst initiative rnns attention automatically learns describe content image proposed xu et al 2015 182 dual state attention based rnn proposed effective time series prediction 183 another difÔ¨Åcult task visual question answering vqa using grus input image natural language question image task provide accurate natural language answer output conditional image textual input cnn used encode image rnn implemented encode sentence 184 another outstanding concept released google called pixel recurrent neural network pixel rnn approach provides performance image completion task 185 new model called residual rnn proposed rnn introduced effective residual connection deep recurrent network 186 electronics 2019 8 292 32 66 rnn application rnns including lstm gru applied tensor processing 187 natural language processing using rnn technique including lstms grus convolutional rnns based identiÔ¨Åcation system ha proposed 2017 190 time series data analysis using rnns 191 recently timenet wa proposed based deep rnns time series classiÔ¨Åcation tsc 192 speech audio processing including lstms acoustic modeling sound event prediction using convolutional rnns 195 audio tagging using convolutional grus 196 early heart failure detection proposed using rnns 197 rnns applied tracking monitoring trafÔ¨Åc forecasting system proposed using graph convolutional rnn gcrnn 25 lstm based network trafÔ¨Åc prediction system proposed neural model 198 bidirectional deep rnn applied driver action prediction 199 vehicle trajectory prediction using rnn 200 action recognition using rnn 201 collection anomaly detection using lstms cybersecurity 202 ae restricted boltzmann machine rbm section discussing one unsupervised deep learning approach auto encoder 61 variational vae 203 denoising ae 65 sparse ae 204 stacked denoising ae 205 ae 206 application different ae also discussed end chapter review ae ae deep neural network approach used unsupervised feature learning efÔ¨Åcient data encoding decoding main objective autoencoder learn represent encoding input data typically data dimensionality reduction compression fusion many autoencoder technique consists two part encoder decoder encoding phase input sample mapped usually lower dimensional feature space constructive feature representation approach repeated desired feature dimensional space reached whereas decoding phase regenerate actual feature lower dimensional feature reverse processing conceptual diagram encoding decoding phase shown figure electronics 2019 8 x peer review 33 67 figure diagram auto encoder encoder decoder transition represented ùúë ùí≥ ‡∞ù ùúë 44 consider simple autoencoder one hidden layer input ùí≥ mapped onto ‚Ñ± expressed follows ùúé‡¨µ ùëè 45 w weight matrix b bias ùúé‡¨µ represents element wise activation function figure diagram auto encoder encoder decoder transition represented œï x œï f œï œï œï 44 electronics 2019 8 292 33 66 consider simple autoencoder one hidden layer input x x mapped onto f expressed follows z wx b 45 w weight matrix b bias represents element wise activation function sigmoid rectiÔ¨Åed linear unit rlu let u consider z mapped reconstructed onto dimension reconstruction expressed 46 model trained minimizing reconstruction error deÔ¨Åned loss function follows l wx b 47 usually feature space f ha lower dimension input feature space x considered compressed representation input sample case multilayer auto encoder operation repeated required encoding decoding phase deep auto encoder constructed extending encoder decoder multiple hidden layer gradient vanishing problem still big issue deeper model ae gradient becomes small pass back many layer ae model different advanced ae model discussed following section variational autoencoders v aes some limitation using simple generative adversarial network gan discussed section Ô¨Årst image generated using gan input noise someone want generate speciÔ¨Åc image difÔ¨Åcult select speciÔ¨Åc feature noise randomly produce desired image requires searching entire distribution second gans differentiate real fake object example want generate dog no constraint dog must look like dog therefore produce style image style look like dog closely observed not exactly however vae proposed overcome limitation basic gans latent vector space used represent image follow unit gaussian distribution conceptual diagram vae shown figure electronics 2019 8 x peer review 34 67 figure variational model two loss one mean squared error determines good network reconstructing image loss kl divergence latent determines closely latent variable match unit gaussian distribution example suppose ùë• input hidden representation parameter weight bias reconstructing phase input ùëß desired output parameter weight bias represent encoder ùëû‡∞è decoder ùëù‡∞• respectively loss function network latent space represented ùúÉ ùúô ùëù ùëß ‡µØ 48 autoencoder recently ae wa proposed berkeley ai research bair lab figure variational model two loss one mean squared error determines good network reconstructing image loss kl divergence latent determines closely latent variable match unit gaussian distribution example suppose x input hidden representation parameter weight bias reconstructing phase input z desired output parameter weight bias represent encoder qŒ∏ decoder pœÜ respectively loss function network latent space represented electronics 2019 8 292 34 66 li Œ∏ œÜ logpœÜ kl qŒ∏ p z 48 autoencoder recently ae wa proposed berkeley ai research bair lab architectural modiÔ¨Åcation traditional autoencoders unsupervised representation learning architecture network split disjoint two network try predict feature representation entire image 206 following figure 36 show concept autoencoder latent determines closely latent variable match unit gaussian distribution example suppose ùë• input hidden representation parameter weight bias reconstructing phase input ùëß desired output parameter weight bias represent encoder ùëû‡∞è decoder ùëù‡∞• respectively loss function network latent space represented ùúÉ ùúô ùëù ùëß ‡µØ 48 autoencoder recently ae wa proposed berkeley ai research bair lab architectural modification traditional autoencoders unsupervised representation learning architecture network split disjoint two network try predict feature representation entire image 206 following figure 36 show concept autoencoder figure autoencoder application ae ae applied cybersecurity 209 apply ae unsupervised feature extraction apply winner take wta clustering sample generating label 210 ae ha used encoding decoding technique deep learning approach including cnn dnn rnn rl last decade however some approach recently published review rbm restricted boltzmann machine rbm another unsupervised deep learning approach training phase modeled using network called restricted boltzmann machine 212 stochastic binary pixel connected stochastic binary feature detector using symmetrically weighted connection rbm undirected generative model us layer hidden variable model distribution visible variable undirected model interaction hidden visible variable used ensure contribution figure autoencoder application ae ae applied cybersecurity 209 apply ae unsupervised feature extraction apply winner take wta clustering sample generating label 210 ae ha used encoding decoding technique deep learning approach including cnn dnn rnn rl last decade however some approach recently published review rbm restricted boltzmann machine rbm another unsupervised deep learning approach training phase modeled using network called restricted boltzmann machine 212 stochastic binary pixel connected stochastic binary feature detector using symmetrically weighted connection rbm undirected generative model us layer hidden variable model distribution visible variable undirected model interaction hidden visible variable used ensure contribution likelihood term posterior hidden variable approximately factorial greatly facilitates inference 213 conceptual diagram rbm shown figure electronics 2019 8 x peer review 35 67 likelihood term posterior hidden variable approximately factorial greatly facilitates inference 213 conceptual diagram rbm shown figure figure block diagram restricted boltzmann machine rbm model mean probability distribution variable interest defined energy function energy function composed set observable variable set hidden variable node visible layer j node hidden layer restricted sense no connection value corresponding visible unit rbm state observed feature detector correspond hidden unit joint configuration v h visible hidden figure block diagram restricted boltzmann machine rbm electronics 2019 8 292 35 66 model mean probability distribution variable interest deÔ¨Åned energy function energy function composed set observable variable v vi set hidden variable hi node visible layer j node hidden layer restricted sense no connection value corresponding visible unit rbm state observed feature detector correspond hidden unit joint conÔ¨Åguration v h visible hidden unit ha energy hopÔ¨Åeld 1982 given e v h aivi j bjhj j viwi jhj 49 vi hj binary state visible unit hidden unit j ai bj bias wij weight network assigns probability possible pair visible hidden vector via energy function p v h 1 z v h 50 partition function z given summing possible pair visible hidden vector z v h v h 51 probability network assigns visible vector v given summing possible hidden vector p v 1 z h v h 52 probability network assigns training sample raised adjusting weight bias lower energy sample raise energy sample especially low energy therefore make big contribution partition function derivative log probability training vector respect weight surprisingly simple v vihj data vihj model 53 angle bracket used denote expectation distribution speciÔ¨Åed subscript follows lead simple learning rule performing stochastic steepest ascent log probability training data wij Œµ vihj data vihj model 54 Œµ learning rate given randomly selected training image v binary state hj hidden unit j set 1 probability p œÉ bj viwij 55 œÉ x logistic sigmoid function 1 e vihj unbiased sample no direct connection visible unit rbm also easy get unbiased sample state visible unit given hidden vector p vi œÉ ai j hjwij 56 electronics 2019 8 292 36 66 getting unbiased sample vihj model much difÔ¨Åcult done starting any random state visible unit performing alternating gibbs sampling long time single iteration alternating gibbs sampling consists updating hidden unit parallel using equation 55 followed updating visible unit parallel using following equation 56 much faster learning procedure wa proposed hinton 2002 start setting state visible unit training vector binary state hidden unit computed parallel using equation 55 binary state chosen hidden unit reconstruction produced setting vi 1 probability given equation 56 change weight given Œµ vihj data vihj recon 57 simpliÔ¨Åed version learning rule us state individual unit instead pairwise product used bias 214 approach mainly used neural network unsupervised manner generate initial weight one popular deep learning approach called deep belief network dbn proposed based approach some example application rbm dbn data encoding news clustering image segmentation cybersecurity shown detail see reference generative adversarial network gan beginning chapter started quote yann lecun gan best concept proposed last ten year Ô¨Åeld deep learning neural network review gan concept generative model machine learning started long time used data modeling conditional probability density function generally type model considered probabilistic model joint probability distribution observation target label value however not see big success generative model recently deep generative model become popular shown enormous success different application domain deep learning technique performs better number input sample increased due reason learning reusable feature representation huge number dataset ha become active research area mentioned introduction computer vision ha different task segmentation classiÔ¨Åcation detection requires large amount labeled data problem ha attempted solved generating similar sample generative model generative adversarial network gan deep learning approach recently invented goodfellow gans offer alternative approach maximum likelihood estimation technique gan unsupervised deep learning approach two neural network compete game case image generation problem generator start gaussian noise generate image discriminator determines good generated image process continues output generator become close actual input sample according figure 38 considered discriminator generator g two player playing game function v g expressed follows according paper mingmaxd v g x log x z log 1 g z 58 practice equation may not provide sufÔ¨Åcient gradient learning g started random gaussian noise early stage early stage reject sample clearly different compared training sample case log 1 g z saturated instead training g minimize log 1 g z train g maximize log g z objective electronics 2019 8 292 37 66 function provides much better gradient early stage learning however some limitation convergence training Ô¨Årst version beginning state gan ha some limitation regarding following issue lack heuristic cost function approximate mean square error mse unstable train sometimes producing nonsensical output electronics 2019 8 x peer review 37 67 technique gan unsupervised deep learning approach two neural network compete game case image generation problem generator start gaussian noise generate image discriminator determines good generated image process continues output generator become close actual input sample according figure 38 considered discriminator generator g two player playing game function v g expressed follows according paper ùëâ ùê∑ ùê∫ ‡Ø´ ·àæùëôùëúùëî ùê∑ ùë• ‡Ø≠ ·àæùëôùëúùëî 1 ùê∫ ùëß ·àø 58 figure conceptual diagram generative adversarial network gan practice equation may not provide sufficient gradient learning g started random gaussian noise early stage early stage reject sample clearly different compared training sample case ùëôùëúùëî 1 ùê∫ ùëß saturated instead training g minimize ùëôùëúùëî 1 ùê∫ ùëß train g maximize ùëôùëúùëî ùê∫ ùëß objective function provides much better gradient early stage learning however some limitation convergence training first version beginning state gan ha some limitation regarding following issue lack heuristic cost function approximate mean square error mse unstable train sometimes producing nonsensical output research area gans ha ongoing many improved version proposed 219 gans able produce photorealistic image application visualization interior industrial design shoe bag clothing item gan also extensively used field game development artificial video generation 220 gans two different area dl fall unsupervised some research area focus topology gan architecture improve functionality training approach deep convolution gan dcgan gan approach proposed 2015 221 approach ha shown promised result compared unsupervised counterpart regenerated result dcgan shown following figure 183 figure 39 according article 221 show output generated bedroom image one training pas dataset figure included section generated experiment theoretically model could learn memorize training example experimentally unlikely train small learning rate mini batch sgd aware no prior empirical evidence demonstrating memorization sgd small learning rate 221 figure conceptual diagram generative adversarial network gan research area gans ha ongoing many improved version proposed 219 gans able produce photorealistic image application visualization interior industrial design shoe bag clothing item gan also extensively used Ô¨Åeld game development artiÔ¨Åcial video generation 220 gans two different area dl fall unsupervised some research area focus topology gan architecture improve functionality training approach deep convolution gan dcgan gan approach proposed 2015 221 approach ha shown promised result compared unsupervised counterpart regenerated result dcgan shown following Ô¨Ågures 183 figure 39 according article 221 show output generated bedroom image one training pas dataset Ô¨Ågures included section generated experiment theoretically model could learn memorize training example experimentally unlikely train small learning rate mini batch sgd aware no prior empirical evidence demonstrating memorization sgd small learning rate 221 electronics 2019 8 x peer review 38 67 figure experimental output bedroom image figure 40 represents generated bedroom image five epoch training appears evidence visual via repeated noise texture across multiple sample baseboard some bed figure experimental output bedroom image figure 40 represents generated bedroom image Ô¨Åve epoch training appears evidence visual via repeated noise texture across multiple sample baseboard some bed electronics 2019 8 292 38 66 figure experimental output bedroom image figure 40 represents generated bedroom image five epoch training appears evidence visual via repeated noise texture across multiple sample baseboard some bed figure reconstructed bedroom image using deep convolution gan dcgan figure 40 according article 221 top row interpolation series nine random point z show learned space ha smooth transition every image space plausibly look like bedroom row see room without window slowly transforming room giant window row see appears tv slowly transformed window following figure 41 show effective application latent space vector latent space vector turned meaning output first performing addition subtraction operation followed decode figure 41 according article 221 show man glass minus man add woman result woman glass figure example smile arithmetic arithmetic wearing glass using gan man glass minus man without glass plus woman without glass equal woman glass figure 42 according article 221 show turn vector wa created four averaged sample face looking left versus looking right adding interpolation along axis random sample pose reliably transformed some interesting application proposed gans example natural indoor scene generated improved gan structure gans learn surface normal combined style gan wang figure reconstructed bedroom image using deep convolution gan dcgan figure 40 according article 221 top row interpolation series nine random point z show learned space ha smooth transition every image space plausibly look like bedroom row see room without window slowly transforming room giant window row see appears tv slowly transformed window following figure 41 show effective application latent space vector latent space vector turned meaning output Ô¨Årst performing addition subtraction operation followed decode figure 41 according article 221 show man glass minus man add woman result woman glass figure experimental output bedroom image figure 40 represents generated bedroom image five epoch training appears evidence visual via repeated noise texture across multiple sample baseboard some bed figure reconstructed bedroom image using deep convolution gan dcgan figure 40 according article 221 top row interpolation series nine random point z show learned space ha smooth transition every image space plausibly look like bedroom row see room without window slowly transforming room giant window row see appears tv slowly transformed window following figure 41 show effective application latent space vector latent space vector turned meaning output first performing addition subtraction operation followed decode figure 41 according article 221 show man glass minus man add woman result woman glass figure example smile arithmetic arithmetic wearing glass using gan man glass minus man without glass plus woman without glass equal woman glass figure 42 according article 221 show turn vector wa created four averaged sample face looking left versus looking right adding interpolation along axis random sample pose reliably transformed some interesting application proposed gans example natural indoor scene generated improved gan structure gans learn surface normal combined style gan wang figure example smile arithmetic arithmetic wearing glass using gan man glass minus man without glass plus woman without glass equal woman glass figure 42 according article 221 show turn vector wa created four averaged sample face looking left versus looking right adding interpolation along axis random sample pose reliably transformed some interesting application proposed gans example natural indoor scene generated improved gan structure gans learn surface normal combined style gan wang gupta 222 implementation author considered style structure gan named generates surface normal map improved version gan 2016 extension gan called infogan wa proposed infogan learn better representation completely unsupervised manner experimental result show unsupervised infogan competitive representation learning fully supervised learning approach 223 2016 another new architecture wa proposed im et al 224 recurrent concept included adversarial network training chen et al 225 proposed info gan igan allowed image manipulation interactively natural image manifold image image translation conditional adversarial network proposed another improved version gans named coupled generative adversarial network cogan learned joint distribution image existing approach doe not need tuples corresponding image different domain training set 226 bidirectional generative adversarial network bigans learned inverse feature mapping shown resulting learned feature representation electronics 2019 8 292 39 66 useful auxiliary supervised discrimination task competitive contemporary approach feature learning 227 included adversarial network training chen et al 225 proposed info gan igan allowed image manipulation interactively natural image manifold image image translation conditional adversarial network proposed another improved version gans named coupled generative adversarial network cogan learned joint distribution image existing approach doe not need tuples corresponding image different domain training set 226 bidirectional generative adversarial network bigans learned inverse feature mapping shown resulting learned feature representation useful auxiliary supervised discrimination task competitive contemporary approach feature learning 227 figure face generation different angle using gan recently google proposed extended version gans called boundary equilibrium generative adversarial network began simple robust architecture 228 began ha better training procedure fast stable convergence concept equilibrium help balance power discriminator generator addition balance image diversity visual quality 228 another similar work called wasserstein gan wgan algorithm show significant benefit traditional gan 229 wgans two major benefit traditional gans first wgan meaningfully correlate loss metric generator convergence sample quality secondly wgans improved stability optimization process improved version wgan proposed new clipping technique penalizes normal gradient critic respect input 230 promising architecture ha proposed based generative model image represented untrained dnn give opportunity better understanding visualization dnns 231 adversarial example generative model also introduced 232 gan wa proposed yann lecun facebook 2016 233 training process difficult gans manifold matching gan mmgan proposed better training process experimented three different datasets experimental result clearly demonstrate efficacy mmgan model 234 gan simulation inversion efficient training approach 235 probabilistic gan pgan new kind gan modified objective function main idea behind method integrate probabilistic model gaussian mixture model gan framework support likelihood rather classification 236 gan bayesian figure face generation different angle using gan recently google proposed extended version gans called boundary equilibrium generative adversarial network began simple robust architecture 228 began ha better training procedure fast stable convergence concept equilibrium help balance power discriminator generator addition balance image diversity visual quality 228 another similar work called wasserstein gan wgan algorithm show signiÔ¨Åcant beneÔ¨Åts traditional gan 229 wgans two major beneÔ¨Åts traditional gans first wgan meaningfully correlate loss metric generator convergence sample quality secondly wgans improved stability optimization process improved version wgan proposed new clipping technique penalizes normal gradient critic respect input 230 promising architecture ha proposed based generative model image represented untrained dnn give opportunity better understanding visualization dnns 231 adversarial example generative model also introduced 232 gan wa proposed yann lecun facebook 2016 233 training process difÔ¨Åcult gans manifold matching gan mmgan proposed better training process experimented three different datasets experimental result clearly demonstrate efÔ¨Åcacy mmgan model 234 gan simulation inversion efÔ¨Åcient training approach 235 probabilistic gan pgan new kind gan modiÔ¨Åed objective function main idea behind method integrate probabilistic model gaussian mixture model gan framework support likelihood rather classiÔ¨Åcation 236 gan bayesian network model 237 variational auto encode popular deep learning approach trained adversarial variational bayes avb help establish principle connection vae gan 238 proposed based general neural network 239 markov gan texture synthesis 240 another generative model based doubly stochastic mcmc method 241 gan 242 unsupervised gan capable learning pixel level domain adaptation transforms pixel space one domain another domain approach provides performance several unsupervised domain adaptation technique large margin 243 new network proposed called schema network generative physic simulator able disentangle multiple cause event reasoning cause achieve goal learned dynamic environment data 244 interesting research ha conducted gan generate adversarial text image synthesis paper new deep architecture proposed gan formulation take text description image produce realistic image respect input effective technique electronics 2019 8 292 40 66 image synthesis using character level text encoder class conditional gan gan evaluated bird Ô¨Çower dataset Ô¨Årst general text image evaluated coco dataset 40 application gan learning algorithm ha applied different domain application discussed following section gan image processing gans used generating image using approach 245 gan semantic segmentation semi weakly supervised approach 246 text conditioned auxiliary classiÔ¨Åer gan used generating synthesizing image text description 247 generative network retains functionality approach fast speed network match image style multiple scale put computational burden training 248 time vision system struggle rain snow fog single image system proposed using gan recently 249 gan speech audio processing dialogue system using generative hierarchical neural network model 250 addition gans used Ô¨Åeld speech analysis recently gans used speech enhancement called segan incorporates design improve performance progressively 251 gan music generation performs comparably melody rnn 252 gan medical information processing gans medical imagining medical information processing 136 gans medical image wasserstein distance perceptual loss 253 gans also used segmentation brain tumor conditional gans cgan 254 general medical image segmentation approach proposed using gan called segan 255 deep learning revolution compressive sensing one hottest topic however deep gan used compressed sensing automates mri 256 addition gans also used health record processing due privacy issue electronic health record ehr limited not publicly available like datasets gans applied synthetic ehr data could mitigate risk 257 time series data generation recurrent gan rgan recurrent conditional gan rcgan ha introduced 258 logan consists combination generative discriminative model detecting overÔ¨Åtting recognition input technique ha compared gan technique including gan dcgan began combination dcgan vae 259 application new approach called bayesian conditional gan generate sample deterministic input simply gan bayesian framework handle supervised unsupervised learning problem machine learning deep learning community online learning important approach gans used online learning trained Ô¨Ånding mixed strategy game named checkov gan 1 262 generative moment matching network based statistical hypothesis testing called maximum mean discrepancy mmd 263 one interesting idea replace discriminator gan based kernel mmd called approach signiÔ¨Åcantly outperforms electronics 2019 8 292 41 66 generative moment matching network gmmn technique alternative approach generative model 264 some application gan include pose estimation 265 photo editing network 266 anomaly detection 267 discogan learning relation gan 40 unsupervised translation generative model 268 single shot learning gan 269 response generation question answering system last not least wavenet generative model ha developed generating audio waveform 272 dual path network 273 deep reinforcement learning drl previous section focused supervised unsupervised deep learning approach including dnn cnn rnn including lstm gru ae rbm gan etc type deep learning approach used prediction classiÔ¨Åcation encoding decoding data generation many application domain however section demonstrates survey deep reinforcement learning drl based recently developed method Ô¨Åeld rl review drl drl learning approach learns act general sense unknown real environment detail please read following article conceptual diagram drl approach shown figure rl applied different scope Ô¨Åeld including fundamental science decision making machine learning computer science point view Ô¨Åeld engineering mathematics optimal control robotics control power station control wind turbine neuroscience reward strategy widely studied last couple decade also applied economic utility game theory making better decision investment choice psychological concept classical conditioning animal learn reinforcement learning technique match situation action reinforcement learning different supervised learning technique kind learning approach study recently including traditional machine learning statistical pattern recognition ann electronics 2019 8 x peer review 42 67 figure conceptual diagram reinforcement learning rl system unlike general supervised unsupervised machine learning rl defined not characterizing learning method characterizing learning problem however recent success dl ha huge impact success drl known drl according learning strategy rl technique learned observation observing environment promising dl technique include cnn rnn lstm gru used depending upon observation space dl technique encode data efficiently therefore following step action performed accurately according action agent receives appropriate reward respectively result entire rl approach becomes efficient learn interact environment better performance however history modern drl revolution began google deep mind 2013 atari game drl drl based approach perform better human expert almost game case environment observed video frame processed using cnn success drl approach depends level difficulty task attempt solved huge success atari google deep mind proposed reinforcement learning environment based starcraft ii 2017 called figure conceptual diagram reinforcement learning rl system unlike general supervised unsupervised machine learning rl deÔ¨Åned not characterizing learning method characterizing learning problem however recent success dl ha huge impact success drl known drl according learning strategy rl technique learned observation observing environment promising dl technique include cnn rnn lstm gru used depending upon observation space dl technique encode data efÔ¨Åciently therefore following step action performed accurately according action agent receives appropriate reward respectively result entire rl approach becomes efÔ¨Åcient learn interact environment better performance electronics 2019 8 292 42 66 however history modern drl revolution began google deep mind 2013 atari game drl drl based approach perform better human expert almost game case environment observed video frame processed using cnn success drl approach depends level difÔ¨Åculty task attempt solved huge success atari google deep mind proposed reinforcement learning environment based starcraft ii 2017 called starcraft ii learning environment 277 game multiple player interaction proposed approach ha large action space involving selection control hundred unit contains many state observe raw feature space us strategy thousand step open source starcraft ii game engine ha provided free online some fundamental strategy essential know working drl first rl learning approach ha function calculates quality combination called algorithm 2 describes basic computational Ô¨Çow deÔ¨Åned reinforcement learning approach used Ô¨Ånd optimal policy any given Ô¨Ånite markov decision process mdp mdp mathematical framework modeling decision using state action reward only need know state available possible action state another improved version known article discussed detail please see reference 278 step choose action maximizes following function q q estimated utility tell u good action given certain state r immediate reward making action best utility q resulting state formulated recursive deÔ¨Ånition follows q r Œ≥ 59 equation called bellman equation core equation rl r immediate reward Œ≥ relative value delay immediate reward 0 1 new state action action sate respectively action selected based following equation œÄ argmaxaq 60 state value assigned called visit state receive reward accordingly use reward update estimated value state reward stochastic result need visit state many time addition not guaranteed get reward rt another episode summation future reward episodic task environment unpredictable future go reward diversely expressed gt rt 61 sum discounted future reward case some factor scalar gt Œ≥ Œ≥trt 62 Œ≥ constant future le take reward account property convergence approximation converged true must visit possible pair inÔ¨Ånitely many time electronics 2019 8 292 43 66 state table size vary depending observation space complexity unseen value not considered observation way Ô¨Åx problem use neural network particularly dnn approximation instead state table input dnn state action output number 0 1 represent utility encoding state action properly place deep learning approach contribute making better decision respect state information case observing environment use several acquisition device including camera sensing device observing learning environment example observed setup challenge seen environment action reward learned based pixel value pixel action detail see reference however difÔ¨Åcult develop agent interact perform well any observation environment therefore researcher Ô¨Åeld select action space environment training agent environment benchmark concept case little bit different compared supervised unsupervised deep learning approach due variety environment benchmark depends level difÔ¨Åculty environment ha considered compared previous exiting research difÔ¨Åculties depend different parameter number agent way interaction agent number player recently another good learning approach ha proposed drl many paper published different network drl including deep dqn double dqn asynchronous method policy optimization strategy including deterministic policy gradient deep deterministic policy gradient guided policy search trust region policy optimization combining policy gradient proposed policy gradient dagger superhuman go using supervised learning policy gradient monte carlo tree search value function robotics manipulation using guided policy search 281 drl game using policy gradient 282 algorithm 2 initialization pair initialize table entry ÀÜ q zero step observed current state repeat select action execute received immediate reward r observe new state update table entry ÀÜ q follows ÀÜ q r Œ≥ q recent trend drl application survey published recently basic rl drl dqn trust region policy optimization asynchronous advantage proposed paper also discus advantage deep learning focus visual understanding via rl current trend research 283 network cohesion constrained based online rl technique proposed health care mobile device called mhealth system help similar user share information efÔ¨Åciently improve convert limited user information policy 284 similar work electronics 2019 8 292 44 66 rl proposed health care mobile device personalized mhealth intervention work clustering applied grouping people Ô¨Ånally shared rl policy group 285 optimal policy learning challenging task rl agent initiation set oois allow agent learn optimal policy challenging task pomdps learned faster rnn 286 bin packing problem bpp proposed drl main objective place number item minimize surface area bin 287 import component drl reward determined based observation action agent reward function not perfect time due sensor error agent may get maximum reward whereas actual reward smaller paper proposed formulation based generalized markov decision problem mdp called corrupt reward mdp 288 trust region optimization based deep rl proposed using recently developed approximation curvature 289 addition some research ha conducted evaluation physic experiment using deep learning approach experiment focus agent learn basic property mass cohesion object interactive simulation environment 290 recently fuzzy rl policy proposed suitable continuous state action space 291 important investigation discussion made policy gradient continuous control general variance algorithm paper also provides guideline reporting result comparison baseline method 292 deep rl also applied high precision assembly task 293 bellman equation one main function rl technique function approximation proposed ensures bellman optimality equation always hold function estimated maximize likelihood observed motion 294 drl based hierarchical system used could resource allocation power management could computing system 295 novel face hallucination proposed deep rl used enhancing quality image single patch image applied face image 296 bayesian deep learning bdl dl approach providing accuracy different application however dl approach unable deal uncertainty given task due model uncertainty learning approach take input assume class probability without justification 2015 two african american human recognized gorilla image classification system 299 several application domain uncertainty raised including car application however bdn intersection dl bayesian probability approach show better result different application understand uncertainty problem including problem uncertainty estimated applying probability distribution model weight mapping output probability bdl becoming popular among dl research community addition bdl approach proposed cnn technique probability distribution applied weight technique help deal model overÔ¨Åtting problem lack training sample two common challenge dl approach finally some research paper published recently some advanced technique proposed bdl transfer learning transfer learning good way explain transfer learning look relationship teacher offer course gathering detail knowledge regarding subject 48 information electronics 2019 8 292 45 66 conveyed series lecture time considered teacher expert transferring information knowledge student learner thing happens case deep learning network trained big amount data training model learns weight bias weight transferred network testing retraining similar new model network start weight instead training scratch conceptual diagram transfer learning method shown figure justification 2015 two african american human recognized gorilla image classification system 299 several application domain uncertainty raised including car application however bdn intersection dl bayesian probability approach show better result different application understand uncertainty problem including problem uncertainty estimated applying probability distribution model weight mapping output probability bdl becoming popular among dl research community addition bdl approach proposed cnn technique probability distribution applied weight technique help deal model overfitting problem lack training sample two common challenge dl approach finally some research paper published recently some advanced technique proposed bdl figure conceptual diagram transfer learning pretrained imagenet transfer learning used retraining pascal dataset transfer learning transfer learning good way explain transfer learning look relationship teacher offer course gathering detail knowledge regarding subject 48 information conveyed series lecture time considered teacher expert transferring information knowledge student learner thing happens case deep learning network trained big amount data training model learns weight bias weight transferred network testing retraining similar new model network start weight instead training scratch conceptual diagram transfer learning method shown figure figure conceptual diagram transfer learning pretrained imagenet transfer learning used retraining pascal dataset model model model already trained domain intended domain example image recognition task inception model already trained imagenet downloaded inception model used different recognition task instead training scratch weight left some learned feature method training useful lack sample data lot model available including vgg resnet inception net different datasets following link use model lot reason using model firstly requires lot expensive computation power train big model big datasets secondly take multiple week train big model training new model weight speed convergence well help network generalization use model need consider following criterion respective application domain size dataset using weight shown table table criterion need considered transfer learning method new dataset small new dataset large model similar new dataset freeze weight train linear classiÔ¨Åer top level feature layer faster convergence better generalization model different new dataset freeze weight train linear classiÔ¨Åer feature layer enhanced convergence speed electronics 2019 8 292 46 66 working inference research group working speciÔ¨Åcally inference application look optimization approach include model compression model compression important realm mobile device special purpose hardware make model energy efÔ¨Åcient well faster myth deep learning myth need million labeled sample training deep learning model answer yes case transfer learning approach used train deep learning approach without large amount label data example following figure 44 demonstrates strategy transfer learning approach detail primary model ha trained large amount labeled data imagenet weight used train pascal dataset actual reality possible learn useful representation unlabeled data transfer learning help learned representation related task 306 take trained network different domain adapted any domain target task first training network close domain easy get labeled data using standard backpropagation example imagenet classiÔ¨Åcation pseudo class augmented data cut top layer network replace supervised objective target domain finally tune network using backpropagation label target domain validation loss start increase some survey paper book published transfer learning learning transfer learning 311 boosting approach transfer learning 312 energy efÔ¨Åcient approach hardware dl overview dnns successfully applied achieved better recognition accuracy different application domain computer vision speech processing natural language processing big data problem many however case training executed graphic processing unit gpu dealing big volume data expensive term power recently researcher training testing deeper wider network achieve even better classiÔ¨Åcation accuracy achieve human beyond human level recognition accuracy some case size neural network increasing becomes powerful provides better classiÔ¨Åcation accuracy however storage consumption memory bandwidth computational cost increasing exponentially hand type massive scale implementation large number network parameter not suitable low power implementation unmanned aerial vehicle uav different medical device low memory system mobile device field programmable gate array fpga much research going develop better network structure network lower computation cost fewer number parameter system without lowering classiÔ¨Åcation accuracy two way design efÔ¨Åcient deep network structure Ô¨Årst approach optimize internal operational cost efÔ¨Åcient network structure second design network low precision operation hardware efÔ¨Åcient network internal operation parameter network structure reduced using low dimensional convolution Ô¨Ålters convolution layer lot beneÔ¨Åt approach firstly convolutional rectiÔ¨Åcation operation make decision discriminative secondly main beneÔ¨Åt approach reduce number computation parameter drastically example one layer ha 5 5 dimensional electronics 2019 8 292 47 66 Ô¨Ålters replaced two 3 3 dimensional Ô¨Ålters without pooling layer better feature learning three 3 3 dimensional Ô¨Ålters used replacement 7 7 dimensional Ô¨Ålters beneÔ¨Åts using Ô¨Ålter assuming present convolutional layer ha c channel three layer 3 3 Ô¨Ålter total number parameter weight 3 3 3 c c weight whereas size Ô¨Ålter 7 7 total number parameter 7 7 c c almost double compared three 3 3 Ô¨Ålter parameter moreover placement layer convolutional pooling network different interval ha impact overall classiÔ¨Åcation accuracy some strategy mentioned optimize network architecture recently design robust deep learning model efÔ¨Åcient implementation cnns fpga platform 314 strategy 1 replace 3 3 Ô¨Ålter 1 1 Ô¨Ålters main reason use lower dimension Ô¨Ålter reduce overall number parameter replacing 3 3 Ô¨Ålters 1 1 reduced number parameter strategy 2 decrease number input channel 3 3 Ô¨Ålters layer size output feature map calculated related network parameter using 1 n input map size f Ô¨Ålter size stride reduce number parameter not only enough reduce size Ô¨Ålters also requires controlling number input channel featuring dimension strategy 3 late network convolution layer activation map output present convolution layer least 1 1 often larger 1 output width height controlled some criterion 1 size input sample 256 256 2 choosing post sample layer commonly pooling layer average max pooling layer used alternative layer convolution 3 3 Ô¨Ålters stride earlier layer larger stride layer small number activation map binary ternary connect neural network computation cost reduced drastically low precision multiplication multiplication drop connection paper also introduced binary connect neural network bnn ternary connect neural network tnn generally multiplication weight activation forward propagation gradient calculation backward propagation main operation deep neural network binary connect bnn technique eliminates multiplication operation converting weight used forward propagation binary constrained only two value 0 1 1 result multiplication operation performed simple addition subtraction make training process faster two way convert real value corresponding binary value deterministic stochastic case deterministic technique straightforward thresholding technique applied weight alternative way stochastic approach matrix converted binary based probability hard sigmoid function used computationally inexpensive experimental result show signiÔ¨Åcantly good recognition accuracy several advantage bnn follows observed binary multiplication gpu almost seven time faster traditional matrix multiplication gpu forward pas bnns drastically reduce memory size access replace arithmetic operation operation lead great increase power efÔ¨Åciency binarized kernel used cnns reduce around 60 complexity dedicated hardware also observed memory access typically consume energy compared arithmetic operation memory access cost increase memory size bnns beneÔ¨Åcial respect aspect electronics 2019 8 292 48 66 some technique proposed last year another power efÔ¨Åcient hardware friendly network structure ha proposed cnn xnor operation xnor based cnn implementation Ô¨Ålters input convolution layer binary result faster convolutional operation memory saving paper wa proposed saved around memory saving make possible implement network cpu use instead gpu network tested imagenet dataset provide only le classiÔ¨Åcation accuracy alexnet measure network requires le power computation time could make possible accelerate training process deep neural network dramatically specialized hardware implementation Ô¨Årst time energy efÔ¨Åcient deep neural network eedn architecture wa proposed neuromorphic system addition released deep learning framework called eedn provides close accuracy accuracy almost popular benchmark except imagenet dataset hardware dl along algorithmic development dl approach many hardware architecture proposed past year 326 detail present trend hardware deep learning published recently mit proposed eyeriss hardware deep convolutional neural network dcnn 327 another architecture machine learning called dadiannao 328 google developed hardware named tensor processing unit tpu deep learning wa released 2017 329 2016 efÔ¨Åcient hardware work inference wa released proposed stanford university called efÔ¨Åcient inference engine eie 330 ibm released neuromorphic system called truenorth 2015 324 deep learning approach not limited hpc platform lot application already developed run mobile device mobile platform provide data relevant everyday activity user make mobile system efÔ¨Åcient robust retraining system collected data some research ongoing develop hardware friendly algorithm dl topic several important topic including framework sdk benchmark datasets related journal conference included appendix summary paper provided review deep learning application past year different deep learning model different category learning including supervised unsupervised reinforcement learning rl well application different domain reviewed addition explained detail different supervised deep learning technique including dnn cnn rnn deep learning technique including ae rbm gan reviewed detail section considered explained unsupervised learning technique proposed based lstm rl section 8 presented survey deep reinforcement learning drl fundamental learning technique called recently developed bayesian deep learning bdl transfer learning tl approach also discussed section 9 10 respectively furthermore conducted survey energy efÔ¨Åcient deep learning approach transfer learning dl hardware development trend dl moreover discussed some dl framework benchmark datasets often used implementation evaluation deep learning approach finally included relevant journal conference dl community ha publishing valuable research article electronics 2019 8 292 49 66 funding work wa supported national science foundation award 1718633 acknowledgment would like thank author mentioned reference paper learned lot thus made review paper possible conÔ¨Çicts interest author declare no conÔ¨Çict interest appendix time people use different deep learning framework standard development kit sdks implementing deep learning approach listed framework tensorÔ¨Çow caffe kera theano torch pytorch lasagne chainer digit cntk microsoft matconvnet minerva mxnet opendeep purine tensorlayer lbann sdks cudnn tensorrt deepstreamsdk cublas cusparse nccl benchmark datasets list benchmark datasets used often evaluate deep learning approach different domain application image classiÔ¨Åcation detection segmentation list datasets used Ô¨Åeld image processing computer vision mnist cifar electronics 2019 8 292 50 66 caltech norb imagenet national data science bowl competition coil coco dataset scene dataset dataset pascal voc 2007 dataset human attribute dataset face recognition dataset visit recently introduced datasets 2016 google open image image video text classiÔ¨Åcation text categorization collection sentiment analysis stanford movie sentiment analysis cornel language modeling free ebooks brown stanford corpus present americal english corpus google word corpus image captioning common object context coco machine translation pair sentence english french european parliament proceeding parallel corpus statistic machine translation question answering stanford question answering dataset squad electronics 2019 8 292 51 66 dataset deepmind amazon dataset qamain speech recognition timit voxforge open speech language resource document summarization sentiment analysis imdb dataset hyperspectral image analysis addition another alternative solution data programming label subset data using weak supervision strategy domain heuristic labeling function even noisy may conÔ¨Çict sample 87 journal conference general researcher publish primary version research arxiv conference accepting paper deep learning related Ô¨Åeld popular conference listed conference neural information processing system nip international conference learning representation iclr deep learning international conference machine learning icml computer vision pattern recognition cvpr deep learning international conference computer vision iccv european conference computer vision eccv british machine vision conference bmvc journal journal machine learning research jmlr ieee transaction neural network learning system itnnls ieee transaction pattern analysis machine intelligence tpami computer vision image understanding cviu pattern recognition letter electronics 2019 8 292 52 66 neural computing application international journal computer vision ieee transaction image processing ieee computational intelligence magazine proceeding ieee ieee signal processing magazine neural processing letter pattern recognition neural network ispprs journal photogrammetry remote sensing tutorial deep learning course reinforcement learning book deep learning reference schmidhuber deep learning neural network overview neural netw 2015 61 crossref pubmed bengio lecun hinton deep learning nature 2015 521 bengio courville vincent representation learning review new perspective ieee trans pattern anal mach intell 2013 35 crossref pubmed bengio learning deep architecture ai found trend mach learn 2009 2 crossref mnih kavukcuoglu silver rusu veness bellemare graf riedmiller fidjeland ostrovski et al control deep reinforcement learning nature 2015 518 crossref pubmed mnih kavukcuoglu silver graf antonoglou wierstra riedmiller playing atari deep reinforcement learning arxiv 2013 krizhevsky sutskever hinton imagenet classiÔ¨Åcation deep convolutional neural network proceeding international conference neural information processing system lake tahoe nv usa december 2012 pp zeiler fergus visualizing understanding convolutional network arxiv 2013 simonyan zisserman deep convolutional network image recognition arxiv 2014 szegedy liu jia sermanet reed anguelov erhan vanhoucke rabinovich going deeper convolution proceeding ieee conference computer vision pattern recognition boston usa june 2015 pp zhang ren sun deep residual learning image recognition proceeding ieee conference computer vision pattern recognition la vega nv usa june 2016 pp canziani paszke culurciello analysis deep neural network model practical application arxiv 2016 electronics 2019 8 292 53 66 zweig classiÔ¨Åcation recognition direct segment model proceeding 2012 ieee international conference acoustic speech signal processing icassp kyoto japan march 2012 pp efÔ¨Åcient segmental conditional random Ô¨Åelds phone recognition proceeding thirteenth annual conference international speech communication association portland usa september 2012 deng yu jiang deep segmental neural network speech recognition interspeech 2013 36 70 tang wang gimpel livescu discriminative segmental cascade phone recognition proceeding 2015 ieee workshop automatic speech recognition understanding asru scottsdale az usa december 2015 pp song cai deep neural network automatic speech recognition 1 error available online accessed 17 january 2018 deng yu deep convolutional neural network using heterogeneous pooling trading acoustic invariance phonetic confusion proceeding 2013 ieee international conference acoustic speech signal processing icassp vancouver bc canada may 2013 pp graf mohamed hinton speech recognition deep recurrent neural network proceeding 2013 ieee international conference acoustic speech signal processing icassp vancouver bc canada may 2013 pp zhang pezeshki brakel zhang bengio courville towards speech recognition deep convolutional neural network arxiv 2017 deng platt ensemble deep learning speech recognition proceeding fifteenth annual conference international speech communication association singapore september 2014 chorowski bahdanau serdyuk cho bengio model speech recognition advance neural information processing system mit press cambridge usa 2015 pp lu kong dyer smith renals segmental recurrent neural network speech recognition arxiv 2016 van essen kim pearce boakye chen lbann livermore big artiÔ¨Åcial neural network hpc toolkit proceeding workshop machine learning computing environment austin tx usa november 2015 5 li yu shahabi liu graph convolutional recurrent neural network trafÔ¨Åc forecasting arxiv 2017 md aspiras taha asari bowen advanced deep convolutional neural network approach digital pathology image analysis comprehensive evaluation different use case proceeding pathology vision 2018 san diego ca usa november 2018 long shelhamer darrell fully convolutional network semantic segmentation proceeding ieee conference computer vision pattern recognition boston usa june 2015 pp alom yakopcic taha asari nucleus segmentation recurrent residual convolutional neural network based proceeding naecon national aerospace electronics conference dayton oh usa july 2018 pp alom yakopcic taha asari microscopic blood cell classiÔ¨Åcation using inception recurrent residual convolutional neural network proceeding naecon national aerospace electronics conference dayton oh usa july 2018 pp chen lin big data deep learning challenge perspective ieee access 2014 2 crossref zhou chawla jin williams big data opportunity challenge discussion data analytics perspective ieee comput intell mag 2014 9 crossref najafabadi villanustre khoshgoftaar seliya wald muharemagic deep learning application challenge big data analytics big data 2015 2 1 crossref electronics 2019 8 292 54 66 goodfellow mirza xu ozair courville bengio generative adversarial net advance neural information processing system mit press cambridge usa 2014 pp kaiser gomez shazeer vaswani parmar jones uszkoreit one model learn arxiv 2017 collobert weston uniÔ¨Åed architecture natural language processing deep neural network multitask learning proceeding international conference machine learning helsinki finland july 2008 pp johnson schuster le krikun wu chen thorat vi√©gas wattenberg corrado et al google multilingual neural machine translation system enabling translation trans assoc comput linguist 2017 5 crossref argyriou evgeniou pontil feature learning advance neural information processing system mit press cambridge usa 2007 pp singh gupta vig shroff agarwal deep convolutional neural network pairwise causality arxiv 2017 yu wang huang yang xu video paragraph captioning using hierarchical recurrent neural network proceeding ieee conference computer vision pattern recognition la vega nv usa june 2016 pp kim cha kim lee kim learning discover relation generative adversarial network arxiv 2017 reed akata yan logeswaran schiele lee generative adversarial text image synthesis arxiv 2016 deng yu deep learning method application found trend signal process 2014 7 crossref gu wang kuen shahroudy shuai liu wang wang cai et al recent advance convolutional neural network arxiv 2015 sze chen yang emer efÔ¨Åcient processing deep neural network tutorial survey proc ieee 2017 105 crossref kwon kim kim suh kim kim survey deep network anomaly detection cluster comput 2017 crossref li deep reinforcement learning overview arxiv 2017 kober bagnell peter reinforcement learning robotics survey int robot 2013 32 crossref pan yang survey transfer learning ieee trans knowl data eng 2010 22 crossref schuman potok patton birdwell dean rose plank survey neuromorphic computing neural network hardware arxiv 2017 mcculloch pitt logical calculus idea immanent nervous activity bull math biophys 1943 5 crossref rosenblatt perceptron probabilistic model information storage organization brain psychol rev 1958 65 386 crossref pubmed minsky papert perceptrons introduction computational geometry mit press cambridge usa 2017 ackley hinton sejnowski learning algorithm boltzmann machine cogn sci 1985 9 crossref fukushima neocognitron hierarchical neural network capable visual pattern recognition neural netw 1988 1 crossref lecun bottou bengio haffner learning applied document recognition proc ieee 1998 86 crossref hinton osindero teh fast learning algorithm deep belief net neural comput 2006 18 crossref pubmed hinton salakhutdinov reducing dimensionality data neural network science 2006 313 crossref pubmed electronics 2019 8 292 55 66 bottou stochastic gradient descent trick neural network trick trade springer germany 2012 pp rumelhart hinton williams learning representation error cogn model 1988 5 1 crossref sutskever marten dahl hinton importance initialization momentum deep learning int conf mach learning 2013 28 yoshua lamblin popovici larochelle greedy training deep network advance neural information processing system 19 nip 2006 mit press cambridge usa 2007 pp erhan manzagol bengio bengio vincent difÔ¨Åculty training deep architecture effect unsupervised artif intell stat 2009 5 mohamed dahl hinton acoustic modeling using deep belief network ieee trans audio speech lang process 2012 20 crossref nair hinton rectiÔ¨Åed linear unit improve restricted boltzmann machine proceeding international conference machine learning haifa israel june 2010 pp vincent larochelle bengio manzagol extracting composing robust feature denoising autoencoders proceeding international conference machine learning helsinki finland july 2008 pp lin chen yan network network arxiv 2013 springenberg dosovitskiy brox riedmiller striving simplicity convolutional net arxiv 2014 huang liu van der maaten weinberger densely connected convolutional network proceeding ieee conference computer vision pattern recognition honolulu hi usa july 2017 pp larsson maire shakhnarovich fractalnet neural network without residual arxiv 2016 szegedy ioffe vanhoucke impact residual connection learning arxiv 2016 szegedy vanhoucke ioffe shlens wojna rethinking inception architecture computer vision proceeding ieee conference computer vision pattern recognition la vega nv usa june 2016 pp zagoruyko komodakis wide residual network arxiv 2016 xie girshick doll√°r tu aggregated residual transformation deep neural network arxiv 2016 veit wilber belongie residual network behave like ensemble relatively shallow network advance neural information processing system mit press cambridge usa 2016 pp abdi nahavandi network improving speed accuracy residual network arxiv 2016 zhang li loy lin polynet pursuit structural diversity deep network proceeding ieee conference computer vision pattern recognition honolulu hi usa july 2017 pp alom hasan yakopcic taha asari improved convolutional neural network object recognition arxiv 2017 ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift arxiv 2015 sabour frosst hinton dynamic routing capsule advance neural information processing system nip mit press cambridge usa 2017 pp ren girshick sun faster towards object detection region proposal network advance neural information processing system mit press cambridge usa 2015 pp chollet xception deep learning depthwise separable convolution arxiv 2016 liang hu recurrent convolutional neural network object recognition proceeding ieee conference computer vision pattern recognition boston usa june electronics 2019 8 292 56 66 alom hasan yakopcic taha inception recurrent convolutional neural network object recognition arxiv 2017 li ouyang wang tang visual phrase guided convolutional neural network proceeding 2017 ieee conference computer vision pattern recognition cvpr honolulu hi usa july 2017 pp bagherinezhad rastegari farhadi lcnn convolutional neural network arxiv 2016 bansal chen russell gupta ramanan pixelnet representation pixel pixel pixel arxiv 2017 huang sun liu sedra weinberger deep network stochastic depth european conference computer vision springer cham switzerland 2016 pp lee xie gallagher zhang tu net proceeding artiÔ¨Åcial intelligence statistic san diego ca usa may 2015 pp pezeshki fan brakel courville bengio deconstructing ladder network architecture proceeding international conference machine learning new york ny usa june 2016 pp rawat wang deep convolutional neural network image classiÔ¨Åcation comprehensive review neural comput 2017 29 crossref pubmed tzeng hoffman darrell saenko simultaneous deep transfer across domain task proceeding ieee international conference computer vision la condes chile december 2015 pp ba caruana deep net really need deep advance neural information processing system nip proceeding mit press cambridge usa 2014 urban geras kahou aslan wang caruana mohamed philipose richardson deep convolutional net really need deep convolutional arxiv 2016 romero ballas kahou chassang gatta bengio fitnets hint thin deep net arxiv 2014 mishkin matas need good init arxiv 2015 pandey dukkipati go deep wide learning arxiv 2014 ratner de sa wu selsam r√© data programming creating large training set quickly advance neural information processing system mit press cambridge usa 2016 pp aberger lamb tu n√∂tzli olukotun r√© emptyheaded relational engine graph processing acm trans database syst 2017 42 20 crossref iandola han moskewicz ashraf dally keutzer squeezenet accuracy fewer parameter mb model size arxiv 2016 han mao dally deep compression compressing deep neural network pruning trained quantization huffman coding arxiv 2015 niepert ahmed kutzkov learning convolutional neural network graph arxiv 2016 awesome deep vision available online accessed 17 january 2018 jia xu cai guo single image using convolutional neural network paciÔ¨Åc rim conference multimedia springer cham switzerland 2017 pp ahn cho convolutional neural network image denoising arxiv 2017 liu chen adaptive deep convolutional neural network photo aesthetic assessment arxiv 2017 cao zhou xu meng xu paisley hyperspectral image classiÔ¨Åcation markov random field convolutional neural network ieee trans image process 2018 27 crossref pubmed electronics 2019 8 292 57 66 de vos berendsen viergever staring i≈°gum unsupervised deformable image registration convolutional neural network deep learning medical image analysis multimodal learning clinical decision support springer cham switzerland 2017 pp wang oxholm zhang wang multimodal transfer hierarchical deep convolutional neural network fast artistic style transfer proceeding ieee conference computer vision pattern recognition honolulu hi usa july 2017 volume 2 7 babaee dinh rigoll deep convolutional neural network background subtraction arxiv 2017 alom sidike hasan taha asari handwritten bangla character recognition using deep convolutional neural network comput intell neurosci 2018 2018 6747098 crossref pubmed alom awwal taha optical beam classiÔ¨Åcation using deep learning comparison classiÔ¨Åcation proceeding optic photonics information processing xi san diego ca usa august 2017 volume 10395 sidike sagan maimaitijiang maimaitiyiming shakoor burken mockler fritschi dpen deep progressively expanded network mapping heterogeneous agricultural landscape using satellite imagery remote sen environ 2019 221 crossref alom alam taha iftekharuddin object recognition using cellular simultaneous recurrent network convolutional neural network proceeding 2017 international joint conference neural network ijcnn anchorage ak usa may 2017 pp ronao cho human activity recognition smartphone sensor using deep learning neural network expert syst appl 2016 59 crossref yang nguyen san li krishnaswamy deep convolutional neural network multichannel time series human activity recognition proceeding international joint conference artiÔ¨Åcial intelligence buenos aire argentina july 2015 hammerla halloran ploetz deep convolutional recurrent model human activity recognition using wearable arxiv 2016 ord√≥√±ez roggen deep convolutional lstm recurrent neural network multimodal wearable activity recognition sensor 2016 16 115 crossref pubmed rad kia zarbo van laarhoven jurman venuti marchiori furlanello deep learning automatic stereotypical motor movement detection using wearable sensor autism spectrum disorder signal process 2018 144 ravi wong lo yang deep learning human activity recognition resource efÔ¨Åcient implementation device proceeding 2016 ieee international conference wearable implantable body sensor network bsn san francisco ca usa june 2016 pp alom yakopcic taha asari microscopic nucleus classiÔ¨Åcation segmentation detection improved deep convolutional neural network dcnn approach arxiv 2018 chen papandreou kokkinos murphy yuille semantic image segmentation deep convolutional net fully connected crfs arxiv 2014 badrinarayanan kendall cipolla segnet deep convolutional architecture image segmentation arxiv 2015 lin milan shen reid reÔ¨Ånenet reÔ¨Ånement network semantic segmentation proceeding 2017 ieee conference computer vision pattern recognition cvpr honolulu hi usa july 2017 pp zhao shi qi wang jia pyramid scene parsing network proceeding ieee conference computer vision pattern recognition cvpr honolulu hi usa july 2017 pp chen papandreou kokkinos murphy yuille deeplab semantic image segmentation deep convolutional net atrous convolution fully connected crfs ieee trans pattern anal mach intell 2018 40 crossref pubmed electronics 2019 8 292 58 66 ronneberger fischer brox convolutional network biomedical image segmentation international conference medical image computing intervention springer cham switzerland 2015 pp alom hasan yakopcic taha asari recurrent residual convolutional neural network based medical image segmentation arxiv 2018 girshick donahue darrell malik rich feature hierarchy accurate object detection semantic segmentation proceeding ieee conference computer vision pattern recognition columbus oh usa june 2014 pp lin goyal girshick doll√°r focal loss dense object detection proceeding ieee international conference computer vision venice italy october 2017 pp wang shrivastava gupta hard positive generation via adversary object detection proceeding ieee conference computer vision pattern recognition honolulu hi usa july 2017 gkioxari doll√°r girshick mask proceeding 2017 ieee international conference computer vision iccv venice italy october 2017 pp redmon divvala girshick farhadi only look uniÔ¨Åed object detection proceeding ieee conference computer vision pattern recognition la vega nv usa june 2016 pp liu anguelov erhan szegedy reed fu berg ssd single shot multibox detector european conference computer vision springer cham switzerland 2016 pp hou wang lai tsao chang wang speech enhancement using multimodal deep convolutional neural network arxiv 2017 xu kong huang wang plumbley convolutional gated recurrent neural network incorporating spatial feature audio tagging proceeding 2017 international joint conference neural network ijcnn anchorage ak usa may 2017 pp litjens kooi bejnordi setio ciompi ghafoorian van der laak van ginneken s√°nchez survey deep learning medical image analysis med image anal 2017 42 crossref pubmed zhang xie xing mcgough yang mdnet semantically visually interpretable medical image diagnosis network proceeding ieee conference computer vision pattern recognition honolulu hi usa july 2017 pp tran fully convolutional neural network cardiac segmentation mri arxiv 2016 tan acharya bhandary chua sivaprasad segmentation optic disc fovea retinal vasculature using single convolutional neural network comput sci 2017 20 crossref moeskops viergever mendrik de vries bender i≈°gum automatic segmentation mr brain image convolutional neural network ieee trans med imaging 2016 35 crossref pubmed alom yakopcic taha asari breast cancer classiÔ¨Åcation histopathological image inception recurrent residual convolutional neural network arxiv 2018 lecun bottou orr efÔ¨Åcient backprop neural network trick trade orr m√ºller ed lecture note computer science springer berlin germany 2012 glorot bengio understanding difÔ¨Åculty training deep feedforward neural network proceeding thirteenth international conference artiÔ¨Åcial intelligence statistic sardinia italy may 2010 pp zhang ren sun delving deep rectiÔ¨Åers surpassing performance imagenet classiÔ¨Åcation proceeding ieee international conference computer vision la condes chile december 2015 pp vedaldi lenc matconvnet convolutional neural network matlab proceeding acm international conference multimedia brisbane australia october 2015 pp laurent pereyra brakel zhang bengio batch normalized recurrent neural network proceeding 2016 ieee international conference acoustic speech signal processing icassp shanghai china march 2016 pp electronics 2019 8 292 59 66 lavin gray fast algorithm convolutional neural network proceeding ieee conference computer vision pattern recognition la vega nv usa june 2016 pp clevert unterthiner hochreiter fast accurate deep network learning exponential linear unit elus arxiv 2015 li fan li wu ming improving deep neural network multiple parametric exponential linear unit neurocomputing 2018 301 crossref jin xu feng wei xiong yan deep learning rectiÔ¨Åed linear activation unit aaai 2016 3 xu wang chen li empirical evaluation rectiÔ¨Åed activation convolutional network arxiv 2015 zhang ren sun spatial pyramid pooling deep convolutional network visual recognition european conference computer vision springer cham switzerland 2014 pp yoo park lee kweon pyramid pooling deep convolutional representation proceeding ieee conference computer vision pattern recognition workshop boston usa june 2015 pp graham fractional arxiv 2014 lee gallagher tu generalizing pooling function convolutional neural network mixed gated tree proceeding artiÔ¨Åcial intelligence statistic cadiz spain may 2016 pp hinton srivastava krizhevsky sutskever salakhutdinov improving neural network preventing feature detector arxiv 2012 srivastava hinton krizhevsky sutskever salakhutdinov dropout simple way prevent neural network overÔ¨Åtting mach learn 2014 15 wan zeiler zhang le cun fergus regularization neural network using dropconnect proceeding international conference machine learning atlanta ga usa june 2013 pp bul√≤ porzi kontschieder dropout distillation proceeding international conference machine learning new york ny usa june 2016 pp ruder overview gradient descent optimization algorithm arxiv 2016 le ngiam coates lahiri prochnow ng optimization method deep learning proceeding international conference international conference machine learning bellevue wa usa 28 june july 2011 pp koushik hayashi improving stochastic gradient descent feedback arxiv 2016 sathasivam abdullah logic learning hopÔ¨Åeld network arxiv 2008 elman finding structure time cogn sci 1990 14 crossref jordan serial order parallel distributed processing approach adv psychol 1997 121 hochreiter bengio frasconi schmidhuber gradient flow recurrent net difÔ¨Åculty learning dependency ieee press new york ny usa 2001 schmidhuber habilitation thesis netzwerkarchitekturen zielfunktionen und kettenregel network architecture objective function chain rule thesis technische universit√§t m√ºnchen m√ºnchen germany 15 april 1993 gers schmidhuber recurrent net time count proceeding international joint conference neural network como italy july 2000 volume 3 gers schraudolph schmidhuber learning precise timing lstm recurrent network mach learn 2002 3 socher lin manning ng parsing natural scene natural language recursive neural network proceeding international conference machine learning bellevue wa usa 28 july 2011 pp mikolov karaÔ¨Å√°t burget Àá cernock√Ω khudanpur recurrent neural network based language model proceeding eleventh annual conference international speech communication association makuhari chiba japan september 2010 volume electronics 2019 8 292 60 66 xingjian chen wang yeung wong woo convolutional lstm network machine learning approach precipitation nowcasting advance neural information processing system nip nip proceeding mit press cambridge usa 2015 pp chung gulcehre cho bengio empirical evaluation gated recurrent neural network sequence modeling arxiv 2014 jozefowicz zaremba sutskever empirical exploration recurrent network architecture proceeding international conference machine learning lille france july 2015 yao cohn vylomova duh dyer recurrent neural network arxiv 2015 koutnik greff gomez schmidhuber clockwork rnn arxiv 2014 greff srivastava koutn√≠k steunebrink schmidhuber lstm search space odyssey ieee trans neural netw learn syst 2017 28 crossref pubmed karpathy li deep alignment generating image description proceeding ieee conference computer vision pattern recognition boston usa june 2015 mikolov chen corrado dean efÔ¨Åcient estimation word representation vector space arxiv 2013 goldberg levy explained deriving mikolov et method arxiv 2014 kunihiko neural network model selective attention visual pattern recognition associative recall appl opt 1987 26 xu ba kiros cho courville salakhudinov zemel bengio show attend tell neural image caption generation visual attention proceeding international conference machine learning lille france july 2015 pp qin song chen cheng jiang cottrell recurrent neural network time series prediction arxiv 2017 xiong merity socher dynamic memory network visual textual question answering proceeding international conference machine learning new york ny usa june 2016 oord kalchbrenner kavukcuoglu pixel recurrent neural network arxiv 2016 xue nachum pandey warrington leung li direct estimation regional wall thickness via residual recurrent neural network international conference information processing medical imaging springer cham switzerland 2017 pp tjandra sakti manurung adriani nakamura gated recurrent neural tensor network proceeding 2016 international joint conference neural network ijcnn vancouver bc canada july 2016 pp wang jing learning natural language inference lstm arxiv 2015 sutskever vinyals le sequence sequence learning neural network advance neural information processing system nip mit press cambridge usa 2014 pp lakhani mahadev identiÔ¨Åcation using convolutional recurrent neural network arxiv 2016 l√§ngkvist karlsson loutÔ¨Å review unsupervised feature learning deep learning modeling pattern recognit lett 2014 42 crossref malhotra vishnu vig agarwal shroff timenet deep recurrent neural network time series classiÔ¨Åcation arxiv 2017 soltau liao sak neural speech recognizer lstm model large vocabulary speech recognition arxiv 2016 sak senior beaufays long memory recurrent neural network architecture large scale acoustic modeling proceeding fifteenth annual conference international speech communication association singapore september 2014 adavanne pertil√§ virtanen sound event detection using spatial feature convolutional recurrent neural network arxiv 2017 chien misbullah deep long memory network speech recognition proceeding 2016 international symposium chinese spoken language processing iscslp tianjin china october electronics 2019 8 292 61 66 choi schuetz stewart sun using recurrent neural network model early detection heart failure onset med inform assoc 2016 24 crossref pubmed azzouni pujolle long memory recurrent neural network framework network trafÔ¨Åc matrix prediction arxiv 2017 olabiyi martinson chintalapudi guo driver action prediction using deep bidirectional recurrent neural network arxiv 2017 kim kang lee chae kim chung choi probabilistic vehicle trajectory prediction occupancy grid map via recurrent neural network arxiv 2017 richard gall equivalent recurrent neural network action recognition comput vi image underst 2017 156 crossref bontemps mcdermott collective anomaly detection based long memory recurrent neural network international conference future data security engineering springer international publishing cham switzerland 2016 kingma welling stochastic gradient vb variational proceeding second international conference learning representation iclr banff ab canada april 2014 ng sparse autoencoder lect note 2011 72 vincent larochelle lajoie bengio manzagol stacked denoising autoencoders learning useful representation deep network local denoising criterion mach learn 2010 11 zhang isola efros autoencoders unsupervised learning prediction arxiv 2016 lu deshpande forsyth cdvae deep variational auto encoder conditional variational generation arxiv 2016 chicco sadowski baldi deep autoencoder neural network gene ontology annotation prediction proceeding acm conference bioinformatics computational biology health 14 niagara fall ny usa august 2010 pp alom taha network intrusion detection cyber security using unsupervised deep learning approach proceeding aerospace electronics conference naecon dayton oh usa june 2017 song liu huang wang tan based data clustering iberoamerican congress pattern recognition springer germany 2013 pp ahmad protasov khan hyperspectral band selection using unsupervised deep auto encoder train external classiÔ¨Åers arxiv 2017 freund haussler unsupervised learning distribution binary vector using two layer network advance neural information processing system mit press cambridge usa 1992 pp larochelle bengio classiÔ¨Åcation using discriminative restricted boltzmann machine proceeding international conference machine learning helsinki finland july 2008 salakhutdinov hinton deep boltzmann machine aistats 2009 1 3 alom bontupalli taha intrusion detection using deep belief network proceeding aerospace electronics conference naecon dayton oh usa june 2015 alom sidike taha asari handwritten bangla digit recognition using deep learning arxiv 2017 albalooshi sidike sagan albalooshi asari deep belief active contour dbac application oil spill segmentation remotely sensed aerial imagery photogramm eng remote sen 2018 84 crossref mao li xie lau wang smolley least square generative adversarial network proceeding ieee international conference computer vision venice italy october 2017 pp salimans goodfellow zaremba cheung radford chen improved technique training gans arxiv 2016 vondrick pirsiavash torralba generating video scene dynamic advance neural information processing system mit press cambridge usa 2016 pp electronics 2019 8 292 62 66 radford metz chintala unsupervised representation learning deep convolutional generative adversarial network arxiv 2015 wang gupta generative image modeling using style structure adversarial network european conference computer vision springer cham switzerland 2016 chen duan houthooft schulman sutskever abbeel infogan interpretable representation learning information maximizing generative adversarial net advance neural information processing system mit press cambridge usa 2016 im kim jiang memisevic generating image recurrent adversarial work arxiv 2016 isola zhu zhou efros translation conditional adversarial network arxiv 2017 liu tuzel coupled generative adversarial network advance neural information processing system mit press cambridge usa 2016 donahue kr√§henb√ºhl darrell adversarial feature learning arxiv 2016 berthelot schumm metz began boundary equilibrium generative adversarial network arxiv 2017 martin chintala bottou wasserstein gan arxiv 2017 gulrajani ahmed arjovsky dumoulin courville improved training wasserstein gans advance neural information processing system mit press cambridge usa 2017 pp wang hopcroft powerful generative model using random weight deep image representation advance neural information processing system mit press cambridge usa 2016 ko fischer song adversarial example generative model arxiv 2017 zhao mathieu lecun generative adversarial network arxiv 2016 park anand moniz lee chakraborty choo park kim mmgan manifold matching generative adversarial network generating image arxiv 2017 laloy h√©rault jacques linde efÔ¨Åcient based geostatistical simulation inversion using spatial generative adversarial neural network arxiv 2017 widmer probabilistic generative adversarial network arxiv 2017 fowkes sutton bayesian network model interesting itemsets joint european conference machine learning knowledge disco database springer international publishing cham switzerland 2016 mescheder nowozin geiger adversarial variational bayes unifying variational autoencoders generative adversarial network arxiv 2017 nowozin cseke tomioka training generative neural sampler using variational divergence minimization advance neural information processing system mit press cambridge usa 2016 li wand precomputed texture synthesis markovian generative adversarial network european conference computer vision springer international publishing cham switzerland 2016 du zhu zhang learning deep generative model doubly stochastic gradient mcmc ieee trans neural network learn syst 2018 29 crossref pubmed 242 hoang quan tu dinh nguyen trung le dinh phung gernerative adversarial net arxiv 2017 bousmalis silberman dohan erhan krishnan unsupervised domain adaptation generative adversarial network proceeding ieee conference computer vision pattern recognition cvpr honolulu hi usa july 2017 volume 1 7 kansky silver m√©ly eldawy lou dorfman sidor phoenix george schema network transfer generative causal model intuitive physic arxiv 2017 ledig theis husz√°r caballero cunningham acosta aitken tejani totz wang et al single image using generative adversarial network arxiv 2016 souly spampinato shah semi weakly supervised semantic segmentation using generative adversarial network arxiv 2017 dash gamboa ahmed liwicki afzal conditioned auxiliary classiÔ¨Åer generative adversarial network arxiv 2017 electronics 2019 8 292 63 66 zhang dana generative network transfer arxiv 2017 zhang sindagi patel image using conditional generative adversarial network arxiv 2017 serban sordoni bengio courville pineau building dialogue system using generative hierarchical neural network model aaai 2016 16 pascual bonafonte serr√† segan speech enhancement generative adversarial network arxiv 2017 yang chou yang midinet convolutional generative adversarial network music generation proceeding international society music information retrieval conference ismir 2017 suzhou china october 2017 yang yan zhang yu shi mou kalra zhang sun wang ct image denoising using generative adversarial network wasserstein distance perceptual loss ieee trans med imaging 2018 37 crossref pubmed rezaei harmuth gierke kellermeier fischer yang meinel conditional adversarial network semantic segmentation brain tumor international miccai brainlesion workshop springer cham switzerland 2017 pp xue xu zhang long huang segan adversarial network l 1 loss medical image segmentation neuroinformatics 2018 16 crossref pubmed mardani gong cheng vasanawala zaharchuk alley thakur han dally pauly et al deep generative adversarial network compressed sensing automates mri arxiv 2017 choi biswal malin duke stewart sun generating multilabel discrete electronic health record using generative adversarial network arxiv 2017 esteban hyland r√§tsch medical time series generation recurrent conditional gans arxiv 2017 hayes melis danezis de cristofaro logan evaluating privacy leakage generative model using generative adversarial network arxiv 2017 gordon bayesian semisupervised learning deep generative model arxiv 2017 abbasnejad shi abbasnejad van den hengel dick bayesian conditional generative adverserial network arxiv 2017 grnarova levy lucchi hofmann krause online learning approach generative adversarial network arxiv 2017 li swersky zemel generative moment matching network proceeding international conference machine learning lille france july 2015 pp li chang cheng yang p√≥czos mmd gan towards deeper understanding moment matching network advance neural information processing system mit press cambridge usa 2017 pp nie feng xing yan generative partition network pose estimation arxiv 2017 saeedi hoffman diverdi ghandeharioun johnson adam multimodal prediction personalization photo edits deep generative model arxiv 2017 schlegl seeb√∂ck waldstein langs unsupervised anomaly detection generative adversarial network guide marker discovery international conference information processing medical imaging springer cham switzerland 2017 pp liu breuel kautz unsupervised translation network advance neural information processing system mit press cambridge usa 2017 pp mehrotra dukkipati generative adversarial residual pairwise network one shot learning arxiv 2017 sordoni galley auli brockett ji mitchell nie gao dolan neural network approach generation conversational response arxiv 2015 yin jiang lu shang li li neural generative question answering arxiv 2015 electronics 2019 8 292 64 66 oord dieleman zen simonyan vinyals graf kalchbrenner senior kavukcuoglu wavenet generative model raw audio arxiv 2016 chen li xiao jin yan feng dual path network advance neural information processing system mit press cambridge usa 2017 pp mahmud kaiser hussain vassanelli application deep learning reinforcement learning biological data ieee trans neural netw learn syst 2018 29 crossref pubmed goodfellow bengio courville deep learning mit press cambridge usa 2016 silver huang maddison guez sifre van den driessche schrittwieser mastering game go deep neural network tree search nature 2016 529 484 crossref pubmed vinyals ewalds bartunov georgiev vezhnevets yeo makhzani k√ºttler agapiou schrittwieser et al starcraft ii new challenge reinforcement learning arxiv 2017 koenig simmons complexity analysis reinforcement learning applied finding shortest path deterministic domain tech report no computer science department university pittsburgh pa decemver 1992 silver schrittwieser simonyan antonoglou huang guez hubert baker lai bolton et al mastering game go without human knowledge nature 2017 550 354 crossref pubmed schulman levine abbeel jordan moritz trust region policy optimization proceeding international conference machine learning lille france july 2015 volume 37 pp levine finn darrell abbeel training deep visuomotor policy mach learn 2016 17 mnih badia mirza graf lillicrap harley silver kavukcuoglu asynchronous method deep reinforcement learning proceeding international conference machine learning new york ny usa june 2016 pp arulkumaran deisenroth brundage bharath brief survey deep reinforcement learning arxiv 2017 zhu liao zhu yao huang online reinforcement learning mhealth intervention arxiv 2017 zhu guo xu liao yang huang reinforcement learning personalized mhealth intervention international conference medical image computing intervention springer cham switzerland 2018 pp steckelmacher roijers harutyunyan vrancx plisnier now√© reinforcement learning pomdps memoryless option initiation set proceeding aaai conference artiÔ¨Åcial intelligence new orleans la usa february 2018 hu zhang yan wang xu solving new bin packing problem deep reinforcement learning method arxiv 2017 everitt krakovna orseau hutter legg reinforcement learning corrupted reward channel arxiv 2017 wu mansimov grosse liao ba scalable method deep reinforcement learning using approximation advance neural information processing system mit press cambridge usa 2017 pp denil agrawal kulkarni erez battaglia de freitas learning perform physic experiment via deep reinforcement learning arxiv 2016 hein hentschel runkler udluft particle swarm optimization generating interpretable fuzzy reinforcement learning policy eng appl artif intell 2017 65 crossref islam henderson gomrokchi precup reproducibility benchmarked deep reinforcement learning task continuous control arxiv 2017 inoue de magistris munawar yokoya tachibana deep reinforcement learning high precision assembly task proceeding 2017 international conference intelligent robot system iros vancouver bc canada september 2017 pp electronics 2019 8 292 65 66 li burdick inverse reinforcement learning large state space via function approximation arxiv 2017 liu li xu xu lin qiu tang wang hierarchical framework cloud resource allocation power management using deep reinforcement learning proceeding 2017 ieee international conference distributed computing system icdcs atlanta ga usa june 2017 pp cao lin shi liang li face hallucination via deep reinforcement learning proceeding ieee conference computer vision pattern recognition honolulu hi usa july 2017 pp kendall gal uncertainty need bayesian deep learning computer vision advance neural information processing system nip mit press cambridge usa 2017 kendall gal cipolla learning using uncertainty weigh loss scene geometry semantics arxiv 2017 google photo labeled black people gorilla available online accessed 1 march 2019 gal ghahramani bayesian convolutional neural network bernoulli approximate variational inference arxiv 2015 kumar laumann maurin olsen bayesian convolutional neural network variational inference arxiv 2018 vladimirova arbel mesejo bayesian neural network become depth proceeding bayesian deep learning workshop conference neural information processing system nip 2018 montr√©al qc canada 7 december 2018 hu moreno lawrence damianou perspective bayesian neural network proceeding bayesian deep learning workshop conference neural information processing system nip 2018 montr√©al qc canada 7 december 2018 salvator han schroers mandt video compression deep bayesian learning bayesian proceeding deep learning workshop conference neural information processing system nip 2018 montr√©al qc canada 7 december 2018 krishnan subedar tickoo bar bayesian activity recognition using variational inference arxiv 2018 chen goodfellow shlens accelerating learning via knowledge transfer arxiv 2015 ganin lempitsky unsupervised domain adaptation backpropagation arxiv 2014 ganin ustinova ajakan germain larochelle laviolette marchand lempitsky training neural network mach learn 2016 17 taylor stone transfer learning reinforcement learning domain survey mach learn 2009 10 mckeough teaching transfer fostering generalization learning routledge london uk 2013 raina battle lee packer ng learning transfer learning unlabeled data proceeding international conference machine learning corvallis usa june 2007 pp wenyuan yang xue yu boosting transfer learning proceeding international conference machine learning corvallis usa june 2007 pp wu schuster chen le norouzi macherey krikun cao gao macherey et al google neural machine translation system bridging gap human machine translation arxiv 2016 qiu wang yao guo li zhou yu tang xu song et al going deeper embedded fpga platform convolutional neural network proceeding 2016 international symposium gate array monterey ca usa february 2016 pp electronics 2019 8 292 66 66 sun convolutional neural network constrained time cost proceeding ieee conference computer vision pattern recognition boston usa june 2015 pp lin courbariaux memisevic bengio neural network multiplication arxiv 2015 courbariaux david bengio training deep neural network low precision multiplication arxiv 2014 courbariaux bengio david binaryconnect training deep neural network binary weight propagation advance neural information processing system mit press cambridge usa 2015 hubara soudry el yaniv binarized neural network arxiv 2016 kim smaragdis bitwise neural network arxiv 2016 dettmers approximation parallelism deep learning arxiv 2015 gupta agrawal gopalakrishnan narayanan deep learning limited numerical precision proceeding international conference machine learning lille france july 2015 pp zhou wu ni zhou wen zou training low bitwidth convolutional neural network low bitwidth gradient arxiv 2016 merolla arthur cassidy sawada akopyan jackson imam guo nakamura et al million integrated circuit scalable communication network interface science 2014 345 crossref pubmed steven merolla arthur cassidy convolutional network fast neuromorphic computing proc natl acad sci usa 2016 27 201604850 zidan strachan lu future electronics based memristive system nat electron 2018 1 22 crossref chen krishna emer sze eyeriss reconÔ¨Ågurable accelerator deep convolutional neural network ieee circuit 2017 52 crossref chen luo liu zhang wang li chen xu sun et al dadiannao supercomputer proceeding annual international symposium microarchitecture cambridge uk december 2014 pp jouppi young patil patterson agrawal bajwa bates bhatia boden borchers et al performance analysis tensor processing unit proceeding 2017 annual international symposium computer architecture isca toronto canada june 2017 pp han liu mao pu pedram horowitz dally eie efÔ¨Åcient inference engine compressed deep neural network proceeding 2016 annual international symposium computer architecture isca seoul korea june 2016 pp zhang zou ming sun efÔ¨Åcient accurate approximation nonlinear convolutional network proceeding ieee conference computer vision pattern recognition boston usa june 2015 pp novikov podoprikhin osokin vetrov tensorizing neural network advance neural information processing system mit press cambridge usa 2005 pp zhu han mao dally trained ternary quantization arxiv 2016 2019 author licensee mdpi basel switzerland article open access article distributed term condition creative common attribution cc license