electron review survey deep learn theori architectur md zahangir alom tarek taha chri yakopc stefan westberg pahed sidik mst shamima nasrin mahmudul hasan brian van essen abdul awwal vijayan asari depart electr comput engin univers dayton dayton oh usa depart earth atmospher scienc saint loui univers saint loui mo usa comcast lab washington dc usa lawrenc livermor nation laboratori llnl livermor ca usa correspond receiv januari accept januari publish march abstract recent year deep learn ha garner tremend success varieti applic domain thi new Ô¨Åeld machin learn ha grow rapidli ha appli tradit applic domain well new area present opportun differ method propos base differ categori learn includ supervis learn experiment result show perform use deep learn compar tradit machin learn approach Ô¨Åeld imag process comput vision speech recognit machin translat art medic imag medic inform process robot control bioinformat natur languag process cybersecur mani thi survey present brief survey advanc occur area deep learn dl start deep neural network dnn survey goe cover convolut neural network cnn recurr neural network rnn includ long memori lstm gate recurr unit gru ae deep belief network dbn gener adversari network gan deep reinforc learn drl addit discuss recent develop advanc variant dl techniqu base dl approach thi work consid paper publish histori deep learn began furthermor dl approach explor evalu differ applic domain also includ thi survey also includ recent develop framework sdk benchmark dataset use implement evalu deep learn approach survey publish dl use neural network survey reinforc learn rl howev paper discuss individu advanc techniqu train deep learn model recent develop method gener model keyword deep learn convolut neural network cnn recurr neural network rnn ae restrict boltzmann machin rbm deep belief network dbn gener adversari network gan deep reinforc learn drl transfer learn electron electron introduct sinc small subset artiÔ¨Åci intellig ai often call machin learn ml ha revolution sever Ô¨Åeld last decad neural network nn subÔ¨Åeld ml wa thi subÔ¨Åeld spawn deep learn dl sinc incept dl ha creat ever larger disrupt show outstand success almost everi applic domain figur show taxonomi ai dl use either deep architectur learn hierarch learn approach class ml develop larg onward learn procedur consist estim model paramet learn model algorithm perform speciÔ¨Åc task exampl artiÔ¨Åci neural network ann paramet weight matric dl hand consist sever layer input output layer allow mani stage inform process unit hierarch architectur present exploit featur learn pattern classiÔ¨Åc learn method base represent data also deÔ¨Ån represent learn recent literatur state dl base represent learn involv hierarchi featur concept concept deÔ¨Ån one concept deÔ¨Ån one articl dl ha describ univers learn approach abl solv almost kind problem differ applic domain word dl task speciÔ¨Åc sinc small subset artifici intellig ai often call machin learn ml ha revolution sever field last decad neural network nn subfield ml wa thi subfield spawn deep learn dl sinc incept dl ha creat ever larger disrupt show outstand success almost everi applic domain figur show taxonomi ai dl use either deep architectur learn hierarch learn approach class ml develop larg onward learn procedur consist estim model paramet learn model algorithm perform specif task exampl artifici neural network ann paramet weight matric dl hand consist sever layer input output layer allow mani stage inform process unit hierarch architectur present exploit featur learn pattern classif learn method base represent data also defin represent learn recent literatur state dl base represent learn involv hierarchi featur concept concept defin one concept defin one articl dl ha describ univers learn approach abl solv almost kind problem differ applic domain word dl task specif type deep learn approach deep learn approach categor follow supervis partial supervis unsupervis addit anoth categori learn approach call reinforc learn rl deep rl drl often discuss scope sometim unsupervis learn approach figur show pictori diagram figur taxonomi ai ai artifici intellig ml machin learn nn neural network dl deep learn snn spike neural network deep supervis learn supervis learn learn techniqu use label data case supervis dl approach environ ha set input correspond output exampl input xt intellig agent predict ùë¶ ùëì agent receiv loss valu ùëô ùë¶ agent iter modifi network paramet better approxim desir output success train agent abl get correct answer question environ differ supervis learn approach deep lean includ deep neural network dnn convolut neural network cnn recurr neural network rnn includ long short term memori lstm gate recurr unit gru network describ detail respect section figur taxonomi ai ai artiÔ¨Åci intellig ml machin learn nn neural network dl deep learn snn spike neural network type deep learn approach deep learn approach categor follow supervis partial supervis unsupervis addit anoth categori learn approach call reinforc learn rl deep rl drl often discuss scope sometim unsupervis learn approach figur show pictori diagram electron x peer review deep learn learn learn occur base partial label dataset case drl gener adversari network gan use learn techniqu gan discuss section section survey drl approach addit rnn includ lstm gru use learn well deep unsupervis learn unsupervis learn system one without presenc data label thi case agent learn intern represent import featur discov unknown relationship structur within input data often cluster dimension reduct gener techniqu consid unsupervis learn approach sever member deep learn famili good cluster dimension reduct includ ae restrict boltzmann machin rbm recent develop gan addit rnn lstm rl also use unsupervis learn mani applic domain section discuss rnn lstm detail deep reinforc learn rl deep reinforc learn learn techniqu use unknown environ drl began googl deep mind sever advanc method propos base rl exampl rl environ sampl input agent predict ùë¶ ùëì agent receiv cost ùë¶ p unknown probabl distribut environ ask agent question give noisi score answer sometim thi approach call learn well mani techniqu implement base thi concept section rl straight forward loss function thu make learn harder compar tradit supervis approach fundament differ rl supervis learn first full access function tri optim must queri interact second interact environ input depend previou action depend upon problem scope space one decid type rl need appli solv task problem ha lot paramet optim drl best way go problem ha fewer paramet optim deriv free rl approach good exampl thi anneal cross entropi method spsa figur categori deep lean approach featur learn key differ tradit ml dl featur extract tradit ml approach use handcraft engin featur appli sever featur extract algorithm appli learn algorithm addit boost approach figur categori deep lean approach electron deep supervis learn supervis learn learn techniqu use label data case supervis dl approach environ ha set input correspond output xt yt exampl input xt intellig agent predict ÀÜ yt f xt agent receiv loss valu l yt ÀÜ yt agent iter modifi network paramet better approxim desir output success train agent abl get correct answer question environ differ supervis learn approach deep lean includ deep neural network dnn convolut neural network cnn recurr neural network rnn includ long short term memori lstm gate recurr unit gru network describ detail respect section deep learn learn learn occur base partial label dataset case drl gener adversari network gan use learn techniqu gan discuss section section survey drl approach addit rnn includ lstm gru use learn well deep unsupervis learn unsupervis learn system one without presenc data label thi case agent learn intern represent import featur discov unknown relationship structur within input data often cluster dimension reduct gener techniqu consid unsupervis learn approach sever member deep learn famili good cluster dimension reduct includ ae restrict boltzmann machin rbm recent develop gan addit rnn lstm rl also use unsupervis learn mani applic domain section discuss rnn lstm detail deep reinforc learn rl deep reinforc learn learn techniqu use unknown environ drl began googl deep mind sever advanc method propos base rl exampl rl environ sampl input xt agent predict ÀÜ yt f xt agent receiv cost ct ÀÜ yt p unknown probabl distribut environ ask agent question give noisi score answer sometim thi approach call learn well mani techniqu implement base thi concept section rl straight forward loss function thu make learn harder compar tradit supervis approach fundament differ rl supervis learn first full access function tri optim must queri interact second interact environ input xt depend previou action depend upon problem scope space one decid type rl need appli solv task problem ha lot paramet optim drl best way go problem ha fewer paramet optim deriv free rl approach good exampl thi anneal cross entropi method spsa featur learn key differ tradit ml dl featur extract tradit ml approach use handcraft engin featur appli sever featur extract algorithm appli learn algorithm addit boost approach often use electron sever learn algorithm appli featur singl task dataset decis made accord multipl outcom differ algorithm hand case dl featur learn automat repres hierarch multipl level thi strong point dl tradit machin learn approach tabl show differ learn approach differ learn step tabl differ featur learn approach approach learn step input featur output tradit machin learn input featur map featur output represent learn input featur map featur output deep learn input simpl featur complex featur map featur output whi appli dl dl employ sever situat machin intellig would use see figur absenc human expert navig mar human unabl explain expertis speech recognit vision languag understand solut problem chang time track weather predict prefer stock price predict solut need adapt particular case biometr person problem size vast limit reason capabl calcul webpag rank match ad facebook sentiment analysi electron x peer review figur accuraci imagenet classif challeng differ dl model alexnet clarifia human erro model year experiment result cdnn pool ctc dcnn ensembl rnn transduc rnn segment rnn phone error rate per percentag figur accuraci imagenet classiÔ¨Åc challeng differ dl model present dl appli almost area result thi approach often call univers learn approach perform dl outstand success Ô¨Åeld comput vision speech recognit discuss imag classiÔ¨Åc imagenet dataset one problem name larg scale visual recognit challeng lsvrc cnn variant one dl branch show electron accuraci imagenet task follow graph show success stori dl techniqu overtim challeng figur show ha achiev error rate outperform human accuraci figur accuraci imagenet classif challeng differ dl model figur phone error rate per timit continu speech corpu dataset b automat speech recognit initi success field speech recognit popular timit dataset common data set gener use evalu wa recognit task timit continu speech corpu contain speaker eight major dialect american english speaker read sentenc figur summar error rate includ earli result measur percent phone error rate per last year bar graph clearli show recent develop dl alexnet clarifia human e model year scrf scrf deep segment nn discrimin segmet cascad dsc dl dsc pass cdnn pool ctc dcnn ensembl rnn transduc rnn segment rnn phone error rate per percentag figur phone error rate per timit continu speech corpu dataset b automat speech recognit initi success Ô¨Åeld speech recognit popular timit dataset common data set gener use evalu wa recognit task timit continu speech corpu contain speaker eight major dialect american english speaker read sentenc figur summar error rate includ earli result measur percent phone error rate per last year bar graph clearli show recent develop dl approach top graph perform better compar ani previou machin learn approach timit dataset exampl applic shown figur electron x peer review approach top graph perform better compar ani previou machin learn approach timit dataset exampl applic shown figur figur traffic forecast use dynam traffic flow road captur spatial depend use diffus convolut recurr neural network lymphocyt detect semant segment road road road figur trafÔ¨Åc forecast use dynam trafÔ¨Åc Ô¨Çow road captur spatial depend use diffus convolut recurr neural network electron figur traffic forecast use dynam traffic flow road captur spatial depend use diffus convolut recurr neural network lymphocyt detect semant segment nuclei segment blood cell classif figur exampl imag dl appli success achiev perform imag taken correspond ding refer whi dl univers learn approach dl approach sometim call univers learn becaus appli almost ani applic domain robust deep learn approach requir precis design featur instead optim featur automat learn task hand result robust natur variat input data achiev gener dl approach use differ applic differ data type thi approach often call transfer learn addit thi approach help problem road figur exampl imag dl appli success achiev perform imag taken correspond refer whi dl univers learn approach dl approach sometim call univers learn becaus appli almost ani applic domain robust deep learn approach requir precis design featur instead optim featur automat learn task hand result robust natur variat input data achiev gener dl approach use differ applic differ data type thi approach often call transfer learn addit thi approach help problem doe sufÔ¨Åcient avail data number literatur discuss thi concept see section scalabl dl approach highli scalabl microsoft invent deep network known resnet thi network contain layer often implement supercomput scale big initi lawrenc livermor nation laboratori llnl develop framework network like thi implement thousand node challeng dl sever challeng dl electron big data analyt use dl scalabl dl approach abil gener data import data avail learn system especi comput vision task invers graphic energi efÔ¨Åcient techniqu special purpos devic includ mobil intellig fpga transfer learn learn thi mean learn differ domain differ model togeth deal causal learn challeng alreadi consid dl commun firstli big data analyt challeng good survey wa conduct thi paper author explain detail dl deal differ criteria includ volum veloc varieti verac big data problem author also show differ advantag dl approach deal big data problem figur clearli demonstr perform tradit ml approach show better perform lesser amount input data amount data increas beyond certain number perform tradit machin learn approach becom steadi wherea dl approach increas respect increment amount data dl approach highli scalabl microsoft invent deep network known resnet thi network contain layer often implement supercomput scale big initi lawrenc livermor nation laboratori llnl develop framework network like thi implement thousand node challeng dl sever challeng dl big data analyt use dl scalabl dl approach abil gener data import data avail learn system especi comput vision task invers graphic energi effici techniqu special purpos devic includ mobil intellig fpga transfer learn learn thi mean learn differ domain differ model togeth deal causal learn challeng alreadi consid dl commun firstli big data analyt challeng good survey wa conduct thi paper author explain detail dl deal differ criteria includ volum veloc varieti verac big data problem author also show differ advantag dl approach deal big data problem figur clearli demonstr perform tradit ml approach show better perform lesser amount input data amount data increas beyond certain number perform tradit machin learn approach becom steadi wherea dl approach increas respect increment amount data figur perform deep learn respect amount data secondli case solv problem solut implement comput hpc system cluster sometim consid cloud comput offer immens potenti busi comput data explod veloc varieti verac volum get increasingli difficult scale comput perform use server storag step increas articl consid demand suggest effici hpc heterogen comput system one exampl lawrenc livermor nation laboratori llnl ha develop framework call livermor big artifici neural network lbann figur perform deep learn respect amount data secondli case solv problem solut implement comput hpc system cluster sometim consid cloud comput offer immens potenti busi comput data explod veloc varieti verac volum get increasingli difÔ¨Åcult scale comput perform use server storag step increas articl consid demand suggest efÔ¨Åcient hpc heterogen comput system one exampl lawrenc livermor nation laboratori llnl ha develop framework call livermor big artiÔ¨Åci neural network lbann implement scale dl clearli supplant issu scalabl dl thirdli gener model anoth challeng deep learn one exampl gan outstand approach data gener ani task gener data distribut fourthli transfer learn discuss section fourthli lot research ha conduct energi efÔ¨Åcient deep learn approach respect network architectur hardwir section discuss thi issu make ani uniform model solv multipl task differ applic domain far system concern one articl googl titl one model learn good exampl thi approach learn differ applic domain includ electron imagenet multipl translat task imag caption dataset speech recognit corpu english pars task discuss challeng respect solut thi survey techniqu propos last year final learn system causal ha present graphic model deÔ¨Ån one may infer causal model data recent dl base approach ha propos solv thi type problem howev mani challeng issu solv last year possibl solv efÔ¨Åcient befor thi revolut exampl imag video caption style transfer one domain anoth domain use gan text imag synthesi mani survey conduct recent dl Ô¨Åeld paper survey dl revolut address recent develop gener model call gan addit discuss littl rl cover recent trend drl approach case survey conduct differ dl approach individu good survey base reinforc learn approach anoth survey exist transfer learn one survey ha conduct neural network hardwar howev main object thi work provid overal idea deep learn relat Ô¨Åeld includ deep supervis dnn cnn rnn unsupervis ae rbm gan sometim gan also use learn task drl case drl consid unsupervis approach addit consid recent develop trend thi Ô¨Åeld applic develop base techniqu furthermor includ framework benchmark dataset often use evalu deep learn techniqu moreov name confer journal also includ consid thi commun publish research articl rest paper ha organ follow way detail survey dnn discuss section section discuss cnn section describ differ advanc techniqu efÔ¨Åcient train dl approach section discuss rnn ae rbm discuss section gan applic discuss section rl present section section explain transfer learn section present energi efÔ¨Åcient approach hardwir dl section discuss deep learn framework standard develop kit sdk benchmark differ applic domain web link given section conclus made section deep neural network histori dnn brief histori neural network highlight key event shown figur comput neurobiolog ha conduct signiÔ¨Åc research construct comput model artiÔ¨Åci neuron artiÔ¨Åci neuron tri mimic behavior human brain fundament compon build ann basic comput element neuron call node unit receiv input extern sourc ha intern paramet includ weight bias learn dure train produc output thi unit call perceptron fundament ann discuss refer electron electron x peer review includ weight bias learn dure train produc output thi unit call perceptron fundament ann discuss refer figur histori deep learn develop ann gener nn consist multilay perceptron mlp contain one hidden layer multipl hidden unit neuron detail mlp pleas see refer gradient descent gradient descent approach optim algorithm use find local minima object function thi ha use train ann last coupl decad success stochast gradient descent sgd sinc long train time main drawback tradit gradient descent approach sgd approach use train deep neural network dnn bp dnn train popular bp algorithm sgd case mlp easili repres nn model use comput graph direct acycl graph represent dl use effici calcul gradient top bottom layer bp shown refer momentum momentum method help acceler train process sgd approach main idea behind use move averag gradient instead use onli current real valu gradient express thi follow equat mathemat Œ≥ ‚Ñ± Œ≥ momentum ùúÇ learn rate tth round train popular approach introduc dure last year explain section ix scope optim approach main advantag use momentum dure train prevent network get stuck local minimum valu momentum Œ≥ note higher momentum valu overshoot minimum possibl make network mcculloch pitt show neuron combin construct ture machin rosenblatt show perceptron converg tri learn repres minski papert show limit perceptron kill research neural network decad backpropag algorithm geoffrey hinton et al revit field neocognitron hierarch neural network capabl visual pattern recognit cnn backpropag document analysi yan lecun hinton lab solv train problem dnn pressent varieti deep learn algorithm increasingli emerg present figur histori deep learn develop ann gener nn consist multilay perceptron mlp contain one hidden layer multipl hidden unit neuron detail mlp pleas see refer gradient descent gradient descent approach optim algorithm use Ô¨Ånding local minima object function thi ha use train ann last coupl decad success stochast gradient descent sgd sinc long train time main drawback tradit gradient descent approach sgd approach use train deep neural network dnn bp dnn train popular bp algorithm sgd case mlp easili repres nn model use comput graph direct acycl graph represent dl use efÔ¨Åcient calcul gradient top bottom layer bp shown refer momentum momentum method help acceler train process sgd approach main idea behind use move averag gradient instead use onli current real valu gradient express thi follow equat mathemat vt Œ≥ Œ∏t vt Œ≥ momentum Œ∑ learn rate tth round train popular approach introduc dure last year explain section scope optim approach main advantag use momentum dure train prevent network get stuck local minimum valu momentum Œ≥ note higher momentum valu overshoot minimum possibl make network unstabl gener Œ≥ set initi learn stabil increas higher electron learn rate Œ∑ learn rate import compon train dnn learn rate step size consid dure train make train process faster howev select valu learn rate sensit exampl choos larger valu Œ∑ network may start diverg instead converg hand choos smaller valu Œ∑ take time network converg addit may easili get stuck local minima typic solut thi problem reduc learn rate dure train three common approach use reduc learn rate dure train constant factor exponenti decay first deÔ¨Ån constant Œ∂ appli reduc learn rate manual deÔ¨Ån step function second learn rate adjust dure train follow equat Œ∑t œµ Œ∑t tth round learn rate initi learn rate Œ≤ decay factor valu rang step function format exponenti decay Œ∑t œµ common practic use learn rate decay Œ≤ reduc learn rate factor stage weight decay weight decay use train deep learn model regular approach help prevent overÔ¨Åt network model gener regular f Œ∏ x deÔ¨Ån ÀÜ Œµ f Œ∏ x Œµ f Œ∏ x œâ gradient weight Œ∏ gener practic use valu Œª smaller Œª acceler train necessari compon effici train includ data preprocess augment network initi approach batch normal activ function regular dropout differ optim approach discuss section last decad mani efÔ¨Åcient approach propos better train deep neural network befor attempt taken train deep architectur fail train deep supervis neural network tend yield wors result train test error shallow one hidden layer hinton revolutionari work dbn spearhead chang thi due composit mani layer dnn capabl repres highli vari nonlinear function compar shallow learn approach moreov dnn efÔ¨Åcient learn becaus combin featur extract classiÔ¨Åc layer follow section discuss detail differ dl approach necessari compon electron convolut neural network cnn cnn overview thi network structur wa Ô¨Årst propos fukushima wa wide use howev due limit comput hardwar train network lecun et al appli learn algorithm cnn obtain success result handwritten digit classiÔ¨Åc problem research improv cnn report result mani recognit task cnn sever advantag dnn includ like human visual process system highli optim structur process imag effect learn extract abstract featur max pool layer cnn effect absorb shape variat moreov compos spars connect tie weight cnn signiÔ¨Åcantli fewer paramet fulli connect network similar size cnn train learn algorithm suffer less diminish gradient problem given algorithm train whole network minim error criterion directli cnn produc highli optim weight figur show overal architectur cnn consist two main part featur extractor classiÔ¨Å featur extract layer layer network receiv output immedi previou layer input pass output input next layer cnn architectur consist combin three type layer convolut classiÔ¨Åc two type layer low network convolut layer layer even number layer convolut layer oper output node convolut layer group plane call featur map plane layer usual deriv combin one plane previou layer node plane connect small region connect plane previou layer node convolut layer extract featur input imag convolut oper input node electron x peer review cnn overview thi network structur wa first propos fukushima wa wide use howev due limit comput hardwar train network lecun et al appli learn algorithm cnn obtain success result handwritten digit classif problem research improv cnn report result mani recognit task cnn sever advantag dnn includ like human visual process system highli optim structur process imag effect learn extract abstract featur max pool layer cnn effect absorb shape variat moreov compos spars connect tie weight cnn significantli fewer paramet fulli connect network similar size cnn train learn algorithm suffer less diminish gradient problem given base algorithm train whole network minim error criterion directli cnn produc highli optim weight figur show overal architectur cnn consist two main part featur extractor classifi featur extract layer layer network receiv output immedi previou layer input pass output input next layer cnn architectur consist combin three type layer convolut classif two type layer low network convolut layer layer even number layer convolut number layer oper output node convolut pool layer group plane call featur map plane layer usual deriv combin one plane previou layer node plane connect small region connect plane previou layer node convolut layer extract featur input imag convolut oper input node figur overal architectur convolut neural network cnn includ input layer multipl altern convolut layer one layer one classif layer featur deriv featur propag lower level layer featur propag highest layer level dimens featur reduc depend size kernel convolut oper respect howev number featur map usual increas repres better featur input imag ensur classif accuraci output last layer cnn use input fulli connect network call classif layer neural network use classif layer better perform classif layer extract featur taken input respect dimens weight matrix final figur overal architectur convolut neural network cnn includ input layer multipl altern convolut layer one layer one classiÔ¨Åc layer featur deriv featur propag lower level layer featur propag highest layer level dimens featur reduc depend size kernel convolut oper respect howev number featur map usual increas repres better featur input imag ensur classiÔ¨Åc accuraci output last layer cnn use input fulli connect network call classiÔ¨Åc layer neural network electron use classiÔ¨Åc layer better perform classiÔ¨Åc layer extract featur taken input respect dimens weight matrix Ô¨Ånal neural network howev fulli connect layer expens term network learn paramet nowaday sever new techniqu includ averag pool global averag pool use altern network score respect class calcul top classiÔ¨Åc layer use layer base highest score classiÔ¨Å give output correspond class mathemat detail differ layer cnn discuss follow section convolut layer thi layer featur map previou layer convolv learnabl kernel output kernel goe linear activ function sigmoid hyperbol tangent softmax rectiÔ¨Å linear ident function form output featur map output featur map combin one input featur map gener xl j f iœµmj ij bl j xl j output current layer previou layer output kl ij kernel present layer bl j bias current layer mj repres select input map output map addit bia b given howev input map convolv distinct kernel gener correspond output map output map Ô¨Ånalli go linear activ function sigmoid hyperbol tangent softmax rectiÔ¨Å linear ident function layer subsampl layer perform sampl oper input map thi commonli known pool layer thi layer number input output featur map doe chang exampl n input map exactli n output map due sampl oper size dimens output map reduc depend size sampl mask exampl sampl kernel use output dimens half correspond input dimens imag thi oper formul xl j j repres function two type oper mostli perform thi layer averag pool case averag pool approach function usual sum n n patch featur map previou layer select averag valu hand case highest valu select n n patch featur map therefor output map dimens reduc n time special case output map multipli scalar altern layer propos fraction layer convolut explain section classiÔ¨Åc layer thi fulli connect layer comput score class extract featur convolut layer preced step Ô¨Ånal layer featur map repres vector scalar valu pass fulli connect layer fulli connect neural layer use classiÔ¨Åc layer strict rule electron number layer incorpor network model howev case two four layer observ differ architectur includ lenet alexnet vgg net fulli connect layer expens term comput altern approach propos dure last year includ global averag pool layer averag pool layer help reduc number paramet network signiÔ¨Åcantli backward propag cnn fulli connect layer updat follow gener approach fulli connect neural network fcnn Ô¨Ålter convolut layer updat perform full convolut oper featur map convolut layer immedi previou layer figur show basic oper convolut input imag electron x peer review layer averag pool layer help reduc number paramet network significantli backward propag cnn fulli connect layer updat follow gener approach fulli connect neural network fcnn filter convolut layer updat perform full convolut oper featur map convolut layer immedi previou layer figur show basic oper convolut input imag figur featur map perform convolut pool oper network paramet requir memori cnn number comput paramet import metric measur complex deep learn model size output featur map formul follow ùëÜ ùëÅ refer dimens input featur map ùêπ refer dimens filter recept field ùëÄ refer dimens output featur map ùëÜ stand stride length pad typic appli dure convolut oper ensur input output featur map dimens amount pad depend size kernel equat use determin number row column pad ùëÉ amount pad ùêπ refer dimens kernel sever criteria consid compar model howev case number network paramet total amount memori consid number paramet layer calcul base follow equat bia ad weight abov equat written follow total number paramet lath yer repres ùëÉ total number output featur map total number input featur map channel exampl let assum layer ha input featur map output featur map filter size thi case total number paramet bia thi layer thu amount memori need oper layer express popular cnn architectur figur featur map perform convolut pool oper network paramet requir memori cnn number comput paramet import metric measur complex deep learn model size output featur map formul follow n n refer dimens input featur map f refer dimens Ô¨Ålter recept Ô¨Åeld refer dimens output featur map stand stride length pad typic appli dure convolut oper ensur input output featur map dimens amount pad depend size kernel equat use determin number row column pad p f p amount pad f refer dimens kernel sever criteria consid compar model howev case number network paramet total amount memori consid number paramet parml lth layer calcul base follow equat parml f f fml bia ad weight abov equat written follow parml f f fml total number paramet lth lath yer repres pl fml total number output featur map total number input featur map channel exampl let assum lth layer ha input featur map fml output featur electron map Ô¨Ålter size f thi case total number paramet bia thi layer parml thu amount memori meml need oper lth layer express meml nl nl fml popular cnn architectur thi section sever popular cnn architectur examin gener deep convolut neural network made key set basic layer includ convolut layer layer dens layer layer architectur typic consist stack sever convolut layer layer follow fulli connect softmax layer end exampl model lenet alexnet vgg net nin convolut conv altern efÔ¨Åcient advanc architectur propos includ densenet fractalnet googlenet incept unit residu network basic build compon convolut pool almost across architectur howev topolog differ observ modern deep learn architectur mani dcnn architectur alexnet vgg googlenet dens cnn fractalnet gener consid popular architectur becaus perform differ benchmark object recognit task among structur architectur design especi data analysi googlenet resnet wherea vgg network consid gener architectur architectur dens term connect densenet fractal network altern resnet model lenet although lenet wa propos limit comput capabl memori capac made algorithm difÔ¨Åcult implement lecun et al howev propos cnn algorithm experi handwritten digit dataset achiev accuraci propos cnn architectur basic conÔ¨Ågur follow see figur two convolut conv layer two layer two fulli connect layer output layer gaussian connect total number weight multipli accumul mac k respect comput hardwar start improv capabl cnn state becom popular effect learn approach comput vision machin learn commun electron x peer review thi section sever popular cnn architectur examin gener deep convolut neural network made key set basic layer includ convolut layer layer dens layer layer architectur typic consist stack sever convolut layer layer follow fulli connect softmax layer end exampl model lenet alexnet vgg net nin convolut conv altern effici advanc architectur propos includ densenet fractalnet googlenet incept unit residu network basic build compon convolut pool almost across architectur howev topolog differ observ modern deep learn architectur mani dcnn architectur alexnet vgg googlenet dens cnn fractalnet gener consid popular architectur becaus perform differ benchmark object recognit task among structur architectur design especi data analysi googlenet resnet wherea vgg network consid gener architectur architectur dens term connect densenet fractal network altern resnet model lenet although lenet wa propos limit comput capabl memori capac made algorithm difficult implement lecun et al howev propos cnn algorithm experi handwritten digit dataset achiev accuraci propos cnn architectur basic configur follow see figur two convolut conv layer two layer two fulli connect layer output layer gaussian connect total number weight multipli accumul mac k respect comput hardwar start improv capabl cnn state becom popular effect learn approach comput vision machin learn commun figur architectur lenet alexnet alex krizheveski propos deeper wider cnn model compar lenet difficult imagenet challeng visual object recognit call imagenet larg scale visual recognit challeng ilsvrc alexnet achiev recognit accuraci tradit machin learn comput vision approach wa signific breakthrough field machin learn comput vision visual recognit classif task point histori interest deep learn increas rapidli th hit f al n h fi th fi l ti l l f figur architectur lenet alexnet alex krizheveski propos deeper wider cnn model compar lenet difÔ¨Åcult imagenet challeng visual object recognit call imagenet larg scale visual recognit challeng ilsvrc alexnet achiev electron recognit accuraci tradit machin learn comput vision approach wa signiÔ¨Åc breakthrough Ô¨Åeld machin learn comput vision visual recognit classiÔ¨Åc task point histori interest deep learn increas rapidli architectur alexnet shown figur Ô¨Årst convolut layer perform convolut local respons normal lrn differ recept Ô¨Ålter use size max pool oper perform Ô¨Ålter stride size oper perform second layer Ô¨Ålter Ô¨Ålter use third fourth Ô¨Åfth convolut layer featur map respect two fulli connect fc layer use dropout follow softmax layer end two network similar structur number featur map train parallel thi model two new concept local respons normal lrn dropout introduc thi network lrn appli two differ way first appli singl channel featur map n n patch select featur map normal base neighborhood valu second lrn appli across channel featur map neighborhood along third dimens singl pixel locat electron x peer review filter stride size oper perform second layer filter filter use third fourth fifth convolut layer featur map respect two fulli connect fc layer use dropout follow softmax layer end two network similar structur number featur map train parallel thi model two new concept local respons normal lrn dropout introduc thi network lrn appli two differ way first appli singl channel featur map patch select featur map normal base neighborhood valu second lrn appli across channel featur map neighborhood along third dimens singl pixel locat figur architectur alexnet convolut local respons normal lrn fulli connect fc layer alexnet ha three convolut layer two fulli connect layer process imagenet dataset total number paramet alexnet calcul follow first layer input sampl filter kernel mask recept field ha size stride output first convolut layer accord equat section calcul thi first layer ha neuron bia weight paramet first convolut layer tabl show number paramet layer million total number weight mac whole network respect zfnet clarifai matthew zeiler rob fergu ilsvrc cnn architectur wa extens alexnet network wa call zfnet author name cnn expens comput optimum use paramet need model complex point view zfnet architectur improv alexnet design tweak network paramet latter zfnet use kernel instead kernel significantli reduc number weight thi reduc number network paramet dramat improv overal recognit accuraci network network nin thi model slightli differ previou model coupl new concept introduc first concept use multilay percept convolut convolut perform filter help add nonlinear model thi help increas depth network regular dropout thi concept use often bottleneck layer deep learn model figur architectur alexnet convolut local respons normal lrn fulli connect fc layer alexnet ha three convolut layer two fulli connect layer process imagenet dataset total number paramet alexnet calcul follow Ô¨Årst layer input sampl Ô¨Ålter kernel mask recept Ô¨Åeld ha size stride output Ô¨Årst convolut layer accord equat section calcul thi Ô¨Årst layer ha neuron bia weight paramet Ô¨Årst convolut layer tabl show number paramet layer million total number weight mac whole network respect zfnet clarifai matthew zeiler rob fergu ilsvrc cnn architectur wa extens alexnet network wa call zfnet author name cnn expens comput optimum use paramet need model complex point view zfnet architectur improv alexnet design tweak network paramet latter zfnet use kernel instead kernel signiÔ¨Åcantli reduc number weight thi reduc number network paramet dramat improv overal recognit accuraci electron network network nin thi model slightli differ previou model coupl new concept introduc Ô¨Årst concept use multilay percept convolut convolut perform Ô¨Ålter help add nonlinear model thi help increas depth network regular dropout thi concept use often bottleneck layer deep learn model second concept use global averag pool gap altern fulli connect layer thi help reduc number network paramet signiÔ¨Åcantli gap chang network structur signiÔ¨Åcantli appli gap larg featur map gener Ô¨Ånal low dimension featur vector without reduc dimens featur map vggnet visual geometri group vgg wa ilsvrc main contribut thi work show depth network critic compon achiev better recognit classiÔ¨Åc accuraci cnn vgg architectur consist two convolut layer use relu activ function follow activ function singl max pool layer sever fulli connect layer also use relu activ function Ô¨Ånal layer model softmax layer classiÔ¨Åc convolut Ô¨Ålter size chang Ô¨Ålter stride three model propos model layer respect vgg network model shown figur electron x peer review vggnet visual geometri group vgg wa ilsvrc main contribut thi work show depth network critic compon achiev better recognit classif accuraci cnn vgg architectur consist two convolut layer use relu activ function follow activ function singl max pool layer sever fulli connect layer also use relu activ function final layer model softmax layer classif convolut filter size chang filter stride three model propos model layer respect vgg network model shown figur figur basic build block vgg network convolut conv fc fulli connect layer version model end three fulli connect layer howev number convolut layer vari contain convolut layer convolut layer convolut layer comput expens model contain mac googlenet googlenet winner ilsvrc wa model propos christian szegedi googl object reduc comput complex compar tradit cnn propos method wa incorpor incept layer variabl recept field creat differ kernel size recept field creat oper captur spars correl pattern new featur map stack figur incept layer naiv version figur basic build block vgg network convolut conv fc fulli connect layer version model end three fulli connect layer howev number convolut layer vari contain convolut layer convolut layer convolut layer comput expens model contain mac googlenet googlenet winner ilsvrc wa model propos christian szegedi googl object reduc comput complex compar tradit cnn propos method wa incorpor incept layer variabl recept Ô¨Åeld creat differ kernel size recept Ô¨Åeld creat oper captur spars correl pattern new featur map stack initi concept incept layer seen figur googlenet improv recognit accuraci use stack incept layer seen figur differ na√Øv incept layer Ô¨Ånal incept layer wa addit convolut kernel kernel allow dimension reduct befor comput expens layer googlenet consist layer total wa far greater ani network befor later improv version thi network propos howev number network paramet electron googlenet use wa much lower predecessor alexnet vgg googlenet network paramet alexnet comput googlenet also mac far lower alexnet vgg convolut layer convolut layer comput expens model contain mac googlenet googlenet winner ilsvrc wa model propos christian szegedi googl object reduc comput complex compar tradit cnn propos method wa incorpor incept layer variabl recept field creat differ kernel size recept field creat oper captur spars correl pattern new featur map stack figur incept layer naiv version initi concept incept layer seen figur googlenet improv recognit accuraci use stack incept layer seen figur differ na√Øv incept layer final incept layer wa addit convolut kernel kernel allow dimension reduct befor comput expens layer googlenet consist layer total wa far greater ani network befor later improv version thi network propos howev number network paramet googlenet use wa much lower predecessor alexnet vgg googlenet figur incept layer naiv version electron x peer review network paramet alexnet comput googlenet also mac far lower alexnet vgg figur incept layer dimens reduct residu network resnet winner ilsvrc wa residu network architectur resnet resnet wa develop kaim intent design network suffer vanish gradient problem predecessor resnet develop mani differ number layer even popular contain convolut layer fulli connect layer end network total number weight mac whole network respect basic block diagram resnet architectur shown figur resnet tradit feedforward network residu connect output residu layer defin base output come previou layer defin ‚Ñ± output perform variou oper convolut differ size filter batch normal bn follow activ function relu final output residualth unit defin follow equat ‚Ñ± figur basic diagram residu block residu network consist sever basic residu block howev oper figur incept layer dimens reduct residu network resnet winner ilsvrc wa residu network architectur resnet resnet wa develop kaim intent design network suffer vanish gradient problem predecessor resnet develop mani differ number layer even popular contain convolut layer fulli connect layer end network total number weight mac whole network respect basic block diagram resnet architectur shown figur resnet tradit feedforward network residu connect output residu layer deÔ¨Ån base output l th come previou layer deÔ¨Ån f output perform variou oper convolut differ size Ô¨Ålter batch normal bn follow activ function relu Ô¨Ånal output residualth unit xl deÔ¨Ån follow equat xl f electron x peer review network paramet alexnet comput googlenet also mac far lower alexnet vgg figur incept layer dimens reduct residu network resnet winner ilsvrc wa residu network architectur resnet resnet wa develop kaim intent design network suffer vanish gradient problem predecessor resnet develop mani differ number layer even popular contain convolut layer fulli connect layer end network total number weight mac whole network respect basic block diagram resnet architectur shown figur resnet tradit feedforward network residu connect output residu layer defin base output come previou layer defin ‚Ñ± output perform variou oper convolut differ size filter batch normal bn follow activ function relu final output residualth unit defin follow equat ‚Ñ± figur basic diagram residu block residu network consist sever basic residu block howev oper residu block vari depend differ architectur residu network wider version residu network wa propos zagoruvko el anoth improv residu network approach known aggreg residu transform recent variant residu model introduc base residu network architectur furthermor sever advanc architectur combin incept figur basic diagram residu block electron residu network consist sever basic residu block howev oper residu block vari depend differ architectur residu network wider version residu network wa propos zagoruvko et al anoth improv residu network approach known aggreg residu transform recent variant residu model introduc base residu network architectur furthermor sever advanc architectur combin incept residu unit basic conceptu diagram unit shown follow figur electron x peer review figur basic block diagram incept residu unit mathemat thi concept repres ‚Ñ± symbol refer concentr oper two output filter convolut oper perform filter final output ad input thi block concept incept block residu connect introduc architectur improv version network also propos dens connect network densenet densenet develop gao et al consist dens connect cnn layer output layer connect successor layer dens block therefor form dens connect layer reward name densenet thi concept effici featur reus dramat reduc network paramet densenet consist sever dens block transit block place two adjac dens block conceptu diagram dens block shown figur figur dens block growth rate layer take preced featur map input deconstruct figur layer receiv featur map previou layer ùë•‡¨µ input ùë•‡¨µ ùë•‡¨µ concaten featur layer consid singl tensor perform three differ consecut oper bn follow relu convolut oper transact block l ti l ti f ith bn f b li l thi figur basic block diagram incept residu unit mathemat thi concept repres xl f k symbol j refer concentr oper two output Ô¨Ålter convolut oper perform Ô¨Ålter final output ad input thi block concept incept block residu connect introduc architectur improv version network also propos dens connect network densenet densenet develop gao et al consist dens connect cnn layer output layer connect successor layer dens block therefor form dens connect layer reward name densenet thi concept efÔ¨Åcient featur reus dramat reduc network paramet densenet consist sever dens block transit block place two adjac dens block conceptu diagram dens block shown figur electron x peer review figur basic block diagram incept residu unit mathemat thi concept repres ‚Ñ± symbol refer concentr oper two output filter convolut oper perform filter final output ad input thi block concept incept block residu connect introduc architectur improv version network also propos dens connect network densenet densenet develop gao et al consist dens connect cnn layer output layer connect successor layer dens block therefor form dens connect layer reward name densenet thi concept effici featur reus dramat reduc network paramet densenet consist sever dens block transit block place two adjac dens block conceptu diagram dens block shown figur figur dens block growth rate layer take preced featur map input deconstruct figur layer receiv featur map previou layer ùë•‡¨µ input ùë•‡¨µ ùë•‡¨µ concaten featur layer consid singl tensor perform three differ consecut oper bn follow relu convolut oper transact block figur dens block growth rate k electron layer take preced featur map input deconstruct figur lth layer receiv featur map previou layer input xl hl concaten featur layer l hl consid singl tensor perform three differ consecut oper bn follow relu convolut oper transact block convolut oper perform bn follow averag pool layer thi new model show accuraci reason number network paramet object recognit task electron x peer review thi architectur advanc altern architectur resnet model effici design larg model nomin depth shorter path propag gradient dure train thi concept base anoth regular approach make larg network result thi concept help enforc speed versu accuraci tradeoff basic block diagram fractalnet shown figur figur detail fractalnet modul left fractalnet right capsulenet cnn effect methodolog detect featur object achiev good recognit perform compar handcraft featur detector limit cnn doe take account special relationship perspect size orient featur exampl face imag doe matter placement differ compon nose eye mouth etc face neuron cnn wrongli activ recognit face without consid special relationship orient size imagin neuron contain likelihood properti featur perspect orient size thi special type neuron capsul detect face effici distinct inform capsul network consist sever layer capsul node first version capsul network capsnet consist three layer capsul node encod unit thi architectur mnist imag kernel appli stride output featur map output fed primari capsul layer modifi convolut layer gener vector instead scalar first convolut layer kernel appli stride output dimens primari capsul use kernel gener group neuron size figur detail fractalnet modul left fractalnet right fractalnet thi architectur advanc altern architectur resnet model efÔ¨Åcient design larg model nomin depth shorter path propag gradient dure train thi concept base anoth regular approach make larg network result thi concept help enforc speed versu accuraci tradeoff basic block diagram fractalnet shown figur capsulenet cnn effect methodolog detect featur object achiev good recognit perform compar handcraft featur detector limit cnn doe take account special relationship perspect size orient featur exampl face imag doe matter placement differ compon nose eye mouth etc face neuron cnn wrongli activ recognit face without consid special relationship orient size imagin neuron contain likelihood properti featur perspect orient size thi special type neuron capsul detect face efÔ¨Åcient distinct inform capsul network consist sever layer capsul node Ô¨Årst version capsul network capsnet consist three layer capsul node encod unit thi architectur mnist imag kernel appli stride output featur map output fed primari capsul layer modiÔ¨Å convolut layer gener vector instead scalar Ô¨Årst convolut layer kernel appli stride output dimens electron primari capsul use kernel gener group neuron size entir encod decod process capsnet shown figur respect use layer cnn often handl translat varianc even featur move still max pool window detect capsul contain weight sum featur previou layer therefor thi approach capabl detect overlap featur import segment detect task electron x peer review figur capsnet encod unit layer instanc class repres vector capsul digitcap layer use calcul classif loss weight primari capsul layer digitcap layer repres ùëä entir encod decod process capsnet shown figur respect use layer cnn often handl translat varianc even featur move still max pool window detect capsul contain weight sum featur previou layer therefor thi approach capabl detect overlap featur import segment detect task figur decod unit digit reconstruct digitcap layer represent euclidean distanc use minim error input sampl reconstruct sampl sigmoid layer true label use reconstruct target dure train tradit cnn singl cost function use evalu overal error propag backward dure train howev thi case weight two neuron zero activ neuron propag neuron signal rout respect featur paramet rather one size fit cost function iter dynam rout agreement detail thi architectur pleas see refer thi new cnn architectur provid accuraci handwritten digit recognit mnist howev applic point view thi architectur suitabl segment detect task compar classif task comparison differ model figur capsnet encod unit layer instanc class repres vector capsul digitcap layer use calcul classiÔ¨Åc loss weight primari capsul layer digitcap layer repres wij electron x peer review figur capsnet encod unit layer instanc class repres vector capsul digitcap layer use calcul classif loss weight primari capsul layer digitcap layer repres ùëä entir encod decod process capsnet shown figur respect use layer cnn often handl translat varianc even featur move still max pool window detect capsul contain weight sum featur previou layer therefor thi approach capabl detect overlap featur import segment detect task figur decod unit digit reconstruct digitcap layer represent euclidean distanc use minim error input sampl reconstruct sampl sigmoid layer true label use reconstruct target dure train tradit cnn singl cost function use evalu overal error propag backward dure train howev thi case weight two neuron zero activ neuron propag neuron signal rout respect featur paramet rather one size fit cost function iter dynam rout agreement detail thi architectur pleas see refer thi new cnn architectur provid accuraci handwritten digit recognit mnist howev applic point view thi architectur suitabl segment detect task compar classif task comparison differ model comparison recent propos model base error network paramet maximum number connect given tabl tabl error comput paramet mac differ deep cnn model figur decod unit digit reconstruct digitcap layer represent euclidean distanc use minim error input sampl reconstruct sampl sigmoid layer true label use reconstruct target dure train tradit cnn singl cost function use evalu overal error propag backward dure train howev thi case weight two neuron zero activ neuron propag neuron signal rout respect featur paramet rather one size Ô¨Åt cost function iter dynam rout agreement detail thi architectur pleas see refer thi new cnn architectur provid accuraci handwritten digit recognit mnist howev applic point view thi architectur suitabl segment detect task compar classiÔ¨Åc task comparison differ model comparison recent propos model base error network paramet maximum number connect given tabl electron tabl error comput paramet mac differ deep cnn model method alexnet overfeat fast googlenet error input size number conv layer filter size number featur map stride number weight k number mac g g g g number fc layer number weight k number mac k total weight k total mac g g g g dnn model mani network architectur fast cnn xception popular comput vision commun new model wa propos use recurr convolut layer name recurr convolut neural network rcnn improv version thi network combin two popular architectur incept network recurr convolut network incept convolut recurr neural network ircnn ircnn provid better accuraci compar rcnn incept network almost ident network paramet visual phase guid cnn vip cnn propos phase guid messag pass structur pmp build connect relat compon show better speed recognit accuraci look base cnn fast compact accur model enabl efÔ¨Åcient infer architectur known fulli convolut network fcn wa propos segment task commonli use recent propos cnn model includ pixel net deep network stochast depth network ladder network addit cnn architectur model explain articl publish deep net realli need deep articl publish Ô¨Åtnet hit initi method deep versu wide net train dl larg train set graph process energi efÔ¨Åcient network architectur applic cnn cnn solv graph problem learn graph data structur common problem variou applic data mine machin learn task dl techniqu made bridg machin learn data mine group efÔ¨Åcient cnn arbitrari graph process wa propos imag process comput vision model discuss abov appli differ applic domain includ imag classif detect segment local caption video classif mani good survey dl approach imag process comput vision relat task includ imag classif segment detect exampl singl imag use cnn method imag denois use cnn photo aesthet assess use adapt deep cnn dcnn hyperspectr imag segment imag registr fast artist style transfer imag background segment use dcnn handwritten charact recognit optic imag classiÔ¨Åc crop map use satellit imageri object recognit cellular simultan recurr network cnn dl approach massiv appli human activ recognit task achiev perform electron compar exit approach howev model classiÔ¨Åc segment detect task list follow model classiÔ¨Åc problem accord architectur classiÔ¨Åc model input imag encod differ step convolut subsampl layer Ô¨Ånalli softmax approach use calcul class probabl model discuss abov appli classiÔ¨Åc problem howev model classiÔ¨Åc layer use featur extract segment detect task list classiÔ¨Åc model follow alexnet vggnet googlenet resnet densenet fractalnet capsulenet ircnn irrcnn dcrn model segment problem sever semant segment model propos last year segment model consist two unit encod decod unit encod unit convolut subsampl oper perform encod lower dimension latent space decod unit decod imag latent space perform deconvolut oper veri Ô¨Årst segment model fulli convolut network fcn later improv version thi network propos name segnet sever new model propos recent includ reÔ¨Ånenet pspnet deeplab unet model detect problem detect problem bit differ compar classiÔ¨Åc segment problem thi case model goal identifi target type correspond posit model answer two question object classiÔ¨Åc problem object regress problem achiev goal two loss calcul classiÔ¨Åc regress unit top featur extract modul model weight updat respect lose veri Ô¨Årst time region base cnn rcnn propos object detect task recent better detect approach propos includ focal loss dens object detector later differ improv version thi network propos call faster rcnn fast rcnn mask onli look onc yolo ssd singl shot multibox detector tissu detect patholog imag speech process cnn also appli speech process speech enhanc use multimod deep cnn audio tag use convolut gate recurr network cgrn cnn medic imag litjen et al provid good survey dl medic imag process includ classif detect segment task sever popular dl method develop medic imag analysi instanc mdnet wa develop medic diagnosi use imag correspond text descript cardiac segment use mri segment optic disc retin vasculatur use cnn brain tumor segment use random forest featur learn fulli convolut neural network fcnn techniqu appli Ô¨Åeld comput patholog achiev perform advanc train techniqu advanc train techniqu compon need consid care efÔ¨Åcient train dl approach differ advanc techniqu appli train deep learn model better techniqu includ input better initi method batch normal altern convolut approach advanc activ function altern pool techniqu network regular approach better optim method train follow section discuss individu advanc train techniqu individu electron prepar dataset present differ approach appli befor feed data network differ oper prepar dataset follow sampl rescal mean subtract random crop Ô¨Çip data respect horizon vertic axi color jitter whiten mani network initi initi deep network ha big impact overal recognit accuraci previous network initi random weight complex task high dimension data train dnn becom difÔ¨Åcult becaus weight symmetr due process therefor effect initi techniqu import train thi type dnn howev mani effect techniqu propos dure last year lecun bengio propos simpl effect approach method weight scale invers squar root number input neuron layer state nl number input neuron lth layer deep network initi approach xavier ha propos base symmetr activ function respect hypothesi linear thi approach known xavier initi approach recent dmytro et al propos lsuv initi approach provid good recognit accuraci sever benchmark dataset includ imagenet one popular initi approach ha propos et al distribut weight lth layer normal distribut mean zero varianc nl express follow wl nl batch normal batch normal help acceler dl process reduc intern covari shift input sampl mean input linearli transform zero mean unit varianc whiten input network converg faster show better regular dure train ha impact overal accuraci sinc data whiten perform outsid network impact whiten dure train model case deep recurr neural network input nth layer combin layer raw featur input train progress effect normal whiten reduc respect caus vanish gradient problem thi slow entir train process caus satur better train process batch normal appli intern layer deep neural network thi approach ensur faster converg theori dure experi benchmark batch normal featur layer independ normal mean zero varianc one algorithm batch normal given algorithm algorithm batch normal bn input valu x b output yi bnŒ≥ Œ≤ xi ¬µb xi mean b xi varianc ÀÜ xi q normal yi Œ≥ ÀÜ xi Œ≤ Œ≤ xi scale shift electron paramet Œ≥ Œ≤ use scale shift factor normal valu normal doe onli depend layer valu use normal techniqu follow criterion recommend consid dure implement increas learn rate dropout batch normal doe job weight regular acceler learn rate decay remov local respons normal lrn use shufÔ¨Ç train sampl thoroughli useless distort imag train set altern convolut method altern comput efÔ¨Åcient convolut techniqu reduc cost multipl factor propos activ function tradit sigmoid tanh activ function use implement neural network approach past decad graphic mathemat represent shown figur output bn‡Æì ‡Æí ‡µü ‡¨µ mean ùúéùîÖ ‡¨∂ ‡¨µ ‡¨∂ varianc ùë• ‡∂ß‡∞ôùîÖ normal ùõæùë• ùõΩ ‡Æí scale shift paramet ùõæ ùõΩ use scale shift factor normal valu normal doe onli depend layer valu use normal techniqu follow criterion recommend consid dure implement increas learn rate dropout batch normal doe job weight regular acceler learn rate decay remov local respons normal lrn use shuffl train sampl thoroughli useless distort imag train set altern convolut method altern comput effici convolut techniqu reduc cost multipl factor propos activ function tradit sigmoid tanh activ function use implement neural network approach past decad graphic mathemat represent shown figur b figur activ function sigmoid function b hyperbol transient sigmoid tanh popular activ function call rectifi linear unit relu propos solv vanish gradient problem train deep learn approach basic concept simpl keep valu abov zero set neg valu zero shown figur relu activ wa first use alexnet figur activ function sigmoid function b hyperbol transient sigmoid ex tanh ex ex popular activ function call rectiÔ¨Å linear unit relu propos solv vanish gradient problem train deep learn approach basic concept simpl keep valu abov zero set neg valu zero shown figur relu activ wa Ô¨Årst use alexnet electron x peer review figur pictori represent rectifi linear unit relu mathemat express relu follow max ùë• activ function play crucial role learn weight deep architectur mani research focu becaus much done thi area meanwhil sever improv version relu propos provid even better accuraci figur pictori represent rectiÔ¨Å linear unit relu electron mathemat express relu follow max x activ function play crucial role learn weight deep architectur mani research focu becaus much done thi area meanwhil sever improv version relu propos provid even better accuraci compar relu activ function shown figur efÔ¨Åcient improv version relu activ function call parametr relu prelu propos kaim et al figur show pictori represent leaki relu elu activ function thi techniqu automat learn paramet adapt improv accuraci neglig extra comput cost figur pictori represent rectifi linear unit relu mathemat express relu follow max ùë• activ function play crucial role learn weight deep architectur mani research focu becaus much done thi area meanwhil sever improv version relu propos provid even better accuraci compar relu activ function shown figur effici improv version relu activ function call parametr relu prelu propos kaim et al figur show pictori represent leaki relu elu activ function thi techniqu automat learn paramet adapt improv accuraci neglig extra comput cost b figur diagram leaki relu rectifi linear unit b exponenti linear unit elu leaki relu max ùëéùë• ùë• ùëé constant valu elu ùë• ùëé ùë• recent propos exponenti linear unit activ function allow faster accur version dcnn structur furthermor tune neg part activ function creat leaki relu multipl expon linear unit melu propos recent shape rectifi linear activ unit propos survey modern activ function wa conduct figur diagram leaki relu rectiÔ¨Å linear unit b exponenti linear unit elu electron x peer review figur averag oper layer pool layer present two differ techniqu use implement deep network pool layer averag concept averag pool layer wa use first time lenet alexnet use layer instead conceptu diagram max pool averag pool oper shown figur concept special pyramid pool ha propos et al shown figur pyramid pool wa propos benjamin propos new architectur fraction max pool provid classif accuraci dataset thi structur gener network consid two import properti layer pool layer first pool layer limit gener deep structur network thi paper propos network overlap instead layer anoth paper ha conduct research differ type pool approach includ mix gate tree gener pool function figur averag oper leaki relu max ax x constant valu elu x x ex x recent propos exponenti linear unit activ function allow faster accur version dcnn structur furthermor tune neg part activ function creat leaki relu multipl expon linear unit melu propos recent shape rectiÔ¨Å linear activ unit propos survey modern activ function wa conduct electron layer pool layer present two differ techniqu use implement deep network pool layer averag concept averag pool layer wa use Ô¨Årst time lenet alexnet use layer instead conceptu diagram max pool averag pool oper shown figur concept special pyramid pool ha propos et al shown figur present two differ techniqu use implement deep network pool layer averag concept averag pool layer wa use first time lenet alexnet use layer instead conceptu diagram max pool averag pool oper shown figur concept special pyramid pool ha propos et al shown figur pyramid pool wa propos benjamin propos new architectur fraction max pool provid classif accuraci dataset thi structur gener network consid two import properti layer pool layer first pool layer limit gener deep structur network thi paper propos network overlap instead layer anoth paper ha conduct research differ type pool approach includ mix gate tree gener pool function figur spatial pyramid pool regular approach dl differ regular approach propos past year deep cnn simplest effici approach call dropout wa propos hinton dropout randomli select subset activ set zero within layer dropout concept shown figur figur spatial pyramid pool pyramid pool wa propos benjamin propos new architectur fraction max pool provid classiÔ¨Åc accuraci dataset thi structur gener network consid two import properti layer pool layer first layer limit gener deep structur network thi paper propos network overlap instead layer anoth paper ha conduct research differ type pool approach includ mix gate tree gener pool function regular approach dl differ regular approach propos past year deep cnn simplest efÔ¨Åcient approach call dropout wa propos hinton dropout randomli select subset activ set zero within layer dropout concept shown figur electron x peer review figur pictori represent concept dropout anoth regular approach call drop connect thi case instead drop activ subset weight within network layer set zero result layer receiv randomli select subset unit immedi previou layer regular approach propos well optim method dl differ optim method sgd adagrad adadelta rmsprop adam activ function improv upon case sgd wa propos ad variabl momentum improv train test accuraci h f ad h b l l l figur pictori represent concept dropout anoth regular approach call drop connect thi case instead drop activ subset weight within network layer set zero result layer receiv randomli select subset unit immedi previou layer regular approach propos well electron optim method dl differ optim method sgd adagrad adadelta rmsprop adam activ function improv upon case sgd wa propos ad variabl momentum improv train test accuraci case adagrad main contribut wa calcul adapt learn rate dure train thi method summat magnitud gradient consid calcul adapt learn rate case larg number epoch summat magnitud gradient becom larg result thi learn rate decreas radic caus gradient approach zero quickli main drawback thi approach caus problem dure train later rmsprop wa propos consid onli magnitud gradient immedi previou iter prevent problem adagrad provid better perform case adam optim approach propos base momentum magnitud gradient calcul adapt learn rate similar rmsprop adam ha improv overal accuraci help efÔ¨Åcient train better converg deep learn algorithm improv version adam optim approach ha propos recent call eve eve provid even better perform fast accur converg recurr neural network rnn introduct human thought persist human throw thing away start think scratch second read thi articl understand word sentenc base understand previou word sentenc tradit neural network approach includ dnn cnn deal thi type problem standard neural network cnn incap due follow reason first approach onli handl vector input imag video frame produc vector output probabl differ class second model oper Ô¨Åxed number comput step number layer model rnn uniqu allow oper sequenc vector time hopÔ¨Åeld newark introduc thi concept idea wa describ shortli pictori represent shown figur electron x peer review figur structur basic recurr neural network rnn loop differ version rnn propos jordan elman elman architectur use output hidden layer input alongsid normal input hidden layer hand output output unit use input input hidden layer jordan network jordan contrast use input output output unit input hidden layer mathemat express elman network jordan network figur structur basic recurr neural network rnn loop differ version rnn propos jordan elman elman architectur use output hidden layer input alongsid normal input hidden layer hand output output unit use input input hidden layer jordan network jordan contrast use input output output unit input hidden layer mathemat express elman network ht œÉh whxt bh electron yt œÉy jordan network ht œÉh bh yt œÉy xt vector input ht hidden layer vector yt output vector w u weight matric b bia vector loop allow inform pass one step network next recurr neural network thought multipl copi network network pass messag successor diagram figur show happen unrol loop architectur use output hidden layer input alongsid normal input hidden layer hand output output unit use input input hidden layer jordan network jordan contrast use input output output unit input hidden layer mathemat express elman network jordan network vector input hidden layer vector output vector w u weight matric b bia vector loop allow inform pass one step network next recurr neural network thought multipl copi network network pass messag successor diagram figur show happen unrol loop figur unrol rnn main problem rnn approach exist vanish gradient problem first time thi problem solv hochreit et al deep rnn consist subsequ layer wa implement evalu solv deep learn task sever solut propos solv vanish gradient problem rnn approach past decad two possibl effect solut thi problem first clip gradient scale gradient norm larg secondli creat better rnn model one better model wa introduc felix el name long memori lstm lstm differ advanc approach propos last year explain follow section diagram lstm shown figur rnn approach allow sequenc input output gener case exampl dl text mine build deep learn model textual data requir represent basic text unit word neural network structur hierarch captur sequenti natur text case rnn recurs neural network figur unrol rnn main problem rnn approach exist vanish gradient problem Ô¨Årst time thi problem solv hochreit et al deep rnn consist subsequ layer wa implement evalu solv deep learn task sever solut propos solv vanish gradient problem rnn approach past decad two possibl effect solut thi problem Ô¨Årst clip gradient scale gradient norm larg secondli creat better rnn model one better model wa introduc felix et al name long memori lstm lstm differ advanc approach propos last year explain follow section diagram lstm shown figur electron x peer review use languag understand languag model tri predict next word set word case sentenc base previou one rnn network loop allow inform persist anoth exampl rnn abl connect previou inform present task use previou video frame understand present tri gener futur frame well figur diagram long memori lstm long memori lstm key idea lstm cell state horizont line run top figur lstm remov add inform cell state call gate input gate forget gate output gate defin œÉ œÉ c tanh c œÉ lstm model popular tempor inform process paper includ figur diagram long memori lstm rnn approach allow sequenc input output gener case exampl dl text mine build deep learn model textual data requir represent basic text unit word neural network structur hierarch captur sequenti natur text case rnn recurs neural network use languag understand languag model tri predict next word set word case sentenc base previou one rnn network loop allow inform persist anoth exampl rnn abl connect previou inform present task use previou video frame understand present tri gener futur frame well electron long memori lstm key idea lstm cell state horizont line run top figur lstm remov add inform cell state call gate input gate forget gate ft output gate ot deÔ¨Ån ft œÉ xt bf œÉ xt bi e ct tan h xt bc ct ft ct ot œÉ xt bo ht h ct lstm model popular tempor inform process paper includ lstm model minor varianc discuss follow section slightli modiÔ¨Å version network peephol connect ger schimidhub propos concept peephol includ almost gate thi model figur diagram long memori lstm long memori lstm key idea lstm cell state horizont line run top figur lstm remov add inform cell state call gate input gate forget gate output gate defin œÉ œÉ c tanh c œÉ lstm model popular tempor inform process paper includ lstm model minor varianc discuss follow section slightli modifi version network peephol connect ger schimidhub propos concept peephol includ almost gate thi model figur diagram gate recurr unit gru gate recurr unit gru gru also came lstm slightli variat gru popular commun work recurr network main reason popular comput cost simplic model shown figur gru lighter version figur diagram gate recurr unit gru gate recurr unit gru gru also came lstm slightli variat gru popular commun work recurr network main reason popular comput cost simplic model shown figur gru lighter version rnn approach standard lstm term topolog comput cost complex thi techniqu combin forget input gate singl updat gate merg cell state hidden state along chang simpler model gru ha grow increasingli popular mathemat gru express follow equat zt œÉ xt rt œÉ xt e ht tan h rt xt ht zt ht question one best accord differ empir studi clear evid winner howev gru requir fewer network paramet make model faster hand lstm provid better perform enough data comput power variant lstm name deep lstm anoth variant electron bit differ approach call clockwork rnn import empir evalu differ version rnn approach includ lstm greff et al Ô¨Ånal conclus wa lstm variant anoth empir evalu conduct thousand rnn architectur includ lstm gru Ô¨Ånding work better lstm certain task convolut lstm convlstm problem fulli connect fc lstm short model handl spatiotempor data usag full connect transact spatial inform ha encod intern gate convlstm tensor last two dimens spatial dimens row column convlstm determin futur state certain cell grid respect input past state local neighbor achiev use convolut oper transit shown figur tanh h question one best accord differ empir studi clear evid winner howev gru requir fewer network paramet make model faster hand lstm provid better perform enough data comput power variant lstm name deep lstm anoth variant bit differ approach call clockwork rnn import empir evalu differ version rnn approach includ lstm greff et al final conclus wa lstm variant anoth empir evalu conduct thousand rnn architectur includ lstm gru find work better lstm certain task convolut lstm convlstm problem fulli connect fc lstm short model handl spatiotempor data usag full connect transact spatial inform ha encod intern gate convlstm tensor last two dimens spatial dimens row column convlstm determin futur state certain cell grid respect input past state local neighbor achiev use convolut oper transit shown figur figur pictori diagram convlstm convlstm provid good perform tempor data analysi video dataset mathemat convlstm express follow repres convolut oper denot hadamard product œÉ œÉ tanh œÉ variant architectur rnn respect applic figur pictori diagram convlstm convlstm provid good perform tempor data analysi video dataset mathemat convlstm express follow repres convolut oper hadamard product œÉ wxi xt whi whi bi ft œÉ wxf xt whf whf bf e ct tan h wxc xt whc bc ct ft ct ot œÉ wxo xt bo ht ot h ct variant architectur rnn respect applic incorpor attent mechan rnn use case word sentenc encod power word embed techniqu predict nn raw text input thi approach use differ Ô¨Åeld applic includ unsupervis learn word relationship learn differ word abil abstract higher mean word base similar sentenc model languag understand mani differ word embed approach propos past year use solv difÔ¨Åcult task provid perform includ machin translat languag model imag video caption time seri data analysi applic point view rnn solv differ type problem need differ architectur rnn shown figur figur input vector repres electron green rnn state repres blue orang repres output vector structur describ one one standard mode classiÔ¨Åc without rnn imag classiÔ¨Åc problem shown figur mani one sequenc input singl output sentiment analysi input set sentenc word output posit neg express shown figur one mani system take input produc sequenc output imag caption problem input singl imag output set word context shown figur mani mani sequenc input output machin translat machin take sequenc word english translat sequenc word french shown figur mani mani sequenc sequenc learn video classiÔ¨Åc problem take video frame input wish label frame video shown figur abil abstract higher mean word base similar sentenc model languag understand mani differ word embed approach propos past year use solv difficult task provid art perform includ machin translat languag model imag video caption time seri data analysi applic point view rnn solv differ type problem need differ architectur rnn shown figur figur input vector repres green rnn state repres blue orang repres output vector structur describ one one standard mode classif without rnn imag classif problem shown figur mani one sequenc input singl output sentiment analysi input set sentenc word output posit neg express shown figur one mani system take input produc sequenc output imag caption problem input singl imag output set word context shown figur mani mani sequenc input output machin translat machin take sequenc word english translat sequenc word french shown figur mani mani sequenc sequenc learn video classif problem take video frame input wish label frame video shown figur b c e figur differ structur rnn respect applic one one b mani one c one mani mani mani e mani mani figur differ structur rnn respect applic one one b mani one c one mani mani mani e mani mani model rnn differ model propos use rnn approach Ô¨Årst initi rnn attent automat learn describ content imag propos xu et al dual state attent base rnn propos effect time seri predict anoth difÔ¨Åcult task visual question answer vqa use gru input imag natur languag question imag task provid accur natur languag answer output condit imag textual input cnn use encod imag rnn implement encod sentenc anoth outstand concept releas googl call pixel recurr neural network pixel rnn thi approach provid perform imag complet task new model call residu rnn propos rnn introduc effect residu connect deep recurr network electron rnn applic rnn includ lstm gru appli tensor process natur languag process use rnn techniqu includ lstm gru convolut rnn base identiÔ¨Åc system ha propos time seri data analysi use rnn recent timenet wa propos base deep rnn time seri classiÔ¨Åc tsc speech audio process includ lstm acoust model sound event predict use convolut rnn audio tag use convolut gru earli heart failur detect propos use rnn rnn appli track monitor trafÔ¨Åc forecast system propos use graph convolut rnn gcrnn lstm base network trafÔ¨Åc predict system propos neural model bidirect deep rnn appli driver action predict vehicl trajectori predict use rnn action recognit use rnn collect anomali detect use lstm cybersecur ae restrict boltzmann machin rbm thi section discuss one unsupervis deep learn approach auto encod variat vae denois ae spars ae stack denois ae ae applic differ ae also discuss end thi chapter review ae ae deep neural network approach use unsupervis featur learn efÔ¨Åcient data encod decod main object autoencod learn repres encod input data typic data dimension reduct compress fusion mani thi autoencod techniqu consist two part encod decod encod phase input sampl map usual lower dimension featur space construct featur represent thi approach repeat desir featur dimension space reach wherea decod phase regener actual featur lower dimension featur revers process conceptu diagram encod decod phase shown figur electron x peer review figur diagram auto encod encod decod transit repres ùúë ùí≥ ‡∞ù ùúë consid simpl autoencod one hidden layer input ùí≥ map onto ‚Ñ± express follow ùúé‡¨µ ùëè w weight matrix b bia ùúé‡¨µ repres element wise activ function figur diagram auto encod encod decod transit repres œï x œï f œï œï œï electron consid simpl autoencod one hidden layer input x x map onto f express follow z wx b w weight matrix b bia repres element wise activ function sigmoid rectiÔ¨Å linear unit rlu let us consid z map reconstruct onto dimens reconstruct express thi model train minim reconstruct error deÔ¨Ån loss function follow l wx b usual featur space f ha lower dimens input featur space x consid compress represent input sampl case multilay auto encod oper repeat requir encod decod phase deep auto encod construct extend encod decod multipl hidden layer gradient vanish problem still big issu deeper model ae gradient becom small pass back mani layer ae model differ advanc ae model discuss follow section variat autoencod v ae limit use simpl gener adversari network gan discuss section Ô¨Årst imag gener use gan input nois someon want gener speciÔ¨Åc imag difÔ¨Åcult select speciÔ¨Åc featur nois randomli produc desir imag requir search entir distribut second gan differenti real fake object exampl want gener dog constraint dog must look like dog therefor produc style imag style look like dog close observ exactli howev vae propos overcom limit basic gan latent vector space use repres imag follow unit gaussian distribut conceptu diagram vae shown figur electron x peer review figur variat thi model two loss one mean squar error determin good network reconstruct imag loss kl diverg latent determin close latent variabl match unit gaussian distribut exampl suppos ùë• input hidden represent paramet weight bias reconstruct phase input ùëß desir output paramet weight bias repres encod ùëû‡∞è decod ùëù‡∞• respect loss function network latent space repres ùúÉ ùúô ùëù ùëß autoencod recent ae wa propos berkeley ai research bair lab figur variat thi model two loss one mean squar error determin good network reconstruct imag loss kl diverg latent determin close latent variabl match unit gaussian distribut exampl suppos x input hidden represent paramet weight bias reconstruct phase input z desir output paramet weight bias repres encod qŒ∏ decod pœÜ respect loss function network latent space repres electron li Œ∏ œÜ logpœÜ kl qŒ∏ p z autoencod recent ae wa propos berkeley ai research bair lab architectur modiÔ¨Åc tradit autoencod unsupervis represent learn thi architectur network split disjoint two network tri predict featur represent entir imag follow figur show concept autoencod latent determin close latent variabl match unit gaussian distribut exampl suppos ùë• input hidden represent paramet weight bias reconstruct phase input ùëß desir output paramet weight bias repres encod ùëû‡∞è decod ùëù‡∞• respect loss function network latent space repres ùúÉ ùúô ùëù ùëß autoencod recent ae wa propos berkeley ai research bair lab architectur modif tradit autoencod unsupervis represent learn thi architectur network split disjoint two network tri predict featur represent entir imag follow figur show concept autoencod figur autoencod applic ae ae appli cybersecur appli ae unsupervis featur extract appli winner take wta cluster sampl gener label ae ha use encod decod techniqu deep learn approach includ cnn dnn rnn rl last decad howev approach recent publish review rbm restrict boltzmann machin rbm anoth unsupervis deep learn approach train phase model use network call restrict boltzmann machin stochast binari pixel connect stochast binari featur detector use symmetr weight connect rbm undirect gener model use layer hidden variabl model distribut visibl variabl undirect model interact hidden visibl variabl use ensur contribut figur autoencod applic ae ae appli cybersecur appli ae unsupervis featur extract appli winner take wta cluster sampl gener label ae ha use encod decod techniqu deep learn approach includ cnn dnn rnn rl last decad howev approach recent publish review rbm restrict boltzmann machin rbm anoth unsupervis deep learn approach train phase model use network call restrict boltzmann machin stochast binari pixel connect stochast binari featur detector use symmetr weight connect rbm undirect gener model use layer hidden variabl model distribut visibl variabl undirect model interact hidden visibl variabl use ensur contribut likelihood term posterior hidden variabl approxim factori greatli facilit infer conceptu diagram rbm shown figur electron x peer review likelihood term posterior hidden variabl approxim factori greatli facilit infer conceptu diagram rbm shown figur figur block diagram restrict boltzmann machin rbm model mean probabl distribut variabl interest defin energi function energi function compos set observ variabl set hidden variabl node visibl layer j node hidden layer restrict sens connect valu correspond visibl unit rbm becaus state observ featur detector correspond hidden unit joint configur v h visibl hidden figur block diagram restrict boltzmann machin rbm electron model mean probabl distribut variabl interest deÔ¨Ån energi function energi function compos set observ variabl v vi set hidden variabl hi node visibl layer j node hidden layer restrict sens connect valu correspond visibl unit rbm becaus state observ featur detector correspond hidden unit joint conÔ¨Ågur v h visibl hidden unit ha energi hopÔ¨Åeld given e v h aivi j bjhj j viwi jhj vi hj binari state visibl unit hidden unit j ai bj bias wij weight network assign probabl possibl pair visibl hidden vector via thi energi function p v h z v h partit function z given sum possibl pair visibl hidden vector z v h v h probabl network assign visibl vector v given sum possibl hidden vector p v z h v h probabl network assign train sampl rais adjust weight bias lower energi sampl rais energi sampl especi low energi therefor make big contribut partit function deriv log probabl train vector respect weight surprisingli simpl v vihj data vihj model angl bracket use denot expect distribut speciÔ¨Å subscript follow thi lead simpl learn rule perform stochast steepest ascent log probabl train data wij Œµ vihj data vihj model Œµ learn rate given randomli select train imag v binari state hj hidden unit j set probabl p œÉ bj viwij œÉ x logist sigmoid function e vihj unbias sampl becaus direct connect visibl unit rbm also easi get unbias sampl state visibl unit given hidden vector p vi œÉ ai j hjwij electron get unbias sampl vihj model much difÔ¨Åcult done start ani random state visibl unit perform altern gibb sampl long time singl iter altern gibb sampl consist updat hidden unit parallel use equat follow updat visibl unit parallel use follow equat much faster learn procedur wa propos hinton thi start set state visibl unit train vector binari state hidden unit comput parallel use equat onc binari state chosen hidden unit reconstruct produc set vi probabl given equat chang weight given Œµ vihj data vihj recon simpliÔ¨Å version learn rule use state individu unit instead pairwis product use bias thi approach mainli use neural network unsupervis manner gener initi weight one popular deep learn approach call deep belief network dbn propos base thi approach exampl applic rbm dbn data encod news cluster imag segment cybersecur shown detail see refer gener adversari network gan begin thi chapter start quot yann lecun gan best concept propos last ten year Ô¨Åeld deep learn neural network review gan concept gener model machin learn start long time befor use data model condit probabl densiti function gener thi type model consid probabilist model joint probabl distribut observ target label valu howev see big success thi gener model befor recent deep gener model becom popular shown enorm success differ applic domain deep learn techniqu perform better number input sampl increas due thi reason learn reusabl featur represent huge number dataset ha becom activ research area mention introduct comput vision ha differ task segment classiÔ¨Åc detect requir larg amount label data thi problem ha attempt solv gener similar sampl gener model gener adversari network gan deep learn approach recent invent goodfellow gan offer altern approach maximum likelihood estim techniqu gan unsupervis deep learn approach two neural network compet game case imag gener problem gener start gaussian nois gener imag discrimin determin good gener imag thi process continu output gener becom close actual input sampl accord figur consid discrimin gener g two player play game function v g express follow accord thi paper mingmaxd v g x log x z log g z practic thi equat may provid sufÔ¨Åcient gradient learn g start random gaussian nois earli stage earli stage reject sampl becaus clearli differ compar train sampl thi case log g z satur instead train g minim log g z train g maxim log g z object electron function provid much better gradient earli stage dure learn howev limit converg dure train Ô¨Årst version begin state gan ha limit regard follow issu lack heurist cost function approxim mean squar error mse unstabl train sometim becaus produc nonsens output electron x peer review techniqu gan unsupervis deep learn approach two neural network compet game case imag gener problem gener start gaussian nois gener imag discrimin determin good gener imag thi process continu output gener becom close actual input sampl accord figur consid discrimin gener g two player play game function v g express follow accord thi paper ùëâ ùê∑ ùê∫ ·àæùëôùëúùëî ùê∑ ùë• ·àæùëôùëúùëî ùê∫ ùëß ·àø figur conceptu diagram gener adversari network gan practic thi equat may provid suffici gradient learn g start random gaussian nois earli stage earli stage reject sampl becaus clearli differ compar train sampl thi case ùëôùëúùëî ùê∫ ùëß satur instead train g minim ùëôùëúùëî ùê∫ ùëß train g maxim ùëôùëúùëî ùê∫ ùëß object function provid much better gradient earli stage dure learn howev limit converg dure train first version begin state gan ha limit regard follow issu lack heurist cost function approxim mean squar error mse unstabl train sometim becaus produc nonsens output research area gan ha ongo mani improv version propos gan abl produc photorealist imag applic visual interior industri design shoe bag cloth item gan also extens use field game develop artifici video gener gan two differ area dl fall unsupervis research area focus topolog gan architectur improv function train approach deep convolut gan dcgan gan approach propos thi approach ha shown promis result compar unsupervis counterpart regener result dcgan shown follow figur figur accord articl show output gener bedroom imag one train pass dataset figur includ thi section gener experi theoret model could learn memor train exampl thi experiment unlik train small learn rate mini batch sgd awar prior empir evid demonstr memor sgd small learn rate figur conceptu diagram gener adversari network gan research area gan ha ongo mani improv version propos gan abl produc photorealist imag applic visual interior industri design shoe bag cloth item gan also extens use Ô¨Åeld game develop artiÔ¨Åci video gener gan two differ area dl fall unsupervis research area focus topolog gan architectur improv function train approach deep convolut gan dcgan gan approach propos thi approach ha shown promis result compar unsupervis counterpart regener result dcgan shown follow Ô¨Ågure figur accord articl show output gener bedroom imag one train pass dataset Ô¨Ågure includ thi section gener experi theoret model could learn memor train exampl thi experiment unlik train small learn rate mini batch sgd awar prior empir evid demonstr memor sgd small learn rate electron x peer review figur experiment output bedroom imag figur repres gener bedroom imag five epoch train appear evid visual via repeat nois textur across multipl sampl baseboard bed figur experiment output bedroom imag figur repres gener bedroom imag Ô¨Åve epoch train appear evid visual via repeat nois textur across multipl sampl baseboard bed electron figur experiment output bedroom imag figur repres gener bedroom imag five epoch train appear evid visual via repeat nois textur across multipl sampl baseboard bed figur reconstruct bedroom imag use deep convolut gan dcgan figur accord articl top row interpol seri nine random point z show learn space ha smooth transit everi imag space plausibl look like bedroom row see room without window slowli transform room giant window row see appear tv slowli transform window follow figur show effect applic latent space vector latent space vector turn mean output first perform addit subtract oper follow decod figur accord articl show man glass minu man add woman result woman glass figur exampl smile arithmet arithmet wear glass use gan man glass minu man without glass plu woman without glass equal woman glass figur accord articl show turn vector wa creat four averag sampl face look left versu look right ad interpol along thi axi random sampl pose reliabl transform interest applic propos gan exampl natur indoor scene gener improv gan structur gan learn surfac normal combin style gan wang figur reconstruct bedroom imag use deep convolut gan dcgan figur accord articl top row interpol seri nine random point z show learn space ha smooth transit everi imag space plausibl look like bedroom row see room without window slowli transform room giant window row see appear tv slowli transform window follow figur show effect applic latent space vector latent space vector turn mean output Ô¨Årst perform addit subtract oper follow decod figur accord articl show man glass minu man add woman result woman glass figur experiment output bedroom imag figur repres gener bedroom imag five epoch train appear evid visual via repeat nois textur across multipl sampl baseboard bed figur reconstruct bedroom imag use deep convolut gan dcgan figur accord articl top row interpol seri nine random point z show learn space ha smooth transit everi imag space plausibl look like bedroom row see room without window slowli transform room giant window row see appear tv slowli transform window follow figur show effect applic latent space vector latent space vector turn mean output first perform addit subtract oper follow decod figur accord articl show man glass minu man add woman result woman glass figur exampl smile arithmet arithmet wear glass use gan man glass minu man without glass plu woman without glass equal woman glass figur accord articl show turn vector wa creat four averag sampl face look left versu look right ad interpol along thi axi random sampl pose reliabl transform interest applic propos gan exampl natur indoor scene gener improv gan structur gan learn surfac normal combin style gan wang figur exampl smile arithmet arithmet wear glass use gan man glass minu man without glass plu woman without glass equal woman glass figur accord articl show turn vector wa creat four averag sampl face look left versu look right ad interpol along thi axi random sampl pose reliabl transform interest applic propos gan exampl natur indoor scene gener improv gan structur gan learn surfac normal combin style gan wang gupta thi implement author consid style structur gan name gener surfac normal map thi improv version gan extens gan call infogan wa propos infogan learn better represent complet unsupervis manner experiment result show unsupervis infogan competit represent learn fulli supervis learn approach anoth new architectur wa propos im et al recurr concept includ adversari network dure train chen et al propos info gan igan allow imag manipul interact natur imag manifold imag imag translat condit adversari network propos anoth improv version gan name coupl gener adversari network cogan learn joint distribut imag exist approach doe need tupl correspond imag differ domain train set bidirect gener adversari network bigan learn invers featur map shown result learn featur represent electron use auxiliari supervis discrimin task competit contemporari approach featur learn includ adversari network dure train chen et al propos info gan igan allow imag manipul interact natur imag manifold imag imag translat condit adversari network propos anoth improv version gan name coupl gener adversari network cogan learn joint distribut imag exist approach doe need tupl correspond imag differ domain train set bidirect gener adversari network bigan learn invers featur map shown result learn featur represent use auxiliari supervis discrimin task competit contemporari approach featur learn figur face gener differ angl use gan recent googl propos extend version gan call boundari equilibrium gener adversari network began simpl robust architectur began ha better train procedur fast stabl converg concept equilibrium help balanc power discrimin gener addit balanc imag divers visual qualiti anoth similar work call wasserstein gan wgan algorithm show signific benefit tradit gan wgan two major benefit tradit gan first wgan meaning correl loss metric gener converg sampl qualiti secondli wgan improv stabil optim process improv version wgan propos new clip techniqu penal normal gradient critic respect input promis architectur ha propos base gener model imag repres untrain dnn give opportun better understand visual dnn adversari exampl gener model also introduc gan wa propos yann lecun facebook train process difficult gan manifold match gan mmgan propos better train process experi three differ dataset experiment result clearli demonstr efficaci mmgan model gan simul invers effici train approach probabilist gan pgan new kind gan modifi object function main idea behind thi method integr probabilist model gaussian mixtur model gan framework support likelihood rather classif gan bayesian figur face gener differ angl use gan recent googl propos extend version gan call boundari equilibrium gener adversari network began simpl robust architectur began ha better train procedur fast stabl converg concept equilibrium help balanc power discrimin gener addit balanc imag divers visual qualiti anoth similar work call wasserstein gan wgan algorithm show signiÔ¨Åc beneÔ¨Åt tradit gan wgan two major beneÔ¨Åt tradit gan first wgan meaning correl loss metric gener converg sampl qualiti secondli wgan improv stabil optim process improv version wgan propos new clip techniqu penal normal gradient critic respect input promis architectur ha propos base gener model imag repres untrain dnn give opportun better understand visual dnn adversari exampl gener model also introduc gan wa propos yann lecun facebook train process difÔ¨Åcult gan manifold match gan mmgan propos better train process experi three differ dataset experiment result clearli demonstr efÔ¨Åcaci mmgan model gan simul invers efÔ¨Åcient train approach probabilist gan pgan new kind gan modiÔ¨Å object function main idea behind thi method integr probabilist model gaussian mixtur model gan framework support likelihood rather classiÔ¨Åc gan bayesian network model variat auto encod popular deep learn approach train adversari variat bay avb help establish principl connect vae gan propos base gener neural network markov gan textur synthesi anoth gener model base doubli stochast mcmc method gan unsupervis gan capabl learn pixel level domain adapt transform pixel space one domain anoth domain thi approach provid perform sever unsupervis domain adapt techniqu larg margin new network propos call schema network gener physic simul abl disentangl multipl caus event reason caus achiev goal learn dynam environ data interest research ha conduct gan gener adversari text imag synthesi thi paper new deep architectur propos gan formul take text descript imag produc realist imag respect input thi effect techniqu electron imag synthesi use charact level text encod class condit gan gan evalu bird Ô¨Çower dataset Ô¨Årst gener text imag evalu ms coco dataset applic gan thi learn algorithm ha appli differ domain applic discuss follow section gan imag process gan use gener imag use approach gan semant segment semi weakli supervis approach text condit auxiliari classiÔ¨Å gan use gener synthes imag text descript gener network retain function approach fast speed thi network match imag style multipl scale put comput burden train time vision system struggl rain snow fog singl imag system propos use gan recent gan speech audio process dialogu system use gener hierarch neural network model addit gan use Ô¨Åeld speech analysi recent gan use speech enhanc call segan incorpor design improv perform progress gan music gener perform compar melodi rnn gan medic inform process gan medic imagin medic inform process gan medic imag wasserstein distanc perceptu loss gan also use segment brain tumor condit gan cgan gener medic imag segment approach propos use gan call segan befor deep learn revolut compress sens one hottest topic howev deep gan use compress sens autom mri addit gan also use health record process due privaci issu electron health record ehr limit publicli avail like dataset gan appli synthet ehr data could mitig risk time seri data gener recurr gan rgan recurr condit gan rcgan ha introduc logan consist combin gener discrimin model detect overÔ¨Åt recognit input thi techniqu ha compar gan techniqu includ gan dcgan began combin dcgan vae applic new approach call bayesian condit gan gener sampl determinist input thi simpli gan bayesian framework handl supervis unsupervis learn problem machin learn deep learn commun onlin learn import approach gan use onlin learn train Ô¨Ånding mix strategi game name checkov gan gener moment match network base statist hypothesi test call maximum mean discrep mmd one interest idea replac discrimin gan base kernel mmd call thi approach signiÔ¨Åcantli outperform electron gener moment match network gmmn techniqu altern approach gener model applic gan includ pose estim photo edit network anomali detect discogan learn relat gan unsupervis translat gener model singl shot learn gan respons gener question answer system last least wavenet gener model ha develop gener audio waveform dual path network deep reinforc learn drl previou section focus supervis unsupervis deep learn approach includ dnn cnn rnn includ lstm gru ae rbm gan etc type deep learn approach use predict classiÔ¨Åc encod decod data gener mani applic domain howev thi section demonstr survey deep reinforc learn drl base recent develop method thi Ô¨Åeld rl review drl drl learn approach learn act gener sens unknown real environ detail pleas read follow articl conceptu diagram drl approach shown figur rl appli differ scope Ô¨Åeld includ fundament scienc decis make machin learn comput scienc point view Ô¨Åeld engin mathemat optim control robot control power station control wind turbin neurosci reward strategi wide studi last coupl decad also appli econom util game theori make better decis invest choic psycholog concept classic condit anim learn reinforc learn techniqu match situat action reinforc learn differ supervis learn techniqu kind learn approach studi recent includ tradit machin learn statist pattern recognit ann electron x peer review figur conceptu diagram reinforc learn rl system unlik gener supervis unsupervis machin learn rl defin character learn method character learn problem howev recent success dl ha huge impact success drl known drl accord learn strategi rl techniqu learn observ observ environ promis dl techniqu includ cnn rnn lstm gru use depend upon observ space dl techniqu encod data effici therefor follow step action perform accur accord action agent receiv appropri reward respect result entir rl approach becom effici learn interact environ better perform howev histori modern drl revolut began googl deep mind atari game drl drl base approach perform better human expert almost game thi case environ observ video frame process use cnn success drl approach depend level difficulti task attempt solv huge success atari googl deep mind propos reinforc learn environ base starcraft ii call figur conceptu diagram reinforc learn rl system unlik gener supervis unsupervis machin learn rl deÔ¨Ån character learn method character learn problem howev recent success dl ha huge impact success drl known drl accord learn strategi rl techniqu learn observ observ environ promis dl techniqu includ cnn rnn lstm gru use depend upon observ space dl techniqu encod data efÔ¨Åcient therefor follow step action perform accur accord action agent receiv appropri reward respect result entir rl approach becom efÔ¨Åcient learn interact environ better perform electron howev histori modern drl revolut began googl deep mind atari game drl drl base approach perform better human expert almost game thi case environ observ video frame process use cnn success drl approach depend level difÔ¨Åculti task attempt solv huge success atari googl deep mind propos reinforc learn environ base starcraft ii call starcraft ii learn environ game multipl player interact thi propos approach ha larg action space involv select control hundr unit contain mani state observ raw featur space use strategi thousand step open sourc starcraft ii game engin ha provid free onlin fundament strategi essenti know work drl first rl learn approach ha function calcul qualiti combin call algorithm describ basic comput Ô¨Çow deÔ¨Ån reinforc learn approach use Ô¨Ånd optim polici ani given Ô¨Ånite markov decis process mdp mdp mathemat framework model decis use state action reward onli need know state avail possibl action state anoth improv version known thi articl discuss detail pleas see refer step choos action maxim follow function q q estim util tell us good action given certain state r immedi reward make action best util q result state thi formul recurs deÔ¨Ånit follow q r Œ≥ thi equat call bellman equat core equat rl r immedi reward Œ≥ rel valu delay immedi reward new state action action sate respect action select base follow equat œÄ argmaxaq state valu assign call visit state receiv reward accordingli use reward updat estim valu state reward stochast result need visit state mani time addit guarante get reward rt anoth episod summat futur reward episod task environ unpredict futur go reward divers express gt rt sum discount futur reward case factor scalar gt Œ≥ Œ≥trt Œ≥ constant futur less take reward account properti converg approxim converg true must visit possibl pair inÔ¨Ånit mani time electron state tabl size vari depend observ space complex unseen valu consid dure observ way Ô¨Åx problem use neural network particularli dnn approxim instead state tabl input dnn state action output number repres util encod state action properli place deep learn approach contribut make better decis respect state inform case observ environ use sever acquisit devic includ camera sens devic observ learn environ exampl observ setup challeng seen environ action reward learn base pixel valu pixel action detail see refer howev difÔ¨Åcult develop agent interact perform well ani observ environ therefor research Ô¨Åeld select action space environ befor train agent environ benchmark concept thi case littl bit differ compar supervis unsupervis deep learn approach due varieti environ benchmark depend level difÔ¨Åculti environ ha consid compar previou exit research difÔ¨Åculti depend differ paramet number agent way interact agent number player recent anoth good learn approach ha propos drl mani paper publish differ network drl includ deep dqn doubl dqn asynchron method polici optim strategi includ determinist polici gradient deep determinist polici gradient guid polici search trust region polici optim combin polici gradient propos polici gradient dagger superhuman go use supervis learn polici gradient mont carlo tree search valu function robot manipul use guid polici search drl game use polici gradient algorithm initi pair initi tabl entri ÀÜ q zero step observ current state repeat select action execut receiv immedi reward r observ new state updat tabl entri ÀÜ q follow ÀÜ q r Œ≥ q recent trend drl applic survey publish recent basic rl drl dqn trust region polici optim asynchron advantag propos thi paper also discuss advantag deep learn focus visual understand via rl current trend research network cohes constrain base onlin rl techniqu propos health care mobil devic call mhealth thi system help similar user share inform efÔ¨Åcient improv convert limit user inform polici similar work electron rl propos health care mobil devic person mhealth intervent thi work cluster appli group peopl Ô¨Ånalli share rl polici group optim polici learn challeng task rl agent initi set ooi allow agent learn optim polici challeng task pomdp learn faster rnn bin pack problem bpp propos drl main object place number item minim surfac area bin import compon drl reward determin base observ action agent reward function perfect time due sensor error agent may get maximum reward wherea actual reward smaller thi paper propos formul base gener markov decis problem mdp call corrupt reward mdp trust region optim base deep rl propos use recent develop approxim curvatur addit research ha conduct evalu physic experi use deep learn approach thi experi focus agent learn basic properti mass cohes object interact simul environ recent fuzzi rl polici propos suitabl continu state action space import investig discuss made polici gradient continu control gener varianc algorithm thi paper also provid guidelin report result comparison baselin method deep rl also appli high precis assembl task bellman equat one main function rl techniqu function approxim propos ensur bellman optim equat alway hold function estim maxim likelihood observ motion drl base hierarch system use could resourc alloc power manag could comput system novel face hallucin propos deep rl use enhanc qualiti imag singl patch imag appli face imag bayesian deep learn bdl dl approach provid accuraci differ applic howev dl approach unabl deal uncertainti given task due model uncertainti learn approach take input assum class probabl without justif two african american human recogn gorilla imag classif system sever applic domain uncertainti rais includ car applic howev bdn intersect dl bayesian probabl approach show better result differ applic understand uncertainti problem includ problem uncertainti estim appli probabl distribut model weight map output probabl bdl becom veri popular among dl research commun addit bdl approach propos cnn techniqu probabl distribut appli weight techniqu help deal model overÔ¨Åt problem lack train sampl two common challeng dl approach final research paper publish recent advanc techniqu propos bdl transfer learn transfer learn good way explain transfer learn look relationship teacher offer cours gather detail knowledg regard subject inform electron convey seri lectur time thi consid teacher expert transfer inform knowledg student learner thing happen case deep learn network train big amount data dure train model learn weight bia weight transfer network test retrain similar new model network start weight instead train scratch conceptu diagram transfer learn method shown figur justif two african american human recogn gorilla imag classif system sever applic domain uncertainti rais includ car applic howev bdn intersect dl bayesian probabl approach show better result differ applic understand uncertainti problem includ problem uncertainti estim appli probabl distribut model weight map output probabl bdl becom veri popular among dl research commun addit bdl approach propos cnn techniqu probabl distribut appli weight techniqu help deal model overfit problem lack train sampl two common challeng dl approach final research paper publish recent advanc techniqu propos bdl figur conceptu diagram transfer learn pretrain imagenet transfer learn use retrain pascal dataset transfer learn transfer learn good way explain transfer learn look relationship teacher offer cours gather detail knowledg regard subject inform convey seri lectur time thi consid teacher expert transfer inform knowledg student learner thing happen case deep learn network train big amount data dure train model learn weight bia weight transfer network test retrain similar new model network start weight instead train scratch conceptu diagram transfer learn method shown figur figur conceptu diagram transfer learn pretrain imagenet transfer learn use retrain pascal dataset model model model alreadi train domain intend domain exampl imag recognit task incept model alreadi train imagenet download incept model use differ recognit task instead train scratch weight left learn featur thi method train use lack sampl data lot model avail includ vgg resnet incept net differ dataset follow link http whi use model lot reason use model firstli requir lot expens comput power train big model big dataset secondli take multipl week train big model train new model weight speed converg well help network gener use model need consid follow criterion respect applic domain size dataset use weight shown tabl tabl criterion need consid transfer learn method new dataset small new dataset larg model similar new dataset freez weight train linear classiÔ¨Å top level featur layer faster converg better gener model differ new dataset freez weight train linear classiÔ¨Å featur layer enhanc converg speed electron work infer research group work speciÔ¨Åc infer applic look optim approach includ model compress model compress import realm mobil devic special purpos hardwar becaus make model energi efÔ¨Åcient well faster myth deep learn myth need million label sampl train deep learn model answer ye case transfer learn approach use train deep learn approach without larg amount label data exampl follow figur demonstr strategi transfer learn approach detail primari model ha train larg amount label data imagenet weight use train pascal dataset actual realiti possibl learn use represent unlabel data transfer learn help learn represent relat task take train network differ domain adapt ani domain target task first train network close domain easi get label data use standard backpropag exampl imagenet classiÔ¨Åc pseudo class augment data cut top layer network replac supervis object target domain final tune network use backpropag label target domain valid loss start increas survey paper book publish transfer learn learn transfer learn boost approach transfer learn energi efÔ¨Åcient approach hardwar dl overview dnn success appli achiev better recognit accuraci differ applic domain comput vision speech process natur languag process big data problem mani howev case train execut graphic process unit gpu deal big volum data expens term power recent research train test deeper wider network achiev even better classiÔ¨Åc accuraci achiev human beyond human level recognit accuraci case size neural network increas becom power provid better classiÔ¨Åc accuraci howev storag consumpt memori bandwidth comput cost increas exponenti hand type massiv scale implement larg number network paramet suitabl low power implement unman aerial vehicl uav differ medic devic low memori system mobil devic field programm gate array fpga much research go develop better network structur network lower comput cost fewer number paramet system without lower classiÔ¨Åc accuraci two way design efÔ¨Åcient deep network structur Ô¨Årst approach optim intern oper cost efÔ¨Åcient network structur second design network low precis oper hardwar efÔ¨Åcient network intern oper paramet network structur reduc use low dimension convolut Ô¨Ålter convolut layer lot beneÔ¨Åt thi approach firstli convolut rectiÔ¨Åc oper make decis discrimin secondli main beneÔ¨Åt thi approach reduc number comput paramet drastic exampl one layer ha dimension electron Ô¨Ålter replac two dimension Ô¨Ålter without pool layer better featur learn three dimension Ô¨Ålter use replac dimension Ô¨Ålter beneÔ¨Åt use Ô¨Ålter assum present convolut layer ha c channel three layer Ô¨Ålter total number paramet weight c c weight wherea size Ô¨Ålter total number paramet c c almost doubl compar three Ô¨Ålter paramet moreov placement layer convolut pool network differ interv ha impact overal classiÔ¨Åc accuraci strategi mention optim network architectur recent design robust deep learn model efÔ¨Åcient implement cnn fpga platform strategi replac Ô¨Ålter Ô¨Ålter main reason use lower dimens Ô¨Ålter reduc overal number paramet replac Ô¨Ålter reduc number paramet strategi decreas number input channel Ô¨Ålter layer size output featur map calcul relat network paramet use n input map size f Ô¨Ålter size stride reduc number paramet onli enough reduc size Ô¨Ålter also requir control number input channel featur dimens strategi late network convolut layer activ map output present convolut layer least often larger output width height control criterion size input sampl choos post sampl layer commonli pool layer averag max pool layer use altern layer convolut Ô¨Ålter stride earlier layer larger stride layer small number activ map binari ternari connect neural network comput cost reduc drastic low precis multipl multipl drop connect paper also introduc binari connect neural network bnn ternari connect neural network tnn gener multipl weight activ forward propag gradient calcul backward propag main oper deep neural network binari connect bnn techniqu elimin multipl oper convert weight use forward propag binari constrain onli two valu result multipl oper perform simpl addit subtract make train process faster two way convert real valu correspond binari valu determinist stochast case determinist techniqu straightforward threshold techniqu appli weight altern way stochast approach matrix convert binari base probabl hard sigmoid function use becaus comput inexpens experiment result show signiÔ¨Åcantli good recognit accuraci sever advantag bnn follow observ binari multipl gpu almost seven time faster tradit matrix multipl gpu forward pass bnn drastic reduc memori size access replac arithmet oper oper lead great increas power efÔ¨Åcienc binar kernel use cnn reduc around complex dedic hardwar also observ memori access typic consum energi compar arithmet oper memori access cost increas memori size bnn beneÔ¨Åci respect aspect electron techniqu propos last year anoth power efÔ¨Åcient hardwar friendli network structur ha propos cnn xnor oper xnor base cnn implement Ô¨Ålter input convolut layer binari thi result faster convolut oper memori save paper wa propos save around memori save make possibl implement network cpu use instead gpu network test imagenet dataset provid onli less classiÔ¨Åc accuraci alexnet measur thi network requir less power comput time thi could make possibl acceler train process deep neural network dramat special hardwar implement Ô¨Årst time energi efÔ¨Åcient deep neural network eedn architectur wa propos neuromorph system addit releas deep learn framework call eedn provid close accuraci accuraci almost popular benchmark except imagenet dataset hardwar dl along algorithm develop dl approach mani hardwar architectur propos past year detail present trend hardwar deep learn publish recent mit propos eyeriss hardwar deep convolut neural network dcnn anoth architectur machin learn call dadiannao googl develop hardwar name tensor process unit tpu deep learn wa releas efÔ¨Åcient hardwar work infer wa releas propos stanford univers call efÔ¨Åcient infer engin eie ibm releas neuromorph system call truenorth deep learn approach limit hpc platform lot applic alreadi develop run mobil devic mobil platform provid data relev everyday activ user make mobil system efÔ¨Åcient robust retrain system collect data research ongo develop hardwar friendli algorithm dl topic sever import topic includ framework sdk benchmark dataset relat journal confer includ appendix summari thi paper provid review deep learn applic past year differ deep learn model differ categori learn includ supervis unsupervis reinforc learn rl well applic differ domain review addit explain detail differ supervis deep learn techniqu includ dnn cnn rnn deep learn techniqu includ ae rbm gan review detail section consid explain unsupervis learn techniqu propos base lstm rl section present survey deep reinforc learn drl fundament learn techniqu call recent develop bayesian deep learn bdl transfer learn tl approach also discuss section respect furthermor conduct survey energi efÔ¨Åcient deep learn approach transfer learn dl hardwar develop trend dl moreov discuss dl framework benchmark dataset often use implement evalu deep learn approach final includ relev journal confer dl commun ha publish valuabl research articl electron fund thi work wa support nation scienc foundat award acknowledg would like thank author mention refer thi paper learn lot thu made thi review paper possibl conÔ¨Çict interest author declar conÔ¨Çict interest appendix time peopl use differ deep learn framework standard develop kit sdk implement deep learn approach list framework tensorÔ¨Çow http caff http kera http theano http torch http pytorch http lasagn http http chainer http digit http cntk microsoft http matconvnet http minerva http mxnet http opendeep http purin http http tensorlay http lbann http sdk cudnn http tensorrt http deepstreamsdk http cubla http cuspars http nccl http benchmark dataset list benchmark dataset use often evalu deep learn approach differ domain applic imag classiÔ¨Åc detect segment list dataset use Ô¨Åeld imag process comput vision mnist http cifar http http electron caltech http http norb http http imagenet http nation data scienc bowl competit http coil http ms coco dataset http scene dataset http dataset http pascal voc dataset http human attribut dataset http face recognit dataset http visit http http recent introduc dataset googl open imag imag video http text classiÔ¨Åc text categor collect http sentiment analysi stanford http movi sentiment analysi cornel http languag model free ebook http brown stanford corpu present amer english http corpu googl word corpu http imag caption http common object context coco http overview http machin translat pair sentenc english french http european parliament proceed parallel corpu http statist machin translat http question answer stanford question answer dataset squad http electron dataset deepmind http amazon dataset http http qamain http http http speech recognit timit http voxforg http open speech languag resourc http document summar http http http sentiment analysi imdb dataset http hyperspectr imag analysi http http http addit anoth altern solut data program label subset data use weak supervis strategi domain heurist label function even noisi may conÔ¨Çict sampl journal confer gener research publish primari version research arxiv http confer accept paper deep learn relat Ô¨Åeld popular confer list confer neural inform process system nip intern confer learn represent iclr deep learn intern confer machin learn icml comput vision pattern recognit cvpr deep learn intern confer comput vision iccv european confer comput vision eccv british machin vision confer bmvc journal journal machin learn research jmlr ieee transact neural network learn system itnnl ieee transact pattern analysi machin intellig tpami comput vision imag understand cviu pattern recognit letter electron neural comput applic intern journal comput vision ieee transact imag process ieee comput intellig magazin proceed ieee ieee signal process magazin neural process letter pattern recognit neural network isppr journal photogrammetri remot sens tutori deep learn http http http cours reinforc learn http book deep learn http http http refer schmidhub deep learn neural network overview neural netw crossref pubm bengio lecun hinton deep learn natur bengio courvil vincent represent learn review new perspect ieee tran pattern anal mach intel crossref pubm bengio learn deep architectur ai found trend mach learn crossref mnih kavukcuoglu silver rusu veness bellemar grave riedmil fidjeland ostrovski et al control deep reinforc learn natur crossref pubm mnih kavukcuoglu silver grave antonogl wierstra riedmil play atari deep reinforc learn arxiv krizhevski sutskev hinton imagenet classiÔ¨Åc deep convolut neural network proceed intern confer neural inform process system lake taho nv usa decemb pp zeiler fergu visual understand convolut network arxiv simonyan zisserman deep convolut network imag recognit arxiv szegedi liu jia sermanet reed anguelov erhan vanhouck rabinovich go deeper convolut proceed ieee confer comput vision pattern recognit boston usa june pp zhang ren sun deep residu learn imag recognit proceed ieee confer comput vision pattern recognit la vega nv usa june pp canziani paszk culurciello analysi deep neural network model practic applic arxiv electron zweig classiÔ¨Åc recognit direct segment model proceed ieee intern confer acoust speech signal process icassp kyoto japan march pp efÔ¨Åcient segment condit random Ô¨Åeld phone recognit proceed thirteenth annual confer intern speech commun associ portland usa septemb deng yu jiang deep segment neural network speech recognit interspeech tang wang gimpel livescu discrimin segment cascad phone recognit proceed ieee workshop automat speech recognit understand asru scottsdal az usa decemb pp song cai deep neural network automat speech recognit error avail onlin http access januari deng yu deep convolut neural network use heterogen pool trade acoust invari phonet confus proceed ieee intern confer acoust speech signal process icassp vancouv bc canada may pp grave moham hinton speech recognit deep recurr neural network proceed ieee intern confer acoust speech signal process icassp vancouv bc canada may pp zhang pezeshki brakel zhang bengio courvil toward speech recognit deep convolut neural network arxiv deng platt ensembl deep learn speech recognit proceed fifteenth annual confer intern speech commun associ singapor septemb chorowski bahdanau serdyuk cho bengio model speech recognit advanc neural inform process system mit press cambridg usa pp lu kong dyer smith renal segment recurr neural network speech recognit arxiv van essen kim pearc boaky chen lbann livermor big artiÔ¨Åci neural network hpc toolkit proceed workshop machin learn comput environ austin tx usa novemb li yu shahabi liu graph convolut recurr neural network trafÔ¨Åc forecast arxiv md aspira taha asari bowen advanc deep convolut neural network approach digit patholog imag analysi comprehens evalu differ use case proceed patholog vision san diego ca usa novemb long shelham darrel fulli convolut network semant segment proceed ieee confer comput vision pattern recognit boston usa june pp alom yakopc taha asari nuclei segment recurr residu convolut neural network base proceed naecon nation aerospac electron confer dayton oh usa juli pp alom yakopc taha asari microscop blood cell classiÔ¨Åc use incept recurr residu convolut neural network proceed naecon nation aerospac electron confer dayton oh usa juli pp chen lin big data deep learn challeng perspect ieee access crossref zhou chawla jin william big data opportun challeng discuss data analyt perspect ieee comput intel mag crossref najafabadi villanustr khoshgoftaar seliya wald muharemag deep learn applic challeng big data analyt big data crossref electron goodfellow mirza xu ozair courvil bengio gener adversari net advanc neural inform process system mit press cambridg usa pp kaiser gomez shazeer vaswani parmar jone uszkoreit one model learn arxiv collobert weston uniÔ¨Å architectur natur languag process deep neural network multitask learn proceed intern confer machin learn helsinki finland juli pp johnson schuster le krikun wu chen thorat vi√©ga wattenberg corrado et al googl multilingu neural machin translat system enabl translat tran assoc comput linguist crossref argyri evgeni pontil featur learn advanc neural inform process system mit press cambridg usa pp singh gupta vig shroff agarw deep convolut neural network pairwis causal arxiv yu wang huang yang xu video paragraph caption use hierarch recurr neural network proceed ieee confer comput vision pattern recognit la vega nv usa june pp kim cha kim lee kim learn discov relat gener adversari network arxiv reed akata yan logeswaran schiel lee gener adversari text imag synthesi arxiv deng yu deep learn method applic found trend signal process crossref gu wang kuen shahroudi shuai liu wang wang cai et al recent advanc convolut neural network arxiv sze chen yang emer efÔ¨Åcient process deep neural network tutori survey proc ieee crossref kwon kim kim suh kim kim survey deep network anomali detect cluster comput crossref li deep reinforc learn overview arxiv kober bagnel peter reinforc learn robot survey int robot crossref pan yang survey transfer learn ieee tran knowl data eng crossref schuman potok patton birdwel dean rose plank survey neuromorph comput neural network hardwar arxiv mcculloch pitt logic calculu idea imman nervou activ bull math biophi crossref rosenblatt perceptron probabilist model inform storag organ brain psychol rev crossref pubm minski papert perceptron introduct comput geometri mit press cambridg usa ackley hinton sejnowski learn algorithm boltzmann machin cogn sci crossref fukushima neocognitron hierarch neural network capabl visual pattern recognit neural netw crossref lecun bottou bengio haffner learn appli document recognit proc ieee crossref hinton osindero teh fast learn algorithm deep belief net neural comput crossref pubm hinton salakhutdinov reduc dimension data neural network scienc crossref pubm electron bottou stochast gradient descent trick neural network trick trade springer germani pp rumelhart hinton william learn represent error cogn model crossref sutskev marten dahl hinton import initi momentum deep learn int conf mach learn yoshua lamblin popovici larochel greedi train deep network advanc neural inform process system nip mit press cambridg usa pp erhan manzagol bengio bengio vincent difÔ¨Åculti train deep architectur effect unsupervis artif intel stat moham dahl hinton acoust model use deep belief network ieee tran audio speech lang process crossref nair hinton rectiÔ¨Å linear unit improv restrict boltzmann machin proceed intern confer machin learn haifa israel june pp vincent larochel bengio manzagol extract compos robust featur denois autoencod proceed intern confer machin learn helsinki finland juli pp lin chen yan network network arxiv springenberg dosovitskiy brox riedmil strive simplic convolut net arxiv huang liu van der maaten weinberg dens connect convolut network proceed ieee confer comput vision pattern recognit honolulu hi usa juli pp larsson mair shakhnarovich fractalnet neural network without residu arxiv szegedi ioff vanhouck impact residu connect learn arxiv szegedi vanhouck ioff shlen wojna rethink incept architectur comput vision proceed ieee confer comput vision pattern recognit la vega nv usa june pp zagoruyko komodaki wide residu network arxiv xie girshick doll√°r tu aggreg residu transform deep neural network arxiv veit wilber belongi residu network behav like ensembl rel shallow network advanc neural inform process system mit press cambridg usa pp abdi nahavandi network improv speed accuraci residu network arxiv zhang li loy lin polynet pursuit structur divers veri deep network proceed ieee confer comput vision pattern recognit honolulu hi usa juli pp alom hasan yakopc taha asari improv convolut neural network object recognit arxiv ioff szegedi batch normal acceler deep network train reduc intern covari shift arxiv sabour frosst hinton dynam rout capsul advanc neural inform process system nip mit press cambridg usa pp ren girshick sun faster toward object detect region propos network advanc neural inform process system mit press cambridg usa pp chollet xception deep learn depthwis separ convolut arxiv liang hu recurr convolut neural network object recognit proceed ieee confer comput vision pattern recognit boston usa june electron alom hasan yakopc taha incept recurr convolut neural network object recognit arxiv li ouyang wang tang visual phrase guid convolut neural network proceed ieee confer comput vision pattern recognit cvpr honolulu hi usa juli pp bagherinezhad rastegari farhadi lcnn convolut neural network arxiv bansal chen russel gupta ramanan pixelnet represent pixel pixel pixel arxiv huang sun liu sedra weinberg deep network stochast depth european confer comput vision springer cham switzerland pp lee xie gallagh zhang tu net proceed artiÔ¨Åci intellig statist san diego ca usa may pp pezeshki fan brakel courvil bengio deconstruct ladder network architectur proceed intern confer machin learn new york ny usa june pp rawat wang deep convolut neural network imag classiÔ¨Åc comprehens review neural comput crossref pubm tzeng hoffman darrel saenko simultan deep transfer across domain task proceed ieee intern confer comput vision la cond chile decemb pp ba caruana deep net realli need deep advanc neural inform process system nip proceed mit press cambridg usa urban gera kahou aslan wang caruana moham philipos richardson deep convolut net realli need deep convolut arxiv romero balla kahou chassang gatta bengio fitnet hint thin deep net arxiv mishkin mata need good init arxiv pandey dukkipati go deep wide learn arxiv ratner de sa wu selsam r√© data program creat larg train set quickli advanc neural inform process system mit press cambridg usa pp aberg lamb tu n√∂tzli olukotun r√© emptyhead relat engin graph process acm tran databas syst crossref iandola han moskewicz ashraf dalli keutzer squeezenet accuraci fewer paramet mb model size arxiv han mao dalli deep compress compress deep neural network prune train quantiz huffman code arxiv niepert ahm kutzkov learn convolut neural network graph arxiv awesom deep vision avail onlin http access januari jia xu cai guo singl imag use convolut neural network paciÔ¨Åc rim confer multimedia springer cham switzerland pp ahn cho convolut neural network imag denois arxiv liu chen adapt deep convolut neural network photo aesthet assess arxiv cao zhou xu meng xu paisley hyperspectr imag classiÔ¨Åc markov random field convolut neural network ieee tran imag process crossref pubm electron de vo berendsen viergev stare i≈°gum unsupervis deform imag registr convolut neural network deep learn medic imag analysi multimod learn clinic decis support springer cham switzerland pp wang oxholm zhang wang multimod transfer hierarch deep convolut neural network fast artist style transfer proceed ieee confer comput vision pattern recognit honolulu hi usa juli volum babae dinh rigol deep convolut neural network background subtract arxiv alom sidik hasan taha asari handwritten bangla charact recognit use deep convolut neural network comput intel neurosci crossref pubm alom awwal taha optic beam classiÔ¨Åc use deep learn comparison classiÔ¨Åc proceed optic photon inform process xi san diego ca usa august volum sidik sagan maimaitijiang maimaitiyim shakoor burken mockler fritschi dpen deep progress expand network map heterogen agricultur landscap use satellit imageri remot sen environ crossref alom alam taha iftekharuddin object recognit use cellular simultan recurr network convolut neural network proceed intern joint confer neural network ijcnn anchorag ak usa may pp ronao cho human activ recognit smartphon sensor use deep learn neural network expert syst appl crossref yang nguyen san li krishnaswami deep convolut neural network multichannel time seri human activ recognit proceed intern joint confer artiÔ¨Åci intellig bueno air argentina juli hammerla halloran ploetz deep convolut recurr model human activ recognit use wearabl arxiv ord√≥√±ez roggen deep convolut lstm recurr neural network multimod wearabl activ recognit sensor crossref pubm rad kia zarbo van laarhoven jurman venuti marchiori furlanello deep learn automat stereotyp motor movement detect use wearabl sensor autism spectrum disord signal process ravi wong lo yang deep learn human activ recognit resourc efÔ¨Åcient implement devic proceed ieee intern confer wearabl implant bodi sensor network bsn san francisco ca usa june pp alom yakopc taha asari microscop nuclei classiÔ¨Åc segment detect improv deep convolut neural network dcnn approach arxiv chen papandr kokkino murphi yuill semant imag segment deep convolut net fulli connect crf arxiv badrinarayanan kendal cipolla segnet deep convolut architectur imag segment arxiv lin milan shen reid reÔ¨Ånenet reÔ¨Ånement network semant segment proceed ieee confer comput vision pattern recognit cvpr honolulu hi usa juli pp zhao shi qi wang jia pyramid scene pars network proceed ieee confer comput vision pattern recognit cvpr honolulu hi usa juli pp chen papandr kokkino murphi yuill deeplab semant imag segment deep convolut net atrou convolut fulli connect crf ieee tran pattern anal mach intel crossref pubm electron ronneberg fischer brox convolut network biomed imag segment intern confer medic imag comput intervent springer cham switzerland pp alom hasan yakopc taha asari recurr residu convolut neural network base medic imag segment arxiv girshick donahu darrel malik rich featur hierarchi accur object detect semant segment proceed ieee confer comput vision pattern recognit columbu oh usa june pp lin goyal girshick doll√°r focal loss dens object detect proceed ieee intern confer comput vision venic itali octob pp wang shrivastava gupta hard posit gener via adversari object detect proceed ieee confer comput vision pattern recognit honolulu hi usa juli gkioxari doll√°r girshick mask proceed ieee intern confer comput vision iccv venic itali octob pp redmon divvala girshick farhadi onli look onc uniÔ¨Å object detect proceed ieee confer comput vision pattern recognit la vega nv usa june pp liu anguelov erhan szegedi reed fu berg ssd singl shot multibox detector european confer comput vision springer cham switzerland pp hou wang lai tsao chang wang speech enhanc use multimod deep convolut neural network arxiv xu kong huang wang plumbley convolut gate recurr neural network incorpor spatial featur audio tag proceed intern joint confer neural network ijcnn anchorag ak usa may pp litjen kooi bejnordi setio ciompi ghafoorian van der laak van ginneken s√°nchez survey deep learn medic imag analysi med imag anal crossref pubm zhang xie xing mcgough yang mdnet semant visual interpret medic imag diagnosi network proceed ieee confer comput vision pattern recognit honolulu hi usa juli pp tran fulli convolut neural network cardiac segment mri arxiv tan acharya bhandari chua sivaprasad segment optic disc fovea retin vasculatur use singl convolut neural network comput sci crossref moeskop viergev mendrik de vri bender i≈°gum automat segment mr brain imag convolut neural network ieee tran med imag crossref pubm alom yakopc taha asari breast cancer classiÔ¨Åc histopatholog imag incept recurr residu convolut neural network arxiv lecun bottou orr efÔ¨Åcient backprop neural network trick trade orr m√ºller ed lectur note comput scienc springer berlin germani glorot bengio understand difÔ¨Åculti train deep feedforward neural network proceed thirteenth intern confer artiÔ¨Åci intellig statist sardinia itali may pp zhang ren sun delv deep rectiÔ¨Å surpass perform imagenet classiÔ¨Åc proceed ieee intern confer comput vision la cond chile decemb pp vedaldi lenc matconvnet convolut neural network matlab proceed acm intern confer multimedia brisban australia octob pp laurent pereyra brakel zhang bengio batch normal recurr neural network proceed ieee intern confer acoust speech signal process icassp shanghai china march pp electron lavin gray fast algorithm convolut neural network proceed ieee confer comput vision pattern recognit la vega nv usa june pp clevert unterthin hochreit fast accur deep network learn exponenti linear unit elu arxiv li fan li wu ming improv deep neural network multipl parametr exponenti linear unit neurocomput crossref jin xu feng wei xiong yan deep learn rectiÔ¨Å linear activ unit aaai xu wang chen li empir evalu rectiÔ¨Å activ convolut network arxiv zhang ren sun spatial pyramid pool deep convolut network visual recognit european confer comput vision springer cham switzerland pp yoo park lee kweon pyramid pool deep convolut represent proceed ieee confer comput vision pattern recognit workshop boston usa june pp graham fraction arxiv lee gallagh tu gener pool function convolut neural network mix gate tree proceed artiÔ¨Åci intellig statist cadiz spain may pp hinton srivastava krizhevski sutskev salakhutdinov improv neural network prevent featur detector arxiv srivastava hinton krizhevski sutskev salakhutdinov dropout simpl way prevent neural network overÔ¨Åt mach learn wan zeiler zhang le cun fergu regular neural network use dropconnect proceed intern confer machin learn atlanta ga usa june pp bul√≤ porzi kontschied dropout distil proceed intern confer machin learn new york ny usa june pp ruder overview gradient descent optim algorithm arxiv le ngiam coat lahiri prochnow ng optim method deep learn proceed intern confer intern confer machin learn bellevu wa usa june juli pp koushik hayashi improv stochast gradient descent feedback arxiv sathasivam abdullah logic learn hopÔ¨Åeld network arxiv elman find structur time cogn sci crossref jordan serial order parallel distribut process approach adv psychol hochreit bengio frasconi schmidhub gradient flow recurr net difÔ¨Åculti learn depend ieee press new york ny usa schmidhub habilit thesi netzwerkarchitekturen zielfunktionen und kettenregel network architectur object function chain rule thesi technisch universit√§t m√ºnchen m√ºnchen germani april ger schmidhub recurr net time count proceed intern joint confer neural network como itali juli volum ger schraudolph schmidhub learn precis time lstm recurr network mach learn socher lin man ng pars natur scene natur languag recurs neural network proceed intern confer machin learn bellevu wa usa juli pp mikolov karaÔ¨Å√°t burget Àá cernock√Ω khudanpur recurr neural network base languag model proceed eleventh annual confer intern speech commun associ makuhari chiba japan septemb volum electron xingjian chen wang yeung wong woo convolut lstm network machin learn approach precipit nowcast advanc neural inform process system nip nip proceed mit press cambridg usa pp chung gulcehr cho bengio empir evalu gate recurr neural network sequenc model arxiv jozefowicz zaremba sutskev empir explor recurr network architectur proceed intern confer machin learn lill franc juli yao cohn vylomova duh dyer recurr neural network arxiv koutnik greff gomez schmidhub clockwork rnn arxiv greff srivastava koutn√≠k steunebrink schmidhub lstm search space odyssey ieee tran neural netw learn syst crossref pubm karpathi li deep align gener imag descript proceed ieee confer comput vision pattern recognit boston usa june mikolov chen corrado dean efÔ¨Åcient estim word represent vector space arxiv goldberg levi explain deriv mikolov et method arxiv kunihiko neural network model select attent visual pattern recognit associ recal appl opt xu ba kiro cho courvil salakhudinov zemel bengio show attend tell neural imag caption gener visual attent proceed intern confer machin learn lill franc juli pp qin song chen cheng jiang cottrel recurr neural network time seri predict arxiv xiong meriti socher dynam memori network visual textual question answer proceed intern confer machin learn new york ny usa june oord kalchbrenn kavukcuoglu pixel recurr neural network arxiv xue nachum pandey warrington leung li direct estim region wall thick via residu recurr neural network intern confer inform process medic imag springer cham switzerland pp tjandra sakti manurung adriani nakamura gate recurr neural tensor network proceed intern joint confer neural network ijcnn vancouv bc canada juli pp wang jing learn natur languag infer lstm arxiv sutskev vinyal le sequenc sequenc learn neural network advanc neural inform process system nip mit press cambridg usa pp lakhani mahadev identiÔ¨Åc use convolut recurr neural network arxiv l√§ngkvist karlsson loutÔ¨Å review unsupervis featur learn deep learn model pattern recognit lett crossref malhotra vishnu vig agarw shroff timenet deep recurr neural network time seri classiÔ¨Åc arxiv soltau liao sak neural speech recogn lstm model larg vocabulari speech recognit arxiv sak senior beaufay long memori recurr neural network architectur larg scale acoust model proceed fifteenth annual confer intern speech commun associ singapor septemb adavann pertil√§ virtanen sound event detect use spatial featur convolut recurr neural network arxiv chien misbullah deep long memori network speech recognit proceed intern symposium chines spoken languag process iscslp tianjin china octob electron choi schuetz stewart sun use recurr neural network model earli detect heart failur onset med inform assoc crossref pubm azzouni pujol long memori recurr neural network framework network trafÔ¨Åc matrix predict arxiv olabiyi martinson chintalapudi guo driver action predict use deep bidirect recurr neural network arxiv kim kang lee chae kim chung choi probabilist vehicl trajectori predict occup grid map via recurr neural network arxiv richard gall equival recurr neural network action recognit comput vi imag underst crossref bontemp mcdermott collect anomali detect base long memori recurr neural network intern confer futur data secur engin springer intern publish cham switzerland kingma well stochast gradient vb variat proceed second intern confer learn represent iclr banff ab canada april ng spars autoencod lect note vincent larochel lajoi bengio manzagol stack denois autoencod learn use represent deep network local denois criterion mach learn zhang isola efro autoencod unsupervis learn predict arxiv lu deshpand forsyth cdvae deep variat auto encod condit variat gener arxiv chicco sadowski baldi deep autoencod neural network gene ontolog annot predict proceed acm confer bioinformat comput biolog health niagara fall ny usa august pp alom taha network intrus detect cyber secur use unsupervis deep learn approach proceed aerospac electron confer naecon dayton oh usa june song liu huang wang tan base data cluster iberoamerican congress pattern recognit springer germani pp ahmad protasov khan hyperspectr band select use unsupervis deep auto encod train extern classiÔ¨Å arxiv freund haussler unsupervis learn distribut binari vector use two layer network advanc neural inform process system mit press cambridg usa pp larochel bengio classiÔ¨Åc use discrimin restrict boltzmann machin proceed intern confer machin learn helsinki finland juli salakhutdinov hinton deep boltzmann machin aistat alom bontup taha intrus detect use deep belief network proceed aerospac electron confer naecon dayton oh usa june alom sidik taha asari handwritten bangla digit recognit use deep learn arxiv albalooshi sidik sagan albalooshi asari deep belief activ contour dbac applic oil spill segment remot sens aerial imageri photogramm eng remot sen crossref mao li xie lau wang smolley least squar gener adversari network proceed ieee intern confer comput vision venic itali octob pp saliman goodfellow zaremba cheung radford chen improv techniqu train gan arxiv vondrick pirsiavash torralba gener video scene dynam advanc neural inform process system mit press cambridg usa pp electron radford metz chintala unsupervis represent learn deep convolut gener adversari network arxiv wang gupta gener imag model use style structur adversari network european confer comput vision springer cham switzerland chen duan houthooft schulman sutskev abbeel infogan interpret represent learn inform maxim gener adversari net advanc neural inform process system mit press cambridg usa im kim jiang memisev gener imag recurr adversari work arxiv isola zhu zhou efro translat condit adversari network arxiv liu tuzel coupl gener adversari network advanc neural inform process system mit press cambridg usa donahu kr√§henb√ºhl darrel adversari featur learn arxiv berthelot schumm metz began boundari equilibrium gener adversari network arxiv martin chintala bottou wasserstein gan arxiv gulrajani ahm arjovski dumoulin courvil improv train wasserstein gan advanc neural inform process system mit press cambridg usa pp wang hopcroft power gener model use random weight deep imag represent advanc neural inform process system mit press cambridg usa ko fischer song adversari exampl gener model arxiv zhao mathieu lecun gener adversari network arxiv park anand moniz lee chakraborti choo park kim mmgan manifold match gener adversari network gener imag arxiv laloy h√©rault jacqu lind efÔ¨Åcient base geostatist simul invers use spatial gener adversari neural network arxiv widmer probabilist gener adversari network arxiv fowk sutton bayesian network model interest itemset joint european confer machin learn knowledg disco databas springer intern publish cham switzerland mesched nowozin geiger adversari variat bay unifi variat autoencod gener adversari network arxiv nowozin cseke tomioka train gener neural sampler use variat diverg minim advanc neural inform process system mit press cambridg usa li wand precomput textur synthesi markovian gener adversari network european confer comput vision springer intern publish cham switzerland du zhu zhang learn deep gener model doubli stochast gradient mcmc ieee tran neural network learn syst crossref pubm hoang quan tu dinh nguyen trung le dinh phung gerner adversari net arxiv bousmali silberman dohan erhan krishnan unsupervis domain adapt gener adversari network proceed ieee confer comput vision pattern recognit cvpr honolulu hi usa juli volum kanski silver m√©li eldawi lou dorfman sidor phoenix georg schema network transfer gener causal model intuit physic arxiv ledig thei husz√°r caballero cunningham acosta aitken tejani totz wang et al singl imag use gener adversari network arxiv souli spampinato shah semi weakli supervis semant segment use gener adversari network arxiv dash gamboa ahm liwicki afzal condit auxiliari classiÔ¨Å gener adversari network arxiv electron zhang dana gener network transfer arxiv zhang sindagi patel imag use condit gener adversari network arxiv serban sordoni bengio courvil pineau build dialogu system use gener hierarch neural network model aaai pascual bonafont serr√† segan speech enhanc gener adversari network arxiv yang chou yang midinet convolut gener adversari network music gener proceed intern societi music inform retriev confer ismir suzhou china octob yang yan zhang yu shi mou kalra zhang sun wang ct imag denois use gener adversari network wasserstein distanc perceptu loss ieee tran med imag crossref pubm rezaei harmuth gierk kellermei fischer yang meinel condit adversari network semant segment brain tumor intern miccai brainles workshop springer cham switzerland pp xue xu zhang long huang segan adversari network l loss medic imag segment neuroinformat crossref pubm mardani gong cheng vasanawala zaharchuk alley thakur han dalli pauli et al deep gener adversari network compress sens autom mri arxiv choi biswal malin duke stewart sun gener multilabel discret electron health record use gener adversari network arxiv esteban hyland r√§tsch medic time seri gener recurr condit gan arxiv hay meli danezi de cristofaro logan evalu privaci leakag gener model use gener adversari network arxiv gordon bayesian semisupervis learn deep gener model arxiv abbasnejad shi abbasnejad van den hengel dick bayesian condit gener adverseri network arxiv grnarova levi lucchi hofmann kraus onlin learn approach gener adversari network arxiv li swerski zemel gener moment match network proceed intern confer machin learn lill franc juli pp li chang cheng yang p√≥czo mmd gan toward deeper understand moment match network advanc neural inform process system mit press cambridg usa pp nie feng xing yan gener partit network pose estim arxiv saeedi hoffman diverdi ghandeharioun johnson adam multimod predict person photo edit deep gener model arxiv schlegl seeb√∂ck waldstein lang unsupervis anomali detect gener adversari network guid marker discoveri intern confer inform process medic imag springer cham switzerland pp liu breuel kautz unsupervis translat network advanc neural inform process system mit press cambridg usa pp mehrotra dukkipati gener adversari residu pairwis network one shot learn arxiv sordoni galley auli brockett ji mitchel nie gao dolan neural network approach gener convers respons arxiv yin jiang lu shang li li neural gener question answer arxiv electron oord dieleman zen simonyan vinyal grave kalchbrenn senior kavukcuoglu wavenet gener model raw audio arxiv chen li xiao jin yan feng dual path network advanc neural inform process system mit press cambridg usa pp mahmud kaiser hussain vassanelli applic deep learn reinforc learn biolog data ieee tran neural netw learn syst crossref pubm goodfellow bengio courvil deep learn mit press cambridg usa silver huang maddison guez sifr van den driessch schrittwies master game go deep neural network tree search natur crossref pubm vinyal ewald bartunov georgiev vezhnevet yeo makhzani k√ºttler agapi schrittwies et al starcraft ii new challeng reinforc learn arxiv koenig simmon complex analysi reinforc learn appli find shortest path determinist domain tech report comput scienc depart univers pittsburgh pa decemv silver schrittwies simonyan antonogl huang guez hubert baker lai bolton et al master game go without human knowledg natur crossref pubm schulman levin abbeel jordan moritz trust region polici optim proceed intern confer machin learn lill franc juli volum pp levin finn darrel abbeel train deep visuomotor polici mach learn mnih badia mirza grave lillicrap harley silver kavukcuoglu asynchron method deep reinforc learn proceed intern confer machin learn new york ny usa june pp arulkumaran deisenroth brundag bharath brief survey deep reinforc learn arxiv zhu liao zhu yao huang onlin reinforc learn mhealth intervent arxiv zhu guo xu liao yang huang reinforc learn person mhealth intervent intern confer medic imag comput intervent springer cham switzerland pp steckelmach roijer harutyunyan vrancx plisnier now√© reinforc learn pomdp memoryless option initi set proceed aaai confer artiÔ¨Åci intellig new orlean la usa februari hu zhang yan wang xu solv new bin pack problem deep reinforc learn method arxiv everitt krakovna orseau hutter legg reinforc learn corrupt reward channel arxiv wu mansimov gross liao ba scalabl method deep reinforc learn use approxim advanc neural inform process system mit press cambridg usa pp denil agraw kulkarni erez battaglia de freita learn perform physic experi via deep reinforc learn arxiv hein hentschel runkler udluft particl swarm optim gener interpret fuzzi reinforc learn polici eng appl artif intel crossref islam henderson gomrokchi precup reproduc benchmark deep reinforc learn task continu control arxiv inou de magistri munawar yokoya tachibana deep reinforc learn high precis assembl task proceed intern confer intellig robot system iro vancouv bc canada septemb pp electron li burdick invers reinforc learn larg state space via function approxim arxiv liu li xu xu lin qiu tang wang hierarch framework cloud resourc alloc power manag use deep reinforc learn proceed ieee intern confer distribut comput system icdc atlanta ga usa june pp cao lin shi liang li face hallucin via deep reinforc learn proceed ieee confer comput vision pattern recognit honolulu hi usa juli pp kendal gal uncertainti need bayesian deep learn comput vision advanc neural inform process system nip mit press cambridg usa kendal gal cipolla learn use uncertainti weigh loss scene geometri semant arxiv googl photo label black peopl gorilla avail onlin http access march gal ghahramani bayesian convolut neural network bernoulli approxim variat infer arxiv kumar laumann maurin olsen bayesian convolut neural network variat infer arxiv vladimirova arbel mesejo bayesian neural network becom depth proceed bayesian deep learn workshop dure confer neural inform process system nip montr√©al qc canada decemb hu moreno lawrenc damian perspect bayesian neural network proceed bayesian deep learn workshop dure confer neural inform process system nip montr√©al qc canada decemb salvat han schroer mandt video compress deep bayesian learn bayesian proceed deep learn workshop dure confer neural inform process system nip montr√©al qc canada decemb krishnan subedar tickoo bar bayesian activ recognit use variat infer arxiv chen goodfellow shlen acceler learn via knowledg transfer arxiv ganin lempitski unsupervis domain adapt backpropag arxiv ganin ustinova ajakan germain larochel laviolett marchand lempitski train neural network mach learn taylor stone transfer learn reinforc learn domain survey mach learn mckeough teach transfer foster gener learn routledg london uk raina battl lee packer ng learn transfer learn unlabel data proceed intern confer machin learn corval usa june pp wenyuan yang xue yu boost transfer learn proceed intern confer machin learn corval usa june pp wu schuster chen le norouzi macherey krikun cao gao macherey et al googl neural machin translat system bridg gap human machin translat arxiv qiu wang yao guo li zhou yu tang xu song et al go deeper embed fpga platform convolut neural network proceed intern symposium gate array monterey ca usa februari pp electron sun convolut neural network constrain time cost proceed ieee confer comput vision pattern recognit boston usa june pp lin courbariaux memisev bengio neural network multipl arxiv courbariaux david bengio train deep neural network low precis multipl arxiv courbariaux bengio david binaryconnect train deep neural network binari weight dure propag advanc neural inform process system mit press cambridg usa hubara soudri el yaniv binar neural network arxiv kim smaragdi bitwis neural network arxiv dettmer approxim parallel deep learn arxiv gupta agraw gopalakrishnan narayanan deep learn limit numer precis proceed intern confer machin learn lill franc juli pp zhou wu ni zhou wen zou train low bitwidth convolut neural network low bitwidth gradient arxiv merolla arthur cassidi sawada akopyan jackson imam guo nakamura et al million integr circuit scalabl commun network interfac scienc crossref pubm steven merolla arthur cassidi convolut network fast neuromorph comput proc natl acad sci usa zidan strachan lu futur electron base memrist system nat electron crossref chen krishna emer sze eyeriss reconÔ¨Ågur acceler deep convolut neural network ieee circuit crossref chen luo liu zhang wang li chen xu sun et al dadiannao supercomput proceed annual intern symposium microarchitectur cambridg uk decemb pp jouppi young patil patterson agraw bajwa bate bhatia boden borcher et al perform analysi tensor process unit proceed annual intern symposium comput architectur isca toronto canada june pp han liu mao pu pedram horowitz dalli eie efÔ¨Åcient infer engin compress deep neural network proceed annual intern symposium comput architectur isca seoul korea june pp zhang zou ming sun efÔ¨Åcient accur approxim nonlinear convolut network proceed ieee confer comput vision pattern recognit boston usa june pp novikov podoprikhin osokin vetrov tensor neural network advanc neural inform process system mit press cambridg usa pp zhu han mao dalli train ternari quantiz arxiv author license mdpi basel switzerland thi articl open access articl distribut term condit creativ common attribut cc licens http