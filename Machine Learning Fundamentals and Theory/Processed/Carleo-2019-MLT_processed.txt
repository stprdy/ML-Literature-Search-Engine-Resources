machin learn physic scienc giusepp carleo center comput quantum physic flatiron institut avenu new york ny ignacio cirac fur quantenoptik garch germani kyle cranmer center cosmolog particl physic center data scienc new york univers broadway new york ny usa laurent daudet lighton rue de la bours pari franc maria schuld univers durban south africa nation institut theoret physic durban south africa xanadu quantum comput bay street toronto canada naftali tishbi hebrew univers jerusalem edmond safra campu jerusalem israel lesli depart chemistri new york univers new york ny usa lenka zdeborová institut de physiqu théoriqu université pari saclay cnr cea machin learn encompass broad rang algorithm model tool use vast array data process task ha enter scientiﬁc disciplin recent year review select way recent research interfac machin learn physic scienc thi includ conceptu develop chine learn ml motiv physic insight applic machin learn techniqu sever domain physic two ﬁeld give basic notion machin learn method principl describ ampl statist physic use understand method ml move dec describ applic ml method particl physic cosmolog quantum mani bodi physic quantum comput chemic materi physic also highlight research develop novel comput architectur aim ate ml section describ recent success well methodolog challeng content introduct concept machin learn supervis learn neural network unsupervis learn gener model reinforc learn ii statist physic histor note theoret puzzl deep learn statist physic unsupervis learn contribut understand basic unsupervis method restrict boltzmann machin modern unsupervis gener model statist physic supervis learn perceptron glm physic result neural network inform bottleneck landscap glassi deep learn applic ml statist physic outlook challeng iii particl physic cosmolog role simul classiﬁc regress particl physic jet physic neutrino physic robust systemat uncertainti trigger theoret particl physic classiﬁc regress cosmolog photometr redshift gravit len ﬁnding paramet estim exampl invers problem infer infer exampl particl physic exampl cosmolog gener model outlook challeng iv quantum matter quantum state represent theori learn data variat learn speed simul classifi quantum phase synthet data experiment data tensor network machin learn outlook challeng quantum comput quantum state tomographi control prepar qubit error correct vi chemistri materi energi forc base atom environ potenti free energi surfac materi properti electron densiti densiti function theori data set gener outlook challeng vii ai acceler classic quantum hardwar beyond von neumann architectur neural network run light reveal featur data machin learn outlook challeng viii conclus outlook acknowledg refer introduct past decad ha seen prodigi rise learn ml base techniqu impact mani area industri includ autonom drive nanc manufactur energi harvest ml larg perceiv one main disrupt nolog age much comput gener goal ml recogn pattern data inform way unseen problem treat exampl highli complex system car vast amount data come sensor turn decis control car comput ha learn recogn pattern danger success ml recent time ha mark ﬁrst signiﬁc improv exist technolog exampl ﬁeld imag tion larg extent advanc constitut ﬁrst demonstr impact ml method special task recent applic dition inaccess autom softwar success enabl particular deep learn nolog demonstr reinforc learn techniqu game play exampl ha deep impact percept whole ﬁeld wa move step closer expect gener artiﬁci intellig parallel rise ml techniqu industri plicat scientist increasingli becom interest potenti ml fundament research physic except extent thi surpris sinc ml physic share method well goal two disciplin concern process gather analyz data design model predict behaviour complex system howev ﬁeld promin diﬀer author gcarleo correspond author way fundament goal realiz one hand physicist want understand mechan natur proud use knowledg intellig intuit inform model hand machin learn mostli doe site model agnost machin provid intellig extract data although ten power result model notori known opaqu understand data tern themselv machin learn tool physic therefor welcom enthusiast eye suspicion diﬃcult deni produc surprisingli good result case thi review attempt provid coher lect account divers intersect ml physic speciﬁc look ampl spectrum ﬁeld rang statist quantum physic high energi cosmolog ml recent made promin appear discuss potenti applic challeng intellig data mine techniqu diﬀer context start thi review ﬁeld statist physic section ii interact machin learn ha long histori draw method physic provid better understand problem machin learn turn wheel direct use machin learn physic section iii treat progress ﬁeld physic cosmolog section iv review ml idea help understand mysteri quantum system section v brieﬂi explor promis machin learn within quantum comput section vi highlight amaz advanc comput chemistri materi design due ml applic section vii discuss vanc instrument lead potenti ware adapt perform machin learn task conclud outlook section viii concept machin learn purpos thi review brieﬂi explain fundament term concept use machin learn read recommend sourc target especi physic audienc histor overview develop ﬁeld recommend ref lecun et schmidhub excel recent introduct machin learn physicist ref mehta et includ notebook tical demonstr veri use onlin resourc florian marquardt cours machin learn cist use textbook written machin learn research christoph bishop standard textbook bishop well goodfellow et focus theori foundat deep learn cover mani aspect research eti onlin tutori lectur use get basic overview get start topic learn theoret progress made tical physic neural network ommend rather access book statist ic learn engel van den broeck learn detail replica method use puter scienc inform theori machin learn would recommend book nishimori nishimori recent statist physic ogi textbook mézard montanari excel refer mézard montanari get basic idea type problem chine learn abl tackl use deﬁn three larg class learn problem supervis learn unsupervis learn reinforc learn thi also allow us state basic terminolog ing basic equip expos basic tool machin learn supervis learn neural network supervis learn given set n sampl data let us denot one sampl xµ µ someth concret mind xµ could instanc photograph anim p number pixel sampl xµ given label yµ commonli label could encod instanc speci anim photograph goal supervis learn ﬁnd function f new sampl xnew present without label output function f xnew approxim well label data see http set xµ yµ n call train set order test result function f one usual split avail data sampl train set use learn function test set evalu perform let us describ train procedur monli use ﬁnd suitabl function commonli function express term set paramet call weight w lead fw one struct loss function l fw xµ yµ sampl µ idea thi loss small fw xµ yµ close vice versa averag loss train set call empir risk r fw pn l fw xµ yµ dure train procedur weight w adjust order minim empir risk train error measur well minim achiev notion error import gener error relat perform predict label ynew data sampl xnew seen train set applic mon practic build test set randomli pick fraction avail data perform train use remain fraction train set note part literatur gener error diﬀer perform test set one train set algorithm commonli use minim empir risk function weight base dient descent respect weight thi mean weight iter adjust direct gradient empir risk wt fw rate γ thi perform call ing rate veri commonli use success variant gradient descent stochast gradient descent sgd full empir risk function r place contribut sampl thi subset sampl call small singl sampl physic term sgd algorithm often compar langevin dynam ﬁnite temperatur langevin dynam zero peratur gradient descent posit temperatur introduc thermal nois certain way similar nois aris sgd diﬀer mani variant sgd algorithm use practic initi weight chang perform practic choic learn rate varieti regular term weight decay penal weight tend converg larg absolut valu choic right version algorithm import mani heurist rule thumb certainli theoret insight question would desir one typic exampl task supervis learn classiﬁc label yµ take valu discret set accuraci sure fraction time learn function siﬁe data point correctli anoth exampl gression goal learn tion accuraci typic measur term error true label learn estim exampl would learn input label vector dimens larger one mani method supervis learn mani variant one basic supervis learn method wide known use linear gression function fw x parameter form fw xµ xµw w data live high dimension space number sampl much larger dimens indispens use regular form linear regress call ridg regress tikhonov regular ridg sion formal equival assum weight w gaussian prior gener form linear regress parameter fw xµ g xµw g output channel function also often use properti describ section anoth popular way regular base rate exampl n classiﬁc task separ categori divid clear gap wide possibl thi idea stand behind deﬁnit support vector machin method rather power gener ridg regress kernel ridg regress kernel ridg regress close relat gaussian process sion support vector machin method often bine kernel method still method mani applic especi number avail sampl veri larg anoth classic supervis learn method base decis tree decis tree use go observ data sampl repres branch conclus item target valu repres leav best known applic decis tree physic scienc data analysi particl acceler discuss sec supervis learn method stand behind machin learn revolut past decad neural network ffnn also sometim call perceptron thi also veri relev method purpos thi review shall describ brieﬂi fulli nect neural network function fw xµ teriz follow fw xµ g l w l g w g w xµ w w w l l w p rl matric weight ri call width hidden layer function g activ function act wise vector note input activ function often slightli gener aﬃn transform output previou layer simpli matrix multipl includ bias number layer l call network depth neural network depth larger small integ call deep neural network subsequ machin learn base deep neural network call deep learn theori neural network tell us without hidden layer l correspond gener linear regress set function proxim thi way veri limit minski papert hand alreadi one hidden layer l wide enough larg enough function g veri gener class function well approxim principl benko theori howev tell us optim set paramet activ function width layer depth der learn w w l tractabl eﬃcient know empir success past decad mani task interest tractabl deep neural network use gradient descent sgd algorithm deep neural network deriv respect weight comput use chain rule lead celebr algorithm take care eﬃcient schedul oper requir comput gradient goodfellow et veri import power variant deep forward neural network convolut neural network goodfellow et input hidden unit obtain via ﬁlter pli small part input space ﬁlter shift diﬀer posit correspond diﬀer hidden unit convolut neural network implement invari translat particular suitabl analysi imag compar fulli connect ral network layer convolut neural network ha much smaller number paramet tice advantag learn algorithm mani type varianc convolut neural work among mention residu neural network resnet use shortcut jump er next neural network call recurr neural network rnn put unit feed back input next time step rnn result thu given set weight also whole tempor sequenc state due intrins dynam natur rnn larli suitabl learn tempor data set speech languag time seri mani type variant rnn one caus excit past decad arguabl long memori lstm network hochreit schmidhub lstm deep variant task speech process music composit natur languag process unsupervis learn gener model unsupervis learn class learn problem input data obtain supervis learn label avail goal learn recov underli possibl structur dataset typic exampl vise learn data cluster data point assign group way everi group ha common properti unsupervis learn one often seek probabl distribut gener sampl statist similar observ data sampl thi often refer gener model case thi iti distribut written explicit form plicitli implicitli parameter gener model intern contain latent variabl sourc dom number latent variabl much smaller dimension data speak dimension reduct one path toward vise learn search valu latent variabl maxim likelihood observ data rang applic likelihood associ observ data known comput self intract case gener model discuss oﬀer altern free path section also discuss call abc method type ferenc turn veri use mani context aris physic scienc basic method unsupervis learn includ cipal compon analysi variant cover theoret insight method tain use physic section physic veri appeal method unsupervis learn call boltzmann machin bm bm basic invers ise model data sampl seen sampl boltzmann distribut teract ise model goal learn valu interact magnet ﬁeld likelihood probabl boltzmann measur observ data larg restrict boltzmann machin rbm particular case bm two kind variabl visibl unit see input data hidden unit interact eﬀect coupl interact thi case onli visibl hidden unit adjust order likelihood observ data larg given appeal interpret term physic model applic bm rbm widespread sever physic domain discuss section veri neat idea perform unsupervis learn yet abl use method algorithm ope supervis learn toencod neural network ha input data input also output aim reproduc data typic go trough tleneck sens intermedi layer veri small width compar dimension data idea autoencod ing ﬁnd succinct represent data still keep salient featur sampl tional autoencod vae kingma well rezend et combin variat infer autoencod provid deep gener model data train unsupervis fashion approach unsupervis learn worth mention adversari gener network gan goodfellow et gan attract substanti attent past year constitut anoth fruit way take advantag progress made supervis learn unsupervis one gan typic use two neural network one call gener anoth call tor gener network use gener output random input design put look like observ sampl discrimin network use discrimin true data ple sampl gener gener network discrimin aim best possibl accuraci thi classiﬁc task wherea gener network adjust make accuraci discrimin smallest possibl gan current art system mani applic imag process interest method model distribut clude normal ﬂow autoregress model advantag tractabl likelihood train via maximum likelihood larochel murray papamakario et uria et hybrid supervis learn unsupervis learn import applic includ supervis learn onli label avail activ learn label acquir lect set data point certain cost reinforc learn reinforc learn sutton barto area machin learn artiﬁci agent take action environ goal maxim reward action chang state ment way agent typic observ inform state environ correspond reward base observ agent decid next action reﬁn strategi action choos order maxim ing reward thi type learn design case onli way learn properti environ interact key concept forcement learn exploit good strategi found far explor order ﬁnd yet better strategi also note reinforc learn intim relat ﬁeld theori control especi optim control theori one main type reinforc learn pli mani work learn base valu matrix q assign qualiti given action environ given state thi valu function q iter reﬁn cent advanc applic set state action larg imposs even store whole matrix case deep neural network use repres function succinct manner thi give rise deep recent exampl success inforc learn comput program alphago alphago zero ﬁrst time histori reach perform tradit board game go anoth well known use reinforc learn locomot robot ii statist physic histor note machin learn tool physic research rel new phenomenon fertil two disciplin date back much especi statist physicist made import contribut theoret understand ing term statist unmistak suggest connect statist mechan learn theori start statist learn ampl took logic rule base ai mid two semin paper mark thi transform valiant theori learnabl valiant open way rigor statist learn ai hopﬁeld neural network model associ ori hopﬁeld spark rich applic concept spin glass theori neural network model thi wa mark memori capac culat hopﬁeld model amit gutfreund sompolinski amit et follow work much tighter applic learn model wa made semin work elizabeth gardner appli replica trick gardner calcul ume weight space simpl neural network supervis unsupervis learn model gardner method enabl explicitli calcul ing curv typic train gener error function number train exampl veri speciﬁc one neural network gyi tishbi seung et sompolinski et analyt statist physic tion demonstr learn dynam hibit much richer behavior predict case distribut free pac bound pac stand abli approxim correct valiant ular learn exhibit phase transit poor good gener györgyi thi rich ing dynam curv appear mani machin learn problem wa shown variou model see recent review zdeborová krzakala statist physic learn reach peak earli rather minor inﬂuenc learn practition theorist focus gener gener bound character dimens rademach complex hypothesi class theoret puzzl deep learn machin learn new millennium wa mark much larger scale learn problem size move hundr million mension train data size number adjust paramet thi wa dramat strate return larg scale neural network model mani hidden layer known deep neural network deep neural network essenti convolut ral network propos alreadi much larger scale input big clean train data trick hack network start beat mani diﬀer pattern recognit machin learn competit roughli amaz perform deep learn train old stochast gradient descent sgd propag algorithm took everyon surpris one puzzl exist learn theori base gener bound unabl explain thi phenomen success ing theori doe predict whi deep network adjust way higher number train sampl good gener properti thi lack theori wa coin classic articl zhang et author show numer neural network use classiﬁc abl classifi perfectli randomli gener label case ing learn theori doe provid ani use bound gener error yet practic observ good gener deep neural network train true label continu open question good understand learn problem putat tractabl thi particularli import sinc point view comput complex theori learn problem encount worst case anoth open question central current deep learn concern choic architectur far guid lot combin impress rienc research time tion ml spread mani domain ﬁeld call systemat approach current basic question minim number sampl need order abl learn given task good precis entir open time current literatur deep ing ﬂourish interest numer observ experi call explan physic audienc situat could perhap compar fundament physic befor quantum mechan wa develop ﬁeld wa full unexplain experi evad ing theoret understand thi clearli perfect time physic idea studi neural network resurrect revisit current question direct machin learn given long histori work done neural work statist physic aim complet review thi direct research focu lectiv way recent contribut origin physic opinion import impact rent theori learn machin learn purpos thi review also put asid larg volum work done statist physic recurr neural network biolog applic mind statist physic unsupervis learn contribut understand basic unsupervis method one basic tool unsupervis learn across scienc method base composit observ data matrix data ing princip compon analysi pca independ compon analysi ica matrix complet method exampl thi class mathemat languag matrix posit problem state follow observ n ple data xi note x n p matrix data idea ing decomposit method assum x function x written noisi version rank r matrix r r rank much lower dimension number sampl therefor name ticularli challeng yet relev interest regim dimension p compar number sampl n level nois larg way perfect estim signal possibl turn matrix estim noisi regim model tistic physic model spin glass vector variabl special plant conﬁgur found concret thi model deﬁn student scenario teacher gener dimension latent variabl n taken given probabl distribut pu latent variabl j j p taken given probabl distribut pv teacher gener compon data matrix x given condit probabl distribut pout j goal student cover latent variabl precis ble knowledg x distribut pout pu pv spin glass theori use obtain rather plete understand thi model matrix estim limit p n α ω r ω one comput replica method best error estim student possibl achiev done decad ago special choic r pout pu pv barkai sompolinski biehl mietzner watkin nadal portanc earli work physic acknowledg landmark paper subject tic see johnston lu howev lack mathemat rigor limit understand algorithm tractabl caus impact work machin learn statist remain limit resurrect interest statist physic proach matrix decomposit came studi stochast block model detect spars network problem commun detect wa studi heurist gorithm extens statist physic view see fortunato howev exact tion understand algorithm limit stochast block model came spin glass theori decel et b work comput rigor asymptot optim perform delimit sharpli region paramet thi formanc reach belief propag bp gorithm yedidia et second order phase sition appear model separ phase cluster perform better random guess region done eﬃcient bp first order phase transit one spinod line separ region cluster imposs possibl doabl bp rithm easi bp algorithm ref decel et b also conjectur bp rithm abl reach optim perform larg instanc model polynomi algorithm work attract larg amount work mathemat statist machin ing comput scienc commun statist physic understand stochast block model conjectur belief propag algorithm optim among polynomi one spire discoveri new class spectral algorithm spars data matrix x spars kala et spectral algorithm basic tool data analysi ng et von luxburg base singular valu decomposit matrix x function yet spars matric x spectrum known lead singular valu local singular vector unrel latent ing structur robust spectral method obtain linear belief propag thu obtain call matrix krzakala et variant thi spectral method base algorithm interpret hessian beth free energi also origin physic saad et thi line inspir research merg mainstream statist machin learn thi larg thank recent progress understand algorithm limit due analysi approxim messag pass amp algorithm bolthausen deshpand nari javanmard montanari matsushita tanaka rangan fletcher rank matrix estim gener equat thouless et well known physic literatur spin glass b progress prove mani correspond result mathemat rigor way inﬂuenti paper thi direct relat matrix estim barbier et et deshpand montanari lelarg miolan proof replica formula optim perform restrict boltzmann machin boltzmann machin particular restrict mann machin anoth method unsupervis learn often use machin learn appar veri name method strong relat statist physic inde boltzmann machin often call invers ise model physic eratur use extens rang area cent review physic boltzmann machin see nguyen et concern restrict boltzmann machin number studi physic clarifi chine work structur learn model random restrict boltzmann machin weight impos random spars learn studi cocco et tubiana monasson rather remark rang tential hidden unit thi work unveil even singl layer rbm abl repres composit structur insight thi work recent use model protein famili sequenc mation tubiana et analyt studi learn process rbm commonli done use contrast diverg algorithm base gibb sampl hinton veri challeng first step studi decel et begin learn process dynam linear anoth ing direct come statist physic replac gibb sampl contrast diverg train algorithm equat thouless et thi ha done gabrié et tramel et train wa shown competit applic approach discuss rbm random weight lation hopﬁeld model wa clariﬁ barra et mézard modern unsupervis gener model dawn deep learn brought excit tion unsupervis learn physic friendli overview classic recent concept wang linear activ function close relat pca variat autoencod vae kingma well rezend et variant much closer physicist mind set autoencod repres via graphic model train use prior latent variabl tional infer vae singl hidden layer close relat wide use techniqu signal ing dictionari learn spars code nari learn problem ha studi statist physic techniqu kabashima et krzakala et sakata kabashima gener adversari network gan power set idea emerg work goodfellow et aim gener sampl imag hotel bedroom type ing set studi gan start appear work solvabl model gan wang et intrigu gener earlier statist physic work onlin learn ceptron also want point reader attent toregress gener model larochel murray papamakario et uria et main interest autoregress model stem fact famili explicit probabilist el direct unbias sampl possibl applic model realiz statist wu et quantum physic lem sharir et statist physic supervis learn perceptron glm arguabl basic method supervis ing linear regress one aim ﬁnd vector coeﬃcient w scalar product data point xiw correspond observ predic thi often solv least squar method minim bayesian guag least squar method correspond ing gaussian addit nois ξ yi xiw ξi high dimension set almost alway indispens use regular weight common ridg regular correspond bayesian pretat gaussian prior weight thi abilist think gener assum eral prior pw gener nois repres condit probabl distribut pout result model call gener linear regress gener linear model glm mani problem interest data analysi learn sent glm instanc spars regress simpli requir pw ha larg weight zero ceptron threshold κ output ha special form pout z κ δ z δ languag neural network glm repres singl layer hidden variabl fulli connect network gener channel pout tradit theori statist readili applic regim veri limit data dimens p number sampl n grow larg tio α remain ﬁxed basic question doe best achiev gener error depend number sampl remain open yet thi regim relat question great interest understand well set glm seem uisit understand involv deep learn method statist physic approach use obtain ciﬁc result glm consid data random independ ident distribut iid matrix model label creat set teacher gener vector weight w wj j teacher use thi vector data matrix x produc label taken pout student know x pw pout pose learn rule teacher use ideal learn alreadi thi set random put data provid interest insight mic tractabl problem number sampl chang thi line work wa pioneer elisabeth ner gardner derrida activ studi physic past special case pout pw see györgyi tishbi seung et sompolinski et replica method use comput mutual inform x thi model relat free energi physic one deduc optim estim error vector well mal gener error remark recent progress wa made barbier et ha proven replica method yield correct result glm random input gener pout pw combin result analysi proxim messag pass algorithm javanmard montanari one deduc case amp algorithm abl reach optim perform region amp algorithm ture best polynomi algorithm thi case model could thu use practition understand far optim gener purpos algorithm case onli veri ite number sampl avail physic result neural network statist physic analysi learn tion properti deep neural network challeng task progress made sever complementari direct one inﬂuenti direct involv studi ear deep neural network linear neural network express power repres gener function learn dynam gradient descent algorithm bear strong resembl learn namic network time namic learn deep linear neural network describ via close form solut sax et learn dynam linear neural network also abl reproduc rang fact gener observ numer network see advani sax anoth special case ha analyz great detail call committe machin review see engel van den broeck committe machin neural network learn random input data onli ﬁrst layer weight learn subsequ one ﬁxed theori restrict limit number hidden neuron k mension input p number sampl n diverg α stochast gradient descent aka onlin learn saad solla b optim aliz error analyz close form thi case schwarz recent replica analysi timal gener properti ha establish orous aubin et key featur tee machin display special phase transit number sampl small optim error achiev everi hidden unit eﬀect plement simpl regress onli number hidden unit exce special threshold diﬀer hidden unit learn diﬀer weight result improv gener error anoth est observ committe machin hard phase good gener achiev tractabl get larger number hidden unit grow committe chine wa also use analyz consequ neural network goldt et b anoth remark limit neural network wa analys recent seri work mei et rotskoﬀand work network analys limit number hidden unit larg dimension put kept ﬁxed thi limit weight interact onli weakli lead term mean ﬁeld lution track via ordinari diﬀerenti tion analog studi glassi system dean relat diﬀer treatment limit hidden layer larg base tion dynam around initi condit lead relat gaussian process kernel method see jacot et lee et inform bottleneck inform bottleneck tishbi et anoth concept stem statist physic ha ﬂuential quest understand theori hind success deep learn theori format bottleneck deep learn tishbi tishbi zaslavski aim tifi notion layer neural network ing oﬀbetween keep enough inform input output label predict forget much unnecessari inform sibl order keep learn represent concis one interest consequ thi tion theoret analysi tradit capac express dimens network vc dimens replac expon mutual format input compress den layer represent thi impli everi bit represent compress equival doubl train data impact gener error analysi tishbi also suggest represent compress achiev stochast gradient descent sgd diﬀus irrelev dimens problem accord thi compress achiev ani unit nonlinear reduc snr irrelev dimens layer layer diﬀus weight ing predict thi insight time converg good gener scale like neg number layer theori also predict nection hidden layer bifurc phase transit inform bottleneck resent mutual inform intern tation intrins hard comput directli larg neural network none abov predict depend explicit estim mutual inform valu relat line work statist physic aim vide reliabl scalabl approxim model mutual inform tractabl mutual mation comput exactli linear network sax et reliabl approxim model neural network learn matric weight close enough rotat invari thi exploit within replica theori order comput desir mutual inform gabrié et landscap glassi deep learn train deep neural network usual done via stochast gradient descent sgd landscap loss function statist physic ha long experi studi complex energi landscap relat dynam behaviour gradient scent algorithm close relat langevin namic often consid physic inspir work choromanska et becam ular somewhat naiv explor thi analog interest insight relat glassi namic learn deep neural network sent et particular role make landscap look less glassi highlight contrast parametr network anoth intrigu line work relat learn neural network properti landscap explor baldassi et thi work base realiz simpl model binari perceptron learn dynam end part ha mani conﬁgur goe suggest learn favour wide part space weight argu thi might explain whi algorithm attract wide local minima whi gener properti improv terest thi theori variant tic gradient descent algorithm suggest chaudhari et applic ml statist physic research theoret physic encount deep neural network earli layer ing repres input data ﬁner scale later layer immedi think tion group use physic order extract scopic behaviour microscop rule thi analog wa explor instanc béni mehta schwab analog renorm group principl compon analysi report bradd bialek natur idea use neural network order learn new renorm scheme first attempt thi direct appear ringel li wang howev remain shown whether thi lead new physic discoveri model well understood previous phase transit boundari diﬀer phase matter usual determin use order paramet system priori clear determin proper order paramet ral idea neural network may abl learn appropri order paramet locat phase sition without priori physic knowledg thi idea wa explor carrasquilla melko ingstar melko tanaka tomiya van nieuwenburg et rang model ing conﬁgur sampl uniformli model interest obtain use mont carlo simul ferent phase diﬀer temperatur use pervis learn order classifi conﬁgur phase extrapol conﬁgur use train set plausibl lead determin phase transit studi model gener guid principl use larg number applic analyz synthet experiment data speciﬁc case context tum physic detail section detail understand limit method term identifi previous unknown der paramet well understand whether reliabl distinguish true thermodynam phase transit mere yet clariﬁ experi present ise model mehta et provid preliminari thought direct underli mechan cuss kashiwa et kernel base learn method learn phase frustrat magnet rial easili interpret abl identifi complex order paramet introduc studi greitemann et liu et disord glassi solid identiﬁc order paramet particularli challeng also studi particular nussinov et ronhovd et use network cluster od identifi spatial structur glass cubuk et learn identifi structur ﬂow defect schoenholz et argu tifi paramet captur histori depend disord system ongo eﬀort go beyond limit supervis learn classifi phase identifi phase transit sever direct toward unsupervis ing begin explor instanc wetzel ise xy model wang zhai frustrat spin system work tiniani et explor direct identifi phase simpl compress underli urat machin learn also provid excit set tool studi predict control dynam system instanc pathak et use recurr neural network call echo state network voir comput jaeger haa predict trajectori chaotic dynam system el use weather predict author reddi et use reinforc learn teach autonom glider liter soar like bird use thermal atmospher outlook challeng describ method statist physic quit power deal data set model largest diﬀer tradit ing theori theori come statist physic later often base toy tive model data thi lead solvabl model sens quantiti interest achiev error comput close form includ constant term thi contrast aim mainstream learn theori aim provid worst case bound error gener assumpt set data structur architectur two approach complementari ideal meet futur onc understand key condit practic case close wors case right model realist data function next challeng statist physic approach formul solv model kind univers class real set interest ing reproduc import aspect haviour observ practic applic neural network thi need model input data longer iid vector instanc output gener neural network gabrié et perceptu manifold chung et teacher network produc label supervis set need model suitabl tion structur data label need ﬁnd analyz stochast gradient descent algorithm relev variant promis work thi direct reli dynam ﬁeld theori glass mannelli et need gener exist methodolog layer network extens width hidden layer go back direct use machin learn physic full potenti ml research linear dynam system statist physic yet uncov abov mention work certainli provid excit appet iii particl physic cosmolog divers portfolio plan ment well pois explor univers unimagin small world fundament particl awe inspir scale univers experi like larg hadron collid lhc larg synopt survey telescop lsst deliv enorm amount data compar predict speciﬁc oret model area well establish ical model serv null hypothes standard model particl physic λcdm cosmolog includ cold dark matter cosmolog constant interestingli altern hypothes consid formul theoret framework name quantum ﬁeld theori gener rel despit sharp theoret tool challeng still daunt expect deviat null expect incred tini reveal subtl eﬀect requir robust treatment complex experiment apparatus complic statist infer predict data come simpl equat instead plex comput simul machin learn make wave particl physic cosmolog oﬀer suit techniqu confront challeng new perspect motiv bold new strategi excit span cal experiment aspect ﬁeld includ applic immedi impact well prospect transform chang longer term role simul import aspect use machin learn particl physic cosmolog use comput simul gener sampl label train data xµ yµ n exampl target refer particl type particular scatter process ter appear fundament theori often speciﬁ directli simul code ulat directli sampl x case simul directli condit provid sampl x z z latent variabl describ happen insid simul observ actual experi target label comput latent variabl via function z label train data xµ zµ n also creat simul use ﬁdeliti simul gener label train data ha onli key earli success supervis learn area also focu research address shortcom thi approach particl physicist develop suit ﬁdeliti simul hierarch compos describ interact across huge rang length scale compon simul includ feynman diagrammat perturb expans quantum ﬁeld theori phenomenolog model complex pattern radiat detail model interact ticl matter detector result simul ha high ﬁdeliti simul ha free paramet tune number residu tainti simul must taken account analysi task similarli cosmologist simul evolut univers diﬀer length scale use gener ativ relev eﬀect matter radiat becom increasingli import ing structur format rich array mation made speciﬁc set provid enorm speedup compar comput pensiv simul billion massiv object interact gravit becom prohibit expens onc feedback eﬀect clude cosmolog simul gener involv istic evolut stochast initi condit due mordial quantum ﬂuctuat simul veri expens rel tion cover larg volum statist isotrop homogen larg scale contrast particl physic simul stochast throughout initi scatter interact detector simul collid experi run commod hardwar parallel manner physic goal quir enorm number simul collis becaus critic role simul ﬁeld much recent research machin learn relat simul one way anoth goal recent work develop techniqu data eﬃcient incorpor domain knowledg directli machin learn model incorpor uncertainti simul train procedur develop weakli supervis procedur appli real data reli tion develop anomali detect algorithm ﬁnd anomal featur data without simul speciﬁc signal hypothesi improv tune simul reweight adjust simul data better match real data use machin learn model residu simul real data learn fast neural network surrog tion use quickli gener synthet data develop approxim infer techniqu make eﬃcient use simul learn fast neural network surrog use directli statist infer classiﬁc regress particl physic machin learn techniqu use decad experiment particl physic aid particl identiﬁc event select seen classiﬁc task machin learn ha also use reconstruct seen regress task supervis learn use train predict model base larg number label train sampl xµ yµ n x denot input data target label case particl identiﬁc put featur x character local energi deposit detector label refer one particl speci electron photon pion struction task type sensor data x use target label refer energi tum particl respons energi deposit algorithm appli bulk data process lhc data event select refer task select small subset collis relev gete analysi task instanc search higg boson supersymmetri dark matter data alyst must select small subset lhc data consist featur hypothet nal process typic event select ment also satisﬁ background cess mimic featur signal either due experiment limit fundament quantum chanic eﬀect search simplest form reduc compar number event data satisfi requir predict onli null hypothesi nate hypothesi thu eﬀect event tion requir reject background process accept signal process power sult statist analysi within physic machin learn classiﬁc techniqu tradit refer multivari analysi emphas contrast tradit techniqu base simpl threshold cut appli care lect engin featur earli simpl ral network commonli use task ral network larg displac boost decis tree bdt classiﬁc sion task decad breiman et freund schapir roe et start around techniqu base deep learn emerg demonstr signiﬁcantli power sever applic recent review histori see ref guest et radov et deep learn wa ﬁrst use task target hypothes particl theori beyond standard model onli boost sion tree also requir engin featur achiev thi impress perform baldi et thi work network wa deep perceptron train veri larg ing set use simpliﬁ detector setup shortli idea parametr classiﬁ wa introduc concept binari classiﬁ wa extend situat signal hypothesi lift composit hypothesi parameter ousli instanc term mass hypothes particl baldi et jet physic copiou interact hadron collid lhc produc high energi quark gluon ﬁnal state quark gluon radiat quark gluon eventu combin neutral composit particl due phenomena ﬁnement result collim spray meson baryon strike detector collect refer jet develop use character ture jet theoret robust use test predict quantum ic qcd ha activ area particl physic research decad furthermor mani scenario physic beyond standard model predict tion particl decay two jet unstabl particl produc larg mentum result jet boost jet overlap singl fat jet nontrivi structur classifi boost fat jet much copious produc jet standard model process involv quark gluon area signiﬁcantli improv physic reach lhc gener identifi progenitor jet cation task often refer jet tag shortli ﬁrst applic deep learn event select deep convolut network use purpos jet tag detector data lend tion baldi et de oliveira et machin learn techniqu use within ticl physic decad practic ha alway strict input featur x ﬁxed dimension one challeng jet physic natur tation data term particl number particl associ jet vari ﬁrst tion recurr neural network particl physic wa context ﬂavor tag guest et recent ha explos research use diﬀer network architectur includ recurr network oper sequenc tree graph see ref larkoski et recent review jet physic thi includ hybrid approach leverag domain knowledg design architectur exampl motiv techniqu natur languag process recurs network design ate creat class jet tere algorithm loupp et similarli work develop motiv invari permut particl present network stabil detail radiat pattern cle komisk et recent ison diﬀer approach speciﬁc benchmark problem organ kasieczka et addit classiﬁc regress machin learn techniqu use densiti tion model smooth spectra analyt form well motiv simul ha niﬁcant uncertainti frate et work also allow one model altern signal hypothes diﬀus prior instead speciﬁc concret physic model abstractli gaussian process thi work ing use model intens inhomogen son point process scenario found particl physic astrophys cosmolog one terest aspect thi line work gaussian process kernel construct use composit rule correspond clearli causal model cist intuit use describ observ aid interpret duvenaud et neutrino physic neutrino interact veri feebli matter thu experi requir larg detector volum achiev appreci interact rate diﬀer type action whether come diﬀer speci trino background cosmic ray process leav diﬀer pattern local energi deposit detector ume detector volum homogen vate use convolut neural network ﬁrst applic deep convolut network analysi data particl physic ment wa context noνa experi use scintil miner oil interact noνa lead product light imag two diﬀer vantag point noνa develop tional network simultan process two imag aurisano et network improv eﬃcienc true posit rate select electron neutrino puriti thi network ha use search appear electron trino hypothet steril neutrino similarli microboon experi detect creat fermilab use ton time project chamber charg particl ioniz liquid argon ioniz electron drift ume three wire plane result data process repres imag nantli popul nois onli veri spars late legitim energi deposit microboon collabor use fasterrcnn ren et identifi local neutrino interact bound box acciarri et thi success import futur neutrino experi base time project chamber deep underground neutrino experi dune addit rel low energi neutrino duce acceler facil machin learn ha also use studi neutrino cube observatori locat south pole ular convolut graph neural network appli signal classiﬁc problem ter approach detector array model graph vertic sensor edg learn tion sensor spatial coordin graph ral network wa found outperform method well classic tional neural network choma et robust systemat uncertainti experiment particl physicist keenli awar simul incred accur perfect result commun ha develop number strategi fall roughli two broad class ﬁrst involv incorpor eﬀect simul use train thi involv either propag underli sourc uncertainti calibr detector respons quark gluon posit proton impact correct perturb theori etc simul analysi chain sourc uncertainti nuisanc paramet ν includ result statist model p ν parameter nuisanc paramet addit hood function data augment term p ν repres uncertainti sourc tainti case penal maximum likelihood analysi context machin learn classiﬁ regressor typic train use data gener nomin simul ν yield predict model f treat thi predict model ﬁxed possibl propag uncertainti ν f use model p ν p ν howev statist analysi base thi approach optim sinc predict model wa train take account uncertainti machin learn literatur thi situat often refer covari shift two domain resent train distribut target distribut variou techniqu domain tation exist train classiﬁ robust thi chang tend restrict binari main ν train target address thi problem adversari train techniqu wa develop tend domain adapt domain parametr ν loupp et adversari proach encourag network learn pivot titi p f x ν independ ν alent p f x p f x p ν thi adversari approach ha also use context algorithm fair one desir train classiﬁ sor independ decorrel ciﬁc continu attribut observ quantiti instanc jet physic one often would like jet ger independ jet invari mass min et previous diﬀer algorithm call uboost wa develop achiev similar goal boost decis tree rogozhnikov et steven william second gener strategi use within particl physic cope systemat ulat avoid use simul model distribut p follow let r denot index variou subset data satisfi respond select requir variou strategi develop relat distribut data control region p r tion region interest p r tionship also involv simul art thi approach base relationship aspect simul consid robust simplest ampl estim distribut p r speciﬁc process identifi subset data r domin p thi extrem situat limit applic recent weakli supervis techniqu develop onli involv identifi region onli class proport known assum rel probabl p linearli dent komisk et metodiev et techniqu also assum distribut p r independ r reason text question approach ha use train jet tagger discrimin quark gluon area ﬁdeliti simul longer adequ assumpt thi method reason thi approach major develop machin learn particl physic though limit set problem exampl thi approach plicabl one target categori correspond hypothet particl may exist present data trigger enorm amount data must collect lider experi lhc becaus ena target exceedingli rare bulk collis involv phenomena previous studi character data volum ciat full data stream impract larg result collid experi use reduct system refer trigger trigger make critic decis event keep futur analysi event discard la cm experi retain onli everi event machin learn techniqu use variou degre system essenti particl identiﬁc classiﬁc task appear thi context though comput demand perform term fals posit neg diﬀer environ lhcb experi ha leader use chine learn techniqu trigger roughli data select lhc trigger select machin learn algorithm initi experi use boost decis tree thi purpos gligorov william wa later replac trixnet algorithm develop yandex likhomanenko et trigger system often use special hardwar ﬁrmware gate array fpga recent tool develop streamlin compil machin learn model fpga target requir trigger system duart et tsari et theoret particl physic bulk machin learn particl physic cosmolog focus analysi observ data also exampl use machin learn tool theoret physic instanc machin learn ha use character landscap string theori cariﬁo et identifi phase transit quantum chromodynam qcd pang et studi denc hashimoto et b thi work close connect use machin learn tool condens matter quantum physic speciﬁc deep learn ha use context lattic qcd lqcd exploratori work thi direct deep neural network use predict paramet qcd lagrangian lattic urat shanahan et thi need number approach aim improv eﬃcienc comput tensiv lqcd calcul thi problem wa setup regress task one challeng rel train exampl addit machin learn techniqu use reduc correl time markov chain albergo et tanaka tomiya order solv thi task train exampl import age known local gaug symmetri lattic data data augment scalabl solut given rich symmetri instead author perform featur engin impos gaug symmetri translat invari thi approach prove eﬀect would abl consid richer class network ariant covari symmetri data approach discuss sec continu thi work support argon leadership comput facil new system rora capabl exaﬂop speciﬁc aim problem combin tradit high formanc comput modern machin learn niqu classiﬁc regress cosmolog photometr redshift due expans univers distant minou object redshift relat fundament compon observ molog veri precis redshift estim obtain spectroscopi howev spectroscop vey expens time consum photometr survey base broadband photometri imag color band give coars approxim tral energi distribut photometr redshift refer regress task estim redshift metric data thi case ground truth train data come precis spectroscop survey tradit approach photometr redshift base templat ﬁtting method benítez mer et feldmann et decad cosmologist also use machin learn method base neural network boost decis tree photometr redshift carrasco kind ner collist lahav firth et one interest aspect thi bodi work eﬀort ha place go beyond point estim redshift variou approach exist determin uncertainti redshift estim obtain terior distribut train data gener ulat still concern distribut train data may repres bution data model appli thi type covari shift result variou select fect spectroscop survey subtleti photometr survey dark energi survey ere number approach establish valid process evalu critic bonnett et recent ha work use erarch model build addit causal structur model robust diﬀer languag machin learn new model aid transfer learn domain adapt chical model also aim combin interpret tradit templat ﬁtting approach ﬂexibl machin learn model leistedt et gravit len ﬁnding paramet estim one strike eﬀect gener rel gravitatioanl lens massiv foreground object warp imag background object strong gravit lens occur exampl sive foreground galaxi nearli coincid sky background sourc event power probe dark matter distribut massiv galaxi provid valuabl cosmolog constraint ever system rare thu scalabl abl len ﬁnding system essenti cope larg survey lsst euclid wfirst simpl feedfoward convolut residu neural network resnet appli thi supervis ﬁcation problem estrada et lanuss et marshal et thi set ing data came simul use pic pipelin imag cosmolog strong lens li et strong lens lenspop collett mock lsst observ onc identiﬁ teriz lens object maximum likelihood estim comput intens timiz task recent convolut network use quickli estim paramet gular isotherm ellipsoid densiti proﬁl commonli use model strong lens system hezaveh et exampl addit exampl abov ground truth object rel unambigu approach cosmologist also leverag machin learn infer quantiti involv servabl latent process paramet mental cosmolog model exampl convolut network train predict fundament cosmolog paramet base dark matter spatial distribut bakhsh et see fig thi work network train use comput intens simul evolut dark matter univers assum speciﬁc valu paramet standard λcdm cosmolog model real applic thi techniqu visibl matter one would need model bia varianc visibl tracer respect underli dark matter bution order close thi gap convolut network train learn fast map dark matter visibl galaxi zhang et low simul accuraci comput cost one challeng thi work common applic solid state physic lattic ﬁeld theori mani bodi quantum system simul comput expens thu rel statist independ tion larg simul xµ deep learn tend requir larg label train variou type subsampl data augment approach explor amelior situat altern approach subsampl backdrop provid stochast gradient loss function even individu sampl introduc stochast mask backpropag pipelin golkar cranmer infer fundament cosmolog model also appear classiﬁc set particular iﬁe graviti model massiv neutrino mimic predict observ predict standard λcdm model degeneraci exist restrict xµ statist broken incorpor statist rich represent weak lens signal ticular author peel et construct novel represent wavelet decomposit weak lens signal input convolut network result approach wa abl discrimin previous degener model accuraci deep learn ha also use estim mass galaxi cluster largest gravit bound structur univers power logic probe much mass galaxi cluster come form dark matter directli observ galaxi cluster mass estim via gravit lens observ cluster medium dynam analysi cluster galaxi ﬁrst use machin learn dynam cluster mass estim wa perform use support distribut machin póczo et simul ntampaka et number network algorithm clude gaussian process regress kernel ridg sion support vector machin gradient boost tree gressor appli thi problem use macsi simul henson et train data thi simul goe beyond simul incorpor impact variou astrophys process allow opment realist process pipelin pli observ data need accur autom mass estim pipelin motiv larg survey eboss desi erosita pol euclid author found compar tradit relat mass ratio use machin learn techniqu reduc factor armitag et recent lution neural network use mitig temat virial scale relat improv dynam mass estim ho et tional neural network also use estim cluster mass synthet mock observ puter scienc carnegi mellon univers forb pittsburgh pa usa enter cosmolog depart physic carnegi mellon univers carnegi forb usa abstract alleng centuri ccurat estim cosmolog univers major approach g cosmolog paramet scale matter distribut xy survey provid mean map structur three mation galaxi locat ariz singl function scale galaxi correl function show possibl estim logic paramet directli matter thi paper present deep convolut network c represent well result obtain use pose distribut regress ng machin learn techniqu abl sometim point estim use al model thi open way paramet univers aci n ha brought us tool method e univers far greater detail us deepli probe fundament gy suit cosmolog rd intern confer machin rk ny usa jmlr w cp volum author figur dark matter distribut three cube produc use differ set paramet cube divid small cube train predict note although cube thi ﬁgure produc use veri differ cosmolog eter constrain sampl set effect visual cernibl servat allow us make seriou inroad derstand univers includ cosmic crowav background cmb planck collabor et hinshaw et supernova perlmutt et riess et larg scale structur galaxi galaxi cluster cole et anderson et parkinson et particular larg scale structur involv measur posit properti bright sourc great volum sky amount inform overwhelm modern method machin learn statist play creasingli import role modern cosmolog ampl common method compar larg scale ture observ theori compar compress figur dark matter distribut three cube produc use diﬀer set paramet cube divid small cube train predict note although cube thi ﬁgure produc use veri diﬀer cosmolog paramet constrain sampl set eﬀect visual discern reproduc ravanbakhsh et galaxi cluster author ﬁnd scatter predict mass reduc compar tradit ray luminos base method ntampaka et invers problem infer stress repeatedli particl physic molog character well motiv forward simul forward simul ther intrins stochast case bilist decay interact found particl physic simul determinist case gravit lens gravit tion howev even determinist physic simul usual follow probabilist descript observ base poisson count model strument nois case one consid ulat implicitli deﬁn distribut p x x refer observ data z unobserv latent variabl take random valu insid simul paramet forward model coeﬃcient lagrangian paramet λcdm cosmolog mani scientiﬁc task acter invers problem one wish infer z x simplest case consid classiﬁc take categor valu regress point estim ˆ x x ˆ z x x use scientiﬁc plicat often requir uncertainti estim mani case solut invers problem sens small chang x lead larg chang estim thi impli estim high varianc case forward model equival linear oper maximum lihood estim ˆ ymle x ˆ zmle x express matrix invers case instabl invers relat matrix forward model poorli condit maximum hood estim may unbias tend high anc penal maximum likelihood ridg regress tikhonov regular gaussian process sion close relat approach within particl physic thi type problem often refer unfold case one often est distribut kinemat properti collis prior detector eﬀect x sent smear version thi quantiti fold detector eﬀect similarli estim parton siti function describ quark gluon insid proton cast invers problem thi sort ball et fort et recent neural work gaussian process sophist physic inspir kernel appli problem bozson et frate et context cosmolog exampl invers problem denois laser interferomet servatori ligo time seri underli waveform gravit wave shen et gener adversari network gan even use context invers problem use nois recov imag galaxi beyond naiv volut limit schawinski et anoth ple involv estim imag background object prior gravit lens foreground ject thi case describ physic motiv prior background object diﬃcult recent recurr infer machin putzki well introduc way implicitli learn prior vers problem success appli strong gravit lens morningstar et ambiti approach invers problem volv provid detail probabilist character given frequentist paradigm one would aim character likelihood function l p x bayesian formal one would wish teriz posterior p x x p analog situat happen infer latent abl z given particl physic cosmolog approach statist infer base detail model likelihood markov chain mont carlo mcmc et hamiltonian mont carlo variat enc jain et lang et regier et howev approach requir likelihood function tractabl infer somewhat surprisingli probabl densiti lihood p x implicitli deﬁn ulat often intract symbol probabl densiti written p r p x dz z latent variabl simul latent space simul enorm highli structur thi integr perform analyt simul singl collis lhc z may hundr million compon practic simul often base mont carlo techniqu gener sampl xµ zµ x densiti estim challeng x diﬃcult accur estim densiti exampl naiv base approach scale high dimens kernel densiti estim techniqu onli thi around ad challeng distribut larg dynam rang interest physic often sit tail bution intract likelihood implicitli deﬁn simul foundat problem onli particl physic cosmolog mani area scienc well includ epidemiolog netic thi ha motiv develop infer algorithm onli requir abil gener sampl simul forward mode one promin techniqu approxim bayesian comput abc abc one perform bayesian ferenc use mcmc reject sampl approach likelihood approxim biliti p ρ x x ϵ x observ data condit ρ x distanc metric tween x output simul ϵ toler paramet ϵ one recov exact bayesian infer howev eﬃcienc dure vanish one challeng abc larli x speciﬁc distanc measur ρ x maintain reason ceptanc eﬃcienc without degrad qualiti infer beaumont et marin et joram et sisson fan sisson et thi approach estim likelihood quit similar tradit practic particl physic use histogram kernel densiti estim imat ˆ p case domain knowledg requir identifi use summari order reduc dimension data interest sion abc techniqu util univers probabilist program particular techniqu known enc compil sophist form import sampl neural network control random number gener probabilist program bia simul produc output closer serv x le et term abc often use synonym gener term infer howev number approach involv learn approxim likelihood likelihood ratio use surrog intract likelihood ratio exampl neural densiti estim autoregress model normal ﬂow larochel murray papamakario et rezend moham use thi purpos scale higher dimension data cranmer loupp papamakario et altern train classiﬁ discrimin x x use estim hood ratio ˆ r use infer either frequentist bayesian paradigm brehmer et cranmer et herman et exampl particl physic thousand publish result within particl physic includ discoveri higg boson involv tical infer base surrog likelihood ˆ p struct densiti estim techniqu appli synthet dataset gener simul typic restrict mari statist featur ber event observ term infer rel new core methodolog experiment particl physic recent suit infer simul machin learn infer x z latexit latexit latexit latexit r x latexit latexit yfnf latexit yfnf latexit kqlq x latexit bmhp latexit whfd latexit whfd latexit gutw latexit acmhicpvlt latexit acmhicpvlt cfelahnftcgllmillywscqnom cevdbvmgnupiidsjazxgmyyri sbmmhergriwykdodxbktvpkh latexit acmhicpvlt cfelahnftcgllmillywscqnom cevdbvmgnupiidsjazxgmyyri sbmmhergriwykdodxbktvpkh latexit acmhicpvlt eluggayhlrrjdosienvfcjrck arg min g l g latexit acq latexit acq latexit acq latexit acq ˆ r latexit latexit latexit latexit latexit latexit latexit latexit latexit latexit latexit latexit atexit paramet latent observ augment data approxim likelihood ratio figur schemat machin learn base approach infer simul provid train data neural network subsequ use surrog intract likelihood dure infer reproduc brehmer et niqu base neural network develop appli model physic beyond dard model express term eﬀect ﬁeld theori eft brehmer et b eft provid temat expans theori around standard model parametr coeﬃcient quantum mechan oper play role thi set one interest observ thi work even though likelihood likelihood ratio tractabl joint likelihood ratio r x joint score x log p x tractabl use augment train data see fig dramat improv sampl eﬃcienc techniqu brehmer et addit infer compil techniqu ha appli infer decay thi eﬀort requir develop probabilist program protocol integr ing simul code sherpa baydin et casado et thi approach provid bayesian infer latent abl p x deep interpret terior correspond distribut complet trace simul allow ani aspect ulat inspect probabilist anoth techniqu infer wa motiv challeng particl physic known adversari variat optim avo loupp et avo parallel gener adversari network gener model longer neural network instead simul instead optim paramet network goal optim paramet simul gener data match target data distribut main challeng unlik neural network scientiﬁc simul diﬀerenti get around thi problem variat optim techniqu use provid diﬀerenti surrog loss function thi techniqu investig tune paramet simul comput intens task bayesian optim ha also recent use ilten et exampl cosmolog within cosmolog earli use abc includ strain thick disk format scenario milki way robin et infer rate morpholog transform galaxi high shift cameron pettitt aim track hubbl paramet evolut type ia supernova measur experi motiv opment tool cosmoabc streamlin plicat methodolog cosmolog tion ishida et recent infer method base machin learn also develop motiv experi cosmolog confront leng abc observ x data compress strategi wa develop learn summari statist maxim fisher tion paramet als et charnock et learn summari statist mate suﬃcient statist implicit likelihood small neighborhood nomin ﬁducial eter valu thi approach close connect brehmer et recent approach extend learn summari statist robust systemat uncertainti als wandelt gener model activ area machin learn research involv use unsupervis learn train gener model produc distribut match empir distribut thi includ gener adversari work gan goodfellow et variat toencod vae kingma well rezend et autoregress model model base normal ﬂow larochel murray pamakario et rezend moham interestingli issu motiv free infer intract densiti implicitli deﬁn simul also appear gener sarial network gan densiti gan tractabl gan would train via standard mum likelihood becaus densiti intract trick wa need trick introduc sari discrimin network use classifi sampl gener model sampl taken target distribut discrimin fectiv estim likelihood ratio two distribut provid direct connect approach infer base ﬁer cranmer loupp oper model play similar role dition scientiﬁc simul though tradit tion code also provid causal model underli data gener process ground physic principl howev tradit scientiﬁc simul often veri slow distribut interest emerg level microphys descript exampl ing collis lhc involv physic ioniz scintil similarli simul cosmolog involv gravit interact among mou number massiv object may also includ complex feedback process involv radiat star format etc therefor learn fast approxim simul great valu within particl physic earli work thi direct includ gan energi deposit particl calorimet paganini et b studi atla collabor atla tion cosmolog gener model use learn simul cosmolog structur format rodríguez et interest brid approach deep neural network wa use predict structur format univers residu fast physic simul base linear perturb theori et case simul way exist impract nevertheless gener model data valuabl purpos calibr illustr exampl thi rection come ravanbakhsh et see fig author point next gener molog survey weak gravit lens reli accur measur appar shape distant galaxi howev shape measur method requir precis calibr meet accuraci requir scienc analysi thi calibr process leng requir larg set high qualiti galaxi imag expens collect therefor gan enabl implicit gener ric bootstrap outlook challeng particl physic cosmolog long tori util machin learn method scope topic machin learn appli ha grown signiﬁcantli machin learn seen key strategi confront challeng grade lhc albertsson et apollinari et inﬂuenc strategi futur experi cosmolog particl physic ntampaka et one area particular ha gather great deal attent lhc challeng identifi track left charg particl environ farrel et ha focu recent kaggl leng almost area machin learn pli physic problem desir incorpor domain knowledg form hierarch structur composit structur geometr structur metri known exist data gener process recent ha spate work machin learn commun thi tion bronstein et cohen well hen et cohen et kondor kondor et kondor trivedi ment follow close physicist readi incorpor contemporari research thi area iv quantum matter intrins probabilist natur quantum ic make physic system thi realm eﬀect inﬁnit sourc big data veri appeal ground ml applic paradigmat exampl thi probabilist natur measur process quantum physic measur posit r tron orbit around nucleu onli mate infer measur inﬁnit cise classic measur devic onli use record outcom speciﬁc observ tron posit ultim complet character measur process given wave function fig sampl dataset versu gener sampl use condit gener adversari network section iii synthet imag color imag invert produc condit set featur pair observ gener imag column correspond valu detail featur see willett et al instanc select unavail model dure train imag condit statist interest bright size galaxi thi allow us thesiz calibr dataset speciﬁc galaxi popul object exhibit realist morpholog relat work machin learn literatur regier et al use convex combin smooth spiral templat uncondit gener model galaxi imag regier et al propos use vae thi follow section give brief background imag gener calibr signiﬁc ern cosmolog review current approach deep condit gener model introduc new techniqu problem set section ii iii section iv assess qualiti gener imag compar condit distribut shape morpholog paramet simul real galaxi ﬁnd good agreement weak gravit lens weak regim gravit lens distort background galaxi imag model anisotrop shear note γ whose amplitud orient depend matter distribut observ distant galaxi thi shear affect particular appar ellipt galaxi denot measur thi weak lens effect made possibl assumpt background galaxi randomli orient ensembl averag shape would averag zero absenc lens appar ellipt e use noisi unbias estim shear ﬁeld γ e e cosmolog current approach address thi problem cosmolog literatur ﬁt analyt parametr light proﬁl deﬁn size intens ellipt steep paramet observ galaxi follow simpl model distribut ﬁtted paramet function quantiti interest galaxi bright thi model usual simpli involv ﬁtting linear depend mean standard deviat gaussian distribut see hoekstra et al appendix howev simpl parametr model galaxi light proﬁl complex morpholog need calibr task onli current avail altern realist galaxi morpholog need use train set imag themselv input simul pipelin thi involv subsampl train set match distribut size redshift bright target galaxi simul leav onli rel small number object reus sever hundr time simul larg survey see jarvi et al section analysi involv comput measur ellipt galaxi differ distanc correl function compar theoret diction order constrain cosmolog model shed light natur dark energi howev measur galaxi ellipt ensembl averag use cosmolog analysi unbias extrem challeng task fig illustr main step involv acquisit scienc imag weakli shear galaxi imag undergo addit distort essenti blur go mospher telescop optic befor acquir imag sensor pixel noisi imag thi ﬁgure illustr cosmolog shear clearli subdomin effect ﬁnal imag need disentangl subsequ blur atmospher telescop option thi blur point spread function psf directli measur use star point sourc shown top fig onc imag acquir shape measur algorithm use estim ellipt galaxi ing psf howev despit best effort weak lens commun nearli two decad current shape measur algorithm still suscept bias infer shear measur bias commonli model term addit multipl bia paramet c deﬁn e e γ c γ true shear depend shape ment method use c depend factor psf level nois imag gener intrins properti galaxi popul like size ellipt distribut etc calibr bias achiev use imag simul close mimick real observ given survey use galaxi imag distort known shear thu allow measur bia paramet eq imag simul pipelin galsim packag row et al use forward model tion reproduc step imag acquisit figur sampl dataset versu gener sampl use condit gener adversari network synthet imag color imag invert produc condit set featur pair observ gener imag column correspond valu reproduc ravanbakhsh et ψ r whose squar modulu ultim deﬁn abil p r r observ electron given posit space case singl electron theoret predict experiment infer p r eﬃcient perform situat becom dramat complex case mani quantum particl exampl probabl observ posit n electron p rn intrins function seldom exactli determin n much larger ten nential hard estim p rn rect consequ estim bodi amplitud ψ rn commonli refer quantum problem quantum problem manifest varieti case chieﬂi includ theoret model simul complex quantum system al molecul onli approxim solut often avail veri import manifest quantum problem includ stand analysi experiment outcom cialli relat complex phase matter follow discuss ml applic focus allevi challeng theoret periment problem pose quantum problem quantum state quantum state nq tation term cial neural network ann carleo troyer commonli adopt choic parameter function amplitud neural network ψ r g l w l g w g w r similar notat introduc eq earli work mostli concentr shallow work notabl restrict boltzmann machin rbm smolenski rbm hidden unit without bias visibl unit malli correspond ffnn depth l tivat g x log cosh x g x exp x import diﬀer respect rbm applic unsupervis learn probabl distribut use nq rbm state typic taken weight carleo troyer deeper architectur consist introduc recent work exampl nq base convolut deep network choo et saito sharir et see fig schemat exampl tion use deep ffnn network apart tical success deep learn industri applic also come gener theoret argument quantum physic exampl ha shown deep nq sustain entangl eﬃcient rbm state levin et liu et extens nq represent concern tion mix state describ densiti matric rather pure thi context possibl deﬁn rbm parametr densiti matrix torlai melko one speciﬁc challeng emerg tum domain impos physic symmetri nq represent case period arrang matter spatial symmetri impos use volut architectur similar use age classiﬁc task choo et saito sharir et select state ent symmetri sector ha also demonstr choo et spatial symmetri analog counterpart ml applic satisfi involv quantum symmetri often need deep ing ann architectur notabl case thi sens exchang symmetri boson thi amount impos tional invari respect exchang particl heisenberg convolut complex deep b sum channel σ σ σ latexit stvuioh figur top exampl shallow convolut neural network use repres system spin particl squar lattic bottom ter convolut rbm found ation learn heisenberg model adapt carleo troyer indic model ha adopt benchmark ann boson architectur result obtain saito saito kato teng leng symmetri howev certainli fermion one thi case nq represent need code antisymmetri ing two particl posit exampl lead minu sign thi case diﬀer approach plore mostli expand exist variat ansatz fermion symmetr rbm ing antisymmetr correl part ha use studi interact lattic fermion mura et approach tackl fermion symmetri problem use backﬂow mation slater determin luo clark directli work ﬁrst quantiz han et situat fermion certainli ing ml approach moment owe ciﬁc natur symmetri applic side nq represent use along three main diﬀer research line represent theori activ area research concern gener sive power nq also compar famili variat state theoret activ tation properti nq seek understand larg deep neural network describ est interact quantum system connect ﬁrst numer result obtain rbm state entangl ha soon identiﬁ possibl didat express power nq rbm state exampl eﬃcient support scale deng et number variat paramet scale onli polynomi system size thi rection languag tensor network ha ticularli help clarifi properti nq chen et pastori et ili nq base rbm state ha shown equival certain famili variat state known clark glasser et question determin larg spectiv class quantum state belong nq form eq comput eﬃcient tensor network howev still open exact represent sever intrigu phase matter includ topolog state stabil code deng et glasser et huang moor kaubruegg et lu et zheng et also obtain close rbm form surprisingli given shallow depth rbm architectur also expect limit gener ground speciﬁc gener possibl write possibl physic state term compact rbm state gao duan order lift intrins limit rbm cientli describ veri larg famili physic state necessari introduc deep boltzmann machin dbm two hidden layer gao duan similar network construct introduc also sibl theoret framework altern standard represent quantum mechan leo et learn data parallel activ understand cal properti nq famili studi thi ﬁeld concern problem understand hard practic learn quantum state numer data thi realiz use either synthet data exampl come numer simul directli experi thi line research ha explor vise learn set understand well nq repres state easili express close analyt form ann goal train nq network repres close possibl tain target state amplitud eﬃcient comput thi approach ha success use learn fermion frustrat boson hamiltonian cai liu repres terest studi case sinc structur target pose challeng dard activ function use ffnn along line supervis approach propos learn random matrix product state shallow nq borin abanin aliz nq includ comput treatabl dbm form pastori et latter case studi reveal eﬃcient strategi perform learn former case hard learn random mp ha show present late thi hard origin entangl structur random mp howev unclear thi relat hard nq optim scape intrins limit shallow nq besid supervis learn given quantum state approach nq larg trate unsupervis approach thi framework onli measur target state densiti matrix avail goal reconstruct full state nq form use measur simplest set one given data set ment r r distribut accord born rule prescript p r r p r struct case posit init onli measur certain basi provid reconstruct p r standard vise learn approach enough reconstruct avail inform underli quantum state thi approach exampl ha demonstr stoquast hamiltonian torlai et use gener model approach base deep vae gener model ha also demonstr case famili sampl quantum state rocchetto et eﬀect network depth ha shown beneﬁci compress gener set problem struct gener quantum state either pure mix use measur singl basi quantum number especi necessari construct also complex phase quantum state thi problem correspond problem quantum inform known quantum state raphi speciﬁc nq approach troduc carrasquilla et torlai et torlai melko discuss detail dedic section also connect ml techniqu use thi task variat learn final one main applic nq represent context variat imat quantum problem goal approach exampl approxim solv schrödinger equat use nq tation thi case problem ﬁnding ground state given quantum nian h formul variat term lem learn nq weight w minim e w w w w w thi achiev use learn scheme base variat mont carlo mizat carleo troyer within thi famili applic extern data repres tum state given thu typic demand larger comput burden supervis unsupervis learn scheme nq experi varieti spin choo et deng et glasser et liang et boson choo et saito saito kato fermion han et luo clark nomura et model shown result competit exist proach obtain case improv exist variat result demonstr notabl lattic model carleo troyer luo clark nomura et topolog phase matter glasser et kaubruegg et nq applic concern solut schrödinger equat carleo troyer czischek et fabiani mentink schmitt heyl applic one use variat principl dirac frenkel dirac frenkel learn mal time evolut network weight thi abli gener also open dissip quantum tem variat solut lindblad equat realiz hartmann carleo nagi savona vicentini et yoshioka hamazaki great major variat applic discuss learn scheme use typic techniqu standard sgd approach stochast reconﬁgur sr approach becca sorella sorella gener case carleo et proven particularli suitabl variat learn nq sr scheme seen quantum analog method learn probabl bution amari build intrins tri associ paramet recent eﬀort use deeper express network initi adopt learn scheme build techniqu sistent use kochkov clark sharir et constitut two diﬀer philosophi proach problem one hand earli cation focus small network learn veri curat expens train techniqu hand later approach focus deeper network cheaper also less learn techniqu combin two philosophi comput cient way one open challeng ﬁeld speed simul use ml method realm tum problem extend well beyond network represent quantum state ful techniqu studi interact model tum mont carlo qmc approach method stochast comput properti quantum system map eﬀect classic model ampl mean represent practic issu often result map provid eﬃcient sampl scheme space path integr perturb seri etc quir care tune often ing sampler represent therefor particularli challeng problem vise ml method howev adopt tool mont carlo sampl classic quantum applic sever approach thi tion propos leverag abil supervis learn well approxim target tribut sampl underli mont carlo scheme rel simpl gener model use earli applic cal system huang wang liu et mont carlo techniqu gener also fermion system chen et liu et nagai et overal ha found approach eﬀect reduc autocorrel time especi compar lie less eﬀect markov chain mont carlo local updat recent gener ml model adopt sampl ciﬁc task notabl wu et use deep toregress model may enabl eﬃcient pling hard classic problem spin glass problem ﬁnding eﬃcient sampl scheme underli classic model transform problem ﬁnding eﬃcient correspond sive deep network represent thi approach ha also gener quantum case sharir et autoregress represent introduc thi represent automat normal allow bypass markov chain mont carlo variat learn discuss abov exact larg famili boson spin tem qmc techniqu typic incur sever sign problem deal sever interest fermion model well frustrat spin hamiltonian thi case tempt use ml approach attempt rect indirect reduct sign problem onli ﬁrst stage thi famili applic ha use infer inform fermion phase den inform green function broecker et similarli ml techniqu help reduc burden subtl manifest sign problem namic properti quantum model particular problem reconstruct spectral function correl imaginari time also ﬁeld ml use altern tional techniqu perform cal continu qmc data arsenault et fournier et yoon et classifi quantum phase challeng pose complex quantum state manifest mani form speciﬁc sever elus phase quantum matter often hard character pinpoint cal simul experi thi reason ml scheme identifi phase matter becom ularli popular context quantum phase follow review speciﬁc applic quantum domain gener discuss identifi phase phase transit found synthet data follow earli develop phase cation supervis approach carrasquilla melko van nieuwenburg et wang mani studi sinc focus analyz phase matter synthet data mostli simul quantum system attempt vide exhaust review mani studi appear thi direct highlight two larg famili lem larg serv benchmark new ml tool ﬁeld ﬁrst challeng test bench phase classiﬁc scheme case quantum local thi elus phase matter show tic ﬁngerprint necessarili emerg tradit der paramet see exampl alet laﬂorenci recent review topic first studi thi direct focus train strategi ing hamiltonian entangl spectra hsu et huemb et schindler et venderley et zhang et work demonstr abil veri eﬀect learn mbl phase transit rel small system cessibl exact diagon techniqu studi instead focus identifi signatur directli experiment relev quantiti tabli dynam local quantiti doggen et van nieuwenburg et latter scheme appear present ing applic experi former use tool identifi exist pect phase presenc correl disord hsu et anoth veri challeng class problem found analyz topolog phase matter larg consid test ml scheme caus phase typic character local order paramet turn order paramet hard learn popular classiﬁc scheme use imag thi speciﬁc issu alreadi present analyz classic model featur logic phase transit exampl presenc transit learn scheme train raw mont carlo conﬁgur eﬀect beach et hu et problem vent devis train strategi use featur broecker et cristoforetti et wang zhai wetzel instead raw mont carlo sampl featur typic reli portant assumpt natur phase transit look thu diminish tive look new phase matter deeper quantum world ha research activ along direct learn supervis fashion topolog invari neural network use exampl classifi famili topolog hamiltonian use input discret cient either real ohtsuki ohtsuki momentum space sun et zhang et case found neural network abl reproduc alreadi known beforehand topolog invari wind number berri curvatur context cal matter larg extent challeng case band model thi case common approach deﬁn set care engin featur use top raw data one well known exampl case quantum loop topographi zhang kim train local oper comput singl shot sampl walker exampl done variat mont carlo ha shown thi veri speciﬁc choic local featur abl distinguish strongli teract fraction chern insul also tum spin liquid zhang et similar eﬀort realiz classifi exot phase ter includ magnet skyrmion phase iakovlev et dynam state antiskyrmion dynam ritzmann et despit progress seen far along mani rection describ fair say topolog phase matter especi interact system stitut one main challeng phase tion good progress ha alreadi made huemb et scheurer futur research need address issu ﬁnding train scheme reli data featur experiment data beyond extens studi data numer ulat supervis scheme found way also tool analyz experiment data quantum tem atom experi supervis ing tool use map topolog phase particl well onset mott insul phase ﬁnite optic trap rem et thi speciﬁc case phase alreadi known identiﬁ approach ever techniqu combin theoret knowledg experiment data hold potenti genuin scientiﬁc discoveri exampl ml enabl scientiﬁc discoveri interest case experiment data ha tribut one mani avail equal like priori theoret model experiment tion hand easili interpret typic est case emerg exampl order eter complex onli implicitli known function experiment outcom thi tion ml approach use power tool eﬀect learn underli trait given theori provid possibl unbias classiﬁc mental data thi case incommensur phase superconductor scan tunnel microscopi imag reveal complex patter hard deciph use convent analysi tool use supervis approach thi context recent work zhang et ha shown possibl infer natur spatial order system also see fig similar idea ha also use anoth totyp interact quantum system fermion hubbard model implement atom periment optic lattic thi case refer model provid snapshot thermal densiti matrix supervis learn ion outcom thi studi bohrdt et experiment result good conﬁdenc compat one theori propos thi case geometr string theori charg carrier last two experiment applic describ abov outcom supervis approach larg extent highli hard predict ori basi inform hand inner bia induc choic theori classiﬁ howev one current limit kind approach face tensor network machin learn research topic review far mainli cern use ml idea tool studi figur exampl machin learn approach ﬁcation experiment imag scan tunnel croscopi superconductor imag classiﬁ accord predict distinct type riodic spatial modul reproduc zhang et lem realm quantum physic plementari thi philosophi interest research direct ﬁeld explor invers direct vestig idea quantum physic inspir devis new power ml tool central develop represent quantum state veri success variat famili wave function ural emerg represent quantum state verstraet et tensor work serv practic conceptu tool ml task supervis vise set approach build idea provid learn scheme network structur altern convent adopt stochast learn scheme ffnn network exampl trix product state mp represent simul interact tum system white perform classiﬁc task liu et novikov et stoudenmir schwab also recent adopt explicit gener model supervis learn han et stoke illa worth mention relat tensor decomposit develop text appli mathemat use ml pose acar yener anandkumar et decomposit oseledet formal equival mp represent introduc parallel tool perform variou machin ing task gorodetski et izmailov et novikov et network close relat mp also explor model guo et eﬀort increas amount entangl encod tensor decomposit recent work concentr tion altern mp form one notabl ple use tree tensor network cal structur hackbusch kühn shi et appli classiﬁc liu et stoudenmir gener ele cheng et task good success exampl use entangl plaquett state changlani et gendiar nishino zacapo et string bond state schuch et show sizabl improv tion task mp state glasser et theoret side deep connect tween tensor network complex measur tum entangl entropi use understand possibl inspir success network design ml purpos network formal ha proven power interpret deep learn len renorm group concept pioneer work thi direct ha connect mera tensor network state vidal cal bayesian network béni later analysi convolut arithmet circuit cohen et famili convolut network product linear introduc conveni model bridg tensor decomposit ffnn architectur besid conceptu relev connect help clarifi role induct bia modern monli adopt neural network levin et outlook challeng applic ml quantum problem seen progress past year touch divers select topic rang meric simul data analysi potenti ml techniqu ha alreadi surfac thi context alreadi show improv perform respect exist techniqu select problem larg extent ever real power ml techniqu thi domain ha onli partial demonstr sever open problem remain address context variat studi nq ampl origin empir success obtain far diﬀer kind neural network quantum state equal well understood famili variat state like tensor network key open leng remain also represent simul fermion system eﬃcient represent still found represent ml purpos well network like use nq play import role bridg ﬁeld back arena comput scienc challeng futur thi research direct consist eﬀect interfac commun retain interest gener physic tool concern ml approach experiment data ﬁeld larg still infanc onli applic demonstr far thi stark contrast ﬁeld astrophys ml approach matur stage often use standard tool data analysi move toward achiev goal quantum domain demand closer collabor theoret experiment eﬀort well deeper understand speciﬁc problem ml make substanti diﬀer overal given rel short time span applic ml approach quantum matter emerg howev good reason believ challeng energet come year quantum comput quantum comput use quantum system process inform popular framework base quantum comput nielsen chuang quantum algorithm describ evolut initi state quantum system n system call qubit ﬁnal state discret format quantum gate gate usual act onli small number qubit sequenc gate deﬁn comput intersect machin learn quantum comput ha becom activ research area last coupl year contain varieti way merg two disciplin see also dunjko briegel review quantum machin learn ask quantum comput enhanc speed innov machin learn biamont et ciliberto et schuld petruccion see also section vii v quantum learn theori highlight retic aspect learn quantum framework arunachalam de wolf thi section concern third angl name machin learn help us build studi quantum comput thi angl includ topic rang use intellig data mine method ﬁnd physic regim materi use qubit kalantr et veriﬁc tum devic agresti et learn design quantum algorithm bang et wecker et facilit classic simul quantum cuit jónsson et autom design tum experi krenn et melnikov et learn extract relev inform measur seif et focu three gener problem relat tum comput target rang ml method problem reconstruct en ing quantum state via measur problem prepar quantum state via quantum control problem maintain inform store state quantum error correct ﬁrst lem known quantum state tomographi pecial use understand improv upon itat current quantum hardwar quantum control quantum error correct solv relat problem howev usual former refer lution latter use algorithm solut problem execut comput protocol quantum system similar disciplin thi review machin learn ha shown promis result area longer run like enter toolbox quantum comput use method quantum state tomographi gener goal quantum state tomographi qst reconstruct densiti matrix unknown tum state experiment avail ment qst central tool sever ﬁeld quantum inform quantum technolog gener often use way assess qualiti limit experiment platform resourc need perform full qst howev extrem mand number requir measur scale exponenti number degre freedom see pari rehacek review topic haah et donnel wright discuss hard learn state tomographi ml tool identiﬁ alreadi sever year ago tool improv upon cost full qst ing special structur densiti matrix press sens gross et one promin proach problem allow reduc number requir measur rd log densiti matrix rank r dimens ful experiment realiz thi techniqu ha exampl implement state tóth et system trap ion ofrío et methodolog side full qst ha recent seen develop deep learn approach exampl use supervis approach base neural network output full densiti matrix input possibl measur outcom xu xu problem choos optim measur basi qst ha also centli address use base approach optim prior distribut target siti matrix use bay rule quek et gener ml approach full qst serv viabl tool allevi measur requir howev provid improv intrins exponenti scale qst exponenti barrier typic overcom onli situat quantum state assum speciﬁc regular properti tomographi base paremeter densiti trix ha import ﬁrst step thi direct allow tomographi larg tum system lanyon et ml approach qst emerg recent time viabl altern especi highli gled state speciﬁc assum nq form see eq case pure state qst reformul unsupervis ml learn task scheme triev phase case pure state ha demonstr torlai et applic complex phase retriev upon reconstruct sever probabl densiti associ measur cess diﬀer basi overal thi approach ha low demonstr qst highli entangl state qubit unfeas full qst techniqu thi tomographi approach suitabl gener case mix state introduc parameter densiti matrix base either puriﬁ nq torlai melko deep normal ﬂow mer et former approach ha also demonstr experiment rydberg atom lai et interest altern nq represent tomograph purpos ha also centli suggest carrasquilla et thi base parameter densiti matrix directli term valu measur povm oper thi approach therefor ha import advantag directli learn measur process ha demonstr scale well rather larg mix state possibl inconveni thi approach densiti matrix onli implicitli deﬁn term gener model oppos explicit tion found approach approach qst explor use quantum state parameter local hamiltonian xin et intrigu sibil bypass qst directli measur quantum entangl gray et extens complex problem quantum process tomographi also promis banchi et scalabl approach larger system still present challeng final problem learn quantum state experiment measur ha also profound tion understand complex tum system thi framework pac iti quantum state aaronson talli demonstr rocchetto et shadow tomographi approach aaronson show even linearli size train set vide suﬃcient inform succeed certain quantum learn task guarante come comput restrict learn ﬁcient onli special class state rocchetto control prepar qubit central task quantum control follow given evolut u θ depend paramet θ map initi quantum state θ u θ paramet overlap distanc prepar state target state θ facilit analyt studi space possibl control intervent often discret u θ u st becom sequenc step st exampl control ﬁeld could appli onli two diﬀer strength goal ﬁnd optim strategi st bring initi state close possibl target state use onli discret action thi setup directli gener reinforc learn framework sutton barto agent pick move list allow control vention two ﬁeld strength appli quantum state qubit thi framework ha proven competit method variou ting state prepar bodi quantum system interact qubit bukov et use strong period oscil prepar state bukov recent studi compar deep reinforc learn tradit optim method stochast gradient descent prepar gle qubit state show learn advantag action space natur discret suﬃcient small zhang et pictur becom increasingli complex slightli realist set exampl control noisi niu et interest twist trol problem ha also tackl predict futur nois use recurr neural network analys time seri past nois use predict ticip futur nois correct mavadia et altogeth diﬀer approach state prepar machin learn tri ﬁnd optim strategi evapor cool creat condens wigley et thi onlin optim egi base bayesian optim frazier jone et gaussian process use statist model captur relationship trol paramet qualiti condens strategi discov machin learn model low cool protocol use time fewer tion pure optim techniqu interest featur contrari common reput machin learn gaussian process allow mine control paramet import anoth angl captur approach learn sequenc optic instrument order pare highli entangl photon quantum state nikov et error correct one major challeng build univers quantum comput error correct dure ani putat error introduc physic imperfect hardwar classic comput allow simpl error correct base duplic mation theorem quantum mechan requir complex solut propos surfac code prescrib encod one ical qubit topolog state sever physic qubit measur physic qubit reveal footprint chain error event call syndrom decod map syndrom error sequenc onc known correct appli error sequenc without aﬀect logic qubit store actual quantum inform roughli state art quantum error correct therefor predict error syndrom task natur ﬁt framework machin learn past year variou model pli quantum error correct rang vise unsupervis reinforc learn detail applic becam increasingli complex one ﬁrst propos deploy boltzmann machin train data set pair error syndrom speciﬁ probabl p error syndrom use draw sampl desir distribut p torlai melko thi simpl recip show perform certain kind error compar common benchmark relat tween syndrom error likewis learn neural network krastanov jiang maskara et varsamopoulo et ever strategi suﬀer scalabl issu space possibl decod explod data acquisit becom issu recent neural network combin concept renorm group address thi problem varsamopoulo et signiﬁc diﬀer neural network ha studi varsamopoulo et besid scalabl import problem quantum error correct syndrom measur cedur could also introduc error sinc involv pli small quantum circuit thi set increas problem complex essenti real applic nois identiﬁc error mitig repeat cycl syndrom measur consid addit time dimens recurr ral network architectur propos baireuth et anoth avenu consid decod reinforc learn problem sweke et agent choos consecut oper act physic qubit oppos logic qubit rect syndrom get reward sequenc correct error much machin learn error correct focus surfac code repres logic qubit physic qubit accord set scheme ment agent also set agnost code one could say learn code along decod strategi thi ha done quantum memori system quantum state suppos store rather manipul nautrup et well feedback control framework protect qubit decoher fösel et final beyond tradit reinforc learn novel gie project simul use combat nois tiersch et summari machin learn quantum error correct problem sever layer complex realist applic requir rather complex learn framework nevertheless veri natur candid machin learn especi ment learn vi chemistri materi machin learn approach appli dict energi properti molecul solid popular applic increas matic quantum natur atom interact make energi evalu comput expens ml method particularli use mani calcul requir recent year expand applic ml chemistri rial research includ predict structur relat molecul calcul energi surfac base molecular dynam md simul identifi structur desir materi properti creat learn densiti function type problem input descriptor must account diﬀer atom environ compact way much current work use ml atomist model base earli work describ local atom environ metri function input neural work behler parrinello repres atom potenti use gaussian process regress method bartók et use sort interatom tanc weight nuclear charg coulomb matrix molecular descriptor rupp et continu develop suitabl structur tation review behler discuss ml chemic system gener includ learn relationship found review butler et al addit focu enabl theoret chemistri review rupp et al section present recent ple ml applic chemic physic energi forc base atom environ one primari use ml chemistri rial research predict rel energi seri relat system typic compar diﬀer structur atom composit cation aim determin structur like observ experiment identifi molecul may synthesiz drug candid exampl supervis learn ml method employ iou quantum chemistri calcul label molecular represent xµ correspond energi yµ gener train test data set xµ yµ n quantum chemistri applic nn method great success predict rel energi wide rang system includ constitut isom conﬁgur molecul use symmetri function describ local atom neighborhood atom behler mani success thi area deriv thi type decomposit molecular energi element repres use separ nn behler parrinello see fig exampl deep nn potenti success train return densiti function theori dft energi ani molecul heavi atom h c n smith et thi work atom coordin train set select use normal mode sampl includ vibrat perturb along optim ometri anoth exampl gener nn ular atom system deep potenti ular dynam dpmd method speciﬁc creat run md simul train energi bulk simul zhang et rather pli includ interact via total energi system anoth approach wa inspir bodi expans use standard comput physic thi case ad layer allow interact nn improv molecular energi diction lubber et exampl abov use invari represent atom environ thank incorpor symmetri function nn input applic ing molecular reaction materi phase mation atom represent must also ou diﬀerenti smooth overlap atom posit soap kernel address ment includ similar metric atom environ bartók et recent work serv symmetri altern molecular represent address thi problem diﬀer way capit known molecular symmetri coulomb matrix put bond rigid dynam symmetri incorpor improv coverag train data conﬁgur space chmiela et thi work also includ forc train allow md simul level coupl cluster lation small molecul would tradit intract molecular symmetri also learn shown determin local environ descriptor make use convolut scribe atom interact schütt et develop atom environ descriptor compact uniqu diﬀerenti certainli tate new use ml model studi molecul materi howev machin learn ha also appli way close integr convent approach easili incorpor ing code exampl atom charg assign patibl classic forc ﬁeld learn without need run new quantum mechan calcul new molecul interest sifain et addit condens phase simul molecular speci requir accur intermolecular tial diﬃcult parameter thi end local nn potenti combin motiv coulomb van der waal bution describ larger molecular system yao et local ml descript also success combin expans method allow applic ml potenti larger system strate water cluster nguyen et tive intermolecular interact ﬁtted set ml model train monom creat transfer model dimer cluster bereau et potenti free energi surfac machin learn method also employ scribe free energi surfac rather learn figur sever represent current use describ molecular system ml model includ atom nate symmetri function encod local bond environ input neural network reproduc gastegg et b nuclear potenti approxim sum gaussian function input kernel ridg regress model electron densiti modiﬁ brockherd et tential energi molecular conform directli describ abov altern approach learn free energi surfac system function collect variabl global steinhardt order paramet local dihedr angl set atom compact ml represent free energi surfac fe use nn allow improv sampl high dimension space calcul observ depend ensembl conform exampl learn fe sampl predict isotherm compress solid xenon pressur expect nmr spin j coupl peptid schneider et small nn repres fe also train tive use data point gener adapt sampl sidki whitmer thi promis approach highlight beneﬁt use smooth sentat full conﬁgur space use ml model themselv gener new train data use fe represent creas import determin limit accuraci small nn use model start point larger network ml tectur onc relev minima identiﬁ fe next challeng understand process take system one basin anoth ampl develop markov state model describ format chang requir dimension reduct translat molecular coordin global reaction coordin space thi end power deep ing autoencod method ha ness identifi slowli chang collect variabl peptid fold exampl wehmey noé variat approach ha also use identifi import kinet process dure protein ing simul provid framework unifi coordin transform fe surfac explor mardt et promis altern approach use ml sampl conform distribut rectli boltzmann gener sampl rium distribut collect variabl space sequent provid set state repres tribut state fe noé et furthermor long histori ﬁnding relationship minima complex energi landscap may also use learn understand whi ml model hibit gener success relationship method idea current use describ molecular system correspond review ballard et go forward mani tool develop physicist explor quantifi featur energi landscap may help creat new algorithm eﬃcient optim model weight dure train see also relat discuss sec thi area interdisciplinari research promis yield method use machin learn physic ﬁeld materi properti use learn interatom potenti base local viron ha also aﬀord improv tion materi properti match experiment data typic requir sampl ensembl possibl conﬁgur come consider cost use larg simul cell convent method recent structur materi properti phou silicon predict use molecular dynam md ml potenti train densiti function theori dft calcul onli small simul cell dering et relat applic use ml potenti model phase chang crystallin amorph region materi gete amorph carbon review sosso et al gener potenti suﬃcient accur describ phase chang rel energi defect atomist terial scale quit diﬃcult howev recent success silicon properti indic ml method challeng bartók et ideal experiment measur could also corpor ml method aim dict materi properti howev report result often limit materi counter exampl train process addit noisi data coupl lack precis structur format need input ml model organ molecular crystal challeng come predict nmr chemic shift veri sensit local environ use gaussian process regress framework train valu known structur paruzzo et ing calcul valu experiment result prior train ml model enabl valid dict pharmaceut crystal structur intrigu direct includ identiﬁc structur similar materi via cluster use convex hull construct determin mani predict structur stabl certain thermodynam constraint anelli et use descriptor construct vex hull ha appli identifi crystallin ice phase wa shown cluster thousand structur fer onli proton disord stack fault engel et see fig method base combin supervis unsupervis techniqu certainli promis fruit research area futur particular remain excit challeng identifi predict even suggest materi exhibit particular desir properti electron densiti densiti function theori mani exampl abov densiti function ori calcul use sourc train data ﬁtting machin learn also play role creat new densiti function machin ing natur choic situat dft knowledg function form exact solut beneﬁt thi approach ing densiti function wa illustr approxim kinet energi function electron distribut potenti well snyder et use standard base dft code deriv ml function must also use ﬁnd priat ground state electron distribut use kernel ridg regress without modiﬁc lead noisi deriv project result energi back onto learn space use pca resolv thi sue li et approach learn potenti ha also strate system nagai et thi case ml method make direct use deriv erat dure nn train step also possibl bypass function deriv entir use ml gener appropri ground state electron densiti correspond nuclear tential brockherd et shown fig b furthermor thi work demonstr energi molecular system also learn electron siti input enabl reactiv md simul proton transfer event base dft energi ingli approxim electron densiti sum densiti isol atom ha also success employ input predict molecular energi eickenberg et relat approach odic crystallin solid use local electron densiti embed atom method train bayesian ml el return total system energi schmidt et sinc total energi extens properti scalabl nn model base summat local electron densiti ha also develop run larg tion porou graphen sheet mill et success ha becom clear given siti function machin learn oﬀer new way learn electron densiti correspond system energi mani approach improv proxim function use today reli impos constraint far includ type restrict method ha met onli partial success exampl requir ml function fulﬁll one constraint scale law improv overal formanc manner hollingsworth et obtain accur deriv larli molecul conform chang still open question ml function potenti explicitli train thi goal bereau et snyder et data set gener applic machin learn parison variou method requir standard data set quantum chemistri includ molecul data set ramakrishnan et benchmark data set compos subset small molecul peptid data set entri optim use comput method smith et chemistri materi research comput data often expens gener select ing data point must care consid put output represent also inform choic data inspect molecular energi data set show import choos input data structur convey conform chang faber et addit dens sampl chemic composit space alway sari exampl initi ani train set lion molecul could replac million ing point select use activ learn method figur cluster thousand possibl ice structur base descriptor identiﬁ observ form group similar structur togeth reproduc engel et ad poorli predict molecular exampl train cycl smith et altern sampl approach also use eﬃcient build train set rang activ learn od estim error multipl nn evalu new molecul gastegg et gener new atom conﬁgur base md simul use model zhang et est insight theoret aspect activ learn wa present seung et work thi area need identifi atom composit conﬁgur import diﬀerenti candid structur nn shown gener accur energi amount data requir prevent prohibit expens mani case speciﬁc task predict anharmon contribut bration frequenc small molecul formaldehy gaussian process method accur use fewer point nn although point need select care kamath et balanc comput cost data gener eas model train model evalu time continu import consider choos appropri ml method applic outlook challeng go forward ml model beneﬁt ing method practic develop problem physic idea alreadi explor exploit input data symmetri molecular conﬁgur still mani niti improv model train eﬃcienc izat promis challeng area includ appli method explor dimension landscap optim identifi includ boundari behavior scale law ml architectur put data format connect directli iment data futur ml method account uncertainti error calcul measur properti avoid prove transfer model vii ai acceler classic quantum hardwar area physic contribut chine learn mean tool theoret investig problem novel ware platform may help expens inform cess pipelin extend number crunch iti cpu gpu also known ai acceler physic research ha oﬀer varieti devic could potenti enhanc machin learn beyond von neumann architectur speak comput usual think versal digit comput base electr circuit boolean logic thi von neumann paradigm modern comput ani physic tem interpret way process inform name map input paramet mental setup measur result output thi way think close idea analog ing ha seem amb berg dwarf digit cousin veri applic context machin ing howev comput execut analog comput devic found new surg interest hardwar use emul full model inspir chip ambrogio et outsourc onli subroutin comput done gate array fpga integr circuit asic fast linear algebra comput jouppi et markidi et follow present select exampl variou research direct investig hardwar platform physic lab optic ic quantum comput becom novel kind ai acceler neural network run light process inform optic natur peal altern least complement comput fast made massiv parallel requir veri low power consumpt optic connect alreadi widespread carri inform short long distanc light interfer properti also leverag order provid advanc process case machin learn one perk standard build block tic lab strike resembl way mation process neural network killoran et lin et shen et insight mean new lu et exampl larg bulk optic experi ic network interferomet interferomet passiv optic element made beam splitter phase shifter clement et reck et consid amplitud light mode ing signal interferomet eﬀect appli unitari transform input see figur left fy damp amplitud understood appli diagon matrix consequ mean singular valu decomposit ampliﬁ sandwich two interferomet implement arbitrari matrix multipl data encod optic plitud ad oper alli hardest precis control lab turn devic emul standard neural network layer lin et shen et speed light interest question ask use tum instead classic light exampl imagin inform encod quadratur tromagnet ﬁeld quadratur much like sition momentum quantum particl two commut oper describ light quantum system exchang setup quantum optic compon squeezer displac get neural network encod quantum properti light killoran et use multipl layer choos nonlinear oper compon optic kerr admittedli still experiment challeng optic setup becom univers tum comput run ani comput quantum comput perform true quantum neural network variat quantum cal neural net exampl inform encod discret rather properti light steinbrech et investig quantum devic mean machin learn exampl whether pattern data easier recogn begun reveal featur data one doe implement full machin ing model physic hardwar outsourc figur illustr method discuss text optic compon interferomet ampliﬁ emul neural network map input x ϕ wx w learnabl weight matrix ϕ nonlinear activ use quantum optic compon displac squeez one encod inform quantum properti light turn neural net univers quantum comput random embed optic process unit data encod laser beam spatial light modul dmd diﬀus medium gener random featur quantum comput use comput distanc data point quantum kernel ﬁrst part quantum algorithm use routin sx emb data hilbert space second part reveal inner product embed vector thi kernel process standard kernel method support vector machin singl compon exampl highlight second applic data preprocess featur extract thi includ map data anoth space either compress blown case reveal featur machin learn algorithm one approach data compress expans physic devic leverag veri statist natur mani machin learn algorithm multipl light tere gener veri ness need random embed see figur top right nutshel multipl set vector random matrix approxim johnson lindenstrauss thi use dimension reduct data compress spirit compress sens donoho eﬃcient nearest neighbor search iti sensit hash thi also use sional expans limit larg sion approxim kernel saad et devic built optic coher laser sourc commerci light modul cmo sensor scatter materi see machin learn applic rang transfer learn deep neural network time seri analysi feedback loop implement network dong et detect keriven et data devic alreadi outperform cpu gpu speed power consumpt machin learn fair amount eﬀort ﬁeld quantum machin learn ﬁeld investig intersect tum inform intellig data mine biamont et schuld petruccion goe applic quantum hardwar ing task et noisi quantum nisq devic onli hope enhanc machin learn applic term speed may lead entir new rithm inspir quantum physic alreadi mention one exampl abov quantum neural network emul classic neural net go beyond thi model fall larger class tional parametr quantum machin learn rithm mcclean et mitarai et idea make quantum algorithm therebi devic implement quantum comput oper depend paramet θ train data measur train devic repres new put artiﬁci gener data sampl er model classiﬁc supervis classiﬁ anoth idea use quantum comput hanc learn inspir kernel method hofmann et see figur bottom right associ paramet quantum algorithm input data sampl x one eﬀect emb x tum state x vector hilbert space havlicek et schuld killoran simpl interfer routin measur overlap two quantum state prepar thi way overlap inner product vector hilbert space machin literatur known kernel distanc measur two data point result quantum comput comput rather exot kernel may classic intract activ area search ﬁnd interest quantum kernel machin learn task beyond quantum kernel variat circuit quantum machin learn present mani idea use quantum hardwar ai acceler ple sampler train infer graphic model adachi henderson benedetti et linear algebra comput lloyd et anoth interest branch research gate quantum devic directli analyz data produc quantum experi without make detour measur cong et explor major challeng still sever tion nisq devic reduc ical experi hardwar demonstr theoret analysi remain riousli diﬃcult machin learn outlook challeng abov exampl demonstr way physic research contribut machin learn name investig new hardwar platform execut tiresom comput standard von neumann gie struggl keep pace moor law thi open number opportun novel comput paradigm simplest embodi take form special acceler devic plug onto standard server access custom api futur search focus hardwar iti innov machin learn adapt program languag well er optim distribut comput task hybrid server mani quantum machin learn algorithm base linear gebra acceler recent shown make unfound claim exponenti speedup tang compar classic algorithm analys dataset strong sampl access howev still interest thi context even constant speedup make diﬀer viii conclus outlook number overarch theme becom appar ter review way machin learn use ha enhanc diﬀer disciplin physic first clear interest machin ing techniqu suddenli surg recent year thi true even area statist physic energi physic connect machin learn techniqu ha long histori see research move exploratori eﬀort toy model toward use real experiment data also see evolut understand limit approach situat perform justiﬁ theoret healthi critic ment potenti power limit chine learn includ analysi od break distinctli good physicist notori hungri veri detail derstand whi method work machin learn incorpor physicist box reason expect physicist may shed light notori diﬃcult question chine learn face speciﬁc physicist readi contribut issu interpret niqu valid guarante result ple way chose variou paramet neural network architectur one direct physic commun ha much learn machin learn commun cultur practic share code develop benchmark dataset thermor physic would well emul practic develop portabl implement key method ideal involv fession softwar engin pictur emerg level activ enthusiasm surround ﬁrst success stori interact machin learn physic scienc mere infanc ticip excit result stem thi play machin learn physic scienc acknowledg thi research wa support part tional scienc foundat grant nsf us armi research oﬃc number well erc european union horizon research innov programm grant agreement smile addit would like thank support moor sloan foundat kavli institut theoret physic ucsb institut vanc studi final would like thank michel ceriotti yoav levin andrea rocchetto mile mire ryan sweke refer aaronson proceed royal societi mathemat physic engin scienc aaronson arxiv acar yener ieee transact edg data engin acciarri et al microboon jinst adachi henderson arxiv preprint advani sax arxiv preprint agresti viggianiello flamini spagnolo crespi osellam wieb sciarrino physic review x albergo kanwar shanahan phi rev albertsson et al proceed intern workshop advanc comput analysi niqu physic research acat seattl wa usa august phi conf ser alet laﬂorenci compt rendu physiqu quantum simul simul quantiqu als wandelt als wandelt feeney mon roy astron soc amari neural comput ambrogio narayanan tsai shelbi boybat nolfo sidler giordano bodini farinha et al natur amb advanc optic technolog amit gutfreund sompolinski ical review anandkumar ge hsu kakad garski journal machin learn research anelli engel pickard ceriotti phi rev mater apollinari brüning nakamoto rossi cern yellow report armitag kay barn mnra arsenault neuberg hannah milli invers problem arunachalam de wolf acm sigact news atla collabor deep gener model fast shower simul atla tech cern geneva aubin maillard krzakala macri zdeborová et al advanc neural inform process system pp preprint aurisano radov rocco himmel messier niner pawloski psiha sousa vahl jinst baireuth brien tarasinski beenakk quantum sagun geiger spigler arou cammarota lecun wyart biroli icml arxiv preprint baldassi borg chay ingrosso lucibello saglietti zecchina proceed nation academi scienc baldassi ingrosso lucibello saglietti zecchina physic review letter baldi bauer eng sadowski whiteson physic review baldi cranmer faucett sadowski whiteson eur phi baldi sadowski whiteson natur mun ball et al nnpdf jhep ballard da martiniani mehta sagun stevenson wale phi chem chem phi banchi grant rocchetto severini new journal physic bang ryu yoo pawłowski lee new journal physic barbier dia macri krzakala lesieur zdeborová advanc neural inform process system pp barbier krzakala macri miolan borová proceed nation academi enc barkai sompolinski physic review e barra genoves sollich tantari physic review e bartók kermod bernstein csányi physic review x bartók kondor csányi phi rev b bartók payn kondor csányi phi rev lett baydin heinrich bhimji loupp shao prabhat cranmer wood beach golubeva melko ical review b beaumont zhang bald netic becca sorella quantum mont carlo approach correl system cambridg univers press cambridg unit kingdom new york ny behler j chem phi behler parrinello phi rev lett benedetti biswa ortiz physic review x benítez astrophi j arxiv béni iclr arxiv preprint bereau distasio tkatchenko von lilienfeld chem phi biamont wittek pancotti rebentrost wieb lloyd natur biehl mietzner epl europhys ter bishop pattern recognit machin ing springer bohrdt chiu ji xu greif greiner demler grusdt knap bolthausen commun mathemat physic bonnett et al de phi rev borin abanin mat physic bozson cowan spanò bradd bialek journal statist physic brammer van dokkum coppi astrophi j brehmer cranmer loupp pavez phi rev brehmer cranmer loupp pavez phi rev lett ph brehmer loupp pavez cranmer breiman friedman olshen stone brockherd vogt li tuckerman burk müller nat commun broecker assaad trebst broecker carrasquilla melko trebst scientiﬁc report bronstein bruna lecun szlam vandergheynst ieee sig proc mag bukov physic review b bukov day sel weinberg polkovnikov mehta physic review x butler davi cartwright isayev walsh natur cai liu physic review b cameron pettitt mnra cariﬁo halverson krioukov nelson jhep carleo becca schiro fabrizio scientiﬁc report carleo nomura imada natur munic carleo troyer scienc carrasco kind brunner mnra carrasquilla melko natur physic carrasquilla torlai melko aolita natur machin intellig casado et al changlani kinder umrigar chan physic review b charnock lavaux wandelt phi rev chaudhari choromanska soatto lecun dassi borg chay sagun zecchina iclr arxiv preprint chen xu liu batrouni scalettar meng physic review b chen cheng xie wang xiang physic review b cheng wang xiang zhang physic stat chmiela sauceda müller tkatchenko nat commun choma monti gerhardt palczewski ronaghi prabhat bhimji bronstein klein bruna arxiv choo carleo regnault neupert physic review letter choromanska henaﬀ mathieu arou lecun artiﬁci intellig statist pp chung lee sompolinski physic review x ciliberto herbster ialongo pontil chetto severini wossnig proceed royal societi mathemat physic neer scienc clark journal physic mathemat theoret clement humphrey metcalf kolthamm walmsley optica cocco monasson posani rosay tubiana physica statist mechan tion cohen sharir shashua nual confer learn theori proceed chine learn research vol edit feldman rakhlin shamir pmlr columbia univers new york new york usa pp cohen well intern confer machin learn pp cohen geiger köhler well proceed intern enc learn represent iclr arxiv preprint cohen weiler kicanaoglu well arxiv krzakala perkin zdeborová advanc mathemat collett astrophys journal collist lahav public astronom societi paciﬁc arxiv cong choi lukin arxiv preprint cranmer golkar pappadopulo physic physic physic ph stat arxiv cranmer loupp brief idea cranmer pavez loupp cristoforetti jurman nardelli furlanello physic cubuk schoenholz rieser malon rottler durian kaxira liu physic review letter cybenko mathemat control signal tem czischek gärttner gasenz physic review b dean journal physic mathemat gener decel fissor furtlehn epl rophys letter decel krzakala moor zdeborová physic review e decel krzakala moor zdeborová physic review letter deng li da sarma physic view b deng li da sarma physic view x dering bernstein bartók cliﬀe kerber marbella grey elliott csányi phi chem lett deshpand montanari ieee ternat symposium inform theori dirac mathemat proceed cambridg philosoph societi doggen schindler tikhonov mirlin neupert polyakov gornyi physic review b dong gigan krzakala wainrib ieee statist signal process workshop ssp ieee pp donoho ieee transact inform ori duart et al jinst dunjko briegel report progress physic duvenaud lloyd gross tenenbaum zoubin intern confer machin learn pp eickenberg exarchaki hirn mallat thiri chem phi engel van den broeck statist ic learn cambridg univers press engel anelli ceriotti pickard need nat commun estrada anni diehl hall la lin makler merritt scarpin lam tucker astrophi j faber hutchison huang gilmer holz dahl vinyal kearn riley von lilienfeld chem theori comput fabiani mentink scipost physic farrel et al intern workshop nect dot seattl washington usa march feldmann carollo porciani lilli pak taniguchi le fèvre renzini ill ajiki aussel contini mccracken mobash murayama sander sasaki lata scodeggio shioya silverman takahashi thompson zamorani mnra arxiv firth lahav somervil mnra arxiv hogg lang goodman fort garrido latorr piccion jhep arxiv fortunato physic report fournier wang yazyev wu physic physic frate cranmer kalia whiteson frazier arxiv preprint frenkel wave mechan advanc gener theori intern seri monograph nuclear energi reactor design physic clarendon press freund schapir comput syst sci fösel tighineanu weiss marquardt physic review x gabrié manoel luneau macri kala zdeborová et al advanc neural inform process system pp preprint gabrié tramel krzakala vanc neural inform process system pp gao duan natur commun gardner epl europhys letter gardner journal physic mathemat gener gardner derrida journal physic mathemat gener gastegg behler marquetand chem sci gendiar nishino physic review e glasser pancotti august rodriguez cirac physic review x glasser pancotti cirac arxiv preprint gligorov william jinst goldt advani sax krzakala borová arxiv preprint goldt advani sax krzakala borová arxiv preprint golkar cranmer goodfellow bengio courvil deep learn mit press goodfellow mirza xu farley ozair courvil bengio vanc neural inform process system pp gorodetski karaman marzouk puter method appli mechan engin gray banchi bayat bose physic review letter greitemann liu pollet et al physic view b gross liu flammia becker eisert physic review letter guest collado baldi hsu urban whiteson phi rev guest cranmer whiteson ann rev nucl part sci guo jie lu poletti physic view e györgyi physic review györgyi tishbi theumann kobrel editor neural network spin glass haah harrow ji wu yu ieee transact inform theori hackbusch kühn journal fourier ysi applic han zhang e physic han wang fan wang zhang physic review x hartmann carleo physic hashimoto sugishita tanaka tomiya phi rev hashimoto sugishita tanaka tomiya phi rev havlicek córcole temm harrow chow gambetta arxiv preprint li feng ho ravanbakhsh chen póczo arxiv henson kay barn mccarthi schay jenkin monthli notic royal astronom societi http herman begi loupp arxiv hezaveh perreault levasseur marshal natur hinton neural comput ho rau ntampaka farahi trac poczo hochreit schmidhub neural tion hofmann schölkopf smola annal statist hollingsworth baker burk chem phi hopﬁeld j proceed nation academi scienc hsu li deng da sarma physic review letter hu singh scalettar physic review e huang wang physic review b huang moor mat huemb dauphin wittek physic review b huemb dauphin wittek gogolin physic arxiv iakovlev sotnikov mazurenko physic review b ilten william yang jinst ishida vitenti cisewski de souza trindad cameron busti coin collabor astronomi comput izmailov novikov kropotov cs stat jacot gabriel hongler advanc neural inform process system pp jaeger haa scienc jain srijith desai javanmard montanari inform infer journal ima johnson lindenstrauss contemporari mathemat johnston lu journal ican statist associ jone schonlau welch journal global optim jouppi young patil patterson agraw bajwa bate bhatia boden borcher et al comput architectur isca annual intern symposium ieee pp jónsson bauer carleo physic physic physic kabashima krzakala mézard sakata zdeborová ieee transact inform theori kalantr zwolak ragol wu merman stewart taylor npj tum inform kamath krem rington jr manzho chem phi kashiwa kikuchi tomiya progress theoret experiment physic kasieczka plehn butter debnath fairbairn fedorko gay gousko komisk leiss et al kaubruegg pastori budich ical review b keriven garreau poli arxiv preprint killoran bromley arrazola schuld quesada lloyd quantum neural network kingma well arxiv preprint ringel natur physic kochkov clark physic physic komisk metodiev nachman schwartz phi rev komisk metodiev thaler jhep komisk metodiev thaler jhep kondor corr kondor lin trivedi neurip kondor trivedi intern enc machin learn pp krastanov jiang scientiﬁc report krenn malik fickler lapkiewicz zeiling physic review letter krzakala mézard zdeborová format theori proceed isit ieee tional symposium ieee pp krzakala moor mossel neeman sli borová zhang proceed nation academi scienc lang hogg mykytyn tractor probabilist astronom sourc detect ment astrophys sourc code librari lanuss li collett li bakhsh mandelbaum póczo mnra lanyon maier holzäpfel baumgratz hempel jurcev dhand buyskikh daley cramer plenio blatt roo natur physic advanc onlin tion larkoski moult nachman larochel murray proceed fourteenth intern confer artiﬁci genc statist pp le baydin wood artiﬁci tellig statist pp lecun bengio hinton natur lee bahri novak schoenholz pennington icrl leistedt hogg wechsler deros arxiv lelarg miolan probabl theori relat field levin sharir cohen shashua physic review letter levin yakira cohen shashua iclr arxiv li snyder pelaschi huang ranjan duncan rupp müller burk int quantum chem li gladder rangel florian bleem heitmann habib fasel astrophys journal li wang phi rev lett liang liu lin guo zhang physic review b likhomanenko ilten khairullin rogozhnikov ustyuzhanin william proceed intern confer comput high ergi nuclear physic chep okinawa japan april phi conf ser lin rivenson yardimci veli luo rahi ozcan scienc liu ran wittek peng garcía su lewenstein physic physic physic stat arxiv liu qi meng fu physic review b liu shen qi meng fu physic review b liu greitemann pollet et al physic view b liu zhang lewenstein ran physic stat arxiv lloyd mohseni rebentrost natur physic loupp cho becot cranmer loupp herman cranmer loupp kagan cranmer lu gao duan physic arxiv lu wu xu franci appli optic lubber smith barro chem phi lundberg ieee control system luo clark mat physic physic arxiv mannelli biroli cammarota krzakala urbani zdeborová arxiv preprint mannelli krzakala urbani zdeborová arxiv preprint mardt pasquali wu noé nat commun marin pudlo robert ryder statist comput marjoram molitor plagnol tavaré proceed nation academi scienc markidi der chien laur peng vetter ieee intern parallel distribut process symposium workshop ipdpsw marshal hogg moustaka nacht bradač schrabback blandford astrophys journal martiniani chaikin levin phi rev x maskara kubica connor physic review matsushita tanaka advanc neural inform process system pp mavadia frey sastrawan dona cuk natur commun mcclean romero babbush guzik new journal physic mehta bukov wang day richardson fisher schwab arxiv preprint mehta schwab arxiv preprint mei montanari nguyen arxiv preprint melnikov nautrup krenn dunjko tiersch zeiling briegel ing nation academi scienc metodiev nachman thaler jhep mézard physic review e mézard montanari inform physic comput oxford univers press mezzacapo schuch boninsegni cirac new journal physic mill ryczko luchak domurad beeler tamblyn chem sci minski papert perceptron tion comput geometri mit press cambridg usa mitarai negoro kitagawa fujii arxiv preprint morningstar melko journal machin learn research morningstar hezaveh perreault asseur blandford marshal putzki wechsler arxiv morningstar perreault levasseur veh blandford marshal putzki rueter wechsler well arxiv nagai akashi sasaki tsuneyuki chem phi nagai shen qi liu fu physic review b nagi savona physic nautrup delfoss dunjko briegel frii arxiv preprint ng jordan weiss advanc neural inform process system pp nguyen zecchina berg advanc physic nguyen székeli imbalzano behler csányi ceriotti götz paesani chem phi nielsen chuang quantum tion quantum inform van nieuwenburg bairey refael ical review b nishimori statist physic spin glass inform process introduct vol press niu boixo smelyanskiy neven arxiv preprint noé olsson köhler wu scienc nomura darmawan yamaji imada physic review b novikov troﬁmov oseledet arxiv ntampaka trac sutherland battaglia póczo schneider astrophi j ntampaka trac sutherland fromenteau póczo schneider astrophi j ntampaka zuhon eisenstein nagai vikhlinin hernquist marinacci nelson pakmor pillepich torrey vogelsberg arxiv ntampaka et al nussinov ronhovd hu chakrabarti sun mauro sahu inform enc materi discoveri design springer pp donnel wright proceed annual acm symposium theori pute acm pp ohtsuki ohtsuki journal physic societi japan ohtsuki ohtsuki journal physic societi japan de oliveira kagan mackey nachman schwartzman jhep oseledet siam journal scientiﬁc comput paganini de oliveira nachman phi rev lett paganini de oliveira nachman phi rev pang zhou su petersen stöcker wang natur commun papamakario murray pavlak advanc neural inform process system pp papamakario sterratt murray arxiv pari rehacek ed quantum state mation lectur note physic berlin heidelberg paruzzo hofstett musil de ceriotti emsley nat commun pastori kaubruegg budich physic physic physic arxiv pathak hunt girvan lu ott physic review letter pathak lu hunt girvan ott chao interdisciplinari journal nonlinear scienc peel laland starck pettorino merten giocoli meneghetti baldi arxiv print benedetti biswa arxiv preprint póczo xiong sutherland schneider corr putzki well arxiv preprint quek fort ng arxiv radov william rousseau kagan corsi himmel aurisano terao rad natur ramakrishnan dral rupp von lilienfeld sci data rangan fletcher inform theori proceed isit ieee intern symposium ieee pp ravanbakhsh lanuss mandelbaum der poczo arxiv ravanbakhsh oliva fromenteau price ho schneider poczo arxiv reck zeiling bernstein bertani physic review letter reddi celani sejnowski vergassola proceed nation academi scienc reddi celani sejnowski vergassola natur regier miller schlegel adam mcauliﬀ prabhat arxiv rem käming tarnowski asteria fläschner becker sengstock berg physic ph arxiv ren girshick sun advanc neural inform process system pp rezend moham intern ferenc machin learn pp rezend moham wierstra proceed intern confer ternat confer machin jmlr org pp riofrío gross flammia monz nigg blatt eisert natur commun ritzmann von malottki kim heinz sinova dupé natur electron robin reylé fliri czekaj robert martin astronomi astrophys rocchetto quantum inform tion rocchetto aaronson severini carvacho poderini agresti bentivegna sciarrino arxiv rocchetto grant strelchuk carleo erini npj quantum inform rodríguez kacprzak lucchi amara sgier fluri hofmann réfrégier comput astrophys cosmolog scheurer arxiv roe yang zhu liu stancu mcgregor nucl instrum meth arxiv physic rogozhnikov bukva gligorov ustyuzhanin william jinst ronhovd chakrabarti hu sahu sahu kelton mauro nussinov pean physic journal e rotskoﬀ advanc neural inform process system pp rupp von lilienfeld burk chem phi rupp tkatchenko müller von lilienfeld phi rev lett saad solla physic review letter saad solla physic review e saad caltagiron carron daudet drémeau gigan krzakala acoust speech signal process icassp ieee intern confer ieee pp saad krzakala zdeborová advanc neural inform process system pp saito journal physic societi japan saito journal physic societi japan saito kato journal physic societi japan sakata kabashima epl europhys letter sax bansal dapello advani kolchinski tracey cox sax mcclelland ganguli iclr arxiv preprint schawinski zhang zhang fowler santhanam monthli notic royal nomic societi letter schindler regnault neupert physic review b schmidhub j corr schmidt fowler elliott bristow comput mater sci schmitt heyl scipost physic schneider dai topper tuckerman phi rev lett schoenholz cubuk kaxira liu proceed nation academi scienc schuch wolf verstraet cirac physic review letter schuld killoran arxiv preprint schuld petruccion quantum comput supervis learn springer schuld petruccion supervis learn quantum comput springer schütt sauceda kinderman tkatchenko müller chem phi schwarz journal physic mathemat gener seif landsman link figgatt roe hafezi journal physic b atom molecular optic physic arxiv seung sompolinski tishbi physic review seung opper sompolinski proceed ﬁfth annual workshop comput learn theori acm pp shanahan trewartha detmold phi rev sharir levin wie carleo shashua shen georg huerta zhao shen harri skirlo prabhu hochberg sun zhao larochel englund et al natur photon shi duan vidal physic review shimmin sadowski baldi weik whiteson goul søgaard phi rev tishbi arxiv preprint sidki whitmer chem phi sifain lubber nebgen smith lokhov isayev roitberg barro tiak phi chem lett sisson fan mcmc chapman new sisson fan tanaka proceed nation academi scienc smith isayev roitberg chem sci smith nebgen lubber isayev roitberg chem phi smolenski chap inform process namic system foundat harmoni theori mit press cambridg usa pp snyder rupp hansen müller burk phi rev lett sompolinski tishbi seung physic review letter sorella physic review letter sosso dering elliott csányi mol simulat steinbrech olson englund carolan quantum optic neural network steven william jinst stoke terilla arxiv preprint stoudenmir schwab advanc neural inform process system edit lee sugiyama luxburg guyon nett curran associ pp stoudenmir quantum scienc technolog sun yi zhang shen zhai ical review b sutton barto reinforc ing introduct mit press sweke kesselr van nieuwenburg eisert arxiv preprint tanaka tomiya journal physic societi japan tanaka tomiya lat tang arxiv preprint teng physic review e thouless anderson palmer philosoph magazin tiersch ganahl briegel scientiﬁc report tishbi pereira bialek arxiv preprint tishbi zaslavski inform theori workshop itw ieee ieee pp torlai mazzola carrasquilla troyer melko carleo natur physic torlai melko physic review letter torlai melko physic review letter torlai timar van nieuwenburg levin omran keesl bernien greiner vuletić lukin melko endr physic arxiv tóth wieczorek gross krischek mer weinfurt physic review letter tramel gabrié manoel caltagiron krzakala physic review x tsari et al proceed intern shop advanc comput analysi techniqu physic research acat seattl wa usa august phi conf ser tubiana cocco monasson arxiv preprint tubiana monasson physic review letter uria côté gregor murray larochel journal machin learn research valiant commun acm van nieuwenburg liu huber natur physic varsamopoulo bertel almudev arxiv preprint varsamopoulo bertel almudev arxiv preprint varsamopoulo criger bertel tum scienc technolog venderley khemani kim physic review letter verstraet murg cirac advanc physic vicentini biella regnault ciuti physic arxiv vidal physic review letter von luxburg u statist comput wang hu lu arxiv preprint wang zhai physic review b wang zhai frontier physic wang physic review b wang gener model physicist watkin nadal journal physic mathemat gener wecker hast troyer physic review wehmey noé chem phi wetzel j physic review e white physic review letter wigley everitt van den hengel bastian sooriyabandara mcdonald hardman quinlivan manju kuhn petersen luiten hope robin hush scientiﬁc report wu wang zhang arxiv preprint xin lu cao anikeeva lu li long zeng arxiv xu xu arxiv yao herr toth mckintyr parkhil chem sci yedidia freeman weiss ing artiﬁci intellig new millennium yoon sim han physic review b yoshioka hamazaki physic zdeborová krzakala advanc physic zhang bengio hardt recht vinyal arxiv preprint zhang han wang car e phi rev lett zhang lin wang car et al arxiv preprint zhang shen zhai physic review letter zhang wang wang physic review b zhang wang zhang sun contardo ho arxiv zhang wei asad yang wang arxiv preprint zhang kim physic review letter zhang melko kim physic review b zhang mesaro fujita edkin hamidian ch ng eisaki uchida davi khatami kim physic physic zheng regnault bernevig