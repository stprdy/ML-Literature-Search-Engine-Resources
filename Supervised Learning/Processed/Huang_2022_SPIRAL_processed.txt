published conference paper iclr 2022 spiral invariant representation learning speech wenyong huang zhenhe zhang yu ting yeung xin jiang qun liu huawei noah ark lab abstract introduce new approach speech named spiral work learning denoising representation perturbed data work speciﬁcally given speech utterance ﬁrst feed utterance teacher network obtain corresponding representation ance perturbed fed student network student network trained output representation resembling teacher time teacher network updated moving average student weight training step order prevent representation collapse apply contrastive loss objective impose position randomization input teacher spiral achieves competitive better result compared art speech method signiﬁcant reduction training cost 80 base model 65 large model furthermore address problem critical speech application propose perturbing student input various type additive noise demonstrate spiral model robust noisy speech relative word ror rate reduction real noisy test data compared applying training solely stage source code available 1 1 introduction automatic speech recognition asr system usually trained hour speech data galvez et 2021 however labeling speech data pensive especially language small speaker population speciﬁc domain legal ﬁnancial scientiﬁc recently method utilizing unlabeled speech data improve speech recognition system achieved remarkable progress amongst manohar et 2015 kahn et synnaeve et chen et xu et 2020 park et xiao et 2021 also known start training asr model labeled speech data referred teacher model teacher model usually combined language model lm used produce unlabeled speech data finally labeled data data combined train new model referred student model process repeated taking student model teacher next iteration another line work speech van den oord et 2019 chung glass 2020 wang et 2020 baevski et liu et 2020 learns speech representation beled data way model labeled data complementary shown recent work xu et 2021 zhang et 2020 paper introduce new speech method work learning ing representation perturbed data framework named 1 1 6 mar 2022 published conference paper iclr 2022 representation learning spiral compared speech training method baevski et hubert hsu et 2021 method allows training single contrastive loss without relying discrete unit discovery technique vector quantization egou et 2011 baevski et b ative clustering process hsu et 2021 apply training spiral seltzer et 2013 ko et 2015 improve downstream speech task spiral motivated observation human tolerates speech perturbation distortion fairly well example people communicate effectively noisy environment distorted telephone channel therefore hypothesize learning representation invariant perturbation model learn representation enhance speech application learn representation way employ framework similar tarvainen valpola 2017 given speech utterance guide student network consumes perturbed utterance learn teacher network consumes clean utterance student trained produce denoised representation perturbed utterance similar teacher representation clean utterance meanwhile teacher share model architecture student updated moving average student weight past training step apply contrastive loss avoid model collapse trivial constant tion chopra et 2005 speech utterance sequential data another possible trivial solution call positional collapse positional collapse occurs student cheat exploiting position correlation teacher representation minimize loss ignoring content input utterance prevent positional collapse propose position randomization adding random number padding side input utterance teacher speech computationally demanding reduce computation cost adopt gradual strategy spiral model ha veriﬁed effective speech recognition literature negligible performance degradation peddinti et 2018 han et huang et 2020 also speculate aggressive help remove redundancy speech evaluate effectiveness spiral conduct experiment librispeech datasets training small convolutional classiﬁer representation frozen spiral model achieve wer librispeech respectively spiral achieves competitive better result compared speech method much also demonstrate trained spiral robust noisy speech relative word error rate wer reduction real noisy test data barker et 2015 compared model ing training solely stage 2 related work mean teacher mt tarvainen valpola 2017 proposes using student network learn teacher network moving average version student learning setting author apply supervised loss labeled data consistency loss teacher student prediction unlabeled data however direct application mt learning lead representation collapse grill et 2020 noisy student training nst xie et 2020 park et method nst demonstrates importance aggressive injection noise student although not emphasized no noise injected process teacher consider work extension approach learning regime instead using teacher provide utilize teacher representation denoising autoencoders vincent et 2008 learn recover clean input corrupted sion however speech data contain redundancy irrelevant some speech application speech recognition previous work baevski et 2019 show speech recovering masked input speech feature not effective spiral instead enforce latent representation corrupted input resemble corresponding clean input 2 published conference paper iclr 2022 bootstrap latent byol grill et 2020 image representation learning method method based framework similar mt author refer student network online network teacher network target network observe naive application mt learning lead trivial constant representation prevent representation collapse appending predictor student network theory behind investigation chen 2021 tian et 2021 method draw inspiration byol share similar architecture crucial difference instead learning single global representation image byol spiral learns sequence representation utterance aim sequence application speech recognition preliminary experiment observe appending predictor student network not sufﬁcient prevent trivial constant representation sequential representation learning use contrastive loss baevski et combined input position randomization successfully avoid representation collapse still keep predictor spiral only sake performance improvement observation another difference byol doe not perform representation denoising byol applies perturbation call augmentation input teacher student demonstrate representation denoising crucial speech perturbation applied teacher input effectiveness speech degrades drastically baevski et speech representation learning method belongs masked prediction family masked prediction method effective text devlin et 2019 not speech naively applied baevski et 2019 reason speech data contains redundancy speaker information nunciation variation irrelevant semantic meaning utterance overcome problem perform masking intermediate latent space performs target cretization differentiable quantization scheme however quantization lead plex model introducing additional additional diversity loss spiral doe not utilize quantization still achieves competitive performance compared hypothesize aggressive learning matching output representation may help remove redundancy learned representation leave investigation whether target discretization could improve spiral future work liang et al 2018 demonstrates supervised learning setting enforcing representation penalizing difference clean noisy data improves asr model accuracy 3 method representation learning spiral figure 1 show diagram spiral stage use two neural network student fθ teacher weight teacher moving average weight student step weight teacher updated 1 θt 1 αt determines rate weight update given speech utterance x xt length student take perturbed version x x xt input perturbation function output student representation sequence z f x θ zt teacher take utterance without perturbation input output representation sequence f x 1 representation zi student trained match teacher representation position amongst k tracting sample distracting sample randomly drawn position utterance found effective sample drawn entire batch terances baevski et contrastive loss deﬁned following sohn 2016 wu et al 2018 l x log exp φ zi p exp φ zi j 2 3 published conference paper iclr 2022 student teacher contrastive loss perturbation moving average random positional padding 𝑧𝑇 𝑧𝑇 gradient 𝑥𝑇 𝑥𝑇 𝜃 figure 1 illustration spiral architecture speech φ b cosine similarity di set index distractors position κ temperature parameter however applying contrastive loss could cause kind representation collapse refer positional collapse contrastive candidate sampled based position utterance teacher representation correlated position correlation introduced positional encoding transformer student could exploit correlation erate representation zi solely based position index ignoring content input case model doe not learn meaningful representation input content therefore prevent positional collapse randomizing position teacher representation particular add random number padding data end input teacher randomly shift position information output representation student thereby unable exploit spurious position information minimize contrastive loss note calculating contrastive loss exclude corresponding representation padded data model architecture transformer layer transformer layer projection head predictor subsampling convolution subsampling convolution encoder rate rate rate 𝑓 𝑔 𝑞 figure 2 architecture student model spiral frame rate input denoted dashed line indicates optional predictor removed small performance degradation structure teacher model without predictor illustrated figure 2 student fθ composed encoder f projection head g chen et optional predictor q grill et 2020 fθ f θ teacher ha structure expect ha no predictor encoder consists two block block ﬁrst apply temporal convolution perform followed transformer vaswani et 2017 convolutional relative position encoding baevski et 4 published conference paper iclr 2022 convolution followed layer normalization ln ba et 2016 relu projection head apply simple linear layer predictor consists two layer temporal convolution linear layer convolution followed batch normalization bn ioffe szegedy 2015 relu add computation noise student teacher applying dropout srivastava et 2014 layerdrop fan et 2020 transformer use dropout layerdrop rate student teacher adaptive specaugment apply adaptive specaugment similar park et al primary perturbation method along either time frequency dimension sample uniformly certain proportion p step start index mask subsequent consecutive l masked ﬁlled zero along frequency dimension along time dimension use gaussian noise masking value avoid numerical problem ln park et spiral perturb input student various type additive noise consider technique implementation training mct seltzer et 2013 setting speciﬁcally input utterance student sample noise clip noise dataset mix noise clip whole utterance addition ﬁrst uniformly sample ratio snr deﬁned range utterance scale noise volume according required snr preliminary experiment found applying additive noise alone perturbation degrades performance therefore apply additive noise perturbation together adaptive specaugment model take encoder student model spiral add randomly initialized convolutional classiﬁer top convolutional classiﬁer composed two layer convolution followed ln relu linear output projection convolution ﬁlters consist 512 channel kernel width model connectionist temporal classiﬁcation ctc graf et 2006 jective speech recognition use 1024 subwords output unit generated training transcript librispeech sentencepiece kudo richardson 2018 investigate spiral ability learn representation speech training addition apply frozen freeze trained parameter only convolutional classiﬁer only perform local siﬁcation due limited receptive ﬁeld 4 experimental setup data use training data ignoring label librispeech otov et 2015 unlabeled audio data kahn et segment data using ofﬁcial tool threshold resulting hour data two datasets derived english audiobooks librivox asr apply subset labeled data entire label labeled data librispeech training use noise dataset reddy et al 2021 dataset consists 181 hour noise data 150 noise type clip shufﬂe split noise data ratio used training synthesizing noisy synthetic noisy 2 5 published conference paper iclr 2022 table 1 detailed conﬁgurations spiral base large model module proj predictor params hyper layer layer kernel size emb dim kernel size emb dim kernel size channel ffn dim channel ffn dim dim channel stride layerdrop stride layerdrop attn head attn head base model 2 10 512 768 2048 3072 256 0 8 12 large model 4 20 512 1024 2048 4096 512 8 16 table 2 comparison cost spiral model unlabeled data training step gpu day mixed precision base baevski et spiral base large baevski et spiral large result appendix respectively snrs speech mixture set 0 30 db evaluate real noisy data test set barker et 2015 comprised speech data recorded real noisy environment bus cafe pedestrian area street junction data recorded microphone array composed multiple microphone channel located different position tablet microphone training setup apply ﬁlterbank extracted 20 window 10 stride input acoustic feature experiment base model large model conﬁgurations shown table number parameter comparable base large model correspondingly specaugment set p l 20 mask p l 20 mask optimize adam kingma ba 2015 optimizer warming ing rate ﬁrst 8 update peak learning rate decay 0 cosine schedule moving average update rate αt teacher weight also follows cosine schedule grill et 2020 increase αt base large model respectively train base model batch size 24 per gpu step 16 gpus take day large model train batch size 20 per gpu step 32 gpus take day shown table 2 signiﬁcant reduction training cost gpu day compared baevski et spiral requires 80 65 le training cost base large respectively note training not applied spiral yet optimize adam rate schedule learning rate warmed ﬁrst 10 update held constant next 40 linearly decayed zero following baevski et al base large batch size 14 18 per gpu respectively 8 gpus step large batch size 10 per gpu 16 gpus step apply specaugment not frozen 6 published conference paper iclr 2022 table 3 asr result language model used coding listed lm compare spiral base spiral large previous method report wer librispeech set model unlabeled lm dev test data clean clean hybrid uscher et 2019 iter xu et 2020 noisy student park et lstm base baevski et spiral base frozen spiral base base baevski et spiral base base baevski et transf spiral base transf large baevski et spiral large frozen spiral large large baevski et transf spiral large transf randomly perturb utterance additive noise 50 probability applying specaugment snr uniformly sampled db language model decoding use transformer lm baevski auli 2019 trained librispeech lm corpus identical synnaeve et al asr setting also evaluate spiral base ofﬁcial librispeech lm observe model subword unit performs worse model character unit decoding lm therefore apply model lm decoding setting result lm decoding model available appendix output frame rate spiral encoder low output sequence may short character unit reuse encoder devise upsampling strategy spiral encoder output stage apply convolution layer project original encoder output dimension vector dimension reshape projected output vector 1 4 frame rate becomes feed upsampled output convolutional classiﬁer perform random search decoding parameter choose best parameter according performance beam ﬁnal test performance measured beam use beam search decoder pratap et al 2019 5 result evaluation labeled data setting ﬁrst evaluate method asr setting model librispeech data result shown table evaluate base model librispeech large model frozen base model performs well achieving wer 7 published conference paper iclr 2022 table 4 asr result language model used decoding listed lm compare spiral large previous method report wer librispeech set model unlabeled lm dev test data clean clean supervised contextnet han et lstm conformer gulati et 2020 lstm ctc transf pl synnaeve et transf pl synnaeve et iter xu et al 2020 noisy student park et lstm large baevski et spiral large frozen spiral large large baevski et transf spiral large transf par base suggests spiral indeed learns meaningful representation way whole base model model achieves wer respectively forming base relative wer reduction decoding transformer lm base model achieves wer respectively result par base spiral large model consists parameter data model achieves wer respectively signiﬁcant improvement large base demonstrates scalability spiral result spiral large competitive large encouraging spiral large only take 35 training cost large evaluate spiral large asr setting data shown table 4 large model achieves wer respectively par large model note supervised model noisy student model park et table 4 autoregressive model model ctc objective generally inferior autoregressive model use ctc objective simplicity comparability previous speech method consider spiral preferred alternative given spiral only requires 20 computation cost expect efﬁciency improvement implement training spiral evaluate model compare effect applying condition training mct stage spiral result shown table vanilla spiral base model base model deteriorate cantly higher wer noisy test data real noisy test speech data different microphone channel ch spiral signiﬁcantly improves speech recognition performance compared model applying mct solely applying mct achieves relative wer reduction ch 1 5 2 respectively smaller performance improvement relative wer reduction ch 0 talking microphone highest snr note ch 2 face backwards speaker snr 8 published conference paper iclr 2022 table 5 evaluation model use base released author baseline spiral base model report wer librispeech real data test set base model librispeech mct mct clean spiral spiral spiral spiral recording ch 2 lowest leading high wer note method including may beneﬁt training worth investigation ablation input perturbation computation noise teacher spiral learns denoised representation perturbed data default only apply perturbation input student alternative method perturb input teacher student optimize consistency representation grill et 2020 chen 2021 conduct experiment evaluate effect perturbing input adding computation noise dropout layerdrop teacher result shown table 8 appendix result suggest applying specaugment teacher input degrades performance signiﬁcantly performance degradation decrease still signiﬁcant lower ratio width mask support necessity representation denoising view spiral extension teacher network fed clean input result also support applying computation noise teacher relative wer reduction computation noise may linked gal ghahramani 2016 effect predictor projection head ablation study understand role predictor projection head spiral sults shown table 9 appendix removing predictor student observe performance degradation representation collapse doe not happen architecture relying predictor prevent collapse grill et 2020 chen 2021 applying batch malization bn predictor essential spiral observe bn predictor replaced layer normalization ln small performance degradation dictor removed observe performance improvement applying convolutional projection head convolutional projection head composed temporal convolution layer ln relu linear layer applying convolutional projection head model predictor no performance improvement suggests convolutional projection head predictor play similar role spiral not complementary 6 conclusion presented spiral new approach speech learning denoising representation perturbed data framework spiral learn speech sentation way training small convolutional classiﬁer frozen representation spiral achieves wer librispeech respectively show spiral achieves competitive better result compared speech method signiﬁcant reduction training cost investigate demonstrates effective solely ing training stage presume spiral general method apply modality image text leave future work 9 published conference paper iclr 2022 reference jimmy lei ba jamie ryan kiros geoffrey hinton layer normalization alexei baevski michael auli adaptive input representation neural language modeling international conference learning representation iclr 2019 alexei baevski michael auli abdelrahman mohamed effectiveness training speech recognition alexei baevski steffen schneider michael auli learning discrete speech representation international conference learning representation iclr 2020 alexei baevski yuhao zhou abdelrahman mohamed michael auli work learning speech representation advance neural information processing system volume 33 jon barker ricard marxer emmanuel vincent shinji watanabe third chime speech separation recognition challenge dataset task baseline 2015 ieee workshop automatic speech recognition understanding asru pp ting chen simon kornblith mohammad norouzi geoffrey hinton simple framework contrastive learning visual representation proceeding international conference machine learning icml pp xinlei chen kaiming exploring simple siamese representation learning proceeding conference computer vision pattern recognition pp yang chen weiran wang chao wang asr proc interspeech 2020 pp chopra hadsell lecun learning similarity metric discriminatively application face veriﬁcation 2005 ieee computer society conference computer vision pattern recognition cvpr 05 pp chung james glass generative speech autoregressive predictive coding 2020 ieee international conference acoustic speech signal processing icassp pp jacob devlin chang kenton lee kristina toutanova bert deep bidirectional transformer language understanding proceeding 2019 conference north american chapter association computational linguistics human language technology 2019 pp angela fan edouard grave armand joulin reducing transformer depth demand structured dropout international conference learning representation iclr 2020 yarin gal zoubin ghahramani dropout bayesian approximation representing model uncertainty deep learning proceeding international conference machine learning icml pp daniel galvez greg diamos juan manuel ciro torres keith achorn anjali gopi david kanter max lam mark mazumder vijay janapa reddi people speech diverse english speech recognition dataset commercial usage conference neural information processing system datasets benchmark track round 1 alex graf santiago andez faustino gomez urgen schmidhuber connectionist poral classiﬁcation labelling unsegmented sequence data recurrent neural network proceeding international conference machine learning icml pp 2006 10 published conference paper iclr 2022 grill florian strub florent e corentin tallec pierre richemond elena buchatskaya carl doersch bernardo avila pires zhaohan guo mohammad gheshlaghi azar bilal piot koray kavukcuoglu remi munos michal valko bootstrap latent new approach learning advance neural information processing system volume 33 pp anmol gulati james qin chiu niki parmar yu zhang jiahui yu wei han shibo wang zhengdong zhang yonghui wu ruoming pang conformer transformer speech recognition proc interspeech 2020 pp wei han zhengdong zhang yu zhang jiahui yu chiu james qin anmol gulati ruoming pang yonghui wu contextnet improving convolutional neural network automatic speech recognition global context proc interspeech 2020 pp wei han zhengdong zhang yu zhang jiahui yu chiu james qin anmol gulati ruoming pang yonghui wu contextnet improving convolutional neural network automatic speech recognition global context proc interspeech 2020 pp hsu hubert tsai benjamin bolte ruslan salakhutdinov abdelrahman mohamed hubert much bad teacher beneﬁt asr 2021 ieee international conference acoustic speech signal processing icassp pp wenyong huang wenchao hu yu ting yeung xiao chen transducer low latency low frame rate streamable speech recognition proc interspeech 2020 pp sergey ioffe christian szegedy batch normalization accelerating deep network training reducing internal covariate shift proceeding international conference machine learning icml pp herve egou matthijs douze cordelia schmid product quantization nearest neighbor search ieee transaction pattern analysis machine intelligence 33 1 jacob kahn ann lee awni hannun speech recognition 2020 ieee international conference acoustic speech signal processing icassp pp 7088 jacob kahn morgane rivi ere weiyi zheng evgeny kharitonov qiantong xu e julien karadayi vitaliy liptchinsky ronan collobert christian fuegen et al light benchmark asr limited no supervision 2020 ieee international ence acoustic speech signal processing icassp pp diederik kingma jimmy ba adam method stochastic optimization tional conference learning representation iclr 2015 tom ko vijayaditya peddinti daniel povey sanjeev khudanpur audio augmentation speech recognition proc interspeech 2015 pp taku kudo john richardson sentencepiece simple language independent subword tokenizer detokenizer neural text processing proceeding 2018 conference empirical method natural language processing system demonstration davis liang zhiheng huang zachary c lipton learning representation robust speech recognition 2018 ieee spoken language technology workshop slt pp andy liu yang chi hsu lee mockingjay vised speech representation learning deep bidirectional transformer encoders 2020 ieee international conference acoustic speech signal processing icassp pp 2020 11 published conference paper iclr 2022 christoph uscher eugen beck kazuki irie markus kitza wilfried michel albert zeyer ralf uter hermann ney rwth asr system librispeech hybrid v attention proc interspeech 2019 pp vimal manohar daniel povey sanjeev khudanpur maximum mutual mation training deep neural network acoustic model proc interspeech 2015 pp 2634 vassil panayotov guoguo chen daniel povey sanjeev khudanpur librispeech asr corpus based public domain audio book 2015 ieee international conference acoustic speech signal processing icassp pp ieee daniel park yu zhang chiu youzheng chen bo li william chan quoc le yonghui wu specaugment large scale datasets 2020 ieee international conference acoustic speech signal processing icassp pp daniel park yu zhang ye jia wei han chiu bo li yonghui wu quoc le improved noisy student training automatic speech recognition proc interspeech 2020 pp vijayaditya peddinti yiming wang daniel povey sanjeev khudanpur low latency acoustic modeling using temporal convolution lstms ieee signal processing letter 25 3 377 vineel pratap awni hannun qiantong xu jeff cai jacob kahn gabriel synnaeve vitaliy liptchinsky ronan collobert fast speech recognition system 2019 ieee international conference acoustic speech signal processing icassp pp chandan ka reddy harishchandra dubey vishak gopal ross cutler sebastian braun hannes gamper robert aichner sriram srinivasan icassp 2021 deep noise suppression challenge 2021 ieee international conference acoustic speech signal processing icassp pp ieee michael seltzer dong yu yongqiang wang investigation deep neural network noise robust speech recognition 2013 ieee international conference acoustic speech signal processing icassp pp kihyuk sohn improved deep metric learning loss objective advance neural information processing system volume 29 nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov dropout simple way prevent neural network overﬁtting journal machine learning research 15 1 gabriel synnaeve qiantong xu jacob kahn tatiana likhomanenko edouard grave vineel pratap anuroop sriram vitaliy liptchinsky ronan collobert asr pervised learning modern architecture workshop audio speech sa international conference machine learning icml 2020 gabriel synnaeve qiantong xu jacob kahn tatiana likhomanenko edouard grave vineel pratap anuroop sriram vitaliy liptchinsky ronan collobert asr pervised learning modern architecture workshop audio speech sa international conference machine learning icml 2020 antti tarvainen harri valpola mean teacher better role model tency target improve deep learning result advance neural information processing system volume 30 yuandong tian xinlei chen surya ganguli understanding learning ic without contrastive pair proceeding international conference machine learning icml pp 2021 12 published conference paper iclr 2022 aaron van den oord yazhe li oriol vinyals representation learning contrastive tive coding ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ł ukasz kaiser illia polosukhin attention need advance neural mation processing system volume 30 pascal vincent hugo larochelle yoshua bengio manzagol extracting composing robust feature denoising autoencoders proceeding international conference machine learning icml pp weiran wang qingming tang karen livescu unsupervised bidirectional speech encoders via masked reconstruction 2020 ieee international conference acoustic speech signal processing icassp pp zhirong wu yuanjun xiong stella x yu dahua lin unsupervised feature learning via parametric instance discrimination proceeding ieee conference computer vision pattern recognition pp alex xiao christian fuegen abdelrahman mohamed contrastive learning asr 2021 ieee international conference acoustic speech signal processing icassp pp qizhe xie luong eduard hovy quoc le noisy student improves imagenet classiﬁcation proceeding conference computer vision pattern recognition cvpr june qiantong xu tatiana likhomanenko jacob kahn awni hannun gabriel synnaeve ronan collobert iterative speech recognition proc interspeech 2020 pp 1010 qiantong xu alexei baevski tatiana likhomanenko paden tomasello alexis conneau ronan collobert gabriel synnaeve michael auli complementary speech recognition 2021 ieee international conference acoustic speech signal processing icassp pp yu zhang james qin daniel park wei han chiu ruoming pang quoc le yonghui wu pushing limit learning automatic speech tion neurips 2020 workshop learning speech audio processing 2020 13 published conference paper iclr 2022 appendix output unit comparison language model decoding evaluate decoding performance different combination spiral output unit former lm lm trained librispeech lm corpus share 1024 subwords spiral model subword unit table 6 asr result model unit language model decoding listed unit lm respectively compare spiral base spiral large previous method report wer librispeech set model unlabeled lm dev test data unit clean clean spiral base subword spiral base char spiral base subword word spiral base subword subword spiral base char word spiral large subword spiral large char spiral large subword word spiral large subword subword spiral large char word spiral large subword spiral large char spiral large subword word spiral large subword subword spiral large char word performance synthetic noisy dataset synthetic noisy dataset matched snr range db training data spiral mct effective applying mct solely observe relative wer reduction synthetic noisy set respectively table 7 evaluation model use base released author baseline spiral base model report wer librispeech test set synthetic noisy librispeech test set 0 30 db base model librispeech mct mct clean clean spiral spiral spiral spiral 14 published conference paper iclr 2022 result ablation study result ablation study discussed section table 8 ablation study input perturbation specaugment computation noise teacher list mask ratio mask length p l time frequency mask ﬁrst row default setting spiral apply spiral base report wer librispeech set time mask frequency mask computation noise dev 20 20 20 20 10 10 table 9 ablation study predictor projection head spiral apply spiral base report wer librispeech set architecture dev spiral base predictor use ln conv proj head predictor conv proj head 15