learning policy ofﬂine reward shaping lina mezghani meta ai linamezghani sainbayar sukhbaatar meta ai piotr bojanowski meta ai alessandro lazaric meta ai karteek alahari abstract developing agent execute multiple skill learning datasets important problem robotics online interaction vironment extremely moreover manually designing reward function every single desired skill prohibitive prior work 1 2 targeted challenge learning policy ofﬂine datasets manually speciﬁed reward hindsight relabeling method suffer issue sparsity reward fail task work propose novel learning phase dataset understand structure dynamic model shape dense reward function learning policy ofﬂine evaluate method three continuous control task show model signiﬁcantly outperforms existing approach 1 2 especially task involve planning keywords ofﬂine rl learning rl 1 introduction goal realizing general autonomous agent requires mastery large diverse set skill achieving focusing skill individually standard reinforcement learning rl framework prohibitive primarily due need manually designed reward tions environment interaction skill unsupervised rl ha opened way learning agent execute diverse ability without supervision reward adapted downstream task generalization 3 4 5 6 ever learning policy method impractical real robot require million interaction trained online recently line study ha emerged us datasets trajectory train cies ofﬂine without additional interaction environment 7 8 precisely given dataset trajectory reward function designed solve speciﬁc task agent learns ofﬂine relabeling transition dataset reward function setting ticularly relevant robotics data collection extremely disentangling data collection policy learning context allows faster policy iteration however would require designing one speciﬁc reward function learning one policy individual task important question scale ofﬂine robot learning therefore ﬁnd way learning policy already collected datasets recent work 1 9 10 targeted problem perspective given dataset previously collected trajectory objective learn agent reach any state dataset advantage formulation ﬁrst make easy interpret skill second doe not require any adaptation grenoble alpes inria cnrs grenoble inp ljk 38000 grenoble france project page conference robot learning corl 2022 auckland new zealand 5 jan 2023 test time making framework unsupervised requires break free reward proposed chebotar et al 1 learn policy ofﬂine hindsight relabeling 2 however approach subject pitfall learning sparse reward inefﬁcient task work present reward shaping method enables building ofﬂine dataset dense reward end develop learning phase aim learning structure dynamic environment training policy phase train reachability network 11 estimate local distance state space ii extract set representative state cover ﬁnally iii build graph set approximate global distance training policy use graph two way compute reward shortest path distance create transition intermediate difﬁculty path goal evaluate method complex continuous control task compare previous ofﬂine 1 2 approach show reward method learns good policy leveraging transition dataset past experience neither any additional interaction environment reward moreover show contrary prior work us datasets collected policy trained supervised reward 1 method allows learning policy even datasets poor quality containing trajectory sampled random policy work thus ﬁrst learn policy ofﬂine datasets without any supervision doe not require any reward function any stage data collection policy training evaluation 2 related work rl original formulation reinforcement learning wa tackled several method 12 13 2 14 policy learning process supervised work set evaluation goal available train time well reward function guide agent goal several work propose solution generating goal automatically training conditioned policy including 15 16 17 adversarial policy 18 recent line research 19 20 21 22 23 24 25 26 focus learning policy unsupervised fashion objective train general agent reach any goal state environment without any supervision reward function train time particular mendonca et al 25 train agent learns discover novel goal explorer model reach achiever policy via imagined rollouts ofﬂine rl data collection technique important aspect studying training policy datasets context ﬁrst work assumed access policy trained reward 27 28 recently method proposed leverage vised exploration collect datasets ofﬂine rl 7 8 particular yarats et al 7 creates dataset trajectory exorl deepmind control suite 29 generated without any reward similar urlb 30 exorl benchmark number exploration gorithms 3 6 31 5 evaluates performance policy trained corresponding ofﬂine datasets relabeled reward ofﬂine rl recent work proposed learn multiple task datasets starting method 32 generate goal improve ofﬂine data collection process supervised way connection ha also studied supervised setting 9 33 learning hierarchical policy 10 setting closely related work actionable model 1 considers problem learning policy ofﬂine datasets without interacting environment no reward employ hindsight relabeling 2 opposed work relies learning sparse reward propose leverage training stage densely shape reward 3 preliminary let e p γ deﬁne markov decision process mdp state action space respectively p probability 2 add edge add node figure 1 overview graph building rithm given transition si add si node distant enough existing node graph moreover add edge graph incoming nearest neighbor si outgoing nearest neighbor function initial state distribution γ discount factor task horizon setting objective learn policy π g maximizes expectation cumulative return goal distribution g denotes goal space make common assumption state goal deﬁned form g assume access dataset episode generated using any data collection algorithm episode stored series tuples general ofﬂine formulation introduced yarats et al 7 dataset relabeled evaluating any reward function r r tuple adding resulting tuple r relabeled dataset extend protocol oriented setting considering goal distribution pg goal space any reward function r g given tuple relabel sampling goal g computing r g adding resulting tuple g r g relabeled dataset dr pg relabeled dataset dr pg generated learn policy executing any ofﬂine rl algorithm algorithm run completely ofﬂine sampling tuples dr pg without any interaction environment policy evaluated online e set ﬁxed evaluation goal not known training 4 reward shaping describe reward shaping method comprises three stage detail ﬁrst stage train reachability network rnet 11 trajectory predict whether two state reachable one another second stage consists building directed graph whose node subset state edge connect reachable state employ rnet criterion avoid adding similar state node cover state uniformly ﬁnal stage consists training policy transition goal sampled trained dense reward computed sum global based graph distance local based rnet distance term important aspect method whole training only us trajectory dataset without running single action environment describe component detail reachability network order learn good local distance state adopt asymmetric version reachability network rnet 11 general idea rnet approximate distance state environment average number step take random policy go one state another adapted original formulation two modiﬁcations ﬁrst use exploration trajectory instead random trajectory second leverage temporal direction state reachable another without converse true let sa 1 sa denote trajectory trajectory index deﬁne reachability label yab ij pair observation sa sb j yab ij b 0 0 otherwise 1 j 1 reachability threshold τreach hyperparameter reachability label equal 1 iff state trajectory number step sa sb j τreach shown 3 training label rnet 1 shortest path b visualisation reward computation figure 2 visualization dense reward shaping method show training label generated training rnet given state si positive pair sampled trajectory within threshold τreach rest trajectory form negative pair b present reward implemented combination global distance term green computed shortest path graph outgoing nearest neighbor nnout state incoming nearest neighbor goal nnin local distance term red computed using rnet value nnin figure note yab ij yab ji train siamese neural network r rnet predict reachability label yab ij pair observation sa sb j rnet consists embedding network g network f compare embeddings r sa sb j σ f g sa g sb j 2 σ sigmoid function higher r value indicates two state reachable easily random walk considered close environment precisely r take value 0 1 reachable r rnet learned fashion label needed train network generated automatically directed graph next phase use trajectory build directed graph capture dynamic environment illustrated figure want node evenly represent state achieved ﬁltering state state added only distant enough node precisely state added only r n r n n 3 note require direction novel ﬁltering avoids redundancy preventing similar state added memory also ha balancing effect limit number state added certain area even visited agent many time node selected connect pair reachable one another end employ trajectory contain actual feasible transition given transition si add edge ni si reached node ni node nj reached sj way chain ni assume nj reachable ni concretely select node ni incoming nearest neighbor nnin si nj outgoing nearest neighbor nnout sj ni nnin si argmax r n si nj nnout sj argmax r sj n 4 performing action transition turn directed graph edge represent reachability one node another distance function policy training use obtained directed graph compute global distance state space indeed rnet predicts reachability si sj directly use distance metric dl si sj 1 si sj sj 5 4 however reachability metric conﬁned certain threshold no guarantee rnet prediction good global property contrast directed graph capture global dynamic environment easily derive distance function dm ni nj any pair node computing length shortest path graph provided graph connected practice use trick connect graph necessary adding edge pair node different connected component maximum rnet value moreover extend distance dm global distance function dg state space ﬁnding any pair si sj nearest neighbor corresponding direction precisely dg si sj dm nnout si nnin sj sj 6 distance dg two state state space becomes length shortest path respective closest node graph process summarized figure propagates good local property rnet get distance function state away since dg capture global distance dl capture local distance use combination ﬁnal distance function sj si sj dg si sj dl si sj policy training last phase method training policy ofﬂine create ﬂine replay buffer b ﬁlled relabeled data randomly sample transition st well goal g relabel transition reward rt g push relabeled transition st g rt order create curriculum artiﬁcially guide agent towards goal experimented two different transition augmentation technique augmentation let st g rt denote relabeled transition np shortest path graph nnout st np nnin g tation technique consists adding replay buffer every transition st ni ri 0 p ri ni word given transition st goal g push replay buffer set relabeled transition goal shortest path st g corresponding reward edge augmentation similar subgoal augmentation technique consider relabeled sition st g rt associated shortest path np time keep goal g every augmented transition every edge ni 1 p add relabeled transition si ai g ri si b si ai si nnout si nnin si ni ri si g note existence transition guaranteed construction edge added graph one node another iff exist transition whose corresponding nearest neighbor two node order replay buffer b ﬁlled policy trained using any algorithm implementation chose soft 34 known require tuning widely used literature 5 experiment environment data collection perform experiment three continuous control task input umaze 35 ﬁrst environment shown figure maze continuous action space ﬁxed initial position generate training data environment deploying random policy randomized start position maze collect trajectory length evaluate agent giving agent goal sampled random environment computing ﬁnal euclidean distance goal roboyoga walker 25 introduced mendonca et al 25 challenging roboyoga mark based walker domain deepmind control suite 29 consists 12 goal 5 umaze b rnet distance c graph distance avg success rate graph reward rnet reward rnet graph reward figure 3 umaze environment heatmap reward computed rnet b graph c distance performance policy trained rnet reward umaze b c high reward shown yellow low reward black correspond body pose inspired yoga lying raising one leg balancing consider version task use dataset yarats et al 7 generated unsupervised exploration policy contains trajectory length obtained deploying proto 5 algorithm walker domain success metric evaluation policy assessed pose humanoid end episode pusher 20 also apply method pusher realistic robotic environment shown ure 7 left robot arm red need push puck blue speciﬁed location table build ofﬂine dataset generated random trajectory length similar prior work 20 22 26 generated 500 goal random state space measured performance ﬁnal euclidean distance puck target location ablation design choice ﬁrst show graph structure necessary planning explain importance directness graph task asymmetric behaviour finally show impact transition augmentation technique labeling data policy necessity reward important component method construction graph enables computing distance good global property empirically validate hypothesis performed comparison policy trained rnet reward using distance dl equation 5 one trained distance term reward run experiment umaze environment show result figure note model trained graph reward outperforms one trained rnet reward overall particularly distant goal ie room 3 4 also notice model trained rnet reward slightly better goal close initial position highlight fact rnet good estimating local distance qualitative visualization figure conﬁrms observation show low value state ﬁrst fourth room importance graph directness investigate importance asymmetry rnet directness graph end implement undirected version method rnet symmetric graph undirected component method unchanged first compare performance variant umaze task figure note asymmetric rnet directed graph approach signiﬁcantly improve policy performance success rate especially goal close initial location goal room 1 analyze qualitative visualization shortest path undirected directed graph roboyoga task shown figure undirected case humanoid deﬁes law gravity encouraged stand head ﬂipping backwards might extremely difﬁcult even infeasible directed case shortest path foster agent ﬁrst get back leg lean forward exemple gravity make dynamic environment reversible justiﬁes directed formulation described method transition sampling strategy ﬁnal ablation study study utility transition mentation technique described subsection evaluate four possible variant method 6 avg success rate directed undirected comparison umaze task b shortest path visualization undirected top directed bottom graph figure 4 importance graph directness umaze task b roboyoga walker task without any augmentation ii edge augmentation only iii subgoal augmentation only iv augmentation execute experiment roboyoga task show result figure observe augmentation technique improve formance agent subgoal augmentation showing greater improvement moreover note combining augmentation improves performance reminder experiment use augmentation technique 0 500 1000 epoch success rate average 0 500 1000 epoch room 1 0 500 1000 epoch room 2 0 500 1000 epoch room 3 0 500 1000 epoch room 4 random negative action actionable model figure 5 performance umaze task show success rate goal sampled random four room well average room comparison prior work baseline compare method prior work unsupervised policy learning perform comparison implementing baseline using ing framework method changing reward relabeling process compare following baseline hindsight experience replay 2 standard supervised rl technique adapted ofﬂine setting precisely relabel trajectory sparse reward equal 1 only ﬁnal transition 0 everywhere else following chebotar et al 1 also label goal sampled random zero reward 2 random negative action variant transition sample action uniformly random action space label zero reward help overcoming problem unseen action mentioned chebotar et al 1 actionable model 1 approach based sight relabeling goal relabeling procedure us ﬁnal state enable goal chaining well negative action sampling trick comparison umaze compare method baseline umaze task show result figure observe model outperforms baseline overall show greater improvement challenging goal far initial position interestingly note actionable model reach goal ﬁrst room only conﬁrms intuition sparse reward make difﬁcult policy learn task 7 0 500 1000 epoch success rate random neg action actionable model comparison baseline success rate subgoal edge subgoal only edge only no augmentation b impact transition augmentation figure 6 performance roboyoga walker task comparison roboyoga walker second experiment compare method baseline roboyoga task shown figure method outperforms prior work actionable model doe not make any signiﬁcant improvement result broken goal shown supplementary material overall result suggest dense reward shaping method allows faster robust ofﬂine policy training 0 500 1000 epoch average distance 0 500 1000 epoch hand distance 0 500 1000 epoch puck distance random neg action actionable model figure 7 performance pusher task lower better report ﬁnal average hand puck distance goal model baseline comparison pusher ﬁnal experiment compared method prior work realistic robotic environment shown figure policy trained ofﬂine evaluated sampling goal random state space measuring three different metric hand distance corresponds ﬁnal distance end robot arm target ii puck distance measure distance ﬁnal puck location target iii average distance average ﬁrst two metric method outperforms baseline task agent able sequentially place puck goal location place hand target location contrary 2 place puck target location performance similar method lack precision hand location 6 conclusion summary limitation proposed method learning policy datasets ofﬂine unsupervised fashion without requiring any additional interaction environment manually designed reward method leverage stage aim ing dynamic environment ofﬂine dataset allows shaping dense reward function show signiﬁcant improvement prior work based hindsight relabeling especially task dense reward crucial learning good policy main limitation method relies availability dataset trajectory sufﬁciently large coverage state space proper policy learning although data already available roboyoga walker task ofﬂine dataset collection could done random policy umaze pusher task step challenging environment another limitation evaluated method exclusively simulated environment not perform any experiment real robot collected dataset expert demonstration available 1 8 acknowledgment karteek alahari supported part anr grant avenue reference 1 chebotar hausman lu xiao kalashnikov varley irpan eysenbach julian finn et al actionable model unsupervised ofﬂine reinforcement learning robotic skill international conference machine learning page pmlr 2021 2 andrychowicz wolski ray schneider fong welinder mcgrew bin pieter abbeel zaremba hindsight experience replay advance neural information processing system 30 2017 3 pathak agrawal efros darrell exploration supervised prediction international conference machine learning page pmlr 2017 4 burda edward storkey klimov exploration random network distillation international conference learning representation 2018 5 yarats fergus lazaric pinto reinforcement learning prototypical resentations international conference machine learning page pmlr 2021 6 eysenbach gupta ibarz levine diversity need learning skill without reward function international conference learning representation 2018 7 yarats brandfonbrener liu laskin abbeel lazaric pinto change algorithm change data exploratory data ofﬂine reinforcement learning iclr 2022 workshop generalizable policy learning physical world 2022 8 lambert wulfmeier whitney byravan bloesch dasagi hertweck riedmiller challenge exploration ofﬂine reinforcement learning arxiv preprint 2022 9 yang lu li sun fang du li han zhang ing supervised learning connection ofﬂine rl arxiv preprint 2022 10 li tang tomizuka zhan hierarchical planning ofﬂine reinforcement learning arxiv preprint 2022 11 savinov raichuk vincent marinier pollefeys lillicrap gelly episodic curiosity reachability international conference learning tions 2018 12 kaelbling learning achieve goal ijcai volume 2 page citeseer 1993 13 schaul horgan gregor silver universal value function approximators international conference machine learning page pmlr 2015 14 nasiriany pong lin levine planning policy vances neural information processing system 32 2019 15 sukhbaatar lin kostrikov synnaeve szlam fergus intrinsic motivation automatic curriculum via asymmetric international conference learning representation 2018 16 sukhbaatar denton szlam fergus learning goal embeddings via hierarchical reinforcement learning arxiv preprint 2018 9 17 openai plappert sampedro xu akkaya kosaraju welinder sa petron pinto et al asymmetric automatic goal discovery robotic manipulation arxiv preprint 2021 18 campero raileanu kuttler tenenbaum aschel grefenstette learning amigo adversarially motivated intrinsic goal international conference learning representation 2020 19 van de wiele kulkarni ionescu hansen mnih vised control discriminative reward international conference learning representation 2018 20 nair pong dalal bahl lin levine visual reinforcement learning imagined goal advance neural information processing system 2018 21 ecoffet huizinga lehman stanley clune new approach problem arxiv preprint 2019 22 pong dalal lin nair bahl levine supervised reinforcement learning international conference machine learning page pmlr 2020 23 venkattaramanujam crawford doan precup learning tance function reinforcement learning arxiv preprint 2019 24 hartikainen geng haarnoja levine dynamical distance learning supervised unsupervised skill discovery international conference learning sentations 2019 25 mendonca rybkin daniilidis hafner pathak discovering achieving goal via world model advance neural information processing system 34 2021 26 mezghani bojanowski alahari sukhbaatar walk random walk learning discover reach goal without supervision iclr workshop agent learning 2022 27 fu kumar nachum tucker levine datasets deep reinforcement learning arxiv preprint 2020 28 gulcehre wang novikov paine omez zolna agarwal merel mankowitz paduraru et al rl unplugged suite benchmark ofﬂine reinforcement learning advance neural information processing system 2020 29 tassa doron muldal erez li casas budden abdolmaleki merel lefrancq et al deepmind control suite arxiv preprint 2018 30 laskin yarats liu lee zhan lu cang pinto abbeel urlb unsupervised reinforcement learning benchmark arxiv preprint 2021 31 pathak gandhi gupta exploration via disagreement national conference machine learning page pmlr 2019 32 endrawis leibovich jacob novik tamar efﬁcient data collection ofﬂine robot learning 2021 ieee international conference robotics automation icra page ieee 2021 33 yan jayaraman bastani far go ofﬂine reinforcement learning via regression arxiv preprint 2022 34 haarnoja zhou abbeel levine soft maximum entropy deep reinforcement learning stochastic actor international conference machine learning page pmlr 2018 35 kanagawa 2020 10 implementation detail reward shaping provide two stage approach graph building process shown algorithm 1 step ﬁlling replay buffer shown algorithm use notation r maximum rnet value node r max r algorithm 1 building directed graph input dataset reachability network r initialize build set node state r r update end end build edge transition st let nt nnin st r n st let nnout r n create directed edge nt end algorithm 2 building replay buffer b ofﬂine policy training input dataset reachability network r rected graph initialize b b not full sample transition st random sample goal g random compute dl g 1 g let nnout r n let ng nnin g r n g compute dg g shortestpathlength ng compute reward rt dl g dg g relabel transition goal g reward rt push st g rt b end actionable model baseline section provide detail actionable model 1 2 since using optimization algorithm ofﬂine policy training baseline method only difference lie transition dataset relabeled build replay buffer 2 idea sample trajectory goal g random cut trajectory step transition trajectory step relabelled twice goal g reward 0 transition goal si ﬁnal state trajectory reward 11 0 transition except ﬁnal one get reward method shown figure actionable model 1 relies similar idea 2 contains two additional step improve method ﬁrst step form goal chaining consists using ﬁnal state trajectory compute reward ﬁnal transition second step aim balancing unseen action effect order regularize action space practice consists sampling negative action policy label transition action implementation trick shown figure implementation third baseline random negative action overall algorithm except also generate transition negative action similar tionable model time negative action not sample policy generated uniformly random action space algorithm 3 input dataset initialize b b not full sample trajectory τ sample goal g randomly cut τ step j 0 sj aj g 0 sj aj si 0 end g 0 si si 1 si end algorithm 4 actionable model input dataset critic network q policy π initialize b b not full sample trajectory τ sample goal g randomly cut τ step j 0 sj aj g 0 sj g sj g 0 sj aj si 0 sj si sj si 0 end g q si g si g g 0 si si 1 si si si 0 si end figure 8 replay buffer ﬁlling 2 actionable model 1 method compare implementation showing red modiﬁcations related goal chaining blue edits related unseen action regularization 12 ﬁrst list reward shaping phase table table 2 detail ofﬂine policy training stage sac 34 able model 1 2 baseline used parameter approach exception some parameter speciﬁc method shown table obtained performing random search method several parameter value experiment work performed 3 random seed common value task umaze roboyoga pusher number training pair 5 105 5 105 5 105 ratio negative ratio negative trajectory reachability threshold τreach 5 2 10 weight local distance reward 1 1 100 batch size 512 512 512 learning rate weight decay total number training epoch 100 100 100 capacity directed graph 1000 10000 1000 table 1 reachability network training directed graph construction value task umaze roboyoga pusher replay buffer capacity 106 106 106 batch size 2048 2048 2048 discount γ number update per epoch 1000 1000 1000 total number epoch 1000 1000 1000 target update interval 1 1 1 soft update coefﬁcient τ sac entropy parameter α optimizer adam adam adam learning rate action repeat 1 2 1 reward scaling factor table 2 ofﬂine policy learning sac 34 method value task umaze roboyoga pusher discount γ sac entropy parameter α learning rate reward scaling factor 1 10 1 table 3 ofﬂine policy learning sac 34 speciﬁc actionable el 1 2 baseline 13 architecture detail reachability network 11 rnet ha siamese architecture two embedding head one observation pair tied weight comparator network compare embeddings return reachability score umaze task used embedding head 3 layer batch normalization tanh activation hidden size 64 embedding size roboyoga walker task embedding network ha architecture increased hidden embedding size comparator network also network contains batch normalization relu activation hidden size umaze respectively roboyoga walker task set 16 resp 128 number layer 2 resp 4 policy network policy network take input observation goal separate head architecture independent weight head implemented network tanh activation hidden size dimension 64 16 dimension feature size output head concatenated fed network width critic network ha architecture observation goal head followed 3 fully width b full result roboyoga walker task show comparison method aforementioned baseline 12 goal roboyoga walker task figure goal illustrated figure see method master goal not require balancing lie back front leg lunge succeeds quite well complicated goal like side angle lean back bridge unable progress complex goal like head stand arabesque success rate lie back lie front leg lunge success rate side angle stand lean back boat 0 500 1000 epoch success rate bridge 0 500 1000 epoch stand one foot 0 500 1000 epoch head stand 0 500 1000 epoch arabesque random neg action actionable model figure 9 performance roboyoga walker talk 12 goal 14 kitchen quadruped walker bin reach left reach right push front push back push front lie back lie front leg lunge side angle stand burner light slide hinge microwave kettle lean back boa lie back stretch lie back 2 leg lie side lie side 2 stand stand light slide light push back push front push back e angle stand crowave kettle lean back boat bridge stand one foot head stand arabesque ie side lie side 2 stand stand 2 point attack balance balance 2 light slide light hinge light kettle slide hinge slide kettle hinge kettle place front place front figure 10 visualization 12 evaluation goal roboyoga walker task 15