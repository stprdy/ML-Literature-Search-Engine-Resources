fedssc shared federated learning sirui hu sea harvard university siruihu ling feng hsph harvard university lingfeng xiaohan yang sea harvard university xiaohan yang yongchao chen sea harvard university yongchaochen learning widely used perform centralized training global model multiple device preserving data privacy device however suffers heterogeneous local data training device increase difﬁculty reach level accuracy centralized training supervised contrastive learning outperform try minimizes difference feature space point belongs class push away point different class propose supervised contrastive federated learning device share learned feature space add learning loss regularization term foster feature space learning loss try minimize cosine similarity distance feature map averaged feature map another device class maximizes distance feature map different class new regularization term added top moon regularization term found outperform regularization term solving heterogeneous data distribution problem index learning contrastive learning sentation sharing introduction federate learning 1 5 ha become hot research topic recent year due application many ﬁelds participant want share private training data want model high quality however usually distribution training data participating client harm model convergence model accuracy large extent supervised contrastive learning try minimize difference feature space point belonging class push away point different class wa found outperform 6 7 moon 8 one regularization term found effective solving ity problem federated learning utilizing similarity model representation correct local training individual party inspired two idea propose shared federated learning fedssc tackle heterogeneity problem fedssc device share learned feature ding add learning loss regularization term foster feature space learning loss try minimize cosine similarity distance current sample feature embedding averaged feature embedding another device class contrary maximizes distance different class new regularization term added top moon regularization term found outperform regularization term getting higher accuracy converging fewer round ii problem solve problem statement particularly interested setting federated learning speciﬁcally assume n device ha training data di 1 2 n di consists different distribution class label goal learn machine learning model without local device directly sharing training data solve argminwl w n x li w 1 li w empirical loss local device background federated learning widely used perform decentralized training global model multiple device ing data privacy device one basic yet popular model fedavg 9 algorithm central server aggregate local model weight device build global model without directly accessing local training data however suffers heterogeneous local data training device increase difﬁculty reaching level accuracy centralized training tackle heterogeneity moon proposes add regularization term prevent local feature representation image far global feature representation image 8 moon outperformed regularization term like fedprox 10 research goal goal research improve federated learning algorithm scenario without sharing raw data across device speciﬁcally would like introduce extra 14 jan 2023 regularization feature space supervised contrastive loss top moon regularization term evaluation metric compare method existing approach evaluate model performance accuracy global model isolated test set moreover use number communication round achieve level accuracy metric convergence speed iii approach inspired idea supervised contrastive learning addition moon loss fedssc utilizes average feature map shared device correct local training tackle heterogeneity problem objective function local device composed three part 1 typical supervised learning loss term calculated entropy lclass 2 moon loss lmoon 3 global wise contrastive loss lglob τ temperature lmoon take projected feature representation z zglob zprev passing image current model current round global model previous epoch model 8 lmoon exp sim z zglob exp sim z zglob exp sim z zprev 2 similarly lglob take temperature τ current projected feature representation zi class shared global wise projected feature representation zsglob total class shared global representation class image treat positive pair whereas any shared global representation different class negative pair lglob exp sim zi zsi glob p exp sim zi zsk glob 3 construct zsglob ﬁrst local device report global server projected feature resentations last round epoch using equation 4 assuming device ha n training data n k class communication round class global server randomly selects device ha least 10 image class locally source class wise feature representation zsk glob pn j zi j n k 4 shown equation 5 tune two parameter µmoon µglob weight moon loss global wise contrastive loss differently local objective minimize l lclass µmoon µglob 5 algorithm 1 fedssc framework input number communication round number vice p number local epoch e temperature τ learning rate η µ total number data n number data devicei ni output final global model wt global server initialize global model classwise feature representation 0 1 1 p send wt zst device wt zst end wt p end return wt localtraining wt 0 wt 0 e batch b x ni lclass fwt x z enc wt x zglob enc wt x zprev enc x l µmoon z zglob zprev µglob lglob z zst wt end end class c zsc 1 n c pn x proj enc wt e xj end return e z iv intellectual point contribution first prior work focused regularizing local device weight only ularizing feature representation local image 8 10 however approach directly take advantage feature representation device without sharing raw data regularizing sample local representation corresponding global feature representation global feature map randomly selected device round furthermore experiment show global tation contrastive loss moon loss complementary considered loss moon loss modifying negative pair model could easily collapse performance would worse fedavg moreover even without moon loss simply adding regularization term top supervised learning loss achieve level accuracy moon however using one outperform combining together key success work performed dataset used experiment dataset relatively small widely used previous paper simulate scenario follow moon setup using dirichlet distribution generate dataset di device speciﬁcally sample pk β class allocate pkj sample class k device default β simulates severe situation larger β get iid device using dirichlet local device total number sample different class distribution implementation detail main difference compared moon approach implemented extension moon two main difference particular modify loss function include supervised contrastive loss shared representation furthermore change communication sharing average representation local model addition local model weight avoid bias limited data point only share representation device ha abundant sample corresponding class 10 sample moreover distributing representation server device randomly sample k representation class take average represent speciﬁc class server ha fewer k representation class average everything represent class model architecture considering time limit study used simple cnn two convolution layer two layer two fully connected layer encoder convolution layer fully connected layer relu activation experiment setup evaluate performance method compared fedavg moon moon expected perform better fedavg setting default set µmoon 5 best parameter reported original paper 8 besides set batch size 64 use sgd optimizer learning rate weight decay momentum furthermore mimic traditional contrastive learning decrease weight global representation contrastive loss throughout communication round cally use following formula control weight µglob initial weight µglob e end weight number warmup round experiment set µglob 1 µglob e 100 µglob µglob 1 µglob e simplify experiment assume no device reject server request not encounter nication failure word device participate communication round default setting set number device utilized 1 gpu 10 cpu experimental setting training took le 1 day ﬁnish experiment vi result table overall performance efﬁciency different method method accuracy num comms acc fedavg 100 moon 61 fedssc 41 overall performance default setting method fedssc outperforms fedavg moon improvement compared fedavg signiﬁcant increase moon smaller however fedssc reach 68 level accuracy 41 communication round moon need 61 round fedavg not reach within 100 round summary approach performs better efﬁcient previous method different scenario better evaluate fedssc compared performance approach various scenario cluding β 1 5 figure 1 see increase β difference method decrease since heterogeneity le severe β fedssc slightly outperforms moon signiﬁcantly better fedavg also converges faster others alternative loss beyond proposed loss extensive experiment variant could potentially help u learn better representation particular tried use trastive learning ﬁrst train encoder certain round freezing classiﬁer ﬁrst 90 round train classiﬁer freezing encoder last 10 round furthermore experimented another alternative adding negative pair positive pair local batch figure show proposed approach outperforms alternative lot especially earlier round understandable learning encoder only supervised contrastive learning would take longer round converge using supervised classiﬁcation loss however could potentially outperform current approach train round β b β c β 1 β 5 fig 1 performance comparison different β scenario loss model contrastive learning b loss local contrastive learning c different k presentation shared fig 2 performance comparison alternative loss component blue line proposed approach furthermore attempted remove lmoon pletely setting µmoon 0 achieves level accuracy figure see lmoon take important role remove performance drop even share representation device fedproc 11 moreover experimented different number shared representation k default µmoon figure show sharing representation doe boost mance although increasing k make big difference therefore experimental result demonstrate lmoon lglob complementary vii related work recently many method proposed improve model accuracy data usage heterogeneous distribution environment ha long key problem federated learning many ﬁelds ﬁnance medicine social medium participant want share private data federated learning based work fedavg 9 many method proposed alleviate geneous distribution problem fedprox 10 add extra regularization term push together local model weight global model weight scaffold 12 method us variance reduction correct heterogeneity local training spherefed 13 make use freezed classiﬁcation head increase similarity global local feature space generally previous method focus bring together local global model contrastive learning method simclr 6 moco 14 become promising proaches computer vision recent year byol 15 simsiam 7 extended idea contrastive learning zero negative sample supcon 16 proposed supervised contrastive learning approach some researcher combined contrastive learning federated learning mitigate heterogeneous distribution problem 8 17 11 approach improved upon idea moon 8 positive pair local representation global representation image negative pair set current local representation local representation previous communication round representation sharing recently representation sharing becomes another direction solve problem neous data distribution quite some research work dedicated direction sharing client representation others fedproc 11 proposed sharing local feature local model weight local feature averaged manner ha achieved better performance moon fedprox another recent work fedpcl 17 applies feature sharing minist dataset viii conclusion summary work proposes utilize contrastive ing representation sharing mitigate lem experiment show method orthogonal federated learning method outperform model typical setting accuracy convergence speed apparently raised admittedly experiment needed test availability performance method different setting neural network tures datasets ix contribution statement four author contributed equally work bers participated fully reviewing literature coming model idea coding different model tuning hyperparameters writing ﬁnal report reference 1 zhang shen ding tao duan global model via knowledge distillation federated learning proceeding conference computer vision pattern recognition 2022 pp 10 183 2 zhu hong zhou knowledge distillation erogeneous federated learning international conference machine learning pmlr 2021 pp 12 889 3 li sahu talwalkar smith federated learning challenge method future direction ieee signal processing magazine vol 37 no 3 pp 2020 4 chen guan liu yang wang anomalous lubrication competition adhesion plasticity applied surface science 153762 2022 5 huang ye du learn others heterogeneous federated learning proceeding conference computer vision pattern recognition 2022 pp 10 153 6 chen kornblith norouzi hinton simple framework contrastive learning visual representation corr vol 2020 online available 7 chen exploring simple siamese representation learning cvpr vol 2021 online available 8 li song federated learning corr vol 2021 online available 9 mcmahan moore ramage arca federated learning deep network using model averaging corr vol 2016 online available 10 sahu li sanjabi zaheer talwalkar smith convergence federated optimization heterogeneous network corr vol 2018 online available 11 mu shen cheng geng fu zhang zhang fedproc prototypical contrastive federated learning data vol 2021 online available 12 karimireddy kale mohri reddi stich suresh scaffold stochastic controlled averaging federated learning pmlr vol 2019 online available 13 dong zhang li kung spherefed hyperspherical federated learning vol 2022 online available 14 fan wu xie girshick momentum contrast unsupervised visual representation learning cvpr vol 2020 online available 05722 15 grill strub e tallec richemond buchatskaya doersch pires guo azar piot kavukcuoglu munos valko bootstrap latent new approach learning vol 2020 online available 16 khosla teterwak wang sarna tian isola maschinot liu krishnan supervised contrastive learning neurips 2020 vol 2020 online available 17 tan long liu zhou jiang federated learning model contrastive learning approach vol 2022 online available http