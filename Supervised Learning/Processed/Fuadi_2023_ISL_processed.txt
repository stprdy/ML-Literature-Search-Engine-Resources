gated learning improving supervised learning erland hilman fuadi aristo renaldo ruslim putu wahyu kusuma wardhana novanto yudistira informatics department faculty computer science brawijaya university abstract past research learning image classiﬁcation use rotation augmentation ha common however relying solely rotation transformation limit ability model learn rich feature data paper propose novel approach learning image classiﬁcation using several localizable augmentation combination gating method approach us ﬂip shufﬂe channel augmentation addition rotation allowing model learn rich feature data furthermore gated mixture network used weigh effect learning loss function allowing model focus relevant transformation classiﬁcation index learning transformation mixture expert gating network introduction recently learning 1 2 ha shown niﬁcant result feature learning learning promising approach ﬁxing fundamental problem training machine learning model labeled data scarce expensive learning one important technique data augmentation case age classiﬁcation learning involves giving model set unlabeled image asking predict some property image predicting image model learn valuable feature classiﬁcation using data augmentation technique image recognition enhances model ability generalize teaching sitive feature spatial change achieved applying various modiﬁcations geometric mations cropping ﬂipping rotating photometric ments brightness contrast color recent method demonstrated best performance balancing complexity accuracy robustness addition various method feature learning speciﬁc certain location also used transfer knowledge task related localization like object detection image labeling feature allow model understand focus making correct prediction rotation ha attempted used mentation relying solely rotation transformation limit ability model learn rich feature data paper propose use ditional localizable augmentation ﬂip shufﬂe channel provide diverse set nals augmentation allow model learn complex feature relevant classiﬁcation ensure model focus relevant augmentation classiﬁcation use mixture expert moe gate effect augmentation loss allows model dynamically adjust tance augmentation allowing focus useful transformation classiﬁcation utilizing additional augmentation moe method proposed approach improve performance image classiﬁcation benchmark allowing model learn complex feature relevant classiﬁcation approach improve performance computer vision task object detection image annotation learned feature transferable izable additionally approach used improve performance model trained scarce expensive beled data overall proposed method using additional localizable augmentation moe method adjust weighting importance effectively improve representation learning computer vision task related work learning learning ha gained signiﬁcant interest recent year aim learn general characteristic solving task explicitly created purpose known pretext task depending ber example used task learning divided approach focus increasing similarity sample transformed positive counterpart some also treat sample negative example table technique category include memory bank 5 6 sampling hand some method use positive pair siamese network 7 add relationship module 8 perform task 14 jan 2023 another popular approach involves generating new class data augmentation predicting relative position patch ing puzzle predicting rotation one unique method category lorot 3 designed different objective aid supervised learning recently attempt transfer beneﬁts learning supervised learning supclr 9 adapted framework utilize labeled data since class label clearly deﬁne tive negative example additionally tation sla 10 expanded label space combining supervised class label data transformation label ing auxiliary pretext task decrease performance hand lorot method rectly applied improve supervised learning mixture expert moe 11 ﬁrst model show major improvement model capacity ing time model quality upon activation switch former 12 simpliﬁes activation selecting only best expert token using softmax hidden state exhibit better scalability previous work previous work required ancillary loss encourage balance provision loss must carefully considered not whelm original loss however auxiliary loss doe not guarantee balance hard power factor must applied therefore many token may not affected moe layer hard moe 13 single decoding layer trained effectively hashtag prediction task base er 14 construct linear assignment maximize expert relationship ensuring expert receives equal number token hash class design hashing niques input token unlike previous work method learned one enables heterogeneous moe effectively improves downstream performance use moe way control effect formation loss function thus ensure model focus relevant augmentation siﬁcation method similar 15 gate attached last classiﬁcation layer fuse cnn backbone video classiﬁcation approach differs previous method typically rely single task set task not way adjust importance different augmentation methodology ﬁrst discussed transformation used data augmentation sec used train imbalanced dataset sec finally proposed gated learning based mixture expert approach allowing model focus relevant mations fig diagram transformation transformation ﬁrst instance usefulness data tion shown using simple modiﬁcations like ﬂipping age horizontally changing color space applying dom croppings figure 1 technique address issue related image recognition task insensitivity spatial change section cover various tions based geometric transformation processing method augmentation discussed table simplicity implementation understanding basic transformation serve foundation ploring advanced data augmentation technique proposed method us combination data mentation mixture expert moe approach improve image classiﬁcation performance imbalanced datasets utilized different tions data augmentation including rotation using e 3 ﬂip shufﬂe channel augmentation chosen easy implement yet effective encoding invariance challenge present image nition task used train imbalanced datasets improves performance accuracy model balancing distribution class finally moe attached gating every task allows model learn importance mation dynamically adjust weighting one classiﬁcation additionally gating network built layer softmax activation output class transformation rotation use 3 transformation rotation transformation image divided four quadrant grid randomly selected rotated 0 90 180 transformation create 16 class rotation degree parameter heavily determines degree rotation augmentation fig diagram mixture expert flip flipping one easiest implement ha proven useful datasets task randomly ﬂip selected quadrant image along number class resulting transformation two class shufﬂe channel transformation used shufﬂe arrangement rgb channel selected quadrant image number class resulting transformation six class permutation margin ferred combination two technique designed improve performance machine learning model situation training data heavily balanced among different class evaluation criterion require good generalization le common class 16 experiment use method gated method propose prove performance accuracy model mixture expert moe type deep learning chitecture combine multiple model referred perts divide complex task simpler addressed individual expert 17 work use moe gating every task used learn importance tion used learning illustrated figure transformation ha linear head output class transformation weight gate transformation also learned using moe allows model importance transformation classiﬁcation dynamically use layer softmax activation function gating network eq 1 g softmax w x b 1 g x w b refer gate baseline output weight gating network bias gating network respectively speciﬁcally gate every loss task l sum gating loss combine loss classiﬁer supervised lc loss supervision follows eq 2 ltot lc λ x gt nln 2 number task λ ssl ratio experimental setup imbalanced task experiment tested gated learning method using google colab pro environment gpu use experiment nvidia gpu fair comparison use bone baseline previous research train model experiment use architecture backbone network 16 line follows baseline setting set batch size 128 epoch training model learning rate set initial value dropped factor epoch epoch optimizer use experiment stochastic dient descent momentum weight decay 2 ssl ratio set experiment also train model using proposed method dataset 18 using single gpu nvidia quadro rtx fair comparison use backbone setup method tested train model set batch size 256 epoch training model used backbone stochastic gradient descent optimizer set momentum weight decay 2 learning rate set initial value decaying factor every 75 epoch finally ssl ratio set experiment experiment experiment use 16 imbalanced classiﬁcation cifar dataset design table imbalanced classiﬁcation accuracy imbalance ratio table additional experiment val accuracy resnet 18 imbalanced scenario create three combination gated learning proposed method moreover also vary balance scenario cifar 10 cifar 100 datasets apply imbalance ratio pare proposed method niques report result 16 ssp 19 20 3 result imbalanced classiﬁcation shown table see combination task moe clear complementary effect improve accuracy ssp method gain however several tions moe improve method ﬂip ﬂip shufﬂe channel however moe combination not improve model two imbalanced datasets cifar 10 cifar 100 thus reducing model accuracy meanwhile combination shufﬂechannel improve accuracy model imbalanced cifar 10 ad cifar speciﬁcally combination successfully improves model accuracy imbalanced scenario cifar 100 dataset therefore best combination improve model accuracy imbalanced task order evaluate effectiveness posed method tested method dataset found consistently outperformed method shown table wa particularly sive given large number class dataset method improved even challenging dataset overall result demonstrate versatility effectiveness method wa able achieve ter performance datasets tested method ha potential widely applicable useful variety image classiﬁcation task work considers transformation one therefore gated learning method tackle problem determining mation important model learn however make improvement need ﬁnd select nation task moe best improve model accuracy conclusion learning ha gained increasing attention recent year way train deep learning model using large amount unlabeled data main idea behind supervised learning use inherent structure data create supervised learning problem used train model work proposed learning method add additional transformation ﬂip shufﬂe channel past method rotation mation result show adding additional formation help increase accuracy model cially using larger datasets cifar thermore compared method proposed method gave better accuracy experiment using cifar cifar 10 method also gave better accuracy certain experiment finally achieve better result using learning method top model overall result suggest adding tions learning combination ing method help improve model performance highlight potential learning erful tool training deep learning model various ting reference 1 ting chen simon kornblith mohammad norouzi geoffrey hinton simple framework contrastive learning visual representation 7 2020 2 kaiming haoqi fan yuxin wu saining xie ross girshick momentum contrast unsupervised visual representation learning proceeding conference computer vision pattern recognition 2020 pp 3 wonjun moon kim heo ing supervised learning pean conference computer vision springer 2022 pp 4 spyros gidaris praveer singh nikos komodakis unsupervised representation learning predicting age rotation 3 2018 5 xinlei chen kaiming exploring simple siamese representation learning proceeding conference computer vision pattern recognition 2021 pp 6 mang ye xu zhang pong c yuen chang unsupervised embedding learning via invariant spreading instance feature proceeding conference computer vision pattern recognition 2019 pp 7 grill florian strub florent e corentin tallec pierre richemond elena buchatskaya carl doersch bernardo avila pires zhaohan guo hammad gheshlaghi azar et bootstrap new approach learning vances neural information processing system vol 33 pp 2020 8 massimiliano patacchiola amos j storkey supervised relational reasoning representation ing advance neural information processing tems vol 33 pp 2020 9 prannay khosla piotr teterwak chen wang aaron sarna yonglong tian phillip isola aaron maschinot ce liu dilip krishnan supervised contrastive learning advance neural information processing system vol 33 pp 2020 10 wanshun gao meiqing wu lam qihui xia jianhua zou decoupled label augmentation image classiﬁcation system vol 235 pp 107605 2022 11 noam shazeer azalia mirhoseini krzysztof maziarz andy davis quoc le geoffrey hinton jeff dean outrageously large neural network layer ternational conference learning representation 2017 12 william fedus barret zoph noam shazeer switch transformer scaling trillion parameter model simple efﬁcient sparsity 2021 13 sam gross marc aurelio ranzato arthur szlam hard mixture expert large scale weakly vised vision proceeding ieee conference computer vision pattern recognition 2017 pp 14 mike lewis shruti bhosale tim dettmers naman goyal luke zettlemoyer base layer fying training large sparse model ings international conference machine learning marina meila tong zhang ed jul 2021 vol 139 proceeding machine learning research pp pmlr 15 novanto yudistira takio kurita gated spatio temporal convolutional neural network activity recognition towards gated multimodal deep learning eurasip journal image video processing vol 2017 no 1 pp 2017 16 kaidi cao colin wei adrien gaidon nikos arechiga tengyu learning imbalanced datasets margin loss advance neural information processing system wallach larochelle beygelzimer fox garnett ed 2019 vol 32 curran associate 17 zixiang chen yihe deng yue wu quanquan gu yuanzhi li towards understanding mixture expert deep learning 8 2022 18 ya le xuan yang tiny imagenet visual tion challenge c vol 7 no 7 pp 3 2015 19 yuzhe yang zhi xu rethinking value label improving learning advance neural information processing system vol 33 pp 2020 20 hankook lee sung ju hwang jinwoo shin supervised label augmentation via input tions proceeding international ference machine learning hal e iii aarti singh ed jul 2020 vol 119 proceeding machine learning research pp pmlr