multimodal generative model scalable learning mike wu department computer science stanford university stanford ca 94025 wumike noah goodman department computer science psychology stanford university stanford ca 94025 ngoodman abstract multiple modality often describing natural phenomenon learning joint representation modality yield deeper useful representation previous generative approach input either not learn joint distribution require additional computation handle missing data introduce multimodal variational autoencoder mvae us inference network training paradigm solve inference problem notably model share parameter efﬁciently learn any combination missing modality apply mvae four datasets match performance using many fewer parameter addition show mvae directly applicable supervised learning robust incomplete supervision consider two case study one learning image detection colorization set modality followed one machine translation two language ﬁnd appealing result across range task 1 introduction learning diverse modality ha potential yield generalizable representation instance visual appearance tactile impression object converge invariant abstract characterization 37 similarly image natural language caption capture complimentary converging information scene 32 35 deep learning approach learn bridge modality generative approach promise capture joint distribution across modality ﬂexibly support missing data indeed multimodal data expensive sparse leading weakly supervised setting only small set example observation present access larger dataset one subset modality propose novel multimodal variational autoencoder mvae learn joint distribution weak supervision vae 13 jointly train generative model latent variable vations inference network observation latents moving multiple modality missing data would naively need inference network combination modality however would result exponential explosion number trainable parameter assuming conditional independence among modality show correct inference work 9 structure reduces number inference network one per modality inference network best trained separately generative model requires joint observation thus propose training paradigm example treated fully partially observed gradient update altogether provides novel useful solution inference problem conference neural information processing system nip 2018 montréal canada 12 nov 2018 report experiment measure quality mvae comparing previous model train mnist 16 binarized mnist 15 multimnist 7 24 fashionmnist 34 celeba 17 several datasets complex sequence rgb requiring large inference network rnns cnns show mvae able support heavy encoders thousand parameter matching performance apply mvae problem two modality first revisit celeba time ﬁtting model 18 attribute individual modality ﬁnd better performance sharing statistical strength explore question choosing handful image transformation commonly studied computer edge detection segmentation synthesizing dataset applying celeba show mvae jointly learn transformation modeling modality finally investigate mvae performs incomplete supervision reducing number example ﬁnd mvae able capture good joint representation only small percentage example show real world applicability investigate weak supervision machine translation language modality 2 method variational autoencoder vae 13 latent variable generative model form pθ x z p z pθ p z prior usually spherical gaussian decoder pθ consists deep neural net parameter θ composed simple likelihood bernoulli gaussian goal training maximize marginal likelihood data evidence however since intractable evidence lower bound elbo instead optimized elbo deﬁned via inference network qφ serf tractable importance distribution elbo x λ log pθ kl qφ p z 1 kl p q divergence distribution p q β 8 λ weight balancing term elbo practice λ 1 β slowly annealed 1 2 form valid lower bound evidence elbo usually optimized via stochastic gradient descent using reparameterization trick estimate gradient 13 z 2 x poe σn µn xn en µ σ z b poe missing σn µn xn en µ σ z c figure 1 graphical model mvae gray circle represent observed variable b mvae architecture n modality ei represents inference network µi σi represent variational parameter represent prior parameter poe combine variational parameter principled efﬁcient manner c modality missing training drop respective inference network thus parameter en shared across different combination missing input multimodal setting assume n modality xn conditionally independent given common latent variable z see fig assume generative model form pθ xn z p z pθ pθ pθ factorization ignore unobserved modality evaluating marginal likelihood write data point collection modality present x modality present elbo becomes elbo x x λi log pθ kl qφ p z 2 2 approximating joint posterior ﬁrst obstacle training mvae specifying inference network q subset modality x xn previous work 27 30 ha assumed relationship inference network unpredictable therefore separate training required however optimal inference network q xn would true posterior p xn conditional independence assumption generative model imply relation among posterior p xn p p z p xn p z p xn n p p z p xn n p p xi p z qn p p z qn p xi p xn qn p p z 3 joint posterior product individual posterior additional quotient prior assume true posterior individual factor p properly contained family variational q eqn 3 suggests correct q xn product quotient expert qn q p z call alternatively approximate p q q p z q underlying inference network avoid quotient term p xn qn p p z qn q p z p z p z n q 4 word use product expert poe including prior expert approximating distribution figure representation simpler describe numerically stable derivation easily extended any subset modality yielding q z q q figure refer version mvae product quotient distribution required not general solvable closed form however p z q gaussian simple analytical solution product gaussian expert gaussian 6 mean µ p µiti p ti covariance v p ti µi vi parameter gaussian expert ti v inverse covariance similarly given two gaussian expert x x show quotient qoe x x also gaussian mean µ covariance v ti v however distribution only simple constraint hard deal practice full derivation poe qoe found supplement thus compute inference network required mvae efﬁciently term n component q additional quotient needed variant also easily calculated requires added constraint variance training paradigm face train mvae simply optimizing evidence lower bound given eqn however doe not uniquely specify component gaussians hence given complete dataset no missing modality optimizing eqn 2 ha unfortunate consequence never train individual inference network small thus not know use presented missing data test time conversely treat every observation independent observation modality adequately train inference network q fail capture relationship modality generative model assumption best approximation product factor may not product best approximation individual factor product q still tractable family approximation 3 propose instead simple training scheme combine extreme including elbo term whole partial observation instance n modality complete example xn split partial example xn train using subset would require evaluating elbo term tionally intractable reduce cost elbo term optimize every gradient step speciﬁcally choose 1 elbo using product n gaussians 2 elbo term using single modality 3 k elbo term using k randomly chosen subset xk minibatch thus evaluate random subset elbo term expectation approximating full objective objective written elbo xn n x elbo xi k x elbo xj 5 explore effect k sec pleasant training scheme generalizes learning given example missing data x modality present still sample partial data x ignoring modality missing 3 related work given two modality many variant vaes 13 12 used train generative model form p including conditional vaes cvae 25 conditional autoencoders cmma 19 similar work ha explored using hidden feature vae trained image generate caption even weakly supervised setting 21 critically model not interested studying model condition interchangeably example bivcca 33 train two vaes together interacting inference network facilitate reconstruction however doe not attempt directly model joint distribution ﬁnd empirically improve ability model learn data distribution several recent model tried capture joint distribution explicitly 27 introduced joint vae jmvae learns p using joint inference network q handle missing data test time jmvae collectively train q two inference network q q author use elbo objective two additional divergence term minimize distance importance distribution unfortunately jmvae train new inference network subset previously argued sec 2 intractable general setting recently 30 introduce another objective vae call triplet elbo like mvae model joint inference network q combine variational distribution using rule unlike mvae author report training process using complete data ﬁt q decoder freezing p p ﬁt inference network q q handle missing data test time crucially training separated model ha ﬁt 2 new inference network handle combination missing data stage two paradigm sufﬁcient two modality doe not generalize truly case best knowledge mvae ﬁrst deep generative model explore two modality efﬁciently moreover training mvae make uniquely applicable learning proposed technique resembles established work several way example poe reminiscent restricted boltzmann machine rbm another latent variable model ha applied learning 18 26 like inference network rbm decomposes posterior product independent component beneﬁt mvae offer rbm simpler training algorithm via gradient descent rather requiring contrastive divergence yielding faster model handle data technique somewhat similar denoising 31 18 subset input partially destructed encourage robust representation autoencoders case think robustness capturing true marginal distribution 4 experiment previous literature transform datasets problem treating label second modality compare existing model vae bivcca jmvae mvae 4 show equal performance four image datasets mnist fashionmnist multimnist celeba dataset keep network architecture consistent across model varying only objective training procedure unless otherwise noted given image label set 1 ﬁnd upweighting reconstruction error modality important learning good joint distribution model binarymnist mnist fashionmnist multimnist celeba vae 730240 730240 3409536 1316936 4070472 cvae 735360 735360 3414656 4079688 bivcca 1063680 1063680 3742976 1841936 4447504 jmvae 2061184 2061184 7682432 4075064 9052504 1063680 1063680 3742976 1841936 4447504 mvae 1063680 1063680 3742976 1841936 4447504 10857048 table 1 number inference network parameter single dataset generative model us inference network architecture modality thus difference parameter solely due inference network interact model note mvae ha number parameter bivcca show number parameter using 19 inference network attribute celeba modality version multimnist contains 0 4 digit composed together canvas unlike 7 digit ﬁxed location generate second modality concatenating digit form string literature use rnn encoder decoder 2 furthermore explore two version learning celeba one treat 18 attribute single modality one treat attribute modality total denote latter scenario approximate full objective set k 1 total 21 elbo term eqn 5 complete detail including training hyperparameters architecture speciﬁcation refer supplement 5 evaluation setting denoting image denoting label measure test marginal log p test joint log p using 100 importance sample celeba 1000 sample datasets choice ence network use example using q estimate log p eq p p z q also compute test conditional log p measure classiﬁcation performance done 27 log p eq p p p z q ep z p celeba use 1000 sample estimate ep z p others use 5000 sample marginal probability measure ability model capture data distribution conditionals higher scoring model better able generate proper sample convert modality exactly ﬁnd desirable generative model quality inference network model inference network function importance distribution approximating intractable posterior better importance distribution accurately approximates posterior result importance weight lower variance thus estimate variance log importance weight measure inference network quality see table 3 fig 2 show image sample conditional image sample dataset using image generative model ﬁnd sample good quality ﬁnd conditional sample largely correctly matched target label table 10 show test model see mvae performs par jmvae using far fewer parameter see table 1 considering only p likelihood image modality alone result used q importance distribution see supplement similar result using q importance sampling either q q yield unbiased estimator marginal likelihood expect agree asymptotically 5 model binarymnist mnist fashionmnist multimnist celeba estimated log p vae bivcca jmvae mvae estimated log p jmvae mvae estimated log p cvae jmvae mvae table 2 estimate using q marginal probability average test example mvae jmvae roughly equivalent data table 1 show mvae us far fewer parameter cvae often better capturing p doe not learn joint distribution b c e f g h figure 2 image sample using mvae c e g show 64 image per dataset sampling z z generating via p similarly b f h show conditional image reconstruction sampling z b 5 ankle boot f 1773 h male mvae also performs best slightly beating even vae indicating solving harder problem doe not sacriﬁce any model capacity perhaps help celeba treat feature independent modality mvae treat feature vector single modality suggests poe approach generalizes larger number modality jointly training share statistical strength moreover show supplement robust randomly dropping modality table 3 show variance log importance weight mvae always produce lower variance method capture joint distribution often lower conditional model furthermore consistently produce lower variance mvae celeba overall suggests poe approach used mvae yield better inference network 6 model binarymnist mnist fashionmnist multimnist celeba variance marginal log importance weight var log p z q vae bivcca jmvae mvae variance joint log importance weight var log p z q jmvae mvae variance conditional log importance weight var log p q cvae jmvae mvae table 3 average variance log importance weight three marginal probability estimated importance sampling q 1000 importance sample used approximate variance lower variance better quality inference network effect number elbo term mvae training paradigm hyperparameter k control number sampled elbo term approximate intractable objective investigate importance vary k 0 50 train celeba ﬁnd increasing k ha little effect data reduces variance importance distribution deﬁned inference network practice choose small k tradeoff computation better importance distribution see supplement detail autoencoder vae jmvae mvae feedforward rbm logreg 2 4 6 8 log number example accuracy dynamic mnist 2 4 6 8 log number example accuracy b fashionmnist 2 4 6 8 log number example accuracy c multimnist figure 3 effect supervision level plot level supervision log number paired example shown model mnist fashionmnist predict target class multimnist predict correct string representing digit compare suite baseline composed model relevant literature commonly used classiﬁers mvae consistently beat baseline middle region enough data ﬁt deep model regime mvae competitive feedforward deep network see supplement accuracy weakly supervised learning dataset simulate incomplete supervision randomly reserving fraction dataset example remaining data split two datasets one only ﬁrst modality one only second shufﬂed destroy any pairing examine effect supervision predictive task p predict correct digit label image mvae total number example shown model always ﬁxed 7 only proportion complete example varied compare performance mvae suite baseline model 1 supervised neural network using architecture stochastic layer removed mvae 2 logistic regression raw pixel 3 autoencoder trained full set image followed logistic regression subset paired example something similar 4 vaes 5 rbms internal latent state used input logistic regression ﬁnally 6 train jmvae α suggested 27 subset paired example fig 3 show performance vary level supervision multimnist string 6 8 1 2 representing number image only include jmvae baseline since not straightforward output raw string supervised manner ﬁnd mvae surpasses baseline middle region enough paired example sufﬁciently train deep network not enough paired example learn supervised network especially emphasized fashionmnist mvae equal fully supervised network even two order magnitude le paired example see fig 3 intuitively result suggest mvae effectively learn joint distribution bootstrapping larger set data second observation mvae almost always performs better jmvae discrepancy likely due directly optimizing marginal distribution rather minimizing distance several variational posterior noticed empirically jmvae using sample q much better accuracy sample q edge detection facial landscape b colorization c fill blank removing watermark figure 4 learning computer vision transformation 4 ground truth image randomly chosen celeba along reconstructed image edge facial landscape mask b reconstructed color image c image completion via reconstruction reconstructed image watermark removed see supplement larger version sample 6 case study computer vision application use mvae learn image transformation inverse conditional distribution particular focus colorization edge detection facial landmark segmentation image completion watermark removal original image modality total six build dataset apply transformation celeba colorization transform rgb color grayscale image completion half image replaced black pixel watermark removal overlay generic watermark extract edge use canny detector 5 28 compute facial landscape mask use dlib 10 opencv 3 ﬁt mvae 250 latent dimension use adam learning rate batch size 50 λi 1 1 n β annealing 20 100 epoch fig 4 show sample showcasing different learned transformation fig encode original image learned encoder decode transformed image learned generative model see reasonable reconstruction good facial landscape edge extraction go opposite direction encoding transformed image sampling generative model reconstruct original result quite good reconstructed agree gaze direction hair color colorizations reasonable trace watermark removed though reconstructed image still suffer blurriness vaes 38 8 7 case study machine translation num aligned data test log p x 133 665 1330 1 6650 5 13300 10 133000 100 table 4 weakly supervised translation log likelihood test set averaged 3 run notably ﬁnd good performance small fraction paired example second case study explore machine lation weak supervision only small subset data consist translated sentence pair many popular translation model 29 fully supervised million parameter trained datasets ten million paired example yet aligning text across language costly requiring input expert human tor even unsupervised machine translation erature relies large bilingual dictionary strong language model synthetic datasets 14 1 23 factor make weak supervision particularly intriguing use dataset tence pair iwslt 2015 treat english en vietnamese vi two modality train mvae 100 latent dimension 100 epoch λen λvi 1 use rnn architecture 2 maximum sequence length 70 token 2 word dropout kl annealing crucial prevent latent collapse type sentence xen wa one highest point life xvi xen đó là một gian tôi vời của cuộc đời tôi google xvi wa great time life xen project also made big difference life people xvi xen tôi án này được ra một điều lớn lao cuộc sống của chúng người sống chữa hưởng google xvi project great thing life people live thrive xvi trước tiên tại sao chúng lại có ấn tượng xấu như vậy xen xvi ﬁrst not good job google xvi first bad xvi ông ngoại của tôi là một người thật đáng unk phục vào thời ấy xen xvi grandfather best experience family google xvi grandfather wa worthy person time table 5 example 1 translating english vietnamese sampling p z 2 inverse use google translate google only 1 aligned example mvae able describe test data almost well could fully supervised dataset table 4 5 aligned example model reach maximum performance table 11 show example translation forward backwards english vietnamese see supplement example ﬁnd many translation not extremely faithful interestingly capture close interpretation true meaning result not competitive translation remarkable given weak supervision future work investigate combining mvae modern translation architecture transformer attention 8 conclusion introduced variational autoencoder new training paradigm learns joint distribution robust missing data optimizing elbo example fully utilize structure share inference network parameter fashion scale arbitrary number modality ﬁnd mvae match four datasets show promise two real world datasets 9 acknowledgment mw supported nsf grfp google cloud platform education grant ndg supported darpa ppaml afrl cooperative agreement thank robert hawkins ben peloquin helpful discussion reference 1 mikel artetxe gorka labaka eneko agirre kyunghyun cho unsupervised neural machine tion arxiv preprint 2017 2 samuel r bowman luke vilnis oriol vinyals andrew dai rafal jozefowicz samy bengio generating sentence continuous space arxiv preprint 2015 3 gary bradski adrian kaehler opencv dobb journal software tool 3 2000 4 yuri burda roger grosse ruslan salakhutdinov importance weighted autoencoders arxiv preprint 2015 5 john canny computational approach edge detection reading computer vision page elsevier 1987 6 yanshuai cao david j fleet generalized product expert automatic principled fusion gaussian process prediction arxiv preprint 2014 7 sm ali eslami nicolas heess theophane weber yuval tassa david szepesvari geoffrey e hinton et al attend infer repeat fast scene understanding generative model advance neural information processing system page 2016 8 irina higgins loic matthey arka pal christopher burgess xavier glorot matthew botvinick shakir mohamed alexander lerchner learning basic visual concept constrained variational framework 2016 9 geoffrey e hinton training product expert minimizing contrastive divergence training 14 8 2006 10 davis e king machine learning toolkit journal machine learning research 10 jul 1758 2009 11 diederik p kingma jimmy ba adam method stochastic optimization arxiv preprint 2014 12 diederik p kingma shakir mohamed danilo jimenez rezende max welling learning deep generative model advance neural information processing system page 2014 13 diederik p kingma max welling variational bayes arxiv preprint 2013 14 guillaume lample ludovic denoyer marc aurelio ranzato unsupervised machine translation using monolingual corpus only arxiv preprint 2017 15 hugo larochelle iain murray neural autoregressive distribution estimator proceeding fourteenth international conference artiﬁcial intelligence statistic page 2011 16 yann lecun léon bottou yoshua bengio patrick haffner learning applied document recognition proceeding ieee 86 11 1998 17 ziwei liu ping luo xiaogang wang xiaoou tang deep learning face attribute wild proceeding international conference computer vision iccv 2015 18 jiquan ngiam aditya khosla mingyu kim juhan nam honglak lee andrew ng multimodal deep learning proceeding international conference machine learning page 2011 19 gaurav pandey ambedkar dukkipati variational method conditional multimodal deep learning neural network ijcnn 2017 international joint conference page ieee 2017 10 20 guim perarnau joost van de weijer bogdan raducanu jose álvarez invertible conditional gans image editing arxiv preprint 2016 21 yunchen pu zhe gan ricardo henao xin yuan chunyuan li andrew stevens lawrence carin variational autoencoder deep learning image label caption advance neural information processing system page 2016 22 alec radford luke metz soumith chintala unsupervised representation learning deep convolutional generative adversarial network arxiv preprint 2015 23 sujith ravi kevin knight deciphering foreign language proceeding annual meeting association computational linguistics human language 1 page association computational linguistics 2011 24 sara sabour nicholas frosst geoffrey e hinton dynamic routing capsule advance neural information processing system page 2017 25 kihyuk sohn honglak lee xinchen yan learning structured output representation using deep conditional generative model advance neural information processing system page 2015 26 nitish srivastava ruslan r salakhutdinov multimodal learning deep boltzmann machine advance neural information processing system page 2012 27 masahiro suzuki kotaro nakayama yutaka matsuo joint multimodal learning deep generative model arxiv preprint 2016 28 stefan van der walt johannes l schönberger juan françois boulogne joshua warner neil yager emmanuelle gouillart tony yu image processing python peerj 2 2014 29 ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez łukasz kaiser illia polosukhin attention need advance neural information processing system page 2017 30 ramakrishna vedantam ian fischer jonathan huang kevin murphy generative model visually grounded imagination arxiv preprint 2017 31 pascal vincent hugo larochelle yoshua bengio manzagol extracting composing robust feature denoising autoencoders proceeding international conference machine learning page acm 2008 32 oriol vinyals alexander toshev samy bengio dumitru erhan show tell neural image caption generator computer vision pattern recognition cvpr 2015 ieee conference page ieee 2015 33 weiran wang xinchen yan honglak lee karen livescu deep variational canonical correlation analysis arxiv preprint 2016 34 han xiao kashif rasul roland vollgraf novel image dataset benchmarking machine learning algorithm arxiv preprint 2017 35 kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan salakhudinov rich zemel yoshua bengio show attend tell neural image caption generation visual attention international conference machine learning page 2015 36 shuo yang ping luo loy xiaoou tang facial part response face detection deep learning approach proceeding ieee international conference computer vision page 2015 37 ilker yildirim perception conception learning multisensory representation university rochester 2014 38 shengjia zhao jiaming song stefano ermon towards deeper understanding variational coding model arxiv preprint 2017 11 dataset description use mnist digit dataset 16 example training validation testing also train binarized version align previous work 15 27 use adam optimizer 11 learning rate minibatch size 100 64 latent dimension train 500 epoch anneal β 0 1 linearly ﬁrst 200 epoch encoders decoder use mlps 2 hidden layer 512 node model p bernoulli likelihood p multinomial likelihood fashionmnist fashion dataset containing 28 x 28 grayscale image clothing 10 shoe etc 34 use identical hyperparameters mnist however employ miniature dcgan 22 image encoder decoder multimnist variant mnist 0 4 digit composed together canvas unlike 7 digit ﬁxed location generate text modality concatenating digit class use 100 latent dimension remaining hyperparameters mnist image encoder decoder retool dcgan architecture 22 text encoder use bidirectional gru 200 hidden unit text decoder ﬁrst deﬁne vocabulary ten digit start token stop token provided start token feed gru linear layer softmax sample new character repeat generating stop token note previous work ha not explored inference network learning show work well mvae celeba celebfaces attribute celeba dataset 36 contains image tie image tagged 40 attribute wear glass ha bang use aligned cropped version selected 18 visually distinctive attribute done 20 image rescaled ﬁrst experiment treat image one modality attribute second modality single inference network predicts 18 attribute also explore variation mvae called treat attribute modality total approximate full objective set k 1 total 21 elbo term use adam learning rate minibatch size 100 anneal kl ﬁrst 20 100 epoch use dcgan image network attribute encoder decoder use mlp 2 hidden layer size 18 encoders decoder b product finite number gaussians section provide derivation parameter product gaussian expert poe derivation summarized lemma give ﬁnite number n gaussian distribution pi x mean µi covariance vi 1 n product qn pi x gaussian mean pn tiµi pn ti covariance pn ti ti v proof write probability density gaussian distribution canonical form k exp ηt 1 λx k normalizing constant λ v η v write product n gaussians distribution qn pi pn ηi pn λi x note product ha form gaussian distribution η pn ηi λ pn λi converting back canonical form see product gaussian ha mean µ pn tiµi pn ti covariance v pn ti c quotient two gaussians similarly may derive form quotient two gaussian distribution qoe lemma give two gaussian distribution p x q x mean µp µq covariance vp vq respectively quotient p x q x gaussian mean tpµp tp covariance tp ti x v x 12 proof write probability density gaussian distribution k exp ηt x λx write quotient two gaussians p q kp exp ηt p 2 xt λpx kq exp ηt q 2 xt λqx ηp x 1 λp x deﬁnes new gaussian distribution λ v p q η v p µp q µq let tp v p tq v q see v tp µ ηv tpµp tp qoe suggests constraint tp tq p v q must hold resulting gaussian experiment p usually product gaussians q product prior gaussians see eqn 3 main paper given n modality decompose vp pn v vq 1 n prior unit gaussian variance thus constraint rewritten pn v n satisﬁed vi n 1 one beneﬁt using regularized importance distribution q p z remove need constraint ﬁt mvae without universal expert add additional nonlinearity inference network variance fed rescaled sigmoid v n sigm v additional result using joint inference network main paper reported marginal probability using q showed mvae similarly compute marginal probability using q importance sampling either induced distribution yield unbiased estimate using large number sample result similar indeed ﬁnd result not differ much main paper mvae still model binarymnist mnist fashionmnist multimnist celeba estimating log p using q jmvae mvae estimating log p using q jmvae mvae estimating log p using q jmvae mvae table 6 similar estimate table 2 main paper using q importance distribution instead q vae cvae not inference network excluded show mvae able match e model architecture specify design inference network decoder used dataset f weak supervision main paper showed not need many complete example learn good joint distribution two modality explore robustness model missing data modality using 19 modality celeba conduct different weak supervision experiment given complete example randomly keep xi probability p 1 example training set simulate effect missing modality beyond setting number example shown model dependent p p suggests average 1 every 2 xi dropped vary p 1 train scratch plot 1 prediction accuracy per attribute 13 variance marginal log importance weight var log p z q model binarymnist mnist fashionmnist multimnist celeba jmvae mvae variance joint log importance weight var log p z q jmvae mvae variance conditional log importance weight var log p q jmvae mvae table 7 average variance log importance weight marginal joint conditional distribution using q lower variance suggest better inference network image linear 512 linear leakyrelu leakyrelu linear 512 linear linear 512 latents linear 512 linear 784 leakyrelu leakyrelu sigmoid b label 10 embedding 512 linear 512 linear leakyrelu leakyrelu linear 512 linear c linear 512 latents linear 512 leakyrelu leakyrelu linear 10 softmax figure 5 mvae architecture mnist q b p c q q speciﬁes image speciﬁes digit label image 64 128 linear 512 swish swish swish linear linear latents linear 512 linear 6152 64 swish swish 1 swish latents linear 512 linear 6152 64 swish swish 1 swish sigmoid b label 10 embedding 512 linear 512 linear leakyrelu leakyrelu linear 512 linear c linear 512 latents linear 512 leakyrelu leakyrelu linear 10 softmax figure 6 mvae architecture fashionmnist q b p c q q speciﬁes image speciﬁes clothing label 2 various data figure 9 conclude method fairly robust missing data even p still see accuracy close prediction accuracy full data g table weak supervision result paper showed series plot detailing performance mvae among many baseline weak supervision task provide table detailing number 14 image 32 64 128 128 linear 512 swish swish swish 64 swish 128 128 256 swish 128 linear linear 64 latents linear 512 64 swish swish latents linear 1024 64 128 swish swish swish 128 32 1 swish 32 sigmoid b label linear linear embedding 200 dropout gru 200 bidirectional c token 10 token 10 loop n time latents latents embedding 200 linear 10 softmax swish gru 2 layer figure 7 mvae architecture multimnist q b p c q q speciﬁes image speciﬁes string 4 digit image 32 64 128 128 linear 512 swish swish swish 64 swish 128 128 256 swish 128 linear linear 64 latents linear 512 64 swish swish latents linear 6400 64 128 swish swish swish 128 32 3 swish 32 sigmoid b label 18 linear 512 512 linear swish swish 512 linear 512 linear c linear 512 latents linear 512 swish linear 18 sigmoid 512 swish 512 linear 512 swish 512 figure 8 mvae architecture celeba q b p c q q speciﬁes image speciﬁes 18 attribute p b log p x c log p x log p figure 9 randomly drop input feature xi probability figure show effect increasing p 1 accuracy sampling correct attribute given image figure b c show change log marginal log conditional approximation p increase case see performance using only 10 complete data 15 model 1 2 5 10 50 100 ae nn logreg rbm vae jmvae mvae table 8 performance several model mnist fraction paired example compute accuracy 1 predicting correct digit image model 1 2 5 10 50 100 nn logreg rbm vae jmvae mvae table 9 performance several model fashionmnist fraction paired example compute accuracy predicting correct class attire image h detail weak supervision baseline vae used image encoder mvae jmvae used identical architecture mvae hyperparameter α rbm ha single layer 128 hidden node trained using contrastive divergence nn us image encoder decoder mvae thereby fair comparison supervised learning mnist trained model 500 epoch fashionmnist multimnist trained model 100 epoch hyperparameters kept constant model effect sampling elbo term main paper stated higher k sampling elbo term see steady decrease variance drop variance attributed two factor 1 additional randomness sampling reparametrizing elbo 4 2 additional elbo term better approximate intractable objective fig 10 c show variance still drop consistently using ﬁxed ϵ 0 1 computing elbo term indicating independent contribution additional elbo term additional randomness j computer vision transformation copy fig 4 main paper show sample increase size image visibility mvae able learn 6 transformation jointly poe inference network model 1 2 5 10 50 100 jmvae mvae table 10 performance several model multimnist fraction paired example compute average accuracy predicting digit correct decomposing string individual digit 4 16 0 10 20 30 40 50 additional elbo term k 6255 6250 6245 6240 6235 log p using q 0 10 20 30 40 50 additional elbo term k 60 80 100 120 140 variance log iw b 0 10 20 30 40 50 additional elbo term k 100 150 200 250 300 variance log iw fixed c figure 10 effect approximating mvae objective elbo term joint b variance log importance weight 3 independent run similarly c compute variance ﬁxes single ϵ 0 1 reparametrizing elbo b c imply switching k 0 k 1 greatly reduces variance importance distribution deﬁned inference network figure 11 edge detection facial landscape top row show 8 ground truth image domly chosen celeba dataset second fourth row respectively plot reconstructed image edge facial landscape mask using trained mvae decoder q figure 12 colorization top row show ground truth grayscale image bottom row show reconstructed color image k machine translation provide sample 1 sampling joint english vietnamese pair sentence prior n 0 1 2 translating english vietnamese sampling p z 3 translating vietnamese english sampling p z refer main analysis explanation 17 figure 13 fill blank top row show ground truth celeba image half image obscured bottom row replaces obscured part reconstruction figure 14 removing watermark top row show ground truth celeba image added watermark bottom row show reconstructed image watermark removed type sentence xen problem xvi nó là một công việc googletranslate xvi job xen idea xvi chúng tôi có thểlàm được googletranslate xvi xen see powerful effect word mouth xvi và một trong những điều này đã xảy ra với những người khác và chúng tôi đã có một sốngười trong sốcác bạn đã từng nghe vềnhững điều này googletranslate xvi one ha happened people some guy already heard xen photograph life xvi đây là một bức ảnh googletranslate xvi photo xen thank xvi xin cảm ơn googletranslate xvi thank xen not kidding xvi tôi không nói đùa googletranslate xvi not joking table 11 example paired reconstruction single sample xvi interestingly many translation not exact instead capture close interpretation true meaning mvae tended perform better shorter sentence 18 type sentence xen wa one highest point life xvi xen đó là một gian tôi vời của cuộc đời tôi googletranslate xvi wa great time life xen stage xvi xen tôi đi trên sân khấu googletranslate xvi stage xen know love xvi xen đó yêu của những googletranslate xvi love xen today 22 xvi xen hãy nay tôi sẽtuổi googletranslate xvi old xen idea xvi xen tôi thếtôi có có thểvài tưởng tuyệt googletranslate xvi some good idea xen project also made big difference life unk xvi xen tôi án này được ra một điều lớn lao cuộc sống của chúng người sống chữa hưởng googletranslate xvi project great thing life people live thrive table 12 example vietnamese mvae translation english sentence sampled empirical dataset pdata use google translate back english type sentence xvi đó là thời điểm tuyệt vọng nhất trong cuộc đời tôi xen xvi bad life googletranslate xvi wa desperate time life xvi cảm ơn xen xvi thank googletranslate xvi thank xvi trước tiên tại sao chúng lại có ấn tượng xấu như vậy xen xvi ﬁrst not good job googletranslate xvi first bad xvi ông ngoại của tôi là một người thật đáng unk phục vào thời ấy xen xvi grandfather best experience family googletranslate xvi grandfather wa worthy person time xvi đứa trẻnày 8 tuổi xen xvi man 8 year old googletranslate xvi child 8 year old table 13 example english mvae translation vietnamese sentence sampled empirical dataset pdata use google translate translate english ground truth 19