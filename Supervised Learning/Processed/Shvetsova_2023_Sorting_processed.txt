learning sorting learning group ordering constraint nina felix anna bernt hilde university frankfurt informatics bonn university watson ai lab shvetsov mail abstract contrastive learning ha become important tool learning representation unlabeled data mainly ing idea minimizing distance positive data pair view image maximizing distance negative data pair view ferent image paper proposes new variation contrastive learning objective group ordering constraint groco leverage idea sorting distance positive negative pair computing tive loss based many positive pair larger distance negative pair thus not ordered correctly end groco loss based tiable sorting network enable training sorting supervision matching differentiable permutation trix produced sorting given set score respective ground truth permutation matrix applying idea groupwise input multiple positive negative pair allows introducing groco loss implicit emphasis strong positive negative leading better optimization local neighborhood uate proposed formulation various learning benchmark show not only lead proved result compared vanilla contrastive learning also show competitive performance comparable od linear probing outperforms current method performance 1 introduction learning ha become topic ing interest last year allows model learn representation data without need human annotation many approach rely idea contrastive learning able not only row gap supervised learning performance sion 23 14 60 30 3 64 also train 1 published iccv cite nina shvetsova felix petersen anna kukleva bernt schiele hilde kuehne learning sorting supervised learning group ordering constraint proceeding international conference computer vision update group ordering constraint groco loss po distance neg distance update contrastive loss swap distance distance figure idea proposed group ordering constraint loss compared pairwise contrastive loss groco arranges positive negative data point largest distance positive must smaller smallest distance negative point end loss implicitly minimizes amount necessary swap operation achieve ordering constraint thus focus overlapping positive negative compared standard trastive loss minimize resp maximize pairwise distance 52 56 multimodal model 40 method rely concept pairwise trastive loss based idea positive pair image serving anchor augmentation image closer embedding space negative pair pair made anchor image different age far away however ha noted idea pairwise contrastive loss also ha some limitation alignment ding space based individual pair several attempt made address issue combining trastive idea concept based local neighborhood clustering swav 11 minimizing distance tween multiple positive pair instance together whitening 24 another limitation contrastive loss embedding space optimized 19 aug 2023 swap group not ok negative position negative position positive position differentiable permutation matrix probability element swapped rank group negative group positive distance anchor differentiable sorting network sort input value ascending order swapping neighboring element 0 0 0 0 0 0 swap within group ok group ordering constraint groco loss sum negative positive position compute bce loss position projection space bce loss anchor encoder 1 2 3 4 negative positive 0 0 0 0 0 0 positive gt negative gt figure overview proposed loss distance positive negative computed respect anchor concatenated distance sorted via differentiable sorting network computes swapping probability result differentiable permutation matrix column value considered probability sorting element corresponding position enforce only relationship group sum positive negative row permutation matrix loss computed bce entry ground truth spect negative even negative far away anchor contribute optimization representation method proposed address issue hard negative selection controlling hardness example 53 negative selection sparse support vector 57 nevertheless method still quire manual selection hardness level 53 incur additional optimization cost 57 shift away concept minimizing resp imizing pairwise distance paper proposes tion contrastive learning formulation namely group ordering constraint groco idea groco positive negative distance sorted way any positive closer anchor image any negative thus forming group positive pair group negative pair idea illustrated figure comparison pairwise contrastive loss groco loss combine distance information group tive negative pair optimization mainly depends incorrectly sorted pair enforce group ordering constraint projection space propose idea learning sorting suggest sorting positive atives distance anchor image differentiable way swapping wrong order lead holistic approach considering ship data point thereby better utilizing timizing embeddings esp multiple positive pair leading improved performance ate training pipeline leverage recent vances differentiable sorting 21 28 44 45 46 47 48 specifically utilize differentiable sorting algorithm obtain differentiable permutation matrix sorting list distance positive negative image shown figure would know full ground truth ordering among positive negative positive ple closer anchor another positive ple could create ground truth permutation matrix calculate much predicted permutation matrix would deviate ground truth one 28 21 45 47 not know ground truth distance ordering within positive negative group propose groco loss relaxed formulation original sorting vision capture many negative element appear positive position vice versa proposed groco loss alleviates some aspect vanilla contrastive learning first treat positive negative pair group instead individual pair second resulting group ing focus optimizing local neighborhood around anchor image mainly optimizing close negative distant positive rather optimizing data point thus implicitly also focus strongest positive furthest anchor strongest negative closest anchor example show capability proposed approach evaluated various competitive learning benchmark namely context linear probing classification transfer learning well image retrieval evaluation show model trained via group dering constraint outperforms contrastive learning work linear probing transfer learning excels context shaping local neighborhood task classification image retrieval contribution work summarized follows advance concept contrastive learning troducing group ordering constraint groco treat positive negative element group rather dividual pair conventional contrastive learning derive loss optimizes proposed constraint harness recent differentiable sorting method tain loss suggests sorting positive negative ments swapping wrong order thus introduce new contrastive learning method called learning sorting proposed method provides embeddings achieve competitive performance linear probing cially suitable model local neighborhood perform contrastive learning framework wide range task related work representation learning contrastive method last year learning method enforce model robust different image distortion achieved great performance provements learning 14 30 15 16 method generally rely sampling two augmented view positive minimize distance embedding space vent model learning trivial solution any put contrastive method introduce concept ative pair two different image contrast positive negative pair earlier contrastive method relied triplet loss 55 probably nent method many learning scenario infonce loss often referred trastive loss 14 30 51 1 requires accumulating strong negative via memory bank 30 large batch size 14 many extension proposed improve performance idea data augmentation strategy 14 62 projection head design 15 hard tive sampling 53 increasing richness positive nearest neighbour 23 mitigating effect false negative 32 variation contrastive method posed work not considered opposed rather orthogonal approach relying positive negative pair change loss function therefore used top technique alternative method also method 17 27 not rely negative only maximize agreement tween positive view method prevent collapsing representation space using asymmetric architecture applied different view 17 27 additional teacher network 27 12 stop gradient 17 27 12 feature ing 24 information maximization 66 4 another set method 11 10 2 utilizes clustering latent bedding ressl 68 leverage relation ding space way namely aligning ilarities image tie similarity strongly augmented image swav 11 additionally proposes sampling tions setting two augmented image sampled together several smaller crop whitening 24 utilized sample currently method relying positive sample seem outperform classical contrastive counterpart show especially local neighborhood learning profit relying positive negative sample differentiable sorting ranking differentiable sorting ranking method provide pipeline allows training neural network ing supervision fashion gradient scent 5 21 28 44 45 46 47 48 49 earlier pairwise method ranknet 8 darank 7 listwise method softrank 59 listnet 9 mostly based heuristic aim mize ranking metric ndcg many latest ferentiable sorting approach 21 28 44 45 47 48 49 focus obtaining differentiable relaxation ing operator sorting operator seen function returning permutation matrix indicates tion necessary sort sequence value matrix multiplied input vector return sorted output tor context differentiable sorting refers relaxing hard permutation matrix differentiable tion matrix via continuous relaxation differentiable permutation matrix given sequence value score predicted neural network used compute loss comparison ground truth permutation matrix recently multiple method ing permutation matrix proposed including argsort approximation unimodal trice 28 49 formulation mal transport 21 well network differentiable swap operation differentiable sorting network 45 47 latter method composes full permutation matrix product permutation matrix arise ing only two element time usually neighbor ther swapping not swapping practically ferentiable sorting ha leveraged various context including recommender system 37 58 image patch lection 20 selection expert learning 29 attention mechanism 67 audio representation ing 13 best knowledge proposed method first work leverage ordering supervision learning visual representation method given dataset image xi goal learn encoder g x extract image tations later used downstream task training pipeline standard contrastive loss proposed method considers several augmented view image positive example close together embedding space different image negative ples apart embedding space ing b image randomly mented view generated image resulting b data point overall per batch note 2 proposed method close original contrastive ing setup 32 14 17 4 66 augmented view cessed encoder network g mlp tion head h map image latent space distance view calculated data point serving anchor xa positive example xp b negative example xn measure distance data point cosine distance defined x group ordering constraint groco order consider positive negative not ually instead group proposed loss extends contrastive loss idea group positive closer anchor image group negative embedding space resulting group ordering straints groco simplify notation distance tween data point xa positive xp negative ples xn denoted dp xa xp dn xa xn assuming k positive xp 1 xp k ordered spect distance anchor xa dp 1 k n negative xn 1 xn n dn 1 n group ordering constraint defined l abe l e q g ro c p k n 1 note element considered straints relevant constraint bold center remark although constraint already fulfilled largest positive distance dp k smaller smallest negative distance dn 1 suboptimal define loss only element comparing only smallest negative largest positive ignores negative second smallest positive second largest might also misaligned loss would ignore next section propose novel loss optimizes groco constraint implicitly learning sorting enforce constraint groco loss leverage cent advance differentiable sorting 45 47 allow derive loss fulfills ordering constraint namely training procedure seen sorting positive negative embedding space respect anchor image swapping incorrect order relates proposed idea learning sorting differentiable sorting network section provides review differentiable sorting algorithm differentiable sorting network 45 used proposed loss function note network sorting network not neural network instead refers step 1 step 2 step 3 step 4 0 0 0 0 0 0 input f f 0 0 f f 0 0 0 0 f f 0 0 f f p differentiable sorting network differentiable conditional swap operation output differentiable permutation matrix p 0 1 1 x p f x arctan βx f x figure overview differentiable sorting network even sorting network compare neighboring element ing odd even index alternatingly step ply differentiable swap operation element wrong order swap operation step also define tiable permutation matrix p network output tiable permutation matrix p defined multiplication trice step category sorting algorithm computer science erature 34 no trainable parameter differentiable sorting network based even sorting network sort input sequence k n ments order shown figure defined concatenation function senting swap operation layer sorting network function refers one step sorting pair element input sequence compared swapped wrong order via conditional swap operation sorting work algorithm compare neighbored element odd even index alternatingly requires k n step sort given input sequence length k laxing conditional swap operator differentiable one sorting network made differentiable 45 ditional swap operation element di dj j defined min di dj j max di dj differentiable relaxation 47 operation b egin split l abel e q ftswap rn thrm oft f f softmax f f split 2 b e l eq swap f x 1 x 3 hyperparameter β 0 denotes inverse temperature β relaxation converges discrete swap operation differentiable conditional swap operation element di dj defined permutation matrix pswap di dj identity trix except entry pii pij pjj pji defined b egi n sp l g alig n ed ii jj f ij ji f aligned split 4 permutation matrix p step product ce corresponding independent thus parallel swap operation step p q pswap di r set odd index odd set even index even complete permutation matrix p defined p discrete case column permutation matrix ha exactly one entry 1 ing position element corresponds column placed relaxed version column value seen distribution possible position element correct order input value known create ground truth matrix q define loss l 1 2 p j bce pij qij bce refers binary groco loss wa known ground truth order positive negative loss could calculated directly based ground truth permutation matrix however algorithm based random augmentation no known ders among positive among negative thus only information available whether pair belongs tive negative group derive loss fulfil group ordering constraint start ordering positive negative first separately respect computed distance anchor image dp 1 k positive dn 1 n tives shown figure 3 positive negative distance concatenated list k n 5 even though element positive negative dered still open constraint dp dn j fulfilled any 1 1 differentiable sorting network applied catenated list differentiable permutation matrix obtained sorting list order shown 3 value permutation matrix column seen probability sort corresponding element different position would probability assigning first element list dp 1 position 1 position 2 etc therefore sum first k element column considered probability sorted inside first k element thus permutation matrix size k n k n sum first k row result probability sorted positive place later column negative place enforce positive sorted positive place negative negative place respective loss 1 indicator function defined lab e l eq los l frac 1 2 k n 1 k n bce k ki 1 k bce k n ki 1 k illustrated figure 2 proposed loss relaxation sorting supervision way considers two type swap operation swap operation within group positive negative sample not tribute loss swap operation group violate ordering assumption used optimization criterion role one relevant hyper parameter inverse perature β differentiable swap operation equation 3 corresponds degree relaxation swap operation converges discrete case β figure 3 therefore lower β swap operation soft beneficial optimization laxation error accumulated step larger vice versa case larger higher β even small difference value result high probability swap not swap operation resulting smaller margin positive negative group number sample since strongest negative strongest effect loss function selection limited only strongest negative negative also result layer sorting network layer contributes overall differential permutation matrix layer also result softer swap probability therefore β selected based number element sort ablation study effect given section role practically positive tives among ing forwarding differentiable sorting work loss still contrast positive negative no matter input ordered not show ordering improves overall performance groco loss attributed fact sorting work perform comparison neighboring element swap wrong order input wa not thus distance positive negative pair would mixed would result tional swap operation using input focus lie comparing strongest positive strongest negative way element considered group border group overlapping part emphasized loss section figure 4 provide additional discussion illustrate behavior experimental evaluation implementation detail unless stated otherwise following setup used experiment model following previous work 14 27 11 17 31 used encoder g mlp block consisting three fully connected layer size 2048 followed batch normalization layer 33 used projection head h batch normalization layer except last one followed relu tion dimensionality representation space latent space 2 048 17 training following previous work 14 27 11 17 use train set imagenet dataset 54 training without any human tion create augmented view per image considering 2 3 4 dino augmentation setup 12 used model trained sgd optimizer 65 learning rate batch 100 epoch batch 200 400 epoch use cosine scheduler without restarts 38 10 epoch 200 400 epoch training 1 epoch ear 100 epoch training find beneficial method not simclr training stop gradient operation used following learning setup 14 12 27 specifically stop ent performed distance computation xa xa stop grad training stop gradient doe not show direct impact overall performance observed allows training larger variation hyperparameters maintaining stable performance default top n 10 strongest negative pled batch inverse temperature β 1 used due resource constraint model trained batch size 1 024 mixed precision gpu nvidia server training 100 epoch 2 view take approximately 22 hour evaluation procedure linear probing linear probing allows evaluate learned embedding space linear evaluation 14 27 11 17 capturing linear separability class linear classifier trained frozen representation way using imagenet train set low standard protocol 17 train linear classifier evaluation analyze local property learned representation namely often neighbored data point correspond semantic class evaluate respect nearest neighbor classification dicting class simple weighted k nearest neighbor classifier k 1 10 20 based cosine tance used 12 11 use imagenet train set supervision test val set method b view 100 ep 200 ep 400 ep linear probing 57 256 moco 16 17 256 14 17 4096 groco 1024 groco 1024 simsiam 17 256 vicreg 4 2048 barlow twin 66 2048 11 4096 ressl 68 256 27 4096 whitening 24 4096 weighted moco 16 256 14 17 4096 groco 1024 groco 1024 simsiam 17 256 swav 11 4096 table comparison linear probing nn classification imagenet report result training 100 200 400 epoch denotes improved reproduction simsiam 17 method epoch batch view oxford paris size h h simsiam 17 100 256 moco 16 200 256 simclr 14 400 4096 swav 11 400 4096 groco 400 1024 table comparison image retrieval evaluate image retrieval performance medium hard h split revisited oxford paris datasets 50 evaluate nearest neighbor retrieval performance trained encoders report map comparison start comparison proposed method learning method linear probing evaluation imagenet 54 image retrieval revised oxford paris dataset 50 well transfer learning linear probing case linear probing table 1 proposed method compared contrastive baseline using positive negative sample namely 57 simclr 14 moco 16 well alternative method observe given setting posed loss able improve contrastive baseline even outperforms strong alternative baseline cept byol 27 whitening 24 400 epoch setup use larger batch size additional teacher network another finding context linear method epoch b aircraft car dtd flower food pet average moco 200 256 simclr 400 4096 simsiam 100 256 swav 400 4096 groco 400 1024 table comparison transfer performance classification 11 classification datasets model trained imagenet method b view 100 ep 200 ep 400 ep linear probing swav 11 4096 groco 1024 weighted swav 11 4096 groco 1024 table comparison swav tion strategy report result training 100 200 400 epoch probing four positive sample doe not significantly improve approach compared only two sample attribute fact missing difference local neighborhood compensated linear layer training thus initial pretraining le relevant respect local neighborhood setting evaluation proposed method ated respect performance compared method officially released weight ble 1 observed proposed method outperforms method setting result strate margin loss improves method increased compared linear probing swav result mainly par case linear probing show tial improvement second hint strength method optimization local neighborhood given fact setting also show proved performance leveraging multiple positive ples indication positive example contribute better local neighborhood setting image retrieval ass potential posed method nearest task model evaluated task image retrieval table result reported mean average precision map medium hard h split datasets 12 method outperforms method task firming good local property learned representation transfer performance finally compare well performance transfer datasets table 3 compare imagenet model evaluation 11 classification datasets including fgvc method view evaluation linear eval simclr groco groco groco groco table comparison simclr contrastive baseline imagenet batch denotes reproduction aircraft 39 26 stanford car 35 36 36 dtd 19 oxford 102 er 41 6 pet 43 61 pascal 25 observe proposed method improves simclr moco baseline class average even outperforms publicly available simsiam swav baseline ticularly attributed improved performance pet food datasets credit increased performance fact food class often pear imagenet pretraining data thus learning good local embedding help datasets specifically case classification augmentation since computational cost grows linearly increasing number tions augmentation strategy proposed swav 11 also considered idea sample resolution local view along standard 224 224 one use 2 224 6 96 scheme two global 224 224 augmented view six local 96 96 view sampled giving eight view per image case low correspondence idea 11 12 use only global view positive local global anchor image proposed method show slightly lower result compared swav improves case classification table 4 comparison contrastive loss evaluate property proposed method rect comparison pairwise contrastive loss method view evaluation linear eval moco groco table comparison moco contrastive line imagenet batch evaluation linear probing triplet loss triplet loss triplet loss groco table comparison imagenet triplet loss batch tion compare classical contrastive ing method simclr 14 moco 18 use popular infonce loss 42 well triplet loss contrastive loss first compare performance simclr method table 5 also analyze property loss align local neighborhood based positive data point ensure identical setting method simclr reproduced mlp projection head 15 since simclr originally us only two augmentation per image extend group positive applying contrastive loss possible positive pair see supplementary material forming simclr baseline linear probing proposed method also advance uation k 20 demonstrating loss help learn better representation not only term linear separability also term local structure considering scenario simclr show mixed result utilizing view benefit ear evaluation performance slightly decrease k 10 20 proposed method profit ing positive evaluation resulting k 1 linear evaluation 1 compare performance recent moco approach 18 small 22 backbone table moco additionally lizes predictor momentum encoder incur ditional computational cost beneficial contrastive training 18 12 plugin proposed groco loss moco setup architecture optimizer etc perform minimal hyperparameter tuning table 6 show without any tip trick proposed method form moco metric ha comparable performance linear probing triplet loss also compare group ordering loss triplet loss formulation l max dp j r 0 inverse temp number negative n β 1 5 10 20 1 2 4 8 16 inverse temperature β number negative evaluation linear probing randomly ordered b group batch size evaluation linear probing 256 512 1024 c batch size batch sampling evaluation linear probing regular many false negative sensitivity false negative batch table ablation experiment report performance linear probing denoted best result bolded option used obtain main result highlighted r margin table 7 fair comparison consider positive 10 strongest negative sample evaluate different margin parameter sorting superior triplet loss hard margin selection ablation study inverse temperature number negative table show influence number nearest neighbor atives n used loss well value inverse temperature parameter β equation 2 observe age many negative not beneficial model since loss focus negative sorted rectly increasing number negative some point doe not bring any new learning signal tant sample unlikely sorted incorrectly ever larger n result step sorting network increasing degree relaxation using larger inverse temperature β leading lower degree relaxation positive negative optimization iteration similarity anchor infonce loss simclr positive negative optimization iteration similarity anchor groco loss figure toy experiment optimize five real variable treating one positive similarity four negative groco loss contrastive infonce loss multiple iteration infonce loss minimizes negative groco loss work behaves similar margin optimization negative away border optimized smaller degree swap operation gain some performance however variance gradient larger larger β not beneficial optimization found n 10 β 1 efficient configuration setting table analyze impact ordering element within negative positive group fore forwarding sorting network find achieves good performance even without also strengthens method batch size large batch size important tor obtaining good performance many learning method result table show method also benefit large batch size attribute utilizing stronger negative larger batch false negative ass sensitivity proposed od false negative artificially sample batch way instance three instance class average acting false negative table observe performance decline nario many false negative given groco plicit emphasis strong negative enhancing ness false negative would require adjustment leave future work training time also consider training time model compared simclr baseline eliminate fluence distributed training measure average time training iteration one gpu find iteration time model comparable simclr v proposed method batch size optimization negative better understand nale behind groco superiority task analyze difference groco loss contrastive fonce loss used simclr structuring embedding space conduct toy experiment optimize five real variable treating one positive similarity four negative similarity loss multiple iteration demonstrate optimization ce figure although loss elevate positive ilarities lower negative similarity differ optimization negative infonce loss imizes negative even pushing blue curve tially zero groco loss work behaves ilar margin optimization negative not border positive optimized smaller degree blue curve pushed only slightly zero light groco focus neighborhood optimization conclusion paper alternative approach common pairwise contrastive learning formulation proposed group ordering constraint consider positive negative group enforce group positive closer anchor image negative group enforce constraint recent progress context tiable sorting approach leveraged formulate group ordering loss based given sorting supervision evaluation show proposed framework doe not only compete current contrastive loss baseline actually outperforms standard contrastive learning many setting regard metric acknowledgement nina shvetsova supported german federal ministry cation research bmbf project stcl reference 1 hassan akbari liangzhe yuan rui qian chuang chang yin cui boqing gong vatt transformer multimodal learning raw video audio text neurips 2021 3 2 ym asano c rupprecht vedaldi via simultaneous clustering representation learning iclr 2020 3 3 philip bachman r devon hjelm william buchwalter learning representation maximizing mutual information across view neurips 32 2019 1 4 adrien bardes jean ponce yann lecun creg regularization supervised learning iclr 2022 3 4 6 5 mathieu blondel olivier teboul quentin berthet josip djolonga fast differentiable sorting ranking icml 2020 3 6 lukas bossard matthieu guillaumin luc van gool discriminative component random forest eccv 2014 7 7 christopher burges robert ragno quoc le learning rank nonsmooth cost function neurips 2006 3 8 chris burges tal shaked erin renshaw ari lazier matt deed nicole hamilton greg hullender learning rank using gradient descent icml 2005 3 9 zhe cao tao qin liu tsai hang li learning rank pairwise approach listwise approach icml 2007 3 10 mathilde caron piotr bojanowski armand joulin matthijs douze deep clustering unsupervised learning visual feature eccv 2018 3 11 mathilde caron ishan misra julien mairal priya goyal otr bojanowski armand joulin unsupervised ing visual feature contrasting cluster assignment neurips 2020 1 3 6 7 14 12 mathilde caron hugo touvron ishan misra e egou julien mairal piotr bojanowski armand joulin ing property vision transformer iccv 2021 3 6 7 8 12 14 13 andrew carr quentin berthet mathieu blondel olivier teboul neil zeghidour learning audio representation permutation differentiable ranking ieee signal processing letter 28 2021 3 14 ting chen simon kornblith mohammad norouzi offrey hinton simple framework contrastive learning visual representation icml 2020 1 3 4 6 8 12 14 15 ting chen simon kornblith kevin swersky mohammad norouzi geoffrey e hinton big model strong learner neurips 33 2020 3 8 16 xinlei chen haoqi fan ross girshick kaiming improved baseline momentum contrastive learning arxiv preprint 2020 3 6 17 xinlei chen kaiming exploring simple siamese resentation learning cvpr 2021 3 4 6 14 18 chen xie empirical study training vision transformer iccv los alamitos ca usa oct ieee computer society 8 19 mircea cimpoi subhransu maji iasonas kokkinos sammy mohamed andrea vedaldi describing texture wild cvpr 2014 7 20 cordonnier aravindh mahendran alexey dosovitskiy dirk weissenborn jakob uszkoreit thomas unterthiner differentiable patch selection age recognition cvpr 2021 3 21 marco cuturi olivier teboul vert ferentiable ranking sorting using optimal transport neurips 2019 2 3 22 alexey dosovitskiy lucas beyer alexander kolesnikov dirk weissenborn xiaohua zhai thomas unterthiner mostafa dehghani matthias minderer georg heigold vain gelly jakob uszkoreit neil houlsby image worth word transformer image recognition scale iclr 2021 8 23 debidatta dwibedi yusuf aytar jonathan tompson pierre sermanet andrew zisserman little help friend contrastive learning visual representation cvpr 2021 1 3 24 aleksandr ermolov aliaksandr siarohin enver sangineto nicu sebe whitening representation learning icml 2021 1 3 6 25 mark everingham luc van gool christopher ki williams john winn andrew zisserman pascal visual object class voc challenge ijcv 88 2 2010 7 26 li rob fergus pietro perona learning ative visual model training example mental bayesian approach tested 101 object category cvpr 2004 7 27 grill florian strub florent e corentin tallec pierre richemond elena buchatskaya carl doersch bernardo avila pires zhaohan guo mohammad laghi azar et al bootstrap new approach learning neurips 33 2020 3 6 28 aditya grover eric wang aaron zweig stefano mon stochastic optimization sorting network via tinuous relaxation iclr 2019 2 3 29 hussein hazimeh zhe zhao aakanksha chowdhery heswaran sathiamoorthy yihua chen rahul mazumder lichan hong ed chi differentiable tion mixture expert application learning neurips 34 2021 3 30 kaiming haoqi fan yuxin wu saining xie ross girshick momentum contrast unsupervised visual resentation learning cvpr 2012 1 3 31 kaiming xiangyu zhang shaoqing ren jian sun deep residual learning image recognition cvpr 2016 6 32 tri huynh simon kornblith matthew r walter michael maire maryam khademi boosting contrastive supervised learning false negative cancellation wacv 2022 3 4 33 sergey ioffe christian szegedy batch normalization accelerating deep network training reducing internal variate shift icml 2015 6 34 donald knuth art computer programming ume 3 sorting searching addison wesley 1998 4 13 35 jonathan krause jia deng michael stark li collecting dataset car 2013 7 36 alex krizhevsky geoffrey hinton et al learning multiple layer feature tiny image 2009 7 37 hyunsung lee sangwoo cho yeongjae jang jaekwang kim honguk woo differentiable ranking metric ing relaxed sorting recommendation ieee access 9 2021 3 38 ilya loshchilov frank hutter sgdr stochastic gradient descent warm restarts iclr 2017 6 39 subhransu maji esa rahtu juho kannala matthew blaschko andrea vedaldi visual fication aircraft arxiv preprint 2013 7 40 antoine miech alayrac lucas smaira ivan laptev josef sivic andrew zisserman learning visual representation uncurated tional video cvpr 2020 1 41 nilsback andrew zisserman automated flower classification large number class 2008 sixth indian conference computer vision graphic image processing page ieee 2008 7 42 aaron van den oord yazhe li oriol vinyals sentation learning contrastive predictive coding arxiv preprint 2018 8 43 omkar parkhi andrea vedaldi andrew zisserman cv jawahar cat dog cvpr ieee 2012 7 44 felix petersen learning differentiable algorithm phd thesis konstanz 2022 2 3 45 felix petersen christian borgelt hilde kuehne oliver deussen differentiable sorting network scalable ing ranking supervision icml 2021 2 3 4 46 felix petersen christian borgelt hilde kuehne oliver deussen learning algorithmic supervision via uous relaxation neurips 2021 2 3 47 felix petersen christian borgelt hilde kuehne oliver deussen monotonic differentiable sorting network iclr 2022 2 3 4 48 felix petersen hilde kuehne christian borgelt oliver deussen differentiable classification learning icml 2022 2 3 49 sebastian prillo julian eisenschlos softsort uous relaxation argsort operator icml 2020 3 50 filip c ahmet iscen giorgos tolias yannis avrithis ondˇ rej chum revisiting oxford paris image retrieval benchmarking cvpr 2018 6 51 alec radford jong wook kim chris hallacy aditya ramesh gabriel goh sandhini agarwal girish sastry amanda askell pamela mishkin jack clark et al ing transferable visual model natural language vision icml 2021 3 52 alec radford jong wook kim chris hallacy aditya ramesh gabriel goh sandhini agarwal girish sastry amanda askell pamela mishkin jack clark gretchen krueger ilya sutskever learning transferable visual model natural language supervision icml 2021 1 53 joshua david robinson chuang suvrit sra stefanie jegelka contrastive learning hard negative sample iclr 2021 2 3 13 54 olga russakovsky jia deng hao su jonathan krause jeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein et al imagenet large scale visual recognition challenge ijcv 115 3 2015 6 55 florian schroff dmitry kalenichenko james philbin facenet unified embedding face recognition tering cvpr 2015 3 12 56 christoph schuhmann romain beaumont richard vencu cade w gordon ross wightman mehdi cherti theo coombes aarush katta clayton mullis mitchell man patrick schramowski srivatsa r kundurthy katherine crowson ludwig schmidt robert kaczmarczyk jenia jitsev open dataset training next generation model neurips datasets benchmark track 2022 1 57 anshul shah suvrit sra rama chellappa anoop cherian contrastive learning proceeding aaai conference artificial intelligence 2022 2 6 58 robin swezey aditya grover bruno charron stefano ermon pirank scalable learning rank via differentiable sorting neurips 2021 3 59 michael taylor john guiver stephen robertson tom minka softrank optimizing rank metric proceeding 2008 international conference web search data mining page 2008 3 60 yonglong tian chen sun ben poole dilip krishnan cordelia schmid phillip isola make good view contrastive learning neurips 2020 1 61 jianxiong xiao james hay krista ehinger aude oliva antonio torralba sun database scene recognition abbey zoo cvpr 2010 7 62 tete xiao xiaolong wang alexei efros trevor rell not contrastive contrastive learning iclr 2021 3 63 hong xuan abby stylianou xiaotong liu robert pless hard negative example hard useful eccv 2020 12 64 mang ye xu zhang pong c yuen chang supervised embedding learning via invariant spreading instance feature cvpr 2019 1 65 yang igor gitman boris ginsburg large batch training convolutional network arxiv preprint 2017 6 14 66 jure zbontar li jing ishan misra yann lecun ephane deny barlow twin learning via redundancy reduction icml 2021 3 4 6 67 fangneng zhan yingchen yu rongliang wu kaiwen cui aoran xiao shijian lu ling shao feature alignment versatile image translation manipulation eccv 2021 3 68 mingkai zheng fei wang chen qian changshui zhang xiaogang wang chang xu ressl relational learning weak augmentation neurips 2021 3 6 supplementary material supplementary material first discus relation groco loss contrastive loss triplet loss tion provide additional experimental evaluation sults section b qualitative analysis section tion describe sorting network finally cover additional implementation detail section discussion groco contrastive triplet loss relation section discus similarity difference tween groco loss contrastive loss triplet loss comparison purpose let consider simplified version loss only one positive example xp one ative example xn anchor xa denote distance anchor xa positive sample xp dp distance anchor xa negative sample xn dn contrastive infonce loss respect anchor xa defined al g ned contr astive r ac e xp 1 aligned 7 τ temperature hyperparameter figure triplet loss defined al ign e r ple r 0 r 0 aligned 8 r margin hyperparameter figure groco loss permutation matrix p sponds only one conditional swap operation defined b egi n sp l b egin align e p 1 1 2 2 f n r ac 1 12 21 f 1 aligned split 9 β inverse temperature therefore groco loss defined n sp lit begin alig n ed l g r co f r ac 1 4 ggl 2 l g g l 1 rctan 2 1 1 1 aligned split 10 β inverse temperature hyperparameter figure figure 5 show loss curve different value spective hyperparameters note simplified example only one positive only one negative three loss try maximize difference distance positive negative example dn temperature τ margin r inverse temperature β define flatness loss curve depending difference dn however case example anchor image different loss integrate information tiple different way triplet loss various strategy sample one positive example one negative example anchor image 55 63 plete loss defined sum average loss chosen triplet p ij max r dn j 0 hand contrastive loss aggregate multiple negative contrasting positive example negative example resulting sum der logarithm log 1 p exp dn contrast explicit sum predefined number negative groco loss aggregate multiple positive negative via tation matrix conditionally swapping neighboring element later applies group ordering supervision enforcing distance positive negative group additional experimental result section provide additional experimental result top negative stop gradient operation analyze using strongest negative stop gradient tion also boost performance considered contrastive learning baseline simclr method stop dient operation stabilizes training fewer spike gradient allows convergence large variation hyperparameters table observe stop gradient doe not boost simclr performance however utilize only strongest atives loss stabilizes simclr training moreover age only negative indeed boost simclr performance k 20 linear probing 1 simclr still significantly underperforms proposed groco method k 20 linear probing longer training ass performance model longer training regime 800 epoch table find proposed model augmentation strategy achieves k 20 linear probing augmentation strategy table evaluate performance model respect different augmentation strategy view sampling follow two setup 1 augmentation egy used simclr 14 method random resized crop color jittering gaussian blur grayscaling horizontal flip 2 augmentation strategy used dino 12 method extends simclr list augmentation larization simclr augmentation considered stronger compared dino augmentation since include larger range cropping size 8 original image compared 14 dino augmentation larger range value color jittering observe stronger simclr 14 mentation beneficial simclr method weaker dino augmentation proposed method dino augmentation strategy beneficial however ference augmentation strategy diminishes ing number training epoch no longer measurable 400 log 1 exp dn 2 1 0 1 2 dn dp 0 5 10 15 20 loss lcontrastive lcontrastive lcontrastive 1 contrastive infonce loss max r dn 0 2 1 0 1 2 dn dp 0 1 2 3 4 loss ltriplet r ltriplet r ltriplet r 2 b triplet loss 1 π arctan β dn 2 1 0 1 2 dn dp 0 1 2 3 loss lgroco lgroco 1 lgroco 4 c groco loss figure comparison contrastive loss triplet loss groco loss simple scenario only one positive example one negative example anchor image denote distance anchor positive sample dp distance anchor negative sample dn note simple case only one positive one negative three loss try maximize difference distance positive negative example dn temperature τ margin r inverse temperature β define flatness loss curve depending difference dn epoch fair comparison use simclr augmentation strategy reproduction simclr method reported main paper projection dimensionality also ablate method respect dimensionality projection space latent space distance sample computed late training loss table show increasing dimensionality projection space increase performance general noticeable performance note not change dimensionality embedding space output space encoder used evaluation linear uation always importance negative also evaluate importance utilizing strong negative successful training model train model using ten random negative instead 10 strongest negative negative group report performance table observe leveraging strongest negative increase performance across metric demonstrating portance hard negative training groco loss similarly contrastive loss benefit hard negative pling 53 projection space also evaluate mance projection space latent space ing loss applied compare performance jection representation space table observe method performance higher use embeddings representation space even though train model compare embeddings projection space could plained fact embedding space contains general image representation since representation projection space could overfitted respective augmentation come agnostic some image attribute like color since train model match view different color jittering ters qualitative analysis learned tation space additionally perform qualitative analysis learned representation figure 7 visualize representation age four class different type cat four class different type dog find method produce much visually separable cluster respect tions cat v dog variation different class cat simclr baseline sorting network sorting network sort sorting algorithm classic computer science literature 34 sorting network network sorting family sorting rithms consist fixed sequence comparison sense next comparison element position pared doe not depend result previous comparison sorting network simple example family algorithm sorting network compare neighbored element starting odd even index alternating step requires n step sort sequence n element present pseudocode sorting network rithm additionally illustrate hard sorting process figure algorithm 1 pseudocode sorting network sorting array number order arr array sort n length array range 1 n 1 range 0 n 1 2 arr arr arr arr arr arr else range 1 n 1 2 arr arr arr arr arr arr evaluation linear probing infonce simclr method infonce grad infonce top 10 neg unstable training infonce top 10 neg grad groco top 10 neg unstable training groco top 10 neg grad usage negative stop gradient operation method view epoch evaluation linear eval groco 800 groco 800 b longer training method augmentation epoch evaluation linear evaluation simclr simclr 12 100 simclr dino 12 100 groco simclr 12 100 groco dino 12 100 groco simclr 12 200 groco dino 12 200 groco simclr 12 400 groco dino 12 400 c augmentation strategy projection dim embedding dim evaluation linear probing 128 2048 512 2048 2048 2048 projection dimentionality evaluation linear probing 10 random negative strongest negative e importance negative method space evaluation simclr projection space simclr representation space groco projection space groco representation space f evaluation table additional experiment best result bolded option used obtain main result highlighted 2 4 1 6 6 1 1 6 4 2 2 4 1 2 4 6 1 2 4 6 step 1 step 2 step 3 step 4 input 6 4 2 1 output 1 2 3 4 figure illustration hard sorting network sorting four element order example sorting 6 1 4 2 array implementation detail linear evaluation detail linear evaluation train linear classifier frozen resentations way using training set agenet training validation set evaluation follow training protocol simclr 14 simsiam 17 train linear classifier 90 epoch using lars optimizer 65 batch size 4 096 momentum linear rate following rule learning rate batch without warmup weight decay following 14 17 use weak data augmentation only random cropping izontal flipping apply gradient stopping input classifier prevent updating encoder simclr multiple positive train simclr one positive view per anchor apply contrastive loss possible positive pair ing view image batch negative batch b example negative view let xb denote th view b th image batch pxb denote set positive sample anchor xb nxb denote set positive sample anchor xb loss calculated n l ign e h cal l sim c lr ra c 1 b b frac 1 1 z z aligned 11 τ temperature parameter extension clr framework 2 view per image used swav evaluation 11 note scenario use only global view positive example lowing correspondence idea 11 12 simclr b groco figure visualization learned representation imagenet validation image four class different type cat egyptian cat persian cat siamese cat tabby cat four class different type dog pomeranian dog african hunting dog tibetan mastiff english setter simclr method proposed method visualization use model encoder trained 100 epoch batch size 1024 2 224 view