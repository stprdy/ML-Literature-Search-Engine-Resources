1 distributed learning using vision transformer sangjoon park lee jun kim jong chul ye fellow ieee recent success deep learning ﬁeld medicine issue data scarcity exacerbated concern privacy data ownership distributed learning approach including federated learning investigated address issue however hindered need cumbersome communication overhead weakness privacy protection tackle challenge propose masked sampling distillation method vision transformer method implemented without continuous communication enhance privacy utilizing vision encryption technique conducted extensive experiment two different task demonstrated effectiveness method achieved superior mance compared existing distributed learning strategy well only baseline furthermore since supervised model created using proposed method achieve general semantic understanding image demonstrate potential foundation model various downstream task thereby expanding applicability medical domain index learning learning random permutation vision transformer privacy protection introduction eep learning ha established standard developing artiﬁcial intelligence ai medical tool however reliance data label necessitates collaboration among multiple institution unfortunately strict regulation often impede free sharing data due privacy concern 1 2 address challenge distributed learning method federated learning fl 3 introduced alleviate issue related data governance ownership method enable sharing data collaborator formal consent overcoming one signiﬁcant obstacle ai research fl objective train model preserving training data multiple edge device global model distributed central server client performs training iteration data parallel local update obtained sent back server conduct parallel training iteration using submitted april 12 2023 sangjoon park department bio brain engineering korea advanced institute science technology kaist daejeon republic korea department radiation oncology yonsei university college medicine seoul republic korea depecher lee jun kim department radiation oncology gangnam severance hospital seoul republic korea junwon jong chul ye kim jaechul graduate school ai kaist daejeon republic korea jong chul ye jun kim author local data resulting local update transmitted back server aggregate average disseminating updated global model iterative process continues model converges despite resolving data sharing issue fl doe not guarantee complete privacy private data compromised inversion attack reconstruct private data utilizing stolen gradient insecure aggregation 4 furthermore fl imposes signiﬁcant computational load edge device computation model update performed locally 5 additionally practical implementation fl introduces considerable communication overhead entire typically model need aggregated distributed server client recently vision community ha introduced vision transformer vit deep learning model exclusively relies attention mechanism 6 vit powerful yet simple attention architecture ha made indispensable tool vision research recent study revealed vit exhibit nature akin human le susceptible perturbation occlusion random patch permutation 7 furthermore various learning ssl method proposed vit including knowledge semantic meaning learning 8 masked image modeling 9 10 building upon valuable characteristic vit propose novel approach distributed ssl facilitates creation model only single round communication server client approach utilizes permutation invariant property provide encryption via random permutation proposed previous work 11 furthermore leverage another important property vit stem based image processing enable random masked based ssl train model solely enables fl without requiring continuous interaction server client ii related work learning vit caron et al 8 presented groundbreaking contrastive learning method called distillation without label dino vit enables model learn semantic meaning image without need cumbersome negative sample traditional contrastive learning method accomplished use knowledge distillation random strategy facilitates 15 apr 2023 2 correspondence learning attention visualization revealed method especially effective vit attention architecture vit ha also applied ssl using masked image modeling approach approach akin masked language modeling approach utilized bidirectional encoder resentations transformer bert natural language processing work bao et al 9 presented bert leverage discrete visual token obtained discrete tokenizer predict masked patch et al 10 hand proposed simpler masked autoencoder mae strategy employ efﬁcient design directly predict pixel within masked patch important note strategy tailored image processing vit may not suitable based model employ shared convolution kernel across image medical imaging some pioneering work attempted combine ssl approach vit fl address challenge label insufﬁciency wu et al 12 performed fl via contrastive learning mae using unlabeled data label demonstrating superior performance compared random initialization local ssl similarly yan et al 13 proposed fl approach divided training process two stage utilizing mae approach fl local client ﬁrst stage performed supervised fl labeled data second stage thereby mitigating issue data heterogeneity label insufﬁciency fl federated learning vit several pioneering study investigated effective use vit distributed learning environment 11 14 16 qu et al 15 recently demonstrated based vit architecture robust fl among client heterogeneous data compared model federated split festa learning method wa developed based modular structure vit model consists embedder head transformer body tail combining strength two different distributed learning method fl split learning enhance performance individual task facilitating collaboration client different task 16 recently new method called federated split learning permutating pure vit ha introduced address limitation festa 11 method utilizes property vit employing random patch permutation enhance privacy reduce tion overhead primary goal method reduce communication server client enhance privacy using permutation module achieved leveraging permutation invariant property core component vit model trained permutated patch feature feature space using innovative permutation module thereby providing privacy protection preventing fig overall framework described follows 1 participating client utilize arbitrary patch embedder permutation module encrypt patch feature prior transmitting server 2 performs training achieve overall semantic understanding modality ultimately producing model 3 model accessible authorized client various downstream task malicious attacker reconstructing private data intermediate feature permutated feature securely stored memory used throughout entire learning process reducing burden approximately 50 compared original festa method however method ha certain limitation restrict general applicability firstly continuous communication server client required model training label necessary update shared transformer body secondly learning only feasible among client concurrently participating distributed learning relevant task lastly method supervised learning approach manually labeled data required hard obtain medical domain iii method introduces masked sampling distillation no label method designed distributed ssl method involves three main step depicted figure 1 initially arbitrary patch embedder employed extract patch feature data along arbitrary position embedding feature randomly permuted using permutation module quently resulting encrypted patch feature stored random masked feature ssl performed independently resulting vit model equipped semantic feature extraction capability finally authorized client access model apply downstream task leading improved performance compared baseline without ssl 3 fig ﬁgure presented illustrates comparison distillation no label dino trained federated learning fl method masked sampling distillation no label method original dino method model acquires general semantic understanding image way correspondence student momentum teacher b contrast method teach model correspondence smaller larger number training randomly permuted patch feature c learning ssl via fl server aggregate distributes weight update client every round necessitating continuous communication contrast proposed method utilizes permutation module receive encrypted feature subsequently employing feature throughout ssl process result training accomplished only communication server client ema exponential moving average rms random masked sampling learning method ssl process illustrated figure method involves extracting patch feature f arbitrary patch feature extractor f subsequently permuting using permutation module permute deﬁned f permute f x image data x similar approach proposed festa 11 encrypted feature transmitted client server stored server memory following subsequent learning process exclusively conducted thus eliminating need additional communication computational overhead f permutate f x 1 ensure encryption feature patch embedder position embedding unknown server any outside attacker employed conjunction space permutation module result server unable accurately reconstruct private data transmitted feature detail regarding formulation experimental outcome pertaining privacy preservation discussed section server employ random masked sampling conduct ssl utilizing encrypted feature participating client achieved modifying original dino correspondence learning strategy large patch feature correspondence depicted figure motivation dino illustrated figure 2 imbue model visual semantics image via knowledge distillation speciﬁcally large crop global view presented teacher multiple small crop local view provided student adapt concept random masked sampling utilized replacing global view sampling large number patch feature local view sampling smalle number patch feature illustrated figure 2 b speciﬁc permuted feature f obtained image feature fl created randomly sampling majority permuted patch feature feature f generated randomly sampling relatively smaller number patch similar dino feature fl fed teacher fl f presented student subsequently student optimized match prediction momentum teacher utilizing relatively small information image accordance correspondence strategy dino assuming sampled feature fl composed large number permuted patch feature sampled feature f composed small number permuted patch feature deﬁne set sample containing differently sampled fl vl f 1 l f l n differently sampled f v f 1 f n let teacher student model denoted gθt gθs respectively let loss denoted student trained mimic teacher 4 prediction following optimization min gθs x x vl l gθs f gθt f 2 learning process momentum teacher model updated exponential moving average ema student update λ follows cosine scheduling gθt λgθt 1 gθs 3 algorithm preparing encrypted feature set conducting learning formally presented algorithm given limited data availability single client model may yield improved generalization performance investigated section algorithm 1 proposed algorithm run client c 1 function clientmain 2 client initialize arbitrary feature embedder f 3 data x 1 2 x 4 f permute f x 5 feature set f fx 6 return f run main server 7 function servermain 8 server initializes student gθs teacher gθt 9 client c 1 2 c parallel 10 f c c 11 memory f 1 f 2 f c c save feature fc memory 12 epoch e 1 2 e 13 feature f run learning server 14 ldino p p l gθs f gθt f 15 θs n update student 16 θt λθt 1 θs ema teacher 17 client c 1 2 c parallel 18 distribute gθ authorized client distribute model task interest resulting ssl model accessible authorized user speciﬁc purpose illustrated figure instance client utilize model foundation improve generalization performance training model oar segmentation radiotherapy planning leveraging ability attend visual semantics within image provide clarity let u consider client c employ model backbone denoted g layer decoder denoted hc fig potential attack scenario presence permutation module involves attacker optimize space jigsaw solver attacker encoder model jointly make challenging achieve successful reconstruction private data along data label represented xc yc respectively additionally let u assume use loss function denoted lc optimization problem formulated follows min g h x lc hc g xc yc 4 protecting privacy permutation module random patch permutation feature space ha previously examined technique 11 since embedded feature kept potential risk privacy infringement curious server malevolent attacker intercept feature transmission endeavor reconstruct original data feature suppose attacker intercept encrypted feature permutation ha adequate access public data domain case would need solve two problem simultaneously 1 training feature extractor embed image feature space identically unpermutated version encrypted feature 2 training jigsaw solver unshufﬂe encrypted feature original order feature space intercepted feature not image space precise let ˆ f represent model permutated original feature embedded fpub fpub respectively let denote number image publicly available computed tomography ct scan hijacked encrypted feature fpriv embedded arbitrary feature embedder f number feature model ˆ f discriminator decoder g trained simultaneously optimizing following two learning objective min ˆ f max x n x log 1 j f priv log j f j pub 5 5 table data learning client 1 client 2 client 3 client 4 source chum chus hmr hgj no patient 21 30 30 30 no slice min g x ldecoder g j f pub x pub 6 ldecoder denotes reconstruction loss decoder second optimization problem jigsaw solve j formulated follow min j x ljigsaw j f pub f pub 7 ljigsaw denotes similarity loss feature space noted simultaneous solution ﬁrst two equation eq 5 eq 6 exact jigsaw solver unraveled obtained eq 7 successfully solved however jigsaw solver trained utilized feature space hijacked encrypted feature necessitates knowing correct solution attacker model ˆ f embed feature space conversely target optimization problem eq 5 combined optimization eq 5 eq 6 eq 7 requires already solution indicating problem underdetermined challenging solve practical scenario experimental ﬁndings support assertion sented section iv result utilized proposed method ct imaging assessed efﬁcacy two downstream task namely oar segmentation intracranial hemorrhage ich detection detail dataset simulate distributed learning utilized cancer imaging archive tcia dataset 17 consists ct data obtained four distinct institution created collaborative setting four institution namely centre hospitalier de l université de montréal chum centre hospitalier taire de sherbrooke chus hôpital hmr de montréal hôpital général juif hgj de montréal participated independent client table provides overview institution practical implementation user typically single institution usually acquires weight tune speciﬁc task however limited availability data label common challenge setting address issue conducted experiment two different scenario full limited additionally evaluated generalizability table ii data evaluate downstream task task amount data test full limited oar segmentation no patient 38 8 7 no slice 608 521 ich detection no patient 67 25 15 no slice 904 476 proposed ssl approach two different downstream task namely oar segmentation ich detection downstream oar segmentation task used medical image computing computer assisted intervention miccai 2015 head neck challenge dataset 18 tune model dataset consisted ct slice label 38 patient patient data utilized setting 8 patient data 608 ct slice used setting downstream ich detection task used computed tomography image intracranial hemorrhage detection segmentation dataset 19 includes brain window ct image 82 patient randomly splitting test set patient used 67 image patient setting 15 image 904 image setting evaluation oar segmentation performance ct roi data meticulously collected delineated radiation oncologist external institution gangnam severance hospital utilized data 44 head neck cancer patient received radiation therapy 2007 2021 collected seven patient data containing roi head neck oar used evaluation evaluate ich detection performance randomly split subset consisting 476 image 15 patient wa obtained ich detection dataset implementation detail ct image preprocessed cropping central area size 224 224 original size 512 data came different source pixel spacing wa adjusted match datasets employed patch embedder dino model wa trained imagenet arbitrary feature embedder embedded feature subjected permutation using permutation module proposed previous work 11 resulting encrypted feature remaining learning process transformer component utilized transformer 6 head 12 layer patch size component wa initialized supervised dino weight imagenet 8 used teacher student model identical size obtain global view sampled large number patch feature leading sampling ratio ranging multiple local view sampled small number patch feature sampling ratio compared 6 approach original dino implementation adopted conﬁguration crop size global local view respectively given relatively smaller size dataset used experiment reduced complexity medical image opted decrease dimension dino head output ssl using proposed method employed adam w optimizer batch size 8 cosine decay scheduler maximum learning rate device model wa trained 50 epoch case ssl dino method fl used optimizer scheduler learning rate batch size 4 per client model wa trained 50 federated round device match total number update learning fl server aggregated averaged distributed student teacher model parameter every round downstream task oar segmentation employed vit backbone upernet 20 decoder encoder decoder part segmentation model following implementation described previous work 10 baseline comparison also used two recent model 20 stdc 21 decoder wa designed perform segmentation various roi including brainstem optic chiasm mandible optic nerve parotid gland submandibular gland used combined focal loss dice loss following prior work 22 optimize model segmentation model employed sgd optimizer learning rate batch size downstream task classiﬁcation added simple linear layer classiﬁcation head similar segmentation two recent model resnext 23 convnext 24 also implemented comparison formulated ich detection task binary classiﬁcation optimized model using bce loss used adam w optimizer learning rate batch size 16 model classiﬁcation task experiment conducted using python pytorch nvidia rtx utilized flower framework 25 simulating distributed learning detail evaluation order ass segmentation performance dice similarity coefﬁcient dsc ha utilized measure degree overlap predicted segmentation mask corresponding ground truth label evaluate detection performance area receiver operating characteristic curve auc ha computed sensitivity speciﬁcity accuracy also calculated statistically compare result strap random sampling method wa employed sample size evaluation set randomly drawn replacement 1000 time conﬁdence interval calculated based relative frequency distribution estimate sample interval percentile used determine range fig attention visualization presented obtained attention learning compared dino imagenet weight attention head observed become reﬁned concentrate various semantic component ssl learning simulation feature inversion attack evaluated whether permutation model effective prevent privacy attack feature hijacking supposing optimal conﬁguration malicious attacker accordance prior work encryption using random patch permutation 11 wa assumed attacker ha gained access encrypted feature transmitted client ha exact knowledge permutation ratio patch size architecture unknown patch embedder dimensionality position embedding achieve architecture original patch embedder wa utilized feature embedder discriminator decoder employed discriminator layered generator dcgan 26 respectively solve random permutation feature space transformer 12 encoder layer 12 attention head wa employed jigsaw solver discriminator wa optimized modiﬁed version gan loss formulated eq 5 6 learning objective decoder consisted combined loss objective jigsaw solver wa loss furthermore wa assumed attacker ha access substantial amount data domain publicly available train network utilized ct slice tcia data 27 model underwent training total 5 epoch using batch size 1 learning rate attention change learning figure 4 illustrates change last layer head attention within vit model prior ssl attention vit model pretrained imagenet dispersed throughout image upper minimal difference attention individual head suggesting many attention head processing image redunduntly however performing ssl different head began attending various semantic component lower potentially leading improved performance diverse pattern 7 table iii segmentation result setting method overall brainstem chiasm mandible optic n optic n rt parotid parotid rt smg smg rt 20 stdc 21 no ssl proposed ssl learning dc fl federated learning table iv segmentation result setting method overall brainstem chiasm mandible optic n optic n rt parotid parotid rt smg smg rt 20 stdc 21 no ssl proposed ssl learning dc fl federated learning table v classification result setting method auc sensitivity speciﬁcity accuracy resnext 23 convnext 24 vit model no ssl proposed ssl learning dc fl federated learning performance comparison downstream oar segmentation table iii iv present comparison result oar segmentation task among method model weight demonstrated performance superior model trained scratch notably supervised model obtained using proposed method performed par model obtained centralized ssl also outperforming model obtained using fl figure 5 present qualitative comparison among different method speciﬁcally model ssl using proposed method yielded accurate prediction oar area compared baseline model wa only moreover performance table vi classification result setting method auc sensitivity speciﬁcity accuracy resnext 23 convnext 24 vit model no ssl proposed ssl learning dc fl federated learning model wa comparable model trained manner outperformed fl performance comparison downstream ich detection table v vi present comparison result different method ich detection task result show model weight generally outperformed baseline trained scratch furthermore model obtained proposed method demonstrated comparable performance learning formed fl among model notably 8 fig qualitative comparison segmentation result model tuned model using method displayed comparable result obtained dino method outperforming no learning ssl baseline result obtained setting proposed method demonstrated remarkable beneﬁt setting highlighting potential improve performance limited data communication cost specifying number data total number training round r round aggregation distribution r model parameter p size encrypted feature data f total communication cost ssl fl expressed follows tfl r p 8 f p 9 constant 4 multiplied account way parameter transmission teacher student model transmission server client fl method proposed study doe not necessitate continuous communication instead table vii comparison communication overhead method method no round communication overhead total feature weight learning 1 federated learning 50 only communication occurs outset involves transmission encrypted feature server client subsequent model download client end learning process approach result signiﬁcant reduction communication overhead illustrated figure 2 b table vii present numerical comparison result proposed method requires communication cost only 25 required fl moreover advantage ampliﬁed number total learning round epoch increase effect permutation module given permutation module one key component proposed method performed experiment examine effect privacy protection downstream performance 1 effect privacy protection table viii fig 6 present qualitative quantitative analysis result reconstruction privacy attack examine role permutation module privacy protection performance result show without permutation module optimal conﬁguration attacker private data reconstructed extent privacy attribute shape anatomic location disease status subject inferred suggests unknown feature embedder approximately solved attacker however permutation module almost impossible reconstruct data extent privacy inferred result conﬁrm claim solving two optimization problem simultaneously need solution considered underdetermined problem practically difﬁcult solve 2 effect performance table ix present tion performance oar segmentation ich detection without permutation module result reveal no statistically signiﬁcant difference performance task according use permutation module ﬁndings provide perimental evidence supporting patch property vit discussion deep vision model exhibited impressive performance however learning paradigm pose practical challenge developing ai model healthcare research training data may contain sensitive personal information 28 additionally label dependency 9 fig reconstruction result privacy attack without space permutation module some degree information level ct slice presence disease etc inferred reconstructed result wa infeasible permutation module wa utilized table viii effect permutation module privacy protection permutation module metric mse ssim mse mean squared error ssim structural similarity index table ix effect permutation module performance oar segmentation ich detection overall dsc auc limited permutation no permutation full permutation no permutation oar ich intracranial hemorrhage dsc dice similairty coefﬁcient auc area curve present another challenge label annotated medical expert expensive difﬁcult obtain address challenge distributed learning method introduced enable model training without directly sharing private data ssl method investigated alleviate label dependency however combining technique ha led suboptimal performance compared approach 29 furthermore fl ha inherent property namely model aggregation averaging tion central server multiple round communication server client potentially compromising privacy enabling attack malicious attacker 30 address aforementioned issue propose dino novel ssl method used distributed manner only communication achieving performance comparable ssl leverage property semantic learning via correspondence using knowledge distillation inspired intriguing property vit 7 speciﬁcally replace strategy random masked sampling strategy consistent pioneering ssl approach use masked image modeling vit manner 9 10 additionally incorporate random space permutation module 11 enhance privacy well enabling distributed learning without compromising performance enables encrypted patch feature storted memory allowing ssl based masked random sampling patch feature performed device preserving privacy participating subject previous work 31 33 investigated use permutation patch data encryption however work apply permutation image relatively easily solved model like jigsaw solver sufﬁcient data similar property available 34 contrast method permutates patch position embedded feature feature space rendering challenging solve moreover little research ha examined use random patch permutation conjunction property vit recently several study proposed method combine vit ssl mae fl 12 13 however ssl mae requires direct calculation error predicted original pixel value making infeasible apply random patch permutation encryption therefore study simply combine mae fl training vit model device mae ssl aggregating weight existing fl method unlike approach leverage unique property vit study ha several limitation firstly proposed dino method ha dependency intrinsic invariant property transformer limit use model not property architecture secondly study ha demonstrated patch permutation enhances privacy still prone malicious attack distributed learning model data poisoning 35 37 membership inference attack 38 41 fall outside scope research proposed method incorporate additional technique privacy protection 42 thirdly study ha not accounted practical consideration data imbalance straggler problem 5 lastly direct comparison proposed method existing distributed learning method festa wa not possible designed supervised learning whereas proposed method designed ssl model vi conclusion given limited data label availability tance privacy health research method ha great 10 promise enable ssl only distributed learning privacy protecting way since model obtained proposed ha general understanding visual semantics used agnostic model enhance performance any downstream task suggesting widespread applicability medical imaging acknowledgement research wa supported part grant scientist training program korea health industry development institute khidi funded ministry health welfare republic korea national research foundation korea nrf grant funded korean government ministry science ict faculty research grant yonsei university college medicine national research foundation korea grant kaist key research institute interdisciplinary research group project reference 1 hoofnagle van der sloot borgesius european union general data protection regulation mean information communication technology law vol 28 no 1 pp 2019 2 edemekong annamaraju haydel health insurance portability accountability act 2018 3 koneˇ cn mcmahan yu richtárik suresh bacon federated learning strategy improving communication efﬁciency arxiv preprint 2016 4 geiping bauermeister dröge moeller inverting easy break privacy federated learning advance neural information processing system vol 33 pp 16 947 2020 5 li sahu talwalkar smith federated learning challenge method future direction ieee signal processing magazine vol 37 no 3 pp 2020 6 dosovitskiy beyer kolesnikov weissenborn zhai unterthiner dehghani minderer heigold gelly et image worth word transformer image recognition scale arxiv preprint 2020 7 naseer ranasinghe khan hayat shahbaz khan yang intriguing property vision transformer advance neural information processing system vol 34 pp 23 308 2021 8 caron touvron misra jégou mairal bojanowski joulin emerging property vision transformer arxiv preprint 2021 9 bao dong wei beit bert image transformer arxiv preprint 2021 10 chen xie li dollár girshick masked autoencoders scalable vision learner proceeding conference computer vision pattern recognition 2022 pp 16 009 11 park ye distributed learning using vision transformer random patch permutation ieee transaction medical imaging pp 2022 12 wu zeng wang sheng yang james shi hu federated contrastive learning masked autoencoder dermatological disease diagnosis arxiv preprint 2022 13 yan qu wei huang shen rubin xing zhou federated learning tackling data heterogeneity medical imaging ieee transaction medical imaging 2023 14 kim kim ye vision transformer distributed learning image processing ieee transaction image processing vol 32 pp 2022 15 qu zhou liang xia wang adeli rubin rethinking architecture design tackling data heterogeneity federated learning proceeding conference computer vision pattern recognition 2022 pp 10 071 16 park kim kim kim ye federated split agnostic vision transformer cxr diagnosis advance neural information processing system vol 34 pp 24 630 2021 17 vallieres perrin liem furstoss aerts khaouam wang sultanem et radiomics strategy risk assessment tumour failure neck cancer scientiﬁc report vol 7 no 1 10117 2017 18 computing intervention head neck auto tation miccai challenge 2015 accessed 19 hssayeni croock salman yahya ghoraani computed tomography image intracranial hemorrhage detection segmentation intracranial hemorrhage segmentation using deep convolutional model data vol 5 no 1 14 2020 20 xiao liu zhou jiang sun uniﬁed perceptual parsing scene understanding proceeding european conference computer vision eccv 2018 pp 21 fan lai huang wei chai luo wei rethinking bisenet semantic segmentation proceeding conference computer vision pattern recognition 2021 pp 22 zhang liu li wang rethinking dice loss deep learning lesion segmentation medical image journal shanghai jiaotong university science vol 26 pp 2021 23 xie girshick dollár tu aggregated residual transformation deep neural network proceeding ieee conference computer vision pattern recognition 2017 pp 1500 24 liu mao wu feichtenhofer darrell xie convnet proceeding conference computer vision pattern recognition 2022 pp 11 986 25 beutel topal mathur qiu parcollet de gusmão lane flower friendly federated learning research framework arxiv preprint 2020 26 radford metz chintala unsupervised representation learning deep convolutional generative adversarial network arxiv preprint 2015 27 clark vendt smith freymann kirby koppel moore phillips mafﬁtt pringle et cancer imaging archive tcia maintaining operating public information repository journal digital imaging vol 26 no 6 pp 2013 28 perone promise limitation deep learning medical image segmentation j med artif intell vol 2 no 1 pp 2019 29 makhija ho ghosh federated learning heterogeneous client arxiv preprint 2022 30 hatamizadeh yin molchanov myronenko li dogra feng flores kautz xu roth gradient inversion attack make federated learning unsafe ieee transaction medical imaging pp 2023 31 kawamura kinoshita kiya machine learning using etc image international workshop advanced imaging technology iwait 2020 vol spie 2020 pp 206 32 ivan convolutional neural network randomized cvpr workshop 2019 pp 33 sharma chen image disguising deep learning proceeding 2018 acm sigsac conference computer communication security 2018 pp 34 noroozi favaro unsupervised learning visual tions solving jigsaw puzzle european conference computer vision springer 2016 pp 35 lyu yu yang threat federated learning survey arxiv preprint 2020 36 tolpegin truex gursoy liu data poisoning attack federated learning system european symposium research computer security springer 2020 pp 37 bagdasaryan veit hua estrin shmatikov backdoor federated learning international conference artiﬁcial intelligence statistic pmlr 2020 pp 38 gupta stripelis lam thompson ambite ver steeg membership inference attack deep regression model 11 neuroimaging medical imaging deep learning pmlr 2021 pp 39 shokri stronati song shmatikov membership inference attack machine learning model 2017 ieee symposium security privacy sp ieee 2017 pp 40 nasr shokri houmansadr comprehensive privacy analysis deep learning passive active inference attack centralized federated learning 2019 ieee symposium security privacy sp ieee 2019 pp 41 zou zhang backes zhang privacy analysis deep learning wild membership inference attack transfer learning arxiv preprint 2020 42 lyu yu sun zhao yang yu privacy robustness federated learning attack defense arxiv preprint 2020