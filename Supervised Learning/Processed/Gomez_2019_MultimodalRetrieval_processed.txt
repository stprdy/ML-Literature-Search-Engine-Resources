learning web data multimodal retrieval raul gomeza b lluis gomezb jaume giberta dimosthenis karatzasb aeurecat centre tecnol ogic de catalunya unitat de tecnologies audiovisual barcelona spain bcomputer vision center universitat aut onoma de barcelona barcelona spain abstract learning multimodal image text data allows deep neural network learn powerful feature no need human annotated data web social medium platform provide virtually unlimited amount multimodal data work propose exploit free available data learn multimodal image text embedding aiming leverage semantic knowledge learnt text domain transfer visual model semantic image retrieval demonstrate proposed pipeline learn image associated text without supervision analyze semantic structure learnt joint image text ding space perform thorough analysis performance comparison ﬁve different state art text embeddings three different benchmark show embeddings learnt web social medium data competitive performance supervised method text based image retrieval task clearly outperform state art mirflickr dataset training target data demonstrate semantic multimodal image retrieval performed using learnt embeddings going beyond classical retrieval lem finally present new dataset composed instagram image associated text used fair comparison embeddings keywords learning webly supervised learning text embeddings multimodal retrieval multimodal embedding introduction annotating data bottleneck training deep neural network large annotated datasets powerful hardware deep learning technique allowing get outstanding machine learning result not only traditional classiﬁcation problem also challenging task image captioning language translation deep neural network allow building pipeline learn pattern any kind data impressive result deep learning ha two strong requirement computation power ton data tation power requirement fulﬁlled gpus ai specialized hardware tpus author email address raul gomez lgomez lluis gomez jaume gibert dimos dimosthenis karatzas preprint submitted scene understanding january 9 2019 7 jan 2019 moreover hardware power evolving fast without apparent roof together deep ing algorithm requirement story data requirement different despite tence annotated datasets imagenet 1 coco 2 place 3 lack data limit application deep learning speciﬁc problem difﬁcult ically get proper annotation although exist some tool facilitate human data annotation amazon mechanical annotating ton data required train supervised deep learning model expensive manual task whose efﬁciency not evolve time alternative annotated data common strategy overcome lack annotated data ﬁrst train model generic datasets imagenet task using smaller speciﬁc datasets 4 still depend existence annotated data train model another strategy overcome insufﬁciency data use computer graphic technique generate artiﬁcial data inexpensively however synthetic data ha proven valuable source training data many application pedestrian detection 5 image semantic segmentation 6 scene text detection recognition 7 8 nowadays still not easy generate realistic complex image some task alternative strategy solution overcome annotated data requirement supervised deep learning technique not fully supervised technique among supervised learning exploit multimodal data learn relation two data ities using paired instance web social medium offer immense amount image panied information image title description date data noisy unstructured free nearly unlimited mentioned data annotation efﬁciency doe not improve time contrast amount available data web doe designing algorithm learn web data interesting research area would disconnect deep learning evolution scaling datasets given enormous amount existing web social medium data call scenario learning cause consists exploiting relation different modality case image text multimodal data supervision exploiting multimodal web data lately web data ha used build classiﬁcation datasets webvision lenge 9 facebook work 10 work build classiﬁcation dataset query made search engine using class name retrieved image labeled ing class conﬁguration learning limited some class thus could not generalize new class working image label convenient ing traditional visual model semantics discrete space limited comparison richness human language expressiveness describing image instead ﬁne scenario exploiting distributional semantics given text corpus 1 2 learn every word associated image illustrated figure 1 leveraging richer semantics encoded learnt embedding space infer previously unseen concept even though might not explicitly present training set noisy unstructured text associated web image provides information image content use learn visual feature strategy embed multimodal data image text vectorial space work represent text using ﬁve different state art method eventually embed image learnt semantic space mean regression cnn compare performance different text space conﬁgurations text based image retrieval task haircut haircut woman haircut man haircut man figure 1 result combined text query semantic image retrieval model learnt joint text embedding permit learn rich semantic manifold even previously unseen concept even though might not explicitly present training set related work multimodal image text embeddings lately active research area possibility learning together different kind data motivated ﬁeld study general applied research ha done devise 11 proposes pipeline instead learning predict imagenet class learns infer 12 representation label result model make semantically relevant prediction even make error generalizes class outside labeled training set gordo larlus 13 use caption associated image learn common embedding space image text perform semantic image retrieval use based bow representation 3 image caption semantic similarity measure image train cnn minimize margin loss based distance triplet image gomez patel et al 14 15 use lda 16 extract topic probability bunch wikipedia article train cnn embed associated image topic space wang et al 17 propose method learn joint embedding image text retrieval training neural net embed space 12 text representation cnn extracted feature semantic retrieval joint embeddings also used ciﬁc application patel et al 18 use lda 16 learn joint embedding generate contextualized lexicon image using only visual information gordo et al 19 bed word image semantic space relying graph taxonomy provided wordnet 20 perform text recognition speciﬁc application salvador et al 21 propose joint bedding food image recipe identify ingredient using 12 lstm representation encode ingredient name cooking instruction cnn extract visual feature associated image exploiting instagram publication related barcelona gomez et al 22 learn relation word image barcelona neighbourhood study word visual feature tourist local relate neighbourhood robustness noisy data ha also addressed community though usually implicit way patrini et al 23 address problem training deep neural network label noise loss correction approach xiau et al 24 propose method train network limited number clean label million noisy label fu et al 25 propose image tagging method robust noisy training data xu et al 26 address social image tagging correction completion zhang et al 27 show label noise affect cnn training process generalization error contribution work presented brings performance comparison ﬁve state art text embeddings learning showing result three different datasets furthermore prof multimodal learning applied web social medium data achieving competitive result image retrieval compared pipeline trained human annotated data finally new dataset formed instagram image associated text presented multimodal embedding one objective work serve fair comparative different text embeddings method learning web social medium data therefore design pipeline test different method condition text embedding module replaced any text representation proposed pipeline follows first train text embedding model dataset composed pair image correlated text x second use text embedding model generate vectorial representation text given text instance x denote embedding φ x third train cnn regress text embeddings directly 4 correlated image given image representation embedding space denoted ψ thereby cnn learns embed image vectorial space deﬁned text embedding model trained cnn model used generate visual embeddings test set image figure 2 show diagram visual embedding training pipeline retrieval procedure image retrieval stage vectorial representation joint space querying text computed using text embedding model image query also handled using visual embedding model instead text embedding model generate query representation furthermore generate complex query combining different query sentations applying algebra joint space retrieve semantically similar image ir query xq compute cosine similarity vectorial representation φ xq visual embeddings test set image ψ retrieve nearest image joint space arg min xq ψ xq 1 state art text embedding method trained large text corpus good generating representation text vector space semantically similar concept fall close proposed pipeline leverage semantic structure text embedding space training visual embedding model generates vectorial representation image space mapping semantically similar image close also close text correlated image content note proposed joint embedding extended task besides image retrieval image annotation tagging captioning text embedding training old car retrieval text embedding visual embedding gt semantic representation old car image text old car ᶪ ᶰ x ᶰ xq xq x figure 2 pipeline visual embedding model training image retrieval text cnn trained regress text embeddings correlated image minimizing moid loss loss used minimize distance text image 5 embeddings let xn n batch pair σ sigmoid function denote pn σ φ xn ˆ pn σ ψ note pn ˆ pn dimensionality joint embedding space let loss l 1 nd n x x pnd log ˆ pnd 1 log 1 pnd 2 googlenet architecture 28 used customizing last layer regress vector dimensionality text embedding train stochastic gradient descent optimizer learning rate multiplied every iteration momentum batch size set 120 random cropping mirroring used online data augmentation setting cnn training converge around iteration use caffe 29 framework initialize imagenet 1 trained model make training faster notice despite initializing model trained data doe not denote dependence annotated data since resulting model generalize much concept imagenet class trained one model scratch obtaining similar result although training iteration needed cross entropy loss not usually used regression problem mean square error loss often used chose cross entropy loss empirically since wa one providing stable training better performance although cross entropy loss tends considered loss classiﬁcation also suitable regression problem despite loss not zero regression solution match groundtruth always minimum compared solution text embeddings text vectorization method diverse term architecture text structure designed deal some method oriented vectorize individual word others vectorize full text paragraph work consider text embeddings test pipeline evaluate performance learning web social medium data explain brieﬂy main characteristic text embedding method used lda 16 latent dirichlet allocation learns latent topic collection text document map word vector probability topic describe document assigning topic distribution turn word distribution assigned advantage method give interpretable topic glove 30 model learns vector essentially dimensionality reduction count matrix training performed aggregated global word statistic corpus 12 learns representation word based context using single hidden layer neural network ha two variant cbow continuous bag word approach neural network trained predict word given input surrounding context 6 surrounding word model opposite cbow model neural network trained predict word context given word input work use extended efﬁcient cbow approach 31 extends idea document able create numeric sentation regardless length extending cbow model add another input vector input context paragraph identiﬁer training word tor document vector trained well end hold numeric representation whole document work use cbow approach fasttext 32 extension treat word composed character ngrams learning representation ngrams instead word idea take account exploit morphology word word split inputted separately model trained using cbow approach vector word made sum character n gram generate embeddings vocabulary word exploiting word morphology fasttext try generate better embeddings rare word assuming character ngrams shared word also allows erate embeddings vocabulary word train fasttext use originaly proposed extended skigram approach best knowledge ﬁrst time text embeddings trained scratch corpus evaluated image retrieval text task used implementation lda fasttext glove implementation maciej lda generate embeddings document glove fasttext only generate word embeddings get document embeddings method consider two standard strategy first computing document embedding mean embedding word second computing weighted mean word document embeddings dimensionality 400 ha used value ha lected one used paper 31 compare text embedding method enough get optimum performance fasttext glove 12 32 30 show respectively lda dimensionality 200 ha also considered benchmark section present datasets used work show some example image associated text dataset formed instagram image associated one 10 populated glish speaking city world image caption one name city 2 3 7 appears contains image city make total image split training image validation image test image interest dataset formed recent social medium data text associated image scription hashtags written photo kind free available data would interesting able learn figure 3 show some example image associated text dataset available fournierstreet church spitalfields home house bricklane london exploring tourist family road trip hello california roadtrip familyfirst springbreak losangeles gluten free raspberry yoghurt elderflower cake glutenfree baking raspberry sweet sydney australia habit wandering path hike toronto figure 3 example dataset image webvision webvision dataset 33 contains million image crawled flickr website google image search concept ilsvrc 2012 dataset 1 used querying image textual information accompanying image caption user tag description provided validation set used test work contains image figure 4 show some example webvision image associated text bobby only super obedient like food offer cocker spaniel bobby dog pet canon two vulture part group 6 sitting roof waiting turn dead squirrel road bird vulture love look mustard fade little flame detail fat cyclist blog archive bicycle built two norwegian boathouse converted contemporary summer home tyin tegnestue boathouse around figure 4 example webvision dataset image 8 mirflickr mirflickr dataset 34 contains image collected flickr annotated using 24 predeﬁned semantic concept 14 concept divided two category 1 strong correlation concept 2 weak correlation concept correlation image concept strong concept appears image predominantly differentiation denote strong correlation concept sufﬁx finally considering strong weak concept separately get 38 concept total image dataset annotated least one concept additionally image associated tag collected flickr following experimental protocol 35 36 37 38 tag appear le 20 time ﬁrst removed instance without tag annotation removed figure 5 show some example mirflickr image associated text macro fisheye portrait explore indoor people male macro tamron india flower animal 2008 canon holiday summer vacation cloud sea people sky water tree male nature verde tree abigfave flower tree theperfectphotographer goldstaraward figure 5 example mirflickr dataset image retrieval webvision datasets section perform image retrieval experiment webvision datasets comparing performance different text embeddings pipeline lyze performance text embedding present error analysis pipeline show qualitative retrieval result image text retrieval image retrieval using multimodal query experiment setup evaluate learnt joint embeddings deﬁne set textual query check visually retrieved image contain querying concept deﬁne 24 different query half single word query half two word query selected cover wide area semantic concept usually present web social medium data simple complex query divided four different category urban weather food people query listed table complex query only image containing querying concept considered correct 9 table 1 query retrieval experiment webvision datasets simple complex urban car skyline bike weather sunrise snow rain food cake pizza people woman man kid table 2 performance webvision first column show mean p 5 query second simple query third complex query text embedding webvision query c c lda 200 lda 400 mean glove glove fasttext table 3 performance transfer learning first umn show mean p 5 query second simple query third complex query text embedding train webvision test instacities train instacities test webvision query c c lda 200 lda 400 mean glove glove fasttext table 4 performance using glove introducing noise changing indicated caption random caption training set experiment query c without introduced noise 10 introduced noise 20 introduced noise 30 introduced noise result conclusion table 2 3 show mean precision 5 webvision datasets transfer learning datasets compute transfer learning result train model one dataset test table 4 show mean precision 5 introduced additional noise model trained mean square error loss noise introduced changing indicated caption random caption training set figure 1 6 7 show ﬁrst retrieved image some complex textual query figure 7 also show result query proving pipeline work beyond traditional level retrieval figure 8 9 show retrieval also work multimodal query combining image text complex query demand two concept appear retrieved image obtain good result query concept tend appear together instance generally retrieve correct image skyline night bike park not retrieve image dog kid failing complex query usually image only one two querying concept appears retrieved figure 10 show some case image 10 baseball basketball shoe basketball shoe figure 6 first retrieved image complex query corresponding semantic concept two querying concept retrieved prof common embedding space ha learnt ha semantic structure performance generally better webvision reason query closer kind image people tend post instagram imagenet class however result transfer learning show webvision better dataset train webvision ha image training image v training image show learned model robust general scalable data even not speciﬁcally related target task allows learning embedding model perform better task result show tested text embeddings method work quite well simple query though lda fails trained webvision lda learns latent topic semantic sense training data every webvision image ated one imagenet class inﬂuences lot topic learning result embedding fails query not related class top performing method glove training training webvision difference performance small fasttext achieves good performance bvision bad performance compared method explanation social medium data contains colloquial vocabulary webvision contains domain speciﬁc diverse vocabulary since fasttext learns representation character ngrams suitable learn representation corpus morphologically rich doe not work well any database oriented deal larger text one ﬁnd accompanying image web social medium word embedding method glove result computing text representation mean weighted mean 11 dog park dog beach food fast food healthy art art street bridge london bridge new york bridge san francisco wild happy monday figure 7 first retrieved image complex query left city related complex query query word embeddings similar overall conclusion performance comparison text embeddings ment word level text embeddings glove perform better ment level text embeddings lda character ngrams level text embeddings text reason caption associated image social medium tend quite concise averaging embeddings caption still give u informative representation allows u take proﬁt rich semantic space learnt kind embeddings fact semantic space quite sparse allows u perform arithmetic embeddings also able learn representation averaged caption word duction additional artiﬁcial noise deteriorates result heavily indicates despite proposed learning pipeline learn powerful visual feature web social medium data inherent noise reducing may lead huge performance improvement error analysis remarkable source error listed explained section visual feature confusion error due confusion visually similar object instance retrieving image quiche querying pizza error could avoided using data higher 12 query top semantically retrieved query top semantically retrieved figure 8 first retrieved image multimodal query concept added removed bias result webvision dimensional representation since problem lack training data learn visual feature generalize unseen sample error dataset statistic important source error due dataset statistic example webvision dataset contains class snow leopard ha many image concept word snow appears frequently image correlated description net learns embed together word snow visual feature snow leopard many image snow leopard snow therefore query snow get snow leopard image figure 11 show error use complex multimodal query bias result word different meaning us word different meaning word people use different scenario introduce pected behavior instance query woman bag dataset usually retrieve image pink bag reason people tend write woman image caption pink stuff appears considered error evaluation inferring image people relate certain word social medium interesting research retrieval mirflickr dataset compare performance pipeline image retrieval text system use mirflickr dataset typically used train evaluate image retrieval system objective prove quality multimodal embeddings learnt solely web data paring supervised method 13 table 5 map image text retrieval task mirflickr deﬁned 36 38 method train map lda 200 lda 400 webvision webvision glove glove webvision fasttext fasttext webvision mirflickr glove mirflickr dch 36 mirflickr lsrh 37 mirflickr csdh 38 mirflickr seph 35 mirflickr scm 39 mirflickr cmfh 40 mirflickr crh 41 mirflickr 42 mirflickr table 6 map image text retrieval task mirflickr deﬁned 43 method train map glove glove mirflickr mml 43 mirflickr infr 43 mirflickr sbow 43 mirflickr slkl 43 mirflickr mlkl 43 mirflickr table 7 ap score 38 semantic concept map mirflickr underlined number compare method trained instacities od trained target dataset method glove mmshl 44 scm 39 glove train mirflickr instacities animal baby baby bird bird car car female female lake sea sea tree tree cloud cloud dog dog sky structure sunset transport water ﬂower ﬂower food indoor plant life portrait portrait river river male male night night people people map 14 query top semantically retrieved figure 9 first retrieved image multimodal complex query webvision experiment setup consider three different experiment 1 using query tag accompanying query image computing map query retrieved image considered correct share least one tag query image experiment split used 5 query set 95 training retrieval set deﬁned 36 38 2 using query class name retrieved image considered correct tagged query concept experiment split used 50 training 50 retrieval set deﬁned 44 3 experiment 1 using mirflickr split proposed zhang et al 43 result conclusion table 5 6 show result experiment 1 3 respectively see pipeline trained web social medium data multimodal fashion achieves 15 building beach car bike car train figure 10 first retrieved image simple left right column complex weighted query snow leopard snow snow leopard plow leopard leopard snow figure 11 first retrieved image text query using webvision concept removed bias result competitive result trained target dataset pipeline outperforms od table 7 show result experiment pipeline glove text embedding trained outperforms state art method class map train target dataset result improved signiﬁcantly notice despite applied class tag existing mirflickr pipeline generic ha learnt produce joint image text embeddings many semantic concept seen tative example comparing image text embeddings section analyze semantic quality learnt joint embedding space showing cnn ha learnt embed image experiment setup evaluate cnn ha learnt map image text embedding space mantic quality space perform following experiment build random image pair mirflickr dataset compute cosine similarity image text embeddings figure 12 plot image embeddings distance v text ding distance random image pair cnn ha learnt correctly map image text embedding space distance embeddings image text pair similar point plot fall around identity line also 16 figure 12 text embeddings distance x v image embedding distance different random image pair lda glove embeddings trained distance normalized point red pair doe not share any tag orange share 1 light orange share 2 yellow share 3 green share coefﬁcient determination image text distance learnt space ha semantic structure distance image embeddings distance text embeddings smaller pair sharing tag plot point color reﬂects number common tag image pair pair sharing tag closer axis origin example take dog image tag dog cat image tag cat one scarab tag scarab text embedding ha learnt correctly distance projection dog scarab tag text embedding space bigger one dog cat tag smaller one pair not related cnn ha correctly learnt embed image animal text embedding space distance dog cat image embeddings similar one tag embeddings any pair point given pair fall identity line furthermore distance nearer coordinate origin point given dog scarab pair also fall identity line nearer coordinate origin another pair ha no relation result conclusion plot figure 12 glove embeddings show similar shape resulting blob elongated along x direction prof image text embeddings tend provide similar distance image pair blob thinner closer identity line distance smaller image pair related mean embeddings provide valid distance semantic concept close enough dog cat fails inferring distance weak related concept car skateboard color point plot show space learnt ha semantic structure point corresponding pair tag common closer coordinate origin smaller distance image text embedding color also deducted cnn good inferring distance related image pair image 17 3 tag common image embedding distance bigger many image bigger distance not tag common however visual embedding sometimes fails infers small distance image pair not related image pair no tag common image embedding distance plot lda embedding show learnt joint embedding not good term cnn image mapping text embedding space term space semantic structure blob doe not follow identity line direction much mean cnn lda not inferring similar distance image text pair point color show cnn inferring smaller distance similar image pair only pair related coefﬁcient determination shown graph measure proportion ance dependent variable predicted linear regression predictor variable case interpreted measure much image distance predicted text distance therefore well visual embedding ha learnt map image joint space ratiﬁes plot visual inspection proving visual embeddings trained glove representation learnt much accurate mapping lda show better term mapping visualizing cnn activation map proved using only social medium data state art cnns trained way learn powerful visual feature capable discriminate among huge variety scene object outdoor scene abstract concept speciﬁc building experiment visualize image retrieval set generated highest activation some cnn unit using googlenet trained scratch glove text embedding also show region image activated selected unit generate activation map used deconvnet proposed zeiler et al 45 caffe implementation presented 46 figure 13 show result selection neuron layer model notice network unit selective speciﬁc building golden gate bridge object guitar drum light identify concert scene even basketball visualizing learned semantic space section use dimensionality reduction method reduce dimensionality joint embedding space 2 dimension show image space visualize semantic structure dimensionality reduction inspired karpathy us visualize cnn layer feature use 47 visualize learnt joint visual textual embedding 4 5 18 dimensionality reduction method use 400 dimensional embeddings produce 2 dimensional embeddings one given 400 dimensional visual textual embeddings computes 2 dimensional embedding arranging element similar representation nearby providing way visualize learnt joint space analyze qualitatively semantic structure visualizing image text embeddings learnt joint image text embedding space apply ities embeddings apply set formed visual embeddings image test set text embeddings selected querying term table 1 experiment use model trained dataset showing image embedding location first set canvas predeﬁned dimension pixel normalize 2 dimensional embeddings given ﬁt canvas size finally visualize image embedding location setting corner embedding location resizing pixel text embeddings use image containing word representation canvas get interpretable visualization avoiding image overlap two image share any pixel output ﬁgure omit one prioritizing word image therefore image surrounding word image not necessary top retrieval result word nearest image one represented ﬁgure semantic space inspection joint embeddings 2 dimensional visualization 14 show semantic structure learnt space show semantic cluster joint embedding ha learnt way data distribution correspond different kind image people tend post instagram instance ﬁgure show cluster food image cluster sport image cluster sunrise image cluster animal image also show image people numerous joint embedding group correctly also appreciated image might consider noise image logo text clustered together majority image far semantic cluster isolated near ﬁgure edge joint embedding able ﬁnd semantic relation image rest assigns embeddings not relation others computing objective place similar image nearby image without semantic relation set far others therefore conclude pipeline quite robust social medium noise visualization learnt joint embeddings avalaible conclusion work learn joint visual textual embedding using web social medium data benchmark state art text embeddings image retrieval text task 19 concluding glove best one data similar mance competitive performance supervised method image retrieval text task show model go beyond image retrieval semantic retrieval handle multiple concept query also multimodal query composed sual query text modiﬁer bias result clearly outperform state art mirflick dataset training target data code used work available acknowledgment work wa supported doctorats industrials program generalitat de catalunya spanish project marie action pean union grant agreement no 712949 tecniospring plus agency business competitiveness government catalonia accio 20 figure 13 activation ﬁve unit layer googlenet model trained scratch using glove activation map 21 figure 14 visualization px joint embedding dataset 22 reference 1 jia wei r kai li imagenet hierarchical image database cvpr 2009 2 lin maire belongie hay perona ramanan dollar zitnick microsoft coco common object context lect note comput 2014 3 zhou lapedriza khosla oliva torralba place 10 million image database scene recognition tpami 2017 4 yosinski clune bengio lipson transferable feature deep neural network nip 2014 5 mar david ger antonio learning apperance virtual scenario pedestrian detection cvpr 2010 6 ro sellart materzynska vazquez lopez synthia dataset large collection synthetic image semantic segmentation urban scene cvpr 2016 7 phan shivakumara tian tan recognizing text perspective distortion natural scene iccv 2013 8 gupta vedaldi zisserman synthetic data text localisation natural image cvpr 2016 9 li wang li agustsson berent gupta sukthankar van gool webvision challenge visual learning understanding web data arxiv 2017 10 mahajan girshick ramanathan paluri li bharambe van der facebook exploring limit weakly supervised pretraining eccv 2018 11 norouzi mikolov bengio singer shlens frome corrado dean learning convex combination semantic embeddings nip 2013 12 mikolov corrado chen dean efﬁcient estimation word representation vector space iclr 2013 13 gordo larlus beyond image retrieval leveraging caption learn global visual representation semantic retrieval cvpr 2017 14 gomez patel nol karatzas jawahar learning visual feature embedding image text topic space cvpr 2017 15 patel gomez gomez nol karatzas jawahar texttopicnet learning visual feature embedding image semantic text space arxiv 2018 16 blei ng jordan latent dirichlet allocation mach learn 2003 17 wang li lazebnik learning deep embeddings cvpr 2016 18 patel gomez nol karatzas dynamic lexicon generation natural scene image eccv 2016 19 gordo almazan murray perronin lewis latent embeddings word image semantics iccv 2015 20 princeton university wordnet 2010 21 salvador hynes aytar marin oﬂi weber torralba learning embeddings cooking recipe food image cvpr 2017 22 gomez gomez gibert karatzas learning barcelona instagram data local tourist post neighbourhood eccv workshop 2018 23 patrini rozza menon nock qu making deep neural network robust label noise loss correction approach cvpr 2016 24 xiao xia yang huang wang learning massive noisy labeled data image cation cvpr 2015 25 fu wu mei wang lu rui relaxing vocabulary robust deep learning image tagging iccv 2015 26 xu lu shimada taniguchi matrix completion social image tagging ieee access 2017 27 melucci relevance feedback algorithm inspired quantum detection ieee trans knowl data 2016 23 28 szegedy vanhoucke ioffe shlens rethinking inception architecture computer vision cvpr 2016 29 jia shelhamer donahue karayev long girshick guadarrama darrell caffe tional architecture fast feature embedding arxiv 2014 30 pennington socher manning glove global vector word representation emnlp 2014 31 le mikolov distributed representation sentence document nip 2014 32 bojanowski grave joulin mikolov enriching word vector subword information arxiv 2016 33 li wang li agustsson van gool webvision database visual learning understanding web data arxiv 2017 34 huiskes lew mir ﬂickr retrieval evaluation acm int conf multimed inf 2008 35 lin ding hu wang hashing retrieval cvpr 2015 36 xu shen yang shen li learning discriminative binary code retrieval ieee trans image 2017 37 li qi ye hua linear subspace ranking hashing retrieval ieee trans pattern anal mach 2017 38 liu lin shao shen ding han sequential discrete hashing scalable similarity retrieval ieee trans image 2017 39 zhang li supervised multimodal hashing semantic correlation maximization aaai 2014 40 ding guo zhou collective matrix factorization hashing multimodal data cvpr 2014 41 zhen yeung hashing multimodal data nip 2012 42 zhou ding guo liu dong supervised hashing similarity search ieee int conf multimed expo 2014 43 zhang zhang li li wang classify social image integrating content multimed tool springer u 2018 44 wang li hashing learning framework automatic image annotation int conf data sci 2017 45 zeiler fergus visualizing understanding convolutional network eccv springer cham 2014 46 yosinski clune nguyen fuchs lipson understanding neural network deep ization arxiv 47 van der maaten courville fergus manning accelerating using algorithm mach learn 2014 24