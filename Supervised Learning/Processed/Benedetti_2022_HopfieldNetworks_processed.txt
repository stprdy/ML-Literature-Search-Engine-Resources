supervised perceptron learning v unsupervised hebbian unlearning approaching optimal memory retrieval network marco 2 giancarlo francesco di fisica sapienza universit di rom moro 2 00185 rom italy de physique de l ecole normale erieure en e psl cnrs sorbonne e e de paris paris france di fisica sapienza universit di rom moro 2 00185 rom italy infn sezione di rom dated march 8 2022 hebbian unlearning algorithm unsupervised local procedure used improve retrieval property neural network numerically compared supervised gorithm train linear symmetric perceptron analyze stability stored memory basin attraction obtained hebbian unlearning technique found comparable size obtained symmetric perceptron two algorithm found converge region gardner space interaction followed similar learning path geometric interpretation hebbian unlearning proposed explain optimal performance hopﬁeld model also prototypical model disordered magnetic system might possible translate result model interest memory storage material introduction neural network successful el associative memory 1 2 framework network composed n binary neuron σi n used store p αn binary memory ξµ p load α used control parameter model memorize mean network must able reconstruct ries basis version achieved endowing appropriate dynamic ﬁxed point attractor ﬁnite basin traction present close proximity memory network initialized close enough one stored memory dynamic drive responding attractor reducing number misaligned spin general dynamic given zero perature asynchronous monte carlo dynamic 2 3 energy landscape given hamiltonian h σ 2 x j jijσiσj 1 every step pick randomly site update spin according σi n x j jijσj 2 detail dynamic depend synaptic interaction matrix j shaped one tial model ﬁeld introduced hopﬁeld 1 two author contributed equally corresponding author coupling matrix built according hebb prescription 2 4 jij 1 n p x ξµ ξµ j jii 0 3 phase diagram model ha sively studied includes recognition phase α αc fraction ﬁxed point tor dynamic close memory ξµ 5 proximity two conﬁgurations urally measured term overlap 1 n n x 4 also well known disorder model implies existence dynamic multistability rough landscape local minimum energy overlap memory 6 rious state thus ﬁxed point dynamic proliferate α hopﬁeld model overlap attractor memory smaller 1 any extensive load α 0 deteriorates memory load increased discontinuous tion zero αc reached often referred blackout catastrophe 2 7 hence error correcting performance hopﬁeld network never perfect memory not ﬁxed point dynamic many strategy proposed order tend capacity model improve error correcting performance retrieval phase 13 note highlight some surprising similarity two popular option supervised symmetric perceptron algorithm sp unsupervised hebbian unlearning hu algorithm 11 per organized follows sec ii introduce 7 mar 2022 2 algorithm sec iii iv present main sults sec v propose key interpret result finally sec vi summarize ﬁndings ii algorithm section detail symmetric tron hebbian unlearning algorithm operate improve performance hopﬁeld model note ﬁrst problem storing ories ﬁxed point dynamic mathematically equivalent ﬁnding set coupling satisfy constraint recall jii 0 ξµ sign n x j jijξµ j 5 written way becomes supervised learning problem binary input vector ξµ j ξµ j must correctly associated binary label ξµ tion n perceptrons weight given vector ji j jij 1 j ξµ sign ji ξµ elegant way solve problem recast term linear regression 14 20 every memory every spin deﬁne pattern ηµ ξµ ξµ 6 multiplying side eq 5 ξµ straints expressed ji ηµ 7 quantity called stability 21 stronger version constraint k satisﬁed vector ηµ lie positive side oriented plane normal ji passing origin tance greater k term lem perfectly stabilizing number p memory ha factorized n independent ear separation problem classifying number p vector parameter k used tune stability memory 14 symmetric perceptron symmetric perceptron algorithm solves aration problem condition jij jji deﬁned following procedure constructing matrix j 15 16 initialize jij symmetric matrix update jij convergence according jij λ p x ϵµ ϵµ j ξµ ξµ j jii 0 ϵµ θ k 8 notice symmetry coupling matrix preserved algorithm algorithm supervised sense need provided full set memory ξµ every step mask ϵµ deﬁned way algorithm stop k µ stability threshold k eter algorithm λ tunable learning rate algorithm k exceeds critical threshold kmax α no symmetric matrix j exists satisfying stability quirement k algorithm doe not converge unsatisﬁable unsat phase hand k kmax α coupling matrix exist algorithm converge one ﬁnite ber step satisﬁable sat phase limit case k kmax α only one coupling matrix meet stability requirement algorithm supposed converge independently value learning rate λ initial ha shown one could expect increasing k towards kmax lead not only stable memory also larger basin attraction memory 16 symmetric case function kmax α ha determined analytically 15 slightly diluted recurrent network network average connectivity ing log numerical result study gorithm deﬁned eq 8 network fully connected fully symmetric suggest degree symmetry kmax given α located slightly one predicted 15 ﬁnding discussed fig 1 suggests reconsider previous interesting 22 open road investigation critical capacity function network tivity 23 analyzing close relationship tween symmetric perceptron hebbian unlearning see sec ii b always initialize coupling ing hebb rule eq 3 keeping mind result k kmax α not depend choice fig 2 plot evolution number step mini µ maxi µ pn p µ averaged many dom realization memory one see sp algorithm slightly reducing crease value negative value not memory stable positive value ories stable prescribed threshold proﬁle stability obtained diﬀerent choice control parameter k α 3 1 fig phase diagram linear symmetric perceptron plane deﬁned pattern density α ity parameter dashed line analytical result kmax α obtained slightly diluted network 15 square show numerical result kmax α fully connected model α simulation run ferent size network measure probability algorithm converge 103 step training hence providing lower bound actual value stability standard ﬁnite size scaling analysis 24 25 ha used extrapolate value kmax α thermodynamic limit hebbian unlearning interesting proposal increase performance hopﬁeld model consists modifying hebbian learning rule deﬁned eq 3 adding ing procedure 11 12 26 go follows initialize coupling matrix according eq 3 repeat time following step initialize neuron random state follow dynamic converges ﬁxed point modify coupling matrix according jij n j jii 0 9 algorithm unsupervised sense doe not need provided explicitly ories ξµ only exploit information encoded hebb learning rule eq 3 easy see step energy conﬁguration algorithm increased making le stable since ery memory surrounded many spurious local energy minimum overall eﬀect smoothing energy landscape around attractor correlated ries destabilizing attractor procedure often referred hebbian unlearning tion algorithm referred dream total number dream parameter algorithm 0 20 40 60 80 100 4 2 0 2 4 6 min max av 0 k fig value minimal stability orange imal stability blue average stability green plotted function number iteration symmetric perceptron algorithm averaged 50 tions memory n 800 α λ black dotted line represents threshold must overcome memory perfectly called gray dotted line represents minimum stability required algorithm reached convergence must chosen maximize recognition formance given load ha shown procedure improves performance model two way 27 one hand critical memory load increased αhu c hand overlap attractor memory go one meaning memory become real ﬁxed point dynamic last fact especially remarkable dreaming procedure unsupervised led u try characterize performance algorithm term stability introduced sec ii approach ha already attempted 28 analysis push reveals new unexpected feature ﬁg 3 show typical behavior unlearning procedure unfolds horizontal axis represents number step performed gorithm rescaled reason choice become clear later focusing see behavior minimal stability grows positive value peak some value dtop decrease back negative value din dfin every stability positive equivalently every memory ﬁxed point dynamic increase α 4 interval din dfin shrink height peak dtop lower reach critical load αc never go zero collecting data network size n 300 400 500 600 800 ent value α ϵ possible extrapolate position din dtop dfin function α ϵ n well critical capacity αc ﬁtting data respect model parameter found number dream every case linear n moreover dtop also depends linearly critical capacity αhu c value dtop approach zero value critical capacity αhu c well linear dependence din dtop dfin consistent past literature 17 18 28 curve quadratic around dtop illustrated fig 3 din dfin tend dtop critical capacity critical exponent resulting scaling relation dtop ϵ α n n ϵ α b 10 din ϵ α n dtop ϵ c α 11 dfin ϵ α n dtop n ϵ e α f 12 b c e f statistical error evaluated using jackknife method iii basin attraction also compared performance sp hu measuring shape basin attraction around memory done initializing network some ﬁxed distance mi one memory ξµ following dynamic convergence ﬁxed point reached measure overlap ξµ averaged many realization memory mf 1 n n x 13 4 2 0 2 4 6 din dtop dfin min max av 0 fig value minimal stability orange imal stability blue average stability green computed hebbian unlearning averaged 50 alizations memory n 800 α ϵ black dotted line represents threshold overcome memory perfectly called denote corresponding value number dream din dtop point algorithm reach maximum value dfin end perfect classiﬁcation regime network used red arrow point din dtop dfin fig 4 plot mf function mi colored dashed curve refer sp diﬀerent value k est k allows algorithm converge 103 ations slight underestimation real kmax α bares little consequence result related underline importance choice λ given value another crucial topic rarely discussed literature higher value λ imply larger learning step smaller value ciated ﬁner exploration space coupling matrix training observed algorithm ating λ 1 converges almost identical matrix already k equal maximal ity diluted network 15 according fig 1 slightly lower actual kmax suggests ﬁnal state lie closely unique optimal lution even not exactly kmax hence no signiﬁcant change expected numerical sults k pushed towards maximal value hand λ assumes smaller value λ basin observed smaller size volume solution larger indicating ﬁnal state remains farer maximal 5 formance order recover numerical result tained larger λ one need progressively increase k value diﬃcult reach numerically result choice λ 1 section seems u well justiﬁed reproduce optimal performance symmetric perceptron k consistently literature ﬁnd ing stability lead increase mf ﬁxed mi 16 particular stability equal zero memory albeit ﬁxed point dynamic zero basin attraction indicated low value mf mi gray dashed line bottom fig 4 refers hopﬁeld model dreaming since α model doe not learn colored continuous line refer hebbian ing diﬀerent amount dreaming speciﬁcally measured performance model three value din dtop dfin deﬁned sec ii clear dreaming improves performance network found performance not imized dtop one could expect 28 din requirement perfect retrieval memory satisﬁed zero margin also found performance hebbian learning din one sp k indistinguishable within numerical resolution remarkable fact since two algorithm radically diﬀerent structure sp algorithm pervised need access every step memory network need memorize hu not only exploit topology rious state generated hebb prescription eq 3 ﬁndings robust change load α ﬁnite size eﬀects illustrated fig mean basin radius ﬁnite n deﬁned 1 selecting value mi 30 ories reconstructed 5 error dot represent extrapolation quantity limit n diﬀerent value lower dot relative sp correspond k kmax value mean basin radius get higher k increased k one see even modynamic limit simulation suggest optimal regime two algorithm perform essentially way iv space interaction one way visualize solution tion problem way solution reached mean algorithm exploit space tions conceived gardner 14 consider spherical surface n n dimension point vector composed element jj connectivity matrix normalized standard mi mf hu din hu dtop hu dfin sp k 0 sp k sp k hopfield fig average size basin attraction n 800 α symmetric perceptron sp hebbian learning hu averaged several realization order colored area around curve represents statistical error continuous line basin din dtop dfin hu ϵ dashed line three value k including k used sp λ gray dotted line represents performance ﬁeld model value notice attraction basin sp hu almost coincide two rithms operate optimal regime namely din k deviation position vector hence r 14 σj v u u 2 n n 1 n x j ij 15 concern sp ﬁxing value α set pattern one imagine sphere posed unsat sat region region connected original sphere one go matrix another one continuous fashion sat region contains point relative unique solution k kmax α deﬁne overlap parameter quantifying covariance two generic symmetric matrix jij uij q 2 n n 1 n x j σjσu 16 average disorder 6 mean basin radius hu din hu dtop hu dfin sp sp sp fig mean attraction basin radius symmetric ceptron sp hebbian unlearning hu measured 16 extrapolated n α λ point sp correspond following value k α α α k error bar smaller symbol size final state ﬁrst evaluate ﬁnal point two rithms converge space interaction hebbian unlearning stopped din relevant amount dream identiﬁed sec ii symmetric perceptron run λ fig 6 left display lap resulting matrix sp formed diﬀerent value k reaching kmax α plot show q increase k suggesting hu push system region solution sp converges k close kmax finite size eﬀects evidently appear near abrupt transition sat unsat increase q size network suggests maximum overlap might associated maximal stability n becomes large enough plot q function α see fig 6 right show distance ﬁnal point initial hebbian matrix increase number memory becomes larger distance two ﬁnal point remains small stable α αhu c learning path gradient comparing ﬁnal state convergence done sec iv conclude two network starting initial matrix end similar ration coupling jij analyze whole trajectory traced two algorithm space interaction set α overlap initial ﬁnal state small enough distant sphere n 800 λ k close kmax one single sample choice small value learning rate λ allows trace continuous path space interaction hu run choosing din 10 sample total fig 7 left report tion resulting trajectory space j along three randomly chosen direction plot show two algorithm explore region space interaction proceeding along similar direction also observe convergence velocity two algorithm diﬀerent indicating time step process fig 7 right show rithm absolute value variation vector j deﬁned j j j j j 17 direction vector coincides one gradient followed algorithm space interaction given time step convergence speed hu doe not niﬁcantly vary sp show any scale λ celeration time resembles exponential law word hu explores space interaction nearly uniformly speed sp take 15 20 time step reach smaller condensed region get conﬁned convergence diﬀerent speed algorithm imply ent diﬃculty comparing trajectory analysis thus rely deﬁning particular tion ˆ v space interaction use compare two trajectory gradient direction deﬁned line connects initial hebbian matrix point convergence sp ˆ v j tmax sp tmax sp j 0 0 j tmax sp tmax sp j 0 0 18 deﬁne two observables help u analysis trajectory qv j j 0 0 j tmax tmax j 0 0 ˆ v 19 measure angular distance any point trajectory line traced direction ˆ v time qv 0 1 smaller quantity evidently trajectory diverging ˆ 7 k q n 400 n 600 n 800 n 400 n 600 n 800 n 400 n 600 n 800 q hopfield hu hopfield sp sp hu fig left overlap q ﬁnal state hebbian unlearning hu ϵ din symmetric perceptron sp λ 1 reached stability measure diﬀerent value n α point represent mean computed 5 realization disorder error bar smaller data symbol value k range 0 slightly kmax α α q peak around kmax α indicating two algorithm converge coupling matrix closest near value k kmax α right overlap q function α n point represent mean 10 realization disorder error bar smaller data symbol orange symbol correspond overlap ﬁnal state sp λ 1 k α hu ϵ hu chose din α αc α αc chose dtop αc represented gray dotted line overlap initial hebbian matrix ﬁnal state hu green sp blue choice parameter also shown algorithm distance initial ﬁnal matrix increase α increased distance ﬁnal point remains small αc one also introduce v j j j j ˆ v 20 projection variation j step along direction ˆ larger quantity aligned ˆ v trajectory fig 8 left represents value qv trajectory depicted ﬁg 7 left one see qv 0 small hu sp mean start wrong direction ticularly reasonable hu involves random picking initialization state however sp rapidly reach qv 0 high initial tion considered value λ hu algorithm qv decreasing lower rate initial overshooting sp high value λ signaled anomalously high value qv second step process direction followed two algorithm respect ˆ v shown fig 8 center sp λ ha peak v ﬁrst part trajectory signaling high degree alignment gradient ˆ later sp rapidly converges towards condensed region gradient lose polarization ˆ v convergence point ha ready reached higher value λ used no relevant polarization measured hu show similar behavior trajectory start along direction barely aligned ˆ v consistently high gree alignment obtained le half iteration eventually hu also converges towards ﬁnal state losing alignment ˆ fig 8 right play direction variation function distance ˆ three diﬀerent behavior tory thus recognized algorithm first trajectory move away ˆ v bad start second phase aligns ˆ v third phase matrix plunge towards convergence state geometric interpretation unlearning order provide argument might explain similarity hu sp algorithm 8 j j j x x z z 2 0 2 4 6 log sp 10 4 sp 10 3 sp 10 2 sp 1 hu mean path fig left projection trajectory followed system space interaction dynamic hebbian unlearning hu symmetric perceptron sp numerical measurement taken one sample n 800 α ϵ λ 10 trajectory hu drawn light blue average unlearning path blue path followed sp depicted red point represent diﬀerent step algorithm hu ha resampled regular interval along trajectory simplicity data analysis right absolute value variation j logarithmic scale function normalized time scale tmax maximum number step reached algorithm given sample numerical measurement one sample n 800 α ϵ λ 1 three sample simulated sp one sample hu qv hu sp 10 4 hu sp 10 2 hu sp 1 q v hu sp 10 4 hu sp 10 2 hu sp 1 qv q v hu sp 10 4 hu sp 10 2 hu sp 1 fig left qv angular distance trajectory reference direction ˆ v function time center v projection variation along direction ˆ v function time right v function qv numerical measurement collected one single sample n 800 α ϵ hebbian unlearning hu circle diﬀerent value λ symmetric perceptron sp triangle rewrite rule eq 9 vectorial fashion ji ji n ηi 21 ji vector element contained ith row connectivity matrix call ηi glassy pattern deﬁned analogy memory pattern ηi µ 9 din dfin min unsat sat 0 fig mean value assumed perceptron overlap ω sat unsat min pattern hebbian learning process measurement performed n 800 α ϵ 200 spurious state several tions disorder error bar indicated shaded region ηi 22 spurious state unlearning rithm converges also introduce perceptron lap ωµ n ηi ηi 23 overbar indicates average spurious state given realization disorder pair µ thus related given constraint associated optimization problem sat constraint 0 unsat one fig 9 show ωµ α n 800 three type constraint sat unsat minimally satisﬁed sat constraint lowest measured stability fact perceptron overlap negative unsat min constraint positive sat constraint suggests distribution ηµ look anisotropic reference frame glassy pattern certainly induced fact glassy pattern ηi sat deﬁnition likely contained half hyperspace deﬁned orthogonal plane ji contains sat memory pattern moreover since minus sign eq 21 hu performing geometric mation perceptron order align ji vector memory pattern ηi only exploiting scape spurious state hopﬁeld model retrieval regime hu algorithm manages complish task optimal way suppose small yet non null overlap spurious state memory important feature ensure maximization size basin attraction vi conclusion ﬁrst part paper ref 17 18 26 analyze eﬀect hebbian unlearning hopﬁeld model giving measure optimal amount tions maximizes performance network term size basin attraction memory retrieval particular present result focus case asynchronous dynamic performed tivity memory homogeneous network autapses dilution absent graph ing analysis study minimum stability reached memory able give new sight classiﬁcation capability network deﬁned three relevant amount iteration din dtop dfin related average radius basin attraction found optimal amount step one memory fectly recalled basin maximal size din variance 28 optimal state network wa assumed coincide point highest minimum stability wa reached accordance 21 relevant quantity scale optimal number iteration measured 17 18 26 leading order system size n appears smaller tion result obtained kind analysis conﬁrmed estimation critical capacity hu perfectly consistent previous result 17 18 26 moreover shown hu performs tently sp near maximal stability result suggests hu optimal unsupervised gorithm term generalization network hebbian perspective large basin attraction imply capability model associate exotic uli known memory higher recognition power respect new input according fig 4 sp k 0 ha no generalization capability meaning fall regime memory ognized only initial state dynamic coincides memory increasing value k lated increase generalization hu able reach highest degree generalization unsupervised fashion second part paper trajectory matrix j space interaction ered ﬁnal state algorithm maximize overlap matrix suggesting algorithm converges region interaction space 10 middle part trajectory two algorithm also explore nearby region suggesting follow well overlapping gradient space coupling pioneering investigation van hemmen laborators 17 18 26 concluded hu wa able move correlation among stored memory turnining memory ﬁxed point dynamic enlarging basin attraction regarding point remark brilliant idea contained ref 19 slightly modiﬁed version unlearning procedure wa proved partially align rule proposed ref 10 nevertheless since network ref 10 tends connectivity matrix 29 poorer ization performance smaller basin attraction expected 8 work suggests tive geometric interpretation hebbian unlearning original version sec v shown ometric transformation accomplished unlearning rule similar one performed linear ceptron particular perceptron feeded noisy sion memory algorithm probe rious state vanishing n yet overlap stored memory weight row nectivity matrix become aligned ble pattern favoring correct classiﬁcation hence geometric transformation permit reach perfect retrieval memory noise added process implies maximally wide basin attraction remark eﬀect consequence tor landscape network alone procedure completely unsupervised hence close analogy eﬀects two formally ent rule ha emerged conclude some highly speculative consideration possible implication work result shed light substantial mechanism might help derstand real process lying behind synaptic development brain 30 mammal 31 context portant remind hebbian unlearning ha troduced simultaneously another remarkable bution crick mitchinson 32 paper tured sort reverse learning procedure assigned ﬁrst time biological function dream coming description mere epiphenomenon neural activity procedure strongly resembles hopﬁeld unlearning kinouchi kinouchi 33 provided some biological example might encourage investigate type synaptic transformation even though clear evidence existence ha not shown yet recently published review hoel 34 orates evolutionary signiﬁcance dream term generalization performance neural network dream due hallucinoid content responsible noise injection learning procedure analogy dropout 35 technique used machine learning dreaming neural network thus able generalize better avoiding importance noise addition learning also suggested recent study try increase generalization deep neural network ing inspiration biology 36 37 according work local action synapsis ensure decorrelation stored memory thus avoid fusion believe found hopﬁeld model coherent picture hebbian ing not form reverse learning repeatedly stated past literature rather responsible learning noisy version memory help minimize one possible development research might deal memory presenting strong structural correlation image case linear regression tion one performed linear perceptrons may not suﬃcient ensure classiﬁcation ha shown hebbian unlearning work well even 18 suggest correlation encoded quenched disorder system thus glassy landscape attractor might drift learning path right optimal region space tions another direction future work would veriﬁcation result type model biologically reliable network 38 random neural network 39 40 continuous attractor neural network 41 last class system aim describing functioning spatial memory encoded hippocampal synapsis might open way experiment inferential analysis real data allowing proper research physiologic mechanism brain finally idea could possibly ﬁnd application analogy training physical system allosteric network see 42 43 acknowledgment thank dario lippi important contribution ﬁrst stage work 1 hopﬁeld neural network physical system emergent collective computational ability proc natl acad sci usa 79 2554 1982 2 amit modeling brain function world tractor neural network cambridge university press 1989 3 peretto collective property neural network statistical physic approach biol cybern 50 51 1984 4 hebb organization behavior chological theory wiley 1949 11 5 amit gutfreund sompolinsky storing ﬁnite number pattern model neural network stat phys 15 1530 1985 6 gardner structure metastable state hopﬁeld model phys 19 1986 7 benedetti dotsenko fischetti marinari oshanin recognition capability hopﬁeld model auxiliary hidden neuron phys rev e 103 2021 8 kanter sompolinsky associative recall ory without error phys rev 37 1 380 1987 9 dotsenko yarunin dorotheyev statistical mechanic neural network modiﬁed interaction stat phys 24 2419 1991 10 plakhov semenov modiﬁed unlearning procedure enhancing storage capacity hopﬁeld work symposium neuroinformatics neurocomputers 1992 11 hopﬁeld feinstein palmer unlearning ha stabilizing eﬀect collective memory nature 304 158 1983 12 fachechi agliari barra dreaming neural network forgetting spurious memory reinforcing pure one neural network 112 24 2019 13 folli leonetti ruocco maximum storage capacity hopﬁeld model front comput neurosci 10 144 2017 14 gardner space interaction neural network model phys 21 257 1988 15 gardner gutfreund yekutieli phase space interaction neural network deﬁnite symmetry phys 22 1995 1989 16 forrest learning neural network phys 21 245 1988 17 van hemmen ioﬀe uhn vaas creasing eﬃciency neural network learning physica 163 386 1990 18 van hemmen klemmer unlearning relevance rem sleep decorrelating correlated data edited taylor caianiello cotterill clark springer london uk 1992 19 wimbauer klemmer van hemmen versality unlearning neural network 7 261 1994 20 minsky papert perceptrons introduction computational geometry mit press 1969 21 krauth nadal mezard role bility symmetry dynamic neural network phys 21 13 2995 1988 22 theumann space interaction deﬁnite try neural network biased pattern problem phys rev e 53 6 6361 1996 23 pastore franz satisﬁability transition asymmetric neural network preparation 2022 24 barber scaling phase transition critical phenomenon vol 8 academic press london 1983 pp 25 altarelli monasson semerjian poni connection statistical physic handbook satisﬁability edition chapter 22 io 2021 pp 26 van hemmen hebbian learning correlation trophe unlearning neural system 9 153 1998 27 kleinﬁeld pendergraft unlearning increase storage capacity content addressable memory biophys j 51 47 1987 28 horas pasinetti unlearning procedure yielding associative memory neural network phys 31 1998 29 personnaz guyon dreyfus information storage retrieval like neural network journal de physique lettres 46 359 1985 30 farooq dragoi emergence preconﬁgured plastic sequence early postnatal development science 363 168 2019 31 payne nadel sleep dream memory solidation role stress hormone cortisol learn mem 11 6 671 2019 32 crick mitchison function dream sleep nature 304 111 1983 33 kinouchi kinouchi dream noids itinerant dynamic neural network elaborating unlearning hypothesis arxiv 2002 34 hoel overﬁtted brain dream evolved assist generalization pattern 2 5 100244 2021 35 srivastava hinton krizhevsky sutskever salakhutdinov dropout simple way vent neural network overﬁtting journal machine learning research 15 1929 2014 36 tadros krishnan ramyaa logically inspired sleep algorithm increased ization adversarial robustness deep neural work international conference learning tations 2019 37 dapello feather le marque cox dermott dicarlo chung neural population geometry reveals role stochasticity robust ception neurips proc 2021 38 treves amit metastable state metrically diluted hopﬁeld network phys 21 3155 1988 39 hwang folli lanza parisi ruocco zamponi number limit cycle asymmetric neural network stat mech theory experiment 5 053402 2019 40 hwang folli lanza parisi rocchi ruocco zamponi number limit cycle diluted neural network stat phys 181 6 2304 2020 41 battista monasson oﬀin optimal learning multiple manifold attractor neural network phys rev lett 124 048302 2019 42 pashine hexner liu nagel rected aging memory nature greed science vances 5 2019 43 keim paulsen zeravcic sastry nagel memory formation matter review modern physic 91 035002 2019