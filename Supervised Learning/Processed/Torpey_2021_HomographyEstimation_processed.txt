explicit homography estimation improves contrastive learning preprint david torpey school computer science applied mathematics university witwatersrand johannesburg south africa richard klein school computer science applied mathematics university witwatersrand johannesburg south africa january 14 2021 abstract typical contrastive algorithm us similarity measure latent space supervision signal contrasting positive negative image directly indirectly although utility algorithm ha improved recently still bottleneck hindering widespread use compute needed paper propose module serf additional objective contrastive learning paradigm show inclusion module regress parameter afﬁne transformation homography addition original contrastive objective improves performance learning speed importantly ensure module doe not enforce invariance various component afﬁne transform not always ideal demonstrate effectiveness additional objective two recent popular algorithm perform extensive experimental analysis proposed method show improvement performance considered datasets ﬁnd although general homography afﬁne transformation sufﬁcient improve performance convergence afﬁne transformation performs better case keywords learning contrastive learning deep learning 1 introduction pool data particularly unstructured data image text video audio vast majority data unlabelled process labelling expensive environment make algorithm leverage fully unlabelled data particularly useful important algorithm fall within realm unsupervised learning particular subset unsupervised learning known learning ssl ssl paradigm data provides supervision signal algorithm somewhat related another core area research known transfer learning 1 context computer vision mean able encoder network ofﬂine large varietal dataset followed bespoke task hand many transfer learning application remains dominated supervised learning technique 2 3 4 5 model large labelled dataset however learning technique recently come fore potential alternative perform similarly downstream task requiring no labelled data technique create supervision signal data one two way one approach technique deﬁne task beforehand neural network trained solve inpainting 6 jigsaw puzzle 7 way task kind proxy solved produce reasonable representation downstream visual task image video recognition object detection semantic segmentation approach class technique known contrastive method 8 9 10 method minimise distance maximise similarity latent representation two augmented view input image simultaneously maximising distance 12 jan 2021 preprint january 14 2021 negative pair way method enforce consistency regularisation 11 approach learning contrastive method often outperform task method current learning however contrastive method several drawback requiring prohibitively large batch size memory bank order retrieve negative pair sample 8 9 intuition behind proposed module any system tasked understanding image beneﬁt understanding geometry image object within afﬁne transformation geometric transformation preserve parallelism line composed any sequence rotation translation shearing scaling homography generalisation notion include perspective warping homography need not preserve parallelism line however ensures line remain straight mathematically homography shown equation ha 8 degree freedom applied vector homogenous coordinate afﬁne transformation ha form added constraint hφ 1 1 ability know source image wa transformed get target image implicitly mean learned something geometry image afﬁne transformation generally homography natural way encode idea forcing network estimate parameter random homography applied source image thereby force learn semantics geometry geometric information supplement signal provided contrastive loss loss latent space paper propose additional module used tandem contrastive learning technique augment contrastive objective additional module highlighted figure 1 module simple used supplement contrastive algorithm improve performance supplement information learned network converge faster module essentially additional stream network objective regressing parameter afﬁne transformation homography way objective network must solve minimising original contrastive objective learning parameter homography applied one input image vector difference latent representation force latent space encode geometric transformation information learning regress parameter transformation mlp take vector difference two latent representation input x transformed analogue including information way network not invariant component transformation still able use signal learning moreover approach serf novel hybrid task contrastive learning enforcing consistency regularisation 11 figure 1 proposed architecture highlighted box highlight additional proposed module tasked regressing parameter afﬁne transformation homography extensive empirical study show additional objective regressing transformation parameter serf useful supplementary task contrastive learning improves performance considered datasets term linear evaluation accuracy convergence speed remainder paper structured follows section 2 cover related work area learning going detail necessary section 3 detail proposed method ﬁrst introduce framework set notation make formalisation approach clear delve detail behind architecture choice various part system followed comprehensive set experiment section 4 including result various datasets well ablative study finally paper concluded some closing remark section 5 2 preprint january 14 2021 2 related work ssl popular research area within computer vision previous approach broadly classed two main category ﬁrst task manually deﬁned goal algorithm solve task 12 13 14 15 16 example method include inpainting 6 colourising 15 jigsaw puzzle 7 patch prediction 13 geometric image transformation 17 using rotation task 14 18 some approach deal geometric image transformation similar spirit method 14 18 two variant predicting image rotation auxiliary task unsupervised learning perhaps closer method 17 set transformation applied image patch network trained manner predict surrogate class deﬁned set transformed image patch minimising log loss method however investigates different particular set transformation deﬁne afﬁne transformation general homography show used aid performance using transformation parameter target need regressed using error contrastive algorithm manner discrepancy network ability predict actual value parameter afﬁne serf additional supervision signal somewhat related approach proposed method within task domain proposed 12 propose augment learning process supervised learning algorithm additional label constructed using label label rotation class colour permutation importantly create loss function based joint distribution original supervised label augmented label way network not forced invariant transformation consideration since ha shown hurt performance 12 method different propose module integrated speciﬁcally algorithm additionally regress transformation parameter real vector space not create class parameter broad category ssl based contrastive learning 8 9 19 class technique represent current learning outperforming task method approach learn representation contrasting positive pair sample negative pair sample latent space method typically require careful attention paid negative sample additionally disadvantage requiring prohibitively large batch size memory bank mechanism retrieve relevant negative sample one popular method known simclr 8 simclr general framework contrastive learning vanilla formulation consists encoder network parameterised cnn usually variant resnet 20 mlp projection head input image sampled two distinct view input image computed using random augmentation augmentation consists colour jiterring gaussian blurring random cropping two view sent encoder network produce two latent representation latent vector sent projection head produce ﬁnal latent vector vector loss computed case simclr loss normalised temperatured recent approach proposed 21 byol somewhat overcomes aforementioned disadvantage requiring negative pair sample implicitly requires large batch size two separate network weight used tandem learn representation online network consisting encoder mlp projection head mlp prediction network trained predict representation outputted target network training online network parameter updated using backpropagation error derivative computed using error loss however target network parameter updated using exponential moving average way byol overcomes collapsed solution every image produce representation test module simclr byol since two method serve two popular recent approach contrastive ssl some helpful ﬁndings guiding research demonstrated 22 core among 1 standard architecture design work well setting not necessarily work well setting 2 setting larger cnns often mean higher quality learned representation 3 linear evaluation paradigm assessing performance may take long time converge moreover 23 ﬁnd effectiveness pretraining decrease amount labelled data increase performance one particular downstream task not necessarily indicative performance downstream task 3 proposed method ﬁrst introduce mathematical framework discussing method let set base transformation base transformation transformation not decomposed basic transformation interpreted per 21 8 example base transformation include colour jittering cropping horizontal ﬂipping deﬁne 3 preprint january 14 2021 possible base transformation next deﬁne new set base spatial transformation correspond general afﬁne transformation rotation translation scaling shearing full homography afﬁne transformation perspective projection impose following condition 2 reason restriction apparent later transformation tb θ parameterised associated base transformation b transformation parameter θ set possible transformation particular base transformation set b may deﬁned ti tb θ 3 clearly may since some parameter may take any value within compact subset important want able sample inﬁnite sample space training ensure network see variety sample deﬁne augmentation ordered sequence n transformation unique ordering necessarily produce unique augmentation ﬂipping cropping different cropping ﬂipping formally augmentation deﬁned x tbn θn x 4 denote set possible augmentation transformation set ti ati deﬁnition set possible afﬁne homographic transformation example afﬁne transformation homographies seen appendix consider input image x sampled random dataset image x x sample space image sample augmentation b apply x produce augmented view respectively sample transformation cφ apply produce note 1 related homography core assumption relied upon inductive bias introduce model describe proposed architecture depicted figure let mapping f x parameterised cnn mapping g rp h rp parameterised mlps p k dimensionality encoder latent vector projection head latent vector homography parameter vector respectively f g encoder projection head original simclr 8 byol 21 formulation respectively whereas h new mlp tasked estimating homography parameter note regressing parameter general afﬁne transformation 6 whereas full homography brevity denoted stream architecture network shared weight although may case two stream consist network different weight case byol loss function method contains two term first original loss function deﬁned original method simclr squared byol deﬁne ﬁrst term g f g f second term seen forcing network explicitly learn afﬁne transformation homography let latent representation 1 f 1 f 1 send vector difference 1 h produce estimate homography parameter regress parameter using error h 1 φ φ ground truth afﬁne transformation parameter thus complete loss function given h 1 φ 5 vector difference naturally describes transformation needed move architecture learning objective force vector difference transformation vector encode homography interpretation may seen natural intuitive hence term enforces invariance transformation enforces transformation note still completely moreover restriction imposed equation 2 necessary not any transformation cφ sequence would destroy fact 1 related homography example adding cropping transformation would break homography assumption one may add transformation not break restriction colour jitter however not explore may interpret extended architecture solving learning objective 1 contrastive loss differing augmented view image must minimised 2 another network must able estimate homography image explicitly force latent space encode spatial information training 4 preprint january 14 2021 4 experiment experimental setup section present empirical study comparing original simclr byol technique svhn benchmark datasets without proposed module goal not achieve near performance datasets rather demonstrate effectiveness proposed additional homography estimation objective consistent experimental setting case proposed module improves performance linear classiﬁer learned representation improves learning speed experimental setup training simclr byol found table batch size somewhat lower original method since original method focused performance imagenet requires considerably larger batch size perform well some additional experiment ﬁnd performance decreased datasets batch size larger 256 method original simclr byol well method found alternative optimised hyperparameter value learning rate optimiser weight decay worked better proposed original formulation simclr byol attributed similar reason batch size argument use type learning rate decay previous method train number epoch warmup epoch simclr use temperature loss keep image default resolution 32 lastly reported conﬁdence interval average across 10 trial full pipeline trained scratch ssl pretraining linear evaluation table 1 experimental setup ssl simclr byol batch size 256 256 optimiser adam sgd lr momentum weight decay epoch warmup 100 10 100 10 lr schedule cosine decay cosine decay linear evaluation simclr byol batch size 64 64 optimiser adam adam lr epoch 200 200 hardware gpu performance measured per literature using linear evaluation relevant dataset experimental setup linear evaluation seen table freeze encoder only optimise weight ﬁnal linear layer using parameterise f g h parameterised relu mlps figure 1 ensure consistency simclr random crop random horizontal ﬂip colour jitter gaussian blur random grayscale output h dimensional real vector six component deﬁned according parameter general afﬁne transform 1 rotation angle 2 vertical translation 3 horizontal translation 4 scaling factor 5 vertical shear angle 6 horizontal shear angle homography output h instead vector detail transformation see appendix afﬁne homography objective table 2 3 h homography afﬁne respectively see estimation afﬁne transformation homography assist performance allow faster learning particular note statistically signiﬁcant improvement across datasets simclr byol afﬁne posit ability explicitly estimate afﬁne transformation homography input image way allows encoder learn complementary information early training not available contrastive supervision signal ability estimate afﬁne transform homography mean network encoding geometry input image explicit geometric information not directly available contrastive signal interestingly afﬁne objective outperforms full homography case even though afﬁne transformation subset homography perform sweep distortion amount homography ﬁnd consistently performs similar little worse afﬁne transform see appendix b distortion factor becomes large accuracy drop noticeably image distorted learn effectively note incorporating module network result average 30 additional training time versus respective original method signiﬁcance level 1 5 preprint january 14 2021 table 2 performance simclr various datasets mean 99 conﬁdence interval svhn simclr simclr h simclr table 3 performance byol various datasets mean 99 conﬁdence interval svhn byol byol h byol figure 2 show linear evaluation accuracy trained embeddings extracted model epoch ssl training see performance convergence improves inclusion proposed module module accompanying additional objective regressing afﬁne may seen regulariser original contrastive objective evidenced shaded region ﬁgures proposed method result stable performance note relative beneﬁt proposed module diminishes longer training time simclr byol make sense relative beneﬁt module decrease time model learns estimate afﬁne transformation homography accurately epoch progress performed additional experiment simclr byol training model longer note proposed module still outperforms performs similarly original method datasets shown table result also verify ﬁndings previous work ﬁnd larger model trained longer beneﬁts architecture 8 21 10 22 table 4 performance comparison simclr byol various datasets trained 500 epoch svhn simclr simclr byol byol note performance gap simclr byol evident table 2 3 general attributed fact original work byol wa trained long simclr whereas trained number epoch original simclr work posit byol ha simply not converged sufﬁciently since simclr result measured every 10 epoch b byol result measured every 10 epoch figure 2 graph linear evaluation accuracy various point training simclr byol without proposed module shaded region indicates one standard deviation mean 6 preprint january 14 2021 byol eventually outperforms simclr evidenced table 4 consistent ﬁndings original work invariance not always desirable order function f invariant transformation must x f x f tx thus one way encourage invariance neural network f add term loss function minimises l f x f tx 6 some measure similarity rewrite loss function equation 5 term input image x augmentation b cφ get g f ax g f bx h f ax cφax φ 7 ﬁrst term loss corresponding loss clearly form expression mean encouraging representation invariant transformation within however second term loss term corresponding afﬁne parameter estimation not form expression 6 since recast objective parameter prediction task thus not encouraging invariance transformation within provide some empirical evidence table recast module minimise f f 1 mean squared error loss performance decrease notably datasets average relative decrease 8 loss enforced invariance transformation particular encouraged invariance element afﬁne transformation prof problematic table 5 performance simclr transformation invariant invariant representation using afﬁne objective svhn 6 v 9 invariant not invariant delve deeper effect transformation invariance performance extract only 6 9 class svhn dataset new dataset repeat ssl linear evaluation task goal experiment observe performance degrades neural network encouraged invariant certain transformation including rotation setting certain invariance rotation not desirable result also seen table suggests invariance certain transformation not always desirable evidence table 5 suggests transformation invariance particular class transformation ssl may not always desirable may fact hurt performance even may not expected case since no class seem affected transformation invariance like 6 v 9 case detail invariance analysis see appendix transform component analysis table 6 show performance various component afﬁne transformation term linear evaluation accuracy compute result output dimensionality mapping h need changed accordingly namely rotation translation scale shear corresponding output dimensionality 1 2 1 2 respectively interestingly shear alone outperforms three transforms datasets simclr byol hypothesise shear corrupts image four transforms still recognisable way force network learn complex geometry information object transforms leave investigation future work additional ablation perform various additional experiment motivate choice architecture experiment mean encoding latent transformation speciﬁcally concatenation instead vector difference however result marginal performance gain average percentage point across 3 datasets result not seem justify noticeable additional computational cost transformation representation twice size apparent instability training byol module output single real value output rotation scale temporarily replace mse logcosh stabilises training setting 7 preprint january 14 2021 table 6 performance comparison component afﬁne transformation simclr byol performing transformation highlighted bold dataset simclr byol svhn svhn rotation translation scale shear primarily reason opt stick vector difference experiment module operate output g instead performance degrades datasets simclr svhn lastly perform some preliminary experiment two module one operating operating instead one module per original experimental setup resulting performance difference negligible setup svhn posit one module solve homography estimation module operating able solve homography estimation since type random transformation applied stream 5 conclusion network size time training bottleneck modern architecture compete performance level like supervised alternative shown proposed module regress parameter afﬁne transformation homography additional objective assist training bottleneck faster convergence better performance architecture module doe not encourage invariance afﬁne homographic transformation invariance ha previously shown potentially harmful 12 rather proposed module encourages transformation encoded within latent space directly estimating parameter transformation lastly note afﬁne transformation performs better case full homography even though homography superset afﬁne transformation experiment suggest additional ability perspective transform homography doe not yield any tangible beneﬁt regular afﬁne transformation setting reference 1 kafeng wang xitong gao zhao xingjian li dou xu pay attention feature transfer learn faster cnns iclr 2020 2 mingxing tan ruoming pang quoc le efﬁcientdet scalable efﬁcient object detection proceeding 2020 ieee conference computer vision pattern recognition 2020 3 brais martinez davide modolo yuanjun xiong joseph tighe action recognition discriminative ﬁlter bank proceeding international conference computer vision iccv october 2019 4 jeff donahue yangqing jia oriol vinyals judy hoffman ning zhang eric tzeng trevor darrell decaf deep convolutional activation feature generic visual recognition volume 32 proceeding machine learning research page 2014 5 ross girshick jeff donahue trevor darrell jitendra malik rich feature hierarchy accurate object detection semantic segmentation proceeding 2014 ieee conference computer vision pattern recognition page 2014 6 deepak pathak philipp krähenbühl jeff donahue trevor darrell alexei efros context encoders feature learning inpainting 2016 7 mehdi noroozi paolo favaro unsupervised learning visual representation solving jigsaw puzzle eccv 2016 8 ting chen simon kornblith mohammad norouzi geoffrey hinton simple framework contrastive learning visual representation arxiv 2020 9 kaiming haoqi fan yuxin wu saining xie ross girshick momentum contrast unsupervised visual representation learning 2019 8 preprint january 14 2021 10 ting chen simon kornblith kevin swersky mohammad norouzi geoffrey hinton big model strong learner arxiv preprint 2020 11 kihyuk sohn david berthelot li zizhao zhang nicholas carlini ekin cubuk alex kurakin han zhang colin raffel fixmatch simplifying learning consistency conﬁdence arxiv preprint 2020 12 hankook lee sung ju hwang jinwoo shin label augmentation via input transformation 2020 13 carl doersch abhinav gupta alexei efros unsupervised visual representation learning context prediction international conference computer vision iccv iccv 15 page 2015 14 spyros gidaris praveer singh nikos komodakis unsupervised representation learning predicting image rotation arxiv 2018 15 richard zhang phillip isola alexei efros colorful image colorization eccv 2016 16 misra maaten learning representation 2020 conference computer vision pattern recognition cvpr page 2020 17 alexey dosovitskiy jost tobias springenberg martin riedmiller thomas brox discriminative unsupervised feature learning convolutional neural network proceeding international conference neural information processing system volume 1 nip 14 page 2014 18 feng xu tao representation learning rotation feature decoupling 2019 conference computer vision pattern recognition cvpr page 2019 19 mathilde caron ishan misra julien mairal priya goyal piotr bojanowski armand joulin unsupervised learning visual feature contrasting cluster assignment 2020 20 kaiming xiangyu zhang shaoqing ren jian sun deep residual learning image recognition 2016 ieee conference computer vision pattern recognition cvpr page 2016 21 grill florian strub florent altché corentin tallec pierre richemond elena buchatskaya carl doersch bernardo avila pires zhaohan daniel guo mohammad gheshlaghi azar bilal piot koray kavukcuoglu rémi munos michal valko bootstrap latent new approach learning 2020 22 alexander kolesnikov xiaohua zhai lucas beyer revisiting visual representation learning 2019 conference computer vision pattern recognition cvpr page 2019 23 alejandro newell jia deng useful pretraining visual task page 06 data augmentation detail table 7 8 detail parameter value value range used experiment base transformation set respectively transformation applied speciﬁed probability also normalise parameter afﬁne transformation following way consider rotation angle α translation value tx ty shear angle sx sy perform following normalisation parameter α tx ty sx sy h w image height width respectively smax maximum allowed shear table 7 parameter value base transformation set transformation parameter value probability colour jitter brightness colour jitter contrast colour jitter saturation colour jitter hue random resized crop 1 random horizontal flip random grayscale gaussian blurring kernel size 3 3 1 gaussian blurring variance 2 1 9 preprint january 14 2021 table 8 parameter value base transformation set base transformation parameter value rotation 90 translation x 0 25 scaling shear x 25 perspective original b afﬁne transformation c homography original e afﬁne transformation f homography g svhn original h svhn afﬁne transformation svhn homography figure 3 example afﬁne transformation homographies considered datasets b afﬁne v homography addition perspective distortion factor perform sweep across parameter value result seen table interestingly distortion factor perform similarly datasets distortion factor performing best average however factor get large case image become corrupted neural network seemingly learn anything useful table 9 sweep across distortion factor homography using simclr svhn factor factor factor factor c invariance analysis figure 4 5 show confusion matrix particular run svhn dataset simclr enforcing not enforcing transformation invariance interestingly transformation invariance negatively affect class dataset unsurprisingly class 6 9 negatively affected transformation invariance enforced rotation invariance context prohibitive performance subsequently drop recasting module proposed encode transformation predict parameter not enforce invariance instead allow network learn richer supervision signal learning estimate homography 10 preprint january 14 2021 figure 4 confusion matrix svhn enforcing transformation invariance figure 5 confusion matrix svhn not enforcing transformation invariance 11