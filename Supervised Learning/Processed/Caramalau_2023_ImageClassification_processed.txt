caramalau et al active learning 1 active learning image classiﬁcation razvan binod danail 1 imperial college london uk 2 university college london uk 3 school computing kaist daejeon south korea abstract active learning al ha recently gained popularity deep learning dl model due efﬁcient informative sampling especially learner requires labelled datasets commonly sampling training happen stage batch added one main bottleneck strategy narrow sentation learned model affect overall al selection present novel active learning framework age classiﬁcation contribution lie lifting moby one successful learning algorithm al pipeline thus add downstream objective function optimize jointly contrastive loss derive selection function labelling new example finally test study pipeline robustness performance image classiﬁcation task successfully achieved result compared recent al method code available 1 introduction active learning al 1 3 6 13 21 22 32 42 ha recently gained popularity research community goal al sample informative diverse example large pool unlabelled data query label existing al od grouped two based selection criterion ﬁrst group based algorithm 12 15 42 select challenging informative example whereas algorithm select diverse example data set select diverse example existing method ﬁrst project image feature space followed applying sampling technique coreset 28 work fall latter category prominent work method al past year led wide range architecture learn image representation convolutional neural network 42 graph convolutional neural network 6 bayesian network 5 copyright document resides author may distributed unchanged freely print electronic form 4 jan 2023 2 caramalau et al active learning variational 21 32 mention work proven learned feature image directly inﬂuenced performance pipeline however method suffer problem know early selection stage limited annotated example architecture hard train small number training example thus feature extracted model get biased beginning continue become subsequent selection stage problem commonly known problem address problem recent work al explored learning method 3 13 17 20 learning method 4 7 16 19 41 made tremendous progress generating discriminative representation image some method even come close supervised method generalization 10 16 41 one earliest work direction 13 employed consistency loss input image geometrically mented version along objective downstream task however method limit augmentation method primitive form similarly bengar et al 3 introduced trastive learning al method objective optimised form make model affecting feature tiveness selection simple random labelling overpass any al criterion thus existing work direction show explicit limitation address issue method introduce contrastive learning predecessor moby 41 al pipeline jointly train learner choose moby ssl address computational complexity shortcoming previous method simclr 8 byol 16 moby ha two branch shown figure 1 one update gradient query encoder another momentum key encoder parameter momentum encoder updated average query one moreover memory bank key mentum encoder keep long dependency several apart minimising contrastive loss another advantage consists asymmetric structure byol tures distance mean representation al process culminates selection function coreset state contribution achievement following method jointly trained learner quantitative evaluation multiple image classiﬁcation mark 24 svhn 14 fashionmnist 40 performance existing al baseline 2 related work recent advance active learning recent advancement al either oriented 5 12 15 30 32 42 data representativeness 1 28 35 some mixture 2 6 13 21 setting 29 deep active learning ha initially tackled certainty estimation classiﬁcation task wa addressed maximum entropy posterior bayesian approximation monte carlo mc dropout 5 12 15 concurrently method used latent representation sample outperformed one explored uncertainty work recognise coreset 28 caramalau et al active learning 3 revised competitive baseline however recently new trend shifted al acquisition process parameterised module ﬁrst work learning loss 42 mi predictor loss learner still tracking uncertainty vaal 32 deploys dedicated variational vae adversarial distinguish labelled unlabelled image coregcn 6 cdal 1 hand proposed improve data representativeness graph convolutional network categorical contextual sity respectively test method experiment section detail supplementary given shared selection criterion coreset al framework fall category learning ssl past year new pillar ssl ha arisen pervised environment linked goal al learning generalised concept scale data critical expansion various vision application divide ssl two approach 4 7 33 34 contrastive 8 10 16 19 25 41 consistency regularisation look preserve class unlabelled data even series augmentation example mixmatch 4 dino 7 sharpen averaged prediction conversely contrastive learning generally demand pair positive negative example optimising dual network usually deployed evaluate loss either within batch simclr 8 within dictionary key method like moco 19 moby 41 contrastive learning foundational proposal revise technique sec al beginning ssl al evolved parallel only cently ﬁelds merged progress data sampling although ssl brings better visual construct still question labelled information allocate leveraging unlabelled data behaviour csal 13 ﬁrstly integrated mixmatch al training selection follow similar strategy training learns contrastive representation despite csal included experiment directly comparable two new work tackle contrastive learning either acquiring language sample cal 26 adapting sequential ssl simsiam 9 3 cal natural language processing 3 al selection ha no effect random sampling extent omit work analysis 3 methodology section explain pipeline detail first introduce deep active learning image classiﬁcation general followed contribution standard al requires online environment task learner selects optimises simultaneously consider large unlabelled pool data du uniformly random sample label initial subset l du let xl yl l available image corresponding class commonly deploy learner dl model comprising feature encoder f class discriminator objective loss learner categorical deﬁned lclassi fication l yl f xl following al objective decide upon conjunction classiﬁcation performance thus set exploitation rate budget b across l guided selection criterion consequently label new sampled subset l learner exploration factor expressed ber stage n l repeat loop according targeted performance may 4 caramalau et al active learning query feature encoder key feature encoder mlp projector mlp projector queue key query discriminator contrastive loss task optimiser update exponential moving average update task discriminator loss augmentation augmentation contrastive optimiser update active learning selection new labelled batch learner figure 1 training framework proposed conﬁguration query feature encoder play two role map feature task discriminator ﬁcation capture contrastive visual representation asymmetry query key module unlabelled data blue line show contrastive loss exponential moving average dashed green line also include loss training annotation available training end unlabelled sample pas learner al selection limit exploration cycle proposal primarily focus exploitation contrastive learning framework tackle contrastive unsupervised learning approach compared previous al technique 13 17 20 rely consistency measurement brieﬂy key aspect previous ssl technique constituent proposal goal learning aligns al problem plenty unlabelled data costly annotation procedure however former tends learn generalised visual representation aid objective task contrastive learning main approach obtaining representation analysing similarity dissimilarity within data space successful work 7 8 16 19 41 broadly form contrastive learning process main part data augmentation without dual encoder projection similarity approximation dedicated loss function design framework according moby 41 method bine two innovative prior work moco 19 byol 16 visual transformer 11 38 moco 19 pioneer contrastive learning addressing similarity image speciﬁc dictionary sample deploy loss positive example required data augmentation input query together negative key nary training pipeline consists two feature encoders two mlp projector mapping query key consequently key permuted large memory bank positive example inferred online encoder gradient dictionary key need slower update thus gradual momentum update implemented byol 16 hand ha different approach contrastive caramalau et al active learning 5 simpliﬁes moco relying only positive example way memory bank discarded infonce 27 loss also replaced loss given new setting contrastive learning strategy byol indirectly obtained batch normalisation achieve modiﬁcations proposed thus architecture dual encoders asymmetric regard moco byol add prediction module projector online encoder following only positive example input two network augmented version image finally byol preserve common mode data inherits contrastive learning passing slow exponential moving average online momentum encoder intuitively explore contrastive learning strategy moco byol align moby 41 present combined pipeline depicted design perspective adopt asymmetric dual encoders byol shown fig top branch figure 1 culminates discriminator gq match output bottom despite branch consist feature extractor tecture followed mlp projector f q f k query key respectively distinctively moby tackle convolutional neural network cnns feature encoders reduce mlp projector query discriminator single layer batch normalisation relu activation asymmetric pipeline help mimic contrastive learning principle byol however include concept moco minimise objective infonce loss case also need keep memory bank queue key deﬁne contrastive loss sum infonce two augmented version query q different key k lcontrastive exp exp exp q exp q 1 size memory bank τ adjusting temperature 39 training online query encoder branch updated gradient key encoder take average momentum ensure combined design vation moco byol representation concept one hand asymmetric structure indirectly ﬁnds discrepancy average image moving average batch normalisation hand contrastive loss queue different key maintains direct distinctiveness image standard ssl technique moco byol moby demand supervision stage model task objective pipeline seem ineffective al 3 paper extended ssl pipeline moby minimise objective downstream task objective jointly joint objective ﬁnal step clarify presenting joint training procedure data augmentation moby derives augmentation strategy byol input suffer strong transformation proposal choose alternation strong weak augmentation similarly 10 change boosted performance predecessor 19 also observed experiment using only strong augmentation affect optimisation branch weak augmentation comprise horizontal ﬂips random crop addition strong transformation includes colour jitter brightness contrast saturation hue gaussian blur grayscale conversion pixel inversion solarise equation 1 q k referred weak transformation query key q k corresponding stronger version 6 caramalau et al active learning element place change learner existing al work modiﬁed moby train jointly pipeline starting ﬁrst cycle consider available labelled sample xl yl l remaining unlabelled xu query key strong augmentation marked xl q xl k weak resented xl q xl k training alternate batch labelled belled data every inference therefore only contrastive loss unlabelled context given pipeline figure 1 contrastive loss lu contrastive q q k k q k q k obtained q q gq f q fq xu q xu q 2 k k f k fk xu k xu k 3 similarly compute contrastive contrastive loss labelled image addition also minimise categorical lclassification output task discriminator computed contrastive classiﬁcation loss therefore combined loss adjusted scaling factor λc expressed combined lclassi fication contrastive 4 contrastive loss computed continuously regarding classiﬁcation loss cide reduce inﬂuence gradient λc finally worth mentioning exponential moving average queue key updated bottom branch labelled unlabelled sample unlabelled sample selection emphasise proposal minimises loss inspired moby objective jointly enriches visual sentations data compared standard al strategy al selection method rely learner data distribution perform better coreset 28 ha proven effective scenario extent primarily choose selection function brieﬂy coreset aim ﬁnd subset data point constant radius bound loss difference entire data space technique approximated greedy algorithm 37 euclidean space feature encoder output fq x thorough visual selection different al selection approach together coreset presented supplementary 4 experiment quantitative evaluation put forward four image tion datasets 24 svhn 14 fashionmnist 40 model mentioned 3 use different cnns feature encoders show robust architectural change opt 31 quantitative experiment 18 svhn fashionmnist training setting train every selection stage 200 epoch keep batch size dictionary size key set moby noticed experiment contrastive loss converge together 200 epoch learning rate start follows schedule queue encoder caramalau et al active learning 7 figure 2 evaluation left right zoom better view figure 3 evaluation svhn left fashionmnist right zoom better view task discriminator decrease ten time 120 160 epoch however keep momentum scheduler update key bottom branch gradual momentum increment contrastive loss queue ﬁx temperature parameter al setting followed al setting vaal 32 cdal 1 coregcn 6 detail please see supplementary baseline compared method wide range method active learning mc dropout 15 dbal 12 learning loss 42 vaal 32 learning loss 42 coregcn 6 cdal 1 quantitative experiment maintain fair comparison figure 2 report performance chart obtained cdal 1 vaal 32 method use feature encoder ha considerable advantage proposed ssl framework experiment ﬁrst selection stage scenario gain 20 testing accuracy standard learning 62 28 justiﬁes tance joint training framework pipeline reﬁned visual representation direct helpful information set selection method thus notice gradual increase figure 2 7 cles 40 labelled data achieves mean accuracy another observation experiment al performance saturates faster effect occurs due large initial labelled pool relation complexity task exploit contrastive information limit exploratory potential next stage 8 caramalau et al active learning deduct figure 3 well balance initial labelled set relatively low number class dark dashed line display supervised baseline training entire labelled set fashionmnist reach parable performance end cycle svhn surpasses sixth one 95 emphasise relevance augmentation enriching discrete data distribution furthermore grayscale data fashionmnist also eﬁt proposed al framework figure 3 keep result previous baseline coregcn 6 even setting outperform noticeable consistent margin svhn fashionmnist gap least 2 3 method v percentage labelled 10 15 20 25 30 csal table 1 comparison method csal learner comparison leverage unlabelled data contrastive ing al framework previously chose amount data equal able labelled sample therefore every al cycle size increase newly selected data another recent baseline csal 13 however deployed tency measurement mixmatch 4 entire unlabelled data could identify csal captured representation condition compare 2 method table 1 adjust feature encoder 43 experiment maintains initial performance gain imbalanced dataset experiment apart svhn previous experiment uniform distribution class rarely occurs acquisition scenario therefore coregcn simulate imbalanced unlabelled set ten class ha originally 5000 training example decide reduce 5 class 500 image resulting pool 27500 learner contains encoder trained initial set 1000 labelled example apply together baseline coregcn 6 7 cycle figure 4 left present ability outperform previous method even possible environment investigation distribution still part future work figure 4 imbalanced dataset experiment left mitigating distribution shift right zoom better view caramalau et al active learning 9 distribution shift discussion deep al cyclical process learner new labelled data may result optimising different local minimum therefore exploration exploitation al method affected distribution shift every stage experiment commonly shown jaggy curve especially method like mc dropout 15 dbal 12 uncertaingcn 6 address known issue 23 analyse performance entire training set providing 1000 2000 sample dark blue bar class figure 4 right level corresponding classiﬁcation accuracy ﬁrst 1000 random sample tracking performance entire set challenge learner prefer certain class continue select another set image consequently resulted accuracy displayed cyan bar clearly observe minimum shifted different direction only some class improved expense others mitigate shift investigated impact unlabelled sample training sample play key role building dictionary key insight coreset selection data representation target primarily high contrastive sample control effect customising unlabelled set deployed training learner extent propose use unlabelled data lowest contrastive loss figure 4 right displayed green bar performance mechanism initial 1000 set accuracy dark blue get effective linear increase 10 class effect consistent throughout previous quantitative experiment well ssl module variation ablation study continue motivate proposed design set ablation ments varying ssl module left side table 2 swap training pipeline original version moby 41 preceding ssl 10 byol 16 apart moby learner not converge any tion cycle ssl module thus setup large batch speciﬁc training condition low learning rate cosine scheduler learner hardly adapt supervision conﬁguration inference learner stabilise performance regard original version furthermore method distance 4 class accuracy al cycle one argue ssl framework comprises ssl model no labelled 1000 2000 3000 byol moby no labelled 1000 2000 3000 discriminator mlp projector strong augmentation table 2 variation ssl pipeline left ablation study right average testing performance 5 trial 3 al cycle encoder several building block implementation deter developer value signiﬁcant dominance al selection still motivate relevance part table 2 right ablation evaluation successfully remove queue inator mlp projector result detect continuous accuracy drop projecting larger feature simulating asymmetry brings advantage contrastive learning moreover strong augmentation also play crucial role ssl pipeline 10 caramalau et al active learning 1000 2000 3000 semi supervised jointly ssl method supervised byol dino test accuracy table 3 v jointly al left learning parison right testing performance encoder ssl result al ssl al strategy designed joint manner end task despite recent work 3 proposes contrastive learning simsiam 9 adopts learning learner pipeline proposed fails sample better random al paradigm table 3 left experiment training unsupervised contrastive learning second task observe performance suffers context limited labelled example used similarly 3 also notice minor improvement adding selected data coreset extent decided use entire training set experiment ssl 10 dino 7 byol 16 5 limitation conclusion although adapt application expect research effect augmentation momentum encoder another limiting factor analysed ﬁrst al selection stage developer may tune exploitation ratio avoid saturation presented al framework image classiﬁcation main contribution lie contrastive learning pipeline retains higher visual concept aligns downstream task joint training efﬁcient modular allowing diverse backbone sampling function conduct quantitative experiment demonstrate four datasets method show robustness even simulated data pool 6 acknowledgement work part sponsored kaia grant molit nst grant crc 21011 msit kocca grant mcst samsung display poration bb funded whole part centre interventional surgical science wei engineering ical science research council epsrc royal academy engineering chair emerging technology scheme domapper project horizon 2020 fet ga 863146 reference 1 sharat agarwal himanshu arora saket anand chetan arora contextual sity active learning eccv caramalau et al active learning 11 2 william h beluch bcai andreas nürnberger jan köhler bcai power ensemble active learning image classiﬁcation cvpr 2018 3 javad zolfaghari bengar joost van de weijer bartlomiej twardowski bogdan raducanu reducing label effort meet active learning iccvw page 2021 4 david berthelot nicholas carlini ian goodfellow avital oliver nicolas papernot colin raffel mixmatch holistic approach learning neurips 2019 5 razvan caramalau binod bhattarai kim active learning bayesian hand pose estimation wacv 2021 6 razvan caramalau binod bhattarai kim sequential graph tional network active learning cvpr 2021 7 mathilde caron hugo touvron ishan misra hervé jégou julien mairal piotr janowski armand joulin emerging property vision former iccv 2021 8 ting chen simon kornblith mohammad norouzi geoffrey hinton simple framework contrastive learning visual representation icml 2020 9 xinlei chen kaiming exploring simple siamese representation learning cvpr 2021 10 xinlei chen haoqi fan ross girshick kaiming improved baseline momentum contrastive learning arxiv preprint 2020 11 alexey dosovitskiy lucas beyer alexander kolesnikov dirk weissenborn xiaohua zhai thomas unterthiner mostafa dehghani matthias minderer georg heigold vain gelly jakob uszkoreit neil houlsby image worth word former image recognition scale 2020 12 yarin gal zoubin ghahramani dropout bayesian approximation ing model uncertainty deep learning icml 2016 13 mingfei gao zizhao zhang guo yu sercan arık larry davis tomas pﬁster active learning towards minimizing labeling cost eccv 2020 14 ian j goodfellow yaroslav bulatov julian ibarz sacha arnoud vinay shet number recognition street view imagery using deep convolutional neural network 2013 15 marc gorriz axel carlier emmanuel faure xavier active learning melanoma segmentation corr 2017 16 grill florian strub florent altché corentin tallec pierre richemond elena buchatskaya carl doersch bernardo avila pires zhaohan guo mohammad gheshlaghi azar bilal piot koray kavukcuoglu remi munos michal valko bootstrap latent new approach learning neurips 2020 12 caramalau et al active learning 17 jiannan guo haochen shi yangyang kang kun kuang siliang tang zhuoren jiang changlong sun fei wu yueting zhuang active learning model exploit adversarial example virtual label iccv 2021 18 kaiming xiangyu zhang shaoqing ren jian sun deep residual learning image recognition cvpr 2016 19 kaiming haoqi fan yuxin wu saining xie ross girshick momentum contrast unsupervised visual representation learning cvpr 2020 20 siyu huang tianyang wang haoyi xiong jun huan dejing dou supervised active learning temporal output discrepancy iccv 2021 21 kwanyoung kim dongwon park kwang kim se young chun variational adversarial active learning cvpr page 2021 22 seong tae kim farrukh mushtaq nassir navab conﬁdent coreset active learning medical image analysis 2020 23 andreas kirsch tom rainforth yarin gal test active learning principled approach distribution shift outlier 2021 24 alex krizhevsky learning multiple layer feature tiny image university toronto 05 2012 25 junnan li caiming xiong steven hoi learning trastive graph regularization iccv 2021 26 katerina margatina giorgos vernikos loïc barrault nikolaos aletras active learning acquiring contrastive example emnlp 2021 27 aaron van den oord yazhe li oriol vinyals representation learning trastive predictive coding perpetual license 2018 28 ozan sener silvio savarese active learning convolutional neural network approach iclr 2018 29 burr settle active learning literature survey computer science technical report 1648 university 2009 30 seung opper sompolinsky query committee proceeding fifth annual workshop computational learning theory colt 92 page new york ny usa association computing machinery isbn doi 31 karen simonyan andrew zisserman deep convolutional network scale image recognition iclr 2015 32 samarth sinha sayna ebrahimi trevor darrell variational adversarial active learning iccv caramalau et al active learning 13 33 kihyuk sohn david berthelot li zizhao zhang nicholas carlini ekin cubuk alex kurakin han zhang colin raffel fixmatch simplifying learning consistency conﬁdence 2020 34 antti tarvainen harri valpola mean teacher better role model averaged consistency target improve deep learning result neurips 2017 35 ivor tsang james kwok cheung core vector machine fast svm training large data set jmlr 2005 36 laurens van der maaten geoffrey hinton visualizing data using jmlr 37 gert wolf facility location concept model algorithm case study butions management science 2011 38 bichen wu chenfeng xu xiaoliang dai alvin wan peizhao zhang zhicheng yan masayoshi tomizuka joseph gonzalez kurt keutzer peter vajda visual former image representation processing computer vision 2020 39 zhirong wu yuanjun xiong x yu stella dahua lin unsupervised feature ing via instance discrimination cvpr 2018 40 han xiao kashif rasul roland vollgraf novel image dataset benchmarking machine learning algorithm 2017 41 zhenda xie yutong lin zhuliang yao zheng zhang qi dai yue cao han hu learning swin transformer arxiv preprint 2021 42 donggeun yoo kweon learning loss active learning cvpr 2019 43 sergey zagoruyko nikos komodakis wide residual network bmvc 2016 14 caramalau et al active learning detailed setting al experiment quantitative evaluation put forward four image tion datasets 24 svhn 14 fashionmnist 40 contain 50000 training example different labelling tems 10 100 class svhn fashionmnist separated ten class however datasets larger 73257 coloured street number 60000 grayscale image fashionmnist although fashionmnist data not case svhn another perspective deploying grayscale image fashionmnist challenge contrastive learning approach ously customised rgb data model mentioned methodology use different cnns feature coder show robust architectural change opt 31 quantitative experiment 18 svhn fashionmnist moreover ssl comparison csal align encoder 43 al setting characterise budget lect exploiting factor exploration captured number selection cycle initial labelled dataset varies experiment consider 10 5000 entire ing set labelled rest unlabelled data budget limited 5 2500 sample selection repeat cycle seven time second set experiment test method restrictive environment starting set 1000 labelled ilar ﬁxed budget despite expanded exploration 10 cycle reaching 10000 labelled data performance measurement evaluate average 5 trial testing accuracy al framework b selection function analysis figure quantitative evaluation different selection function left right zoom better view proposed pipeline easily adapt multiple selection method quantitatively motivate choice coreset section therefore caramalau et al active learning 15 evaluate benchmark figure vary selection new budget random maximum class entropy coreset intuitively also analyse effect selecting unlabelled example high contrastive loss benchmark sampling random max entropy beneﬁts le pipeline hand method like coreset suit hypothesis better sampling high contrastive loss detected repetitive example some speciﬁc class explained higher contextual variance category speciﬁcally animal class cat deer dog stronger pattern preferred vehicle one car truck ship better visual analysis simulated experiment ﬁrst ﬁve class svhn take 36 representation query coder output unlabelled data figure sample marked cross construct new labelled set selection behaviour max entropy coreset max entropy highest contrastive loss coreset figure qualitative al selection analysis representation ﬁrst selection stage 5 class svhn zoom better view preted expected left side technique track variant image coreset right side sample according euclidean space