ellen extremely lightly supervised learning efficient named entity recognition haris riaz dumitru mihai surdeanu university arizona tucson az usa hriaz rdumitru msurdeanu abstract work revisit problem named entity recognition ner focusing extremely light supervision consisting lexicon containing only 10 example per class introduce ellen simple fully modular method blend language model linguistic rule rule include insight one sense per discourse using masked language model unsupervised ner leveraging tag identify eliminate unlabeled entity false negative intuition classifier confidence score local global context ellen achieves strong performance dataset using minimal supervision lexicon also outperforms existing considerably complex ner method supervision setting commonly used literature 5 training data evaluate model scenario find outperforms achieves comparable performance setting ellen also achieves 75 performance strong fully supervised model trained gold data code publicly available keywords learning named entity recognition rule language model modular architecture introduction named entity recognition ner task identifying named sometimes numeric tie person organization name drug protein name disease date one earliest formal natural language processing nlp task grishman sundheim 1996 ner remains critical many application question answering information traction yadav bethard 2019 despite tremendous progress observed ner task past almost three decade argue several practical limitation way task generally formalized impact standing method perform best practice particular 1 current setting ner task require amount annotation unrealistic many application example common setting ner us 5 corpus tjong kim sang de der 2003 training data total ken chen et 2020 zheng et 2023 work vacareanu et 2024 observed ner annotation take approximately onds per token practice thus annotating equivalent amount data new domain would take approximately 9 person hour tic many scenario intelligence pandemic surveillance require rapid development custom model domain expert not want come willingly not come 1 2 recent direction use ing icl ner autoregressive based large language model llm perform well chen et 2023 not scale well method due decoder high inference overhead generated token requires forward pas model 3 recent trend rely mostly neural network nns learn ner task ignoring linguistic hint one sense per discourse gale et 1992 might present likely useful setting remedy limitation propose tremely lightly supervised scenario ner only supervision come form lexicon containing 10 example per entity class importantly 10 example selected domain expert doe not access any dataset annotation propose simple ner approach scenario efficient performs well despite limited supervision method us inference strategy training time combine multiple gy including language model several guistic heuristic call method extremely program manager personal communication 26 mar 2024 lightly supervised learning efficient named entity recognition ellen main contribution follows 1 demonstrate effectiveness ing language model commonsense tic rule inspired liao veeramachaneni 2009 aggregated ular architecture approach considerably simpler complex tical method ner nagesh surdeanu 2018 lakshmi narayan et 2019 peng et 2019 zhou et 2022 chen et 2019 clark et 2018 chen et 2020 zheng et 2023 inter alia 2 approach includes novel component called masked language modeling mlm heuristic fully unsupervised ner method achieves 55 precision ner dataset component complement self training well linguistic heuristic setting 3 evaluate method tjong kim sang de meulder 2003 three ferent degree supervision setting derczynski et al 2017 conll proposed setting extremely ited supervision show method achieves score increase degree supervision match method ner setting find method achieves comparable performance also show method continues scale even using data available supervision evaluation find method comparable llm openai openai also obtaining 75 performance fully supervised model trained related work recently large language model llm emerged dominant approach wide ety nlp task including named entity nition wang et 2023 zhou et 2023 show llm consistenly achieve sota mance many ner datasets learning llm also proven useful fewshot ner setting recently shown ashok lipton 2023 however llm cally high inference overhead narayanan et 2023 may not always perform well specialized domain moreover increasing concern data nation golchin surdeanu 2023 demonstrate 2 encountered test data label nlp benchmark datasets sainz et 2023 claim true one evaluation datasets focusing ner not fewshot ner current method clude jointprop zheng et 2023 learning framework jointly try solve relation classification ner using erogeneous graph structure chen et 2020 adapts mixup data augmentation technique sequence labeling train linearly interpolated pair sample method use least 5 labeled data minimally supervised setting argue impractical level supervision ner another class statistical method peng et 2019 zhou et 2022 try solve reliance gold label resorting distant supervision construct lexicon based large dataset independent knowledge base method use pu learning train classifier using only labeled tive example set unlabeled data ing positive negative approach like liu et showed benefit augmenting neural ner tagger external gazetteer however external gazetteer may not always available particular domain contrast propose method ner along line approach taken liao veeramachaneni 2009 one bine generalizability contemporary deep learning intuitive reasoning linguistic sight demonstrate strong performance setting extremely low supervision posit approach rival faceted statistical technique ner pu learning data augmentation among others proposed method method doe not rely any explicitly beled text begin discarding label ner dataset paper ask domain expert generate small lexicon 10 example named entity per class four class per org loc misc lexicon sourced entirely token dataset b constructed without looking any label dataset c doe not rely any external dictionary serf sole source gold supervision method domain expert able construct lexicon refer table 1 le 30 minute believe figure 1 proposed method illustrated refers subset unlabeled data added back labeled data retraining next iteration ospd refers one sense per discourse rule global rule indicate rule described section color used figure represent decreasing quality generated annotation three stage stage green category entity org reuters puk nato honda ajax terdam motorola psv eindhoven pkk hansa rostock commonwealth loc germany australia britain spain italy london russia china japan new york misc dutch british french russian german iraqi israeli english australian per clinton yeltsin arafat lebed wasim akram waqar younis mushtaq ahmed netanyahu williams rubin table 1 lexicon generated domain expert lowing criterion outline section lexicon serf seed set entity model sole source gold supervision 10 example per class reasonable number producing informative lexicon minimum amount effort required similar relation extraction focus 5 shot han et 2018 use lexicon annotate small portion entirely unlabeled data term degree sion refer table 4 annotate entity present conll training data using lexicon refer annotated subset l remaining unlabeled data u following convention traditional learning literature high level approach extends simple algorithm presented liao veeramachaneni 2009 shown algorithm however argue procedure delineated algorithm abstract offering no guidance precise sequence ner classifier ck any linguistic rule applied extract new data ambiguity could potentially lead common fall inherent classic approach study determining optimal order ner various approach combined proved nuanced task work motivated curriculum learning argues better model learned training data presented meaningful order lustrates gradually complex example gio et 2009 sachan xing 2016 paper adjust principle mean gradually noisier example based observation critical initial model algorithm 1 simple ner algorithm 1 given 2 l small set labeled training data 3 u unlabeled data 4 k iteration 5 step 1 train ner ck based l 6 step 2 extract new data based ck 7 step 3 add l 8 end higher quality reduce noise future tions inspired idea propose intuitive framework illustrated figure 1 fectively combining linguistic rule language model et 2023 design framework simultaneously balance two orthogonal goal avoiding pitfall classic model failing correct error instead amplifying extent possible still conceptually similar high level proposed ner method fully modular us neural component blend various linguistic statistical heuristic sieve lee et 2013 first describe heuristic describe integrated ellen framework unsupervised entity recognition using masked language model mlm motivated observation any language model subjected pretraining via masked guage modeling mlm objective likely acquires mantic syntactic world knowledge esize capability discern named entity also inherently embedded within model present novel fully unsupervised algorithm implemented rule ular architecture allows u gain additional free supervision beyond small lexicon ten entity algorithm relies small lm liu et leverage icon extract new named entity unlabeled data unlike recent approach ner particularly ner involve either current based model many benchmark nlp task key advantage using deberta relative positional encoding allows model generalize better longer sequence prompting llm example ject ner ability chen et 2023 involve structing dynamic template based label aware pivot word approach much simpler constrained first use simple guistically inspired regular expression based po tag detecting named entity span po tag ral proper noun po tag assigned preposition unlabeled portion conll training dataset rule detect named entity boundary precision shown table note rule ably simpler efficient recent approach tity typing span identification method shen et al 2023 span classification approach adopted arora park 2023 chen et al 2022 typically approach initially train model span classification followed model entity type classification precision recall score table 2 score regex rule detecting named entity boundary entity span identified label using masking heuristic intuitively method selects entity label whose exemplar con fill span highest likelihood formally first mask span identified regex number mask token equal token included span example span john doe sentence john doe happy masked mask mask happy iteratively fill lexicon entry length across entity class keep track token probability example entry dole person lexicon tokenized produce sentence le happy lastly select entity label based following formula c arg max k max j 1 n n x p ti ith masked token example x sentence masked token n total number mask rent example 2 example use bert tokenizer convention token word readability j index iterates exemplar current entity label k iterates entity class con exemplar first compute average probability token pick exemplar highest probability given lexicon probability ing entity label lastly select entity label c highest probability denote rule masked language modeling mlm heuristic demonstrate ner effectiveness development set fully unsupervised fashion ble shown table score mlm heuristic 56 development set iteration procedure shown figure 1 mlm used intermediate stage annotate subset unlabeled data u eventually added back entity type precision recall overall loc misc org per table 3 masked language modeling heuristic fully unsupervised ner algorithm development set obtains score 60 overall precision dynamic window filtering lightly supervised setting ner model tend suffer unlabeled entity problem described li et 2021 entity sentence may not fully annotated tends seriously degrade model performance since model treat unlabeled entity negative instance even method may not sufficient completely alleviate false negative problem since susceptible confirmation bias arazo et 2020 erroneously predicted likely deteriorate model performance subsequent round training contrast li et 2021 method us negative sampling avoid training ner unlabeled entity propose simple efficient linguistically inspired algorithm controlling effect false negative sparsely annotated data setting refer algorithm dynamic window using po tag intuition token labeled posse po tag nnp singular proper noun highly likely unlabeled named entity thus discarded ner training data implement algorithm following rule slide contextual window across sentence labeled subset data l named entity segment encounter whose label known create window size w dynamically expands direction around labeled entity token also tagged po tag nnp singular proper noun also emphasize po tag inherently noisy since obtained external based po tagger wa trained exclusively penn treebank corpus marcus et 1993 not use gold po tag conll data example illustrates rule example 1 eu reject german call boycott british lamb suppose eu british named tie known label also proper noun suppose german also proper noun unknown named entity label thus currently labeled dynamic window ing creates contextual window around eu british expanding size encounter token german algorithm would thus break original example two new segment 1 eu reject 2 call boycott british every stage method apply dynamic window filtering data ner trained includes initial set sparsely annotated gold data augmentation data extracted unlabeled data u iteration find algorithm achieves goal method presented li et 2021 discarding named entity unknown label much simpler computationally cheaper tag available many language see obtained various model guage processing tool note alternative dynamic windowing ist managing unlabeled entity instance demonstrated vacareanu et 2024 unlabeled tities remain ner training data impact mitigated excluding loss lation backpropagating only token global rule lightly supervised ner model may confuse named entity organization son organization location vice versa due shared context remedy ply series commonsense linguistic rule aggregated prediction ner model masked language modeling mlm heuristic apply rule disambiguating named entity segment tagged person per end company suffix update label segment including company suffix org example entity segment walt disney tagged per immediately lowed company suffix rule would force whole segment walt disney org similarly any named entity segment tagged location followed follows segment tagged organization update label location segment organization segment org additionally observe many instance validation set consist terse port score game sport team organization entity also semantically overlap location name ample name somerset could refer county england loc cricket club org mon lightly supervised model confuse label assigned example edy propose additional heuristic identifies segment labeled location loc segment followed score token least two integer number resembling force label org one sense per discourse amalvy et 2023 demonstrated significance local global context enhancing efficacy based model ner work harness metadata provided integrate one sense per discourse ospd principle gale et 1992 approach primarily conceived word sense ambiguation ospd posit term sense main consistent repeatedly used within cohesive discourse operationalize idea asserting named entity predominant sification within conll discourse lean towards particular label instance entity within discourse adopt dominant label instance ibm appear five time org twice use regular expression detect score token integer pattern hyphen respectively method dictate mention ibm labeled org due majority occurrence rule learning classifier confidence effectively guide inclusion unlabeled data nonetheless contemporary deep neural network often produce overconfident prediction ness heuristic outlined liao veeramachaneni 2009 adopt smoothed generalized cross entropy loss zhang sabuncu 2018 dimachkie 2023 ha shown regulate calibrate model prediction include following rule method any segment token classified org loc per classifier confidence score 8 find mention segment within conll ument force label high confidence segment known heuristic addition high confidence segment end company suffix remove company suffix apply mention property remaining segment apply rule high confidence segment begin person title list common english honorific furthermore segment ending company suffix starting son prefix remove affix retaining context form new previously unseen sentence reclassify example suppose sentence training data per segment tagged high confidence follows meeting wa led ing person title would yield new sentence meeting wa led dicted label altered segment new sentence differ affix removal especially classified without high confidence designate sentence inclusion subset subset reintroduced training data subsequent learning ation illustrated figure apply heuristic sieve order decreasing precision minimizing dependency lexicon minimize dependency algorithm lexicon chosen domain pert outlining process must follow picking lexicon using simple regular expression defined section able detect named entity boundary high sion harvest named entity candidate training data fully unsupervised experiment empirically set manner candidate ranked quency occurrence data presented domain expert tasked select frequent unambiguous one class class lexicon not contain tie overlap another class enforcing criterion objectivity implicitly minimize chance multiple domain expert picking vastly different lexicon thereby minimizing effect lexicon variability method ellen integrating neural symbolic component figure 1 depict framework amalgamating heuristic determining subset unlabeled data u augment labeled set l learning ation procedure involves three stage initial subsequent intermediate stage cluding stage selection criterion straightforward only sentence entity bel update due heuristic considered approach aim expand model knowledge within cycle curtailing classic fall prediction masked language modeling heuristic mlm combined ner favoring mlm due ner initial weakness heuristic reliant ner output ferred global rule ospd applied solely sentence modified mlm ner tures training mlm output diate phase give precedence mlm heuristic solely target ner diction global rule ospd extend any ner sentence phase relax constraint allowing full training model robust gleans any residual data u final training iteration experimental result data setup evaluate method using three different degree supervision see table 4 define degree supervision percentage named entity annotated tive total number entity present data first setting proposed extremely lightly supervised setting equivalent term degree supervision 1 term number labeled sentence borrow second 5 data setting sponds first 700 sentence training split current proaches ner chen et 2020 zheng et 2023 however unlike chen et al 2020 us fairseq ott et 2019 augmenting unlabeled data equivalent german sample unlabeled sentence without any mentation third setting fully supervised setting evaluate effectiveness ellen ace wang et 2021 rent sota method deberta et 2023 finetuned summarize three different source pervision experiment follows 1 1 data setting use unambiguous lexicon produced domain expert sisting 10 example four class misc org loc per 2 5 data setting setting also tract unambiguous lexicon entity within first 700 sentence training split adhering consistent tion unambiguous described section within class must not lap class lexicon comprising 98 example misc class 174 org 189 loc 274 per used annotating unlabeled data mlm fully supervised setting setting not use lexicon annotating data since gold label available however since mlm heuristic section requires lexicon extract unambiguous one mlm labeled sentence training split lexicon tains 868 example misc class 2329 org 1245 loc 3598 per refer appendix c result using 1 labeled data shown table 5 extremely lightly vised setting restrictive typical ner approach find method achieves score test set only supervision come domain expert lexicon doe not use any gold label result indicates method tiveness annotation scarce con like only source supervision 3 random augmentation choosing unlabeled sentence first 700 sentence chosen without randomization keep data setting exactly chen et 2020 jointprop zheng et 2023 10 setting 1 5 supervised supervision degree 100 labeled token 2569 6971 34043 table 4 statistic degree supervision used work 5 term number sentence common setting ner 1 5 setting calculate supervision degree based unambiguous lexicon type precision recall overall loc misc org per table 5 score ellen extremely lightly supervised ting run averaged 3 random seed present metric using official scoring script result using 5 labeled data 5 data setting term degree supervision show method achieves score table 6 forming complex method like pu learning zhou et 2022 model based hierarchical latent variable chen et 2019 ing noise strategy lakshmi narayan et 2019 find also performs favorably compared chen et 2020 without using any unlabeled data portantly highlight method outperforms approach whilst using much fewer exemplar per class method peng et 2019 zhou et 2022 rely lexicon contains person name 748 location name 353 organization name 104 misc entity include table 10 appendix directly taken peng et 2019 illustrate furthermore also highlight figure 4 wang et 2023 show performance ace wang et 2021 resource context specifically ace trained 1 subset term number tences training data score fall 20 70 trained 5 subset although fair comparison method not made due differing definition low resource noteworthy ellen tains score equivalent 1 5 setting respectively gesting method significantly outperforms ace scenario method p r mt noise jointprop table 6 performance conll 2003 5 labeled data noted jointprop zheng et 2023 learning framework run averaged 3 random seed evaluation apply extremely lightly supervised 1 method manner dataset social medium domain ized noisy text using model wa trained only lexicon 10 sample per class provided domain expert proceed evaluate model 17 test dataset aligning prediction model label space see appendix b detail observe ellen achieves comparable performance also achieves relatively strong performance fully vised library ushio 2021 wa actually trained gold data see table 7 result exciting indicates potential method used across domain given relatively small size model tremely light supervision result using full supervision lastly show method also adapted setting table 8 show obtain respectable score use roberta large model able method loc misc org per avg table 7 comparing score ellen evaluated mode fully supervised model test set bel alignment indicates framework ellen report average score 3 different random initialization training run model extremely light supervision relative ace standard supervised classifier note approach effective low source setting full supervision available method may outperform ily due noise introduced various tic propose section 3 mlm one sense per discourse rule may roneously annotate entity belonging class leading model tively retrained some noisy data shown figure 1 model ellen deberta ace wang et 2021 table 8 performance ellen test using available annotation 2003 training data fully supervised setting ellen report average training run 3 random seed error analysis ablation experiment focusing extremely lightly supervised setting observed 30 model error validation data involve confusing org loc entity error attributed combination factor bias validation test data towards sporting event not sufficiently reflected training data b inadequacy global rule refer section differentiate sport team org tions loc nuanced context shire headingley would labeled org sentence yorkshire ingley even though headingley loc c error arising noisy po tag rectly identified entity span dhaka stock exchange would identified two separate tities dhaka stock exchange regular expression leading incorrect label mlm heuristic training confusion org misc class partly misc class lexicon primarily includes nationality doe not fully represent broader scope event product work art ablation dev set table 9 found mlm impactful ponent wa followed dynamic window ing allows method achieve score importantly reintegrating heuristic global enhances mance underscoring collective contribution method effectiveness ablation p r full system mlm cr gr ospd mlm cr gr ospd table 9 ablation major component tem measured validation data cr refers gr refers global ospd refers one sense per course mlm refers masked language model conclusion paper present framework moniously blend linguistics deep learning overcome paucity labeled data ner quiring significantly le supervision previous method entity extraction often dered lack annotated data especially domain llm offer tial remedy not without limitation solution ellen introduces efficient only method enables assembly ner system little half day requiring only single lexicon show ellen strong performance extremely low resource setting showing scale well varying supervision level also outperforming complex approach acknowledgement work wa partially supported defense advanced research project agency darpa der habitus program national ence foundation nsf grant hai surdeanu declares financial interest interest ha properly disclosed versity arizona institutional review committee managed accordance conflict interest policy limitation proposed method showcasing promising result setting lightly supervised named tity recognition ner face certain limitation warrant discussion primarily evaluation wa conducted only two flat ner datasets adapting method across broader spectrum datasets especially may feature complex nested entity structure need ther exploration consequently current proach doe not explicitly address challenge associated intricate ner task nested hierarchical tional ner require identification tie within entity recognition novel entity type beyond traditional category some rule employed method domain could limit wider applicability however also light four eleven total rule method domain suming existence language model include masked guage model mlm heuristic free vision exemplar come any domain language dynamic window filtering assumes po tag available given language distinguishes common noun proper noun one sense per discourse ospd simply propagates majority label within ument basic heuristic label propagation only us classifier dence score mlm dynamic window filtering language two component contribute formance ner method shown table 9 certain rule dependent company suffix person honorific not transferable across language adaptation affix adaptability suggests pathway applying method new language provided list relevant suffix honorific used ever may challenge directly applying some rule language use vastly different convention naming entity theless presenting finding not claimed method universally applicable across language domain instead aimed demonstrate integration guistic insight neural network mitigate scarcity labeled data ner framework harmoniously blend element point significant step forward highlighting necessity research extend bility diverse complex ner scenario ethical consideration work utilizes two public datasets named entity recognition ner one dataset derived social medium domain mainly consists ments small portion may include language some might find inappropriate fensive furthermore approach incorporates language model thus any bias inherent model due data would also apply work garding selection named entity seed see section effort made minimize subjectivity creation lexicon oretically possible proposed method used intentionally introduce bias ner model however believe apart potential issue already mentioned work doe not raise any significant ethical concern bibliographical reference arthur amalvy vincent labatut richard four role global local context named entity recognition proceeding annual meeting association putational linguistics volume 2 short paper page toronto canada association computational linguistics eric arazo diego ortego paul albert noel connor kevin mcguinness confirmation bias deep learning 2020 international joint conference neural network ijcnn page jatin arora youngja park ner named entity recognition via two classification proceeding annual meeting association computational linguistics volume 2 short paper page toronto canada sociation computational linguistics dhananjay ashok zachary lipton promptner prompting named entity tion yoshua bengio jérôme louradour ronan lobert jason weston curriculum learning proceeding annual national conference machine learning page haiyan chen shuwei yuan xiang zhang robust named entity recognition insufficient labeled data proceeding international joint ference knowledge graph ijckg 21 page new york ny usa association puting machinery jiaao chen zhenghui wang ran tian zichao yang diyi yang local additivity based data augmentation ner proceeding 2020 conference ical method natural language processing emnlp page online tion computational linguistics jiawei chen yaojie lu hongyu lin jie lou wei jia dai dai hua wu boxi cao xianpei han le sun learning learning named entity recognition mingda chen qingming tang karen livescu kevin gimpel variational sequential er learning kevin clark thang luong christopher ning quoc le sequence modeling training chady dimachkie need medium accessed william gale kenneth church david yarowsky one sense per discourse speech natural language proceeding workshop held harriman new york february shahriar golchin mihai surdeanu time travel llm tracing data contamination large language model corr ralph grishman beth sundheim sage understanding brief history coling 1996 volume 1 tional conference computational linguistics xu han hao zhu pengfei yu ziyun wang yuan yao zhiyuan liu maosong sun fewrel supervised lation classification dataset evaluation proceeding 2018 ence empirical method natural language processing page brussels gium association computational linguistics pengcheng jianfeng gao weizhu chen improving deberta using disentangled embedding sharing carina kauf anna ivanova ter way masked language model scoring arxiv pooja lakshmi narayan ajay nagesh mihai surdeanu exploration noise strategy named entity classification proceeding eighth joint conference lexical computational semantics sem 2019 page minneapolis minnesota association computational linguistics heeyoung lee angel chang yves peirsman nathanael chamber mihai surdeanu dan jurafsky deterministic coreference lution based rule computational linguistics 39 4 yangming li lemao liu shuming shi empirical analysis unlabeled entity problem named entity recognition wenhui liao sriharsha veeramachaneni simple algorithm named entity recognition proceeding naacl hlt 2009 workshop ing natural language processing plearn 09 page usa association computational linguistics tianyu liu yao lin towards improving neural named entity tion gazetteer proceeding annual meeting association tional linguistics page florence italy association computational linguistics yinhan liu myle ott naman goyal jingfei du mandar joshi danqi chen omer levy mike lewis luke zettlemoyer veselin stoyanov roberta robustly optimized bert pretraining approach corr tong luo kurt kramer dmitry goldgof lawrence hall scott samson andrew sen thomas hopkins active learning recognize multiple type plankton journal machine learning research 6 20 mitchell marcus beatrice santorini mary ann marcinkiewicz building large annotated corpus english penn bank computational linguistics 19 2 ajay nagesh mihai surdeanu keep bearing information tion ladder network avoids semantic drift proceeding 2018 conference north american chapter association computational linguistics human language technology volume 2 short paper page new orleans louisiana association computational linguistics deepak narayanan keshav santhanam peter henderson rishi bommasani tony lee percy liang cheaply evaluating inference efficiency metric autoregressive transformer apis openai technical report arxiv openai introducing chatgpt accessed myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier michael auli fairseq fast ble toolkit sequence modeling minlong peng xiaoyu xing qi zhang jinlan fu xuanjing huang distantly vised named entity recognition using unlabeled learning proceeding annual meeting association tional linguistics page florence italy association computational linguistics mrinmaya sachan eric xing easy tions first case study curriculum learning question answering proceeding annual meeting association putational linguistics volume 1 long paper page oscar sainz jon ander campos iker ferrero julen etxaniz eneko agirre lm contamination index accessed tobias scheffer christian decomain stefan wrobel active hidden markov model information extraction international sium intelligent data analysis yongliang shen zeqi tan shuhui wu wenqi zhang rongsheng zhang yadong xi ing lu yueting zhuang ner prompt locating typing named entity recognition asahi ushio jose ner python library based named entity recognition proceeding conference european chapter association computational linguistics system demonstration page online association computational linguistics robert vacareanu enrique gu marco mihai surdeanu active learning sign choice ner transformer ceedings 2024 joint international ence computational linguistics language resource evaluation torino italy pean language resource association shuhe wang xiaofei sun xiaoya li rongbin ouyang fei wu tianwei zhang jiwei li guoyin wang named entity recognition via large language model xinyu wang yong jiang nguyen bach tao wang zhongqiang huang fei huang kewei tu automated concatenation embeddings structured prediction joint conference annual meeting association computational linguistics national joint conference natural language processing 2021 association computational linguistics tingyu xie qi li jian zhang yan zhang zuozhu liu hongwei wang empirical study ner chatgpt vikas yadav steven bethard vey recent advance named entity nition deep learning model arxiv preprint zhilu zhang mert sabuncu ized cross entropy loss training deep neural network noisy label yandan zheng anran hao anh tuan luu jointprop joint learning entity relation extraction neous propagation proceeding annual meeting association computational linguistics volume 1 long paper page toronto canada association computational linguistics kang zhou yuepei li qi li tantly supervised named entity recognition via positive beled learning proceeding annual meeting association computational guistics volume 1 long paper page 7211 dublin ireland association tional linguistics wenxuan zhou sheng zhang yu gu muhao chen hoifung poon universalner targeted distillation large language model open named entity recognition language resource reference derczynski leon nichols eric van erp marieke limsopatham nut result shared task novel emerging entity recognition association computational linguistics tjong kim sang erik de meulder fien introduction shared task named entity recognition statistic labeling pu learning lexicon type precision recall per loc org misc table 10 data labeling result using lexicon used pu learning method number labeled word precision true labeled total labeled word recall table 10 directly taken work peng et 2019 illustrates data labeling statistic large lexicon sourced ternal dictionary used method stark contrast method lexicon contains only 10 exemplar per class 1 data setting hundred per class 5 data setting evaluation allow model wa trained 2003 fairly compared ting fully supervised model ushio collados 2021 library trained 17 gold data mapped generated label fully supervised model onto label space aligned class conll class org loc per misc based semantic overlap aligned product class misc class conll misc class conll also contains many product name work art no telling jimi hendrix aligned group class org conll many group name test data semantic overlap organization nirvana san diego based upon inspection data group class also includes entity like musical band sport team ganizations political group etc entity fit well within typical conll understanding remaining class corporation location son mapped directly ing equivalent also applied mapping prediction allow model fairly pared evaluated model shown table 7 full test set 1287 sample accessed azure openai service using 0613 model deterministic result borrow prompt format vanilla prompt used xie et 2023 shown figure low evaluation observed issue similar served wang et 2023 llm often fail match output length input tence token count sequence labeling task like ner challenge amplified longer sentence documented table 11 distinguishing alignment error discrepancy number llm generated ner tag versus sentence parsing error llm eration doe not form valid sequence ner label hence not parsed showing pronounced issue mitigate alignment problem used simple approach output fewer ner tag input token padded sequence right tag equalize length output excess ner tag cated surplus right match input token sequence length evaluated aligned corrected prediction llm test set using official script result reported table 7 model misalignment error parsing error 426 80 195 44 table 11 comparison error count prompt used evaluation given entity label set org based given entity label set please recognize named entity token given text return answer list named entity tag text input text answer chatgpt response masked language model mlm inverse breaking tie order obtain robust annotation masked language model mlm only consider entity span xi labeled mlm difference score class predicted highest probability score class predicted second highest probability greater some threshold tclass xi p yi yi tclass likely class label second likely class label according mlm motivated breaking tie active learning method scheffer et al 2001 luo et al 2005 aim select token sample difference top two prediction smallest order increase likelihood confident classification however mlm adopt inverse breaking tie maximize difference top two diction based threshold use different threshold class shown table empirically observe obtain slightly higher score mlm unsupervised ner development set using ferent threshold class instead single threshold value class experiment also empirically observe mlm tends produce robust bilities lexicon entity filling mask slot segmented fewer subwords model tokenizer supported finding kauf ivanova 2023 observe od estimate sentence yield inflated score word hence employ additional heuristic filter lexicon entity only single subword entity believe better method estimating aggregating probability tences contain word explored future work kauf ivanova 2023 troduce one method ha shown address issue attributing uneven likelihood word specifically prof cial mask not only current token also subsequent token part word furthermore given large size lexicon extracted mlm 5 fully supervised setting filter lexicon keep only top 20 entity class sorted frequency occurrence training data class threshold org per loc misc table 12 per class threshold used masked language model mlm implement inverse breaking tie hyperparameters hardware instead regular loss use generalized cross entropy loss function label smoothing dimachkie 2023 training model offer better mean absolute error noise sensitivity cross entropy loss controlled hyperparameter experiment multiple setting vary q along learning rate dynamic window size number intermediate stage total number training iteration search involved 20 run based development partition use dynamic window size 5 batch size 16 training learning rate across data setting show hyperparameters table experiment carried system 2 nvidia rtx 3090 gpus setting stage intermediate stage stage q iteration label smoothing 1 data 1 2 1 4 5 data 1 2 0 3 100 data 1 1 0 2 table 13 hyperparameters use training ellen various supervision setting