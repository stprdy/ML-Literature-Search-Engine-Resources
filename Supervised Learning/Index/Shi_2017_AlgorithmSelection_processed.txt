supervised learning based algorithm selection deep neural network shaohuai shi pengfei xu xiaowen chu department computer science hong kong baptist university csshshi pengfeixu chxw recent deep learning platform rely party library cublas utilize computing power modern hardware accelerator gpus however observe may achieve suboptimal performance library function not used appropriately paper target optimizing operation multiplying matrix transpose another matrix referred nt operation hereafter contribute half training time fully connected deep neural network rather directly calling library function propose supervised learning based algorithm selection approach named mtnn us gradient boosted decision tree select one two alternative nt implementation intelligently 1 calling cublas library function 2 calling proposed algorithm tnn us efﬁcient matrix transpose evaluate performance mtnn two modern gpus nvidia gtx 1080 nvidia titan x pascal mtnn achieve 96 prediction accuracy low computational overhead result average 54 performance improvement range nt operation evaluate impact mtnn training process deep neural network integrated mtnn popular deep learning platform caffe experimental result show revised caffe outperform original one average 28 mtnn revised caffe index algebra matrix multiplication pose gpu deep neural network introduction deep neural network recently achieved great success computer vision speech recognition natural language processing 1 2 forwarding backwarding phase backpropagation based training process deep neural network requires two different form matrix multiplication equation 1 equation 2 dominate training time regular form matrix multiplication two major matrix b represented follows c b 1 b c paper call equation 1 nn operation n mean no transpose another form matrix multiplication multiplied transpose b c bt 2 bt transpose b bt ji bij b paper call equation 2 nt operation mean transpose time complexity schoolbook matrix multiplication k n make large matrix nowadays exist many optimized software library matrix operation including atlas lapack openblas gotoblas intel mkl eigen cublas etc gpus become mainstream hardware accelerator cublas library nvidia becomes major linear algebra library deep learning software tool 3 example sgemm function cublas library running nvidia card achieve 3000 gflops performing matrix plication faster mkl library intel cpu ivybridge 4 some recent work ha proposed understand improve performance nn operation gpus 5 considering complexity gpu architecture challenging design single algorithm single set kernel conﬁguration optimal case hence tuning method ha become attractive approach choosing best algorithm kernel conﬁgurations gpus 6 7 however nt operation not received much attention research community previous work show many deep learning software tool overlook importance nt operation only achieve suboptimal performance some deep neural network 3 paper ﬁrst show performance nt operation cublas often much lower nn operation recent gpus propose simple method called tnn implement nt operation carrying efﬁcient matrix transpose ﬁrst performing nn operation general tnn outperforms cublas large matrix not efﬁcient cublas small matrix order achieve best average performance design algorithm selection method named mtnn intelligently select appropriate algorithm carry nt operation based some gpu architecture information matrix size notice idea algorithm selection date back 1976 8 becomes successful recent year choose optimal implementation set algorithm 9 10 11 order verity effectiveness mtnn integrate popular real world deep learning platform caffe 12 relies cublas accelerate nn nt operation gpus evaluate performance mtnn revised caffe two modern gpus nvidia geforce titan x 17 mar 2017 pascal experimental result show 1 mtnn solution achieves improvement average nt operation cublas 2 revised achieves 28 speedup original caffe tested gpus rest paper organized follows present motivation work section ii introduce related work section iii tnn method described section iv followed mtnn framework section experimental result presented section vi conclude paper discus future work section vii ii motivation deep neural network especially fully connected network 13 multiplication nn ations multiplication nt operation two major computational task training process type matrix multiplication monly implemented sgemm routine blas library practice standard sgemm ha following form c α op op b β c op represents whether matrix transposed not α β scalar simplify calculation ignore second term set α cublas sgemm api cublassgemm second third parameter value op b respectively value op cublas op pose cublas op n no transpose understand performance difference nn nt operation cublas conduct experiment evaluate running time performance sgemm nn nt operation different size input matrix table show detail two tested platform table experimental gpu hardware gpu model core memory core frequency 2560 8 gb ubuntu 1607 mhz titan x 3584 10 gb ubuntu 1417 mhz use palgorithm denote performance speciﬁc algorithm unit gflops illustrate ence pnn pnt run experiment 1000 case different matrix size show distribution resulted fig noted case performance nn pnn much better nt pnt no overhead matrix transpose percentage number case pnn higher pnt 71 62 titan x respectively surprisingly around 20 case gpus low performance nt cublas may caused inefﬁcient memory access element another possible reason source code found cublas us slow matrix transpose algorithm reduce memory footprint 14 observing low efﬁciency issue motivated propose method tnn nt operation ﬁnds transpose b ﬁrst call nn function cublas ﬁnish calculation bt gpus performance tnn better cublas case still exist case cublas outperforms tnn end design algorithm selection approach select appropriate algorithm set tnn nt based supervised learning algorithm notice tnn requires gpu memory large enough store additional bt not case framework simply choose original nt operation 0 50 100 150 200 250 frequency 0 50 100 150 200 250 frequency b titanx fig frequency performance ratio pnn pnt among 1000 tested case gpu last value mean iii related work sgemm algorithm cublas ha intensively mized gpus kernel optimization 5 15 16 17 algorithm 18 6 7 information different level gpu memory access latency 15 instruction computation 5 extracted help increase parallelism gpu kernel achieve excellent performance close theoretical hardware capacity based multiplication algorithm targeting fermi gpu dgemm gemm double precision nath et al 16 propose double blocking algorithm reduce impact latency accessing register shared memory achieve 58 peak performance even though kernel gpu discrepancy among distinct gpus could require different conﬁgurations obtain best performance instead conducting detailed kernel analysis method investigated select optimal conﬁguration achieve better performance kernel 18 6 7 however little work ha done evaluate mance nt operation since bt ji bij perform nt changing access row corresponding column matrix b sgemm routine however might cause extra latency due uncoalesced global memory access conﬂicted shared memory access fetching umn element matrix kernel optimization nt challenging performance depends not only gpu architecture also input matrix size therefore instead optimizing kernel algorithm ﬁrst propose simple approach called tnn alternative sgemm notice tnn signiﬁcantly outperform sgemm many case sometimes performance could worse sgemm end formulate algorithm tion problem order select appropriate algorithm nt operation machine learning approach become useful choosing efﬁcient algorithm high accuracy 9 11 10 spillinger et al 9 exploit svm model 19 predict better implementation matrix multiplication algorithm runtime among two implementation mkl carma three different cpu platform achieves 26 performance improvement average beside svm el applied solve algorithm selection problem 9 11 decision tree classiﬁer also used solve automatic selection sparse matrix representation gpus obtains no average slowdown compared existing ideal approach 11 paper make use machine learning technique choose efﬁcient algorithm proposed tnn original cublas implementation improve average performance calculating c bt iv tnn transpose multiply already show fig 1 directly calculating c bt usually inefﬁcient propose simple tnn method replaces nt operation operation transpose b ﬁrst make use nn overall performance improved tt nn tnn tnt talgorithm computation time algorithm note ttranspose includes time gpu memory allocation release matrix transpose memory bound operation 20 two different way perform matrix transpose place matrix transpose rithm doe not require extra memory space however matrix transposition factored product disjoint circle 21 number circle could much lower rectangular matrix length not uniform result difﬁculty parallelization 14 implementation matrix transposition achieves only gtx 980 peak memory bandwidth 224 telsa peak memory bandwidth 208 respectively single precision 14 contrary matrix transposition exploit gpu shared memory achieve efﬁcient lization gpu memory bandwidth 20 optimized transpose kernel achieves 80 peak bandwidth tested gpus much higher compared algorithm therefore rest gpu memory available store bt perform matrix transpose choose transpose routine implement tnn algorithm tnn shown algorithm 1 tnn 1 procedure tnn b c n k 2 allocate gpu memory transpose b 3 bt cudamemalloc n k sizeof ﬂoat 4 tranpose b gpu store bt 5 transposeongpu b n k bt 6 call gemm cublas nn parameter 7 cublassgemm cublashandler cublas op n cublas op n lda bt ldb c ldc 8 free gpu memory bt 9 cudafree bt algorithm since tnn requires additional transpose operation gpu time used transpose operation ttranspose n k not larger difference nt tnt n k nn tnn n k word guarantee tt nn n k smaller tnt n k ttranspose n k tnt n k n k 3 however performance transpose operation highly affected hardware platform size matrix difﬁcult guarantee equation 3 practice exist case difference tnt n k tnn n k small even tnt tnn like case show experimental result nt tnn fig 2 fig height matrix n height matrix b k width fig 2 using scale value n varied 27 value k also chosen 27 216 form total 1000 case show detailed visual result display value k fig 2 various value ﬁgure red rectangle indicates performance nt better tnn green circle symbol indicates performance nt worse tnn blue dash symbol indicates performance nt tnn equal size symbol determined value pnt nn pt larger symbol size indicates higher value ratio fig 2 noticed some case nt outperforms tnn method especially value k small half case nt better tnn k 128 gpus among tested case maximum speedup tnn nt whilst maximum speedup nt tnn fig 3 easy see great portion case 43 titanx located left side pt therefore perform faster calculation c bt choose nt algorithm tnn algorithm appropriately fig performance comparison nt cublas tnn calculating c bt red rectangle symbol legend indicates performance nt better tnn green circle symbol indicates performance nt worse tnn blue dash symbol indicates performance nt tnn equal size rectangle circle symbol reﬂect value pnt nn pt respectively larger symbol size indicates higher ratio value top two row result bottom two row result titan mtnn supervised learning based algorithm selection method section ﬁrst formulate algorithm selection problem classiﬁcation problem two given input size matrix speciﬁc gpu platform let class denote pt nn pnt class 1 denote pt nn given gpu platform g size matrix k size matrix b n k exists function f g n k 1 4 need learn function ˆ f ˆ f argmin x g n k ˆ f g n k g n k 5 learning function ˆ f regarded binary siﬁcation problem 4 main step learning based method mtnn first need construct training testing data set proper preprocessing data benchmarking performance nt tnn second learn decision model ˆ f training sample supervised machine learning algorithm third evaluate learned model testing data set lastly apply trained model predict better implementation nt 0 20 40 60 80 100 120 140 160 frequency 0 20 40 60 80 100 120 140 160 frequency b titanx fig frequency performance ratio pt nn pnt among 1000 tested case gpu last value mean pt greater equal value pt tnn calculating c bt data collection according result fig 1 choose range matrix size 7 8 16 word n k n k ha 1000 combination measure performance nt tnn calculating c bt let pnt n k pt nn n k denote performance nt tnn respectively two matrix b b difference value pnt n k pt nn n k denoted n k n k label 1 otherwise label record following format n k label type gpu 1000 case tested some sample not ﬁtted memory not included evaluation number valid sample gpu le 1000 sample distribution shown table ii besides variety input size matrix table ii sample distribution tested gpus gpu model titanx 649 535 1 242 406 sample 891 941 total 1832 gpu platform also different thus need extract feature represent different gpus detail tested gpus shown table iii used input feature gpu platform table iii characteristic tested gpus gpu titanx compute capability global mem gb 8 10 sm 20 28 core clock mhz 1607 1417 mem clock mhz 5005 5005 mem bus width 256 384 cache kb 2048 3072 combined different value characteristic gpu table iii input sample x formed 5 dimension gpu speciﬁcation 3 dimension matrix size ﬁrst 5 dimension global memory gm number sm sm core clock cc memory bus width mbw size cache note feature generation 1 computation crucial reduce overhead using predictor runtime ﬁnal format input sample x follows gm sm cc mbw n k label not need normalize input feature using decision tree learning algorithm contrast dimension input feature normalized range 0 1 training svms model training given training set g n k g feature combination table iii want learn function ˆ f ˆ f x pnt x pt nn x pnt x nn x ˆ f x choose tnn otherwise choose nt learning algorithm svm 22 power tool learning algorithm solving classiﬁcation problem ha successfully applied solve algorithm selection problem related multiplication 9 10 another ful learning algorithm decision tree dt also prosperously used solving problem automatic best algorithm selection 11 extended algorithm decision tree named gradient boosted decision tree gbdt 23 24 paper choose gbdt learning algorithm three main reason 1 doe not require input feature normalization since decision tree recursive partitioning based rithm reduces overhead feature preprocess runtime 2 among 10 popular supervised learning algorithm boosted decision tree outperforms algorithm cluding svm traditional decision tree variety tested data set 25 3 prediction time complexity acceptable say h h depth trained decision tree restricted ﬁxed value several algorithm tree decision learning 26 27 cart 28 cart would competitive some case compared others 29 choose cart model training algorithm use implementation gradient boosting framework named xgboost 24 ﬂexible portable highly efﬁcient parameter conﬁguration need consider two main impact setting parameter one hand crucial depth decision tree not deep otherwise increase overhead predictor runtime hand need set proper parameter prediction accuracy high enough paper set maximum depth decision tree 8 number estimator boosting also set step size shrinkage eta 1 minimum loss reduction gamma 0 make boosting algorithm progressive training instead training model separately ent gpus hope model equipped robustness different gpu hardware put input feature dimension vector including 5 characteristic gpu one model training randomly split data set training data set 80 testing data set 20 note 80 training data set include 80 sample gpu remainder used testing data set validate whether chosen model generalize data set presented work evaluation whole data set used training data learn ﬁnal model put application integration use learned model predictor selection system choose better algorithm nt tnn model ha well trained ﬁnal algorithm calculating c bt derived call mtnn mtnn shown algorithm algorithm 2 mtnn 1 procedure mtnn b c n k 2 get gpu property 1 complexity 3 cudadeviceprop gpuprop 4 cudagetdeviceproperties gpuprop devid 5 predict best algorithm 6 int label globalloadedpredictor gpuprop n k 7 select algorithm nt 1 tnn 8 label 1 9 call nt cublas 10 cublassgemm cublashandler cublas op n cublas op lda b ldb c ldc 11 else 12 call tnn procedure 13 tnn b c n k vi evaluation ﬁrst demonstrate evaluation accuracy predictor ﬁgures performance classiﬁer present overall performance improvement trained predictor performance mtnn display well selection system performance classiﬁcation evaluate performance classiﬁcation algorithm use metric classifying accuracy measure classiﬁers average accuracy mean predictor make calculation c bt fast enough case since testing data set imbalanced set larger number negative sample positive sample accuracy negative positive class recorded table iv show detail accuracy table iv accuracy class minimum maximum average negative positive total also make comparison svm algorithm ing axial basis function kernel polynomial kernel commonly used supervised machine learning algorithm use libsvm 30 svm implementation widely used tool parameter svm c gamma input feature normalized range 0 1 learning algorithm traditional decision tree dt also included comparison show gbdt ha better performance term accuracy running efﬁciency tested experimental environment table v learning algorithm performance classiﬁers shown table vi table experimental environment classifier cpu memory frequency intel cpu 64 gb ubuntu ghz table vi comparison svm dt classiﬁer accuracy train time predict time gbdt 7 47 30 dt 1 table vi term prediction accuracy gbdt much better svms dt regarding training prediction efﬁciency gbdt outperforms two type svms even though prediction time gbdt slightly longer dt could neglectable only compared computation time multiplication putting model mtnn algorithm accuracy model trained speciﬁc parameter training sample cross validation ha veriﬁed model admissible 80 training sample exists question many training sample chosen better convergence model mtnn ha higher prediction accuracy use different size training data set ﬁgure many sample proper train predictor 1832 sample x percent selected training data set whole sample used testing data set x selected 10 100 step size training accuracy different size training data set shown fig display tend higher accuracy larger size training data set 0 20 40 60 80 100 percentage training sample 70 75 80 85 90 95 accuracy fig training accuracy different size training data set performance selection section want show much performance improved using mtnn algorithm integrated trained predictor algorithm mtnn integrated predictor trained data set achieve higher performance instead using 80 data training data higher accuracy general see fig 4 100 data training set trained predictor gbdt achieves accuracy classiﬁcation mean selection system make correct decision choose better algorithm nt tnn case presenting statistic result mtnn compared nt tnn visualized comparison mtnn nt tested gpus shown fig compared fig 2 red rectangle indicate performance tnn worse nt reduced small portion mtnn method word case performance mtnn better equal nt only minority case performance mtnn worse nt statistic frequency performance mtnn nt shown fig portion case mtnn outperforms nt titanx show futher optimization space multiplication algorithm pascal gpus fig 2 maximum value pnt nn fig 5 display maximum pnt nn only similar work 9 make comparison statistic way use gow gain worst denote gain performance mtnn worst algorithm sample gow calculated gow pmt nn pnt pt nn min pnt pt nn 6 let lub loss best denote percent loss mtnn best algorithm sample calculated lub pmt nn pnt pt nn max pnt pt nn 7 deﬁne some metric measure performance mtnn compared nt tnn description metric displayed table vii corresponding evaluated value shown table viii table vii metric description metric description mtnn v nt average percent improvement using mtnn versus versus always choosing tn mtnn v tnn average percent improvement using mtnn versus versus always choosing tnn gowavg average gow sample gowmax maximum gow sample lubavg average lub sample lubmin maximum lub sample table viii value performance metric mtnn metric titanx total mtnn v nt mtnn v tnn gowavg gowmax lubavg lubmin table viii see mtnn achieves performance improvement compared use nt algorithm only compared tnn average compared worst case nt tnn mtnn achieves performance improvement average some particalar case some case predictor make wrong decision slowdown performance only word compared best case nt tnn performance mtnn only worse predictor chooses lower performance algorithm nt tnn two gpus speedup time efﬁciency card slightly higher titanx card evaluation caffe test performance mtnn plication integrate mtnn algorithm caffe 12 one popular deep learning framework choose two type fully connected network one mnist data set whose input output dimension small one synthetic data whose input output dimension large type fully connected network variety hidden layer conﬁgured namely 2 3 4 layer conﬁguration detail neural network shown table ix performance comparison two type network running original version fig performance comparison nt mtnn method calculating c bt rectangle symbol legend indicates performance nt better mtnn circle symbol green color indicates performance nt worse mtnn dash symbol blue color indicates performance nt mtnn equal 0 100 200 300 400 500 frequency 0 100 200 300 400 500 600 frequency b titanx fig frequency performance ratio pmt nn pnt among tested case gpu last value mean pmt greater equal value pmt table ix fully connected network configuration evaluation data set mnist synthetic input 784 26752 output 10 26752 2 hidden layer 3 hidden layer 4 hidden layer caffe caffent caffe mtnn caffemtnn displayed fig 7 fig 8 respectively integrating method caffe performance optimized caffe accomplishes slightly improvement mnist data set performance improvement much synthetic data set 256 512 1024 2048 4096 size 0 2 4 6 8 10 12 14 16 time per 2 hidden layer mnist caffent caffemtnn 2 hidden layer 256 512 1024 2048 4096 size 0 2 4 6 8 10 12 14 time per 2 hidden layer mnist caffent caffemtnn b 2 hidden layer titanx 256 512 1024 2048 4096 size 0 10 20 30 40 50 60 time per 3 hidden layer mnist caffent caffemtnn c 3 hidden layer 256 512 1024 2048 4096 size 0 5 10 15 20 25 30 35 40 time per 3 hidden layer mnist caffent caffemtnn 3 hidden layer titanx 256 512 1024 2048 4096 size 0 20 40 60 80 100 120 time per 4 hidden layer mnist caffent caffemtnn e 4 hidden layer 256 512 1024 2048 4096 size 0 10 20 30 40 50 60 70 80 90 time per 4 hidden layer mnist caffent caffemtnn f 4 hidden layer titanx fig performance comparison mnist original caffe optimzed caffe lower better one hand fig 7 noted training time speed caffent caffemtnn close size main reason speciﬁc number neuron two adjacent layer size mb size multiplication decided mb value mb small performance tnn ha no advantage compared original nt cublas explained performance comparison fig 5 many dash symbol side ﬁgure mtnn only par nt cublas exists particular case mtnn slightly worse nt cublas 4096 network 3 hidden layer titanx reason minor slowdown predictor make error prediction may occur only small probability since accuracy predictor 96 hand fig 8 larger neural network input size output size 27652 tested case larger size larger 512 speedup caffemtnn signiﬁcant multiplication mapped 256 512 1024 2048 4096 size 0 200 400 600 800 1000 1200 time per 2 hidden layer synthetic caffent caffemtnn 2 hidden layer 256 512 1024 2048 4096 size 0 100 200 300 400 500 600 700 800 900 time per 2 hidden layer synthetic caffent caffemtnn b 2 hidden layer titanx 256 512 1024 2048 4096 size 0 200 400 600 800 1000 1200 1400 time per 3 hidden layer synthetic caffent caffemtnn c 3 hidden layer 256 512 1024 2048 4096 size 0 100 200 300 400 500 600 700 800 900 time per 3 hidden layer synthetic caffent caffemtnn 3 hidden layer titanx 256 512 1024 2048 4096 size 0 200 400 600 800 1000 1200 1400 time per 4 hidden layer synthetic caffent caffemtnn e 4 hidden layer 256 512 1024 2048 4096 size 0 100 200 300 400 500 600 700 800 900 time per 4 hidden layer synthetic caffent caffemtnn f 4 hidden layer titanx fig performance comparison larger fcn original caffe optimzed caffe lower better case side fig 5 ha numerous green circle mean deep neural network beneﬁt higher performance algorithm mtnn multiplication only impact either table breakdown average running time millisecond speedup data set gpu phase caffent caffemtnn speedup forward backward mnist total forward titanx backward total forward backward total etic forward titanx backward total forward propagation backward propagation training deep neural network demonstrate phase beniﬁts mtnn method break running time one forward phase backward phase experimental result instead showing tested case seperately show statistic result different data set different gpus averaging size layer result shown table noted running time backward propagation almost case main speedup training process contributed forward phase mnist data set whose network size small caffemtnn par caffent sythetic data set whose network size large speedup forward propagation caffemtnn signiﬁcant obtains much speedup compared caffent titanx respectively vii conclusion future work paper ﬁrst ﬁgure low performance cublas calculating tiplication compared multiplication benchmarking variety case accelerate tion multiplication propose simple solution named tnn carry efﬁcient tranpose algorithm ﬁrst make use high performance multiplication algorithm tnn achieves some performance improvement still may fall even worse efﬁciency order obtain best average performance design supervised learning based algorithm named mtnn make gent choose proper algorithm calculating transpose multiplication using boost gradient decision tree learning algorithm mtnn carry transpose multiplication faster routine accuracy 96 evaluate performance algorithm two modern gpus titan x pascal mental result show mtnn method achieves performance improvement compared cublas verify effectiveness mtnn application integrate mtnn popular deep learning framework caffe optimized caffe obtains average 28 improvement fully connected network transpose algorithm used paper method requires extra memory store transpose matrix selection system could not used gpu card ha no enough memory therefore plan exploit place matrix transpose algorithm ﬁnding good memory overhead throughput reference 1 lecun et convolutional neural network url lecun 2015 2 krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural network advance neural mation processing system 2012 pp 3 shi wang xu chu benchmarking deep learning software tool arxiv preprint 2016 4 nvidia cublas nvidia 2017 accessed 5 lai seznec performance upper bound analysis mization sgemm fermi kepler gpus code generation optimization cgo 2013 international symposium ieee 2013 pp 6 kurzak tomov dongarra autotuning gemm kernel fermi gpu ieee transaction parallel distributed system vol 23 no 11 pp 2012 7 abdelfattah haidar tomov dongarra performance design autotuning batched gemm gpus international conference high performance computing springer 2016 pp 38 8 rice algorithm selection problem advance computer vol 15 pp 1976 9 spillinger eliahu fox demmel matrix multiplication algorithm selection support vector machine eec department university california berkeley 2015 10 benatia ji wang shi sparse matrix format selection multiclass svm spmv gpu parallel processing icpp 2016 international conference ieee 2016 pp 11 sedaghati mu pouchet parthasarathy pan automatic selection sparse matrix representation gpus proceeding acm international conference computing acm 2015 pp 12 jia shelhamer donahue karayev long girshick guadarrama darrell caffe convolutional architecture fast feature embedding proceeding acm international conference multimedia 2014 pp 13 theory backpropagation neural network neural network 1989 international joint conference ieee 1989 pp 14 sung chang guil hwu matrix transposition gpus ieee transaction parallel distributed system vol 27 no 3 pp 2016 15 volkov demmel benchmarking gpus tune dense linear algebra high performance computing networking storage analysis sc international conference ieee 2008 pp 16 nath tomov dongarra improved magma gemm fermi graphic processing unit international journal high performance computing application vol 24 no 4 pp 2010 17 tan li triechle phillips bao sun fast implementation dgemm fermi gpu proceeding 2011 international conference high performance computing networking storage analysis acm 2011 35 18 li dongarra tomov note gemm gpus international conference computational science springer 2009 pp 19 cortes vapnik network machine learning vol 20 no 3 pp 1995 20 ruetsch micikevicius optimizing matrix transpose cuda nvidia cuda sdk application note vol 18 2009 21 hungerford abstract algebra introduction cengage ing 2012 22 suykens vandewalle least square support vector machine classiﬁers neural processing letter vol 9 no 3 pp 1999 23 friedman greedy function approximation gradient boosting machine annals statistic pp 2001 24 chen guestrin xgboost reliable tree boosting system proceeding sigkdd conference knowledge discovery data mining san francisco ca usa 2016 pp 25 caruana empirical comparison supervised learning algorithm proceeding international conference machine learning acm 2006 pp 26 quinlan induction decision tree machine learning vol 1 no 1 pp 1986 27 5 program machine learning elsevier 2014 28 loh classiﬁcation regression tree wiley interdisciplinary review data mining knowledge discovery vol 1 no 1 pp 23 2011 29 anyanwu shiva comparative analysis serial sion tree classiﬁcation algorithm international journal computer science security vol 3 no 3 pp 2009 30 chang lin libsvm library support vector machine acm transaction intelligent system technology vol 2 pp 2011 software available