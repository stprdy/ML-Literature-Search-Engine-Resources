see discussion stats author profile publication supervised machine learning algorithm classiﬁcation comparison article june 2017 doi citation 604 read 1 author j e akinsola michael cecilia ibru university mciu 28 publication 732 citation see profile content following page wa uploaded j e akinsola 11 july user ha requested enhancement downloaded file international journal computer trend technology ijctt volume 48 number 3 june 2017 issn page 128 supervised machine learning algorithm classification comparison osisanwo 1 akinsola 2 awodele 3 hinmikaiye j 4 olakanmi 5 akinjobi j 6 department computer science babcock university ogun state nigeria department computer science crawford university igbesa ogun state nigeria abstract supervised machine learning sml search algorithm reason externally supplied instance produce general hypothesis make prediction future instance supervised classification one task frequently carried intelligent system paper describes various supervised machine learning ml classification technique compare various supervised learning algorithm well determines efficient classification algorithm based data set number instance variable feature different machine learning algorithm considered decision table random forest rf naïve bayes nb support vector machine svm neural network perceptron jrip decision tree using waikato environment knowledge analysis weka machine learning implement algorithm diabetes data set wa used classification 786 instance eight attribute independent variable one dependent variable analysis result show svmwas found algorithm precision accuracy naïve bayes random forest classification algorithm found next accurate svm accordingly research show time taken build model precision accuracy factor one hand kappa statistic mean absolute error mae another factor hand therefore ml algorithm requires precision accuracy minimum error supervised predictive machine learning keywords machine learning classifier data mining technique data analysis learning algorithm supervised machine learning introduction machine learning one fastest growing area computer science application refers automated detection meaningful pattern data machine learning tool concerned endowing program ability learn adapt 19 machine learning ha become one mainstay information technology rather central albeit usually hidden part life ever increasing amount data becoming available good reason believe smart data analysis become even pervasive necessary ingredient technological progress several application machine learning ml significant data mining people often prone making mistake analysis possibly trying establish relationship multiple feature 9 data mining machine learning siamese twin several insight derived proper learning algorithm ha tremendous progress data mining machine learning result evolution smart nano technology brought curiosity finding hidden pattern data derive value fusion statistic machine learning information theory computing ha created solid science firm mathematical base powerful tool machine learning algorithm organized taxonomy based desired outcome algorithm supervised learning generates function map input desired output unprecedented data generation ha made machine learning technique become sophisticated time time ha called utilization several algorithm supervised unsupervised machine learning supervised learning fairly common classification problem goal often get computer learn classification system created 21 ml perfectly intended accomplishing accessibility hidden within big data ml hand guarantee extracting importance international journal computer trend technology ijctt volume 48 number 3 june 2017 issn page 129 big distinct data source outlying le dependence scheduled individual track data determined spurt machine scale machine learning fine suitable towards intricacy handling dissimilar data origin vast range variable well amount data concerned ml prospers increasing datasets extra data supply ml structure able trained concern consequence superior value insight liberty confines individual level thought study ml clever find show pattern hidden data 15 one standard formulation supervised learning task classification problem learner required learn approximate behavior function map vector one several class looking several output example function inductive machine learning process learning set rule instance example training set generally speaking creating classifier used generalize new instance process applying supervised ml problem described figure figure 1 process supervised machine learning work focus classification ml algorithm determining efficient algorithm highest accuracy precision well establishing performance different algorithm large smaller data set view classify correctly give insight build supervised machine learning model remaining part work arranged follows section 2 present literature review discussing classification different supervised learning algorithm section 3 present methodology used section 4 discus result work section 5 give conclusion recommendation work literature review classification supervised learning algorithm according 21 supervised machine learning algorithm deal classification includes following linear classifier logistic regression naïve bayes classifier perceptron support vector machine quadratic classifier clustering boosting decision tree random forest rf neural network bayesian network 1 linear classifier linear model classification separate input vector class using linear hyperplane decision boundary 6 goal classification linear classifier machine learning group item similar feature value group 23 stated linear classifier achieves goal making classification decision based value linear combination feature linear classifier often used situation speed classification issue since rated fastest classifier 21 linear classifier often work well number dimension large document classification element typically number count word document rate convergence among data set variable however depends margin roughly speaking margin quantifies linearly separable dataset hence easy solve given classification problem 18 2 logistic regression classification function us class building us single multinomial logistic regression model single estimator logistic regression usually state boundary class exists also state class probability depend distance international journal computer trend technology ijctt volume 48 number 3 june 2017 issn page 130 boundary specific approach move towards extreme 0 1 rapidly data set larger statement probability make logistic regression classifier make stronger detailed prediction fit different way strong prediction could wrong logistic regression approach prediction like ordinary least square ols regression however logistic regression prediction result dichotomous outcome 13 logistic regression one commonly used tool applied statistic discrete data analysis logistic regression linear interpolation 11 3 naive bayesian nb network simple bayesian network composed directed acyclic graph only one parent representing unobserved node several child corresponding observed node strong assumption independence among child node context parent 7 independence model naive bayes based estimating 14 bayes classifier usually le accurate sophisticated learning algorithm anns 5 performed comparison naive bayes classifier algorithm decision tree induction learning rule induction standard benchmark datasets found sometimes superior learning scheme even datasets substantial feature dependency bayes classifier ha independence problem wa addressed averaged estimator 8 4 perceptron classifier weight network found solving quadratic programming problem linear constraint rather solving convex unconstrained minimization problem standard neural network training 21 known algorithm based notion perceptron 17 algorithm used learning batch training instance running algorithm repeatedly training set find prediction vector correct training set prediction rule used predicting label test set 9 5 support vector machine svms recent supervised machine learning technique 24 vector machine svm model closelyrelated classical multilayer perceptron neural revolve around notion side hyperplane separate two data class maximizing margin thereby creating largest possible distance separating hyperplane instance either side ha proven reduce upper bound expected generalisation error 9 6 according 2 22 mean one simplest unsupervised learning algorithm solve clustering problem procedure follows simple easy way classify given data set certain number cluster assume k cluster fixed algorithm employed labeled data not available 1 method converting rough rule thumb highly accurate prediction rule given learning algorithm consistently find classifier least slightly better random say accuracy 55 sufficient data boosting algorithm provably construct single classifier high accuracy say 99 16 7 decision tree decision tree dt tree classify instance sorting based feature value node decision tree represents feature instance classified branch represents value node assume instance classified starting root node sorted based feature value 9 tree learning used data mining machine learning us decision tree predictive model map observation item conclusion item target value descriptive name tree model classification tree regression tree 20 tree classifier usually employ technique evaluate performance decision tree pruned using validation set any node removed assigned common class training instance sorted 9 8 neural network 2 opined neural network nn actually perform number regression classification task international journal computer trend technology ijctt volume 48 number 3 june 2017 issn page 131 although commonly network performs only one vast majority case therefore network single output variable although case classification problem may correspond number output unit stage take care mapping output unit output variable neural network ann depends upon three fundamental aspect input activation function unit network architecture weight input connection given first two aspect fixed behavior ann defined current value weight weight net trained initially set random value instance training set repeatedly exposed net value input instance placed input unit output net compared desired output instance weight net adjusted slightly direction would bring output value net closer value desired output several algorithm network trained 12 9 bayesian network bayesian network bn graphical model probability relationship among set variable feature bayesian network representative statistical learning algorithm 9 interesting feature bns compared decision tree neural network certainly possibility taking account prior information given problem term structural relationship among feature 9 problem bn classifier not suitable datasets many feature 4 prior expertise domain knowledge structure bayesian network take following form declaring node root node ha no parent declaring node leaf node ha no child declaring node direct cause direct effect another node declaring node not directly connected another node declaring two node independent given providing partial node ordering declare node appears earlier another node ordering providing complete node ordering feature machine learning algorithm supervised machine learning technique applicable numerous domain number machine learning ml application oriented paper found 18 25 generally svms neural network tend perform much better dealing dimension continuous feature hand system tend perform better dealing feature neural network model svms large sample size required order achieve maximum prediction accuracy whereas nb may need relatively small dataset general agreement sensitive irrelevant feature characteristic explained way algorithm work moreover presence irrelevant feature make neural network training inefficient even decision tree algorithm not perform well problem require diagonal partitioning division instance space orthogonal axis one variable parallel ax therefore resulting region partitioning hyperrectangles anns svms perform well present nonlinear relationship exists input output feature naive bayes nb requires little storage space training classification stage strict minimum memory needed store prior conditional probability basic knn algorithm us great deal storage space training phase execution space least big training space contrary learner execution space usually much smaller training space since resulting classifier usually highly condensed summary data moreover naive bayes knn easily used incremental learner whereas rule algorithm not naive bayes naturally robust missing value since simply ignored computing probability hence no impact final decision contrary knn neural network require complete record work international journal computer trend technology ijctt volume 48 number 3 june 2017 issn page 132 finally decision tree nb generally different operational profile one accurate not vice versa contrary decision tree rule classifier similar operational profile svm ann also similar operational profile no single learning algorithm uniformly outperform algorithm datasets different data set different kind variable number instance determine type algorithm perform well no single learning algorithm outperform algorithm based data set according no free lunch theorem 10 table 1 present comparative analysis various learning algorithm table 1 comparing learning algorithm star represent best star worst performance 9 ii research methodology research data wa obtained national institute diabetes digestive kidney disease wa made available online university california irvive website 2017 data wa chosen accuracy ha also anonymized therefore confidentiality ensured number attribute 8 one class making attribute follows number time pregnant plasma glucose concentration 2 hour oral glucose tolerance test diastolic blood pressure mm hg triceps skin fold thickness mm 5 serum insulin mu body mass index weight height diabetes pedigree function age year class variable 0 1 table 2 class distribution class value 1 interpreted tested positive diabetes class value 0 interpreted tested negative diabetes class value number instance converted value attribute 0 500 no 1 268 yes international journal computer trend technology ijctt volume 48 number 3 june 2017 issn page 133 table 2 show 768 total number instance used research work 500 tested positive diabetes 268 tested negative diabetes comparative analysis among various supervised machine learning algorithm wa carried using weka weka waikato environment knowledge analysis data set wa trained reflect one nominal attribute column dependent variable value 1 class distribution class variable changed yes mean tested positive diabets value class distribution class variable changed no mean tested negative diabetes essential algorithm require must least one nominal variable column seven classification algorithm used course research namely decision table random forest naïve bayes svm neural network perceptron jrip decision tree following attribute considered comparative analysis time correctly classified incorrectly classified test mode no instance kappa statistic mae precision yes precision no classification order predict accuracy ensure precision different machine learning algorithm research work wa carried tuning parameter two different set number instance first category wa 768 instance 9 attribute follows number time pregnant plasma glucose concentration 2 hour oral glucose tolerance test diastolic blood pressure mm hg triceps skin fold thickness mm serum insulin mu body mass index weight height diabetes pedigree function age year class variable 0 1 one dependent variable eight independent variable second category data set wa 384 instance 6 attribute follows number time pregnant plasma glucose concentration 2 hour oral glucose tolerance test serum insulin mu diabetes pedigree function age year class variable 0 1 one dependent variable five independent variable iv result discussion result weka wa used classification comparison various machine leaning algorithm table 3 show resultswith 9 attribute well parameter considered table 3 comparison various classification algorithm large data set attribute algorithm time sec correctly classified incorrectly classified test mode attribute no instance kappa statistic mae precision yes precision no cation decision table validation 9 768 rule random forest validation 9 768 tree naïve bayes validation 9 768 bayes svm validation 9 768 function neural network validation 9 768 function jrip validation 9 768 rule decision tree validation 9 768 tree time time taking build model international journal computer trend technology ijctt volume 48 number 3 june 2017 issn page 134 mae mean absolute error measure close forecast prediction eventual outcome kappa statistic metric compare observed accuracy expected accuracy random chance yes mean tested positive diabetes no mean tested negative diabetes table 4 show result 6 attribute classification comparison various machine leaning algorithm parameter considered table 4 comparison various classification algorithm smaller data set le attribute algorithm time correctly classified incorrectly classified test mode attribute no instance kappa statistic mae precision yes precision no ation decision table validation 6 384 rule random forest validation 6 384 tree naïve bayes validation 6 364 bayes svm validation 6 384 function neural network perceptron 59 41 validation 6 384 function jrip 64 36 validation 6 384 rule decision tree 64 36 validation 6 384 tree time time taking build model mae mean absolute error measure close forecast prediction eventual outcome kappa statistic metric compare observed accuracy expected accuracy random chance yes mean tested positive diabetes no mean tested negative diabetes international journal computer trend technology ijctt volume 48 number 3 june 2017 issn page 135 table 5 6 ranking precision positive diabetes negative diabetes using different algorithm showing smaller larger data set respectively smaller dataset 384 large data set 768 algorithm precision yes positive diabetes precision no negative diabetes algorithm precision yes positive diabetes precision no negative diabetes svm svm random forest naïve bayes naïve bayes jrip decision table random forest decision tree neural network perceptron jrip decision tree neural network perceptron decision table table 7 8 ranking correctly classified incorrectly classified time build model showing smaller larger data set respectively using different algorithm smaller dataset 384 large data set 768 algorithm time correctly classified incorrectly classified algorithm time correctly classified incorrectly classified svm sec svm sec random forest sec naïve bayes sec naïve bayes sec neural network perceptron sec decision table sec random forest sec jrip sec 64 36 jrip sec decision tree sec 64 36 decision tree sec neural network perceptron sec 59 41 decision table sec table analysis various datasetattributes attribute number mean standard deviation 1 2 3 4 5 6 7 8 international journal computer trend technology ijctt volume 48 number 3 june 2017 issn page 136 discussion show comparison result 768 instance 9 attribute wa observed algorithm higher kappa statistic compared mae mean absolute error also correctly classified instance higher incorrectly classified instance indication higher data set predictive analysis reliable svm nbrequire large sample size order achieve maximum prediction accuracy shown table 3 decision tree decision table least precision show comparison result 384 instance 6 attribute kappa statistic neural network jrip lower compared mae doe not portray precision accuracy show smaller datasets neural network jrip show drastic reduction percentage correctly classified instance comparison incorrectly classified instance however smaller data set svm rf show high accuracy precision whereas decision table built model time compare jrip decision tree therefore le time doe not guarantee accuracy kappa statistic le mean absolute error mae algorithm not show precision accuracy follows algorithm characteristic not used data set not show precision accuracy table precision larger data set smaller data set svm reflecting algorithm highest prediction also table 5 show svm algorithm highest precision smaller data set tale 7 8 show comparison percentage correctly classified incorrectly classified smaller large datasets respectively time build model table 7 result reveal naive bayes jrip algorithm fastest time build however percentage correctly classified lower jrip show time build model not tantamount accuracy vein svm ha highest level accuracy time second comparing result table network perceptron wa third correctly classified algorithm mean neural network performs well large dataset compared small data set also result show decision table doe not perform well large dataset large svm algorithm show highest classification larger dataset higher precision table 9 show mean standard deviation attribute used research reveals plasma glucose concentration attribute 2 ha highest mean well diabetes pedigree function attribute 7 lowest mean indication strong influence small data set however lower standard deviation sd not necessarily desirable mean diabetes pedigree function attribute 7 might not significance vale analyzing large data set conclusion recommendation work ml classification requires thorough fine tuning parameter time sizeable number instance data set not matter time build model algorithm only precision correct classification therefore best learning algorithm particular data set doe not guarantee precision accuracy another set data whose attribute logically different however key question dealing ml classification not whether learning algorithm superior others condition particular method significantly outperform others given application problem moving direction trying find function map datasets algorithm performance 12 end us set attribute called attribute represent characteristic learning task search correlation attribute performance learning algorithm some characteristic learning task number instance proportion categorical attribute proportion missing value entropy class etc 3 provided extensive list information statistical measure dataset better understanding strength limitation method possibility integrating two algorithm together solve problem investigated objective utilize strength one method complement weakness another only interested best possible classification accuracy might difficult impossible find single classifier performs well good ensemble nb rf machine learning algorithm deliver high precision accuracy international journal computer trend technology ijctt volume 48 number 3 june 2017 issn page 137 regardless number attribute data instance research show time build model one factor one hand precision kappa statistic mae another factor hand therefore ml algorithm requires precision accuracy minimum error supervised predictive machine learning work recommends large data set distributed processing environment considered create room high level correlation among variable ultimately make output model efficient reference 1 alex vishwanathan 2008 introduction machine learning published press syndicate university cambridge cambridge united kingdom copyright cambridge university press isbn available kth website retrieved website 2 bishop 1995 neural network pattern recognition clarendon press oxford england oxford university press new york ny usa available 3 brazdil soares da costa j 2003 ranking learning algorithm using ibl accuracy time learningvolume 50 issue academic publisher manufactured netherlands available springer website 4 cheng greiner kelly bell liu 2002 learning bayesian network data theory based approach artificial intelligence volume copyright published elsevier science right reserved pp 43 available science direct 02001911 5 domingo pazzani 1997 optimality simple bayesian classifier loss machine learning volume 29 pp copyright 1997 kluwer academic publisher manufactured netherlands available university trento website 6 elder j introduction machine learning pattern recognition available lassonde university eec department york website 7 good 1951 probability weighing evidence philosophy volume 26 issue 97 published charles griffin company london royal institute philosophy 1951 pp availableat royal institute philosophy website 8 hormozi hormozi nohooji 2012 classification applicable machine learning method robot manipulator international journal machine learning computing ijmlc vol 2 no 5 2012 doi 560 available ijmlc website 9 kotsiantis b 2007 supervised machine learning review classification technique informatica 31 2007 pp 249 retrieved ijs website 10 lemnaru 2012 strategy dealing real world classification problem unpublished phd thesis faculty computer science automation universitatea technica din available website 11 logistic regression pp 223 available 12 neocleous schizas 2002 artificial neural network learning comparative review vlahavas spyropoulos ed method application artificial intelligence hellenic conference artificial intelligencesetn lecture note computer science volume springer berlin heidelberg doi pp available 13 newsom 2015 data analysis ii logistic regression available 14 nilsson 1965 learning machine new york journal ieee transaction information theory volume 12 issue 3 doi pp 407 available acm digital library website 15 pradeep naveen 2017 collective study machine learning ml algorithm big data analytics bda healthcare analytics hca international journal computer trend technology ijctt volume 47 number 3 issn doi pp 149 available ijctt website 16 rob schapire machine learning algorithm classifrication 17 rosenblatt 1962 principle neurodynamics spartan new york 18 setiono loew 2000 fernn algorithm fast extraction rule neural network applied intelligence international journal computer trend technology ijctt volume 48 number 3 june 2017 issn page 138 19 shai shai 2014 understanding machine learning theory algorithm 20 hastie tibshirani friedman 2001 element statistical learning data mining inference prediction 2001 new york springer verlag 21 taiwo 2010 type machine learning algorithm new advance machine learning yagang zhang ed isbn intech university portsmouth united kingdom pp 3 available intech open website 22 tapa kanungo 2002 local search approximation algorithm clustering proceeding eighteenth annual symposium computational geometry barcelona spain acm press 23 timothy jason shepard j 1998 decision fusion using classifier proceeding international conference information fusion 24 vapnik 1995 nature statistical learning theory springer verlag pp 1 retrieved website 25 witten frank 2005 data mining practical machine learning tool technique ed isbn morgan kaufmann publisher san francisco ca 2005 elsevier website ftp mining practical machine learning tool technique view publication stats