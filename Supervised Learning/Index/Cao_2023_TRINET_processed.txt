trinet stabilizing learning complete slow collapse asr lixin cao jun wang ben yang dan dong ai lab china 2 peking university ai lab usa abstract learning ssl model confront challenge abrupt informational collapse slow dimensional collapse propose trinet introduces novel architecture preventing collapse stabilizing training trinet learns ssl latent embedding space incorporates higher level space predicting pseudo target vector generated frozen teacher tal result show proposed method notably stabilizes accelerates achieves relative word ror rate reduction werr compared sota downstream benchmark asr task release code index learning collapse pseudo label bootstrapping introduction learning ssl model leverage unlabeled data make signiﬁcant advance 1 reach formances almost par supervised baseline many downstream task speech processing 2 3 4 among model contrastive learning method 3 5 1 6 learn reduce distance positive pair sample distorted version increasing tance negative pair different sample yield good performance large amount contrastive pair 1 difﬁcult mine computationally intensive training challenge motivate alternative method strapping approach 7 2 8 emerge avoid using negative example two network used predict sentation augmented pair one teacher network sg operation otherwise complete informational collapse may happen learned resentations would rapidly collapse towards single vector regardless input student network updating online among approach simsiam 8 ply copied student network weight teacher equal contribution contribution made internship tencent network byol 7 updated teacher network ing exponential moving average ema student network weight 2 also took ema update teacher network used masking prediction task similar 3 feeding student network masked data teacher network original data objective predict averaged embedding several top layer teacher network different using only top layer byol reported 2 collapse issue nounced speech task computer vision natural guage processing task due correlated adjacent target speech modality may come two ferent nature 9 1 complete collapse 2 slow lapse like observation made 10 architectural trick byol siasiam not perfectly maintaining variance representation slow collapse still happening method given challenge motivated study novel regularization method effective practical ssl model susceptible complete slow collapse hence pose novel network trinet analogy stabilizing stand trivet following contribution contrast approach trinet doe not require technique tering alignment etc example unlike bert 4 build ﬁxed set discrete target unit clustering trinet learns ssl latent embedding space incorporates higher level space predicting pseudo target vector generated frozen teacher not requiring distract any negative sample like 11 spiral 12 requiring any statistical assumption advanced tion approach decorrelation 10 ing log determinant 13 may not always tenable sequence task hand trinet instead ploy third branch generate stable stale target tor sequence space construct regularization loss act effectively barrier embedding space degeneracy experiment show proposed method stabilizes 14 mar 2023 accelerates 1 lead signiﬁcant performance improvement no requirement data augmentation larger model capacity meanwhile would like point trinet achieves advance provided frozen teacher model although trinet notably surpass frozen teacher demonstrate experiment related work aside contrastive method preventing informational collapse main trend regularization method maximizing information content embedding vent collapse recently various regularization approach proposed prevent collapse ding variable contain highly redundant information among 14 15 vicreg 10 tempt produce embedding variable decorrelated whereas corinfomax 13 doe not constrain variable uncorrelated instead avoids covariance matrix degeneracy using izer loss function however recent investigation show regularization term worked effectively only given speciﬁc ssl structural setting 10 strong data mentation 16 note regularization method 10 13 14 15 adopt structure sg mean branch network learnable no instead optimization some regularization term together structure 7 8 wa found hard 10 also empirically observed adding ance regularization term wa not effective structure 2 employ structural trick akin byol 7 simsiam 8 rely mechanism normalizing target prevent collapse strategy seems effective difﬁcult interpret may lead instability training 10 2 idea also related different research area 17 employ projection quantizer generate discrete pseudo label hubert 4 us ofﬂine clustering step provide discrete pseudo label masked region take iterative process labeling method simplify ssl target level cluster essentially require downstream task appropriate clustering level model learn well another related idea combination ssl 18 19 20 21 ssl model 22 23 vised teacher model 24 used initial teacher model unlabeled set student model trained combined labeled data pretraining time ssl model frozen teacher without not counting training frozen teacher model fig trinet structure left right teacher network perform different mode produce resentations based original input dicted middle network student mode based perturbed version input bottom tion latent embedding trinet prior work ssl stabilization including c 11 spiral 12 based contrastive mechanism prevent collapse maximizing distance negative pair approach trinet address collapsed minimum independent input hence fundamental question arises multiple factor like ema teacher network ularization come play avoid collapse lead experimental study like trinet theoretical study like 25 26 moreover maintains consistency towards input feature motivation itate codebook learning reconstructing discrete code input contrast trinet predicts towards target space motivation lize learning contextualized latent representation hence require codebook learning quantization loss algorithm codebook spiral posed contrastive loss position randomization avoid model collapse positional collapse arise speciﬁc sg teacher design not nonlinear learning dynamic ssl 25 method network architecture illustrated top fig 1 proposed trinet consists three supporting network middle leg represents dent network simultaneously regress predicts get left right teacher network left teacher track student parameter generates regression get right teacher frozen model automatic speech recognition asr generate target vector stabilizing whole training due different nature target project student right teacher embedding space mask span input sequence x generate perturbed sequence feed standard conformer coder 27 student target zstruc constructed encoding intact input x network eterized ema teacher shown left leg fig 1 summarizing teacher layer output 28 7 leg adopts ssl structure 2 byol 7 straightforward comparison paper whereas alternative ssl structure equally ble meanwhile trinet stabilizes training introducing third leg shown right branch fig 1 take teacher encode intact input x generates pseudo target yregul original input data design prevents rest joint embedding architecture abrupt slow collapse output vector duced branch identical constant end spanning subspace proposed trinet pretrain student encoder simultaneously learn contextualized representation ent level structural nature ema teacher relies structural trick averaging including moving average model weight averaging layer output keep prediction target relatively stable allowing student evolve freely hopefully learn level contextual representation freedom edged sword downside student start collapse ema teacher end collapsing beit slowly demonstrated fig 2 address either abrupt slow collapse third branch play important role anchor regularizing avoiding case student ema teacher degenerate together trinet employ frozen teacher provide target regularization space different embedding space student ema teacher maintains ssl property bottom fig 1 use 29 visualize latent embedding generated trinet point denotes sample random batch color denotes class indicates trinet actually arranges sample layout get obvious embedding space space sample scatter space regularization given predicted latent embedding bedding space contextualized target use squared norm loss regress target lstruc 1 x n zstruc n 2 1 n index total b element batch b batch frame dimension size given prediction space target examine compare two kind objective one squared norm regression lregre 1 x n yregul n 2 2 loss classiﬁcation lregul 1 crossentropy softmax yregul 3 ablation study show lregul effective lregre consider space lregul suitable measure well diction made classiﬁcation rather latent embedding regression echoing ence space complementary ularization effect consequently adopt l lstruc lregul training objective experiment experiment model librispeech 30 contains 960 hour speech asr clean subset also much larger dataset 2 31 tune asr evaluate standard rispeech set implemented reference base large model based fairseq 27 proposed model trinet corresponding backbone architecture dropout masking strategy see detailed conﬁgurations 3 save memory footprint fair comparison model apply input 16 khz waveform ﬁrst transformed ﬁlter bank raw waveform processed feature extractor ing two subsampling layer 576 nels stride kernel width result output sequence original length put applied layer norm sending encoder took sample le constructed subset fig loss including annealing rate er learning rate scheduler regime also follow 2 otherwise would described different experiment used frozen teacher labelled data pretraining respectively demonstrate trinet no requirement additional data larger model capacity varying model structural trick surpass frozen teacher trinet us last conformer block encoding latent ding space illustrated blank block fig 1 dedicates last conformer block space blue block fig 1 overall learnable model size identical base large arbitrary teacher heterogeneous architecture single pas using generated pseudo target also applicable trinet leave part investigation future work result fig 2 drop loss epoch 350 450 actually reﬂected slow collapsing case downstream model ha word error rate wer via greedy search set degenerating epoch 300 epoch 350 epoch chose best checkpoint epoch 300 following comparison meanwhile observe loss trinet much smoother stabler light blue curve without smoothing note absolute loss value not directly comparable indicating frozen teacher trinet work effectively anchor providing ble stale regularization preventing ema teacher student drifting together toward collapsed space meanwhile trinet manages converge within overall epoch model labeled asr mapping representation via randomly ized linear projection top network 32 class representing vocabulary model optimized table wer librispeech set pretrained unlabeled ﬁnetuned labeled respectively model unlabled data dev test clean clean 3 hubert 4 trinet ablated trinet trinet mizing ctc 32 loss shown table 1 approach achieves relative word error rate reduction werrs average librispeech benchmark moreover achieves werrs average pretraining much larger unlabeled reﬂecting scalability proposed method ablation exam different nature two space make ablation study removing projector spare no conformer layer speciﬁc target blue block fig 1 turn training becomes rather unstable shown orange curve training loss fig 2 indicating learning process dragged two space different nature not converge well another ablation study comparing tion term lregul lregre second line bottom table 1 indicates result replacing lregul lregre although result marginally outperforms much worse trinet ablation validates effectiveness lregul suitable measure space mse loss suitable embedding regression reﬂecting complementary nature space constructed different level via triple leg trinet conclusion proposed trinet address challenge complete slow collapse ssl architecture show efﬁcacy downstream asr task trinet employ novel architecture utilizes frozen teacher generate pseudo target anchor stabilizing remaining part joint bedding ssl architecture besides succeeds accelerating obtaining signiﬁcant werr compared sota model benchmark asr task reference 1 chen kornblith swersky norouzi ton big model strong learner neurips 2020 2 baevski hsu xu babu gu auli general framework learning speech vision language icml 2022 3 baevski zhou mohamed auli framework learning speech sentations neurips 2020 4 hsu tsai bolte salakhutdinov mohamed hubert much bad teacher ﬁt asr icassp 2021 5 chen kornblith norouzi hinton simple framework contrastive learning visual icml 2020 6 jiang li cao zou li speech simclr combining contrastive reconstruction objective supervised speech representation interspeech 2020 7 grill strub altche tallec richemond buchatskaya doersch pires guo azar valko bootstrap latent new approach learning neurips 2020 8 chen exploring simple siamese representation learning cvpr 2021 9 jing vincent lecun tian understanding dimensional collapse contrastive learning iclr 2022 10 bardes ponce lecun vicreg regularization ing iclr 2022 11 sadhu hu huang mallidi wu trow stolcke droppo maas model speech representation learning interspeech 2021 2021 12 huang zhang yeung jiang liu spiral tion learning speech iclr 2022 13 ozsoy hamdan arik yuret gan learning information tion criterion iclr 2022 14 ermolov siarohin sangineto sebe ing representation learning icml 2021 15 zbontar jing misra lecun deny barlow twin learning via redundancy reduction icml 2021 16 lepage dehak speaker veriﬁcation information maximization trastive learning interspeech 2022 17 chiu qin zhang yu wu learning quantizer speech tion icml 2022 18 li sainath pang wu training model via weak distillation icassp 2019 19 kahn lee hannun speech recognition icassp 2020 20 xu likhomanenko kahn hannun synnaeve collobert iterative speech nition interspeech 2020 21 park zhang jia han chiu li wu le improved noisy student training automatic speech recognition interspeech 2020 22 zhang qin park han chiu pang le wu pushing limit learning automatic speech recognition neurips sa workshop 2020 23 xu baevski likhomanenko tomasello neau synnaeve collobert auli complementary speech recognition icassp 2021 24 wang wang wu chen li liu wei codebooks masked prediction speech interspeech 2022 25 tian chen ganguli understanding supervised learning dynamic without contrastive pair icml 2021 26 pokle tian li risteski contrasting scape contrastive learning aistats 2022 27 ott edunov baevski fan gross ng grangier auli fairseq fast extensible toolkit sequence modeling naacl 2019 28 caron touvron misra jegou mairal janowski joulin emerging property supervised vision transformer iccv 2021 29 maaten hinton visualizing data using journal machine learning research 2008 30 panayotov chen povey khudanpur rispeech asr corpus based public domain audio book icassp 2015 31 kahn et benchmark asr limited no supervision icassp 2020 32 graf andez gomez connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural network icml 2006