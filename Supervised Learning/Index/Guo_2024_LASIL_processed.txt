lasil supervised imitation learning microscopic traffic simulation ke zhenwei wei weiwei weizi dayang jia university hong kong group tennessee knoxville 11932061 weizili jpan abstract microscopic traffic simulation play crucial role transportation engineering providing insight dividual vehicle behavior overall traffic flow ever creating realistic simulator accurately cates human driving behavior various traffic condition present significant challenge traditional simulator ing heuristic model often fail deliver accurate ulations due complexity traffic ronments due covariate shift issue existing tion simulator often fail generate stable simulation paper propose novel proach called supervised imitation learning address covariate shift problem tation learning leveraging variational autoencoder simultaneously modeling expert learner state tribution approach augments expert state augmented state aware learner state distribution method applied urban traffic simulation strates significant improvement existing art baseline microscopic term macroscopic realism evaluated dataset pneuma introduction microscopic traffic simulation cornerstone portation engineering enables engineer predict analyze individual vehicle behavior providing crucial sight alteration road structure traffic agement strategy might influence overall traffic flow low testing diverse scenario without disrupting traffic enhances safety pinpointing tential hazard devising strategy mitigate risk leveraging simulation engineer optimize traffic flow reduce congestion enhance overall efficiency proving particularly advantageous designing enhancing road network using simulation data policymakers urban planner make informed decision aligned community need regarding transportation infrastructure however creating realistic simulator taneously replicate microscopic response human driver various traffic condition scopic traffic statistic challenging recent year significant effort develop realistic traffic ulators aim accurately model human driving iors traditional traffic simulator sumo 23 aimsun 2 mitsim 39 typically rely tic model like intelligent driver model idm 38 however despite carefully calibrating ters simplified model often fail deliver accurate simulation due complexity traffic environment 11 27 factor road ture neighboring vehicle even driver psychology influence decision human driver making lenging achieve accurate simulation pursuit realistic traffic simulation researcher turned neural network represent driving model imitation learning il human tions traffic simulation approach 4 31 37 age behavior cloning bc 29 learn driving policy minimizing disparity model output human demonstration training data however bc method hindered covariate shift 34 state induced learner policy progressively diverges expert distribution existing tor succeeded no 20 onds simulation application like autonomous driving test often fail generate stable simulation address covariate shift existing method dagger 34 dart 24 incorporate supervisor tions learner perturbed expert state however collecting human supervision data challenging due intensive labor judgment error recent traffic lators 36 41 propose using inverse reinforcement learning irl method generative adversarial 1 26 mar 2024 tion learning gail 16 adversarial inverse ment learning airl 12 learn reward function using discriminator neural network within generative ial network gans 13 policy network trained maximize learned reward online reinforcement learning rl enabling agent handle state however directly applying gail traffic lation problematic 5 dynamic nature environment system lead noise policy learning resulting highly biased estimated gradient furthermore training discriminator gail challenging due instability sensitivity parameter optimization address issue covariate shift itation learning without depending costly expert vision unstable discriminator rl propose supervised imitation learning lasil tackling covariate shift problem icy learning mitigate distribution shift pert learner state distribution augmenting expert state distribution however no expert supervision any augmented state ensure augmented state close expert state future trajectory expert state serve target trajectory augmented state constrain learner within expert state distribution hence goal augment expert state cover learner state distribution ing close original expert state distribution achieve use variational autoencoder vae 20 taneously model distribution expert learner state minimizing vae latent space ularization loss modeling distribution project expert learner state unified latent space minimizing vae reconstruction loss resulting reconstruction leveraging learnt latent space semble expert learner state distribution sult inputting expert state trained vae obtain augmented expert state practice divide agent state two part past trajectory context feature state ing vehicle type waypoints destination observe distribution context consistent regardless learnt policy due static feature traffic ditions embedded context leading le covariate shift therefore propose vae model trajectory distribution expert learner state decoder conditioned vae receive latent variable context yield only trajectory information summary contribution follows propose supervised imitation learning lasil achieve stable learning alleviate covariate shift imitation learning propose data augmentation method based vae generates augmented expert state approach tailored urban traffic simulation best knowledge first imitation traffic simulator reproduce term 10 minute stable microscopic tion achieving simulation length improvement previous 4 37 41 evaluate method dataset pneuma 3 half million trajectory approach outperforms baseline microscopic macroscopic ulations code available related work imitation learning existing imitation learning il method broadly egorized behavior cloning bc inverse ment learning irl approach bc 29 learns policy supervised fashion minimizing discrepancy tween learner action expert however bc suffers issue covariate shift state distribution induced learner policy gradually ate expert address method like dagger 34 dart 24 request supervisor correction learner perturbed expert state method low similar supervised learning approach dagger dart doe not require access expert policy due challenge obtaining expert supervision recent method utilize feedback neural discriminator handle covariate shift ically method involve iterative process ing reward estimation reinforcement ing earlier method 14 32 42 require quent dynamic programming process recent versarial il approach integrate reward function learning policy learning using gan formulation however gan rl training process known stable sensitive hyperparameters poor ple efficiency 8 moreover discriminator easily exploit insignificant difference expert policy sample leading undesirable performance 40 trast method avoids optimization problem rl process requiring minimal instead utilize real future trajectory target state learn corrective behavior imitation traffic simulation recent advancement traffic simulator focused enhancing realism leveraging imitation learning il 2 human driving demonstration simulator tend traditional bc irl method tackle ing il problem traffic simulator like trafficsim 37 simnet 4 typically begin training prediction model subsequently adjust predicted trajectory vent collision adhere traffic regulation ulation however method face challenge achieving simulation due covariate shift problem mitigate issue augment expert state based policy distribution enabling stable simulation additionally enhance performance ifying predicted trajectory simulation road projection ensuring smoothness current state notably skip computationally intensive lision removal operation prioritizing capture scopic influence detail simulator learn underlying reward tion human driving behavior derive driving policy maximizing learned reward adversarial irl method theoretically address covariate shift bc context online interaction mance deteriorates il domain due dynamic environment complicating training process tackle challenge approach like 36 41 adopt learning gradually troduce vehicle environment nonetheless method still exhibit significant undesirable traffic ena driving collision abrupt ing building upon tion learning rail method 5 21 penalizes undesirable phenomenon introducing reward ever maximizing new reward doe not guarantee recovery trajectory despite numerous hancements original gail framework based method often struggle produce stable traffic flow evidenced experiment contrast method supervised learning approach leading faster simpler stable learning driving policy background markov decision process model human driving process using markov sion process mdp denoted r porating time horizon represent continuous state action space respectively tic function 0 1 describes system dynamic r reward function icy π determines probability selecting action state trajectory τ st represents sequence pair marginal state distribution policy π computed ρπ 1 p δs dπ denotes set state size trajectory induced policy π δs signifies dirac distribution centered similarly marginal distribution ρπ computed imitation learning application il learning driving policy sume agent adopt policy objective learn policy π minimizes tween marginal distribution expert demonstration ρexp learner policy bution ρπ example bc optimizes policy minimize kl divergence expert state distribution eρexp kl πexp dagger minimizes eρπ kl πexp hand gail minimizes divergence dj ρexp airl mizes kl divergence kl ρπ however gail airl often exhibit unsatisfactory practical performance due optimization process instability sample inefficiency involve gans rl contrast bc only requires simple stable supervised learning suffers covariate shift sue only minimizes policy difference pert state distribution without guaranteeing performance learner state distribution dagger address ate shift problem requires access expert policy mitigate covariate shift problem without depending expert policy propose maximizing transition probability ϵ π ϵ ϵ small augmentation term note policy learns predict distribution next state instead action approach assumes expert original future trajectory serve supervision guide agent back towards expert distribution variational autoencoder vae defines generative model given pθ x z p z pθ z latent variable prior bution p z pθ represents conditional bution modeling likelihood data x given ing objective maximize training sample marginal log pθ x however due intractability marginalization vae maximizes variational lower bound using qϕ approximate posterior begin split log vx mathbb e kl p l rec l kl split 1 lrec x represents reconstruction loss nalizes network creating output different input lkl x represents kl divergence loss make 3 continuous smooth latent space allowing easy random sampling interpolation intuitively kl loss age encoder distribute encoding evenly around center latent space method fig 1 present overview method model comprises three module data augmentation module policy network lqr projection module training augment expert data generating augmented expert state vae reconstruction past jectory using augmented state along nal future trajectory train learner policy network supervised learning simulation roll policy network several step cluding sampling projecting trajectory onto road smoothing projected trajectory state representation human driver make decision mainly depending surrounding information build graph model traffic system driving agent node node state agent divided two component past trajectory sp context sc practice past trajectory composed agent sitions several past time step including current time step context includes type nearby waypoints sitions corresponding road width traffic light status road traveling destination waypoints composed center point routing road fixed interval transform agent state individual coordinate system learn policy formation invariance reduce implicit covariate shift set agent current position adding gaussian turbation origin direction pointing towards agent destination like 15 edge agent state coordinate formed global coordinate system ual coordinate system information relative tions among agent lost however traffic model need relative information agent understand interact preserve relationship introduce directed edge neighboring agent edge feature relative position destination node coordinate origin source node coordinate system data augmentation address covariate shift issue expert learner state distribution propose minimize pert learner embedded state distribution difference practice propose utilizing vae model expert learner state distribution simultaneously expert learner state projected latent space distribution state reconstructed joint latent space resemble distribution contrast past trajectory context distribution challenging model exhibit le covariate shift therefore propose using vae specifically model tory distribution rather state distribution expert learner state represented sp sc ploy encoder qϕ sc obtain latent variable distribution sample latent variable z distribution applying reparameterization trick sequently decoder pθ sc used reconstruct distribution past trajectory sp given z sc vae trained maximize variational lower bound incorporates expert learner past trajectory thcal l vae b e l e l 2 λ hyperparameter control degree augmented past trajectory distribution aligns learner past trajectory distribution term l given split al l c thbb e kl p split 3 simplicity assume pθ qϕ multivariate normal distribution diagonal variance matrix prior tribution p z set isotropic unit gaussian n 0 graph attention network build model approach including vae coder decoder learner policy network based graph attention egat network 10 30 model interaction aggregating neighboring node edge information using attention mechanism node feature first layer obtained embedding node input fully connected layer node feature layer calculated v h si gma left sum j thcal n j j 4 hl feature node l th layer eij coordinate origin position node j relative node concatenation w l learnable weight matrix ni set neighbor node including node σ activation function attention coefficient αl ij indicates tance node j node considering node edge feature computed l pha j softmax j x 5 4 figure overview approach process purple red arrow handle only expert learner data respectively process represented black arrow apply data state state represented graph model including vae encoder decoder policy network implemented using egat network wl learnable weight vector normalization performed weight across neighbor node using softmax function passing multiple egat layer node feature last layer fed fully connected layer obtain output network policy loss train learner policy network first sample augmented expert state conditioned vae expert state policy network predicts future position distribution time step denoted p ˆ pm 1 ˆ pm 2 ˆ pm assumed product gaussian distribution p h v p 1 h vp 2 n 6 ˆ µm ˆ σm represent mean covariance trix predicted position ˆ pm future time step assume no correlation position tributions different future time step learn policy network minimize negative nll loss agent future trajectory h c al l nll u 1 n 7 pm denotes ground truth position agent future time step total number agent simulation process training simultaneously simulate roll learned policy network iteratively get learner state ples instead directly updating agent predicted position apply several step diction better realism firstly sample tribution project sampled position onto nearest point smooth projected jectory regulator lqr 1 imizing total commutative quadratic cost linear namic system described eft beg n r r c l e v p array array ccc 0 array array c array array c array 8 diagonal matrix interval time step diagonal entry pm vm represent position velocity acceleration system subject quadratic cost function given ath c l j u 1 um 9 projected predicted position pt considered target pose ηa used nalize high acceleration finally agent updated first position planned trajectory training process training step policy network trained imize probability expert future trajectory ing context state history trajectory augmented vae input simultaneously vae iteratively trained reconstruct expert learner data pert data policy network training data augmentation learner data sampled replay buffer every n training step empty replay 5 buffer roll current policy time step generate learner data store replay buffer start dom time step training dataset apply model manner time step sample future trajectory vehicle gaussian distribution predicted policy network project predicted trajectory onto road smooth trajectory lqr finally update vehicle first position smoothed trajectory experiment dataset utilize urban dataset called pneuma 3 wa collected 10 drone athens span 4 day dataset encompasses half million vehicle trajectory within large area encompassing 100 km lane approximately 100 intersection recording conducted 5 interval day period lasting 15 minute data collection time interval second enhance computational ciency adopt time step second dataset divided training set comprising recording first 3 day set consisting ings final day notably choose not utilize popular traffic datasets like ngsim 7 highd 22 autonomous driving datasets like lyft 17 nuplan 6 only encompass scenario tion make inadequate evaluate macroscopic realism metric evaluate realism simulator measuring similarity simulation result real data ing evaluation assume vehicle enters ulator first recorded time position controlled simulator complete recorded route agent reach final recorded position moved simulator microscopic following prior work 5 36 conduct microscopic evaluation lating 20 second random time step test dataset first measure similarity lated real data using position velocity rmse rics calculated p e ra r nam e r e rac 1 1 10 sm ˆ sm real simulated value position velocity agent time step tively wa total simulated time step wa total simulated agent number besides measure minimum average displace error minade not nalize reasonable trajectory unlike real one r n ame na e ac 1 1 n 11 n 20 time also calculate road rate avarage proportion vehicle deviate road time step common collision rate metric not used focus impact dataset doe not provide accurate vehicle size heading information macroscopic also evaluate model macroscopic accuracy five period 800 second initial recording time measure performance use two standard macroscopic metric traffic flow data 26 33 35 namely road sity speed rmse addition rate density road time step calculated dividing number vehicle road total lane length suming lane width meanwhile road speed computed mean speed vehicle road quantify similarity simulated ground truth value still use rmse able becomes total number road performance compare method baseline sumo 23 use idm model 38 model mobil 18 model tune idm parameter 6 type vehicle minimizing mse idm calculated eration real acceleration using adam optimizer 19 bc 29 learn model directly bc method marl 25 train model using ippo 9 rl algorithm reward function composed three part displacement reward average distance agent trajectory gt trajectory penalty terminal reward 28 add behavior cloning term loss function learning policy marl 36 let vehicle share policy parameter critic parameter learn policy using reward function computed gail rail 5 learn model additional displacement terminal reward train evaluate model five time obtain mean standard deviation std various metric note not apply projection lqr baseline evaluate formance shown tab 1 tab 2 respectively method achieves better result baseline term position velocity rmse road density speed rmse minor rate 6 table comparison baseline ablated model microscopic metric 20 second model position rmse velocity rmse sumo 23 0 bc 29 marl 25 28 36 rail 5 lasil augmentation projection lqr table comparison baseline ablated model macroscopic metric 800 second model road density rmse road speed rmse sumo 23 0 bc 29 marl 25 28 36 rail 5 lasil augmentation projection lqr ablation study conducted series ablation experiment ass individual contribution crucial component proach result presented tab 1 tab 2 light impact component performance traffic simulation model augmentation removing vae module rectly learning original expert state action alyze importance data augmentation technique result demonstrate augmentation play vital role improving simulation formance mitigating covariate shift vae emphasize cance modeling trajectory bution rather whole state distribution replaced vae naive vae directly model expert learner state distribution served drop performance demonstrates challenge reconstructing context distribution projection ablation study projection module aim show impact reducing rate result demonstrate jection module lead notable decrease rate moderate improvement performance rics lqr removing lqr module allowed u evaluate effectiveness lqr led higher rate due constrained movement driving due inertia removal deteriorated term metric reason lqr smooth trajectory thus leading realistic tion qualitative result fig 2 present mean road density speed data sumo simulation proposed method time step result demonstrate proposed method excels replicating scopic traffic pattern surpassing capability sumo simulator achievement attributed model enhanced ability replicate typical scopic driving behavior long period 7 figure mean density speed road time step evaluation method density speed similar color one compared sumo conclusion conclusion addressed challenge creating realistic traffic simulator accurately model human driving behavior various traffic condition traditional imitation simulator often fail deliver curate simulation due covariate shift lem imitation learning tackle issue proposed supervised imitation learning method leverage vae erate augmented expert state leverage vae simultaneously reconstruct expert learner state approach enables duction stable microscopic traffic simulation marking significant advancement field urban traffic simulation method ha demonstrated superior performance existing simulator evaluated dataset pneuma achieving better microscopic macroscopic similarity data baseline 8 reference 1 karl johan om richard murray feedback tems introduction scientist engineer princeton university press 2021 5 2 jaime jordi casas dynamic network simulation aimsun simulation approach transportation ysis recent advance challenge page 2005 1 3 emmanouil barmpounakis nikolas geroliminis new era urban traffic monitoring massive drone data pneuma field experiment transportation research part c emerging technology 2020 2 6 4 luca bergamini yawei ye oliver scheel long chen chih hu luca del pero blazej osinski hugo grimmett ter ondruska simnet learning reactive ulations observation ieee international conference robotics automation page 2021 1 2 3 5 raunak bhattacharyya derek phillips changliu liu jayesh gupta mykel kochenderfer simulating emergent property human driving behavior using reward augmented tation learning international conference robotics automation page 2019 2 3 6 7 6 holger caesar juraj kabzan kok seang tan whye kit fong eric wolff alex lang luke fletcher oscar jbom sammy omari nuplan planning benchmark autonomous vehicle computer vision pattern recognition conference workshop 2021 6 7 james colyar john halkias u highway 101 dataset federal highway administration tech 2007 6 8 robert dadashi l eonard hussenot matthieu geist olivier pietquin primal wasserstein imitation learning international conference learning representation 2021 2 9 christian schroeder de witt tarun gupta denys ichuk viktor makoviychuk philip h torr mingfei sun shimon whiteson independent learning need starcraft challenge arxiv preprint 2020 6 10 frederik diehl thomas brunner michael truong le alois knoll graph neural network modelling traffic participant interaction ieee intelligent vehicle sium page 2019 4 11 shuo feng xintao yan haowei sun yiheng feng henry liu intelligent driving intelligence test tonomous vehicle naturalistic adversarial ment nature communication 12 2021 1 12 justin fu katie luo luo sergey levine learning bust reward adverserial inverse reinforcement ing international conference learning representation 2018 2 13 ian goodfellow jean mehdi mirza bing xu david sherjil ozair aaron courville yoshua bengio generative adversarial network munications acm 144 2014 2 14 ke guo wenxi liu jia pan trajectory bution prediction based occupancy grid map conference computer vision pattern recognition page 2022 2 15 ke guo wei jing junbo chen jia pan ccil imitation learning urban driving robotics science system 2023 4 16 jonathan ho stefano ermon generative adversarial itation learning advance neural information processing system 29 2016 2 17 john houston guido zuidhof luca bergamini yawei ye long chen ashesh jain sammy omari vladimir iglovikov peter ondruska one thousand one hour motion prediction dataset conference robot learning 2020 6 18 arne kesting martin treiber dirk helbing eral model mobil model transportation research record 1999 1 2007 6 19 diederik kingma jimmy ba adam method stochastic optimization corr 2014 6 20 diederik p kingma max welling tional bayes arxiv preprint 2013 2 21 yann koeberle stefano sabatini dzmitry tsishkou christophe sabourin exploring trade man driving imitation safety traffic simulation ieee international conference intelligent transportation tems page 2022 3 22 robert krajewski julian bock laurent kloeker lutz eckstein highd dataset drone dataset istic vehicle trajectory german highway validation highly automated driving system international ference intelligent transportation system page 2125 2018 6 23 daniel krajzewicz jakob erdmann michael behrisch laura bieker recent development application urban mobility international journal advance system measurement 5 3 4 2012 1 6 7 24 michael laskey jonathan lee roy fox anca dragan ken goldberg dart noise injection robust imitation learning conference robot learning page 2017 1 2 25 quanyi li zhenghao peng lan feng zhizheng liu chenda duan wenjie mo bolei zhou scenarionet platform traffic scenario lation modeling advance neural information cessing system 2023 6 7 26 michael james lighthill gerald beresford whitham kinematic wave ii theory traffic flow long crowded road proceeding royal society london series mathematical physical science 229 1178 1955 6 27 weiwei liu wei jing ke guo gang xu yong liu et al traco learning virtual traffic coordinator cooperation reinforcement learning conference robot learning page 2023 1 9 28 yiren lu justin fu george tucker xinlei pan eli stein becca roelofs benjamin sapp brandyn white sandra faust shimon whiteson et al imitation not enough robustifying imitation reinforcement ing challenging driving scenario arxiv preprint 2022 6 7 29 donald michie michael bain jean nitive model subcognitive skill ieee control neering series 1990 1 2 6 7 30 xiaoyu mo zhiyu huang yang xing chen lv trajectory prediction heterogeneous enhanced graph attention network ieee transaction intelligent transportation system 2022 4 31 jeremy morton tim wheeler mykel j kochenderfer analysis recurrent neural network probabilistic eling driver behavior ieee transaction intelligent transportation system 18 5 2016 1 32 andrew ng stuart russell et al algorithm inverse reinforcement learning international conference chine learning page 2 2000 2 33 paul richards shock wave highway operation research 4 1 1956 6 34 ephane ross geoffrey gordon drew bagnell duction imitation learning structured prediction regret online learning international conference tificial intelligence statistic page 2011 1 2 35 jason sewall david wilkie paul merrell ming c lin continuum traffic simulation computer graphic forum number 2 page 2010 6 36 jiaming song hongyu ren dorsum sadigh stefano mon generative adversarial imitation ing advance neural information processing system 31 2018 1 3 6 7 37 simon suo sebastian regalado sergio casas raquel urtasun trafficsim learning simulate realistic agent behavior conference computer sion pattern recognition page 2021 1 2 3 38 martin treiber ansgar hennecke dirk helbing gested traffic state empirical observation scopic simulation physical review e 62 2 2000 1 6 39 qi yang haris n koutsopoulos microscopic fic simulator evaluation dynamic traffic management system transportation research part c emerging nologies 4 3 1996 1 40 kaifeng zhang rui zhao ziming zhang yang gao adversarial imitation learning arxiv preprint 2022 2 41 guanjie zheng hanyang liu kai xu zhenhui jessie li traffic simulation via inverse ment learning international joint conference artificial intelligence 2021 1 2 3 42 brian ziebart andrew l maas j andrew bagnell anind k dey maximum entropy inverse reinforcement learning aaai conference artificial intelligence page 2008 2 10 lasil supervised imitation learning microscopic traffic simulation supplementary material model detail model architecture listed tab model trained using adam optimizer learning rate batch size 4 8 force vae trained simultaneously policy network using online simulated learned data offline expert data table value history time step 10 future time step 10 route point number 30 neighbor number 6 neighbor maximum distance 20 origin perturbation std 1 egat hidden size 512 vae latent dim 8 vae encoder layer number 1 vae decoder layer number 1 policy network layer number 1 lqr acceleration weight ηa 1 learner vae loss weight λ 1 training simulation interval n 50 training simulation length 50 baseline detail sumo us mobil model idm various tuned parameter including desired speed acceleration ation minimum gap time headway 6 type including motorcycle car taxi bus medium heavy vehicle vehicle parameter shown bc model without vae lqr projection module rl baseline trained using ippo default rameters ray library marl reward sum displacement reward weight penalty weight 1 terminal reward weight add bc term weight 1 loss tion marl policy learns policy using reward function discriminator network also trained using adam learning rate rail additional reward marl dataset preparation data preparation process pneuma dataset duced section trajectory data trajectory data downloaded official site cally data recorded drone period except first period due large position error caused wind gust routing determine route vehicle trajectory used method leuvenmapmatching however method generated many circular route rarely observed real data address issue skipped intermediate routing node point far away actual tory helped reduce number unrealistic circular route road network map information downloaded openstreetmap import sumo generate road work only include highway vehicle road network excluding road type walk railway however find map data not always accurate manually adjust road shape reduce number driving case recorded trajectory data additionally modify lane connection relation junction alleviate traffic jam sumo simulation traffic light traffic light information not provided dataset design algorithm estimate traffic light formation recorded trajectory data firstly filter vehicle starting stopping point near signaled intersection trajectory data secondly cluster point based located edge assume lane one edge controlled traffic light thirdly obtain time step traffic light turn green identifying corresponding clustered point 1 table sumo motorcycle car taxi bus medium vehicle heavy vehicle desired speed 30 30 30 30 acceleration deceleration minimum gap time headway whose time gap previous point larger seven second similarly obtain time step turn red fourthly based time step traffic light turn green need calculate traffic light first ing green time step green time cycle length sume traffic light junction cycle length only 45 90 second estimate first green time cycle length use cost function negative cost given filtered ing green time step match estimated turning green time step positive cost given no filtered turning green time step matching estimated turning green time step enumerating first turning green time step interval second cycle length 45 90 second output result minimal cost based traffic light turning green time step junction obtain turning red time no traffic light junction need estimate turning red time estimation green time based turning red time turning green time obtain green time traffic light runtime perform runtime experiment using single nvidia geforce gtx 1080 gpu intel cpu experiment take account component traffic model including input preparation tory prediction action generation runtime result time step recorded tion shown fig see runtime crease almost linearly number agent besides method finish one simulation step thousand agent within acceptable time limit smaller 1 ond qualitative result prediction planned trajectory fig 4 present trajectory produced vae training based augmented past trajectory context information figure runtime time step tion icy network predicts future trajectory quently refined lqr module different scenario lane keeping turning lane changing sults illustrate vae capable generating wide range past trajectory pas distribution possible policy remaining reasonable closely resembling actual past tory moreover method accurately predicts future jectories closely align actual path based augmented history context additionally ration lqr module enhances smoothness trajectory importantly approach also demonstrates ability generate diverse behavior comply surrounding environment statistical distribution fig 5 illustrate distribution speed tance leading vehicle simulation method produce similar speed distribution ground truth sumo since intelligent driver model idm always aim move highest speed furthermore method generates leader distance 2 figure trajectory augmented vae predicted policy network subsequent planned trajectory lqr module lane keeping turning lane changing scenario 3 butions closely match ground truth road shape change experiment microscopic traffic simulator help portation engineer planner analyze predict impact microscopic adjustment traffic pattern disrupting traffic example help analyze changing road shape affect traffic pattern fig 6 present mean road density speed change simulator modifying several road shape see local microscopic modification road network cause traffic congestion alleviation distant area 4 figure distribution speed leader vehicle distance evaluation figure mean density speed change modifying road network 5