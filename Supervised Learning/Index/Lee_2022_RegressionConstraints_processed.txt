17 jan 2022 iterative supervised learning regression constraint tejaswi taeyoung lee supervised learning often requires enforcement constraint ensure trained model consistent underlying structure input output data paper present iterative procedure perform regression arbitrary constraint achieved alternating learning step constraint enforcement step afﬁne extension function incorporated show lead contraction mapping mild assumption convergence guaranteed analytically presented proof convergence regression constraint unique contribution paper furthermore numerical experiment illustrate improvement trained model term quality regression satisfaction constraint also stability training compared existing algorithm introduction enforcing constraint supervised learning critical underlying structure data respected trained model required overcome bias data set instance 1 ha studied constraint caused length angle collision projection predicting motion physical system neural network 2 fairness respect protected feature race gender addressed socially sensitive decision making ha illustrated 3 performance deep learning improved integrating domain knowledge form constraint imposing constraint desirable injecting prior knowledge model encoded indirectly data supervised learning explicitly one common technique implement constraint augmenting loss function additional penalty violation constraint presented 4 5 hand constraint also implemented directly hard constraint satisﬁed strictly imposing hard constraint deep neural network presented 6 customizing optimization technique alternatively 7 handle output label restriction lagrangian based formulation approach based additional regularization term hard constrained optimization involve process actively adjusting model parameter training word possibly conﬂicting goal regression constraint enforcement addressed simultaneously may hinder efﬁciency training procedure making susceptible various numerical issue mechanical aerospace engineering george washington university washington dc 20052 tylee recently iterative procedure ha proposed 8 constraint enforced adjusting get instead manipulating model parameter directly thereby addressing aforementioned challenge able feature any supervised learning technique developed without constraint consideration adopted conjunction nonlinear constrained optimization tool however approach heuristic sense no analytical assurance convergence iteration performance illustrated several numerical example fact challenging present convergence property any supervised learning constraint main objective paper establish certain vergence guarantee regression constraint follow procedure presented 8 target adjusted satisfy constraint speciﬁcally ideal target projected intersection set possible output chosen model set feasible output model parameter optimized adjusted target two step repeated proposed approach motivated alternating projection 9 10 dykstra algorithm 11 particular two step iteration adjusting target training model considered certain projection operator convergence established banach ﬁxed point theorem 12 desirable feature certain assurance convergence regression constraint another interesting feature general formulation discussed framework integrated any supervised learning technique address challenge adjusting model parameter satisfaction constraint performing regression simultaneously one downside not enforce constraint strictly hard constraint design parameter provides regression constraint satisfaction numerical experiment demonstrate proposed proach improves regression performance similar level constraint violation importantly exhibit consistent result validation proposed convergence proof actually beneﬁcial numerical implementation paper organized follows problem lated proposed algorithm described section ii along proof convergence condition section iii numerical result presented various loss function parameter value datasets followed concluding remark section iv ii iterative learning constraint section formulate problem supervised learning regression constraint present proposed iterative scheme convergence proof problem formulation consider regression problem predict ideal output given input x n denotes number point dataset corresponds number feature data point let model supervised learning denoted ˆ f x θ θ model parameter ˆ output predicted current model parameter goal regression identify optimal model parameter minimizes given loss function l ˆ l addition enforce constraint predicted output belongs feasible set denoted c ˆ thus optimization problem regression constraint formulated arg min θ l ˆ ˆ f x θ ˆ alternatively reorganized optimization output space z arg min ˆ l ˆ ˆ 1 arg min θ l z ˆ ˆ f x θ θ 2 proposed 8 b ˆ ˆ f x θ θ set possible output current model word 1 ﬁnd alternative optimal target z closest ideal target restriction given constraint model bias next 2 model parameter optimized predicted output match optimal target z not ideal target intriguing feature supervised learning 2 corresponds usual supervised learning without constraint constraint enforced indirectly 1 any supervised learning scheme utilized 2 1 standard tool nonlinear constrained optimization applied iterative learning algorithm constraint 8 problem tackled clever combination two iteration veriﬁed various numerical example might heuristic sense no gence property established propose following alternative iterative scheme 1 2 summarized algorithm 1 provides certain convergence property regression α β parameter adjustment step ni total number iteration procedure ﬁrst step initial training supervised learning performed without considering constraint next iteration composed two part target adjustment unconstrained training target adjustment step ha two algorithm 1 regression constraint input α β ni 1 ˆ arg minˆ l ˆ ˆ initial training 1 ni ˆ yi zi arg minz l z 1 αˆ yi z infeasible adjustment else zi arg minz l z l z ˆ yi z feasible adjustment end 8 ˆ arg minˆ l ˆ zi ˆ unconstrained training end output ˆ yni depending output previous step particular critical step output trained model doe not satisfy constraint step 4 denoted infeasible adjustment target adjusted minimize l z 1 αˆ yi ﬁnd feasible target z closest point line connecting ˆ term loss function opposition obtaining vector considers original label current prediction ˆ separately presented 8 form l z 1 αl z ˆ next output trained model satisﬁes constraint step feasible adjustment target z moved closer original target within ball radius β measured term loss finally model trained adjusted target whole procedure repeated proposed algorithm key idea selecting jective function infeasible adjustment l z αˆ yi establishes convergence property presented next subsection improves numerical property illustrated section iii interestingly speciﬁc choice loss function namely mean squared error mse loss equivalent form l z 1 αl z ˆ described remark loss function mean squared error procedure algorithm 1 moving target algorithm 8 equivalent adjusting parameter proof since main difference two infeasible adjustment case compare corresponding optimization problem l z pn 2 mse loss algorithm 1 address za arg min z n x zk 1 yk yk 2 z arg min z n x k 1 zkyk yk z whereas master step moving target zm arg min z n x zk 2 1 αm n x zk yk 2 z arg min z 1 αm n x αm 1 k yk z subscript represents algorithm variable subscript moving target tive function za zm differ only scale αa αm 1 1 3 hence solution obtained identical za zm convergence property present convergence property algorithm 1 ha motivated proposed form objective function infeasible adjustment step let any norm euclidean space denoted rn also euclidean norm norm denoted respectively projection operator pz l rn set z respect loss l deﬁned pz l x arg min z l z x z 4 word x projected z distance x z minimized term loss consider convex subset z normed vector space x exists unique projection pz x x x x inf z underlying geometric constraint satisﬁed see 13 proposition pz not contained some line segment parallel some line segment boundary unit assumption make following assumption set b c convex projection operator b c lipschitz exists norm k 0 l x l x b loss function projection 4 mse two statement actually equivalent 13 concerned convergence sequence ˆ yi generated training step word wish show ˆ yi some convergence algorithm 1 established follows theorem suppose α k lipschitz constant introduced assumption iteration algorithm 1 ha unique ﬁxed point b limit sequence ˆ yi initial ˆ β sufﬁciently small proof β algorithm 1 iterates infeasible adjustment step unconstrained training written afﬁne extension yα 1 αˆ yi adjustment zi pc l yα arg minz l z 1 αˆ yi z learning ˆ pb l zi arg minˆ l ˆ zi ˆ therefore algorithm 1 corresponds concatenation two projection ˆ pb l pc l h ˆ yi 5 h rn afﬁne extension function deﬁned h ˆ yi 1 αˆ yi consider any two point ˆ ˆ pb l pc l h ˆ l pc l h ˆ pc l h ˆ l h ˆ h ˆ ˆ ˆ 1 iteration contraction mapping b metric induced norm x since b complete metric space series iteration ha unique ﬁxed point f according banach ﬁxed point theorem 12 moreover sequence ˆ ˆ converges any ˆ short lipschitz property assumption 1 ensures iteration contraction critical question ensure lipschitz property projection operator corollary convergence algorithm 1 guaranteed described theorem 1 following loss function mean squared error mse given l z 1 n pn zk 2 algorithm converges parameter α 0 1 mean absolute error mae given l z 1 n pn algorithm converges eter α 0 proof mse projection 4 corresponds pz l arg min z 1 n n x zk 2 z arg min z n 2 z equal minimization respect standard euclidean norm proximity map closed convex set hilbert space euclidean inner product satisﬁes condition 9 13 since schitz constant k 1 according theorem 1 algorithm 1 converges α 0 1 next mae loss projection becomes pz l arg min z 1 n n x z arg min z z optimization respect norm shown 14 lipschitz constant k 2 respect norm hence convergence guaranteed α 0 corollary 1 main result paper establishing convergence iterative algorithm regression constraint next show proposed algorithm exhibit improved numerical property several example beyond providing mathematical assurance iii numerical simulation evaluate performance proposed algorithm various datasets parameter value loss function first underscore section meant exercise understanding algorithmic procedure resulting output supposed interpreted purely technical result type constraint going consider regression called fairness constraint socially sensitive decision making see 15 measured form disparate impact discrimination index didir z x x 1 n n x zi 1 x v zi 6 dp set value protected feature gender disability set p xp v represents input whose feature ha value roughly speaking represents difference mean output mean conditioned protected feature higher didi dataset suffers disparate impact constraint didi value ǫ taken fraction didi value training set three different datasets considered regression problem fairness constraint student dataset n 649 point 33 attribute portuguese class uci repository ha used predict secondary school student performance 16 going protect feature sex trying estimate ﬁnal grade student feature like romantic interest likely no relation output removed according 16 crime dataset also uc irvine machine ing repository 17 ha n 2 215 since target variable violentperpop representing per caput violent crime want impose fairness constraint protected feature race feature lot nan value removed along others directly dependent target act output blackfriday dataset available online 18 original training data not utilized limited amount computing resource available since large n 000 12 select sample data start size n 50 goal estimate amount money spent purchase ensuring prediction fair respect protected feature gender new attribute value count introduced since represents number time product ha purchased also identity feature removed categorical feature data encoded integer array using ordinal encoder finally obtained value normalized 0 1 ensure balanced regression table parameter α weight ˆ algorithm 1 αa moving target 8 αm le 9 equal 1 next value parameter α β chosen follows extensive tuning term ha already completed 8 observed β work well empirically value β adopted well parameter α three different value chosen corresponding value moving target algorithm 8 calculated 3 remark listed table according relative weight ˆ respect infeasible adjustment step αa increased weight assigned ˆ ha adjusted constraint compared original target therefore emphasis satisfaction constraint machine learning model regression gradient boosted tree chosen ensures repeatability also achieves higher accuracy well better constraint tion study convergence property algorithm executed total ni 30 iteration cross validation performed obtain reliable estimate mance well standard deviation utilize computer quad core intel cpu nvidia gpu 16 gb ram solve optimization problem adjustment step algorithm 1 utilize ibm software cplex 19 mse mae additionally consider mean huber loss mhl l z 1 n pn g zk g x 2 7 implemented cvxpy 20 result table ii present result varying loss function datasets α value performance measured regression coefﬁcient ratio c didi 6 predicted output training data also compare algorithm denoted moving target 8 corresponding α value table according remark 1 method equivalent mse blackfriday dataset two case α left since could not solved available computing resource discussed αa represents satisfaction constraint regression well reﬂected table ii αa increased c decrease cost reduced next bold font table ii represent case algorithm performs better moving target statistically meaningful manner italic font represent opposite case statistical importance assumed occur σm difference mean ﬁgures greater sum standard deviation observed procedure performs better term c case crime blackfriday datasets student dataset smallest one n 649 result mostly comparable beyond regression result summarized table ii advantage proposed approach well illustrated investigating learning process figure 1 2 present evolution c iteration crime blackfriday data respectively αa small algorithm yield similar result seen figure 1 however αa increased emphasis constraint satisfaction proposed algorithm 1 performs noticeably better shown figure 1 b e also figure 2 c proposed approach yield greater lower next figure 1 c f figure 2 b exhibit greater value comparable term constraint tion importantly proposed approach display uniform performance validation standard deviation much lower example illustrated figure 1 b 1 c c suggests presented proof convergence fact beneﬁcial numerical implementation well improved numerical property iteration may important scalability regression complexity constraint iv conclusion proposed iterative algorithm regression constraint composed adjustment training convergence guarantee also provided afﬁne extension function infeasible adjustment step furthermore result specialized form parameter constraint selected loss function later result numerical experiment presented varying datasets parameter value proposed convergence proof vised learning constraint unique contribution shown performance aspect regression constraint satisfaction training stability improved existing technique future direction aim study convergence guarantee generic lipschitz condition even classiﬁcation setup reference 1 yang zhu learning physical constraint neural projection arxiv preprint 2020 2 berk heidari jabbari joseph kearns morgenstern neel roth convex framework fair regression arxiv preprint 2017 3 borghesi baldo milano improving deep learning model via domain knowledge brief survey arxiv preprint 2020 4 mehta lee carbonell towards ing deep semantic role labeling arxiv preprint 2018 5 diligenti roychowdhury gori integrating prior edge deep learning 2017 ieee international conference machine learning application icmla ieee 2017 pp 6 salzmann fua imposing hard straints deep network promise limitation arxiv preprint 2017 7 nandwani pathak singla et primal dual formulation deep learning constraint 2019 8 detassis lombardi milano teaching old dog new trick supervised learning constraint arxiv preprint 2020 9 cheney goldstein proximity map convex set proceeding american mathematical society vol 10 no 3 pp 1959 10 boyd dattorro alternating projection stanford university 2003 11 bauschke borwein dykstra alternating projection algorithm two set journal approximation theory vol 79 no 3 pp 1994 12 ciesielski et stefan banach some result banach journal mathematical analysis vol 1 no 1 pp 2007 13 balestro martini teixeira convex analysis normed space metric projection onto convex body arxiv preprint 2019 14 de figueiredo karlovitz radial projection normed space djairo de paper springer 1967 pp 15 aghaei azizi vayanos learning optimal fair decision tree proceeding aaai conference artiﬁcial intelligence vol 33 no 01 2019 pp 16 cortez silva using data mining predict secondary school student performance 2008 17 dua graff uci machine learning repository 2017 online available 18 black friday dataset online available 19 ibm ilog cplex 1 user manual cplex international business machine corporation vol 46 no 53 157 table ii performance ni 30 iteration shown mean std 5 fold crime student blackfriday loss αa mse c c c mae c c c mhl c c c 0 5 10 15 20 25 30 iteration tr std movtar affine αa using mae 0 5 10 15 20 25 30 iteration tr std movtar affine b αa using mae 0 5 10 15 20 25 30 iteration tr std movtar affine c αa using mhl 0 5 10 15 20 25 30 iteration didi tr std movtar affine c αa using mae 0 5 10 15 20 25 30 iteration didi tr std movtar affine e c αa using mae 0 5 10 15 20 25 30 iteration didi tr std movtar affine f c αa using mhl fig comparison algorithm blue v moving target red crime dataset error bar represent standard deviation 20 diamond boyd cvxpy modeling guage convex optimization journal machine learning research vol 17 no 83 pp 2016 0 5 10 15 20 25 30 iteration tr std movtar affine αa 0 5 10 15 20 25 30 iteration tr std movtar affine b αa 0 5 10 15 20 25 30 iteration didi tr std movtar affine c c αa 0 5 10 15 20 25 30 iteration didi tr std movtar affine c αa fig comparison algorithm blue v moving target red blackfriday dataset using mae error bar represent standard deviation