preprint 1 learning minimize remainder supervised learning yan luo id student member ieee yongkang wong id member ieee mohan kankanhalli id fellow ieee qi zhao id member ieee learning process deep learning method usually update model parameter multiple iteration iteration viewed approximation taylor series expansion remainder consists term usually ignored learning process simplicity learning scheme empowers various based application image retrieval recommendation system video search generally multimedia data image hence remainder approximation possibly work consider remainder informative study affect learning process end propose new learning approach namely gradient adjustment learning gal leverage knowledge learned past training iteration adjust vanilla gradient remainder minimized approximation improved proposed gal easy adapt standard learning framework evaluated three task image classiï¬cation object detection regression model optimizers experiment show proposed gal consistently enhances evaluated model whereas ablation study validate various aspect proposed gal code available index learning deep learning remainder gradient adjustment introduction multimedia application system aim deal variety type medium 1 2 3 4 5 image text etc speciï¬cally image classiï¬cation 6 7 8 object detection 9 10 common component processing image data one major challenge image data large scale 11 12 therefore efï¬ciently learn mapping image label crucial speciï¬cally learning process consists multiple iteration parameter model updated manuscript received august 15 2021 revised january 11 2022 accepted february 20 research wa funded part nsf grant 1908711 1849107 part supported national research foundation singapore strategic capability research centre ing initiative any opinion ï¬ndings conclusion recommendation expressed material author not reï¬‚ect view national research foundation singapore associate editor coordinating review manuscript approving publication wa xxx xxx corresponding author zhao email qzhao luo zhao department computer ence engineering university minnesota email qzhao wong kankanhalli school computing national university singapore email mohan training sample loss training process figure 1 illustration problem minimizing remainder ğ‘Ÿ ğ‘§ğ‘¡ highlighted red usually ignored Ë† simplicity standard learning process ğ‘Ÿ ğ‘§ğ‘¡ possibly not zero learning task work study learn minimize ğ‘Ÿ ğ‘§ğ‘¡ adjusting Î´ğ‘§ğ‘¡and inï¬‚uence learning process following convention â„“ğœ â„“ ğœ represents activation function followed loss function â„“ğœ ğ‘§ğ‘¡ ğ‘¦ğ‘¡ simpliï¬ed â„“ğœ ğ‘§ğ‘¡ minimizing scalar parameterized objective function given some training sample loss function training iteration performs approximation taylor series expansion omitting term 13 fig 1 illustrates approximation brieï¬‚y given sample ğ‘¥ğ‘¡ ğ‘¦ğ‘¡ loss â„“ğœ ğ‘§ğ‘¡ iteratively minimized subtracting term ğœ ğ‘§ğ‘¡ Î´ğ‘§ğ‘¡while discarding remainder ğ‘Ÿ ğ‘§ğ‘¡ gradient descent simple yet effective solution us gradient expand approximation however remainder left training iteration possibly reason term problem nature learning framework model ity firstly diversity semantics high dimensionality image data form learning problem difï¬cult ï¬nd approximation zero remainder secondly stochastic process proven helpful preventing learning process overï¬tting 14 15 computer vision task 11 12 inevitably approximation stochastic process could affected underlying noise distribution 16 lastly although deep learning technique 6 7 8 achieved remarkable success generalizability model still ha room improvement producing better approximation 6 mar 2022 preprint 2 smaller remainder using variety labeled image work study remainder three task namely image classiï¬cation object detection regression remainder informative could helpful improving learning process thus aim minimize remainder difï¬cult compute study affect learning process end propose learning approach named gradient adjustment learning gal leverage knowledge learned past learning step adjust current gradient remainder minimized advantage formulating minimization remainder learning problem firstly instead limiting observed sample iteration proposed gal ha broader view correlation seen sample till current iteration resulting remainder secondly remainder contains order term informative good indicator gauge adjusted gradient better ï¬ts approximation vanilla gradient however challenging predict gradient adjustment vector prediction continuous real value instead discrete label expected precision remarkably higher one classiï¬cation task value gradient sensitive yet decisive learning process solve problem devise proposed gal determine much adjustment take place easy work any network model perceptron mlp since optimization process gradient work closely optimization method investigate efï¬cacy proposed gal several model optimizers image classiï¬cation object detection regression task main contribution follows propose novel learning approach named gradient adjustment learning gal learns adjust vanilla gradient minimizing remainder tions learning process provide ical analysis generalization bound error bound proposed learning approach proposed approach propose safeguard mechanism conditional update policy verifying update using adjusted gradient guarantee adjusted gradient would lead effective descent conduct comprehensive experiment analysis 17 imagenet 11 coco 12 boston housing 18 diabetes 19 california ing 20 experiment show proposed gal demonstrably improves learning process ii related work optimization method stochastic optimization method ten use gradient update model parameter 21 22 23 24 15 25 deep learning stochastic gradient descent sgd 15 inï¬‚uential practical optimization method take parameter update descent based approximation 13 along line several method devised guarantee convergence local minimum certain condition 26 27 28 nevertheless method computationally expensive not feasible learning setting data contrast adaptive method adam 22 rmsprop 21 adabound 24 show remarkable efï¬cacy broad range problem 21 22 24 zhang et al propose optimization method wrap arbitrary optimization method component improve learning stability 29 30 31 32 learn optimizer adaptively compute step length updating model synthetic datasets method contingent vanilla gradient update model work conduct study show adjusted gradient inï¬‚uence learning process method given training data responding gradient computed ing semantics gradient crucial enables learning process date model weight loss minimized 33 method proven modern deep learning model 6 7 8 34 35 36 serve backbone facilitate broad range multimedia tions 1 2 4 5 37 38 39 40 41 42 43 except updating model weight gradient tile regulating regularizing learning process gradient alignment 44 45 searching adversarial turbation 46 sharpness minimization 47 making decision choosing hyperparameters 48 etc speciï¬cally paz ranzato propose gradient episodic memory method alleviates catastrophic forgetting continual learning maintaining gradient update ï¬t memory constraint 49 gradient aligned improve agreement knowledge learned completed training step new information used updating model 50 transfer learning gradient computed multiple source domain combined minimize loss target domain 44 proposed gal thus beneï¬t application remainder approximation approximation theory branch mathematics study process imating general function 51 52 exact mapping problem deterministic function considerable number work study evaluate remainder dimensional variable space 53 54 55 56 however no exact mapping input output computer vision task input image space 11 12 make difï¬cult exactly compute remainder result remainder approximation ignored sake simplicity learning process 6 7 8 work ï¬rst study effect minimizing remainder learning problem data iii problem formalization without loss generality consider standard siï¬cation problem formulation adapted learning problem minor modiï¬cations given preprint 3 fox fox standard learning paradigm proposed gradient adjustment learning gal figure 2 illustration standard proposed learning paradigm note proposed learning paradigm â„ ğ‘§ ğœƒ always output 0 proposed learning paradigm reduced standard learning paradigm training set ğ‘¥ğ‘– ğ‘¦ğ‘– data 0 1 ğ‘‘is corresponding ğ‘‘ dimensional binary label learnable model ğ‘š x ğœ” parameter ğœ”is optimized minimize loss according empirical risk minimization principle 57 written minimize ğœ” 1 ğ‘¥ğ‘¡ ğ‘¦ğ‘¡ ğ‘š ğ‘¥ğ‘¡ ğœ” ğ‘¦ğ‘¡ 1 cardinality ğ·and ğœ rğ‘‘ ğœ” 0 1 ğ‘‘is activation function softmax layer problem design training ğ‘š ğœ” ha extensively studied 6 7 8 not focus work instead focus loss discriminative feature ğ‘§ output ğ‘š ğœ” let â„“ğœ ğ‘§ denote â„“ ğœ ğ‘§ ğ‘¦ simplicity taylor series expansion â„“ğœ â„“ğœ ğ‘§ğ‘¡ ğœ ğ‘§ğ‘¡ ğ‘Ÿ ğ‘§ğ‘¡ 2 loss remainder ğ‘Ÿ ğ‘§ğ‘¡ ğ‘œ Î´ğ‘§ğ‘¡ higher order term Î´ğ‘§ğ‘¡ second term ğœ ğ‘§ğ‘¡ Î´ğ‘§ğ‘¡ tional derivative ğ‘§ğ‘¡in direction Î´ğ‘§ğ‘¡ mathematically difï¬cult compute higher order derivative therein ğ‘œ Î´ğ‘§ğ‘¡ therefore maximizing margin â„“ğœ ğ‘§ğ‘¡ â„“ğœ equivalent convergence enhancement challenging moreover ğ‘§ğ‘¡ ğ‘¦ğ‘¡ follows some stochastic process would vary iteration different ğ‘§ğ‘¡ ğ‘¦ğ‘¡ pair may contribute unevenly learning process fig 2 left show standard learning approach ğ‘Ÿ ğ‘§ğ‘¡ omitted denote Ï† Î¸ optimizer set hyperparameters Î¸ learning rate momentum weight decay etc key step optimization process loss function â„“takes prediction Ë† ğœ ğ‘§ truth ğ‘¦as input compute gradient Ë† ğ‘¦ ğ‘¦ according chain rule gradient computed next Ï† Î¸ computed update standard learning approach gradient mathematically computed considered local choice observed input ğ‘¥ ğ‘¦ iteration making local choice step viewed greedy strategy may ï¬nd solution 58 contrast work adjusts gradient adjustment module aim minimize remainder shown fig 2 right correspondingly adjustment viewed addition two vector one vanilla gradient vector generated adjustment module geometric interpretation shown fig iv gradient adjustment learning section ï¬rst describe gradient adjustment mechanism supervised learning framework training process proposed gradient adjustment module detailed finally discus theoretical property gradient adjustment learning process introduce integration proposed gal standard learning approach ï¬rst deï¬ne gradient adjustment module â„ ğœƒ see fig 2 aim model correlation adjustment point ğ‘§and corresponding loss remainder ğ‘Ÿ â„ ğ‘§ ğœƒ 3 different classiï¬er predicts conï¬dence score 0 1 proposed gal learns predict gradient adjustment vector tends small sophisticated subtle curb volatility could overwhelm gradient ruin learning process apply ization norm adaptively scale coincide gradient ğ›¼ 4 0 1 scalar constrains relative strength adjustment referencing magnitude 0 implies no adjustment performed normalized feature ğ‘£is added computed vanilla gradient used input optimizer model updating ğ‘£ 5 gradient adjustment module â„ ğœƒ any type dnns mlp cnn rnn computed ment possibly negative some dimension remove ï¬nal activation layer softmax layer line algorithm 1 conditional update policy compute update Î´ğœ”based relationship â„“ğœ ğœ‚ğ‘” â„“ğœ ğ‘§ ğœ‚is tentative learning rate â„“ğœ ğœ‚ğ‘” tentative loss detailed section checking â„“ğœ ğœ‚ğ‘” ğ‘§ able detect ğ‘”is not good ï¬t reduce loss case alternatively use preprint 4 figure 3 geometric interpretation proposed gal adjustment performed vector addition operation algorithm 1 gradient adjustment learning 1 input ğ· ğ‘š ğœ” â„ ğœƒ Ï† Î¸ learning rate magnitude ratio 0 1 adaptive scalar ğ›½so ğ›½ğœ‚ 2 pair ğ‘¥ ğ‘¦ 3 ğ‘š ğ‘¥ ğœ” Ë† ğœ ğ‘§ 4 ğ‘§ 5 predict gradient adjustment â„ ğ‘§ ğœƒ 6 adjust gradient ğ‘£ 7 â„“ğœ ğœ‚ğ‘” ğ‘§ 8 Ï† Î¸ 9 else 10 Ï† Î¸ 11 update parameter 12 minimize remainder objective 6 13 compute 14 compute update Ï† Î¸ 15 update adjustment module parameter 16 vanilla gradient update regarded safeguard mechanism verify whether adjusted gradient ğ‘”leads effective descent adjustment module training discussed section iii remainder ğ‘Ÿ ğ‘§ğ‘¡ eq 2 difï¬cult estimate practice however remainder modeled three term equation turn estimation learning problem minimize ğœƒ ğ‘§ğ‘¡ 6 ğ‘Ÿ ğ‘§ğ‘¡ â„“ğœ ğœ‚ğ‘” ğ‘§ğ‘¡ ğ‘§ğ‘¡ 7 â„“ğœ ğœ‚ğ‘” tentative loss ğ›½ğœ‚is tentative learning rate brieï¬‚y tentative loss used evaluate whether adjusted gradient ğ‘”is better although ğœ‚ğ‘”is decision condition still need learning rate ï¬t gradient descent scheme straightforward way using hyperparameter ğ›½as weight learning rate ğœ‚for parameter update way ğœ‚is adaptive note ğ‘§ğ‘¡ minimized objective 6 rather ğ‘Ÿ ğ‘§ğ‘¡ prediction subtle possible overï¬t underï¬t remainder eq 3 4 seen ğ‘”is function objective 6 provides information adjusting gradient direction reduces remainder taylor approximation theoretical property section present learning guarantee remainder error bound gal problem simplicity denote â„ ğ‘§ ğœƒ â„ ğ‘§ let target adjustment ğœ‚ ğ‘§ gradient adjustment vector usually small assume exist ğ‘ ğ‘£ ğ‘ ğ‘ ğ‘§is drawn according unknown distribution ğ‘§ target labeling function moreover follow problem setting 59 restrict loss function â„“ğ‘loss generalization bound gal considered variant regression problem ï¬nds hypothesis â„ ğ‘ ğ‘ ğ‘‘in set h small generalization error ğ‘…d â„ â„“ğ‘ â„ ğ‘§ ğ‘§ practice unknown use empirical error approximation sample dataset ğ· Ë† ğ‘…ğ· â„ 1 â„“ğ‘ ğ‘§ğ‘– ğ‘– theorem generalization bound gal denote h ï¬nite hypothesis set given ğ‘£ ğ‘ ğ‘ ğ‘‘ any ğ›¿ 0 probability least 1 following inequality hold â„ ğ‘…ğ· â„ ğ‘ ğ‘‘ log log 2 ğ›¿ proof proof sketch similar classiï¬cation eralization bound provided 59 first â„“ğ‘ ğ‘£ Ã­ğ‘‘ ğ‘ 1 ğ‘‘ ğ‘ 1 ğ‘ know â„“ğ‘is bounded ğ‘‘ ğ‘ 1 union bound given error ğœ‰ ğ‘ƒğ‘Ÿ sup â„ ğ‘… â„ ğœ‰ ğ‘ƒğ‘Ÿ â„ ğ‘… â„ ğœ‰ hoeffding bound ğ‘ƒğ‘Ÿ â„ ğ‘… â„ ğœ‰ exp ğ‘‘ ğ‘ 2 ğ‘ due probability deï¬nition exp ğ‘‘ ğ‘ 2 ğ‘ considering ğœ‰is function variable range ğ‘‘ ğ‘ 1 ğ‘ log 2 ğ›¿ since know ğ‘ƒğ‘Ÿ ğ‘“ ğ‘… ğ‘“ ğœ‰ probability ğ›¿ inferred ğ‘ƒğ‘Ÿ ğ‘“ ğ‘… ğ‘“ ğœ‰ least 1 completes proof remark theorem support general intuition training data produce better generalization aligned conventional learning problem classiï¬cation regression 59 furthermore distinct conventional learning problem range gradient ments dimension could affect generalization bound preprint 5 gradient descent rmsprop adam lookahead adabound figure 4 illustration effect gal red convergence various optimizers comparison standard process blue top row convergence path bottom corresponding loss curve problem publicly available 50 theorem conventional remainder error bound 60 let ğ¿ rğ‘› ğ‘“is continuously differentiable rğ‘›and partial derivative lipschitz continuous constant ğ¿ any ğ‘“ ğ‘§ ğ‘§ 2 theorem revisited remainder error bound let ğ¿ rğ‘› given 0 1 any denote minimal angle vector ğœ ğ‘§ ğ›¾and assume two vector ğ‘“ ğ‘§ ğ‘§ 2 co proof similar proof theorem 60 use integral form remainder taylor expansion ğ‘“ ğ‘§ ğ‘§ 0 ğœ ğ‘§ 0 ğœ ğ‘§ 0 co ğœ ğ‘§ co 0 ğœ ğ‘§ co 0 ğœğ‘‘ğœ ğ¿ 2 co completes proof remark theorem show tighter error bound remainder bound theorem 60 justiï¬es properly adjusting gradient direction lead effective descent new insight compared theorem moreover indicates optimal condition geometric perspective perpendicular ğœ ğ‘§ remainder error bound zero feasible liable small term magnitude ğœ ğ‘§ not vary dramatically 0 1 addition theorem provides some guideline design eq 4 force adjustment module ï¬nd direction instead vector stability adaptivity optimization method illustrate effectiveness proposed gal optimization process employ problem used 50 ğ‘“ ğ‘¥ ğ‘¦ ğ‘¥ ğ‘¦ visualize convergence path various optimizers fig 4 show convergence path top row corresponding curve ğ‘§against step bottom row speciï¬cally blue produced standard process red one produced proposed gal given starting point convergence affected problem optimizers proposed gal observes completed convergence step learn adjust gradient resulting convergence curve show ï¬nds shortcut reach local minimum efï¬ciently furthermore fig 4 veriï¬es proposed gal general nature work various optimizers experiment comprehensively evaluate proposed gal iou model optimizers speciï¬cally conduct iment image classiï¬cation task 24 29 object detection task 61 regression task 18 19 20 datasets following experimental protocol 24 29 use 17 imagenet 11 evaluation image classiï¬cation task speciï¬cally consists image 10 100 class imagenet ha 1000 visual concept class provides average 1000 image class object detection experiment follow experimental protocol 61 use coco 2017 12 evaluation coco preprint 6 table image classiï¬cation performance average error standard deviation three run architecture used gal number parameter gal model optimizer error lookahead 29 adabound 24 efï¬cientnet 8 efï¬cientnet sgd 50 effcientnet sgd reproduced effcientnet sgd gal effcientnet lookahead reproduced effcientnet lookahead gal effcientnet adabound reproduced effcientnet adabound gal object detection benchmark dataset consists training image validation image 80 object category moreover three datasets boston housing 18 diabetes 19 california housing 20 used regression task speciï¬cally boston housing includes 506 entry entry ha 14 feature diabetes consists 442 sample 10 feature california housing ha 20640 sample sample ha 8 feature model training scheme image classiï¬cation task adopt efï¬cientnet 8 cifar resnet 6 efï¬cientnet imagenet originally efï¬cientnet trained cloud tpu 350 epoch batch size 20481 8 due limitation computation resource follow training scheme 50 train efï¬cientnet model cifar similarly employ publicly available train resnet efï¬cientnet imagenet 8 nvidia gpus batch size train model 90 epoch 6 29 provide comparable result object detection task detection transformer detr originally trained 16 nvidia gpus 500 epoch 61 due limitation computation resource follow detr train model 4 nvidia 2080 ti gpus 150 epoch use hyperparameters 61 regarding optimization method model trained cifar sgd lookahead 29 adabound 24 following 29 lookahead wrapped around sgd experiment model trained rmsprop 21 imagenet detr trained adamw 62 coco regression experiment run cpu adam 22 proposed gal employ mlp simpler cnn rnn throughout work gal take feature ğ‘§ input yield dimension output gradient adjustment simplicity denote mlp ğ‘ example 1 2 3 table ii image classiï¬cation performance average error standard deviation three run architecture used gal number parameter gal model optimizer error lookahead 29 adabound 24 efï¬cientnet 8 efï¬cientnet sgd 50 effcientnet sgd reproduced effcientnet sgd gal effcientnet lookahead reproduced effcientnet lookahead gal effcientnet adabound reproduced effcientnet adabound gal indicates architecture consists four linear transformation afï¬ne matrix use architecture imagenet regarding ğ›¼ ğ›½ use 1 1 10 sgd lookahead adabound respectively 1 5 10 sgd lookahead adabound respectively imagenet respectively object detection task minimize remainder predicted bounding box feature four ï¬‚oats indicating box correspondingly use 1 arch ğ›¼and ğ›½ respectively regression task architecture regression model 64 boston housing diabetes california housing respectively architecture proposed gradient justment module boston housing diabetes california housing respectively ï¬x three datasets performance experimental result imagenet coco reported ii table iii iv respectively shown table ii proposed gal able work various optimization method sgd lookahead adabound improve performance also table iii show able work different model provides performance gain consistent improvement object detection observed table iv coco overall proposed gal improves convergence training process achieve better accuracy standard process various model task aligned implication theorem evaluate proposed method apply regression task speciï¬cally proposed method applied three regression datasets boston housing 18 diabetes 19 california housing 20 three metric mean absolute error mae mean squared error mse coefï¬cient termination used evaluate performance measure accuracy efï¬ciency model data preprint 7 table iii image classiï¬cation performance imagenet average accuracy standard deviation three run arch used gal resnet efï¬cientnet respectively use 90 epoch model training fair comparison 6 29 model optimizer parameter sgd 6 lookahead 29 rmsprop 350 epoch 8 rmsprop reproduced rmsprop gal rmsprop reproduced rmsprop gal table iv object detection performance coco validation faster follow detr suggestion use 150 epoch model training 61 setting take approximate 9 day training server model epoch parameter ap aps apm apl 61 500 500 reproduced 150 gal 150 reproduced 150 gal 150 table v regression performance boston housing 18 diabetes 19 california housing 20 dataset resp indicates larger resp smaller score suggests better performance experiment run 5 time different random seed also include analysis performance baseline performance proposed method measure improvement ğ‘¡ğ‘ ğ‘¡ğ‘ğ‘¡and ğ‘are p value respectively dataset method mean absolute error mae mean squared error mse coefï¬cient determination boston housing baseline proposed ğ‘¡ğ‘ ğ‘¡ğ‘ğ‘¡ ğ‘ diabetes baseline proposed ğ‘¡ğ‘ ğ‘¡ğ‘ğ‘¡ ğ‘ california housing baseline proposed ğ‘¡ğ‘ ğ‘¡ğ‘ğ‘¡ ğ‘ popular metric regression larger score indicates better performance regression task smaller mae mse score indicate better performance experimental result reported table proposed method improves performance three metric understand statistical signiï¬cance efï¬cacy proposed method perform result baseline one proposed method according ğ‘values result yielded proposed method statistically signiï¬cantly one yielded baseline signiï¬cance level lower vi analysis generalization ability approximation remainder check generalization ability model trained gal plot loss curve validation test set fig loss model learned adjusted gradient lower model using vanilla gradient implies adjusted gradient better vanilla gradient term generalizability fig 6 show corresponding remainder computed eq 7 cosine similarity vanilla gradient adjustment vector imagenet positive similarity implies direction adjustment vector ha overall smaller angle vanilla gradient smaller overall proposed adjusted gradient converge local preprint 8 imagenet coco figure 5 loss curve various datasets figure 6 remainder curve left cosine similarity curve right imagenet minimum efï¬ciently vanilla gradient datasets note imagenet training cause series ï¬‚uctuations early epoch stabalizes epoch effect random noise table vi show effect random noise training efï¬cientnet random noise generated uniform normal distribution replace proposed adjustment eq 3 note ğ›¼ part proposed learning approach see eq 4 result show normalizing adjustment vector appropriate range deï¬nitely required gradient subtle sophisticated large adjustment vector could lead divergence training moreover properly injecting some random noise using proposed approach see eq 4 5 improves performance yet noise still le effective adjustment vector generated gal training time understand computation overhead report training time using baseline proposed method table vii imagenet experiment learning process without proposed gal take second per image train model take second per image proposed gal extra time 12 millisecond proposed method used forward backward process similarly proposed method take extra 15 11 millisecond training note experiment run workstation equipped 4 nvidia 2080 ti gpus experiment imagenet run workstation equipped 8 nvidia gpus table vi effect random noise generated uniform normal distribution training efï¬cientnet sgd error rate standard learning process gal ğ‘£ ğ‘£ error u 1 ğ›¼ eq 4 u 1 n 0 1 ğ›¼ eq 4 n 0 1 table vii training time using proposed method image comparison one using baseline note proposed gradient adjustment only take place training phase word test time model trained proposed method identical one model trained baseline method dataset method time imagenet baseline proposed baseline proposed baseline proposed effect hyperparameters analyse effect ğ›¼ ğ›½and various gal chitectures sgd lookahead performance shown fig proposed gal us hyperparameters 1 architecture sgd 5 architecture lookahead vary one hyperparameter time hyperparameters kept unchanged plot shown ï¬gure range ğ›¼consistently lead lower classiï¬cation error contrast classiï¬cation error sensitive ğ›½ dependent 1 lead best performance sgd 5 lead best performance lookahead regarding effect architecture use architecture 256 fig 7 right four architecture parameter respectively overall 32 give rise lower classiï¬cation error preprint 9 figure 7 effect ğ›¼ left ğ›½ middle architecture right varying ğ›¼ 1 resp 5 sgd resp lookahead varying ğ›½ resp sgd resp lookahead using different architecture 256 1 resp 5 sgd resp lookahead update policy b improved sample figure 8 ablation study proposed gal sgd effect update policy refers algorithm 1 line line x mean use line x generate update line 7 mean modify statement line 7 â„“ğœ ğœ‚ğ‘” â„“ğœ ğ‘§ b effect adjusted gradient ğ‘”and vanilla gradient tentative loss architecture sgd lookahead corresponding computational overhead relatively low effect various update policy introduced algorithm 1 line tentative loss â„“ğœ ğœ‚ğ‘” le equal loss â„“ğœ ğ‘§ use ğ‘”to update gradient weight according chain rule denote case line 7 standard process always used update gradient weight denote case line discus two possible update policy always using ğ‘”and using ğ‘” â„“ğœ ğœ‚ğ‘” le loss denote two case line 8 line 7 respectively shown fig policy line 8 outperforms line 10 not optimal line 7 training process close local minimum loss remainder much smaller line 10 would efï¬cient line moreover line 7 slightly better line 7 adjusted gradient vanilla gradient proposed gal aim yield adjusted gradient ğ‘” would good know whether ğ‘”leads better descent lower loss use tentative loss test ğ‘” table viii effect mlps cnns sgd case cnns feature 100 would feature multiple convolutional layer kernel would performed feature example cnn indicates convolutional layer 256 kernel followed convolutional layer 64 kernel mlps cnns ï¬nal layer cnns additional adaptive spatial pooling layer prior ï¬nal layer reduces width height dimension model arch parameter error mlp 256 cnn 256 fig show many time ğ‘”outperforms sample result implies gal indeed help adjust vanilla gradient tentative loss considerable amount sample early stage mlps cnns explore effect using cnns instead mlps proposed gradient adjustment module classiï¬cation task result analysis reported table viii seen cnns much larger number parameter mlps except single layer variant achieve lower performance mlps mlps desired choice architecture well aligned fact discriminative feature modern deep learning model usually vii conclusion propose new learning approach formulates remainder problem leverage knowledge learned past approximation enhance learning end propose gradient adjustment preprint 10 learning gal method employ model learn predict adjustment gradient fashion easy simple adapt standard training process correspondingly provide theoretical ing experimental result model optimizers image classiï¬cation object detection sion task ï¬ndings experimental result aligned theoretical understanding error bound one intriguing extension work explore model design capture subtle characteristic gradient adjustment vector adjustment prediction reference 1 xia shao luo fu understanding kin relationship photo ieee trans vol 14 no 4 pp 2012 2 ding tao robust face recognition via multimodal deep face representation ieee trans vol 17 no 11 pp 2015 3 zhan zhang image sharpness assessment based maximum gradient variability gradient ieee trans vol 20 no 7 pp 2018 4 cho kang gradient cnn denoiser separable optimization feature dimension ieee trans vol 21 no 2 pp 2019 5 xu li wong zhao kankanhalli interact intend interaction detection ieee trans vol 22 no 6 pp 2020 6 zhang ren sun deep residual learning image recognition ieee conference computer vision pattern recognition 2016 pp 7 krizhevsky sutskever hinton imagenet tion deep convolutional neural network advance neural information processing system 2012 pp 8 tan le efï¬cientnet rethinking model scaling convolutional neural network international conference machine learning ser proceeding machine learning research chaudhuri salakhutdinov vol pmlr 2019 pp 9 gkioxari dollÃ¡r girshick mask ieee international conference computer vision 2017 pp 10 ren girshick sun faster towards time object detection region proposal network advance neural information processing system 2015 pp 11 deng dong socher li li imagenet hierarchical image database ieee conference computer vision pattern recognition 2009 pp 12 lin maire belongie hay perona ramanan dollÃ¡r zitnick microsoft coco common object context european conference computer vision ser lecture note computer science vol 8693 2014 pp 13 boyd boyd vandenberghe convex optimization cambridge university press 2004 14 bottou curtis nocedal optimization method scale machine learning siam review vol 60 no 2 pp 2018 15 robbins monro stochastic approximation method annals mathematical statistic pp 1951 16 zhu wu yu wu anisotropic noise stochastic gradient descent behavior escaping sharp minimum regularization international conference machine learning 2019 pp 17 krizhevsky learning multiple layer feature tiny image master thesis university toronto 2009 18 harrison jr rubinfeld hedonic housing price demand clean air journal environmental economics management vol 5 no 1 pp 1978 19 dua graff uci machine learning repository 2017 20 pace barry sparse spatial autoregressions statistic probability letter vol 33 no 3 pp 1997 21 hinton srivastava swersky neural network machine learning lecture overview gradient 22 kingma ba adam method stochastic optimization international conference learning representation 2015 23 liu jiang chen liu gao han variance adaptive learning rate beyond international conference learning representation 2020 24 luo xiong liu adaptive gradient method namic bound learning rate international conference learning representation 2019 25 zhuang tang ding tatikonda dvornek pademetris duncan adabelief optimizer adapting stepsizes belief observed gradient advance neural information processing system vol 33 2020 26 carmon duchi hinder sidford accelerated method nonconvex optimization siam journal optimization vol 28 no 2 pp 2018 27 jin ge netrapalli kakade jordan escape saddle point efï¬ciently international conference machine learning 2017 pp 28 reddi zaheer sra poczos bach salakhutdinov smola generic approach escaping saddle point international conference artiï¬cial intelligence statistic 2018 pp 29 zhang lucas ba hinton lookahead optimizer k step forward 1 step back advance neural information processing system 2019 pp 30 andrychowicz denil gomez hoffman pfau schaul shillingford de freitas learning learn gradient descent gradient descent advance neural information processing system 2016 pp 31 chen hoffman colmenarejo denil lillicrap botvinick freitas learning learn without gradient descent gradient descent international conference machine learning 2017 pp 32 ji chen wang yu li learning learn gradient aggregation gradient international joint conference artiï¬cial intelligence 2019 pp 33 rumelhart hinton williams learning sentations error nature vol 323 no 6088 pp 1986 34 tan le smaller model faster ing international conference machine learning ser proceeding machine learning research meila zhang vol pmlr 2021 pp 10 106 35 dosovitskiy beyer kolesnikov weissenborn zhai unterthiner dehghani minderer heigold gelly uszkoreit houlsby image worth word former image recognition scale international conference learning representation 2021 36 tolstikhin houlsby kolesnikov beyer zhai terthiner yung steiner keysers uszkoreit lucic dosovitskiy architecture vision corr vol 2021 37 cong yuan luo towards scalable summarization consumer video via sparse dictionary selection ieee trans vol 14 no 1 pp 2012 38 yadati katti kankanhalli cavva computational affective advertising ieee trans vol 16 no 1 pp 2014 39 bu liu han wu ji learning feature deep belief network model retrieval recognition ieee trans vol 16 no 8 pp 2014 40 zhang gao xia lu shen ji representative discovery structure cue image segmentation ieee trans vol 16 no 2 pp 2014 41 cho courville bengio describing multimedia content using network ieee trans vol 17 no 11 pp 2015 42 zhang cheng tian unsupervised image classiï¬cation weak semantic consistency ieee trans vol 21 no 10 pp 2019 43 li wong zhao kankanhalli visual social relationship recognition int comput vol 128 no 6 pp 1764 2020 44 li xu wong zhao kankanhalli gradmix source transfer across domain task ieee winter conference application computer vision 2020 pp 45 luo wong kankanhalli zhao transfer learning saliency prediction european conference computer preprint 11 vision ser lecture note computer science vedaldi bischof brox frahm vol springer 2020 pp 46 goodfellow shlens szegedy explaining harnessing adversarial example international conference learning sentations bengio lecun 2015 47 foret kleiner mobahi neyshabur minimization efï¬ciently improving generalization international conference learning representation 2021 48 pedregosa hyperparameter optimization approximate gradient international conference machine learning ser jmlr workshop conference proceeding balcan weinberger vol 2016 pp 49 ranzato gradient episodic memory continual learning advance neural information processing system 2017 pp 50 luo wong kankanhalli zhao direction concentration learning enhancing congruency machine learning ieee tions pattern analysis machine intelligence pp 2019 51 achieser theory approximation courier corporation 2013 52 timan theory approximation function real variable elsevier 2014 53 berz hoffstÃ¤tter computation application taylor nomials interval remainder bound reliable computing vol 4 no 1 pp 1998 54 milne remainder linear method approximation journal research national bureau standard vol 43 no 5 pp november 1949 55 stancu evaluation remainder term approximation mulas bernstein polynomial mathematics computation vol 17 no 83 pp 1963 56 remainder certain linear approximation formula two variable journal society industrial applied ic series b numerical analysis vol 1 no 1 pp 1964 57 vapnik overview statistical learning theory ieee action neural network vol 10 no 5 pp 1999 58 black greedy algorithm dictionary algorithm data structure vol 2 62 2005 59 mohri rostamizadeh talwalkar foundation machine learning mit press 2012 60 nesterov introductory lecture convex optimization basic course springer science business medium 2013 61 carion massa synnaeve usunier kirillov zagoruyko object detection transformer ropean conference computer vision ser lecture note computer science vol 12346 2020 pp 62 loshchilov hutter decoupled weight decay regularization international conference learning representation yan luo currently pursuing degree department computer science ing university minnesota umn twin city prior umn joined social medium sesame centre interactive ital medium institute national university singapore nu research assistant also joined visual information processing laboratory nu student received degree computer science xi university science technology worked industry several year distributed system research interest include computer vision computational visual cognition deep learning student member ieee since yongkang wong senior research fellow school computing national university singapore also assistant director nu centre research privacy technology obtained beng versity adelaide phd university queensland ha worked graduate researcher nicta queensland laboratory brisbane old australia 2008 current research interest area processing machine learning action recognition human centric analysis member ieee since mohan kankanhalli provost chair professor computer science national university singapore nu dean nu school computing also directs nu centre research privacy technology conduct research privacy structured well unstructured multimedia sensor iot data mohan obtained btech iit kharagpur phd rensselaer polytechnic institute mohan research interest multimedia puting computer vision information security privacy processing ha made many contribution area multimedia vision image video understanding data fusion visual saliency well multimedia security content authentication privacy surveillance mohan fellow ieee qi zhao associate professor department computer science engineering sity minnesota twin city main research interest include computer vision machine learning cognitive neuroscience healthcare received computer engineering sity california santa cruz wa postdoctoral researcher computation neural system california institute technology 2009 joining university minnesota qi wa assistant professor department electrical computer engineering department ophthalmology national university singapore ha published 100 journal conference paper edited book springer titled computational cognitive neuroscience vision provides systematic comprehensive overview vision various perspective serf associate editor ieee tnnls ieee tmm program chair wacv 22 organizer area chair cvpr major venue computer vision ai regularly member ieee since 2004