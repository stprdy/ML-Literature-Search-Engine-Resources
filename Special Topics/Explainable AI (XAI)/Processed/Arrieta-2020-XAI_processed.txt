information fusion 58 2020 content list available sciencedirect information fusion journal homepage explainable artiÔ¨Åcial intelligence xai concept taxonomy opportunity challenge toward responsible ai alejandro barredo arrieta natalia b javier del ser c adrien bennetot b e f siham tabik g alberto barbado h salvador garcia g sergio daniel molina g richard benjamin h raja chatila f francisco herrera g tecnalia derio 48160 spain b ensta institute polytechnique paris inria flower team palaiseau france c university basque country bilbao 48013 spain basque center applied mathematics bcam bilbao 48009 bizkaia spain e segula technology parc activit√© de pissaloup trappes france f institut de syst√®mes intelligents et de robotique sorbonne universit√® france g dasci andalusian institute data science computational intelligence university granada granada 18071 spain h telefonica madrid 28050 spain r c l e n f keywords explainable artiÔ¨Åcial intelligence machine learning deep learning data fusion interpretability comprehensibility transparency privacy fairness accountability responsible artiÔ¨Åcial intelligence b r c last year artiÔ¨Åcial intelligence ai ha achieved notable momentum harnessed appropriately may deliver best expectation many application sector across Ô¨Åeld occur shortly machine learning entire community stand front barrier explainability inherent problem latest technique brought ensemble deep neural network not present last hype ai namely expert system rule based model paradigm underlying problem fall within explainable ai xai Ô¨Åeld widely acknowledged crucial feature practical deployment ai model overview presented article examines existing literature contribution already done Ô¨Åeld xai including prospect toward yet reached purpose summarize previous eÔ¨Äorts made deÔ¨Åne explainability machine learning establishing novel deÔ¨Ånition explainable machine learning cover prior conceptual proposition major focus audience explainability sought departing deÔ¨Ånition propose discus taxonomy recent contribution related explainability diÔ¨Äerent machine learning model including aimed explaining deep learning method second dedicated taxonomy built examined detail critical literature analysis serf motivating background series challenge faced xai interesting crossroad data fusion explainability prospect lead toward concept responsible artiÔ¨Åcial intelligence namely methodology implementation ai method real organization fairness model explainability accountability core ultimate goal provide newcomer Ô¨Åeld xai thorough taxonomy serve reference material order stimulate future research advance also encourage expert professional discipline embrace beneÔ¨Åts ai activity sector without any prior bias lack interpretability introduction artiÔ¨Åcial intelligence ai lie core many activity sector embraced new information technology 1 root ai trace back several decade ago clear consensus paramount importance featured nowadays intelligent machine endowed learning reasoning adaptation capability virtue capability ai method achieving unprecedented corresponding author tecnalia tecnologico ed 700 48170 derio bizkaia spain address del ser level performance learning solve increasingly complex putational task making pivotal future development human society 2 sophistication system ha lately increased extent almost no human intervention quired design deployment decision derived system ultimately aÔ¨Äect human life medicine law defense emerging need understanding sion furnished ai method 3 received 22 october 2019 received revised form 19 december 2019 accepted 25 december 2019 available online 26 december 2019 2019 elsevier right reserved barredo arrieta del ser et al information fusion 58 2020 fig evolution number total publication whose title abstract keywords refer Ô¨Åeld xai last year data retrieved december 2019 using search term indicated legend querying database interesting note latent need interpretable ai model time conforms intuition interpretability requirement many scenario yet ha not 2017 interest technique explain ai model ha permeated throughout research community Ô¨Årst ai system easily interpretable last year witnessed rise opaque decision system deep neural network dnns empirical success deep learning dl model dnns stem combination eÔ¨Écient learning algorithm huge parametric space latter space comprises hundred layer million parameter make dnns considered complex model 4 opposite ness transparency search direct understanding mechanism model work 5 machine learning ml model increasingly employed make important prediction critical context mand transparency increasing various stakeholder ai 6 danger creating using decision not justiÔ¨Åable legitimate simply not allow obtaining detailed explanation behaviour 7 explanation supporting output model crucial precision medicine expert require far information model simple binary prediction ing diagnosis 8 example include autonomous vehicle transportation security Ô¨Ånance among others general human reticent adopt technique not rectly interpretable tractable trustworthy 9 given ing demand ethical ai 3 customary think focusing solely performance system increasingly opaque true sense performance model transparency 10 however improvement understanding system lead correction cies developing ml model consideration ity additional design driver improve implementability 3 reason interpretability help ensure impartiality detect consequently correct bias training dataset interpretability facilitates provision robustness ing potential adversarial perturbation could change tion interpretability act insurance only meaningful ables infer output guaranteeing underlying truthful causality exists model reasoning mean interpretation system order considered practical provide either understanding model mechanism prediction visualization model tion rule hint could perturb model 11 order avoid limiting eÔ¨Äectiveness current tion ai system explainable ai xai 7 proposes creating suite ml technique 1 produce explainable model taining high level learning performance prediction accuracy 2 enable human understand appropriately trust eÔ¨Äectively manage emerging generation artiÔ¨Åcially intelligent partner xai draw well insight social science 12 considers psychology explanation fig 1 display rising trend contribution xai related concept literature outbreak share rationale research agenda national government agency although some recent survey summarize upsurge activity xai across sector discipline overview aim cover creation complete uniÔ¨Åed framework category concept allow scrutiny understanding Ô¨Åeld xai method furthermore pose intriguing thought around explainability ai model data fusion context regard data privacy model ity along research opportunity challenge tiÔ¨Åed throughout study serve pull factor toward responsible artiÔ¨Åcial intelligence term refer series ai ples necessarily met deploying ai real application later show detail model explainability among crucial aspect ensured within methodological framework novel contribution overview summarized follows grounded Ô¨Årst elaboration concept term used related research propose novel deÔ¨Ånition explainability place audience fig 2 key aspect considered plaining ml model also elaborate diverse purpose sought using xai technique trustworthiness privacy awareness round claimed importance purpose targeted audience model explainability deÔ¨Åne examine diÔ¨Äerent level transparency ml model feature well diverse approach explainability namely explanation ml model not transparent design 83 barredo arrieta del ser et al information fusion 58 2020 fig diagram showing diÔ¨Äerent purpose explainability ml model sought diÔ¨Äerent audience proÔ¨Åles two goal occur prevail across need model understanding regulatory compliance image partly inspired one presented 29 used permission ibm thoroughly analyze literature xai related concept published date covering approximately 400 contribution ranged two diÔ¨Äerent taxonomy Ô¨Årst taxonomy address explainability ml model using previously made tion transparency explainability including model transparent deep shallow learning model second taxonomy deal xai od suited explanation deep learning model using siÔ¨Åcation criterion closely linked family ml method layerwise explanation representation vector attention enumerate series challenge xai still remain Ô¨Åciently addressed date speciÔ¨Åcally identify research need around concept metric evaluate explainability ml model outline research direction toward making deep ing model understandable augment scope prospect toward implication xai technique regard conÔ¨Ådentiality robustness adversarial setting data diversity area intersecting explainability previous prospective discussion arrive concept responsible artiÔ¨Åcial intelligence manifold concept pose systematic adoption several ai principle ai model practical use addition explainability guideline behind responsible ai establish fairness accountability vacy also considered implementing ai model real environment since responsible ai blend together model explainability design call profound reÔ¨Çection around beneÔ¨Åts risk xai technique scenario dealing sitive information conÔ¨Ådential ml model later show regulatory push toward data privacy quality integrity governance demand eÔ¨Äorts ass role xai arena regard provide insight implication xai term privacy security diÔ¨Äerent data fusion paradigm remainder overview structured follows Ô¨Årst section 2 subsection therein open discussion terminology concept revolving around explainability interpretability ai ending aforementioned novel deÔ¨Ånition interpretability section general criterion categorize analyze ml model xai perspective section 3 4 proceed viewing recent Ô¨Åndings xai ml model transparent model technique respectively comprise main division aforementioned taxonomy also include review hybrid proaches among two attain xai beneÔ¨Åts caveat ergies among family method discussed section 5 present prospect general challenge some consequence cautious finally section 6 elaborates concept sponsible artiÔ¨Åcial intelligence section 7 concludes survey outlook aimed engaging community around vibrant research area ha potential impact society particular tor progressively embraced ml core technology activity explainability proceeding literature study convenient Ô¨Årst establish common point understanding term ability stand context ai speciÔ¨Åcally ml indeed purpose section namely pause numerous deÔ¨Ånitions done regard concept argue explainability important issue ai ml introduce general classiÔ¨Åcation xai approach drive literature study thereafter terminology clariÔ¨Åcation one issue hinders establishment common ground interchangeable misuse interpretability explainability literature notable diÔ¨Äerences among concept begin interpretability refers passive characteristic model referring level given model make sense human observer feature also expressed transparency contrast explainability viewed active characteristic model noting any action procedure taken model intent clarifying detailing internal function summarize commonly used nomenclature section clarify distinction similarity among term often used ethical ai xai community understandability equivalently intelligibility denotes characteristic model make human understand function model work without any need explaining internal structure algorithmic mean model process data internally 18 comprehensibility conceived ml model bility refers ability learning algorithm represent learned knowledge human understandable fashion 84 barredo arrieta del ser et al information fusion 58 2020 notion model comprehensibility stem postulate michalski 22 stated result computer induction symbolic description given entity semantically turally similar human expert might produce observing entity component description comprehensible single chunk information directly interpretable natural language relate quantitative qualitative concept integrated fashion given diÔ¨Écult quantiÔ¨Åcation comprehensibility mally tied evaluation model complexity 17 interpretability deÔ¨Åned ability explain provide meaning understandable term human explainability explainability associated notion nation interface human decision maker time accurate proxy decision maker comprehensible human 17 transparency model considered transparent understandable since model feature diÔ¨Äerent degree understandability transparent model section 3 divided three category simulatable model decomposable model gorithmically transparent model 5 deÔ¨Ånitions understandability emerges essential concept xai transparency interpretability strongly tied concept transparency refers teristic model understandable human derstandability measure degree human understand decision made model comprehensibility also connected understandability relies capability audience understand knowledge contained model standability matter model understandability human understandability reason deÔ¨Ånition xai given section refers concept audience cognitive skill pursued goal user model taken count jointly intelligibility comprehensibility model use prominent role taken understandability make cept audience cornerstone xai next elaborate detail although might considered beyond scope per worth noting discussion held around general theory explanation realm philosophy 23 many proposal done regard suggesting need general uniÔ¨Åed theory approximates structure intent explanation however nobody ha stood critique presenting general theory time thought blend together ferent approach explanation drawn diverse knowledge plines similar problem found addressing interpretability ai appears literature not yet common point understanding interpretability explainability however many contribution claim achievement interpretable model technique empower explainability shed some light lack consensus might ing place reference starting point deÔ¨Ånition term explainable artiÔ¨Åcial intelligence xai given gunning 7 xai create suite machine learning technique enables man user understand appropriately trust eÔ¨Äectively manage emerging generation artiÔ¨Åcially intelligent partner deÔ¨Ånition brings together two concept understanding trust need addressed advance however miss sider purpose motivating need interpretable ai model causality transferability informativeness fairness dence later delve topic mentioning supporting example incompleteness tion exempliÔ¨Åed deÔ¨Ånition thorough complete nition explainability ai still slip Ô¨Ångers broader formulation deÔ¨Ånition explainable artiÔ¨Åcial intelligence one produce explanation functioning would fail fully characterize term question leaving aside important aspect purpose build upon completeness deÔ¨Ånition tion Ô¨Årst required extracted cambridge dictionary english language explanation detail reason someone give make thing clear easy understand 27 context ml model rephrased detail reason model give make functioning clear easy understand point opinion start diverge inherently stemming previous deÔ¨Ånitions two ambiguity pointed first detail reason used explain completely dependent audience presented second whether explanation ha left concept clear easy understand also depends completely audience therefore deÔ¨Ånition must rephrased reÔ¨Çect explicitly dependence explainability model audience end reworked deÔ¨Ånition could read given certain audience explainability refers detail reason model give make functioning clear easy understand since explaining argumenting may involve weighting comparing convincing audience formalization counter argument 28 explainability might convey u realm tive psychology psychology explanation 7 since measuring whether something ha understood put clearly hard task gauged objectively however measuring extent ternals model explained could tackled objectively any mean reduce complexity model simplify output considered xai approach big leap term complexity simplicity correspond explainable sulting model underlying problem remains unsolved interpretability gain provided xai approach may not straightforward quantify instance model simpliÔ¨Åcation evaluated based reduction number architectural ments number parameter model often made instance dnns contrary use visualization method natural language purpose doe not favor clear tiÔ¨Åcation improvement gained term interpretability derivation general metric ass quality xai approach main open challenge spotlight Ô¨Åeld forthcoming year discus research direction section 5 explainability linked explainability since cover technique used convert model able one remaining manuscript explainability considered main design objective since represents broader concept model explained interpretability model something come design model bearing observation mind explainable ai deÔ¨Åned follows given audience explainable artiÔ¨Åcial intelligence one duce detail reason make functioning clear easy stand deÔ¨Ånition posed Ô¨Årst contribution present overview implicitly assumes ease understanding clarity targeted xai technique model hand reverts diÔ¨Äerent application purpose better trustworthiness model output audience 85 barredo arrieta del ser et al information fusion 58 2020 table 1 goal pursued reviewed literature toward reaching explainability main target audience xai goal main target audience fig 2 reference trustworthiness domain expert user model affected decision causality domain expert manager executive board member regulatory transferability domain expert data scientist informativeness conÔ¨Ådence domain expert developer manager regulatory fairness user affected model decision regulatory accessibility product owner manager user affected model decision interactivity domain expert user affected model decision privacy awareness user affected model decision regulatory 89 stated introduction explainability one main er ai facing nowadays regard practical implementation inability explain fully understand reason ml algorithm perform well problem Ô¨Ånd root two diÔ¨Äerent cause conceptually illustrated fig 2 without doubt Ô¨Årst cause gap research munity business sector impeding full penetration newest ml model sector traditionally lagged behind tal transformation process banking Ô¨Ånances security health among many others general issue occurs strictly regulated sector some reluctance implement technique may put risk asset second axis knowledge ai ha helped research across world task inferring relation far beyond human cognitive reach every Ô¨Åeld dealing huge amount able data ha largely beneÔ¨Åted adoption ai ml niques however entering era result mance metric only interest shown research study though certain discipline might fair case science society far concerned performance search understanding open door model ment practical utility following section develops idea analyzing goal motivating search explainable ai model research activity around xai ha far exposed diÔ¨Äerent goal draw achievement explainable model almost none paper reviewed completely agrees goal required scribe explainable model compel however diÔ¨Äerent goal might help discriminate purpose given exercise ml explainability performed unfortunately scarce tributions attempted deÔ¨Åne goal conceptual spective synthesize enumerate deÔ¨Ånitions xai goal settle Ô¨Årst classiÔ¨Åcation criterion full suit paper covered review trustworthiness several author agree upon search thiness primary aim explainable ai model ever declaring model explainable per capability ducing trust might not fully compliant requirement model explainability trustworthiness might considered conÔ¨Ådence whether model act intended facing given problem although certainly property any explainable model doe not imply every thy model considered explainable worthiness property easy quantify trust might far only purpose explainable model since relation among two agreed upon not reciprocal part viewed paper mention concept trust stating pose achieving explainability however seen table 1 not amount large share recent contribution related xai causality another common goal explainability Ô¨Ånding causality among data variable several author argue able model might ease task Ô¨Ånding relationship occur could tested stronger causal link involved variable inference causal ship observational data Ô¨Åeld ha broadly studied time 161 widely acknowledged community ing topic causality requires wide frame prior knowledge prove observed eÔ¨Äects causal ml model only discovers correlation among data learns therefore might not suÔ¨Éce unveiling relationship however causation involves correlation explainable ml model could validate result provided causality inference technique provide Ô¨Årst intuition possible causal relationship within available data table 1 reveals causality not among tant goal attend amount paper state explicitly goal transferability model always bounded constraint allow seamless transferability main son approach used dealing ml problem explainability also advocate ability since may ease task elucidating boundary might aÔ¨Äect model allowing better understanding plementation similarly mere understanding inner tions taking place within model facilitates ability user reuse knowledge another problem case lack proper understanding model might drive user toward incorrect assumption fatal consequence transferability also fall resulting property explainable model not every transferable model considered explainable observed table 1 amount paper stating ability rendering model explainable better understand concept needed reuse improve performance second used reason pursuing model explainability informativeness ml model used ultimate intention supporting decision making 92 however not ten problem solved model not equal faced human counterpart hence great deal tion needed order able relate user decision solution given model avoid falling misconception pitfall purpose explainable ml model give mation problem tackled reason found among paper reviewed extracting information 86 barredo arrieta del ser et al information fusion 58 2020 fig conceptual diagram exemplifying diÔ¨Äerent level transparency characterizing ml model ùùã ùùã denoting parameter set model hand simulatability b decomposability c algorithmic transparency without loss generality example focus ml model explanation target however target explainability may include given example output class dataset inner relation model almost rule extraction technique substantiate approach search simpler ing model internally doe stating knowledge information expressed simpler proxy consider explaining antecedent used argument found among reviewed paper back expect reaching explainable model conÔ¨Ådence generalization robustness stability dence always assessed model reliability expected method maintain conÔ¨Ådence control diÔ¨Äerent depending model stated stability drawing interpretation certain model trustworthy interpretation not produced model not stable hence explainable model contain tion conÔ¨Ådence working regime fairness social standpoint explainability considered capacity reach guarantee fairness ml model certain literature strand explainable ml model suggests clear visualization relation aÔ¨Äecting result allowing fairness ethical analysis model hand likewise related objective xai highlighting bias data model wa exposed support algorithm model growing fast Ô¨Åelds involve human life hence explainability considered bridge avoid unfair unethical use rithm output accessibility minor subset reviewed contribution argues explainability property allows end user get involved process improving developing certain ml model seems clear explainable model ease burden felt user deal algorithm seem incomprehensible Ô¨Årst sight cept expressed third considered goal among veyed literature interactivity some contribution include ability model interactive user one goal targeted explainable ml model goal related Ô¨Åelds end user great importance ability tweak interact model ensures success privacy awareness almost forgotten reviewed literature one byproduct enabled explainability ml model ability ass privacy ml model may complex representation learned pattern not able understand ha captured model 4 stored internal representation may entail privacy breach contrarily ability explain inner relation trained model third party may also compromise diÔ¨Äerential privacy data origin due criticality sector xai foreseen play crucial role conÔ¨Ådentiality privacy issue covered section respectively subsection ha reviewed goal encountered among broad scope reviewed paper goal clearly face concept explainability introduced section round prior analysis concept explainability last subsection deal diÔ¨Äerent strategy followed community address explainability ml model literature make clear distinction among model pretable design explained mean nal xai technique duality could also regarded diÔ¨Äerence interpretable model model interpretability technique widely accepted classiÔ¨Åcation transparent model explainability duality also appears paper sented 17 distinction author make refers method solve transparent box design problem lem explaining problem work extends distinction made among transparent model including diÔ¨Äerent level transparency considered within transparency three level contemplated algorithmic transparency decomposability simulatability 1 among technique may distinguish among text explanation visualization local explanation explanation example explanation simpliÔ¨Åcation feature relevance context broader distinction posed 24 discerning 1 opaque system ping input output invisible user 2 interpretable tems user mathematically analyze mapping 3 comprehensible system model output symbol rule along speciÔ¨Åc output aid understanding process rationale behind mapping made last classiÔ¨Åcation criterion could considered included within one proposed earlier hence paper attempt following speciÔ¨Åc one level transparency machine learning model transparent model convey some degree interpretability self model belonging category also approached term domain interpretable namely mic transparency decomposability simulatability elaborate next connection fig 3 class contains cessors simulatable model time model decomposable algorithmically transparent simulatability denotes ability model simulated thought strictly human hence complexity take dominant place class said simple extensive large amount rule rule based system fall characteristic whereas single perceptron neural network fall within aspect aligns claim sparse linear model 1 alternative term simulability also used literature refer capacity system process simulated however note term doe not appear current english dictionary 87 barredo arrieta del ser et al information fusion 58 2020 interpretable dense one 170 pretable model one easily presented human mean text visualization 32 endowing able model simulatability requires model ha enough human think reason whole decomposability stand ability explain part model input parameter calculation considered telligibility stated 171 characteristic might empower ability understand interpret explain behavior model however occurs algorithmic transparency not every model fulÔ¨Åll property decomposability requires every input readily interpretable cumbersome feature not Ô¨Åt premise added constraint algorithmically transparent model become decomposable every part model must understandable human without need additional tool algorithmic transparency seen diÔ¨Äerent way deal ability user understand process followed model produce any given output input data put ently linear model deemed transparent error surface understood reasoned allowing user stand model act every situation may face 163 contrarily not possible understand deep architecture loss landscape might opaque since not fully observed solution ha approximated tic optimization stochastic gradient descent main constraint algorithmically transparent model model ha fully explorable mean mathematical analysis method explainability technique machine learning model explainability target model not readily pretable design resorting diverse mean enhance terpretability text explanation visual explanation local nation explanation example explanation simpliÔ¨Åcation feature relevance explanation technique technique cover one common way human explain system process along river actual technique better put actual group technique speciÔ¨Åed ease future work any researcher intends look speciÔ¨Åc technique suit knowledge not ending classiÔ¨Åcation also includes type data technique ha applied note many technique might suitable many diÔ¨Äerent type data although categorization only considers type used author proposed nique overall explainability technique divided Ô¨Årst intention author explanation technique explanation simpliÔ¨Åcation method utilized actual technique sitivity analysis Ô¨Ånally type data wa applied image text explanation deal problem bringing explainability model mean learning generate text explanation help explaining result model 169 text explanation also include every method generating symbol represent tioning model symbol may portrait rationale algorithm mean semantic mapping model bols visual explanation technique explainability aim sualizing model behavior many visualization method existing literature come along dimensionality reduction technique allow human interpretable simple tion visualization may coupled technique prove understanding considered suitable way introduce complex interaction within variable involved model user not acquainted ml modeling local explanation tackle explainability segmenting solution space giving explanation le complex solution subspace relevant whole model explanation formed mean technique diÔ¨Äerentiating property only explain part whole system functioning explanation example consider extraction data example relate result generated certain model enabling get better understanding model similarly human behave attempting explain given process explanation example mainly centered extracting representative ples grasp inner relationship correlation found model analyzed explanation simpliÔ¨Åcation collectively denote technique whole new system rebuilt based trained model explained new simpliÔ¨Åed model usually attempt ing resemblance antecedent functioning reducing complexity keeping similar performance score interesting byproduct family technique simpliÔ¨Åed model general easier implemented due reduced complexity respect model represents finally feature relevance explanation method ability clarify inner functioning model computing evance score managed variable score quantify aÔ¨Äection sensitivity feature ha upon output model comparison score among diÔ¨Äerent variable unveils importance granted model variable producing output feature relevance method thought indirect method explain model classiÔ¨Åcation portrayed graphically fig 4 used reviewing xai technique ml model following section table 2 ml model distinction proposition category presented order pose overall image Ô¨Åeld trend transparent machine learning model previous section introduced concept transparent model model considered transparent understandable model surveyed section suit transparent model fall one level model transparency described viously namely simulatability decomposability algorithmic parency follows provide reason statement graphical support given fig 5 regression logistic regression lr classiÔ¨Åcation model predict dent variable category dichotomous binary however dependent variable continuous linear regression would homonym model take assumption linear dependence tween predictor predicted variable impeding Ô¨Çexible Ô¨Åt data speciÔ¨Åc reason stiÔ¨Äness model one maintains model umbrella transparent method however stated section 2 explainability linked certain audience make model fall category depending interpret way logistic linear regression although clearly meeting characteristic transparent model algorithmic transparency decomposability simulatability may also demand explainability technique mainly visualization particularly model explained audience usage model ha largely applied within social ences quite long time ha pushed researcher create way explaining result model user author agree diÔ¨Äerent technique used analyze express ness lr including overall model evaluation statistical 88 barredo arrieta del ser et al information fusion 58 2020 fig conceptual diagram showing diÔ¨Äerent explainability approach available ml model ùùã fig graphical illustration level transparency diÔ¨Äerent ml model considered overview linear regression b decision tree c neighbor learner e generalized additive model f bayesian model test individual predictor statistic validation predicted probability overall model evaluation show provement applied model baseline showing fact improving model without prediction statistical signiÔ¨Åcance single predictor shown calculating wald statistic statistic show quality Ô¨Åtness model data signiÔ¨Åcant achieved resorting diÔ¨Äerent technique statistic validation predicted probability involves testing whether output model corresponds shown data technique show mathematical way representing Ô¨Åtness model behavior 89 barredo arrieta del ser et al information fusion 58 2020 table 2 overall picture classiÔ¨Åcation ml model attending level explainability model transparent ml model analysis simulatability decomposability algorithmic transparency regression predictor human readable interaction among kept minimum variable still readable number interaction predictor involved grown force decomposition variable interaction complex analyzed without mathematical tool not needed decision tree human simulate obtain prediction decision tree without requiring any mathematical background model comprises rule not alter data whatsoever preserve readability rule explain knowledge learned data allows direct understanding prediction process not needed neighbor complexity model number variable understandability similarity measure use match human naive capability simulation amount variable high similarity measure complex able simulate model completely similarity measure set variable decomposed analyzed separately similarity measure not decomposed number variable high user ha rely mathematical statistical tool analyze model not needed rule based learner variable included rule readable size rule set manageable human user without external help size rule set becomes large analyzed without decomposing small rule chunk rule become complicated rule set size ha grown much mathematical tool needed inspecting model behaviour not needed general additive model variable interaction among per smooth function involved model must constrained within human capability understanding interaction become complex simulated decomposition technique required analyzing model due complexity variable interaction not analyzed without application mathematical statistical tool not needed bayesian model statistical relationship modeled among variable variable directly understandable target audience statistical relationship involve many variable must decomposed marginals ease analysis statistical relationship not interpreted even already decomposed predictor complex model only analyzed mathematical tool not needed tree ensemble needed usually model simpliÔ¨Åcation feature relevance technique support vector machine needed usually model simpliÔ¨Åcation local explanation technique neural network needed usually model simpliÔ¨Åcation feature relevance visualization technique convolutional neural network needed usually feature relevance visualization technique recurrent neural network needed usually feature relevance technique technique discipline besides statistic adopted explaining regression model visualization niques powerful presenting statistical conclusion user not statistic instance work 178 show usage probability communicate result implied user able estimate outcome correctly 10 case opposed 46 case using natural cies although logistic regression among simplest classiÔ¨Åcation model supervised learning concept must taken care line reasoning author 179 unveil some concern interpretation derived lr Ô¨Årst mention gerous might interpret log odds ratio odd ratio tive eÔ¨Äects since also represent unobserved heterogeneity linked Ô¨Årst concern 179 also state comparison ratio across model diÔ¨Äerent variable might problematic since unobserved heterogeneity likely vary thereby invalidating comparison finally also mention comparison odds across diÔ¨Äerent sample group time also risky since tion heterogeneity not known across sample group time point last paper serf purpose visualizing problem model interpretation might entail even construction simple lr also interesting note model logistic ear regression maintain decomposability simulatability size must limited variable used must understandable user stated section 2 input model highly engineered feature complex diÔ¨Écult understand model hand far decomposable similarly model large human not think model whole simulatability put question decision tree decision tree another example model easily Ô¨Åll every constraint transparency decision tree hierarchical 90 barredo arrieta del ser et al information fusion 58 2020 structure decision making used support regression Ô¨Åcation problem simplest Ô¨Çavors decision tree simulatable model however property render decomposable algorithmically transparent decision tree always lingered diÔ¨Äerent gories transparent model utilization ha closely linked decision making context reason complexity understandability always considered paramount matter proof relevance found upsurge contribution literature dealing decision tree simpliÔ¨Åcation generation noted although capable Ô¨Åtting ery category within transparent model individual characteristic decision tree push toward category algorithmically transparent model simulatable decision tree one manageable human user mean size somewhat small amount feature meaning easily understandable increment size transforms model decomposable one since size impedes full evaluation simulation human finally increasing size using complex feature relation make model rithmically transparent loosing previous characteristic decision tree long used decision support context due transparency many application model fall Ô¨Åelds computation ai even information gy meaning expert Ô¨Åelds usually feel comfortable terpreting output model however poor generalization property comparison model make model family le interesting application scenario balance predictive performance design driver utmost portance tree ensemble aim overcoming poor performance aggregating prediction performed tree learned ent subset training data unfortunately combination sion tree loos every transparent property calling adoption explainability technique one reviewed later manuscript neighbor another method fall within transparent model nearest neighbor knn deal classiÔ¨Åcation problem methodologically simple way predicts class test sample voting class k nearest neighbor neighborhood relation induced measure distance sample used context regression problem voting replaced aggregation average target value associated nearest neighbor term model explainability important observe diction generated knn model rely notion distance similarity example tailored depending speciÔ¨Åc problem tackled interestingly prediction approach resembles human decision making cides upon result past similar case lie rationale knn ha also adopted widely context model interpretability requirement furthermore aside simple explain ability inspect reason new sample ha classiÔ¨Åed inside group examine prediction evolve number neighbor k increased decreased empowers interaction user model one must keep mind mentioned knn class transparency depends feature number neighbor distance function used measure similarity data instance high k impedes full simulation model performance human user similarly usage complex feature distance function would hinder decomposability model restricting interpretability solely transparency algorithmic operation learning learning refers every model generates rule characterize data intended learn rule take form simple conditional rule complex combination simple rule form knowledge also connected general family model fuzzy rule based system designed broader scope action allowing deÔ¨Ånition verbally formulated rule imprecise domain fuzzy system improve two main axis relevant paper first empower understandable model since operate linguistic term second perform better classic rule system context certain degree uncertainty rule based learner clearly transparent model often used plain complex model generating rule explain prediction rule learning approach extensively used knowledge representation expert system 192 however central problem rule generation approach coverage amount Ô¨Åcity length rule generated problem relates directly intention use Ô¨Årst place building rule database typical design goal sought user able analyze understand model amount rule model clearly prove performance model stake compromising intepretability similarly speciÔ¨Åcity rule play also interpretability since rule high number antecedent consequence might become diÔ¨Écult interpret line reasoning two feature rule based learner play along class transparent model presented section 2 greater coverage speciÔ¨Åcity closer model algorithmically transparent sometimes reason transition classical rule fuzzy rule relax constraint rule size since greater range covered le stress interpretability rule based learner great model term interpretability across Ô¨Åelds natural seamless relation human behaviour make suitable understand explain model certain threshold coverage acquired rule wrapper thought contain enough information model explain behavior user without forfeiting possibility using generated rule standalone prediction model general additive model statistic generalized additive model gam linear model value variable predicted given gation number unknown smooth function deÔ¨Åned tor variable purpose model infer smooth function whose aggregate composition approximates predicted variable structure easily interpretable since allows user verify importance variable namely aÔ¨Äects sponding function predicted output similarly every transparent model literature replete case study gam use specially Ô¨Åelds related risk assessment compared model able enough make user feel conÔ¨Ådent using practical application Ô¨Ånance environmental study 196 ogy 197 healthcare 44 biology energy 200 contribution use visualization method ease terpretation model gam might also considered simulatable decomposable model property mentioned deÔ¨Ånitions fulÔ¨Ålled extent depends roughly eventual iÔ¨Åcations baseline gam model introduction link function relate aggregation predicted output consideration interaction predictor application gam like one exempliÔ¨Åed share one common factor understandability main driver conducting study gam understand underlying relationship 91 barredo arrieta del ser et al information fusion 58 2020 build case scrutiny case research goal not accuracy sake rather need understanding problem behind relationship underneath variable involved data gam accepted certain community de facto modeling choice despite acknowledged forming behavior compared complex counterpart bayesian model bayesian model usually take form probabilistic directed acyclic graphical model whose link represent conditional dencies set variable example bayesian network could represent probabilistic relationship disease symptom given symptom network used compute probability presence various disease similar gam model also convey clear representation relationship feature target case given explicitly connection linking variable bayesian model fall ceiling transparent model categorization leaf simulatable decomposable algorithmically transparent however worth noting certain circumstance overly complex cumbersome variable model may loose Ô¨Årst two property bayesian model shown lead great insight assorted application cognitive modeling Ô¨Åshery gaming 204 climate 205 rics 206 robotics 207 furthermore also utilized explain model averaging tree ensemble 208 explainability technique machile learning model taxonomy shallow model deep learning ml model not meet any criterion imposed clare transparent separate method must devised applied model explain decision purpose explainability technique also referred ity aim communicating understandable information already developed model produce prediction any given input section categorize review diÔ¨Äerent algorithmic proaches explainability discriminating among 1 designed application ml model any kind 2 designed speciÔ¨Åc ml model thus not directly extrapolated any learner elaborate trend tiÔ¨Åed around explainability diÔ¨Äerent ml model illustrated fig 6 form hierarchical bibliographic category summarized next technique explainability section applied seamlessly any ml model disregarding inner processing internal representation explainability tailored speciÔ¨Åcally designed explain certain ml model divide literature analysis two main branch contribution dealing ability shallow ml model collectively refers ml model not hinge layered structure neural processing unit section technique devised deep learning model correspondingly denote family neural network lated variant convolutional neural network recurrent ral network section hybrid scheme encompassing deep neural network transparent model model perform thorough review latest method proposed search community along identiÔ¨Åcation trend followed contribution end literature analysis section present second taxonomy complement general one fig 6 classifying contribution dealing nation deep learning model end focus ular aspect related family ml method expose link classiÔ¨Åcation criterion used Ô¨Årst taxonomy technique explainability technique explainability designed plugged any model intent extracting some tion prediction procedure sometimes simpliÔ¨Åcation technique used generate proxy mimic antecedent pose something tractable reduced complexity time intent focus extracting knowledge directly el simply visualizing ease interpretation ior following taxonomy introduced section 2 technique may rely model simpliÔ¨Åcation feature relevance estimation visualization technique explanation simpliÔ¨Åcation arguably broadest nique category model agnostic method cal explanation also present within category since time simpliÔ¨Åed model only representative certain section model almost technique taking path model pliÔ¨Åcation based rule extraction technique among known contribution approach encounter technique local interpretable explanation lime 32 variation lime build locally linear model around prediction opaque model explain contribution fall explanation simpliÔ¨Åcation well local nation besides lime related Ô¨Çavors another approach rule extraction 212 although wa not originally intended extracting rule opaque model generic proposition ha extended also account model explainability purpose line rule extraction method work 215 present novel approach learn rule cnf tive normal form dnf disjunctive normal form bridge complex model model another tion fall oÔ¨Äthe branch 218 author formulate model simpliÔ¨Åcation model extraction process proximating transparent model complex one simpliÔ¨Åcation approached diÔ¨Äerent perspective 120 proach distill audit black box model presented two main idea exposed method model distillation parison audit risk scoring model statistical test check auditing data missing key feature wa trained popularity model simpliÔ¨Åcation evident given porally coincides recent literature xai including technique lime symptomatically reveals explainability approach envisaged continue ing central role xai feature relevance explanation technique aim describe ing opaque model ranking measuring inÔ¨Çuence vance importance feature ha prediction output model explained amalgam proposition found within category resorting diÔ¨Äerent algorithmic approach targeted goal one fruitful contribution path 224 called shap shapley additive explanation thor presented method calculate additive feature importance score particular prediction set desirable tie local accuracy missingness consistency antecedent lacked another approach tackle contribution feature prediction ha coalitional game theory 225 local gradient 234 similarly mean local gradient 230 test change needed feature produce change put model 228 author analyze relation dependency found model grouping feature bined bring insight data work 173 present broad variety measure tackle quantiÔ¨Åcation degree 92 barredo arrieta del ser et al information fusion 58 2020 fig taxonomy reviewed literature trend identiÔ¨Åed explainability technique related diÔ¨Äerent ml model reference boxed blue green red correspond xai technique using image text tabular data respectively order build taxonomy literature ha analyzed depth discriminate whether technique seamlessly applied any ml model even explicitly mention deep learning title abstract see reference interpretation reference colour Ô¨Ågure legend reader referred web version article 93 barredo arrieta del ser et al information fusion 58 2020 inÔ¨Çuence input output system qii quantitative input inÔ¨Çuence measure account correlated input suring inÔ¨Çuence contrast 222 author build upon existing sa sensitivity analysis construct global sa tends applicability existing method 227 image saliency method proposed applicable tiable image classiÔ¨Åers study 123 present tomatic structure identiÔ¨Åcation method astrid inspect attribute exploited classiÔ¨Åer generate prediction method Ô¨Ånds largest subset feature accuracy classiÔ¨Åer trained subset feature not guished term accuracy classiÔ¨Åer built original feature set 221 author use inÔ¨Çuence function trace model prediction back training data only requiring oracle version model access gradient vector product heuristic creating counterfactual example modifying input model also found contribute explainability compared attempting planation simpliÔ¨Åcation similar amount publication found tackling explainability mean feature relevance niques many contribution date 2017 some 2018 implying model simpliÔ¨Åcation technique feature relevance ha also become vibrant subject study current xai landscape visual explanation technique vehicle achieve explanation representative work area found 222 present portfolio visualization technique help explanation ml model built upon set extended technique mentioned earlier global sa another set visualization technique presented 223 author present three novel sa method data based sa sa based sa one novel input importance measure average lute deviation finally 238 present ice individual conditional expectation plot tool visualizing model estimated any supervised learning algorithm visual explanation le mon Ô¨Åeld technique ability since design method must ensure seamlessly applied any ml model disregarding inner structure creating visualization input output opaque model complex task almost ization method falling category work along feature evance technique provide information eventually displayed end user several trend emerge literature analysis begin rule extraction technique prevail contribution der umbrella explainability could itively expected bear mind wide use rule based learning explainability wrapper anticipated section complexity imposed not able get model similarly large group contribution deal feature relevance lately technique gathering much attention community dealing dl model hybrid approach utilize particular aspect class model therefore compromise dence feature relevance method model explained finally visualization technique propose interesting way ing output feature relevance technique ease task model interpretation contrast visualization technique aspect trained model structure operation etc tightly linked speciÔ¨Åc model explained explainability shallow ml model shallow ml cover diversity supervised learning model within model strictly interpretable transparent approach knn decision tree already discussed section 3 however shallow ml model rely sophisticated learning algorithm require additional layer explanation given prominence notable performance predictive task section concentrate two popular shallow ml model tree ensemble support vector machine svms require adoption explainability technique explaining decision tree ensemble random forest multiple classiÔ¨Åer system tree ensemble arguably among accurate ml el use nowadays advent came eÔ¨Écient mean prove generalization capability single decision tree usually prone overÔ¨Åtting circumvent issue tree ensemble combine diÔ¨Äerent tree obtain aggregated result eÔ¨Äective overÔ¨Åtting combination model make interpretation overall ensemble complex compounding tree learner forcing user draw explainability technique tree ensemble technique found literature explanation simpliÔ¨Åcation feature vance technique next examine recent advance technique begin many contribution presented simplify tree ensemble maintaining part accuracy accounted added complexity author 119 pose idea training single albeit le complex model set random sample data ideally following real data distribution labeled semble model another approach simpliÔ¨Åcation 118 author create simpliÔ¨Åed tree ensemble learner stel wise 122 present usage two model simple complex former one charge interpretation latter prediction mean divergence opposed wa seen technique not many technique board explainability tree ensemble mean model simpliÔ¨Åcation derives either posed technique good enough technique cover scope simpliÔ¨Åcation already following simpliÔ¨Åcation procedure feature relevance technique also used Ô¨Åeld tree ensemble breiman 286 wa Ô¨Årst analyze variable importance within random forest method based measuring mda mean decrease accuracy mie mean crease error forest certain variable randomly permuted sample following contribution 241 show real setting usage variable importance reÔ¨Çects lying relationship complex system modeled random forest finally crosswise technique among explainability 240 pose framework pose recommendation taken would convert example one class another idea attempt entangle variable importance way descriptive article author show method used elevate recommendation improve malicious online ad make rank higher paying rate similar trend shown technique tree ensemble simpliÔ¨Åcation feature relevance technique seem used scheme however contrarily wa observed paper date back 2017 place focus mostly bagging ensemble shifting focus towards ensemble strategy scarce activity ha recently noted around ability boosting stacking classiÔ¨Åers among latter worth highlighting connection reason compounding learner ensemble produce speciÔ¨Åc prediction given data contribution output ensemble ing auxiliary feature swaf approach proposed 242 point direction harnessing integrating explanation stacking ensemble improve generalization strategy allows not only relying output compounding learner also gin output consensus across entire ensemble interesting study explainability ensemble technique include scheme deepshap 226 put practice 94 barredo arrieta del ser et al information fusion 58 2020 stacking ensemble multiple classiÔ¨Åer system addition deep learning model combination explanation map multiple siÔ¨Åers produce improved explanation ensemble belong 243 recent insight dealing traditional gradient boosting ensemble support vector machine another shallow ml model historical presence literature svm svm model complex tree ensemble much opaquer structure many implementation ity technique proposed relate mathematically scribed internally model diÔ¨Äerent author considered explanation problem hand technically svm construct set high space used classiÔ¨Åcation regression task outlier detection intuitively good separation achieved hyperplane ha largest distance functional gin nearest point any class since general larger margin lower generalization error classiÔ¨Åer svms among used ml model due excellent diction generalization capability technique stated section 2 explainability applied svms cover explanation simpliÔ¨Åcation local explanation visualization explanation ample among explanation simpliÔ¨Åcation four class simpliÔ¨Åcations made diÔ¨Äerentiates deep go algorithm inner structure first some author propose niques build rule based model only support vector trained model approach 93 proposes method extract rule directly support vector trained svm ing modiÔ¨Åed sequential covering algorithm 57 author propose eclectic rule extraction still considering only support tor trained model work 94 generates fuzzy rule instead classical propositional rule author argue long tecedents reduce comprehensibility hence fuzzy approach allows linguistically understandable result second class cation exempliÔ¨Åed 98 proposed addition svm hyperplane along support vector component charge creating rule method relies creation rectangle intersection support vector third approach model simpliÔ¨Åcation another group author considered adding actual training data component building rule author proposed tering method group prototype vector class combining support vector allowed deÔ¨Åning ellipsoid rectangle input space similarly 106 author proposed rule extraction algorithm based svc support vector clustering Ô¨Ånd prototype vector class deÔ¨Åne small around 105 author formulate rule extraction problem tion create set rule rule conveys empty shared edge similar study conducted 245 extracting rule gene expression data author presented novel technique component svm method consists feature selection prediction modeling rule extraction finally study 134 make use growing svc give interpretation svm decision term linear rule deÔ¨Åne space voronoi section extracted prototype leaving aside rule extraction literature ha also contemplated some technique contribute interpretation svms three visualization technique clearly used toward plaining svm model used concrete application instance 77 present innovative approach visualize trained svm extract information content kernel matrix center study support vector regression model show ability rithm visualize input variable actually related associated output data 68 visual way combine output svm heatmaps guide modiÔ¨Åcation compound late stage drug discovery assign color atom based weight trained linear svm allows much hensive way debugging process 116 author argue many presented study interpreting svms only account weight vector leaving margin aside study show margin important create statistic itly account svm margin author show statistic speciÔ¨Åc enough explain multivariate pattern shown roimaging noteworthy also intersection svms bayesian tems latter adopted technique explain cisions made svm model case 248 247 study svms interpreted map maximum posteriori solution inference problem gaussian process framework make tuning sible give capability predicting class probability instead classical binary classiÔ¨Åcation svms interpretability svm model becomes even involved dealing ditional positive deÔ¨Ånite kernel usually harder interpret due missing geometrical theoretical understanding work 102 revolves around issue geometrical interpretation deÔ¨Ånite kernel svms showing not classify margin optimization instead minimize distance vex hull space diÔ¨Äerence might appreciated technique applied model noted svms previous model model simpliÔ¨Åcation broad sense wa prominent method hoc explainability svms local explanation started take some weight among proposition however simpliÔ¨Åcation based method average much older local explanation Ô¨Ånal remark none reviewed method treating svm plainability dated beyond 2017 might due sive proliferation dl model almost discipline another sible reason model already understood hard improve upon ha already done explainability deep learning local explanation feature relevance technique creasingly adopted method explaining dnns section review explainability study proposed used dl model namely neural network convolutional neural network cnn recurrent neural network rnn neural network inception neural network also known perceptrons warmly welcomed academic community due huge ability infer complex relation among variable however stated introduction developer neers charge deploying model production Ô¨Ånd questionable explainability common reason reluctance neural network always considered el fact explainability often must model practical value forced community generate multiple ity technique neural network including model Ô¨Åcation approach feature relevance estimator text explanation local explanation model visualization several model simpliÔ¨Åcation technique proposed ral network one single hidden layer however work presented neural network multiple hidden layer one work deepred algorithm 257 extends compositional approach rule extraction splitting neuron level 95 barredo arrieta del ser et al information fusion 58 2020 presented 259 neural network adding sion tree rule some work use model simpliÔ¨Åcation ity approach instance 56 present simple distillation method called interpretable mimic learning extract interpretable model mean gradient boosting tree direction author 135 propose hierarchical partitioning feature space veal iterative rejection unlikely class label association predicted addition several work addressed distillation edge ensemble model single model given fact simpliÔ¨Åcation neural network complex number layer increase explaining model feature relevance method ha become progressively popular one representative work area 60 present method decompose network classiÔ¨Åcation decision contribution input element consider neuron object decomposed expanded aggregate propagate decomposition network resulting deep taylor decomposition direction author 110 posed deeplift approach computing importance score neural network method compare activation neuron reference activation assigns score according diÔ¨Äerence hand some work try verify theoretical soundness current explainability method example author 262 bring fundamental problem feature relevance technique designed network showed two axiom technique ought fulÔ¨Åll namely sensitivity implementation variance violated practice approach following axiom author 262 created integrated gradient new feature relevance method proven meet aforementioned axiom similarly author 61 analyzed correctness current feature relevance explanation approach designed deep neural network e convnet guided backprop lrp simple linear neural network analysis showed method not produce cally correct explanation presented two new explanation method patternnet patternattribution theoretically sound simple deep neural network convolutional neural network currently cnns constitute model fundamental computer vision task image classiÔ¨Åcation object detection instance segmentation typically model built sequence convolutional layer pooling layer automatically learn ingly higher level feature end sequence one multiple fully connected layer used map output feature map score structure entail extremely complex internal relation diÔ¨Écult explain fortunately road explainability cnns easier type model human cognitive skill favor understanding visual data existing work aim understanding cnns learn divided two broad category 1 try understand decision process mapping back output input space see part input discriminative output 2 try delve inside network interpret intermediate layer see external world not necessarily related any speciÔ¨Åc put general one seminal work Ô¨Årst category wa 291 input image run cnn layer output number feature map strong soft activation author 291 used deconvnet network designed previously author 142 fed feature map selected layer reconstructs maximum activation reconstruction give idea part image produced eÔ¨Äect sualize strongest activation input image author used occlusion sensitivity method generate saliency map 136 consists iteratively forwarding image network occluding diÔ¨Äerent region time improve quality mapping input space eral subsequent paper proposed simplifying cnn architecture visualization method particular 96 included global erage pooling layer last convolutional layer cnn layer predicts object class simple architectural modiÔ¨Åcation cnn author built class activation map help identify image region ticularly important speciÔ¨Åc object class projecting back weight output layer convolutional feature map later 143 author showed layer used replace convolutional layer large stride without loss curacy several image recognition benchmark obtained cleaner visualization deconvnet using guided backpropagation method increase interpretability classical cnns author 113 used loss Ô¨Ålter high level convolutional layer force Ô¨Ålter learn speciÔ¨Åc object component obtained vation pattern much interpretable exclusiveness respect diÔ¨Äerent label predicted author 72 posed visualizing contribution prediction single pixel input image form heatmap used relevance propagation lrp technique relies taylor series close prediction point rather partial derivative diction point improve quality visualization attribution method heatmaps saliency map class activation method gradcam 292 used see fig 7 particular thor 292 proposed class activation mapping us gradient any target concept Ô¨Çowing Ô¨Ånal convolutional layer produce coarse localization map highlighting important region image predicting cept addition aforementioned feature relevance visual nation method some work proposed generating text explanation fig example rendering diÔ¨Äerent xai visualization technique image 96 barredo arrieta del ser et al information fusion 58 2020 fig feature visualization diÔ¨Äerent level certain network 293 fig example explanation using lime image 71 visual content image example author 91 combined cnn feature extractor rnn attention model automatically learn describe content image line 278 sented attention model perform cation task general model pipeline integrates three type attention object level attention model proposes candidate image region patch input image attention model Ô¨Ålters patch certain object last attention model localizes discriminative patch task video captioning author 111 use cnn model combined lstm model encoder extract video feature feed feature lstm decoder generate textual description one seminal work second category 137 der analyse visual information contained inside cnn author proposed general framework reconstruct image cnn internal representation showed several layer retain photographically accurate information image diÔ¨Äerent degree geometric photometric invariance visualize tion class captured cnn author created image maximizes class score based computing gradient class score respect input image 272 direction author 268 introduced deep generator network dgn generates representative image given output neuron cnn quantifying interpretability latent representation cnns author 125 used diÔ¨Äerent approach called network dissection run large number image cnn analyze top activated image considering unit concept detector evaluate unit semantic segmentation paper also examines eÔ¨Äects classical training technique interpretability learned model although many technique examined utilize local planation achieve overall explanation cnn model others plicitly focus building global explanation based locally found prototype author empirically showed local planation deep network strongly dominated lower level feature demonstrated deep architecture provide strong prevent altering representation captured visualization mixed feature relevance method arguably adopted approach explainability cnns instead using one single interpretability technique framework proposed 295 combine several method provide much formation network example combining feature tion neuron looking attribution doe aÔ¨Äect output allows exploring network decides label visual interpretability interface display diÔ¨Äerent block feature visualization attribution depending visualization goal interface thought union individual element long layer input hidden output atom neuron channel spatial neuron group content activation amount neuron Ô¨Åres tribution class spatial position contributes tends meaningful later layer presentation tion visualization feature visualization fig 8 show some example attribution method normally rely pixel association displaying part input example responsible network activating particular way 293 much simpler approach previously cited method wa proposed lime framework 71 wa described section lime perturbs input see prediction change image Ô¨Åcation lime creates set perturbed instance dividing input image interpretable component contiguous superpixels run perturbed instance model get probability ple linear model learns data set locally weighted end process lime present superpixels highest positive weight explanation see fig 9 completely diÔ¨Äerent explainability approach proposed sarial detection understand model failure detecting adversarial example author 264 apply neighbor algorithm representation data learned layer cnn test input image considered adversarial representation far representation training image recurrent neural network occurs cnns visual domain rnns lately used extensively predictive problem deÔ¨Åned inherently tial data notable presence natural language processing time series analysis type data exhibit dependency complex captured ml model rnns able trieve relationship formulating retention 97 barredo arrieta del ser et al information fusion 58 2020 fig pictorial representation hybrid model neural network considered explained ating interpretable model decision tree 306 fuzzy system 19 knn 264 knowledge neuron another parametric characteristic learned data contribution made explaining rnn model study divided two group 1 explainability derstanding rnn model ha learned mainly via feature relevance method 2 explainability modifying rnn architecture vide insight decision make local explanation Ô¨Årst group author 280 extend usage lrp rnns propose speciÔ¨Åc propagation rule work tiplicative connection lstms long short term memory unit grus gated recurrent unit author 281 propose visualization technique based Ô¨Ånite horizon inates interpretable cell within lstm gru network following premise not altering architecture 296 extends pretable mimic learning distillation method used cnn model lstm network interpretable feature learned Ô¨Åtting dient boosting tree trained lstm network focus aside approach not change inner working rnns 285 present retain reverse time attention model detects inÔ¨Çuential past pattern mean neural tention model create interpretable rnn author 283 pose rnn based sista sequential iterative algorithm model sequence correlated observation quence sparse latent vector making weight interpretable parameter principled statistical model finally 284 construct combination hmm hidden markov model rnn overall model approach harness interpretability hmm accuracy rnn model hybrid transparent method use background knowledge form logical statement constraint knowledge base kb ha shown not only improve explainability also performance respect purely approach positive side eÔ¨Äect shown hybrid approach provides robustness learning system error present training data label approach shown able jointly learn reason symbolic representation inference interesting aspect blend allows expressive reasoning fashion 300 successful use case dietary recommendation explanation extracted reasoning behind model 301 future data fusion approach may thus consider endowing dl model explainability externalizing domain information source deep formulation classical ml model ha done deep kalman Ô¨Ålters dkfs 302 deep variational bayes filter dvbfs 303 structural variational autoencoders svae 304 conditional random Ô¨Åelds rnns 305 approach provide deep model interpretability inherent probabilistic graphical model instance svae combine probabilistic graphical model embedding space neural network enhance ity dkfs particular example classical ml model enhanced dl counterpart deep nearest neighbor dknn 264 neighbor constitute explanation prediction intuition based rationalization dnn prediction based evidence evidence consists characterization conÔ¨Ådence termed credibility span hierarchy representation within dnn must supported training data 264 diÔ¨Äerent perspective hybrid xai model consists enriching model knowledge one transparent one posed 24 reÔ¨Åned 169 307 particular done constraining neural network thanks semantic kb concept 169 stacking ensemble jointly compassing model 307 example hybrid symbolic method tool enhances neural language 308 model reinforcement learning example symbolic graphical 311 relational hybrid model exist recommendation system instance explainable autoencoders proposed 313 speciÔ¨Åc transformer architecture symbolic visualization method applied music rially show attention work 314 visualizing reference last layer attention weight arc show note past informing future attention skip le relevant section transformer also help explain image caption sually 315 another hybrid approach consists mapping uninterpretable system twin interpretable ample opaque neural network combined transparent case based reasoning cbr system 318 dnn cbr case knn paired order improve ity keeping accuracy explanation example consists analyzing feature weight dnn used cbr order retrieve case explain dnn prediction fig 10 alternative taxonomy explainability technique deep learning dl model family research ha concentrated recent time become central recent erature xai division speciÔ¨Åc common distinction made community ha not only relied criterion classify xai method instance some method shap 224 widely used explain dl model several xai method easily categorized diÔ¨Äerent taxonomy branch depending angle method looked example lime also used cnns spite not exclusive deal image searching within ternative dl taxonomy show u lime explicitly used explaining deep network processing kind linear proxy model another type classiÔ¨Åcation indeed proposed 13 mentation based 3 category Ô¨Årst category group method plaining processing data network thus answering question doe particular input lead particular output second one concern method explaining representation data inside network answering question information doe network contain third approach concern model speciÔ¨Åcally designed simplify interpretation behavior multiplicity classiÔ¨Åcation possibility lead diÔ¨Äerent way structing xai taxonomy 98 barredo arrieta del ser et al information fusion 58 2020 fig 11 alternative deep learning speciÔ¨Åc taxonomy extended categorization 13 b connection taxonomy fig 6 see reference fig 11 show alternative deep learning taxonomy inferred 13 latter deduced complementarity lapping taxonomy fig 6 some method classiÔ¨Åed distinct category namely feature relevance cnn feature relevance rnn fig 6 included single category explanation deep network processing salience mapping considering classiÔ¨Åcation 13 some method classiÔ¨Åed single category tion simpliÔ¨Åcation neural network fig 6 2 diÔ¨Äerent category namely explanation deep network processing decision tree explanation deep network sentation role representation vector 13 shown fig 11 classiÔ¨Åcation based explanation model processing planation model representation relevant lead tiation execution trace model internal data structure mean depending failure reason complex model would possible right xai method according information needed execution trace data structure idea analogous testing debugging method used lar programming paradigm 346 xai opportunity challenge future research need capitalize performed literature review put forward critique achievement trend challenge still addressed Ô¨Åeld explainability ml data fusion model actually discussion advance taken far Ô¨Åeld ha already anticipated some challenge section revisit explore new research opportunity xai identifying sible research path followed address eÔ¨Äectively year come introducing overview section 1 already mentioned existence tradeoÔ¨Äbetween model interpretability mance sense making ml model understandable could eventually degrade quality produced decision section stress potential xai development eÔ¨Äectively achieve optimal balance interpretability performance ml model section stressed imperative need reaching sensus explainability entail within ai realm reason pursuing explainability also assorted assessment literature far not unambiguously mentioned throughout related work section delve important issue given notable prevalence xai literature section revolved explainability deep learning model ing advance reported far around speciÔ¨Åc bibliographic omy go direction section expose several challenge hold regard explainability family model finally close prospective discussion section place table several research niche despite connection model explainability remain insuÔ¨Éciently studied community delving identiÔ¨Åed challenge important bear mind prospective section complemented section 6 enumerates research need open question related xai within broader context need responsible ai tradeoÔ¨Äbetween interpretability performance matter interpretability versus performance one repeat time any big statement ha surroundings Ô¨Ålled myth misconception 99 barredo arrieta del ser et al information fusion 58 2020 fig model pretability performance tation area improvement potential xai technique tool resides perfectly stated 347 not necessarily true model complex inherently accurate statement false case data well structured feature disposal great quality value case somewhat common some industry environment since feature analyzed strained within controlled physical problem feature highly correlated not much possible landscape value explored data 348 hold true complex model enjoy much Ô¨Çexibility pler counterpart allowing complex function mated returning statement model complex accurate given premise function approximated entail certain complexity data available study greatly widespread among world suitable value variable enough data harness complex model statement present true statement situation oÔ¨Äbetween performance interpretability observed noted attempt solving problem not respect aforementioned premise fall trap attempting solve problem doe not provide enough data diversity variance hence added complexity model only Ô¨Åght task accurately solving problem path toward performance performance come hand hand complexity interpretability encounter ward slope appeared unavoidable however tion sophisticated method explainability could invert least cancel slope fig 12 show tentative representation spired previous work 7 xai show power improve common model interpretability performance another aspect worth mentioning point due close link model interpretability performance approximation dilemma explanation made ml model must made drastic mate enough match requirement audience sought ensuring explanation representative studied model not oversimplify essential feature concept metric literature clearly asks uniÔ¨Åed concept explainability order Ô¨Åeld thrive imperative place common ground upon community enabled contribute new technique method common concept must convey need expressed Ô¨Åeld propose common structure every xai system paper attempted new proposition concept explainability built upon gunning 7 proposition following stroke complete section explainability deÔ¨Åned ability model ha make functioning clearer audience address type method exist concept portrayed survey might not complete stand allows Ô¨Årst common ground reference point sustain proÔ¨Åtable discussion matter paramount Ô¨Åeld xai reach agreement respect combining shattered eÔ¨Äorts widespread Ô¨Åeld behind banner another key feature needed relate certain model crete concept existence metric metric group allow meaningful comparison well model Ô¨Åts deÔ¨Ånition explainable without tool any claim respect dilutes among literature not providing solid ground stand metric classic one accuracy sensitivity express well model performs certain aspect plainability some attempt done recently around surement xai reviewed thoroughly general xai measurement evaluate goodness usefulness satisfaction explanation improvement mental model audience induced model explanation impact explanation performance model trust reliance audience measurement technique surveyed 349 350 goodness checklist explanation satisfaction scale elicitation method tal model computational measure explainer Ô¨Ådelity explanation trustworthiness model reliability seem good push rection evaluating xai technique unfortunately conclusion drawn overview aligned prospect Ô¨Åeld quantiÔ¨Åable general xai metric really needed support ing measurement procedure tool proposed community survey doe not tackle problem designing suite metric since task approached community whole prior acceptance broader concept explainability hand one aim current work nevertheless advocate eÔ¨Äorts towards new proposal evaluate performance xai technique well comparison methodology among xai approach allow contrasting quantitatively diÔ¨Äerent application context model purpose challenge achieve explainable deep learning many eÔ¨Äorts currently made area xai still many challenge faced able obtain ability dl model first explained section lack 100 barredo arrieta del ser et al information fusion 58 2020 agreement vocabulary diÔ¨Äerent deÔ¨Ånitions surrounding xai example often see term feature importance feature relevance referring concept even obvious visualization method absolutely no consistency behind known saliency map salient mask heatmaps neuron vations attribution approach alike xai relatively young Ô¨Åeld community doe not standardized terminology yet ha commented section tween interpretability accuracy 13 simplicity information given system internal functioning exhaustiveness description whether observer expert Ô¨Åeld user without machine learning edge intelligibility doe not level order provide audience understanding 6 one reason mentioned challenge xai establishing objective metric constitutes good explanation possibility reduce subjectivity taking inspiration experiment human chology sociology cognitive science create objectively ing explanation relevant Ô¨Åndings considered creating explainable ai model highlighted 12 first explanation better constrictive meaning prerequisite good nation doe not only indicate model made decision x also made decision x rather decision also plained probability not important causal link order provide satisfying explanation considering black box model tend process data quantitative manner would necessary translate probabilistic result qualitative notion ing causal link addition state explanation selective meaning focusing solely main cause process suÔ¨Écient wa also shown use counterfactual explanation help user understand decision model combining connectionist symbolic paradigm seems favourable way address challenge one hand connectionist method precise opaque hand symbolic method popularly considered le eÔ¨Écient oÔ¨Äer greater explainability thus respecting condition mentioned ability refer established reasoning rule allows symbolic method constrictive use kb formalized ontology allow data processed directly qualitative way selective le straightforward connectionist model symbolic one recalling good explanation need inÔ¨Çuence mental model user representation external reality ing among thing symbol seems obvious use symbolic learning paradigm appropriate produce nation therefore interpretability could provide vincing explanation keeping improving generic performance 297 stated 24 truly explainable model not leave nation generation user diÔ¨Äerent explanation may deduced depending background knowledge semantic tation knowledge help model ability produce explanation natural language 169 combining common sense reasoning feature furthermore objective metric ha adopted appears necessary make eÔ¨Äort rigorously formalize evaluation method one way may drawing inspiration social science consistent choosing evaluation question lation sample used 354 Ô¨Ånal challenge xai method dl need address ing explanation accessible society policy maker law whole particular conveying explanation require expertise paramount handle tie develop social right available right explanation eu general data protection regulation gdpr 355 explanation ai security xai adversarial machine learning nothing ha said conÔ¨Ådentiality concern linked xai one last survey brieÔ¨Çy introduced idea algorithm property trade secret 14 however not much attention ha payed concept conÔ¨Ådential property make thing secret ai context many aspect involved model may hold property example imagine model some company ha developed many year research speciÔ¨Åc Ô¨Åeld knowledge synthesized model built might considered Ô¨Ådential may compromised even providing only input output access 356 latter show minimal assumption data model functionality stealing possible approach ha served make dl model robust intellectual property exposure based sequence non accessible query 357 recent work expose need research toward development xai tool capable explaining ml model keeping model conÔ¨Ådentiality mind ideally xai able explain knowledge within ai model able reason model act upon however information revealed xai technique used generate eÔ¨Äective attack adversarial context aimed fusing model time develop technique better protect private content exposure using information versarial attack 358 try manipulate ml algorithm learning speciÔ¨Åc information fed system lead speciÔ¨Åc output instance regarding supervised ml classiÔ¨Åcation model adversarial attack try discover minimum change applied input data order cause diÔ¨Äerent classiÔ¨Åcation ha happened regarding computer vision system autonomous vehicle minimal change stop signal perceptible human eye led vehicle detect 45 mph signal 359 particular case dl model available solution cleverhans 360 seek detect adversarial vulnerability provide diÔ¨Äerent approach harden model example include alfasvmlib 361 svm model adversariallib 362 evasion attack even available solution unsupervised ml like clustering algorithm 363 xai technique used furnish eÔ¨Äective ial attack reveal conÔ¨Ådential aspect model some cent contribution capitalized possibility generative versarial network gans 364 variational autoencoders 365 generative model towards explaining decision trained generative model generate instance learned based noise input vector interpreted tent representation data hand manipulating latent resentation examining impact output generative model possible draw insight discover speciÔ¨Åc pattern lated class predicted generative framework ha adopted several recent study mainly attribution method relate particular output deep learning model input variable another interesting research direction use erative model creation counterfactuals modiÔ¨Åcations input data could eventually alter original prediction model 368 counterfactual prototype help user understand performance boundary model consideration improved trust informed criticism light recent trend deÔ¨Ånitely believe road ahead generative ml model take part scenario demanding understandable machine sion 101 barredo arrieta del ser et al information fusion 58 2020 xai output conÔ¨Ådence safety issue also studied regard process pend output ai model vehicular perception driving autonomous vehicle automated surgery support medical diagnosis insurance risk assessment tems manufacturing among others 369 scenario roneous model output lead harmful consequence ha yielded comprehensive regulatory eÔ¨Äorts aimed ensuring no cision made solely basis data processing 3 parallel research ha conducted towards minimizing risk uncertainty harm derived decision made put ml model result many technique reported reduce risk among pause evaluation model output conÔ¨Ådence decide upon case inspection share epistemic uncertainty namely uncertainty due lack knowledge input data correspondence model output conÔ¨Ådence inform user eventually trigger jection model output end explaining via xai technique region input data model focused producing given output discriminate possible source epistemic uncertainty within input domain xai rationale explanation critical data study shifting focus research practice seen data ence ha noted reproducibility stringently subject not only mere sharing data model result community also availability information full discourse around data collection understanding assumption held insight drawn model construction result analysis 372 word order transform data valuable actionable asset individual must engage collaborative sharing context ducing Ô¨Åndings wherein context refers set narrative story around data processed cleaned modeled analyzed discourse Ô¨Ånd also interesting space adoption xai technique due powerful ability describe model understandable hence conveyable fashion towards colleague social science politics humanity legal Ô¨Åelds xai eÔ¨Äectively ease process explaining reason model reached decision accessible way user rationale explanation conÔ¨Çuence team project related data science search methodology make appraise ethical implication choice ha lately coined critical data study 373 Ô¨Åeld xai signiÔ¨Åcantly boost exchange information among heterogeneous audience knowledge learned model xai data science envision exciting synergy xai realm guided data science paradigm exposed 374 merges data science classic theoretical principle underlying data produced rationale behind rising paradigm need model generate knowledge prior knowledge brought Ô¨Åeld operates mean model type chosen according type relation intend encounter structure also follow previously known similarly training approach not allow optimization process enter region not plausible cordingly regularization term stand prior premise Ô¨Åeld avoiding elimination badly represented true relation spurious deceptive false relation finally output model inform everything model ha come learn allowing reason merge new knowledge wa already known Ô¨Åeld many example implementation approach currently available promising result study carried diverse Ô¨Åelds showcasing potential new paradigm data science relevant notice resemblance concept requirement data science share xai addition presented 374 push toward technique would eventually render model explainable furthermore edge consistent concept knowledge beginning central data science must also consider knowledge tured model explained assessing compliance theoretical principle known beforehand open cent window opportunity xai guideline ensuring interpretable ai model recent survey emphasized multidisciplinary inclusive nature process making model interpretable along process utmost importance scrutinize take proper account interest demand requirement stakeholder teracting system explained designer tem decision maker consuming produced output user undergoing consequence decision made therefrom given conÔ¨Çuence multiple criterion need human loop some attempt establishing procedural guideline implement explain ai system recently tributed among pause thorough study 383 suggests incorporation consideration explainability practical ai design deployment workÔ¨Çows comprise four jor methodological step contextual factor potential impact need must taken account devising approach pretability include thorough understanding purpose ai model built complexity explanation required audience performance ity level existing technology model method latter pose reference point ai system deployed lieu thereof interpretable technique preferred possible considering explainability development ai system decision xai approach chosen gauge risk need available data resource existing domain knowledge suitability ml model meet requirement computational task addressed conÔ¨Çuence three design driver guideline postulated 383 study line thinking 384 recommend Ô¨Årst consideration standard interpretable model rather sophisticated yet opaque modeling method practice aforementioned aspect contextual factor impact need make transparent model preferable complex modeling alternative whose interpretability require application xai technique contrast model reviewed work namely support tor machine ensemble method neural network selected only superior modeling capability Ô¨Åt best characteristic problem hand model ha chosen third guideline establishes impact weighed speciÔ¨Åcally responsibility design implementation ai system ensured checking whether identiÔ¨Åed impact mitigated counteracted supplementing system xai tool provide level explainability quired domain deployed end third guideline suggests 1 detailed articulation examination uation applicable explanatory strategy 2 analysis whether coverage scope available explanatory proaches match requirement domain application 102 barredo arrieta del ser et al information fusion 58 2020 context model deployed 3 formulation interpretability action plan set forth explanation livery strategy including detailed time frame execution plan clearance role responsibility team involved workÔ¨Çow finally fourth guideline encourages rethink interpretability term cognitive skill capacity limitation dividual human important question study measure explainability intensively revolving considering human mental model accessibility audience laries explanatory outcome mean involve pertise audience decision explanation provide foresee set guideline proposed 383 marized complemented enriched future methodological study ultimately heading responsible use ai methodological principle ensure purpose ability pursued met bringing manifold requirement participant process along universal aspect equal relevance no discrimination sustainability privacy ability challenge remains harnessing potential xai realize responsible ai discus next section toward responsible ai principle artiÔ¨Åcial intelligence fairness privacy data fusion year many organization private public published guideline indicate ai developed used guideline commonly referred ai principle tackle issue related potential ai threat individual society whole section present some important widely recognized principle order link xai normally appears inside principle responsible implementation use ai model sought practice Ô¨Årm claim xai doe not suÔ¨Éce important principle artiÔ¨Åcial intelligence privacy fairness must carefully dressed practice following section elaborate concept responsible ai along implication xai data fusion fulÔ¨Ållment postulated principle principle artiÔ¨Åcial intelligence recent review some main ai principle published since 2016 appears 385 work author show visual work diÔ¨Äerent organization classiÔ¨Åed according lowing parameter nature could private sector government governmental organization civil society multistakeholder content principle eight possible principle privacy explainability fairness also consider coverage document grant considered principle target audience principle aimed mally organization developed could also destined another audience see fig 2 whether not rooted international human right well whether explicitly talk instance 386 illustrative example document ai principle purpose overview since account some common principle deal explicitly explainability author propose Ô¨Åve principle mainly guide ment ai within company also indicating could also used within organization business author principle aim develop ai way directly reinforces inclusion give equal opportunity everyone contributes common good end following aspect considered output using ai system not lead any kind discrimination individual collective relation race religion gender sexual orientation disability ethnic origin any personal condition thus fundamental criterion consider optimizing result ai system not only output term error optimization also system deal group deÔ¨Ånes principle fair ai people always know communicating person communicating ai system ple also aware personal information used ai system purpose crucial ensure certain level understanding decision taken ai system achieved usage xai technique important generated explanation consider proÔ¨Åle user receive explanation ence per deÔ¨Ånition given section order adjust transparency level indicated 45 deÔ¨Ånes principle transparent explainable ai ai product service always aligned united nation sustainable development goal 387 contribute positive tangible way thus ai always erate beneÔ¨Åt humanity common good deÔ¨Ånes principle ai also referred ai social good 388 ai system specially fed data always sider privacy security standard life cycle principle not exclusive ai system since shared many software product thus inherited process already exist within company deÔ¨Ånes principle privacy security design wa also identiÔ¨Åed one core ethical societal challenge faced smart information system responsible research innovation paradigm rri 389 rri refers package methodological guideline recommendation aimed considering wider context entiÔ¨Åc research perspective lab global societal challenge sustainability public engagement ethic science education gender equality open access governance ingly rri also requires openness transparency ensured project embracing principle link directly ple transparent explainable ai mentioned previously author emphasize principle always extended any provider consultant partner going beyond scope Ô¨Åve ai principle european commission ec ha recently published ethical guideline worthy ai 390 assessment checklist completed diÔ¨Äerent proÔ¨Åles related ai system namely product manager developer role assessment based series ciples 1 human agency oversight 2 technical robustness safety 3 privacy data governance 4 transparency diversity discrimination fairness 5 societal environmental 6 accountability principle aligned one detailed section though scope ec principle general including any type organization involved development ai worth mentioning ai principle guide directly approach xai key aspect consider include ai system fact overview principle introduced 385 indicates 28 32 ai principle guide covered analysis itly include xai crucial component thus work scope article deal directly one important aspect regarding ai worldwide level 103 barredo arrieta del ser et al information fusion 58 2020 fairness accountability mentioned previous section many critical pects beyond xai included within diÔ¨Äerent ai principle guideline published last decade however aspect not pletely detached xai fact intertwined section present two key component huge relevance within ai ciples guide fairness accountability also highlight connected xai fairness discrimination identiÔ¨Åcation implicit correlation tected unprotected feature xai technique Ô¨Ånd place within data mining method analyzing output model behaves respect input feature model designer may unveil hidden correlation input ables amenable cause discrimination xai technique shap 224 could used generate counterfactual outcome explaining decision ml model fed protected unprotected ables recalling fair ai principle introduced previous section 386 reminds fairness discipline generally includes posals bias detection within datasets regarding sensitive data aÔ¨Äect protected group variable like gender race indeed ethical concern model arise tendency unintentionally create unfair decision considering sensitive factor individual race age gender 391 unfortunately unfair decision give rise discriminatory issue either itly considering sensitive attribute implicitly using factor correlate sensitive data fact attribute may implicitly encode protected factor occurs postal code credit rating 392 aforementioned proposal centered fairness aspect permit cover correlation variable sensitive one detect imbalanced outcome algorithm penalize ciÔ¨Åc subgroup people discrimination mitigate eÔ¨Äect bias model decision approach deal individual fairness fairness analyzed modeling ences subject rest population group fairness deal fairness perspective dividuals counterfactual fairness try interpret cause bias using example causal graph source bias indicated 392 traced skewed data bias within data acquisition process tainted data error data modelling deÔ¨Ånition wrong feature labelling possible cause limited feature using feature could lead inference false feature relationship lead bias sample size disparity using sensitive feature disparity tween diÔ¨Äerent subgroup induce bias proxy feature may correlated feature sensitive one induce bias even sensitive feature not present dataset next question asked criterion could used deÔ¨Åne ai not biased supervised ml 393 present framework us three criterion evaluate group fairness sensitive feature present within dataset independence criterion fulÔ¨Ålled model prediction independent sensitive feature thus proportion itive sample namely one belonging class interest given model subgroup within sitive feature separation met model prediction independent sensitive feature given target variable instance classiÔ¨Åcation model true positive tp rate false tive fp rate subgroup within sensitive feature criterion also known equalized odds suÔ¨Éciency accomplished target variable dent sensitive feature given model output thus tive predictive value subgroup within tive feature criterion also known predictive rate parity although not criterion fulÔ¨Ålled time optimized together order minimize bias within ml model two possible action could used order achieve criterion one hand evaluation includes measuring amount bias present within model regarding one criterion tioned many diÔ¨Äerent metric used depending criterion considered regarding independence criterion possible metric statistical parity diÔ¨Äerence disparate impact case separation criterion possible metric equal opportunity diÔ¨Äerence average odds diÔ¨Äerence 393 another possible metric theil index 394 measure inequality term individual group fairness hand mitigation refers process Ô¨Åxing some aspect model order remove eÔ¨Äect bias term one several sensitive feature several technique exist within literature classiÔ¨Åed following category group technique applied ml model trained looking remove bias Ô¨Årst step learning process example reweighing 395 iÔ¨Åes weight feature order remove discrimination sensitive attribute another example 396 hinge transforming input data order Ô¨Ånd good representation obfuscates information membership sensitive feature technique applied training ce ml model normally include fairness optimization constraint along cost function ml model example adversarial debiasing 397 technique optimizes jointly ability predicting target variable minimizing ability predicting sensitive feature using gan technique applied ml model trained le intrusive not modify input data ml model example equalized odds 393 technique allows adjust threshold classiÔ¨Åcation model order reduce diÔ¨Äerences tp rate fp rate sensitive subgroup even though reference apparently address ai principle appears independent xai literature show tertwined instance survey 385 evinces 26 28 ai principle deal xai also talk fairness explicitly fact elucidates organization usually consider aspect together implementing responsible ai literature also exploses xai proposal used bias detection example 398 proposes framework visually analyze bias present model individual group fairness thus fairness report shown like visual summary used within xai explainability approach eas understanding measurement bias system must report bias justify quantitatively indicate degree fairness explain user group would treated unfairly available data similarly xai technique shap 224 could used generate terfactual outcome explaining decision ml model fed protected unprotected variable identifying implicit lations protected unprotected feature xai niques model designer may unveil hidden correlation input variable amenable cause discrimination 104 barredo arrieta del ser et al information fusion 58 2020 another example 399 author propose design approach order develop ml model jointly le bias include explanation human comprehensible rule posal based locally generative model use only small part whole dataset available weak supervision Ô¨Årst Ô¨Ånds recursively relevant prototype within dataset extract empirical distribution density point around generates rule format explain data point classiÔ¨Åed within speciÔ¨Åc category similar some totypes proposal includes algorithm generates explanation reduces bias demonstrated use case recidivism using correctional oÔ¨Äender management proÔ¨Åling ternative sanction compas dataset 400 goal ha cently pursued 401 showing xai technique forge fairer explanation truly unfair model finally fai counterfactual explanation robustness transparency pretability fairness artiÔ¨Åcial intelligence model 402 us customized genetic algorithm generate counterfactuals help see robustness ml model generate explanation ine fairness individual level group level time strongly linked concept fairness much attention ha lately devoted concept data diversity essentially refers capability algorithmic model ensure diÔ¨Äerent type object represented output 403 therefore diversity thought indicator quality collection item taking form model output quantify ness model produce diverse result rather highly accurate prediction diversity come play application ethical restriction permeate ai modeling phase 404 likewise certain ai problem content recommendation mation retrieval also aim producing diverse recommendation rather yet similar result scenario secting internals model via xai technique help identifying capability model maintain input data versity output learning strategy endow model sity keeping capability could complemented xai technique order shed transparency model internals ass eÔ¨Äectiveness strategy respect diversity data model wa trained conversely xai could help criminate part model compromising overall ability preserve diversity accountability regarding accountability ec 390 deÔ¨Ånes following aspect consider auditability includes assessment algorithm data sign process preserving intellectual property related ai system performing assessment internal external auditor making report available could contribute trustworthiness technology ai system aÔ¨Äects mental right including application always audited external third party minimization reporting negative impact consists porting action decision yield certain outcome tem also comprises assessment outcome respond address development ai system also consider identiÔ¨Åcation assessment documentation minimization potential negative impact order minimize potential negative impact impact assessment carried prior development deployment use ai system also important guarantee protection anyone raise concern ai system blower assessment must proportionate risk ai system pose case any tension arises due implementation requirement could considered only ethically acceptable reasoned explicitly acknowledged documented must ated term risk ethical principle decision maker must accountable manner appropriate oÔ¨Äis made continually reviewed ensure appropriateness decision no ethically acceptable development deployment use ai system not proceed form redress includes mechanism ensure adequate redress situation unforeseen unjust adverse impact take place anteeing redress scenario key sure trust special attention paid vulnerable person group aspect addressed ec highlight diÔ¨Äerent connection xai accountability first xai contributes auditability help explaining ai system diÔ¨Äerent proÔ¨Åles including tory one also since connection fairness xai stated xai also contribute minimization report negative impact privacy data fusion number information source nowadays exist almost domain activity call data fusion approach aimed exploiting simultaneously toward solving learning task merging heterogeneous information data fusion ha proven improve performance ml model many application industrial prognosis 348 social system 407 internet thing 408 among others section speculates potential data fusion technique enrich explainability ml model compromise privacy data ml model learned end brieÔ¨Çy overview diÔ¨Äerent data sion paradigm later analyze perspective data privacy later despite relevance context sible ai conÔ¨Çuence xai data fusion uncharted research area current research mainstream basic level data fusion depart diÔ¨Äerent level data fusion identiÔ¨Åed comprehensive survey matter context subsection distinguish among fusion data level fusion model level fusion knowledge level furthermore allel categorization established depending data processed fused yielding centralized distributed method data fusion centralized approach node deliver locally tured data centralized processing system merge together contrast distributed approach node merges locally captured information eventually sharing result local fusion counterpart fusion information generation process ha property peculiarity depending level fusion formed data level fusion deal raw data schematically shown fig 13 fusion model stage receives raw data diÔ¨Äerent information source combine create coherent compliant robust simply representative data Ô¨Çow hand fusion model level aggregate model learned subset data set fused finally knowledge level fusion approach deal knowledge form rule ontology knowledge representation technique intention merging create new better complete knowledge wa originally provided structured knowledge information extracted data source every item data set using multiple knowledge extractor reasoning engine 105 barredo arrieta del ser et al information fusion 58 2020 fig diagram showing diÔ¨Äerent level data fusion performed data level b model level c knowledge level big data fusion e federated learning f multiview learning operating open semantic database produced information fused ensure quality correctness manageability produced knowledge item data set data fusion approach exist beyonds one represented fig 13 fusion performed either nique speciÔ¨Åcally devoted end depicted fig 13 stead performed along learning process ml model done dl model similarly data fusion made combining decision diÔ¨Äerent model done tree ensemble emerging data fusion approach next subsection examine data fusion approach recently come scene due implication term data privacy big data fusion fig 13 local model learned split original data source submitted worker node charge performing learning process map task reduce node several reduce node depending application combine output produced map task therefore big data fusion conceived mean distribute complexity ing ml model pool worker node wherein strategy design fused together map reduce task deÔ¨Ånes quality Ô¨Ånally generated outcome 413 contrast federated learning computation ml model made data captured locally remote client device fig 13 upon local model training client transmit encrypted formation learned knowledge central server take form gradient case neural ml model any content alike central server aggregate fuse knowledge contribution received client yield shared model harnessing collected tion pool client important observe no client data delivered central server elicits preserving nature federated learning furthermore computation set closer collected data reduces processing tency alleviates computational burden central server finally multiview learning 417 construct diÔ¨Äerent view object per information contained diÔ¨Äerent data source fig 13 view produced multiple source information diÔ¨Äerent feature subset 418 multiview learning devise strategy jointly optimize ml model learned aforementioned view enhance generalization formance specially application weak data supervision hence prone model overÔ¨Åtting joint optimization resort diÔ¨Äerent algorithmic mean 419 opportunity challenge privacy data fusion responsible ai paradigm ai system specially dealing multiple data source need explicitly include privacy consideration system life cle specially critical working personal data respecting people right privacy always addressed ec highlight privacy also address data governance covering quality integrity used data 390 also include deÔ¨Ånition access protocol capability process data way ensures privacy ec guide break privacy ciple three aspect privacy data protection guaranteed ai system throughout entire lifecycle includes information provided user information generated user derived interaction system since digital information user could used negative way discrimination due sensitive feature unfair treatment crucial ensure proper usage data collected quality integrity data quality data set fundamental reach good performance ai system fueled data like ml however sometimes data collected contains socially constructed bias inaccuracy error mistake 106 barredo arrieta del ser et al information fusion 58 2020 tackled training any model data collected tionally integrity data set ensured access data individual personal data way data protocol data governance protocol indicate may access data circumstance aforementioned example ec show data fusion directly intertwined privacy fairness regardless technique employed notwithstanding explicit concern regulatory body loss privacy ha compromised dl method scenario no data fusion performed instance image enough threaten user privacy even presence image obfuscation 420 model parameter dnn exposed simply ing input query model approach explain loss privacy using privacy loss intent loss subjective score former provides subjective measure severity privacy lation depending role face image latter tures intent bystander appear picture kind explanation motivated instance secure matching graphic protocol photographer bystander preserve privacy deÔ¨Ånite advocate eÔ¨Äorts invested direction namely ensuring xai method not pose threat regard privacy data used training ml model target data fusion enters picture diÔ¨Äerent implication arise context explainability covered survey begin sical technique fusion data level only deal data no connection ml model little ability however advent dl model ha blurred distinction information fusion predictive modeling Ô¨Årst layer dl architecture charge learning feature raw data posse relevance task hand learning process thought aim solving data level fusion problem yet directed learning fashion make fusion process tightly coupled task solved context many technique Ô¨Åeld xai posed deal analysis correlation feature pave way explaining data source actually fused dl model yield interesting insight tive task hand induces correlation among data source spatial time domain ultimately gained information fusion could not only improve usability model result enhanced understanding user could also help identifying data source potential interest could incorporated model even contribute eÔ¨Écient data fusion context unfortunately previously mentioned concept fusion data level contemplates data certain constraint known form source origin presented 423 big data era present vironment premise not taken granted method board big data fusion illustrated fig 13 thought conversely concern model fusion context emerges possibility xai technique could explanatory enough compromise conÔ¨Ådentiality private data could eventually occur sensitive information ownership could inferred explained fusion among protected unprotected feature turning prospect data fusion model level already argued fusion output several transparent el tree ensemble could make overall model opaque thereby making necessary resort explainability solution ever model fusion may entail drawback endowed powerful xai technique let u imagine relationship model input feature discovered mean technique one feature hidden unknown possible infer another model feature previous feature wa known used model would possibility uncover problem privacy breach case related protected input variable not even shared Ô¨Årst place get example clearer 424 multiview perspective lized diÔ¨Äerent single view representing source tend model fused model contain among others phone data transportation data etc might introduce lem information not even shared discovered source actually shared example instead feature model share another layer part chitecture federated learning would sharing make possible infer information exchanged part model extent allowing design adversarial attack better success rate upon antecedent model focused knowledge level fusion similar reasoning hold xai comprises technique extract knowledge ml model ability explain model could impact necessity discovering new knowledge complex interaction formed within ml model xai might enrich knowledge fusion paradigm bringing possibility discovering new knowledge extractor evance task hand purpose paramount portance knowledge extracted model mean xai technique understood extrapolated domain knowledge extractor operate concept match ease transfer learning portrayed 425 although xai not plated surveyed process extracting knowledge model trained certain feature space distribution utilized environment previous condition not hold deployed xai pose threat explanation given model reversely engineered knowledge fusion paradigm tually compromise instance diÔ¨Äerential privacy overall model distinction centralized distributed data fusion also spur challenge regard privacy explainability centralized approach doe not bring any concern presented however distributed fusion doe arise new problem distributed fusion might applied diÔ¨Äerent reason mainly due environmental constraint due security privacy issue latter context may indulge some danger among goal tional eÔ¨Éciency data fusion performed distributed fashion ensure no actual data actually shared rather part ml model trained local data rationale lie heart federated learning model exchange locally learned tion among node since data not leave local device only transmission model update required across distributed device lightens training process setting guarantee data privacy 416 upon use explainability technique node could disguise sensitive information cal context received ml model part wa trained fact wa shown model based dnn query interface given used accurately predict every single hyperparameter value used training allowing tial consequence relates study showing blurring image doe not guarantee privacy preservation data fusion privacy model explainability concept not analysed together far discussion clear unsolved concern caveat demand study community forthcoming time implementing responsible ai principle organization increasingly organization publishing ai principle declare care avoiding unintended negative quences much le experience actually implement principle organization looking several example 107 barredo arrieta del ser et al information fusion 58 2020 fig summary xai challenge discussed overview impact principle responsible ai principle declared diÔ¨Äerent organization 385 divide two group principle focus aspect speciÔ¨Åc ai explainability fairness human agency principle cover aspect involved ai including also privacy security safety ec guideline trustworthy ai example principle 390 telefonica large spanish ict company operating worldwide 386 example safety security relevant any connected system therefore also ai system hold privacy probably true privacy context ai system even important general system due fact ml model need huge amount data importantly xai tool data fusion technique pose new challenge preserve privacy protected record come implement ai principle organization important operationalize part time leverage process already existing generic principle indeed many organization already exist norm procedure privacy security safety implementing ai principle requires methodology presented 386 break ce diÔ¨Äerent part ingredient methodology include least ai principle already discussed earlier set value boundary awareness training potential issue technical questionnaire force people think certain impact ai system impact explanation questionnaire give concrete guidance certain undesired impact detected tool help answering some question help ing any problem identiÔ¨Åed xai tool fairness tool fall category well recent proposal model card 426 governance model assigning responsibility accountability responsibility explanation two philosophy nance 1 based committee review approve ai opments 2 based employee possible given fact agility key successful digital world seems wiser focus awareness employee responsibility only use committee speciÔ¨Åc important issue elaboration clear implementation responsible ai principle company balance two quirements 1 major cultural organizational change needed force principle process endowed ai functionality 2 feasibility compliance implementation principle asset policy resource already available company gradual process rising corporate awareness around principle value responsible ai envision xai make place create huge impact conclusion outlook overview ha revolved around explainable artiÔ¨Åcial gence xai ha identiÔ¨Åed recent time utmost need adoption ml method application study ha elaborated topic Ô¨Årst clarifying diÔ¨Äerent concept underlying model explainability well showing diverse purpose motivate search interpretable ml method tual remark served solid baseline systematic review cent literature dealing explainability ha approached two diÔ¨Äerent perspective 1 ml model feature some gree transparency thereby interpretable extent 2 xai technique devised make ml model terpretable literature analysis ha yielded global taxonomy diÔ¨Äerent proposal reported community classifying uniform criterion given prevalence contribution dealing explainability deep learning model inspected depth literature dealing family model giving rise tive taxonomy connects closely speciÔ¨Åc domain explainability realized deep learning model moved discussion beyond ha made far xai realm toward concept responsible ai paradigm imposes series ai principle met implementing ai model practice including fairness transparency privacy also discussed implication adopting xai technique context data fusion unveiling potential xai compromise privacy protected data involved fusion process implication xai fairness also discussed detail vision xai core concept ensure aforementioned principle responsible ai summarized graphically fig 14 reÔ¨Çections future xai conveyed sion held throughout work agree compelling need proper understanding potentiality caveat opened xai technique vision model interpretability must dressed jointly requirement constraint related data vacy model conÔ¨Ådentiality fairness accountability responsible implementation use ai method organization institution 108 barredo arrieta del ser et al information fusion 58 2020 worldwide only guaranteed ai principle studied jointly declaration competing interest author declare no known competing Ô¨Ånancial interest personal relationship could appeared inÔ¨Çuence work reported paper credit authorship contribution statement alejandro barredo arrieta conceptualization investigation ing original draft natalia conceptualization tigation writing review editing javier del ser conceptualization investigation writing original draft writing review editing ization project administration supervision adrien bennetot gation writing review editing siham tabik investigation writing review editing alberto barbado investigation writing review editing salvador garcia conceptualization writing review ing sergio conceptualization writing review editing daniel molina writing review editing richard benjamin ing review editing raja chatila supervision francisco herrera conceptualization investigation writing review editing sion acknowledgment alejandro javier del ser sergio would like thank basque government funding support received emaitek elkartek program javier del ser also acknowledges funding support consolidated research group mathmode granted department tion basque government siham tabik salvador garcia daniel molina francisco herrera would like thank spanish ment funding support project well bbva foundation ayudas fundaci√≥n bbva equipos de investigaci√≥n cient√≠Ô¨Åca 2018 call deepscop project work wa also funded part european union horizon 2020 research innovation programme grant agreement 825619 also thank chris olah alexander mordvintsev wig schubert borrowing image illustration purpose part overview inspired preliminary work concept ble ai benjamin barbado sierra responsible ai design appear proceeding ai trustworthiness ai model data hai track aaai fall symposium dc november 2019 386 reference 1 russell norvig artiÔ¨Åcial intelligence modern approach malaysia son education limited 2016 2 west future work robot ai automation brookings institution press 2018 3 goodman flaxman european union regulation algorithmic ing right explanation ai magazine 38 3 2017 4 castelvecchi open black box ai nature news 538 7623 2016 20 5 lipton mythos model interpretability queue 16 3 2018 6 preece harborne braines tomsett chakraborty stakeholder explainable ai 2018 7 gunning explainable artiÔ¨Åcial intelligence xai technical report defense vanced research project agency darpa 2017 8 tjoa guan survey explainable artiÔ¨Åcial intelligence xai towards medical xai 2019 9 zhu liapis risi bidarra youngblood explainable ai designer perspective 2018 ieee conference computational intelligence game cig 2018 doi 10 ƒá brc ƒá hlupi ƒá explainable artiÔ¨Åcial intelligence survey international convention information communication technology electronics microelectronics mipro 2018 pp 11 hall art science machine learning explanation 2018 12 miller explanation artiÔ¨Åcial intelligence insight social science artif intell 267 2019 13 gilpin bau yuan bajwa specter kagal explaining tions overview interpretability machine learning 2018 14 adadi berrada peeking inside survey explainable Ô¨Åcial intelligence xai ieee access 6 2018 15 biran cotton explanation justiÔ¨Åcation machine learning survey workshop explainable ai xai 8 2017 1 16 shane mueller hoÔ¨Äman clancey klein explanation system literature synopsis key idea publication bibliography explainable ai technical report defense advanced research project agency darpa xai program 2019 17 guidotti monreale ruggieri turini giannotti pedreschi survey method explaining black box model acm computing survey 51 5 2018 18 montavon samek m√ºller method interpreting standing deep neural network digital signal processing 73 2018 doi 19 fernandez herrera cordon jose del jesus marcelloni ary fuzzy system explainable artiÔ¨Åcial intelligence ieee computational intelligence magazine 14 1 2019 20 gleicher framework considering comprehensibility modeling big data 4 2 2016 21 craven extracting comprehensible model trained neural network technical report university department computer ences 1996 22 michalski theory methodology inductive learning machine ing springer 1983 pp 23 d√≠ez khalifa leuridan general theory explanation buyer beware synthese 190 3 2013 24 doran schulz besold doe explainable ai really mean new conceptualization perspective 2017 25 kim towards rigorous science interpretable machine ing 2017 26 vellido lisboa making machine learning el european symposium artiÔ¨Åcial neural network putational intelligence machine learning esann 12 citeseer 2012 pp 27 walter cambridge advanced learner dictionary cambridge university press 2008 28 besnard hunter element argumentation mit press 2008 29 rossi ai ethic enterprise ai 2019 30 holzinger biemann pattichis kell need build plainable ai system medical domain 2017 31 kim glassman johnson shah ibcm interactive bayesian case model empowering human via intuitive interaction technical report 2015 32 ribeiro singh guestrin trust explaining diction any classiÔ¨Åer acm sigkdd international conference knowledge discovery data mining acm 2016 pp 33 fox long magazzeni explainable planning 2017 34 lane core van lent solomon gomboc explainable artiÔ¨Åcial intelligence training tutoring technical report university southern california 2005 35 murdoch singh kumbier yu interpretable machine learning deÔ¨Ånitions method application 2019 36 haspiel du meyerson robert jr tilbury yang pradhan explanation expectation trust building automated vehicle ion international conference interaction acm 2018 pp 37 chander srinivasan chelian wang uchino working belief ai transparency workshop acm conference intelligent user interface 2018 38 tickle andrew golea diederich truth come light rections challenge extracting knowledge embedded within trained Ô¨Åcial neural network ieee transaction neural network 9 6 1998 1068 39 louizos shalit mooij sontag zemel welling causal eÔ¨Äect inference deep model advance neural information processing system 2017 pp 40 goudet kalainathan caillou guyon sebag ing functional causal model generative neural network explainable interpretable model computer vision machine learning springer 2018 pp 41 athey imbens machine learning method estimating heterogeneous causal eÔ¨Äects stat 1050 5 2015 42 nishihara chintala scholkopf bottou discovering causal signal image proceeding ieee conference computer vision pattern recognition 2017 pp 43 barabas dinakar ito virza zittrain intervention prediction reframing ethical debate actuarial risk assessment 2017 44 caruana lou gehrke koch sturm elhadad intelligible el healthcare predicting pneumonia risk hospital readmission proceeding acm sigkdd international conference knowledge covery data mining kdd 15 2015 pp 109 barredo arrieta del ser et al information fusion 58 2020 45 theodorou wortham bryson designing implementing parency real time inspection autonomous robot connection science 29 3 2017 46 samek wiegand m√ºller explainable artiÔ¨Åcial intelligence standing visualizing interpreting deep learning model 2017 47 wadsworth vera piech achieving fairness adversarial learning application recidivism prediction 2018 48 yuan zhu li adversarial example attack defense deep learning ieee transaction neural network learning system 30 9 2019 49 letham rudin mccormick madigan et al interpretable classiÔ¨Åers using rule bayesian analysis building better stroke prediction model annals applied statistic 9 3 2015 50 harbers van den bosch meyer design evaluation explainable bdi agent international conference web intelligence intelligent agent technology 2 ieee 2010 pp 51 aung lisboa etchells testa van calster van huÔ¨Äel valentin timmerman comparing analytical decision support model boolean rule extraction case study ovarian tumour malignancy tional symposium neural network springer 2007 pp 52 weller challenge transparency 2017 53 freitas comprehensible classiÔ¨Åcation model position paper acm sigkdd exploration newsletter 15 1 2014 54 schetinin fieldsend partridge coat krzanowski son bailey hernandez conÔ¨Ådent interpretation bayesian decision tree ensemble clinical application ieee transaction information technology biomedicine 11 3 2007 55 marten vanthienen verbeke baesens performance classiÔ¨Åcation model user perspective decision support system 51 4 2011 793 56 che purushotham khemani liu interpretable deep model icu outcome prediction amia annual symposium proceeding 2016 american medical informatics association 2016 371 57 barakat diederich eclectic support vector machine ternational journal computer electrical automation control information engineering 2 5 2008 58 garcia robb liu laskov patron hastie explain natural language interface scrutable autonomous robot 2018 59 langley meadow sridharan choi explainable agency gent autonomous system aaai conference artiÔ¨Åcial intelligence 2017 pp 60 montavon lapuschkin binder samek m√ºller explaining linear classiÔ¨Åcation decision deep taylor decomposition pattern recognition 65 2017 61 kindermans sch√ºtt alber m√ºller erhan kim d√§hne learning explain neural network patternnet patternattribution 2017 62 ra van gerven haselager explanation method deep learning user value concern challenge explainable interpretable model puter vision machine learning springer 2018 pp 63 bach binder m√ºller samek controlling explanatory heatmap olution semantics via decomposition depth ieee international conference image processing icip ieee 2016 pp 64 katuwal chen machine learning model interpretability precision medicine 2016 65 neerincx van der waa kaptein van diggelen using perceptual cognitive explanation enhanced team performance tional conference engineering psychology cognitive ergonomics springer 2018 pp 66 olden jackson illuminating black box randomization approach understanding variable contribution artiÔ¨Åcial neural network ecological modelling 154 2002 67 krause perer ng interacting prediction visual inspection machine learning model chi conference human factor computing system acm 2016 pp 68 rosenbaum hinselmann jahn zell interpreting linear support vector machine model heat map molecule coloring journal cheminformatics 3 1 2011 11 69 tan ung cheng greene unsupervised feature construction knowledge extraction assay breast cancer denoising autoencoders paciÔ¨Åc symposium biocomputing world scientiÔ¨Åc 2014 pp 70 krening harrison feigh isabell riedl thomaz learning explanation using sentiment advice rl ieee transaction cognitive developmental system 9 1 2017 71 ribeiro singh guestrin interpretability machine learning 2016 72 bach binder montavon klauschen m√ºller samek wise explanation classiÔ¨Åer decision relevance agation plo one 10 7 2015 73 etchells lisboa orthogonal rule extraction osre trained neural network practical eÔ¨Écient approach ieee transaction neural network 17 2 2006 74 zhang sreedharan kulkarni chakraborti zhuo kambhampati plan explicability predictability robot task planning 2017 ieee national conference robotics automation icra ieee 2017 pp 1320 75 santoro raposo barrett malinowski pascanu battaglia licrap simple neural network module relational reasoning advance neural information processing system 2017 pp 76 peng stage john use interpretation logistic regression higher education journal research higher education 43 3 2002 77 √ºst√ºn melssen buydens visualisation interpretation support vector regression model analytica chimica acta 595 2007 78 zhang yang wu interpreting cnns via decision tree ieee conference computer vision pattern recognition 2019 pp 79 wu hughes parbhoo zazzi roth beyond sity tree regularization deep model interpretability aaai conference artiÔ¨Åcial intelligence 2018 pp 80 hinton vinyals dean distilling knowledge neural network 2015 81 frosst hinton distilling neural network soft decision tree 2017 82 augasta kathirvalavakumar reverse engineering neural network rule extraction classiÔ¨Åcation problem neural processing letter 35 2 2012 83 zhou jiang chen extracting symbolic rule trained neural network ensemble ai communication 16 1 2003 84 tan hooker well tree space prototype another look making tree ensemble interpretable 2016 85 fong vedaldi interpretable explanation black box ful perturbation ieee international conference computer vision 2017 pp 86 miller howe sonenberg explainable ai beware inmate running asylum international joint conference artiÔ¨Åcial intelligence workshop explainable ai xai 36 2017 pp 87 goebel chander holzinger lecue akata stumpf berg holzinger explainable ai new 42 international conference machine learning knowledge extraction springer 2018 pp 88 belle logic meet probability towards explainable ai system tain world international joint conference artiÔ¨Åcial intelligence 2017 pp 89 edward veale slave algorithm right explanation probably not remedy looking duke tech rev 16 2017 18 90 lou caruana gehrke hooker accurate intelligible model wise interaction acm sigkdd international conference knowledge covery data mining acm 2013 pp 91 xu ba kiros cho courville salakhudinov zemel bengio show attend tell neural image caption generation visual attention international conference machine learning 2015 pp 92 huysmans dejaeger mues vanthienen baesens empirical uation comprehensibility decision table tree rule based predictive model decision support system 51 1 2011 93 barakat bradley rule extraction support vector machine quential covering approach ieee transaction knowledge data ing 19 6 2007 94 adriana da costa vellasco tanscheit fuzzy rule extraction support vector machine international conference hybrid intelligent tems ieee 2005 pp 95 marten baesens van gestel vanthienen comprehensible credit scoring model using rule extraction support vector machine european journal operational research 183 3 2007 96 zhou khosla lapedriza oliva torralba learning deep feature discriminative localization ieee conference computer vision pattern recognition 2016 pp 97 krishnan sivakumar bhattacharya extracting decision tree trained neural network pattern recognition 32 12 1999 98 fu ong keerthi hung goh extracting knowledge ded support vector machine ieee international joint conference neural network 1 ieee 2004 pp 99 green fair risk assessment precarious approach criminal justice form workshop fairness accountability transparency machine learning 2018 100 chouldechova fair prediction disparate impact study bias vism prediction instrument big data 5 2 2017 101 kim reingold rothblum fairness awareness advance neural information processing system 2018 pp 102 haasdonk feature space interpretation svms indeÔ¨Ånite kernel ieee transaction pattern analysis machine intelligence 27 4 2005 103 palczewska palczewski robinson neagu interpreting random est classiÔ¨Åcation model using feature contribution method integration reusable system springer 2014 pp 104 welling refsgaard brockhoÔ¨Ä clemmensen forest Ô¨Çoor alizations random forest 2016 105 fung sandilya rao rule extraction linear support vector machine acm sigkdd international conference knowledge discovery data mining acm 2005 pp 106 zhang su jia chu rule extraction trained support vector chine conference knowledge discovery data mining springer 2005 pp 107 linsley shiebler eberhardt serre attention network visual recognition 2018 110 barredo arrieta del ser et al information fusion 58 2020 108 zhou gan interpretability interpretability uniÔ¨Åed view interpretable fuzzy system modelling fuzzy set system 159 23 2008 109 burrell machine think understanding opacity machine learning algorithm big data society 3 1 2016 110 shrikumar greenside shcherbina kundaje not black box ing important feature propagating activation diÔ¨Äerences 2016 111 dong su zhu zhang improving interpretability deep neural network semantic information ieee conference computer vision pattern recognition 2017 pp 112 ridgeway madigan richardson kane interpretable boosted na√Øve bayes acm sigkdd conference knowledge discovery data mining 1998 pp 113 zhang nian wu zhu interpretable convolutional neural network proceeding ieee conference computer vision pattern recognition 2018 pp 114 seo huang yang liu interpretable convolutional neural network dual local global attention review rating prediction proceeding eleventh acm conference recommender system acm 2017 pp 305 115 larsen petersen endahl interpreting parameter logistic regression model random eÔ¨Äects biometrics 56 3 2000 116 gaonkar shinohara davatzikos initiative et al interpreting port vector machine model multivariate group wise analysis neuroimaging medical image analysis 24 1 2015 117 xu park yi sutton interpreting deep classiÔ¨Åer visual distillation dark knowledge 2018 118 deng interpreting tree ensemble intrees 2014 119 domingo knowledge discovery via multiple model intelligent data analysis 2 1998 120 tan caruana hooker lou auditing model using transparent model distillation conference ai ethic society acm 2018 pp 121 berk bleich statistical procedure forecasting criminal behavior parative assessment criminology public policy 12 3 2013 122 hara hayashi making tree ensemble interpretable 2016 123 henelius puolam√§ki ukkonen interpreting classiÔ¨Åers attribute interaction datasets 2017 124 hastie garcia robb patron laskov miriam multimodal interface autonomous system acm international conference multimodal interaction acm 2017 pp 125 bau zhou khosla oliva torralba network dissection quantifying interpretability deep visual representation proceeding ieee ence computer vision pattern recognition 2017 pp 126 n√∫√±ez angulo catal√† rule extraction support vector european symposium artiÔ¨Åcial neural network computational intelligence machine learning esann 2002 pp 127 n√∫√±ez angulo catal√† learning system support vector machine neural processing letter 24 1 2006 128 kearns neel roth wu preventing fairness gerrymandering auditing learning subgroup fairness 2017 129 akyol langbort basar price transparency strategic machine learning 2016 130 erhan courville bengio understanding representation learned deep architecture department dinformatique et recherche operationnelle university montreal qc canada tech rep 1355 2010 1 131 zhang wallace sensitivity analysis practitioner guide lutional neural network sentence classiÔ¨Åcation 2015 132 quinlan simplifying decision tree international journal study 27 3 1987 133 zhou hooker interpreting model via single tree approximation 2016 134 support vector machine interpretation neurocomputing 69 2006 135 thiagarajan kailkhura sattigeri ramamurthy treeview peeking deep neural network via partitioning 2016 136 zeiler fergus visualizing understanding convolutional network european conference computer vision springer 2014 pp 137 mahendran vedaldi understanding deep image representation ing proceeding ieee conference computer vision pattern recognition 2015 pp 138 wagner kohler gindele hetzel wiedemer behnke pretable visual explanation convolutional neural network proceeding ieee conference computer vision pattern tion 2019 pp 139 kanehira harada learning explain complemental example ceedings ieee conference computer vision pattern recognition 2019 pp 140 apley visualizing eÔ¨Äects predictor variable black box supervised learning model 2016 141 staniak biecek explanation model prediction live breakdown package r journal 10 2 2018 142 zeiler krishnan taylor fergus deconvolutional cvpr 10 2010 7 143 springenberg dosovitskiy brox riedmiller striving simplicity convolutional net 2014 144 kim wattenberg gilmer cai wexler viegas sayres pretability beyond feature attribution quantitative testing concept activation vector tcav 2017 145 polino pascanu alistarh model compression via distillation zation 2018 146 murdoch szlam automatic rule extraction long short term memory network 2017 147 craven shavlik using sampling query extract rule trained neural network machine learning proceeding 1994 elsevier 1994 pp 148 arbatli akin rule extraction trained neural network using genetic algorithm nonlinear analysis theory method application 30 3 1997 149 johansson niklasson evolving decision tree using oracle guide 2009 ieee symposium computational intelligence data mining ieee 2009 pp 150 lei barzilay jaakkola rationalizing neural prediction 2016 151 radford jozefowicz sutskever learning generate review ering sentiment 2017 152 selvaraju da vedantam cogswell parikh batra say 2016 153 tishby opening black box deep neural network via information 2017 154 yosinski clune nguyen fuchs lipson understanding neural work deep visualization 2015 155 pope kolouri rostami martin hoÔ¨Ämann explainability od graph convolutional neural network proceeding ieee ence computer vision pattern recognition 2019 pp 156 gajane pechenizkiy formalizing fairness prediction machine learning 2017 157 dwork ilvento composition fairsystems 2018 158 barocas hardt narayanan fairness machine learning 2019 159 wang fratiglioni frisoni viitanen winblad smoking occurence alzheimer disease longitudinal data study american journal epidemiology 149 7 1999 160 rani liu sarkar vanman empirical study machine learning technique aÔ¨Äect recognition interaction pattern analysis application 9 1 2006 161 pearl causality cambridge university press 2009 162 kuhn johnson applied predictive modeling 26 springer 2013 163 james witten hastie tibshirani introduction statistical learning 112 springer 2013 164 szegedy zaremba sutskever bruna erhan goodfellow fergus intriguing property neural network 2013 165 ruppert robust statistic approach based inÔ¨Çuence function taylor francis 1987 166 basu kumbier brown yu iterative random forest discover dictive stable interaction proceeding national academy science 115 8 2018 167 yu et al stability bernoulli 19 4 2013 168 burn hendricks saenko darrell rohrbach woman also board overcoming bias captioning model 2018 169 bennetot laurent chatila towards explainable visual reasoning nesy workshop ijcai 2019 macau china 2019 170 tibshirani regression shrinkage selection via lasso journal royal statistical society series b methodological 58 1 1996 171 lou caruana gehrke intelligible model classiÔ¨Åcation regression acm sigkdd international conference knowledge discovery data ing acm 2012 pp 172 kawaguchi deep learning without poor local minimum advance neural information processing system 2016 pp 173 datta sen zick algorithmic transparency via quantitative input ence theory experiment learning system 2016 ieee symposium security privacy sp ieee 2016 pp 174 bursac gauss williams hosmer purposeful selection ables logistic regression source code biology medicine 3 1 2008 17 175 jaccard interaction eÔ¨Äects logistic regression quantitative application social science sage thousand oak ca 2001 176 hosmer jr lemeshow sturdivant applied logistic regression 398 john wiley son 2013 177 peng lee ingersoll introduction logistic regression analysis reporting journal educational research 96 1 2002 178 hoÔ¨Ärage gigerenzer using natural frequency improve diagnostic ences academic medicine 73 5 1998 179 mood logistic regression not think european sociological review 26 1 2010 180 laurent rivest constructing optimal binary decision tree information processing letter 5 1 1976 181 utgoÔ¨Ä incremental induction decision tree machine learning 4 2 1989 182 quinlan induction decision tree machine learning 1 1 1986 183 rokach maimon data mining decision tree theory application 69 world scientiÔ¨Åc 2014 184 rovnyak kretsinger thorp brown decision tree transient stability prediction ieee transaction power system 9 3 1994 111 barredo arrieta del ser et al information fusion 58 2020 185 nefeslioglu sezer gokceoglu bozkir duman assessment slide susceptibility decision tree metropolitan area istanbul turkey mathematical problem engineering 2010 2010 901095 186 imandoust bolandraftar application neighbor knn approach predicting economic event theoretical background international journal engineering research application 3 5 2013 187 li umbach terry taylor application method seldi proteomics data bioinformatics 20 10 2004 188 guo wang bell bi greer knn approach application text categorization international conference intelligent text processing computational linguistics springer 2004 pp 189 jiang pang wu kuang improved algorithm text categorization expert system application 39 1 2012 1509 190 johansson k√∂nig niklasson truth extraction opaque model using genetic flair conference miami beach fl 2004 pp 191 quinlan generating production rule decision ijcai 87 seer 1987 pp 192 langley simon application machine learning rule induction munications acm 38 11 1995 193 berg bankruptcy prediction generalized additive model applied stochastic model business industry 23 2 2007 194 calabrese et al estimating bank loan loss given default generalized additive model ucd geary institute discussion paper series 2012 195 taylan weber beck new approach regression generalized additive model continuous optimization modern application Ô¨Ånance science technology optimization 56 2007 196 murase nagashima yonezaki matsukura kitakado application generalized additive model gam reveal relationship environmental factor distribution pelagic Ô¨Åsh krill case study sendai bay japan ice journal marine science 66 6 2009 197 tomi ƒá bo ≈æ ƒá modiÔ¨Åed geosite assessment model cation lazar canyon area serbia international journal environmental research 8 4 2014 198 guisan edward jr hastie generalized linear generalized additive model study specie distribution setting scene ecological modelling 157 2002 199 rothery roy application generalized additive model butterÔ¨Çy sect count data journal applied statistic 28 7 2001 200 pierrot goude electricity load forecasting generalized tive model intelligent system application power system conference isap 2011 ieee 2011 pp 201 griÔ¨Éths kemp tenenbaum bayesian model 2008 doi 202 neelon malley normand bayesian model repeated sures count data application outpatient psychiatric service use statistical modelling 10 4 2010 203 mcallister kirkwood bayesian stock assessment review example plication using logistic model ice journal marine science 55 6 1998 204 synnaeve bessiere bayesian model opening prediction rts game application starcraft computational intelligence game cig 2011 ieee conference ieee 2011 pp 205 min simonis hense probabilistic climate change prediction applying bayesian model averaging philosophical transaction royal society mathematical physical engineering science 365 1857 2007 2116 206 koop poirier tobias bayesian econometric method cambridge versity press 2007 207 cassandra kaelbling kurien acting uncertainty discrete bayesian model navigation proceeding national conference intelligent robot system iros 96 2 ieee 1996 pp 208 chipman george mcculloch bayesian cart model search journal american statistical association 93 443 1998 209 kim rudin shah bayesian case model generative approach reasoning prototype classiÔ¨Åcation advance neural tion processing system 2014 pp 210 kim khanna koyejo example not enough learn criticize icism interpretability advance neural information processing system 2016 pp 211 johansson niklasson k√∂nig accuracy comprehensibility data ing model proceeding seventh international conference information fusion 1 2004 pp 212 konig johansson niklasson versatile framework evolutionary data mining 2008 ieee international conference data mining workshop ieee 2008 pp 213 lakkaraju kamar caruana leskovec interpretable explorable proximations black box model 2017 214 mishra sturm dixon local interpretable explanation music content ismir 2017 pp 215 su wei varshney malioutov interpretable boolean rule learning classiÔ¨Åcation 2015 216 ribeiro singh guestrin nothing else matter tions identifying prediction invariance 2016 217 craven extracting comprehensible model trained neural network 1996 thesis 218 bastani kim bastani interpretability via model extraction 2017 219 hooker discovering additive structure black box function proceeding tenth acm sigkdd international conference knowledge discovery data mining acm 2004 pp 220 adler falk friedler nix rybeck scheidegger smith venkatasubramanian auditing model indirect inÔ¨Çuence edge information system 54 1 2018 221 koh liang understanding prediction via inÔ¨Çuence function proceeding international conference machine 70 jmlr org 2017 pp 222 cortez embrechts opening black box data mining model using sensitivity analysis 2011 ieee symposium computational intelligence data mining cidm ieee 2011 pp 223 cortez embrechts using sensitivity analysis visualization technique open black box data mining model information science 225 2013 224 lundberg lee uniÔ¨Åed approach interpreting model prediction advance neural information processing system 2017 pp 225 kononenko et al eÔ¨Écient explanation individual classiÔ¨Åcations using game theory journal machine learning research 11 jan 2010 226 chen lundberg lee explaining model propagating shapley value local component 2019 227 dabkowski gal real time image saliency black box classiÔ¨Åers vances neural information processing system 2017 pp 228 henelius puolam√§ki bostr√∂m asker papapetrou peek black box exploring classiÔ¨Åers randomization data mining knowledge covery 28 2014 229 moeyersoms alessandro provost marten explaining classiÔ¨Åcation model built sparse data 2016 230 baehrens schroeter harmeling kawanabe hansen m√£≈æ ller explain individual classiÔ¨Åcation decision journal machine learning research 11 jun 2010 231 adebayo kagal iterative orthogonal feature projection diagnosing bias model 2016 232 guidotti monreale ruggieri pedreschi turini giannotti local explanation black box decision system 2018 233 krishnan wu palm machine learning explanation iterative debugging proceeding workshop data analytics acm 2017 4 234 ≈° ikonja kononenko explaining classiÔ¨Åcations individual stance ieee transaction knowledge data engineering 20 5 2008 235 ribeiro singh guestrin anchor planation aaai conference artiÔ¨Åcial intelligence 2018 pp 1535 236 marten provost explaining document classiÔ¨Åcations mi terly 38 1 2014 237 chen fraiberger moakler provost enhancing transparency trol drawing inference individual big data 5 3 2017 238 goldstein kapelner bleich pitkin peeking inside black box izing statistical learning plot individual conditional expectation journal computational graphical statistic 24 1 2015 239 casalicchio molnar bischl visualizing feature importance black box model joint european conference machine learning knowledge discovery database springer 2018 pp 240 tolomei silvestri haines lalmas interpretable prediction ensemble via actionable feature tweaking proceeding acm sigkdd international conference knowledge discovery data mining acm 2017 pp 241 auret aldrich interpretation nonlinear relationship process ables use random forest mineral engineering 35 2012 242 rajani mooney stacking auxiliary feature visual question swering proceeding 2018 conference north american chapter association computational linguistics human language technology volume 1 long paper pp 243 rajani mooney ensembling visual explanation explainable terpretable model computer vision machine learning springer pp 244 n√∫√±ez angulo catal√† learning system support vector machine neural processing letter 24 1 2006 245 chen li wei multiple kernel support vector machine scheme feature selection rule extraction gene expression data cancer tissue artiÔ¨Åcial intelligence medicine 41 2 2007 246 n√∫√±ez angulo catal√† support vector machine symbolic tation vii brazilian symposium neural network sbrn ieee 2002 pp 247 sollich bayesian method support vector machine evidence predictive class probability machine learning 46 2002 248 sollich probabilistic method support vector machine advance ral information processing system 2000 pp 249 landecker thomure bettencourt mitchell kenyon brumby interpreting individual classiÔ¨Åcations hierarchical network 2013 ieee symposium computational intelligence data mining cidm ieee 2013 pp 112 barredo arrieta del ser et al information fusion 58 2020 250 jakulin mo ≈æ ina dem ≈° ar bratko zupan nomogram alizing support vector machine proceeding eleventh acm sigkdd international conference knowledge discovery data mining acm 2005 pp 251 fu rule generation neural network ieee transaction system man cybernetics 24 8 1994 252 towell shavlik extracting reÔ¨Åned rule neural network machine learning 13 1 1993 253 thrun extracting rule artiÔ¨Åcial neural network distributed tations proceeding international conference neural information processing system nip 94 1994 pp 254 setiono leow fernn algorithm fast extraction rule neural network applied intelligence 12 1 2000 255 taha ghosh symbolic interpretation artiÔ¨Åcial neural network ieee transaction knowledge data engineering 11 3 1999 256 tsukimoto extracting rule trained neural network ieee transaction neural network 11 2 2000 257 zilke menc√≠a janssen extraction deep neural network international conference discovery science springer 2016 pp 258 schmitz aldrich gouws algorithm extraction cision tree artiÔ¨Åcial neural network ieee transaction neural network 10 6 1999 259 sato tsukimoto rule extraction neural network via decision tree induction ijcnn 01 international joint conference neural network ceedings cat no 3 ieee 2001 pp 260 f√©raud cl√©rot methodology explain neural network classiÔ¨Åcation neural network 15 2 2002 261 shrikumar greenside kundaje learning important feature propagating activation diÔ¨Äerences 2017 262 sundararajan taly yan axiomatic attribution deep network ternational conference machine learning 70 jmlr org 2017 pp 263 adebayo gilmer goodfellow kim local explanation method deep neural network lack sensitivity parameter value 2018 264 papernot mcdaniel deep neighbor towards conÔ¨Ådent pretable robust deep learning 2018 265 li chen hovy jurafsky visualizing understanding neural model nlp 2015 266 tan sim gale improving interpretability deep neural network stimulated learning 2015 ieee workshop automatic speech tion understanding asru ieee 2015 pp 267 rieger singh murdoch yu interpretation useful penalizing explanation align neural network prior knowledge 2019 268 nguyen dosovitskiy yosinski brox clune synthesizing preferred input neuron neural network via deep generator network advance neural information processing system 2016 pp 269 li yosinski clune lipson hopcroft convergent learning ferent neural network learn representation iclr 2016 270 liu shi li li zhu liu towards better analysis deep tional neural network ieee transaction visualization computer graphic 23 1 2016 271 goyal mohapatra parikh batra towards transparent ai system terpreting visual question answering model 2016 272 simonyan vedaldi zisserman deep inside convolutional network alising image classiÔ¨Åcation model saliency map 2013 273 nguyen yosinski clune deep neural network easily fooled high conÔ¨Ådence prediction unrecognizable image proceeding ieee ference computer vision pattern recognition 2015 pp 274 donahue anne hendricks guadarrama rohrbach venugopalan saenko darrell recurrent convolutional network visual nition description proceeding ieee conference computer vision pattern recognition 2015 pp 275 lin chen yan network network 2013 276 hendricks akata rohrbach donahue schiele darrell ating visual explanation 2016 277 wang jiang qian yang li zhang wang tang residual attention network image classiÔ¨Åcation proceeding ieee conference computer vision pattern recognition 2017 pp 278 xiao xu yang zhang peng zhang application attention model deep convolutional neural network image siÔ¨Åcation proceeding ieee conference computer vision pattern recognition 2015 pp 279 zhang cao nian wu zhu growing interpretable part graph convnets via learning 2016 280 arras montavon m√ºller samek explaining recurrent neural work prediction sentiment analysis 2017 281 karpathy johnson visualizing understanding recurrent work 2015 282 clos wiratunga massie towards explainable text classiÔ¨Åcation jointly learning lexicon modiÔ¨Åer term workshop explainable ai xai 2017 19 283 wisdom power pitton atlas interpretable recurrent neural network using sequential sparse recovery 2016 284 krakovna increasing interpretability recurrent neural work using hidden markov model 2016 285 choi bahadori sun kulas schuetz stewart retain pretable predictive model healthcare using reverse time attention mechanism advance neural information processing system 2016 pp 286 breiman classiÔ¨Åcation regression tree routledge 2017 287 lucic haned de rijke explaining prediction boosting ensemble 2019 288 lundberg erion lee consistent individualized feature attribution tree ensemble 2018 289 bucilu «é caruana model compression acm sigkdd international conference knowledge discovery data mining acm 2006 pp 290 traor√© lesort sun cai rodr√≠guez filliat discorl continual reinforcement learning via policy distillation 2019 291 zeiler taylor fergus et al adaptive deconvolutional network mid high level feature iccv 1 2011 6 292 selvaraju cogswell da vedantam parikh batra cam visual explanation deep network via localization proceeding ieee international conference computer vision 2017 pp 293 olah mordvintsev schubert feature distill 2017 doi http 294 adebayo gilmer muelly goodfellow hardt kim sanity check saliency map advance neural information processing system 2018 pp 295 olah satyanarayan johnson carter schubert ye mordvintsev building block interpretability distill 2018 296 che purushotham khemani liu distilling knowledge deep work application healthcare domain 2015 297 donadello seraÔ¨Åni garcez logic tensor network semantic image interpretation proceeding international joint conference artiÔ¨Åcial intelligence ijcai 2017 298 donadello semantic image numerical data ical knowledge cognitive vision university trento 2018 thesis 299 avila garcez gori lamb seraÔ¨Åni spranger tran symbolic computing eÔ¨Äective methodology principled integration chine learning reasoning 2019 300 manhaeve dumancic kimmig demeester de raedt deepproblog neural probabilistic logic programming advance neural information cessing system 31 2018 pp 301 donadello dragoni eccher persuasive explanation reasoning inference dietary data first workshop semantic explainability iswc 2019 2019 302 krishnan shalit sontag deep kalman filter 2015 303 karl soelch bayer van der smagt deep variational bayes filter unsupervised learning state space model raw data 2016 304 johnson duvenaud wiltschko adam datta ing graphical model neural network structured representation fast inference advance neural information processing system 29 2016 pp 305 zheng jayasumana vineet su du huang torr conditional random Ô¨Åelds recurrent neural network proceeding ieee international conference computer vision 2015 pp 306 narodytska ignatiev pereira learning optimal decision tree sat proceeding international joint ence artiÔ¨Åcial intelligence 2018 pp 307 understanding advantage weakness practical point view ieee access 7 2019 308 petroni rockt√§schel lewis bakhtin wu miller riedel guage model knowledge base 2019 309 bollacker li extending knowledge graph tive inÔ¨Çuence network personalized fashion portmann tabacchi seising habenstein ed designing cognitive city springer international publishing 2019 pp 310 shang trott zheng xiong socher learning world graph erate hierarchical reinforcement learning 2019 311 zolotas demiris towards explainable shared control using augmented reality 2019 312 garnelo arulkumaran shanahan towards deep symbolic reinforcement learning 2016 313 bellini schiavone di noia ragone di sciascio toencoders explainable recommender system proceeding shop deep learning recommender system dlrs 2018 2018 pp 314 huang vaswani uszkoreit shazeer hawthorne dai hoÔ¨Äman eck music transformer generating music structure 2018 315 cornia baraldi cucchiara smart training shallow former robotic explainability 2019 316 aamodt plaza reasoning foundational issue methodological variation system approach 7 1 1994 317 caruana explanation artiÔ¨Åcial neural net artiÔ¨Åcial neural network medicine biology proceeding conference 2000 pp 318 keane kenny approach one generic solution xai overview twin explaining deep learning 2019 319 hailesilassie rule extraction algorithm deep neural network review 2016 320 benitez castro requena artiÔ¨Åcial neural network black box ieee trans neural network 8 5 1997 113 barredo arrieta del ser et al information fusion 58 2020 321 johansson k√∂nig niklasson automatically balancing accuracy prehensibility predictive modeling proceeding international ference information fusion 2 2005 322 smilkov thorat kim vi√©gas wattenberg smoothgrad removing noise adding noise 2017 323 ancona ceolini √∂ztireli gross towards better understanding attribution method deep neural network 2017 324 yosinski clune bengio lipson transferable feature deep neural network 2014 325 sharif razavian azizpour sullivan carlsson cnn feature astounding baseline recognition 2014 326 du guo simpson car steering angle prediction based image recognition technical report technical report stanford university 2017 327 zhou khosla lapedriza oliva torralba object detector emerge deep scene cnns 2014 328 zhang chen explainable recommendation survey new perspective 2018 329 frankle carbin lottery ticket hypothesis finding sparse trainable neural network 2018 330 vaswani shazeer parmar uszkoreit jones gomez kaiser polosukhin attention need 2017 331 lu yang batra parikh hierarchical visual question answering proceeding international conference neural information processing system nip 16 2016 pp 332 da agrawal zitnick parikh batra human attention visual question answering human deep network look region 2016 333 huk park hendricks akata rohrbach schiele darrell rohrbach multimodal explanation justifying decision pointing dence 2018 334 slavin ross hughes right right reason training diÔ¨Äerentiable model constraining explanation 2017 335 jolliÔ¨Äe principal component analysis factor analysis springer new york pp 336 hyv√§rinen oja oja independent component analysis algorithm plication neural network 13 neural network 13 2000 337 berry browne langville pauca plemmons algorithm application approximate nonnegative matrix factorization computational statistic data analysis 52 2007 338 kingma welling variational bayes 2013 339 higgins matthey pal burgess glorot botvinick mohamed lerchner learning basic visual concept constrained variational framework iclr 2017 340 chen duan houthooft schulman sutskever abbeel infogan interpretable representation learning information maximizing generative versarial net 2016 341 zhang yang liu nian wu zhu unsupervised learning neural network explain neural network 2018 342 sabour frosst e hinton dynamic routing capsule 2017 343 agrawal lu antol mitchell zitnick batra parikh vqa visual question answering 2015 344 fukui huk park yang rohrbach darrell rohrbach multimodal compact bilinear pooling visual question answering visual grounding 2016 345 bouchacourt denoyer educe explaining model decision vised concept extraction 2019 346 hofer denker ducasse design implementation debugger node 2006 lecture note informatics 2006 pp 347 rudin please stop explaining black box model high stake decision 2018 348 del ser galar sierra data fusion machine learning industrial prognosis trend perspective towards industry information fusion 50 2019 349 hoÔ¨Äman mueller klein litman metric explainable ai lenges prospect 2018 350 mohseni zarei ragan multidisciplinary survey framework design evaluation explainable ai system 2018 351 byrne counterfactuals explainable artiÔ¨Åcial intelligence xai evidence human reasoning proceeding international joint conference artiÔ¨Åcial intelligence 2019 pp 352 garnelo shanahan reconciling deep learning symbolic artiÔ¨Åcial ligence representing object relation current opinion behavioral science 29 2019 353 marra giannini diligenti gori integrating learning reasoning deep logic model 2019 354 kelley clark brown sitzia good practice conduct reporting survey research international journal quality health care 15 3 2003 355 wachter mittelstadt floridi right explanation automated doe not exist general data protection regulation international data privacy law 7 2 2017 356 orekondy schiele fritz knockoÔ¨Änets stealing functionality model 2018 357 oh schiele fritz towards neural work explainable ai interpreting explaining visualizing deep learning springer 2019 pp 358 goodfellow shlens szegedy explaining harnessing adversarial amples 2014 359 eykholt evtimov fernandes li rahmati xiao prakash kohno song robust attack deep learning model 2017 360 goodfellow papernot mcdaniel cleverhans adversarial chine learning library 2016 361 xiao biggio nelson xiao eckert roli support vector machine adversarial label contamination neurocomputing 160 c 2015 362 biggio corona maiorca nelson ≈° rndi ƒá laskov giacinto roli evasion attack machine learning test time proceeding european conference machine learning knowledge discovery database volume part iii ecmlpkdd 13 2013 pp 363 biggio pillai bul√≤ ariu pelillo roli data clustering sarial setting secure 2018 364 pan yu yi khan yuan zheng recent progress generative adversarial network gans survey ieee access 7 2019 365 charte charte garc√≠a del jesus herrera practical tutorial autoencoders nonlinear feature fusion taxonomy model software line information fusion 44 2018 366 baumgartner koch tezcan xi ang konukoglu visual ture attribution using wasserstein gans proceeding ieee conference computer vision pattern recognition 2018 pp 367 biÔ¨É oktay tarroni bai de marvao doumou rajchl bedair prasad cook et al learning interpretable anatomical feature deep generative model application cardiac remodeling tional conference medical image computing tion springer 2018 pp 368 liu kailkhura loveland han generative counterfactual introspection explainable deep learning 2019 369 varshney alemzadeh safety machine learning system decision science data product big data 5 3 2017 370 wei mining rarity unifying framework acm sigkdd exploration newsletter 6 1 2004 371 attenberg ipeirotis provost beat machine challenging human Ô¨Ånd predictive model unknown unknown journal data information quality jdiq 6 1 2015 1 372 neÔ¨Ä tanweer osburn critique contribute framework improving critical data study data science big data 5 2 2017 373 iliadis russo critical data study introduction big data society 3 2 2016 2053951716674238 374 karpatne atluri faghmous steinbach banerjee ganguly shekhar samatova kumar data science new paradigm scientiÔ¨Åc discovery data ieee transaction knowledge data neering 29 10 2017 375 hautier fischer jain mueller ceder finding nature missing ternary oxide compound using machine learning density functional theory chemistry material 22 12 2010 376 fischer tibbetts morgan ceder predicting crystal structure merging data mining quantum mechanic nature material 5 8 2006 641 377 curtarolo hart nardelli mingo sanvito levy highway computational material design nature material 12 3 2013 191 378 wong wang shi active model orthotropic hyperelastic material cardiac image analysis international conference functional imaging modeling heart springer 2009 pp 379 xu sapp dehaghani gao horacek wang robust transmural electrophysiological imaging integrating sparse dynamic physiological model inference international conference medical image computing intervention springer 2015 pp 380 lesort seurin li filliat unsupervised state sentation learning robotic prior robustness benchmark 2017 381 leibo liao anselmi freiwald poggio face nition hebbian learning imply neural tuning head tation current biology 27 1 2017 382 schrodt kattge fazayeli joswig banerjee reichstein b√∂nisch d√≠az dickie et al hierarchical bayesian approach trait prediction macroecology functional biogeography global ecology biogeography 24 12 2015 383 leslie understanding artiÔ¨Åcial intelligence ethic safety 2019 doi 384 rudin stop explaining black box machine learning model high stake sion use interpretable model instead 2018 385 fjeld hilligoss achten daniel feldman kagay principled tiÔ¨Åcial intelligence map ethical approach 2019 386 benjamin barbado sierra responsible ai design 2019 387 transforming world 2030 agenda sustainable opment technical report esocialsciences 2015 388 hager drobnis fang ghani greenwald lyon parkes schultz saria smith tambe artiÔ¨Åcial intelligence social good 2019 389 stahl wright ethic privacy ai big data implementing sible research innovation ieee security privacy 16 3 2018 390 high level expert group artiÔ¨Åcial intelligence ethic guideline thy ai technical report european commission 2019 114 barredo arrieta del ser et al information fusion 58 2020 391 alessandro neil lagatta conscientious classiÔ¨Åcation data entist guide classiÔ¨Åcation big data 5 2 2017 134 392 barocas selbst big data disparate impact rev 104 2016 671 393 hardt price srebro et al equality opportunity supervised learning advance neural information processing system 2016 pp 394 speicher heidari gummadi singla weller zafar uniÔ¨Åed approach quantifying algorithmic unfairness measuring individual group unfairness via inequality index proceeding acm sigkdd international conference knowledge discovery data mining acm 2018 pp 395 kamiran calder data preprocessing technique classiÔ¨Åcation without discrimination knowledge information system 33 1 2012 396 zemel wu swersky pitassi dwork learning fair representation international conference machine learning 2013 pp 397 zhang lemoine mitchell mitigating unwanted bias adversarial learning proceeding 2018 conference ai ethic society acm 2018 pp 398 ahn lin fairsight visual analytics fairness decision making ieee transaction visualization computer graphic 2019 399 soares angelov explainable model prediction vism arxiv preprint 2019 400 dressel farid accuracy fairness limit predicting recidivism science advance 4 1 2018 401 aivodji arai fortineau gambs hara tapp fairwashing risk rationalization international conference machine learning 2019 pp 402 sharma henderson ghosh certifai counterfactual explanation ness transparency interpretability fairness artiÔ¨Åcial intelligence model arxiv preprint 2019 403 drosou jagadish pitoura stoyanovich diversity big data review big data 5 2 2017 404 lerman big data exclusion stan rev online 66 2013 55 405 agrawal gollapudi halverson ieong diversifying search result proceeding second acm international conference web search data mining acm 2009 pp 406 smyth mcclave similarity diversity international conference reasoning springer 2001 pp 407 wang yang li chen hu data fusion tems perspective information fusion 51 2019 408 ding jing yan yang survey data fusion internet thing towards secure fusion information fusion 51 2019 409 smirnov levashova knowledge fusion pattern survey information fusion 52 2019 410 ding jing yan yang survey data fusion internet thing towards secure fusion information fusion 51 2019 411 wang yang li chen hu data fusion tems perspective information fusion 51 2019 412 lau marakkalage zhou hassan yuen zhang tan survey data fusion smart city application information fusion 52 2019 413 fern√°ndez garc√≠a chen herrera big data tutorial guideline information process fusion analytics algorithm mapreduce information fusion 42 2018 414 kone ƒçn√Ω mcmahan ramage richt√°rik federated optimization tributed machine learning intelligence 2016 415 mcmahan moore ramage hampson arca Ô¨Åcient learning deep network decentralized data artiÔ¨Åcial intelligence statistic 2017 pp 416 kone ƒçn ·ª≥ mcmahan yu richt√°rik suresh bacon federated learning strategy improving communication eÔ¨Éciency 2016 417 sun survey machine learning neural computing tions 23 2013 418 zhang nie li wei feature selection data survey information fusion 50 2019 419 zhao xie xu sun learning overview recent progress new challenge information fusion 38 2017 420 oh benenson fritz schiele faceless person recognition privacy implication social medium computer vision eccv 2016 european conference amsterdam proceeding part iii 2016 pp 421 aditya sen druschel joon oh benenson fritz schiele tacharjee wu platform image capture ceedings annual international conference mobile system tions service acm 2016 pp 422 sun tewari xu fritz theobalt schiele hybrid model tity obfuscation face replacement proceeding european conference computer vision eccv 2018 pp 423 dong srivastava big data integration 2013 ieee international conference data engineering icde ieee 2013 pp 424 zhang zhao zhang comobile human mobility modeling urban scale using learning proceeding sigspatial international conference advance geographic information system acm 2015 40 425 pan yang survey transfer learning ieee transaction knowledge data engineering 22 10 2009 426 mitchell wu zaldivar barnes vasserman hutchinson spitzer raji gebru model card model reporting proceeding ference fairness accountability transparency acm 2019 pp 115