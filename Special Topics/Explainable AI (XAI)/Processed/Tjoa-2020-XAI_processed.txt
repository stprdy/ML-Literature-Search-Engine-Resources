ieee transaction neural network learning system vol 32 no 11 november 2021 4793 survey explainable artiﬁcial intelligence xai toward medical xai erico tjoa cuntai guan fellow ieee artiﬁcial intelligence machine learning general demonstrated remarkable performance many task image processing natural language processing especially advent deep learning dl along research progress encroached upon many different ﬁelds discipline some require high level accountability thus transparency example medical sector explanation machine decision prediction thus needed justify reliability requires greater interpretability often mean need understand mechanism underlying algorithm unfortunately blackbox nature dl still unresolved many machine decision still poorly understood provide review interpretabilities suggested different research work categorize different category show different dimension interpretability research approach provide obviously interpretable information study complex pattern applying categorization interpretability medical research hoped 1 clinician practitioner subsequently approach method caution 2 insight interpretability born consideration medical practice 3 initiative push forward mathematically grounded technically grounded medical education encouraged index artiﬁcial intelligence xai pretability machine learning ml medical information system survey introduction achine learning ml ha grown large research industrial application especially success deep learning dl neural network nns large impact possible no longer taken granted some ﬁelds failure not option even momentarily dysfunctional computer vision algorithm autonomous vehicle easily lead fatality medical ﬁeld clearly human life line detection disease early phase often critical recovery patient prevent disease advancing severe stage ml method artiﬁcial nns interface related subﬁelds recently demonstrated promising manuscript received october 15 2019 revised june 7 2020 august 10 2020 accepted september 24 date publication october 20 2020 date current version october 28 work wa supported division damo academy alibaba group holding talent program corresponding author erico tjoa erico tjoa wa healthtech division alibaba group holding hangzhou 311121 china school computer ence engineering nanyang technological university singapore 639798 cuntai guan school computer science engineering nanyang technological university singapore digital object identiﬁer performance performing medical task hardly perfect 1 9 interpretability explainability ml algorithm thus become pressing issue accountable thing go wrong explain thing go wrong thing working well know leverage many article suggested different measure framework capture interpretability topic able artiﬁcial intelligence xai ha become hotspot ml research community popular dl library started include xai library pytorch captum tensorﬂow furthermore proliferation interpretability assessment criterion reliability ity usability help ml community keep track algorithm used usage improved providing guiding post development 10 12 particular ha demonstrated visualization capable helping researcher detect erroneous reasoning classiﬁcation problem many previous researcher possibly missed 13 said seems lack uniform adoption interpretability assessment criterion across research community attempt deﬁne notion interpretability explainability along reliability trustworthiness similar notion clear exposition incorporated great diversity implementation ml el consider 10 14 18 survey instead use explainability interpretability ably considering research related interpretability doe show any attempt 1 explain decision made algorithm 2 uncover pattern within inner mechanism algorithm 3 present system coherent model mathematics include even loose attempt raise credibility machine algorithm work survey research work related interpretability ml computer algorithm general categorize apply category pretability medical ﬁeld categorization especially aimed give clinician practitioner perspective use interpretable algorithm available diverse form tradeoff ease interpretation need specialized mathematical knowledge may create bias preference one method compared another without justiﬁcation based medical practice may provide ground specialized education medical sector aimed realize potential work licensed creative common attribution license information see 4794 ieee transaction neural network learning system vol 32 no 11 november 2021 table list journal article arranged according interpretability method used interpretability presented suggested mean interpretability tabulation provides nonexhaustive overview interpretability method placing some derivative method umbrella main method derive hsi human study human study designed verify suggested method interpretable human subject ann explicitly introduces new artificial nn architecture modifies existing network performs test nns reside within algorithm also ﬁnd many journal article ml ai community often assume algorithm used obviously interpretable without conducting human subject test verify interpretability see column hsi table ii noted assuming model obviously interpretable not necessarily wrong some case human test might irrelevant example predeﬁned model based commonly accepted knowledge speciﬁc may considered interpretable without human subject test table also include column indicate whether interpretability method applies artiﬁcial nn since issue interpretability recently gathering attention due blackbox nature not attempt cover related work many already presented research article survey cite 1 2 15 30 extend integrated interpretability 16 including consideration model compared 17 also overview mathematical formulation common ular method revealing great variety approach interpretability categorization draw starker borderline different view interpretability seem difﬁcult reconcile sense survey suitable technically oriented reader due some mathematical detail although casual reader may ﬁnd useful reference relevant popular item may develop interest young research ﬁeld conversely algorithm user need interpretability work might develop inclination understand previously hidden thick veil ematical formulation might ironically undermine bility interpretability clinician medical practitioner already some familiarity mathematical term may get glimpse some proposed interpretability tjoa guan survey xai toward medical xai 4795 table ii continued table list journal article arranged according interpretability method used interpretability presented suggested mean interpretability method might risky unreliable survey 30 view interpretability term extraction relational knowledge speciﬁcally scrutinizing method symbolic cycle present framework subcategory within interpretability literature include verbal interpretability though framework doe demonstrate method category perceived verbal interpretability well extensive survey 18 provides large list research categorized transparent model model requiring post hoc analysis multiple egories survey hand aim overview state interpretable ml applied medical ﬁeld article arranged following section ii duce generic type interpretability subtypes section applicable provide challenge future prospect related category section iii applies categorization interpretabilities section ii medical ﬁeld list risk machine interpretability medical ﬁeld proceed also imperative point issue accountability interpretability ha spawned discussion recommendation 31 33 even entered sphere ethic law enforcement 34 engendering movement protect society possible misuse harm wake increasing use ai 4796 ieee transaction neural network learning system vol 32 no 11 november 2021 fig overview categorization illustration orange box pretability interface demarcate separation interpretable mation cognitive process required understand gray box algorithm proposed provide interpretability black arrow computing comprehension process perceptive interpretability method generate item usually considered immediately interpretable hand method provide interpretability via mathematical structure generate output require one layer cognitive processing interface reaching interpretable interface eye ear icon represent human sens interacting item generated interpretability ii type interpretability ha yet widely adopted standard understand ml interpretability though work proposing framework interpretability 10 13 35 fact ent work use different criterion justiﬁable one way another network dissection ha suggested 36 37 evaluate interpretability visual representation deep nn dnn inspired neuroscientist procedure understand biological neuron article also offer way quantify neuronal network unit activation response different concept detected interactive website 38 39 suggested uniﬁed framework study interpretabilities studied separately article 40 deﬁnes uniﬁed measure feature importance shapley additive explanation shap framework rize existing interpretabilities present nonexhaustive list work category two major category presented namely perceptive interpretability interpretability mathematical structure illustrated fig 1 appear present different tie within notion interpretability example difﬁculty perceptive interpretability following visual evidence given erroneously algorithm method used generate evidence underlying mathematical structure sometimes not offer any useful clue ﬁx mistake hand mathematical analysis pattern may provide information high sion only easily perceived pattern brought lower dimension abstracting some information could not yet prove not discriminative measurable certainty perceptive interpretability include category interpretabilities humanly perceived often one considered example shown fig 2 algorithm classiﬁes image cat category considered obviously interpretable provides segmented patch showing cat explanation note alone might hand considered insufﬁcient 1 still doe not unblackbox algorithm 2 ignores possibility using background object decision following subcategories perceptive interpretability refer fig 3 overview common subcategories 1 saliency saliency method explains decision algorithm assigning value reﬂect importance input component contribution decision value could take form probability heatmaps etc example fig 2 show model predicts patient suffers ﬂu series factor lime 14 explains choice highlighting importance particular symptom indicate illness indeed ﬂu similarly jacovi et al 41 putes score reﬂecting activating convolution ﬁlters natural language processing nlp fig 2 strates output lime provide explanation choice classiﬁcations cat fig 2 demonstrates kind heatmap show contribution pixel segmentation result segmentation result not shown ﬁgure only demonstration formally given model f make prediction f x input x some metric v typically large magnitude v xi indicates component xi signiﬁcant reason output saliency method via decomposition oped general decompose signal propagated within algorithm selectively rearrange process provide interpretable information class activation map cam ha popular method generate use term interchangeably corresponds discriminative feature classiﬁcations 42 44 original implementation cam 42 produce heatmaps using fk x activation unit k across spatial coordinate x last convolutional layer weighted wc k coefﬁcient corresponding unit k class cam pixel x thus given mc x k fk x similarly widely used relevance tion lrp introduced 45 some article use lrp construct saliency map interpretability include 13 46 51 also applicable video processing 52 short summary lrp given 53 lrp considered decomposition method 54 indeed importance score decomposed sum score layer equal output short relevance score intensity input layer r 0 r l j l ij l ij r j relevance score neuron layer l input layer l pixel x input layer assigned importance value r 0 x although some combination relevance score r l c inner layer l different channel c demonstrated meaningful well though possibly le precise see tutorial website lrp understood deep taylor decomposition tjoa guan survey xai toward medical xai 4797 fig 2 using lime generate explanation text classiﬁcation headache sneeze assigned positive value mean factor positive contribution model prediction hand weight no fatigue contribute negatively prediction lime used generate classiﬁcation adc modality slice mri scan isle 2017 segmentation competition reddish intensity region reﬂects possible explanation choice segmentation segmentation not shown b optimized image maximize activation neuron indicated layer shallower layer simple pattern activate neuron strongly deeper layer complex feature dog face ear figure b obtained permission chris olah fig overview perceptive interpretability method saliency method decomposition mechanism input image cat fed model processing along blue arrow resulting output intermediate signal green arrow decomposed selectively picked processing hence providing information intermediate mechanism model form often heatmappings shown color b saliency method sensitivity mechanism idea show small change input black ﬁgures bird duck affect information extracted explainability red silhouette example red region indicate high relevance sometimes observe edge boundary object gradient high c signal method inversion optimization inverse signal data propagated model could possibly reveal sensible information see arrow labeled inverse adjusting input optimize particular signal shown ith component function may provide u reveals explainable information see arrow labeled optimization illustration show probability correctly predicting duck improves greatly head changed head duck model recognizes verbal interpretability typically achieved ensuring model capable providing humanly understable statement logical relation positive word shown framework 55 though speak many version lrp developed code implementation also found aforementioned website automatic explanation ace algorithm 56 us explanation decomposition method developed include deeplift dient input 57 prediction difference analysis 58 41 peak response mapping 59 generated backpropagating peak signal peak signal normalized treated probability method seen decomposition 4798 ieee transaction neural network learning system vol 32 no 11 november 2021 probability transition 60 removed correlation ρ proposed metric measure quality signal estimator proposes patternnet tribution backpropagate parameter optimized ρ resulting saliency map well smoothgrad 61 improves technique adding noise visit related website display numerous visual comparison saliency method mindful some heatmaps highlight apparently irrelevant region nlp sentiment analysis saliency map also take form heat score word text demonstrated arras et al 62 using lrp karpathy et al 63 medical ﬁeld see later section irvin et al 6 zhao et al 44 paschali et al 64 couture et al 65 li et al 66 qin et al 67 tang et al 68 tasopoulos et al 69 lee et al 70 studied method employing saliency visual explanation noted also subcategorize lime method us optimization sensitivity underlying mechanism many research interpretability span one subcategories challenge future prospect seen la cam lrp given heuristic certain way interaction weight strength activation some unit within model eventually produce interpretable information intermediate process not amenable scrutiny example taking one weight changing value doe not easily reveal any useful information prescribed way translate pretable information may also beneﬁt stronger evidence especially evidence beyond visual veriﬁcation localized object signal method investigate ml model see later section exist method probe respect method not attempted systematically possibly opening different research direction 2 signal method method interpretability observe stimulation neuron collection neuron called signal method 71 one hand activated value neuron manipulated transformed interpretable form example activation neuron layer used reconstruct image similar input possible neuron store information systematically 36 72 feature map deeper layer activate strongly complex feature human face keyboard feature map shallower layer show simple pattern line curve example feature map output convolutional ﬁlter convolutional nn cnn network dissection procedure evaluates neuronal unit activation computing iou score relevant concept question 36 37 hand parameter even input data might optimized respect activation value particular neuron using method known activation optimization see later section following relevant subcategories feature map inversion input reconstruction feature map often look like highly blurred image region showing zero low intensity except patch human could roughly discern detected feature sometimes discernible feature considered interpretable 72 however might distorted else feature map related humanly perceptible feature inverse convolution map deﬁned example feature map layer 2 computed network via x x input consists 7 7 convolution stride 2 followed likewise 72 reconstructs image using deconvolution network approximately inversing trained convolutional network x deconv ˆ f 2 ˆ f 1 approximation layer no unique inverse shown x doe appear like slightly blurred version original image distinct human eye inversion image tions within layer ha also used demonstrate cnn layer store important information input image accurately 73 74 guided backpropagation 75 modiﬁes way backpropagation performed achieve inversion zeroing negative signal output input signal backwards layer indeed method use saliency map visualization activated signal b activation optimization besides transforming vation neuron signal method also includes ﬁnding input image optimize activation neuron collection neuron called activation maximization starting noise input x noise slowly adjusted increase activation select collection neuron ak simple mathematical term task ﬁnd argmax ak optimization performed input x suitable metric measure combined strength activation finally optimized input maximizes activation neuron emerge something visually recognizable example image could surreal fuzzy combination swirling pattern part dog face shown fig 2 b research work activation maximization include 76 mnist data set 77 78 us regularization function particular olah et al 38 provides lent interactive interface feature visualization demonstrating image googlenet 79 googlenet ha deep architecture see neuron deeper layer store complex feature shallower layer store simple pattern see fig 2 b bring one step semantic dictionary used 39 provide visualization activation within higher level organization semantically meaningful arrangement c observation signal activation ablation study 80 81 also study role neuron shallower deeper layer essence some neuron corrupted output corrupted nn compared original network challenge future prospect signal method might revealed some part mechanism many question still remain follows 1 partially reconstructed image image optimize activation tjoa guan survey xai toward medical xai 4799 2 might learned approximately inverse signal recover image help improve pretability 3 component part intermediate process reconstruct approximate image might contain important information able utilize future 4 explaining component inverse space useful explaining signal forward propagated 5 similarly doe looking intermediate signal lead activation optimization help u pinpoint role collection neuron 6 optimization highly parameterized function ously give nonunique solution sure optimization yield combination surreal dog face not yield strange image minor alteration process answering question may ﬁnd hidden clue required get closer interpretable ai 3 verbal interpretability form interpretability take form verbal chunk human grasp naturally example include sentence indicate causality shown example logical statement formed proper tion predicate connective etc example logical statement conditional statement conditional statement statement form another word ml model logical statement extracted directly ha considered obviously interpretable survey 30 show interpretability method general viewed symbolic relational system medical ﬁeld see 82 83 similarly decision set rule set studied interpretability 84 following single line rule set rainy grumpy calm vegetable directly quoted article line rule set contains clause input disjunctive normal form dnf mapped output dnf well example formally written rainy vegetable comparing three different variable suggested interpretability explanation form rule set affected cognitive chunk explanation size little effected variable repetition cognitive chunk deﬁned clause input dnf number repeated cognitive chunk rule set varied explanation size line rule set line rule set muse 85 also produce explanation form decision set interpretable model chosen approximate function optimized number metric including direct optimization interpretability metric not surprising verbal segment provided explanation nlp problem framework 86 extract segment like pleasant ruby color justify 5 rating product review given sequence word x xl xk rd explanation given subset sentence give summary rating justiﬁed subset expressed binary sequence zl zk 1 0 indicates xk not subset z follows probability distribution p decomposed assuming dence p p σz w z hk hk bz ht ht usual hidden unit recurrent cell forward backward respectively similar segment generated using probability density function improve relation activation certain ﬁlters speciﬁc attribute 87 earlier work visual question answering vqa 88 90 concerned generation text discussing object appearing image challenge future prospect text appear provide explanation underlying mechanism used generate text not necessarily explained example nns common used task recurrent nn rnn long ory lstm still black box hard troubleshoot case wrong prediction le work probe inner signal lstm rnn nns possible research direction although similar problem mentioned section may arise intermediate signal furthermore word embedding often optimized usual loss minimization doe not seem coherent explanation process shape optimized embedding may some clue regarding optimization residing within embedding thus successfully interpreting shape embedding may help shed light mechanism algorithm interpretability via mathematical structure mathematical structure used reveal anisms ml nn algorithm previous section deeper layer nn shown store complex information shallower layer store simpler information 72 testing concept activation vector tcav 96 ha used show similar trend suggested fig 4 method include clustering stochastic neighbor embedding shown fig 4 b method example singular vector ical correlation analysis svcca 97 used ﬁnd signiﬁcant direction subspace input accurate prediction shown fig 4 c information theory ha used study interpretability considering information bottleneck principle 98 99 rich way mathematical structure add interpretability pave way comprehensive view interpretability algorithm hopefully providing ground unifying different view coherent framework future fig 5 provides overview idea category 1 predeﬁned model study system interest cially complex system not behavior mathematical formula parametric model help simplify task proper hypothesis relevant term parameter designed model tion term come naturally hypothesis either consistent available knowledge least developed 4800 ieee transaction neural network learning system vol 32 no 11 november 2021 fig 4 tcav 96 method ﬁnds hyperplane cav separate concept interest accuracy cav applied different layer support idea deeper nn layer contain complex concept shallower layer contain simpler concept b svcca 97 ﬁnds signiﬁcant subspace direction contains information graph show 25 direction 500 enough produce accuracy full network c cluster image meaningful arrangement example dog image close together figure used permission author kim ﬁgure b c maithra raghu jascha fig overview method whose interpretability depend interpreting underlying mathematical structure predeﬁned model modeling clear easily understandable model linear model help improve readability hence interpretability hand using nn could obscure meaning input variable b feature extraction data predicted value signal parameter model processed transformed selectively picked provide useful information mathematical knowledge usually required understand resulting pattern c sensitivity model rely sensitivity gradient perturbation related concept try account different data differently represented ﬁgure small change transforming bird duck traced along map obtained using clustering good reason system better understood formula improved inclusion complex component medical ﬁeld see later section example kinetic modeling ml used compute parameter deﬁned model method exist integrating commonly available methodology subject speciﬁc content etc example generative criminative model 100 combine ridge regression least square method handle variable analyzing alzheimer disease schizophrenia linearity simplest interpretable predeﬁned model linear combination variable ai degree much xi contributes prediction linear combination model xi 0 1 ha referred additive feature attribution method 40 model performs well considered highly interpretable however many model highly nonlinear case studying interpretability via linear property example using linear probe see useful several way including ease implementation linear property appears insufﬁcient nonlinearity introduced typically not difﬁcult replace linear component w within system nonlinear version f w linear probe used 101 extract information layer nn technically assume dl classiﬁer f x 0 1 fi x 0 1 probability input x classiﬁed class class given set feature hk layer k nn tjoa guan survey xai toward medical xai 4801 linear probe fk layer k deﬁned linear classiﬁer fk hk 0 1 f hk softmax whk b another word probe tell u well information only layer k predict output predictive probe linear classiﬁer design article show plot error rate prediction made fk k demonstrates linear classiﬁers generally perform better deeper layer larger b general additive model linear model generalized generalized additive model gam 102 103 standard form g e f j x j g link function equation general speciﬁc mentation f j link function depend task familiar general linear model glm gam speciﬁc implementation linear f j g identity modiﬁcations duly implemented natural extension model interaction term variable fij xi x j used 104 certainly extend indeﬁnitely protoattend 105 us probability weight linear component nn model considered inherently interpretable author medical ﬁeld see 82 100 106 107 c model some algorithm considered obviously interpretable within ﬁeld model designed based existing knowledge empirical evidence thus interpretation model innately embedded system ml algorithm incorporated rich diverse way example parameter ﬁtting following list work illustrate usage diversity ml algorithm deep tensor nn used quantum system 108 atomistic nn architecture quantum chemistry used 109 atom like node graph set feature vector speciﬁcs depend nn used model considered inherently interpretable nn ha used programmable wireless environment pwes 110 approximation 111 fuzzy network approximation nns approximate fuzzy system constructed choice component adapted context interpretation article us membership function considers interpretable reinforcement ing rl suggested interpretable addition knowledge system realized bayesian structure 112 challenge future prospect challenge formulating correct model exists regardless ml trend might interesting system found fundamentally operating speciﬁc ml model dnn inspired brain not operating fundamental level similarity any guarantee model exists interpretability concerned fundamental similarity real existing system may push forward understanding ml model unprecedented way otherwise standard us ml algorithm different optimization paradigm still discovered optimization paradigm specialized speciﬁc model may contribute new aspect interpretable ml 2 feature extraction give intuitive explanation via hypothetical example classiﬁer prediction given say feature including eating pattern job residential area subject kernel function used ﬁnd strong predictor heart attack vector signiﬁcant following ax eating pattern exercise frequency sleeping pattern model considered interpretable link risk healthy habit rather say factor information drawn next signiﬁcant predictor correlation method discussed section include use correlation general sense naturally include covariance matrix correlation coefﬁcients transformation kernel function kernel function transforms vector transformed vector better distinguish different feature data example principal component pc analysis transforms vector pc ordered eigenvalue svd covariance matrix pc highest eigenvalue roughly informative feature many kernel function introduced ing canonical correlation analysis cca 113 cca vides set feature transforms original variable pair canonical variable pair pair variable best correlated not correlated pair quoted 114 feature inherently characterize object thus better explore insight ﬁner detail problem previous section interpretability research using correlation includes 60 svcca combine cca svd analyze interpretability 97 given input data set x xm input xi possibly multidimensional denote activation neuron layer l zl zl zl xm noted one output deﬁned entire input data set svcca ﬁnds relation two layer network lk zlk 1 mk k 1 2 taking input generally lk doe not entire layer svcca us svd extract informative component k us cca transform 1 2 1 1 2 2 maximum correlation ρ ρmin one svcca experiment demonstrates only 25 signiﬁcant ax k needed obtain nearly full accuracy 512 dimension besides similarity two compared layer deﬁned ρ min successful development generative adversarial work gans 115 117 generative task spawned many derivative work model able generate new image not distinguishable synthetic image perform many task including transferring style one set image another even producing new design product art study related tie exist example 118 us system perform multistage pca generative model used show natural image distribution modeled using probability 4802 ieee transaction neural network learning system vol 32 no 11 november 2021 density fundamentally difﬁcult interpret 119 demonstrated use gan estimation image distribution density resulting density show preferential accumulation density image certain feature example image featuring small object foreground distraction pixel space article suggests interpretability improved embedded deep feature space example gan sense interpretability offered better correlation density image correct identiﬁcation object consider also work cite b clustering algorithm ha used cluster input image based activation neuron network 77 120 core idea relies distance object considered distance two object short some measurement space similar possibly appeal notion human learning law association differs method provides some metric relate change one variable another two related object originate completely different domain clustering simply present similarity sensibly similar domain subset thereof 120 activation x layer cnn collected input x x fed arranged embedded two dimension visualization point visually represented input image x activation atlas duced 121 similarly us arrange some activation fact x except point represented average activation feature visualization material design 122 design pattern optical response encoded latent variable characterized variational auto encoder vae used visualize latent space medical ﬁeld also see later section 123 124 us laplacian eigenmap le pretability 125 introduces representation method autistic spectrum diagnosis c challenge future prospect section pliﬁes difﬁculty integrating mathematics human intuition extracted relevant signiﬁcant tures sometimes left still combination vector analysis come form correlation metric attempt show larities proximity interpretation may stay ematical artifact potential separation concept attained method used reorganize model within might interesting research direction lack justiﬁcation term application however progress unraveling may investment 3 sensitivity group together method rely localization gradient perturbation category method rely notion small change dx calculus neighborhood point metric space sensitivity input noise neighborhood data point some method rely locality some input let model f predicts f x accurately some denote slightly noisy version model locally faithful f x δ produce correct prediction otherwise model unfaithful clearly instability reduces reliability fong vedaldi 126 introduces pretability method emphasizes importance variation input x nn explaining network deﬁne explanation local explanation term response blackbox f some input amongst many study conducted provide experimental result effect varying input via deletion some region input likewise random pixel image deleted hence data point shifted neighborhood feature space resulting change output tested 57 pixel important prediction determined text classiﬁcation jaakkola 127 vides explanation form partitioned graph explanation produced three main step ﬁrst step involves sampling perturbed version data using vae tcavs ha also introduced technique pret representation nn layer 96 first concept activation vector cav deﬁned given input x rn feedforward layer l neuron activation layer given fl rn interested concept c example striped pattern using tcav supply set pc example corresponding striped pattern zebra clothing pattern etc negative example collection used train binary classiﬁer vl c layer l partition fl x x fl x x another word kernel function extract feature mapping set vations ha relevant information stripe cav thus deﬁned normal vector hyperplane separate positive example negative one shown fig 4 computes directional derivative sv k l x k fl x vl c obtain sensitivity model respect concept c hl k logit function class k c layer lime 14 optimizes model g g set interpretable model g minimizing loss complexity another word seek obtain optimal model ξ x l f g πx g complexity f true function want model example loss function l f g πx z f x 2 πx z example euclidean distance z vicinity equation seen desired g close f vicinity z x f z z another word noisy input z not add much loss explanation vector ξ p g x introduced 128 bayesian classiﬁer g x c p x x ξ any 1 high absolute value ξ mean component contributes signiﬁcantly decision classiﬁer positive higher value le likely contributes decision g ace algorithm 56 us tcav compute saliency score generate explanation 43 tjoa guan survey xai toward medical xai 4803 saliency method us gradient sensitivity sure 129 inﬂuence function used theoretical article also practically demonstrates understanding underlying mathematics help develop perturbative training point adversarial attack b sensitivity data set model possibly sensitive training data set xi well inﬂuence function also used understand effect removing xi some show consequent possibility adversarial attack 129 study adversarial training example found article citation seemingly random insigniﬁcant noise degrade machine decision considerably representer theorem introduced studying extent effect xi ha decision made dnn 130 c challenge future prospect seems concern locality globality concept mentioned 96 achieve global quantiﬁcation interpretability explanation must given set ples entire class rather explain individual data speciﬁc example may concern globality tcav understanding tcav perturbation method virtue stable continuity usual derivative global whole subset data set label k concept c ha shown well distinguished tcav however may want point despite claim globality possible view success tcav local since only global within label k rather within data set considered point view image processing hood data point image feature space pose rather subtle question also refer fig 5 c related illustration example rotating stretching image deleting some pixel doe position image feature space change any way control effect random noise improve robustness machine prediction way sensible human perception transition feature space one point another point belongs different class also unexplored related note gradient played important role formulating interpretability method image processing ﬁelds current trend recognizes region input space signiﬁcant gradient provide interpretability deforming region quickly degrades prediction conversely particular value region important reach certain prediction helpful since calculus exists help analyse gradient however ha shown disruptive well example imperceptible noise degrade prediction drastically see manipulation tions section since gradient also core loss optimization natural target study 4 optimization described several research seek attain interpretability via optimization method some optimization core algorithm interpretability left visual observation others mize interpretability mathematically quantitatively maximizing interpretability imate function f previously mentioned lime 14 performs optimization ﬁnding optimal model ξ f z z z vicinity x local ﬁdelity said achieved concurrently complexity ξ minimized minimized mean model interpretability maximized muse 85 take blackbox model prediction feature output decision set based optimization respect ﬁdelity interpretability unambiguity available measure interpretability optimized include size lap etc refer table ii appendix b activation optimization activation optimization used research work 38 76 78 explained previous section interpretability relies direct observation image quality optimized image not evaluated fact part coherent image emerge respect lection neuron doe demonstrate some organization information nns perspective interpretability many concept related interpretability selvaraju et al 43 conducted experiment test improvement human performance task given explanation form visualization produced ml algorithm believe might exemplary form interpretability evaluation example want compare ml algorithm mla mlb say human subject given difﬁcult classiﬁcation task attain baseline 40 accuracy repeat task different set human subject given explanation churned mla mlb accuracy attained 50 80 respectively mlb interpretable even human subject not really explain perform better given explanation interpretability may questionable brings u question kind interpretability necessary different task certainly point possibility no need uniﬁed version interpretability 1 interpretability data catalog large amount data ha crucial functioning many ml algorithm mainly input data section mention work put different emphasize treatment data arranged catalog essence kim 10 suggests create matrix whose row different task pneumonia detection column different method decision tree different depth entry performance method some gather large collection entry large matrix apart competition challenge effort aid formation database 148 149 clear problem multidimensional gigantic tabulation become not mention collection entry likely uncountably many izing interpretability mean pick latent dimension common criterion human evaluate example time constraint cognitive chunk deﬁned basic unit explanation also see deﬁnition 84 etc 4804 ieee transaction neural network learning system vol 32 no 11 november 2021 dimension reﬁned along iterative process user input enter repository b incompleteness 10 problem ness problem formulation ﬁrst posed issue interpretability incompleteness present many form impracticality produce test case difﬁculty justifying choice proxy best some scenario end suggests interpretability criterion born collective agreement majority cyclical process discovery justiﬁcations rebuttal opinion disadvantage possibility no unique convergence born situation may aggravate say two different conﬂicting faction born enough advocate advantage lie existence strong root advocacy certain choice interpretability prevents malicious intent tweaking interpretability criterion suit ad hoc purpose 2 invariance implementation invariance sundararajan et al 94 suggests implementation invariance axiomatic ment interpretability article stated following deﬁne two functionally equivalent function x fx x any x regardless implementation detail given any two network using attribution method attribution functional map importance component input way doe another word x j x j any j 1 dimension input statement easily extended method not use attribution well b input invariance illustrate using image classiﬁcation problem translating image also translate demarcating area provides explanation choice classiﬁcation correspondingly clearly property desirable ha proposed axiomatic invariance reliable saliency method ha also study input invariance some saliency method respect translation input x c some c 71 method studied based method 128 signal method 72 75 input invariant some attribution method integrated gradient 94 not 3 interpretabilities utility following categorization interpretability proposed 10 first evaluation human give explanation xa speciﬁc application doctor performs diagnosis human b b performs task ha given b useful explanation b performs better task suppose ml model model highly interpretable human b performs task improved performance given xa some medical segmentation work fall category well since segmentation constitute visual explanation 144 145 also see category grand challenge evaluation performed example 43 proposed applied guided backpropagation proposed 75 alexnet cnn vgg produced visualization used help human subject amazon mechanical turk identify object higher accuracy predicting voc 2007 image human subject achieved accuracy higher visualization provided guided backpropagation b evaluation involves real human simpliﬁed task used some reason another human give good explanation xa challenging possibly performance task not evaluated easily explanation requires specialized knowledge case simpliﬁed partial problem may posed xa still demanded unlike approach necessary look xa speciﬁcally interpretability evaluation bigger pool human subject hired give generic valuation xa create model answer ˆ xa compare xa generic valuation computed suppose ml model interpretable compared another ml model score better generic valuation 146 ml model given document containing conversation human making plan ml model produce report containing relevant cates word task inferring ﬁnal plan metric used interpretability evaluation example percentage predicate appear compared report believe format evaluation need not strictly like example hybrid human interactive ml classiﬁers require human user nominate feature training 147 two different standard ml compared hybrid one said interpretable another pick feature similar hybrid assuming perform similarly acceptable level c third evaluation ally grounded exist proxy deﬁned priori evaluation example sparsity 10 some article 2 5 42 44 96 97 144 145 use metric rely evaluation include many supervised learning model clearly deﬁned metric 1 dice coefﬁcients related visual interpretability 2 attribution value component canonically transformed variable see example cca value obtained dimensionality reduction method component principal nents pca corresponding eigenvalue interpretability related degree object relates feature example classiﬁcation dog ha high value feature space related four limb shape snout paw etc suitable metric use highly dependent task hand iii xai medical field ml ha also gained traction recently medical ﬁeld large volume work automated diagnosis nosis 150 see many different challenge medical ﬁeld emerged galvanized research use ml ai method amongst tjoa guan survey xai toward medical xai 4805 table iii categorization organ affected disease neuro refers any neurological neurodevelopmental neurodegenerative etc disease row arranged according focus interpretability following appl application method methodology comp comparison successful dl model 2 5 using medical segmentation however dl nn still blackbox not interpretable domain speciﬁc method special transformation denoising etc published well consider example 131 many work miccai publication medical ﬁeld question interpretability far intellectual curiosity speciﬁcally pointed interpretabilities medical ﬁelds include factor ﬁelds not consider including risk responsibility 21 151 152 medical response made life may stake leave important decision machine could not provide accountability would akin shirking responsibility altogether apart ethical issue serious loophole could turn catastrophic exploited malicious intent many work thus dedicated ing explainability medical ﬁelds 11 20 44 provide summary previous work 21 including review 25 chest radiograph sentiment analysis medicine 161 least set aside section promote awareness importance interpretability medical ﬁeld 162 163 stated directly black box strong limitation ai dermatology not capable performing customized assessment certiﬁed dermatologist used explain clinical evidence hand exposition 164 argues certain degree opaqueness acceptable might important produce empirically veriﬁed accurate result focusing much unravel recommend reader consider ﬁrst least overview interpretability medical ﬁeld apply categorization previous section ml ai medical ﬁeld table iii show rization obtained tagging 1 interpretability method incorporated either direct application existing method methodology improvement comparison interpretability method 2 organ targeted disease example brain skin etc not yet substantial number signiﬁcant medical research address interpretability refrain presenting any conclusive trend however quick overview see xai research community might beneﬁt study comparing different existing method especially informative conclusion contribute interpretability perceptive interpretability medical data could come form traditional image complex format nifti dcom contain image multiple modality even image volume ﬁculties using ml data include following medical image sometimes far le available tity common image obtaining data requires consent patient administrative barrier data also add complexity data ing large memory space requirement might prevent data input without modiﬁcation random sampling may compromise analysis possible difﬁculties data collection management include patient death due unrelated cause complication etc medical data available image may not not only data require some cialized knowledge understand lack sive understanding biological component complicates analysis example adc modality mr image isotropic version dwi some sense derivative since computed raw image collected scanner furthermore many ct mri scan presented preprocessing however without complete knowledge ﬁne detail might accidentally removed not guarantee algorithm capture correct feature 1 saliency following article consist direct application existing saliency method chexpert 6 us gradcam visualization pleural effusion radiograph cam also used interpretability brain tumor ing 153 tang et al 68 us guided feature occlusion providing complementary heatmaps cation alzheimer disease pathology integrated gradient method smoothgrad applied visualization cnn ensemble classiﬁes estrogen receptor status using breast mri 69 lrp deeplight 48 wa applied fmri data human connectome project generate heatmap visualization saliency map ha also computed using primitive gradient loss providing interpretability nn used electroencephalogram eeg sleep stage ing 154 ha even direct comparison feature map within cnn skin lesion image 155 overlaying scaled feature map top image mean interpretability some image correspond relevant feature lesion others appear explicitly capture artifact might lead prediction bias following article focused comparison popular saliency method including improved version jansen et al 159 train artiﬁcial nn classiﬁcation insomnia using physiological network pn feature relevance score computed 4806 ieee transaction neural network learning system vol 32 no 11 november 2021 several method including deeplift 57 comparison four different visualization performed 158 show different attribution different method concluded lrp guided backpropagation provide coherent attribution map alzheimer disease study basic test gradcam shap dermoscopy image melanoma classiﬁcation conducted concluding need signiﬁcant improvement heatmaps practical deployment 160 following includes slightly different focus methodological improvement top visualization 44 derived 42 43 vides saliency map form image obtained cellular electron high sity heatmap mark region macromolecular complex present multilayer cam mlcam duced 91 glioma type brain tumor localization multiinstance mi aggregation method used cnn classify breast tumor tissue microarray tma image ﬁve different task 65 example classiﬁcation histologic subtype map indicate region tma image tumor cell label responds class tumor map proposed mean visual interpretability also see activation map 66 interpretability studied corrupting image inspecting region interest roi autofocus module 67 promise improvement visual interpretability segmentation pelvic ct scan segmentation tumor brain mri using cnn us attention mechanism proposed 92 improves adaptive selection scale network see object within image correct scale adopted network performing single task human observer analyzing network understand nn properly identifying object rather mistaking combination object plus surrounding object also different formulation generation saliency map 70 deﬁnes different formula extract signal dnn visual justiﬁcation classiﬁcation breast mass textual justiﬁcation generated well 2 verbal 82 system could provide statement ha asthma risk risk refers death risk due pneumonia likewise letham et al 83 creates model called bayesian rule list provides statement stroke prediction textual justiﬁcation also provided breast mass classiﬁer system 70 argumentation theory implemented ml training process 156 extracting argument decision rule explanation prediction stroke based asymptomatic carotid stenosis risk stroke acsrs data set one indeed look closer interpretability 82 many ml able extract some humanly itive pattern system seems captured strange link asthma pneumonia link becomes clear actual explanation based real situation provided pneumonia patient also suffers asthma often sent directly intensive care unit icu rather standard ward obviously variable icu 0 1 indicates admission icu better model provide coherent explanation asthma lower article model appears not identify variable see interpretability issue not always several research vqa medical ﬁeld also developed initiative imageclef 165 166 appears center though vqa ha yet gain traction successful practical demonstration medical sector widespread adoption challenge future prospect perceptive pretability medical sector many case saliency map provided provided insufﬁcient tion respect utility within medical practice example providing importance attribution ct scan used lesion detection radiologist interested heatmaps highlighting lesion interested looking reason hemorrhage epidural subdural lesion not clear naked eye may many medically related subtlety interpretable ai researcher may need know interpretability via mathematical structure 1 predeﬁned model model help interpretability providing generic sense variable doe output variable question whether medical ﬁelds not parametric model usually designed least estimate working mechanism system simpliﬁcation based empirically observed pattern example ulas et al 131 us kinetic model cerebral blood ﬂow cbf f exp pld sipd 1 1 depends image obtained signal difference labeled image arterial blood water treated rf pulse control image function incorporated loss function training pipeline fully cnn least interpretation made partially nn model designed denoise image thus improve quality considering cbf network understands cbf interpretability problem nn ha yet resolved inherent simplicity interpretability model based linearity thus considered obviously interpretable well some example include linear combination clinical variable 100 metabolite signal magnetic resonance spectroscopy mr 106 etc linearity different model used estimation brain state discussed 107 including misinterpreted compare refers forward backward model suggested improvement linear model 82 logistic regression model picked relation tjoa guan survey xai toward medical xai 4807 asthma lower risk pneumonia death asthma ha negative weight risk predictor regression model generative discriminative machine gdm combine ordinary least square regression ridge regression handle confounding variable alzheimer disease schizophrenia data set 100 gdm parameter said interpretable since linear combination clinical variable dl ha used pet pharmacokinetic pk modeling quantify tracer target density 132 cnn ha helped pk modeling part sequence process reduce pet acquisition time output interpreted respect golden standard pk model linearized version simpliﬁed reference tissue model srtm dl method also used perform parameter ﬁtting mr 106 parametric part mr signal model speciﬁed x e fmt consists linear combination metabolite signal xm article show error measured symmetric mean absolute percentage error smape smallest metabolite cnn model used case like clinician may ﬁnd model interpretable long parameter although nn may still not interpretable model use linearity study related brain disease beyond linear model brain modeled relevant knowledge better interpretability well segmentation task detection brain midline shift performed using using cnn standard structural knowledge porated 133 template called age norm derived mean value sleep eeg feature healthy subject 157 interpretability given deviation feature unhealthy subject age norm different note rl ha applied ized healthcare particular zhu et al 134 introduces rl personalized healthcare taking siderations different group similar agent usual optimized respect policy πθ qualitatively interpreted maximization reward time choice action selected many participating agent system challenge future prospect model may simplifying intractable system full potential ml especially dnn huge number parameter may possible research direction tap onto hype predictive science following given model possible augment model new sophisticated component part component identiﬁed thus interpreted new insight naturally augmented model need comparable previous model shown clear interpretation new component correspond insighs previously missed note critique hype around potential ai leave reader 2 feature extraction vanilla cnn used 142 suggested interpretability attained using separable model separability achieved scalar variable ing giving rise weight useful interpretation 123 fmri analyzed using functional graph clustered consisting network deﬁned interpretable convolutional layer used reference nns designed problem see article citation following subcategorization od revolve around feature extraction evaluation measurement correlation used obtain feature similar previous section correlation method discrete wavelet transform used perform feature extraction tually feeding eeg data series processing nn epilepsy classiﬁcation 135 fuzzy relation analogous correlation coefﬁcient deﬁned transform method component wavelet interpreted componentwise simple illustration component fourier transform could taken much certain frequency contained time series zhang et al 136 mentioned host feature extraction method introduced maximal overlap discrete wavelet package transform modwpt also applied eeg data epilepsy classiﬁcation frame singular value decomposition introduced classiﬁcations electromyography emg data 114 pipeline involving number processing includes dwt cca svd achieving around 98 accuracy classiﬁcations amyotrophic lateral sclerosis thy healthy subject consider also article cited article particular citation emg eeg signal b clustering vae used obtain vector latent dimension predict whether subject suffer hypertrophic cardiomyopathy hcm 124 nonlinear formation used create le two dimension suggested mean interpretability skin image clustered 139 melanoma classiﬁcation using neighbor customized include cnn triplet loss queried image compared training image ranked according similarity measure visually displayed result activation map pair ha applied human genetic data shown provide robust dimensionality reduction compared pca method 137 multiple map introduced 138 performing clustering phenotype similarity data c sensitivity regression concept vector rcvs proposed along metric br score improvement tcav concept separation 140 method applied breast cancer histopathology classiﬁcation problem thermore unit ball surface sampling ubs metric duced 141 address shortcoming br score us nns classiﬁcation nodule mammographic image guidelinebased additive explanation gax duced 93 diagnosis using ct lung image pipeline includes perturbation analysis shap comparison made lime feature importance generated shap 4808 ieee transaction neural network learning system vol 32 no 11 november 2021 fig overview challenge future prospect arranged venn diagram challenge future prospect observe popular us certain method ingrained speciﬁc sector one hand hand emerging application sophisticated ml algorithm medical ml particular application recently successful dnn still young ﬁeld see fragmented experimental us existing customized interpretable method medical ml research progress tradeoff many practical factor ml method ease use ease interpretation mathematical structure possibly regarded complex contribution subject matter become clearer future research application may beneﬁt tice consciously consistently extracting interpretable information processing process systematically documented good dissemination currently feature selection extraction focused improving accuracy performance may still vast unexplored opportunity interpretability research perspective 1 reasoning cbr performs medical evaluation classiﬁcations etc comparing query case new data similar existing data database lamy et al 143 combine cbr algorithm present similarity case visually ing proxy measure user interpret observing proxy user decide take decision gested algorithm not article also asserts medical expert appreciate visual information clear system risk machine interpretation medical field 1 jumping conclusion according 82 logical ments ha asthma risk considered interpretable however example statement indicates patient asthma ha lower risk death pneumonia might strange without any clariﬁcation intermediate thought process human infer lowered risk due fact pneumonia patient asthma history tend given aggressive treatment not always assume similar humanly inferable reason behind decision furthermore pretability method lrp deconvolution guided backpropagation introduced earlier shown not work simple model linear model bringing question reliability 60 iv conclusion present survey interpretability explainability ml algorithm general place different interpretation suggested different research work distinct category general interpretabilities apply categorization medical ﬁeld some attempt made formalize interpretabilities mathematically some provide visual nation others might focus improvement task performance given explanation produced rithms section also discus related challenge future prospect fig 6 provides diagram summarizes challenge prospect manipulation explanation given image similar image generated perceptibly indistinguishable original yet produce radically different output 95 naturally signiﬁcance attribution interpretable information become unreliable furthermore explanation even manipulated ily 167 example explanation classiﬁcation cat image particular signiﬁcant value contribute tjoa guan survey xai toward medical xai 4809 prediction cat implanted image dog algorithm could fooled classifying dog image cat image risk medical ﬁeld clear even without malicious intentional manipulation noise render explanation wrong manipulation algorithm designed provide explanation also explored 168 incomplete constraint 131 loss function training fully lutional network includes cbf constraint however many constraint may play important role mechanism living organ tissue not mention applying kinetic model simpliﬁcation giving interpretation within limited constraint may place undue emphasis constraint work use predeﬁned model might suffer similar problem 100 106 132 noisy training data ground truth medical task provided professional not always absolutely correct fact news regarding ai beat human performance medical imaging diagnosis 169 indicates human judgment could brittle true even trained medical personnel might give rise classic situation risk presented large part reminder nature automation true algorithm used extract invisible pattern some success ever one ought view scientiﬁc problem correct order priority society not risk resource building machine dl model especially since due improvement understanding underlying ence might key solving root problem example higher quality mri scan might reveal key tion not visible current technology many model built nowadays might not successful simply not enough detailed information contained currently available mri scan future direction clinician practitioner visual textual explanation supplied algorithm might seem like obvious choice unfortunately detail algorithm dnns still not clearly exposed otherwise reliable dl model provides strangely wrong visual textual explanation tematic method probe wrong explanation not seem exist let alone method correct specialized education combining medical expertise applied mathematics data science etc might necessary overcome interpretable algorithm deployed medical practice human supervision still necessary interpretability information considered nothing mentary support medical practice robust way handle interpretability future direction algorithm developer researcher blackbox unblackboxed machine decision always carry some exploitable risk also clear uniﬁed notion interpretability elusive medical ml interpretability comparative study performance method useful interpretability output heatmaps displayed compared clearly including poor result best case scenario clinician practitioner recognize shortcoming interpretable method general idea handle way suitable medical practice worst case scenario inconsistency method exposed troubling trend journal publication emphasizing good result precarious thus continue interpretability research mindset open evaluation related party clinician practitioner need given opportunity fair judgment utility proposed interpretability method not ﬂooded performance metric possibly irrelevant adoption medical technology also may need shift interpretability study away study authoritative body setting standard requirement deployment model building might stiﬂe progress research though might efﬁcient way reach agreement might necessary prevent damage seeing even corporate company body demic traditional sense joined fray consider implication acknowledging machine dl might not fully mature deployment might wise deploy algorithm secondary support system leave decision traditional method might take long time humanity graduate stage might timely collect data compare machine prediction traditional prediction sort data ownership issue along way acknowledgment program collaboration alibaba nanyang technological university singapore reference 1 lee kim kim kang deep brain artiﬁcial intelligence stroke imaging stroke vol 19 no 3 pp 2017 2 ronneberger fischer brox lutional network biomedical image segmentation corr vol no 3 pp 2015 online able 3 dzindolet peterson pomranky pierce beck role trust automation reliance int comput vol 58 no 6 pp jun 2003 4 chen bentley rueckert fully automatic acute ischemic lesion segmentation dwi using convolutional neural network neuroimage vol 15 pp jun 2017 5 çiçek abdulkadir lienkamp brox ronneberger learning dense volumetric segmentation sparse annotation corr vol pp 2016 online available 6 irvin et chexpert large chest radiograph dataset uncertainty label expert comparison corr vol pp jul 2019 online available 7 milletari navab ahmadi fully tional neural network volumetric medical image segmentation corr vol pp jun 2016 online available 4810 ieee transaction neural network learning system vol 32 no 11 november 2021 8 chen papandreou kokkinos murphy yuille deeplab semantic image segmentation deep convolutional net atrous convolution fully connected crfs corr vol pp jun 2016 online available 9 kelly karthikesalingam suleyman corrado king key challenge delivering clinical impact artiﬁcial intelligence bmc vol 17 no 1 195 2019 10 kim towards rigorous science pretable machine learning 2017 online able 11 tonekaboni joshi mccradden goldenberg clinician want contextualizing explainable machine learning clinical end use corr vol pp may 2019 online available 12 herlocker konstan riedl explaining collaborative ﬁltering recommendation proc acm conf comput supported cooperat work cscw new york ny usa association computing machinery 2000 pp 13 lapuschkin wäldchen binder montavon samek müller unmasking clever han predictor assessing machine really learn nature vol 10 no 1 1096 2019 14 ribeiro singh guestrin trust explaining prediction any classiﬁer proc acm sigkdd int conf knowl discovery data mining new york ny usa association computing machinery 2016 pp 15 lipton mythos model interpretability corr vol pp mar 2016 online available 16 dosilovic brcic hlupic explainable artiﬁcial intelligence survey proc int conv inf commun electron microelectron mipro may 2018 pp 17 gilpin bau yuan bajwa specter kagal explaining explanation overview interpretability machine learning proc ieee int conf data sci adv anal dsaa 2018 pp 18 arrieta et explainable artiﬁcial intelligence xai concept taxonomy opportunity challenge toward responsible ai inf fusion vol 58 pp jun 2020 19 soekadar birbaumer slutzky cohen interface neurorehabilitation stroke neurobiol disease vol 83 pp 2015 20 holzinger langs denk zatloukal müller causability explainability artiﬁcial intelligence medicine wire data mining knowl discovery vol 9 no 4 jul 2019 21 xie gao chen outlining design space explainable intelligent system medical diagnosis corr vol pp mar 2019 online available 22 vellido importance interpretability visualization machine learning application medicine health care neural comput early access 4 2019 doi 23 topol medicine convergence human artiﬁcial intelligence nature vol 25 no 1 pp 2019 24 fernandez herrera cordon jesus marcelloni evolutionary fuzzy system explainable artiﬁcial intelligence ieee comput intell vol 14 no 1 pp 2019 25 kallianos et far come artiﬁcial intelligence chest radiograph interpretation clin vol 74 no 5 pp may 2019 26 montavon samek müller method interpreting understanding deep neural network digit signal vol 73 pp 2018 27 samek wiegand müller explainable artiﬁcial ligence understanding visualizing interpreting deep learning model corr vol pp 2017 online available 28 rieger chormai montavon hansen müller structuring neural network explainable prediction explainable interpretable model computer vision machine learning cham switzerland springer 2018 pp 29 meacham isaac nauck virginas towards able ai design development explanation machine learning prediction patient readmittance medical application ligent computing arai bhatia kapoor ed cham switzerland springer 2019 pp 30 townsend chaton monteiro extracting relational explanation deep neural network survey symbolic perspective ieee trans neural netw learn vol 31 no 9 pp 2020 31 2016 open black box ai online able 32 heinrichs eickhoff evidence machine learning algorithm medical diagnosis prediction hum brain mapping vol 41 no 6 pp apr 2020 33 brundage et toward trustworthy ai development mechanism supporting veriﬁable claim eur commission brussels belgium tech 2020 online available 34 2019 ethic guideline trustworthy ai online available 35 wang yang abdul lim designing explainable ai proc chi conf hum factor comput syst chi new york ny usa association computing machinery 2019 pp 36 bau zhou khosla oliva torralba network dissection quantifying interpretability deep visual representation proc ieee conf comput vi pattern recognit cvpr jul 2017 pp 37 zhou bau oliva torralba interpreting deep visual representation via network dissection ieee trans pattern anal mach vol 41 no 9 pp 2019 38 olah mordvintsev schubert feature visualization distill vol 2 no 11 2017 39 olah et building block interpretability tech 2020 online available 40 lundberg lee uniﬁed approach interpreting model prediction advance neural information processing system 30 guyon et ed red hook ny usa curran associate 2017 pp 41 jacovi shalom goldberg understanding convolutional neural network text classiﬁcation corr vol pp apr 2018 online available http 42 zhou khosla lapedriza oliva torralba learning deep feature discriminative localization proc ieee conf comput vi pattern recognit cvpr jun 2016 pp 43 selvaraju da vedantam cogswell parikh batra say visual nation deep network via localization corr vol pp 2016 online available 44 zhao zhou wang jiang xu cam analyzing deep model imaging data visualization medical image computing computer assisted 2018 frangi schnabel davatzikos fichtinger ed cham switzerland springer 2018 pp 45 bach binder montavon klauschen müller samek explanation classiﬁer sion relevance propagation plo one vol 10 no 7 pp 2015 46 samek binder montavon lapuschkin müller evaluating visualization deep neural network ha learned ieee trans neural netw learn vol 28 no 11 pp 2017 47 becker ackermann lapuschkin müller samek interpreting explaining deep neural network classiﬁcation audio signal corr vol 3 jul 2018 online available 48 thomas heekeren müller samek lyzing neuroimaging data recurrent deep learning model frontier vol 13 1321 2019 49 arras horn montavon müller samek relevant text document interpretable machine learning approach corr vol pp 2016 online available tjoa guan survey xai toward medical xai 4811 50 srinivasan lapuschkin hellge müller samek interpretable human action recognition compressed domain proc ieee int conf speech signal process icassp mar 2017 pp 51 eberle buttner krautli mueller valleriani montavon building interpreting deep similarity model ieee trans pattern anal mach 1 2020 52 hiley preece hick chakraborty gurram tomsett explaining motion relevance activity recognition video deep learning model pp mar 2020 online available 53 samek montavon binder lapuschkin müller interpreting prediction complex ml model evance propagation pp 2016 online available 54 2018 machine learning ai pretability online available 55 2016 deep taylor decomposition neural network online available 56 ghorbani wexler zou kim towards matic explanation advance neural information processing system 32 wallach larochelle beygelzimer fox garnett ed red hook ny usa curran associate 2019 pp 57 shrikumar greenside kundaje learning tant feature propagating activation difference corr vol pp 2017 online available 58 zintgraf cohen adel welling ing deep neural network decision prediction difference analysis corr vol pp 2017 online available 59 zhou zhu ye qiu jiao weakly vised instance segmentation using class peak response corr vol no 2 pp jun 2018 online available 60 kindermans et learning explain neural work patternnet patternattribution proc iclr may 2018 pp 61 smilkov thorat kim viégas wattenberg smoothgrad removing noise adding noise corr vol pp jun 2017 62 arras montavon müller samek explaining recurrent neural network prediction sentiment analysis proc workshop comput approach subjectivity sentiment social medium anal copenhagen denmark association computational linguistics 2017 pp 63 karpathy johnson visualizing understanding recurrent network pp jun 2015 online available 64 paschali conjeti navarro navab ity robustness investigating medical imaging network using adversarial example medical image computing computer assisted 2018 frangi schnabel davatzikos fichtinger ed cham switzerland springer 2018 pp 65 couture marron perou troester niethammer multiple instance learning heterogeneous image training cnn histopathology medical image computing computer assisted 2018 frangi schnabel davatzikos fichtinger ed cham switzerland springer 2018 pp 66 li dvornek zhuang ventola duncan brain biomarker interpretation asd using deep learning fmri medical image computing computer assisted miccai 2018 frangi schnabel davatzikos fichtinger ed cham switzerland springer 2018 pp 67 qin et autofocus layer semantic segmentation corr vol pp jun 2018 online available 68 tang et interpretable classiﬁcation alzheimer disease pathology convolutional neural network pipeline nature vol 10 no 1 2173 2019 69 papanastasopoulos et explainable ai medical imaging cnn ensemble classiﬁcation estrogen tor status breast mri proc spie vol 11314 pp mar 2020 70 lee kim ro generation multimodal tion using visual word constraint model explainable diagnosis interpretability machine intelligence medical image computing multimodal learning clinical decision support suzuki et ed cham switzerland springer 2019 pp 71 kindermans et un reliability saliency method cham switzerland springer 2019 pp 72 zeiler fergus visualizing understanding lutional network corr vol pp 2013 online available 73 mahendran vedaldi understanding deep image sentations inverting corr vol pp 2014 online available 74 dosovitskiy brox inverting convolutional network convolutional network corr vol pp apr 2015 online available 75 springenberg dosovitskiy brox miller striving simplicity convolutional net corr vol pp apr 2015 76 erhan bengio courville vincent ing feature deep network dept informatique recherche operationnelle univ montreal montreal qc canada tech 1341 jun 2009 77 nguyen yosinski clune multifaceted feature alization uncovering different type feature learned neuron deep neural network corr vol pp may 2016 online available 78 yosinski clune nguyen fuchs lipson understanding neural network deep visualization corr vol pp jun 2015 online available 79 szegedy et going deeper convolution proc ieee conf comput vi pattern recognit cvpr jun 2015 pp 80 meyes lu de puiseau meisen ablation study artiﬁcial neural network corr vol pp 2019 online available 81 meyes de puiseau meisen hood neural network characterizing learned resentations functional neuron population network tions pp may 2020 online available 82 caruana lou gehrke koch sturm elhadad intelligible model healthcare predicting pneumonia risk hospital readmission proc acm sigkdd int conf knowl discovery data mining kdd new york ny usa ation computing machinery 2015 pp 83 letham rudin mccormick madigan pretable classiﬁers using rule bayesian analysis building better stroke prediction model ann appl vol 9 no 3 pp 2015 84 lage et evaluation nation corr vol pp 2019 online available 85 lakkaraju kamar caruana leskovec faithful customizable explanation black box model proc conf ai ethic soc new york ny usa association computing machinery 2019 pp 86 lei barzilay jaakkola rationalizing neural prediction proc conf empirical method natural lang process austin tx usa association computational linguistics 2016 pp 87 guo anderson pearson farrell neural work interpretation via ﬁne grained textual summarization corr vol pp 2018 online available 88 agrawal et vqa visual question answering int comput vol 123 no 1 pp may 2017 89 lu yang batra parikh hierarchical visual question answering proc int conf neural inf process syst red hook ny usa curran associate 2016 pp 90 da et visual dialog proc ieee conf comput vi pattern recognit cvpr jul 2017 pp 91 izadyyazdanabadi et ture localization confocal laser endomicroscopy glioma image medical image computing computer assisted miccai 2018 frangi schnabel davatzikos fichtinger ed cham switzerland springer 2018 pp 4812 ieee transaction neural network learning system vol 32 no 11 november 2021 92 bahdanau cho bengio neural machine translation jointly learning align translate 2014 online available 93 zhu ogino additive explanation diagnosis lung nodule interpretability machine intelligence medical image computing multimodal learning clinical decision support suzuki et ed cham switzerland springer 2019 pp 94 sundararajan taly yan axiomatic attribution deep network proc int conf mach vol 70 2017 pp 95 ghorbani abid zou interpretation neural network fragile proc aaai jul 2019 pp 96 kim et interpretability beyond feature attribution quantitative testing concept activation vector tcav proc icml vol 80 dy krause ed pmlr 2018 pp 97 raghu gilmer yosinski svcca gular vector canonical correlation analysis deep learning dynamic interpretability proc int conf neural inf process syst red hook ny usa curran associate 2017 pp 98 tishby zaslavsky deep learning information bottleneck principle proc ieee inf theory workshop itw apr 2015 pp 99 tishby opening black box deep neural network via information corr vol pp apr 2017 online available 100 varol sotiras zeng davatzikos generative criminative model multivariate inference statistical mapping medical imaging medical image computing computer assisted 2018 frangi schnabel davatzikos fichtinger ed cham switzerland springer 2018 pp 101 alain bengio understanding intermediate layer using ear classiﬁer probe pp 2018 online available 102 hastie tibshirani generalized additive model stat vol 1 no 3 pp 1986 103 lou caruana gehrke intelligible model tion regression proc acm sigkdd int conf knowl discovery data mining kdd new york ny usa association computing machinery 2012 pp 104 lou caruana gehrke hooker accurate intelligible model pairwise interaction proc acm sigkdd int conf knowl discovery data mining kdd new york ny usa association computing machinery 2013 623 105 arik pﬁster prototypical learning towards interpretable conﬁdent robust deep neural network corr vol pp 2019 online available 106 hatami sdika ratiney magnetic resonance spectroscopy quantiﬁcation using deep learning corr vol 3 jun 2018 online available 107 haufe et interpretation weight vector ear model multivariate neuroimaging neuroimage vol 87 pp 2014 108 schütt arbabzadah chmiela müller tkatchenko insight deep tensor neural network nature vol 8 no 1 13890 apr 2017 109 schütt gastegger tkatchenko müller insight interpretable atomistic neural work cham switzerland springer 2019 pp 110 liaskos tsioliaridou nie pitsillides ioannidis akyildiz interpretable neural network conﬁguring programmable wireless environment corr vol pp may 2019 online available 02495 111 bede fuzzy system membership function interpretable neural network fuzzy technique theory cation kearfott batyrshin reformat ceberio kreinovich ed cham switzerland springer 2019 pp 112 kaiser otte runkler ek pretable dynamic model reinforcement learning corr vol pp jul 2019 online available 113 hardoon szedmak cal correlation analysis overview application ing method neural vol 16 no 12 pp 2004 114 hazarika barthakur dutta bhuyan based algorithm variability stability measurement feature extraction fusion pattern recognition biomed signal process control vol 47 pp 2019 115 goodfellow et generative adversarial net advance neural information processing system 27 ghahramani welling cortes lawrence weinberger ed red hook ny usa curran associate 2014 pp 116 arjovsky chintala bottou wasserstein generative adversarial network proc int conf mach vol 70 2017 pp 117 zhu park isola efros unpaired translation using adversarial network proc ieee int conf comput vi iccv 2017 pp 118 zhu suri kulkarni chen duan kuo interpretable generative model handwritten digit si proc ieee int conf image process icip 2019 pp 119 krusinga shah zwicker goldstein jacob understanding un interpretability natural image tions using generative model corr vol pp 2019 online available 120 karpathy 2014 visualization cnn code online available 121 carter armstrong schubert johnson olah 2019 exploring neural network activation atlas online able 122 cheng xu wen liu probabilistic tation inverse design metamaterials based deep generative model learning strategy adv vol 31 no 35 2019 art no 1901111 123 yan zhu duda solarz sripada koutra groupinn interpretable neural network ﬁcation limited noisy brain data proc acm sigkdd int conf knowl discovery data mining new york ny usa association computing machinery 2019 pp 124 bifﬁet learning interpretable anatomical feature deep generative model application cardiac remodeling corr vol pp jul 2018 online available 125 wang zhang huang shen liu representation autism spectrum disorder identiﬁcation medical image computing computer assisted miccai 2018 frangi schnabel davatzikos lópez fichtinger ed cham switzerland springer 2018 pp 126 fong vedaldi interpretable explanation black box meaningful perturbation corr vol pp 2017 online available 127 jaakkola causal framework explaining prediction el proc conf empirical method natural lang process copenhagen denmark association computational linguistics 2017 pp 128 baehrens schroeter harmeling kawanabe hansen müller explain individual classiﬁcation decision mach learn vol 11 pp 2010 129 koh liang understanding prediction via inﬂuence function proc int conf mach vol 70 2017 pp 130 yeh kim yen ravikumar representer point selection explaining deep neural network proc int conf neural inf process syst red hook ny usa curran associate 2018 pp 131 ulas tetteh kaczmarz preibisch menze deepasl kinetic model incorporated loss denoising arterial spin labeled mri via deep residual learning medical image computing computer assisted 2018 frangi schnabel davatzikos fichtinger ed cham switzerland springer 2018 pp 132 scott et reduced acquisition time pet netic modelling using simultaneous proof concept cerebral blood flow metabolism vol 39 no 12 pp 2019 133 pisov et incorporating structural knowledge cnns brain midline shift detection interpretability machine intelligence medical image computing multimodal learning clinical decision support suzuki et ed cham switzerland springer 2019 pp tjoa guan survey xai toward medical xai 4813 134 zhu guo xu liao yang huang driven reinforcement learning personalized mhealth intervention medical image computing computer assisted miccai 2018 frangi schnabel davatzikos fichtinger ed cham switzerland springer 2018 pp 135 kocadagli langari classiﬁcation eeg signal epileptic seizure using hybrid artiﬁcial neural network based wavelet transforms fuzzy relation expert syst vol 88 pp 2017 136 zhang chen li classiﬁcation ictal eeg using modwpt dimensionality reduction rithms comparative study biomed signal process control vol 47 pp 2019 137 li cerise yang han application human genetic data bioinf comput vol 15 no 4 2017 art no 1750017 138 xu jiang hu li visualization genetic phenotype similarity multiple map laplacian larization bmc med genomics vol 7 no 2 2014 139 codella lin halpern hind feris smith collaborative chai interpretable melanoma classiﬁcation dermoscopic image understanding interpreting machine learning medical image computing application stoyanov et ed cham switzerland springer 2018 pp 140 graziani andrearczyk müller regression concept tor bidirectional explanation histopathology understanding interpreting machine learning medical image computing application stoyanov et ed cham switzerland springer 2018 pp 141 yeche harrison berthier ubs metric concept vector interpretability applied radiomics interpretability machine intelligence medical image computing multimodal learning clinical decision support suzuki et ed cham switzerland springer 2019 pp 142 cerna et interpretable neural network dicting mortality risk using electronic health record corr vol pp 2019 online available 143 lamy sekar guezennec bouaud séroussi explainable artiﬁcial intelligence breast cancer visual based reasoning approach artif intell vol 94 pp mar 2019 144 choi kwon lee kim paik ensemble deep convolutional neural network prognosis ischemic stroke brainlesion glioma multiple sclerosis stroke traumatic brain injury crimi menze maier reyes winzeck handel ed cham switzerland springer 2016 pp 145 maier handel predicting stroke lesion clinical come random forest brainlesion glioma multiple sclerosis stroke traumatic brain injury crimi menze maier reyes winzeck handel ed cham switzerland springer 2016 pp 146 kim chacha shah inferring robot task plan human team meeting generative modeling approach based prior proc aaai conf artif intell palo alto ca usa aaai press 2013 pp 147 cheng bernstein flock hybrid learning classiﬁers proc acm conf comput supported cooperat work social comput new york ny usa association computing machinery 2015 pp 148 kuhlmann et ducible seizure prediction human intracranial eeg brain vol 141 pp 2018 149 wiener sommer ives poldrack litt enabling open data ecosystem neuroscience neuron vol 92 no 4 929 2016 150 jiang et artiﬁcial intelligence healthcare past present future stroke vascular vol 2 no 4 pp 2017 151 cassel jameton dementia elderly analysis medical responsibility ann intern vol 94 no 6 pp jun 1981 152 croskerry cosby graber singh diagnosis interpreting shadow london crc press 2017 153 pereira meier alves reyes silva automatic brain tumor grading mri data using convolutional neural network quality assessment understanding preting machine learning medical image computing tions stoyanov et ed cham switzerland springer 2018 pp 154 vilamala madsen hansen deep convolutional neural network interpretable analysis eeg sleep stage scoring proc ieee int workshop mach learn signal process mlsp 2017 pp 155 van molle de strooper verbelen vankeirsbilck simoens dhoedt visualizing convolutional neural work improve decision support skin lesion classiﬁcation understanding interpreting machine learning medical image computing application stoyanov et ed cham switzerland springer 2018 pp 156 prentzas nicolaides kyriacou kakas pattichis integrating machine learning symbolic reasoning build explainable ai model stroke prediction proc ieee int conf bioinf bioeng bibe 2019 pp 157 sun et brain age electroencephalogram sleep neurobiol aging vol 74 pp 2019 158 eitel ritter testing robustness attribution method convolutional neural network alzheimer disease classiﬁcation interpretability machine intelligence medical image computing multimodal learning clinical decision support suzuki et ed cham switzerland springer 2019 pp 159 jansen penzel hodel breuer spott krefting network physiology insomnia patient assessment relevant change network topology interpretable machine learning el chaos interdiscipl nonlinear vol 29 no 12 2019 art no 123129 160 young booth simpson dutton shrapnel deep neural network dermatologist interpretability machine ligence medical image computing multimodal learning clinical decision support suzuki et ed cham switzerland springer 2019 pp 161 zucco liang fatta cannataro explainable sentiment analysis application medicine proc ieee int conf bioinf biomed bibm 2018 pp 162 langlotz et roadmap foundational research artiﬁcial intelligence medical imaging 2018 academy workshop radiology vol 291 no 3 pp jun 2019 163 gomolin netchiporouk gniadecki litvinov ﬁcial intelligence application dermatology stand frontier vol 7 100 mar 2020 164 london artiﬁcial intelligence medical decision accuracy versus explainability hastings center vol 49 no 1 pp 2019 165 hasan ling farri liu lungren müller overview imageclef 2018 medical domain visual question answering task proc clef working note ceur workshop avignon france 2018 online available 166 ben abacha hasan datla liu müller overview medical visual question answering task imageclef 2019 proc clef working note ceur workshop lugano switzerland 2019 online available 167 dombrowski alber anders ackermann müller kessel explanation manipulated geometry blame advance neural information processing system 32 wallach larochelle beygelzimer fox garnett ed red hook ny usa curran associate 2019 pp 168 lakkaraju bastani fool manipulating user trust via misleading black box explanation proc conf ai ethic 2020 pp 169 liu et detecting cancer metastasis gigapixel pathology image corr vol pp mar 2017 online available