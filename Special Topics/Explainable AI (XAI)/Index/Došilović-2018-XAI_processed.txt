mipro 2018 may 2018 opatija croatia explainable artificial intelligence survey filip karlo mario nikica student university ofzagreb faculty electrical engineering computing zagreb croatia university zagreb faculty electrical engineering computing zagreb croatia abstract last decade availability large datasets com puting power machine learning system achieved super human performance wide variety task example rapid development seen image recognition speech analysis strategic game planning many problem many model lack transparency interpretability lack thereof major drawback many application healthcare finance rationale model decision requirement trust light issue explainable artificial intelligence xai ha become area interest research community paper summarizes recent development xai supervised learning start discussion connection artificial general intelligence give proposal research direction keywords explainable artificial intelligence interpretability explainability comprehensibility introduction last decade especially since 2012 artificial intelligence ai machine learning ml system achieved super human performance many task previously thought computationally unattainable 1 advance field achieved due rise available information major hardware improvement combined new optimization algorithm also attribute advancement library allowed developer researcher quickly code test model improvement speech recognition image classification object detection classical board game texas many led proliferation percolation application outside research lab mostly area supervised learning also saw advance application critical area medicine mance car government bioinformatics churn prediction content recommendation application also brought attention crucial problem future application addition extension aforementioned area also include cognitive assistance interpretable science reliable ml 2 3 pervasive utilization system significantly transform social landscape world change include many ethical challenge society adapt fast order steer development favorable direction outcome automation significantly change job market 2 may lead unfair wealth distribution content recommendation 4 generation fake content 5 coupled technology deeply impact social dynamic 6 many thing prescribed algorithm affect human life way maybe unimagined people need trust order accept prescription system must like human satisfy many criterion assurance order boost trust 2 unbiasedness fairness reliability safety explanatory justifiability privacy usability etc among human assurance assumed due bias towards human decision making since people social creature accustomed life human community artificial intelligence due novel status life well human making cause much skepticism rightfully deep learning one successful machine learning approach supervised learning ha criticized 7 working well approximation answer often not fully trusted pointing model vulnerability language vision model spoofability biasedness demonstrated visual recognition 8 9 natural language processing 10 11 no robust solution ha found problem far potential ethical pitfall addressed soon possible since inactivity could lead unforeseeable split difference future society european union introduced right explanation general data protection regulation gdpr 12 attempt remedy potential problem rising importance algorithm since aforementioned trust criterion hard formalize quantify usually criterion interpretability explainability used intermediate goal afterwards following stage system explanation checked ifthey satisfy desirable trust criterion generally abstracted explanation utilized finding useful property generating hypothesis process causal relationship crucial application science well future artificial general intelligence agi generated hypothesis basis automated manual experimentation knowledge discovery optimization view supported 13 encompasses checking satisfaction optimization ethical outcome due technology assisted automated scientific discovery transferring skill etc mentioned 3 14 16 previous overview survey interpretability machine learning given 2 3 14 15 17 19 210 authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply paper survey advance interpretability explainability machine learning model supervised learning paradigm much recent work area deep learning due remarkable performance gain model one hand intrinsic opaqueness hand paper organized follows section 2 deal preliminary definition section 3 categorize work method interpretability section 4 offer discussion current state research field list future research idea section 5 concludes paper ii preliminary definition section offer defmitions trust interpretability comprehensibility explainability trust defmed 2 psychological state agent willingly securely becomes vulnerable depends trustee taken consideration characteristic ofthe trustee author 15 claim unlike normal ml objective function hard formalize defmitions criterion crucial trust acceptance view backed 2 3 case incomplete problem formalization interpretability used fallback proxy criterion however no unique defmition interpretability 3 15 3 interpretability found not monolithic concept fact reflects several distinct idea many paper interpretability proclaimed axiomatically author 15 define interpret mean explain present understandable term interpretability context ml system ability explain present understandable term human iii method interpretability explainability two category approach interpretability explainability integrated transparency 3 one property enable interpretability transparency wa traditional first step protection right institution analogy ported algorithmic concern unfairness discrimination 20 model ai becoming much complex institution becomes hard find meaningful explanation user might able understand also human thinking including not transparent u justification form explanation interpretation may differ actual decision mechanism addition predictive performance transparency conflicting objective model 21 22 7 stated not clear much transparency matter long run system robust may not necessary part system transparency good debuggability interpretability extract information already learned model doe not precisely depend model work advantage approach doe not impact performance model treated bb similar mode people make justification decision without fully knowing real functioning making mechanism however special care must taken order avoid system generate plausible misleading explanation explanation could satisfy law like gdpr problem checking veracity interpretability explainability often used interchangeably literature some paper make distinction 17 interpretation mapping abstract concept domain human make sense explanation collection feature interpretable domain contributed given example produce decision edward veale 20 split explanation centric notion correspond definition interpretability explainability 17 similar role 15 take global local interpretability respectively view see gdpr cover only explainability comprehensibility 14 used literature synonym interpretability transparency 3 used synonym model interpretability some sense understanding working logic model tree based model rule based model linear model intrinsically linear model none aforementioned definition specific restrictive enough enable formalization implicitly depend expertise preference contextual variable figure 8 integrated intepretability best explanation simple model model perfectly represents easy understand 23 approach limited model family lower complexity flexibility linear model decision tree rule hand 211 authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply model family artificial neural network ann support vector machine svm boosted tree random forest considered opaque complexity prevents user tracing logic behind prediction latter often considered box dealt manner transparency performance conceptually depicted fig majority work done classification task different constraint imposed model order increase interpretability model size sparsity monotonicity 14 18 some constraint used literature even choice model family representation considered constraint model affect interpretability transparent model interpretable explainable study interpretability rule table tree algorithm classification 24 25 latter paper decision table found easiest use inexperienced user model size general ha negative impact interpretability answer time confidence freitas 14 give overview work comprehensible classification model transparency play major role interpretability decision tree classification rule decision table nearest neighbor bayesian network classifier discussed monotonicity constraint advocated improving model transparency two pure transparent hybrid pure transparent approach restricted use model family considered transparent evolutionary programming wa used 26 search set interpretable classification rule small number rule condition interpretable decision set 27 set independent rule since rule applied independently interpretation simple model found optimizing objective take account accuracy interpretability oblique treed sparse additive model predictive model proposed 28 achieved competitive performance kernel svms providing interpretable model prototype selection 29 us set covering optimization problem achieve sparsity sample classification minimal set prototype selected order get good nearest neighbor classifier wa tested recognizing handwritten digit showed reasonable performance hybrid approach combine transparent model family method order get appropriate model interpretability predictive performance combination logistic regression svms wa used credit scoring 30 order improve accuracy initial interpretable model learning hybrid classifier wa utilized 31 learn hybrid tree certain leaf substituted classifier boosting accuracy expense interpretability 212 method hardware improvement increased availability data predictive performance benefit using complex opaque model increased however interpretability explainability issue properly addressed approach start trained predictor sometimes used training data some method deal interpretability others explainability according definition section method work only input output bb model use idiosyncrasy some representation interpretability transparent proxy model approach fmds interpretable model globally approximates prediction model approach offer interpretability explainability 32 method wa used learn single decision tree ensemble decision tree learned model wa accurate decision tree learned directly data rule extracted svm 33 make interpretable model credit scoring interpretability wa gained only small loss performance compared svms symbolic rule extracted neural network ensemble 34 bayesian regression mixture multiple elastic net wa proposed 35 used dnn svm random forest explain individual decision look model vulnerability image recognition text classification model vulnerability tested adversarial example adversarial training scheme wa used dnns order increase interpretability 36 using adversarial example wa found normally trained dnns neuron not detect semantic part only discriminative part patch also representation not robust code visual concept adversarial training scheme representation interpretable enabling trace outcome influential neuron transparent way making prediction indicative approach give weaker notion interpretability bb model method some aspect model functioning elicited conceptual representation visualization technique data like 37 used 38 visualization technique based trained deconvolutional network wa created visualization intermediate layer convolutional neural network insight visualization enabled creation improved architecture outperformed existing approach time similar wa done recurrent neural network 39 language model used interpretable benchmark experiment revealed existence interpretable cell keep track range dependency text work wa done 40 interpreting neural model natural language processing visualization unit salience visualization method based sensitivity analysis wa proposed 41 effect model response inferred visualization barplots feature importance variable effect characteristic curve experiment performed neural authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply network ensemble svm model method auditing indirect influence bb model wa presented 42 procedure find indirect influence attribute output related feature even attribute direct influence tamagnini et al 43 created pedagogical system rivelo interpretation bb classifier using level explanation user interactively explore set explanation create mental model 2 explainability method mostly output prediction also explanation form feature importance making decision relevance propagation sensitivity analysis presented 16 explain prediction deep learning model term input variable 44 deep taylor decomposition propagates explanation output ofdnn contribution input method capturing degree influence input output ofthe system wa presented 45 local approximation method shapley additive explanation shap wa used 23 explain prediction f x single input shap unified framework value estimation additive feature attribution generalizes several work literature 45 49 efficient variant proposed dnn identifies content image generates caption wa described 50 word caption explanation generated form highlighted relevant region ofthe input image method give explanation form visualization text example etc dnns trained visual question answering explaining human activity 51 justification decision given textually evidence image wa emphasized using attention mechanism textual explanation generated together visual classification using dnn 52 reinforcement learning based discriminative loss function wa used explanation model iv discussion introduction mentioned utility abstracted explanation finding useful property generating hypothesis process important science well future artificial general intelligence agi system generated hypothesis reasoned experimented leading bootstrapped process iterative improvement however author 19 point approach listed paper only enable explanation decision instead actually generating user hypothesis generation experimentation reasoning research direction interesting science agi bridging gap seamless integration learning reasoning 53 54 successful implementation would enable automatic generation interpretation explanation reasoning also seen researcher use different name similar identical concept definition vocabulary fixed community order enable easier transfer result information factor criterion formalized since concept ambiguous need split smaller specific constituent model framework human trust found extension work 2 done similar approach proposed 15 interpretability lack empirical study measure interpretability also richer loss function need developed take account criterion performance real world machine learning work ha revolved around scalar objective problem talking multicriteria some criterion not even explicitly given form optimization objective research extension interpretability research reinforcement learning another potential venue pedagogical interactive approach 43 promising enable people create mental model complex algorithm 20 boost trust increasing familiarity model decision conclusion evident problem well implication using ai current form scenario several front people yet see major impact application judicial governmental fmancial autonomous transport some time already human life influenced algorithmic content recommendation shape opinion taste greater spread ai application problem likely become pressing issue trust boosted specific criterion prominent problem incompleteness problem formalization barrier straightforward optimization approach some notion complex multidimensional ambiguous hard put formal way interpretability explainability offer abstracted explanation finding checking reasoning useful property used not only verifying trust criterion also scientific discovery future agi system reference 1 lecun bengio hinton deep learning nature vol 521 no 7553 pp may 2015 2 israelsen 1 assure going definition case survey algorithmic assurance trust relationship c stat 2017 3 lipton mythos model interpretability c stat jun 2016 4 nguyen hui harper terveen konstan exploring filter bubble effect using recommender system content diversity proceeding international conference world wide web new york ny usa 2014 pp 5 kumar sotelo kumar de brebisson bengio obamanet text c 2017 6 kucharski study epidemiology fake news nature vol 540 525 2016 7 marcus deep learning critical appraisal arxivi 8010063 1 c stat 2018 213 authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 8 szegedy et intriguing property neural network presented international conference learning representation 2014 pp 9 crawford opinion artificial intelligence white guy problem new york time 10 bolukbasi chang zou saligrama kalai man computer programmer woman homemaker debiasing word embeddings advance neural information processing system 29 barcelona spain 2016 ii wolf miller grodzinsky seen coming comment tay wider implication sigcas comput soc vol 47 no 3 pp 2017 12 goodman flaxman european union regulation algorithmic explanation arxiv 160608813 c stat jun 2016 13 lake ullman tenenbaum gershman building machine learn think like people behav brain vol 40 ed 2017 14 freitas comprehensible classification model position paper sigkdd explor newsl vol 15 no pp mar 2014 15 kim towards rigorous science interpretable machine learning c stat 2017 16 samek wiegand muller explainable artificial intelligence understanding visualizing interpreting deep learning model itu ict discov spec issue impact artif intell ai commun netw vol pp 2017 17 montavon samek muller method interpreting understanding deep neural network digit signal vol 73 pp 2018 18 marten vanthienen verbeke baesens performance classification model user perspective decis support vol no 4 pp 20 19 doran schulz besold doe explainable ai really mean new conceptualization perspective arxiv 171000794 c 2017 20 edward veale slave algorithm explanation probably not remedy looking social science research network rochester ny ssrn scholarly paper id 2972855 may 20 21 jin sendhoff multiobjective machine learning overview case study ieee trans syst man cybern part c appl vol 38 no 3 pp may 2008 22 freitas critical review optimization data mining position paper sigkdd explor newsl vol 6 no 2 pp 2004 23 lundberg lee unified approach interpreting model prediction arxivl 70507874 c stat may 2017 24 allahyari lavesson assessment classification model understandability diva 20 25 huysmans dejaeger mues vanthienen baesens empirical evaluation comprehensibility decision table tree rule based predictive model decis support vol 51 no pp apr 20 26 cano zafra ventura interpretable classification rule mining algorithm inf vol 240 pp 2013 27 lakkaraju bach leskovec interpretable decision set joint framework description prediction proceeding acm sigkdd international conference knowledge discovery data mining new york ny usa 2016 pp 28 wang fujimaki motohashi trading interpretability accuracy oblique treed sparse additive model proceeding acm sigkdd international conference knowledge discovery data mining new york ny usa 2015 pp 29 bien tibshirani prototype selection interpretable classification ann appl vol 5 no 4 pp 201 30 van gestel baesens van dijcke suykens garcia alderweireld linear credit scoring combining logistic regression support vector machine credit risk vol 2005 3 piltaver lustrek gam learning ofaccurate comprehensible classifier case study 2014 32 assche blockeel seeing forest tree learning comprehensible model ensemble machine learning ecml 2007 2007 pp 33 marten baesens van gestel vanthienen comprehensible credit scoring model using rule extraction support vector machine eur oper vol 183 no 3 pp 2007 34 zhou jiang chen extracting symbolic rule trained neural network ensemble al vol 16 no pp 2003 35 guo zhang lin huang xing towards interrogating discriminative machine learning model c stat may 2017 36 dong su zhu bao towards interpretable deep neural network leveraging adversarial example c 2017 37 van der maaten hinton visualizing data using sne mach learn vol 9 no nov pp 2008 38 zeiler fergus visualizing understanding convolutional network computer vision eccv 2014 2014 39 karpathy johnson visualizing understanding recurrent network c jun 2015 40 li chen hovy jurafsky visualizing understanding neural model nlp proceeding 20 conference north american chapter association computational linguistics human language technology 2016 pp 41 cortez embrechts using sensitivity analysis visualization technique open black box data mining model inf vol 225 pp mar 2013 42 adler et auditing model indirect influence knowl infsyst vol 54 no pp 2018 43 tamagnini krause dasgupta bertini interpreting classifier using visual explanation proceeding workshop data analytics new york ny usa 2017 44 montavon lapuschkin binder samek muller explaining nonlinear classification decision deep taylor decomposition pattern vol 65 pp 21 may 2017 45 datta sen zick algorithmic transparency via quantitative input influence theory experiment learning system 2016 ieee symposium security privacy sp 2016 pp 46 ribeiro singh guestrin trust explaining prediction any classifier proceeding acm sigkdd international conference knowledge discovery data mining new york ny usa 2016 47 shrikumar greenside kundaje learning important feature propagating activation difference c apr 2017 48 strumbelj kononenko explaining prediction model individual prediction feature contribution knowllnf syst vol 41 no 3 pp 2014 49 bach binder montavon klauschen muller samek explanation classifier decision relevance propagation plo one vol 10 no 7 jul 2015 214 authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 50 xu et show attend tell neural image caption generation visual attention international conference machine learning 2015 pp 51 park hendricks akata schiele darrell rohrbach attentive explanation justifying decision pointing evidence c 2016 52 hendricks akata rohrbach donahue schiele darrell generating visual explanation computer vision eccv 2016 2016 pp 53 besold kiihnberger towards integrated symbolic system aj two research program helping bridge gap bioi inspired cogn vol 14 pp ocl 2015 54 besold et learning reasoning survey interpretation c 2017 215 authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply