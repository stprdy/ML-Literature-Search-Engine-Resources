artiﬁcial intelligence 296 2021 103473 content list available sciencedirect artiﬁcial intelligence want explainable artiﬁcial intelligence xai stakeholder perspective xai conceptual model guiding interdisciplinary xai research markus langer daniel oster timo speith b c holger hermann c lena kästner b eva schmidt e andreas sesing f kevin baum b c department psychology saarland university saarbrücken germany b institute philosophy saarland university saarbrücken germany c department computer science saarland university saarbrücken germany institute intelligent software guangzhou china e institute philosophy political science technical university dortmund germany f institute legal informatics saarland university saarbrücken germany r c l e n f b r c article history received 1 may 2020 received revised form 8 february 2021 accepted 9 february 2021 available online 15 february 2021 keywords explainable artiﬁcial intelligence explainability interpretability explanation understanding interdisciplinary research interaction previous research explainable artiﬁcial intelligence xai suggests main aim explainability approach satisfy speciﬁc interest goal expectation need demand regarding artiﬁcial system call stakeholder desideratum variety context however literature xai vast spread across multiple largely disconnected discipline often remains unclear explainability approach supposed achieve goal satisfying stakeholder desideratum paper discus main class stakeholder calling explainability artiﬁcial system review desideratum provide model explicitly spell main concept relation necessary consider investigate evaluating adjusting choosing developing explainability approach aim satisfy stakeholder desideratum model serve researcher variety different discipline involved xai common ground emphasizes interdisciplinary potential evaluation development explainability approach 2021 elsevier right reserved introduction background motivation related work explainable artiﬁcial intelligence xai burgeoning multidisciplinary area research general xai perceived topic research ﬁeld concerned developing approach explain make artiﬁcial system understandable human stakeholder paper wa funded volkswagen foundation grant az 95143 az 98509 az 98510 az 98511 az 98512 az 98513 az 98514 explainable intelligent system eis dfg grant 389792660 part trr 248 erc advanced grant 695614 powver author thank anonymous reviewer feedback initial draft paper paper part special issue explainable ai corresponding author address speith 1 markus langer daniel oster timo speith contributed equally article share ﬁrst authorship 2021 elsevier right reserved langer oster speith et al artiﬁcial intelligence 296 2021 103473 put several central aspect focus xai research first artiﬁcial system primary object tigation system range system following predeﬁned set rule expert system system relying machine learning insight xai research become important system plex allow human oversight inherently opaque precludes human insight 8 second view xai emphasizes importance approach enable provide insight artiﬁcial system functioning output approach call explainability approach encompass method procedure strategy provide explanatory information helping u better understand artiﬁcial system third decisive need xai human stakeholder user developer regulator 2 whose interest goal expectation need demand regarding artiﬁcial system fair trustworthy system call greater understandability artiﬁcial system call conglomeration stakeholder interest goal expectation need demand regarding artiﬁcial system stakeholder desideratum large part previous xai research wa mainly concerned developing new explainability approach without evaluating whether method useful satisfy stakeholder desideratum except maybe desideratum developer fact only minority paper concerned explainability approach also evaluated proposed method contrast nowadays increasing number researcher strongly suggest putting human stakeholder center attention evaluating developing explainability approach instance researcher proposed comprehensively examine perspective stakeholder involved discussion around xai introduced evaluation method metric systematically empirically investigate effect explainability approach human stakeholder desideratum paper reinforces extends focus human stakeholder well development evaluation explainability approach provides three main contribution first propose evaluating adjusting ing developing explainability approach research need pay attention stakeholder speciﬁc desideratum given context crucial success explainability approach depends well satisfy ata current measure metric focus well explainability approach calibrate trust much increase performance see 17 however two many desideratum driving xai research though research investigates class stakeholder hold essential desideratum xai lack research identifying deﬁning empirically investigating desideratum let alone research link explainability approach suitable satisfaction paper identiﬁes desideratum different class holder call systematic empirical research investigating explainability approach facilitation understanding lead satisfaction desideratum second emphasize central role understanding path explainability approach satisfy stakeholder desideratum although research ha highlighted importance human understanding xai understanding sometimes seems considered one many important outcome explainability approach 21 claim increasing human understanding not one many important effect explainability approach crucial satisfaction desideratum general reason introduce model emphasizes critical importance human understanding mediator explainability approach satisfaction desideratum related model focused user performance see hoffman et al 17 third propose model used guide evaluating adjusting choosing developing explainability approach particular model highlight main concept relation explainability approach supposed lead satisfaction stakeholder desideratum clearly deﬁning analyzing capturing concept well clarifying relation central systematic evaluation success explainability approach help identify potential reason explainability approach not satisfy given desideratum similarly considering concept relation crucial choice different explainability approach successful development approach support derivation requirement explainability approach ha potential satisfy stakeholder desideratum furthermore model useful detect input discipline outside computer science psychology philosophy law sociology 22 crucial evaluating developing explainability approach thus model serf identify interdisciplinary potential aimed establish common ground different discipline involved xai overall current paper intended interdisciplinary readership interested xai conceptual model relation explainability approach stakeholder desideratum purpose paper introduce conceptual model see fig 1 organizes make explicit central concept explainability approach relate satisfaction stakeholder desideratum well lations concept main concept model explainability approach explanatory information stakeholder understanding desideratum satisfaction given context overall idea model success explainability approach depends satisfaction er desideratum consisting substantial epistemic facet desideratum satisfaction see section desideratum 2 generally speak single stakeholder since not consider stakeholder individually treat representative member speciﬁed stakeholder class 2 langer oster speith et al artiﬁcial intelligence 296 2021 103473 fig proposed model explainability approach relate satisfaction stakeholder desideratum satisfaction thus motivates explanation process including explainability approach explanatory information holder understanding speciﬁcally explanation process assume explainability approach provide tory information human stakeholder human stakeholder engage information facilitate understanding artiﬁcial system functioning output consequence adjusted understanding stakeholder affect extent desideratum satisﬁed context human stakeholder cial system operate interact affect relation concept inﬂuences relation explanatory information stakeholder understanding well relation understanding desideratum isfaction identifying deﬁning well capturing empirically examining concept relation guide evaluating adjusting choosing developing explainability approach aim satisfy stakeholder ata focus stakeholder desideratum following section elaborate model concept lations detail well explicate shortcoming current view concept relation paper structured follows start right side fig 1 continue work backwards holder desideratum section 2 describe different class stakeholder provide example pertinent desideratum elaborate central role understanding desideratum satisfaction section evoking standing turn requires explanatory information illuminate section section 5 shed light connection explainability approach explanatory information throughout section point ward interdisciplinary potential becomes apparent transition one concept next model section 6 exemplify model used evaluate adjust choose develop explainability approach stakeholder desideratum stakeholder desideratum one not reason rising popularity xai see also since stakeholder combination concrete desideratum motivate guide affect explanation process depicted fig 1 propose identifying clarifying desideratum various class stakeholder related artiﬁcial system crucial ﬁrst step evaluating adjusting choosing developing explainability approach artiﬁcial system given context stakeholder class need explainability start increasing societal impact artiﬁcial system fact many system still operated human affect human life indicates various group people different interest explainability artiﬁcial system people operate system try improve affected decision based output deploy system everyday task set regulatory frame use people commonly called previous research ha discussed varying class stakeholder context xai instance preece et al 18 distinguish four main class stakeholder developer theorist ethicist user arrieta et al 14 rize main class stakeholder domain data owner user affected model decision board member regulatory entity see also follow arrieta et al distinguish ﬁve class stakeholder user system developer affected party deployers regulator see fig 2 3 according dictionary stakeholder among others someone involved affected course action 23 reason use general term refer speciﬁc stakeholder class appropriate 3 langer oster speith et al artiﬁcial intelligence 296 2021 103473 developer system deployer user affected party regulator fig different class stakeholder associated artiﬁcial system relation clearly one person member several stakeholder class user instance affected output system operates additionally prototypical class stakeholder distinction stakeholder possible 15 example not one prototypical developer developer differ expertise factor personality novice developer may different desideratum expert similar way lay user desideratum might differ expert user section moreover want emphasize list stakeholder not necessarily exhaustive distinction based previous research mainly come computer scientiﬁc background thus might neglect class stakeholder exemplary stakeholder desideratum desideratum arising ﬁve class stakeholder diverse based term search identiﬁed 100 journal conference publication postulate xai indispensable come satisfying different desideratum see table 1 table 1 present exemplary desideratum extracted literature review row contains tum partnered source claim propose show research explainability approach ﬁndings output contribute satisfaction desideratum furthermore table present stakeholder class may prone one desideratum whenever could not extract stakeholder class respective paper mostly mapping notably source present table only claim research contribute satisfy respective desideratum only subset paper instance providing empirical evidence claim regarding mapping desideratum stakeholder class regarding relation explanatory information desideratum follows present two important exemplary desideratum class stakeholder user paper concerning stakeholder xai class stakeholder common see among others user take account recommendation artiﬁcial system make decision 24 some prototypical member stakeholder class medical doctor loan oﬃcers judge hiring manager usually user no expert regarding technical detail functioning system use however work effectively form adequate expectation concerning system functioning case not case expectation violated need information go beyond knowledge purely operating system motivates least following two central desideratum user usability trust many case system usable offer meaningful information alongside output information help user adequately link knowledge assessment given situation information used system help make decision quickly increase decision quality 116 contribute usefulness another important desideratum user system important scenario user decides basis system recommendation closely linked desideratum adequately calibrating trust system undertrust overtrust negatively affect appropriate use system 117 case undertrust user may constantly try supervise system behavior even attempt intervene system process thereby undermining effectiveness system interaction 118 case overtrust people may use system without questioning behavior decrease effectiveness interaction human rely system output even situation challenge explainability approach potential provide mean let user adequately calibrate trust artiﬁcial system 17 developer individual design program build artiﬁcial system developer naturally count class stakeholder without system would not exist ﬁrst place generally developer high expertise concerning system interest improving especially important desideratum developer veriﬁcation check whether system work intended many classiﬁers consider instance irrelevant input relevant see 125 126 increasing insight system process using certain explainability approach help 4 langer oster speith et al artiﬁcial intelligence 296 2021 103473 table 1 exemplary list desideratum stakeholder holding desideratum source claim propose show research explainability approach ﬁndings output contribute satisfaction desideratum desideratum tentative description stakeholder without empirical emp investigation emp evidence emp mixed emp inconclusive acceptance improve acceptance system deployer regulator 49 50 accountability provide appropriate mean determine accountable regulator accuracy ass increase system predictive accuracy developer autonomy enable human retain autonomy interacting system user conﬁdence make human conﬁdent using system user 82 controllability retain complete human control concerning system user 39 43 debugability identify ﬁx error bug developer education learn use system system peculiarity user effectiveness ass increase system effectiveness work effectively system developer user eﬃciency ass increase system eﬃciency work eﬃciently system developer user 31 fairness ass increase system actual fairness affected regulator informed consent enable human give informed consent concerning system decision affected regulator legal compliance ass increase legal compliance system deployer ass increase system compliance moral ethical standard affected regulator performance ass increase performance system developer 49 privacy ass increase system privacy practice user responsibility provide appropriate mean let human remain responsible increase perceived responsibility regulator 68 robustness ass increase system robustness adversarial manipulation developer safety ass increase system safety deployer user satisfaction satisfying system user 31 science gain scientiﬁc insight system user 106 security ass increase system security transferability make system learned model transferable context developer transparency transparent system regulator trust calibrate appropriate trust system user deployer 114 trustworthiness ass increase system trustworthiness regulator usability usable system user usefulness useful system user veriﬁcation able evaluate whether system doe supposed developer classify source provide no empirical investigation claim show empirical evidence explainability approach affected desideratum satisfaction provide mixed empirical evidence some explainability approach positive effect given desideratum satisfaction whereas others no effect present inconclusive empirical evidence effect explainability approach desideratum satisfaction wa not signiﬁcant 5 langer oster speith et al artiﬁcial intelligence 296 2021 103473 developer recognize correct mistake accordingly case xai contributes determine whether system work intended thus explainability approach support veriﬁcation system another important desideratum developer performance many way system achieve better performance example predictive accuracy ml algorithm seen performance measure though some claim explainability accuracy diﬃcult combine also opposite view see xai way actually make system accurate particular help developer estimate system accuracy mean getting information led system outcome developer detect derrepresented erroneous training data thus learning process achieve higher accuracy another way performance understood interaction better user interact system better system combination user system perform end insight system functioning output fruitful way improve performance affected party inﬂuence artiﬁcial system constantly growing decision people increasingly automated often without knowing affected party group people scope system impact stakeholder much hinge decision artiﬁcial system patient job loan applicant defendant court typical example class crucial desideratum affected party fairness desideratum closely related system fair instance inﬂuence protected attribute gender ethnicity adequately limited controlled system process case ethical system process rely morally permissible consideration according certain moral theory autonomous car dilemma situation never let affected party age contribute process see 128 consideration fairness ethic evolved increasing number affected party lead discrimination individual concerning distribution job loan healthcare not basis action characteristic basis action characteristic social group belong woman ethnic minority older people 54 one hope establishing automated wa make decision le prone human bias 129 however commonly acknowledged artiﬁcial system reproduce process even intensify human bias see 54 130 counteract bias therefore crucial enable detection explainability approach may aid regard providing mean track factor may contributed unfair unethical process either eliminate factor mitigate least aware deployers people decide employ certain system hospital manager decides implement diagnosis system hospital deployers count another class stakeholder decision inﬂuence many class stakeholder example user work deployed system consequently new people fall inside range affected party deployers want system bring use accepted eye deployers worst case term acceptance user reject appropriately working system system end never used 132 therefore low acceptance undermines deployers intend achieve providing system user previous research claim explainability approach aid case providing people insight system improve acceptance another desideratum deployers system legal compliance deployers bear certain degree responsibility system bring use ensure system comply legislation safety system two important factor legal compliance explainability approach promise enable deployers stakeholder check whether system indeed safe moreover european general data protection regulation gdpr often discussed right explanation 101 arguably explicitly require explanation regulator finally regulator stipulating legal ethical norm general use deployment ment system class stakeholder occupies somewhat extraordinary role since watchdog function not only regard system whole interaction process system stakeholder class class consists ethicist lawyer politician must ass control regulate whole process using artiﬁcial system regulator call instance trustworthy system however concept trustworthiness still only vaguely deﬁned 133 example high level expert group artiﬁcial intelligence hlegai initiated european commission doe not provide common deﬁnition trustworthiness only proposes trustworthy system three property lawful ethical robust 10 without examining trustworthiness closely hlegai emphasizes signiﬁcance trustworthy artiﬁcial system stating trustworthiness system imperative realization potentially vast social economic beneﬁts regulator eu well previous research artiﬁcial intelligence call trustworthy system described 53 agree explainability approach one central way facilitate trustworthiness system accountability another important desideratum regulator accountability able identify blamable culpable mistake increasing use artiﬁcial system accountability gap might emerge instance use artiﬁcial system harm person may not clear accountable many party may contributed harm opaque artiﬁcial system only amplify issue example person acting output system may not able know output wa erroneous blaming 6 langer oster speith et al artiﬁcial intelligence 296 2021 103473 ensuing problem might inadequately ignore contribution artiﬁcial system overall regulator want avoid situation existing legislation hard apply no one feel accountable mistake case explainability approach may restore accountability making error cause unfavorable outcome detectable attributable involved party interdisciplinary potential artiﬁcial system continue inﬂuence human every part life thus likely new desideratum emerge desideratum might evolve societal legal political philosophical psychological need regarding artiﬁcial system competence relatedness autonomy 136 example artiﬁcial system healthcare 137 pressing need formulating relevant ethical legal desideratum addition also possible explanatory information provided artiﬁcial system doe not only aim improve task achievement also entertain user 138 conducted literature review derive overview stakeholder desideratum clearly possible extend list table fact developing reﬁning list desideratum important point reveals interdisciplinary potential first source referred table 1 only claim desideratum relevant stakeholder need thorough empirical investigation probably done interdisciplinary team psychologist philosopher scholar law show actual importance desideratum certain stakeholder class furthermore overview desideratum denotation stem case directly source paper ever some desideratum closely related especially given interdisciplinary research contributing xai plausible different author actually mean refer desideratum give different term use term refer different desideratum consistent terminology conceptual clarity desideratum pivotal need explicate various desideratum precisely different discipline like law philosophy psychology need come together discus conception various desideratum agree common deﬁnitions desideratum insight different discipline regarding respective desideratum kind explanatory information required satisfy given desideratum might not adequately integrated common stream research additionally need research explicitly analyzes society stakeholder class affected artiﬁcial system instance collaborating sociologist could offer broader nuanced picture class stakeholder considered within scope xai any case order comprehensively address stakeholder desideratum need detailed understanding stakeholder class promising consult discipline outside computer science focusing society sociology well individual difference within group society psychology furthermore researcher different discipline may able take perspective certain stakeholder class help reﬁne list desideratum instance computer scientist take perspective opers psychologist take perspective user affected people management scholar could take perspective deployers philosopher political scientist well researcher law take perspective regulator working together interdisciplinary team thus contribute comprehensive consideration important ata given context however comprehensiveness one side coin justiﬁcation desideratum another concerning justiﬁcation two main perspective one ethic one jurisprudence ethical perspective judge whether desideratum compatible some many even established moral theory similarly legislation consulted ass whether law demanding prohibiting meet certain ata engaging thorough moral legal justiﬁcation might conclude desideratum not justiﬁable decision instance individual user might want system best case autonomous car driver probably want car decide way make likely vive car face imminent accident 139 might case decision neither morally legally justiﬁable le drastic example user may ask explanation received low score personnel selection test however providing explanation might render given test obsolete explanation possibly enables participant game test 8 suggest given context well moral legal consideration decisive factor determining whether certain stakeholder desideratum justiﬁed desideratum satisfaction requires understanding previous section introduced claim need xai arises stakeholder desideratum precisely need arises case certain stakeholder desideratum not suﬃciently satisﬁed reason take look mean desideratum satisﬁed facet desideratum satisfaction propose satisfaction desideratum take two facet call facet epistemic substantial desideratum satisfaction respectively one hand stakeholder want system certain property make 7 langer oster speith et al artiﬁcial intelligence 296 2021 103473 actually fair transparent usable line desideratum fairness substantially satisﬁed system suﬃciently posse corresponding property hand stakeholder want know able ass whether system substantially satisﬁes certain desideratum whether system ha required property epistemic facet fairness desideratum satisﬁed stakeholder position ass know whether extent system fair naturally xai epistemic facet important one since explanatory information contribute satisfaction epistemic facet every desideratum whereas not case substantial facet example take desideratum usable system successful explanation process depicted model may enable user recognize whether system usable optimally also increase system usability certain degree case epistemic satisfaction consists stakeholder able check whether system output usable task hand lesser extent however explanation process also contribute substantial satisfaction desideratum since provides additional knowledge system make usable stakeholder larger deﬁcits usability addressed however explanatory information might not directly help entire artiﬁcial system may need redesigned depending desideratum two facet correlated certain degree possibly even completely fying epistemic facet also completely satisﬁes substantial facet illustrate consider desideratum retaining user autonomy scenario let u assume explanation process ha helped satisfy temic facet desideratum certain degree ha enabled user ass extent retain autonomy making decision based recommendation system additionally understanding user ha system output autonomous decide based thus explanation process ha helped satisfy epistemic substantial facet desideratum hence case substantial epistemic facet desideratum satisfaction highly correlated consider desideratum system adhere certain ethical principle suﬃcient information system regulator evaluate whether system complies ethical standard explanation process serf satisfy epistemic facet desideratum however doe not directly make system process output likely comply ethical standard consequently explanation process indirectly satisfy substantial facet desideratum based understanding obtained explanation process fault identiﬁed step improve system regarding ethical property initiated case epistemic substantial facet desideratum satisfaction only loosely correlated one hand distinction two facet show explanation process contribute satisfaction epistemic facet desideratum concerning artiﬁcial system hand show explanation process alone doe sometimes not suﬃce satisfy substantial facet desideratum concerning artiﬁcial system many case however epistemic satisfaction enables substantial one mean even better understanding system triggered explanatory information doe not always directly lead substantial satisfaction desideratum form necessary basis achieving epistemic satisfaction desideratum closely linked better understanding system understanding pivotal point endeavor satisfying desideratum understanding throughout history xai research author highlighted central importance understanding xai 2 overall goal xai advance human understanding artiﬁcial system order satisfy given desideratum ongoing debate philosophical literature constitutes understanding comprehensive review concept beyond scope current paper see instance 144 review understanding paper concept understanding relation explanation understanding 150 related topic cognitive process knowledge acquisition furthermore see 151 broad overview theoretical basic understanding relevant xai research some aspect understanding however typically agreed upon different depth breadth understanding following use term degree understanding address depth breadth understanding similar different kind understanding evaluation explainability approach thus crucial determine stakeholder understanding artiﬁcial system examining understanding software ha long history interaction education typically referring understanding artiﬁcial system discipline also refer user mental model system mental model system understood mental representation system functioning according rouse et mental model allow human generate description system purpose form explanation system functioning observed system state prediction future state 154 word mental model allow human mentally simulate aspect system instance order understand cause 17 translated case artiﬁcial system mean could examine understanding investigating holder mental model system well doe person mental model mirror actual system gap current understanding system functioning people overconﬁdent understand system work actually only illusion understanding 155 learned misconception system 8 langer oster speith et al artiﬁcial intelligence 296 2021 103473 output need revised 156 example possible investigate stakeholder mental model technique stakeholder tasked describe system inner working letting stakeholder draw mental model given system overview method elicit mental model see 17 similarly possible measure understanding capturing human mental model enable relation system output example kulesza et al 157 used explanation task ass whether participant derstood kind information used predict outcome study used prediction task ass instance whether participant anticipate predictive model would produce better outcome 115 whether participant predict outcome predictive model would produce person given proﬁle 21 whether participant foresee inﬂuence given feature outcome 21 another possibility would use manipulation task order ass whether people understood kind information might add predictive accuracy model like 115 possible task might perception task naming recognized characteristic model imagination task estimating model would predict given input would reﬂect different degree understanding furthermore task reveal misconception system functioning edge gap need adjusted ﬁlled additional alternative explanatory information additionally way capture stakeholder understanding common might help u examine whether given desideratum ha potential satisﬁed example developer ha understood system way imagine situation system might fail ability make system robust likely crease furthermore initial degree stakeholder understand artiﬁcial system speciﬁcally stakeholder without any prior experience given system likely start degree understanding corresponds background knowledge artiﬁcial system arose initial instruction received regarding system 17 158 thus incomplete even faulty mental model given system 20 instance stakeholder might know might informed based system usually trained historical data order predict new data degree understanding augmented explanatory information generated explainability approach higher degree understanding consequently detailed accurate mental model system stakeholder might understand kind training data underlie given system kind algorithm used given system kind output data system produce thus increasing degree understanding stakeholder able ass whether given system ha desired characteristic adequate process produce expected outcome word increasing degree understanding satisfy epistemic facet desideratum satisfaction desideratum substantial facet however opposite might sometimes case discus next section relation understanding desideratum satisfaction certain case stakeholder degree understanding extent desideratum satisfaction positively related instance desideratum retain autonomy interaction system usually higher degree understanding satisﬁes epistemic substantial facet desideratum greater extent however also complex case assume desideratum trust certain system acquiring higher degree ing increase stakeholder epistemic satisfaction desideratum better ass whether extent trust system substantial facet actual trust inﬂuenced negative way stakeholder still posse low degree understanding likely unaware problematic feature system ha certain context complex environment certain kind input data noisy input low degree understanding stakeholder likely trust system although inadequately contrast higher degree understanding stakeholder able recognize even explain condition system tend fail therefore aware system problematic feature may consequently decrease trust additionally happen understanding contributes satisfaction single desideratum stakeholder greater extent satisfaction desideratum stakeholder suffers 63 illustrate take transparency system deployers system want comply legislation consequently want system transparent understanding necessary condition perceived transparency however making system transparent diminish another desideratum deployers system 8 word not possible particular user manipulate system way systematically evoke beneﬁcial output however transparency caused higher degree ing may enable some people exploit system 63 personnel selection test example better understanding selection system may enable participant game test preventing proper use selecting suitable cant point another potential considered regarding relation advanced understanding desideratum satisfaction felzmann colleague 15 argue different stakeholder might hold different expectation regarding extent single desideratum ha satisﬁed furthermore possible desideratum one stakeholder inﬂuenced positively increase understanding desideratum stakeholder suffer ple case wa described langer et al 160 provided additional information accompanying automated 9 langer oster speith et al artiﬁcial intelligence 296 2021 103473 personnel selection system participant resulted perceived transparency time duced acceptance system similar ﬁnding see 161 case happen two desideratum transparency acceptance arise different perspective characteristic stakeholder instance legislation regulator might call transparency system whereas company using system deployer desire system accepted explaining system lead satisfaction legal desideratum price impairing company example indicate degree kind understanding artiﬁcial system explainability approach evoke may depend variety desideratum variety stakeholder consequently development implementation use explainability approach go hand hand evaluation relevant stakeholder desideratum estimating effect explainability approach central investigate perspective not only one stakeholder potentially variety conﬂicting desideratum regard given system factor inﬂuencing relation understanding desideratum satisfaction characteristic context artiﬁcial system operate moderate relation understanding desideratum satisfaction may affect degree kind understanding necessary satisfy given desideratum no deﬁnition term context 162 deeper dive discussion term go beyond achieve paper discussion topic see following dourish 163 hold context set given situation interaction stakeholder artiﬁcial system given activity task environment make impossible anticipate contextual inﬂuences affect process explainability approach aim satisfy stakeholder desideratum without knowing concrete situation theless crucial consider anticipate contextual inﬂuences evaluating developing explainability approach instance stake given situation affect relation understanding stakeholder desideratum whether context high low stake scenario may determine degree understanding sary satisfy given desideratum research indicates certain situation tend require greater understanding event situation speciﬁcally situation instrumental relational moral legal value stake might likely require extensive understanding instrumental value stake personal nomical etc beneﬁts loss expect speciﬁc situation artiﬁcial system handle ﬁnancial transaction relational value stake important interpersonal relationship might affected event artiﬁcial system used employee layoff moral value stake moral right might violated using artiﬁcial system sentencing court finally legal value stake legal right might violated artiﬁcial system output conﬂict right depending concrete situation autonomous car instrumental relational moral legal value determine stake situation additionally identifying value stakeholder regard relevant value indeed relevant situation may allow drawing inference whether situation considered high low stake consequently value serve orientation stakeholder likely demand higher degree understanding speciﬁcally mean satisfaction given desideratum low stake scenario lower degree understanding might suﬃcient compared high stake scenario furthermore context involving artiﬁcial system may differ outcome favorability case unfavorable outcome people likely call additional information order understand reason outcome able better ass control similar outcome one hand may increasing extent understanding case favorable outcome ha little potential positively affect desideratum satisfaction hand better understanding central positively affecting desideratum satisfaction case unfavorable outcome instance perceived fairness central desideratum many decision situation 169 understanding may negligible effect perceived fairness favorable outcome improve perceived fairness unfavorable outcome 170 supporting claim kizilcec 38 found available explanatory information only affected perceived fairness people expectation concerning outcome violated may however certain condition better understanding negatively affect perceived fairness even people experience favorable outcome 166 stake situation outcome favorability two many possible contextual factor may inﬂuence relation understanding desideratum satisfaction candidate application context home work time constraint 171 social circumstance whether people present given situation 163 4 note example also describes situation two desideratum company conﬂict user acceptance adhering legislation 10 langer oster speith et al artiﬁcial intelligence 296 2021 103473 interdisciplinary potential interdisciplinary potential see relation understanding desideratum satisfaction described research question doe stakeholder initial degree understanding prior knowledge artiﬁcial system relate desideratum satisfaction b desideratum single stakeholder desideratum multiple stakeholder implication understanding regarding c doe degree kind understanding relate satisfaction desideratum task context affect relation understanding desideratum satisfaction given situation scholar educational science could collaborate computer scientist order investigate design adequate instruction achieve proper basic understanding background knowledge artiﬁcial system serve general basis partially satisfy large variety desideratum furthermore necessary involve psychologist order experimentally examine relation understanding desideratum satisfaction well inﬂuences affecting relation regard central determine mean certain desideratum satisﬁed requires ﬁnding adequate way measuring satisfaction desideratum using measure expert interview legal analysis well understanding requirement satisﬁed deﬁning minimum legal standard enabling stakeholder perform speciﬁc task successfully practice involves elaborated research design clear conceptual deﬁnitions appropriate operationalization measurement method ﬁtting research discipline iterative empirical research finally scholar requirement engineering may help understand relationship several desideratum based result scholar law philosophy help determine morally legally justiﬁed general interdisciplinary collaboration contribute aware potential relationship certain desideratum understanding requires explanatory information providing explanatory information given phenomenon default procedure facilitate understanding explanatory information help human navigate complex environment facilitating better ing prediction control situation information narrow possible reason event decrease uncertainty corrects misconception facilitates generalization reasoning enables person draw causal sion context xai explanatory information put stakeholder position grasp generalizable pattern underlying production system outcome lime 115 pattern allow drawing inference potentially causal connection system input output narrowing possible way tem might failed additionally pattern may help stakeholder tell apart correct unexpected behavior malfunction order debug system general reduce people uncertainty concerning system uncertainty concerning behave towards react think whether recommend whether disseminate 63 overall explanatory information lead better understanding turn positively affect satisfaction stakeholder desideratum depending explanatory information different degree kind understanding acquired reason shed light teristics explanatory information characteristic explanatory information important characteristic explanatory information concern kind presentation format various kind explanatory information teleological information appeal function explanandum 176 ical information refers law nature 177 statistical relevance information statistically relevant explanandum contrastive information highlight event p happened not event q 6 counterfactual information appeal hypothetical case thing went differently 180 mechanistic information appeal mechanism underlying certain process 181 causal information appeal cause event network information appeal topological property network model describing system 184 many concerning presentation format also various possibility roughly distinguish multimedia presentation 36 presentation natural language text rule extracted based system execution trace simply program source code multimedia presentation include graphic visualization image animation instance neural activity popular presentation format explanatory information neural network 14 aside general characteristic kind presentation format characteristic tory information inﬂuence whether evokes understanding soundness accurate information completeness novelty whether information new recipient complexity depending number feature interrelation feature information contains 171 some various example characteristic explanatory information see 63 11 langer oster speith et al artiﬁcial intelligence 296 2021 103473 importance acknowledging characteristic explanatory information highlighted research cognitive educational psychology show effect explanatory information vary depending characteristic take complexity example study show people prefer simpler information information tioning fewer cause 185 another example ﬁnding explanatory information aligns stakeholder goal certain situation preferred 186 vasilyeva et al 186 showed people evaluate teleological explanatory information compared mechanistic explanatory information useful asked name function nomenon conversely perceive mechanistic explanatory information useful asked name cause event factor inﬂuencing relation explanatory information understanding relation explanatory information understanding inﬂuenced variety characteristic holder context interaction characteristic considering inﬂuences central evaluate develop explainability approach characteristic stakeholder since every stakeholder posse individual degree understanding system dividual ability understand individual set desideratum need satisﬁed certain extent characteristic stakeholder receive explanatory information inﬂuence relation explanatory information understanding 18 some characteristic obviously inﬂuence relation stakeholder ground knowledge belief learning capacity desideratum concerning respective system instance explanatory information including technical detail might increase expert developer degree understanding technical detail hamper understanding novice developer stakeholder 175 furthermore desideratum salient respective stakeholder inﬂuence relation explanatory information understanding speciﬁcally stakeholder engage information order advance ing artiﬁcial system motivation prior belief may affect interpret given set information 67 instance stakeholder primary desideratum ensure system provides fair output scrutinize explanatory information sign bias might lead unfair outcome contrast stakeholder primary atum improve system predictive accuracy might pay special attention information providing insight improve system performance indicates depending given desideratum amount kind information lead different degree kind understanding consequently important provide propriate information given purpose assumption supported research proposing human reasoning process strongly inﬂuenced explanatory information information ha potential improve human may also hamper explanatory information attenuates human bias human need invest much effort use information list individual difference inﬂuencing relation explanatory information understanding go yond scope article cover personality trait conscientiousness 191 need need closure well people preference detailed versus coarse explanation 196 age ences stakeholder 197 characteristic context time pressure relevant contextual inﬂuence stakeholder high time pressure explanatory information may lead le understanding compared situation low time pressure 198 workload another contextual inﬂuence 199 situation high perceived workload explanatory information lead different degree understanding compared low workload condition similar thing true situation likely stakeholder experience higher level stress high stake high risk situation multitasking environment general depending situation task hand effect explanatory information understanding may differ therefore important investigate contextual condition stakeholder interaction artiﬁcial system detail theorizing explanatory information best improve understanding given impact context given fact might not possible anticipate relevant contextual inﬂuences 163 especially important ass effect explanatory information laboratory ﬁeld setting although ha advantage investigate effect instance stress relation explanatory information understanding controlled laboratory setting result may not translate ﬁeld setting ﬁnding line call experiment involving human participant proxy task order show given explanatory information not only elicits understanding simulating context also real world condition investigate whether given explanatory information equally valuable wild although impossible fully anticipate context alter relation explanatory information understanding interpretation 5 need cognition personality trait distinguishes people like put effort cognitive activity prefer le cognitively demanding activity 12 langer oster speith et al artiﬁcial intelligence 296 2021 103473 contextual inﬂuences depends relevant stakeholder 162 least crucial aware contextual inﬂuences may central success failure explanatory information interaction characteristic explanatory information stakeholder context finally characteristic tory information stakeholder context interact way affect stakeholder engage explanatory information advance understanding artiﬁcial system instance level detail explanatory tion interact prior knowledge stakeholder whereas expert developer understanding may beneﬁt higher degree explanatory information much detail level detail contrary effect novice developer time novice user want learn use system might want detailed explanatory information whereas expert user want use system task fulﬁllment every unnecessary piece information could lead rejection system 36 context change relation explanatory information understanding prone change well instance soon time pressure aforementioned situation plausible neither expert novice developer user pragmatically beneﬁt detailed explanatory information interdisciplinary potential following research question reinforce call extensive validation experimental study investigating effect explainability approach different context relation different stakeholder refer reader sokol et al 63 description important characteristic explanation stakeholder context affect relation explanation stakeholder understanding classify explanatory information b doe different explanatory information lead understanding c different stakeholder engage tory information optimally evoke understanding explanatory information e stakeholder difference background knowledge personality characteristic affect understanding f contextual inﬂuences different level risk environment time pressure affect relation explanatory information understanding g explanatory information designed include kind tion format textual graphical appropriate stakeholder interactively engage information one hand research question call uniﬁed classiﬁcation explanatory information often computer scientist classify explanatory information based presentation format based explainability approach inated 205 philosopher psychologist however usually classify explanatory information term kind mentioned causal nomological 206 order prevent debate drifting apart philosopher psychologist computer scientist collaborate ﬁnd standardized way classify explanatory information hand research question call empirical evaluation value different kind explanatory mation different stakeholder variety context consideration different desideratum may primarily call empirically working psychologist tool expertise design experimental study philosopher determine quality good explanatory information computer scientist adjust tation explanatory information well possible furthermore cognitive scientist might need examine exactly understanding artiﬁcial system actually mean measure see 17 idea capture human understanding artiﬁcial system however deriving explanatory information artiﬁcial system task requires lot interdisciplinary research following section completes speciﬁcation concept relation model describing order get required explanatory information system need ﬁtting explainability approach explanatory information requires explainability approach order provide explanatory information facilitates understanding thus affect satisfaction desideratum different class stakeholder xai research ha developed wide variety explainability approach approach encompass method procedure strategy provide explanatory information help stakeholder better understand artiﬁcial system inner working output speciﬁc explainability approach characterized step effort undertaken extract explanatory information system adequately provide stakeholder given context explainability approach take many guise literature commonly distinguishes two family approach approach family explainability approach approach aim designing system inherently transparent explainable rely system constructed basis model not require additional procedure extract meaningful information inner working output example model linear approximation commonly seen inherently explainable given limited size human principle directly extract information model order enhance understanding system work system arrived particular output unfortunately way deriving explanatory information transparent model 13 langer oster speith et al artiﬁcial intelligence 296 2021 103473 might only useful stakeholder certain expertise reason approach make system directly explainable only developer member stakeholder class posse enough expertise artiﬁcial system furthermore explainability also lead loss predictive power not system designed inherently explainable approach try circumvent aforementioned shortcoming approach principle applicable kind model difference approach approach not aim particular system procedure method allow extracting explanatory information system underlying model usually not inherently transparent explainable ﬁrst place approach example based analysis approximation opaque model model inherently explainable many case however approach restricted respect present explanatory information given speciﬁc model one output information approach provide repeated usage format approach provides information similar hence approach hold approach not guaranteed stakeholder able understand provided information given format explanatory information accessible approach often only interpretable developer expert stakeholder mean information doe not directly facilitate understanding 16 one solution combine several explainability approach order cover broad range different mation presentation mode another solution ha received increasing attention recourse interactive explainability approach interactive approach based idea user some stakeholder provided information concerning system information initially received doe not suﬃce based need person interacting system call information speciﬁc aspect decision request presentation different format date however approach fully interactive remain rare third solution human facilitator expert stakeholder explain system stakeholder instance regulator want satisfy desideratum concerning artiﬁcial system case not directly interact artiﬁcial system instead human facilitator expert user developer derive suitable explanatory information regulator sense process introduces desideratum hierarchy based stakeholder fig word one desideratum might satisﬁed one stakeholder class some desideratum another stakeholder class satisﬁed example facilitator desideratum case able explain system regulator must satisﬁed ﬁrst precondition satisfaction regulator desideratum able ass fairness system thus complete process fig 1 nested within explainability approach human facilitator developer deriving explanatory information assisted explainability approach another stakeholder regulator considered hybrid approach explainability factor inﬂuencing relation explanatory information explainability approach characteristic explainability approach worth mentioning since likely inﬂuence provided kind explanatory information presentation format first important distinguish proaches work regardless underlying model type approach one only work speciﬁc type model approach approach aim deliver explanatory information system solely observing pair approach also factoring speciﬁc feature model hand creating prototype vector support vector machine approach advantage working type model drawback tend le eﬃcient le accurate well le explanatory powerful explanatory information level detail lower regard individual phenomenon former second previous research distinguished scope explainability approach some approach provide information only single prediction model scope approach local often offer alized prototype outcome example general type approach ha global scope approach designed uncover overall decision process model usual way provide information approximating complex model simpler one inherently explainable depending explainability approach explanatory information provided differ global explainability proaches instance likely produce complex information requires background knowledge stakeholder understood local explainability approach hand only show limited picture tem inner working may not representative overall process based difference conclude certain explainabilty approach suitable satisfaction given desideratum speciﬁc stakeholder class others elaborate take user want calibrate trust system need different kind information compared user want usable system ﬁrst case explainability approach likely need extract information robustness system condition output system trustworthy situation user aware output might misleading case usable system user might 14 langer oster speith et al artiﬁcial intelligence 296 2021 103473 want information directly match speciﬁc goal given task another example desideratum user learning use system may need different kind explanatory information compared simply want fulﬁll task help artiﬁcial system 211 user want learn use system need detail whereas user want fulﬁll task need directly useful information order not reduce productivity overly detailed information difference information needed also due difference perspective stakeholder take desideratum fairness affected party likely focus aspect individual fairness may call explainability approach facilitate local explainability regulator however may focus general notion fairness calling explainability approach facilitating global explainability interdisciplinary potential design explainability approach goal providing adequate explanatory information potential affect stakeholder desideratum hold untapped interdisciplinary potential research question pinpoint explanatory information explainability approach provide case b design interactive explainability approach c explainability approach involving human facilitator optimally signed guarantee explainability approach provides required explanatory information e stakeholder desideratum degree understanding taken account generating explanatory information developing new explainability approach research question see potential collaboration computer scientist philosopher chologists cognitive scientist instance philosopher psychologist determine information needed ass whether system fair whereas computer scientist develop explainability approach provide information aware different approach well technical constraint furthermore investigating examination stakeholder desideratum narrow option possibly successful explainability approach might fruitful area future research thorough discussion desideratum stakeholder class call kind explainability proach go far beyond single paper achieve however section 6 show model could inspire work necessary evaluate usefulness explainability approach relation given desideratum show model support development new explainability approach satisfy given set desideratum thus next section aim show model lead actionable insight xai research analyzing concept relation propose fig mean hypothetical application scenario want stimulate idea speciﬁc application model bringing together hypothetical application scenario following present previous section come together within hypothetical application scenario narios highlight importance different stakeholder class desideratum furthermore scenario emphasize understanding affect satisfaction desideratum explanatory information provided explainability proaches facilitate understanding analyzing investigating concept relation central aim xai well development explainability approach successfully satisfy stakeholder desideratum main application model derived idea explanation process doe not change certain desideratum extent satisfaction corresponding explainability approach might not suitable mean satisfying desideratum given context discovery resulting stakeholder feedback empirical tigation provide feedback regarding explainability approach kind explanatory information work desideratum context thus feedback serve input improvement explanation ce consequently help evaluate adjust choose develop explainability approach given purpose context propose step model fig 1 explainability approach information standing satisfaction allows drawing inference explanation process involving explainability approach e kind explanatory information stakeholder understanding following question arise different point model particular interest relevant stakeholder speciﬁc characteristic relevant desideratum speciﬁc context satisﬁed stakeholder acquired suﬃcient degree right kind understanding allows assessing whether given desideratum satisﬁed facilitate satisfaction doe provided explanatory information format presentation facilitate stakeholder understanding given context consideration stakeholder characteristic explainability approach able provide right kind amount explanatory information right sentation format contextual inﬂuences hindering promoting satisfaction desideratum explanation process 15 langer oster speith et al artiﬁcial intelligence 296 2021 103473 investigating question requires empirical research hypothesis testing interdisciplinary cooperation eventually aid evaluating adjusting choosing developing explainability approach e ﬁnding explanatory mation order adequately satisfy stakeholder desideratum mind believe model useful several important application scenario first model useful choosing adequate explainability approach novel application context artiﬁcial system guiding development new explainability approach speciﬁcally model used inform project develop explainability approach satisfy certain desideratum given class stakeholder second model useful evaluating stage explainability process failed contribute satisfying relevant desideratum let u assume use explainability approach doe not lead satisfaction certain desideratum case explainability approach not suitable satisfaction desideratum explainability approach provides false kind explanatory information replaced another approach doe error lie somewhere else explanation process some case may able adjust explainability approach appropriately achieve intended purpose general application scenario structured attempt evaluate adjust choose develop explainability approach given context roughly distinguish two scenario call evaluation discovery scenario evaluation scenario want investigate whether use speciﬁc explainability approach wa adequate not needed ﬁx coming discovery scenario want ﬁnd adequate explainability approach satisfy stakeholder desideratum take one two form choosing among existing approach no adequate approach available developing new one stakeholder desideratum evaluation discovery scenario start examining stakeholder clarifying desideratum given context discovery scenario examine relevant class stakeholder desideratum concerning application system consideration evaluation scenario check even scenario already identiﬁed desideratum satisﬁed whether explanation process ﬁts relevant class stakeholder desideratum ensure not overlook important stakeholder desideratum need input wide variety discipline including not limited scholar law sociology psychology philosophy computer science combination expertise perspective help identify relevant stakeholder class list desideratum pertaining given context deﬁning desideratum fall within expertise philosopher elaboration desideratum relevant moral legal aspect task ethicist scholar law furthermore assessing contextual peculiarity stakeholder characteristic relevant given context require psychologist individual level sociologist group level well domain expert judge personnel manager particular application scenario desideratum satisfaction discovery scenario next step determine mean stakeholder ata satisﬁed speciﬁc context make estimate provide guideline create measure identiﬁed desideratum satisﬁed evaluation scenario check whether relevant desideratum ﬁed not explanation process wa successful chosen explainability approach appropriate not substantially satisﬁed two possible case first necessary understanding wa not acquired thus desideratum also not epistemically satisﬁed improvement stakeholder understanding required second adequate degree kind understanding reached thus epistemic facet desideratum isﬁed case may conclude regarded desideratum not directly substantially satisﬁable mean explainability approach regardless case point move investigate stakeholder ing determining condition desideratum satisfaction interdisciplinary task psychologist philosopher scholar law furthermore computer scientist domain expert give practical input satisfaction condition desideratum speciﬁc domain making satisfaction desideratum measurable examining extent satisfaction job psychologist develop measure task help ass extent desideratum satisfaction additionally task scholar law philosopher provide clear guideline desideratum satisﬁed stakeholder understanding discovery scenario continues investigating deﬁning requirement holder understanding needed satisfy desideratum consideration speciﬁcally determine priate degree kind understanding concerning system output promise enable epistemic facet desideratum satisfaction time mean need ass study using task measure stakeholder mental model system study using task reveal whether stakeholder able instance explain system functioning predict system behavior outcome stakeholder actual degree kind derstanding especially important evaluation scenario deﬁcits substantial desideratum satisfaction become apparent result circumstance assessing whether required degree kind understanding ha achieved allows drawing inference whether fundamental gap explanatory process substantial desideratum satisfaction whether provided information not appropriate evoke 16 langer oster speith et al artiﬁcial intelligence 296 2021 103473 standing due fact explanation process only serve maximize understanding desideratum still remains substantially unsatisﬁed full satisfaction go beyond scope any explainability approach philosopher help investigate explicate mean certain degree kind understanding building psychologist design way empirically ass measure understanding necessary given desideratum stakeholder able predict system output necessary stakeholder able anticipate situation system likely fail furthermore scholar law contribute condition traceability auditability explanatory information next step discovery scenario pin explanatory information ha potential facilitate right kind degree understanding predetermined context implies evaluation different dimension explanatory information respect expected effect within explanation process pay attention instance kind information presentation format 36 quality 17 amount 171 completeness complexity adequacy given context 171 evaluation scenario check whether explainability approach provides explanatory information suﬃciently meet previously identiﬁed requirement sometimes also need whether requirement concerning information indeed adequate respect philosopher explanation scientist help distinguish different kind feature explanatory information furthermore scholar law examine current legislation ﬁnd whether prescribes certain kind explanatory information case gdpr instance specify mean provide meaningful information logic involved gdpr art 13 2 f 101 finally based differentiation educational cognitive psychologist task characterize explanatory information best suited facilitate required kind degree understanding certain context explainability approach insight assessment concept relation model guide inform requirement explainability approach aim satisfy given desideratum particular importance discovery scenario primary objective identify approach expected appropriate providing speciﬁc information assume evaluation scenario point desideratum not satisﬁed adequate degree kind understanding not evoked required explanatory information not delivered case necessary investigate whether explainability approach even capable producing explanatory information right feature insight available point indicate whether existing explainability approach provides explanatory information suﬃcient satisfy stakeholder desideratum additionally learn whether given explainability approach ha potential derive explanatory information satisfy stakeholder desideratum certain degree whether necessary adjust explainability approach whether suﬃcient choose another one whether need develop entirely new one stage computer scientist improve adjust design explainability approach main contributor integrating mentioned insight first ability ass technically feasible possible second actually implement demand regarding explainability approach resulting previous step speciﬁc application scenario conclude thought application model mean speciﬁc example consider situation user want system produce fair output ﬁrst need explanation process enables ass whether system produce fair output consequently need ﬁnd adequate explainability approach ﬁrst clarify relevant user prototypical characteristic instance novice expert user determine whether stakeholder class also considered example need consider perspective regulator regarding fairness furthermore anticipate context talking personnel selection task court case completely different contextual peculiarity subsequently determine user mean desire fair output clarify satisfaction condition desideratum substantial facet kind algorithmic fairness expect time important aware relevant desideratum satisfaction user desideratum could affected trying satisfy fairness desideratum similarly may unanticipated effect stakeholder desideratum instance using speciﬁc explainability approach prof suitable ass system fairness also affect predictive accuracy artiﬁcial system next consider desideratum epistemic facet circumstance would user enabled ass whether output system fair need understand regard system functioning output answer question must determine degree kind understanding appropriate epistemic satisfaction fairness desideratum need detailed investigation contextual inﬂuences erate relation furthermore must aware given stakeholder characteristic user background knowledge estimated degree kind understanding required enable user ass whether system output fair determine kind explanatory information facilitates understanding example might need explanatory information inﬂuence feature based protected attribute race system output alternatively might need information contrast treatment different group people minority majority knowledge determine explainability approach provides kind explanatory 17 langer oster speith et al artiﬁcial intelligence 296 2021 103473 information likely would local approach provide explanatory information either highlight feature relevance allow user compare subset instance regarding predicted outcome result speciﬁc demand regarding explainability approach satisfying fairness desideratum case consideration knowledge either choose adequate explainability approach existing one design new one afterwards investigate whether explanatory information resulting respective explainability approach lead better understanding system output mean position empirically evaluate success selected approach corresponding explanation process may conduct stakeholder study given deﬁnitions desideratum satisfaction may ﬁnd study epistemic facet may satisﬁed user actually ass whether system produce fair output user explain whether respective system output fair user predict kind input lead fair unfair outcome not substantial one system output actually not fair may conclude substantial facet desideratum not satisﬁable altering explainability approach nevertheless satisfying epistemic facet desideratum help u ﬁgure satisfy substantial facet desideratum beyond strategy instance may obtain information developer use improve system fairness case artiﬁcial system ha adjusted changed additionally satisfy substantial facet desideratum satisfaction hand stakeholder study reveals epistemic facet desideratum not satisﬁed user fail explaining whether respective system output fair conclude user degree kind understanding system output doe not suﬃce many situation may case explanatory information wa not suitable evoke necessary understanding explainability approach not provide explanatory information suitable allow assessing whether system produce fair outcome depending context task stakeholder individual characteristic may necessary iteratively adjust characteristic explanatory information stakeholder engaging information achieve right kind degree understanding alternatively may necessary adjust explainability approach not suitable provide explanatory information useful facilitating stakeholder understanding conclusion increasing number people affected artiﬁcial system number stakeholder desideratum continue grow although focus xai research ha shifted towards human stakeholder evaluation explainability approach shift still need incorporate comprehensive view stakeholder desideratum artiﬁcial system used socially relevant context well empirical investigation explainability approach respect desideratum satisfaction engenders even pressing need interdisciplinary collaboration order consider stakeholder perspective empirically evaluate optimally design explainability approach satisfy holder desideratum current paper ha introduced model highlighting central concept relation along explainability approach aim satisfy stakeholder desideratum hope model inspires guide ture interdisciplinary evaluation development explainability approach thereby advance xai research concerning satisfaction stakeholder desideratum declaration competing interest author declare no known competing ﬁnancial interest personal relationship could appeared inﬂuence work reported paper reference 1 brock learning artiﬁcial intelligence previous awakening history expert system ai mag 39 3 2018 1609 2 clancey epistemology expert system framework explanation artif intell 20 3 1983 1016 83 90008 3 swartout xplain system creating explaining expert consulting program artif intell 21 3 1983 83 80014 4 johnson johnson explanation facility interactive system proceeding international conference intelligent user interface iui association computing machinery new york ny usa 1993 pp 5 biran cotton explanation justiﬁcation machine learning survey proceeding ijcai 2017 workshop explainable artiﬁcial intelligence xai 2017 pp 6 miller explanation artiﬁcial intelligence insight social science artif intell 267 2019 007 7 mittelstadt russell wachter explaining explanation ai proceeding 2019 conference fairness accountability parency association computing machinery new york ny usa 2019 pp 8 burrell machine think understanding opacity machine learning algorithm big data soc 3 1 2016 2053951715622512 9 kim towards rigorous science interpretable machine learning corr ab 2017 18 langer oster speith et al artiﬁcial intelligence 296 2021 103473 10 eu expert group artiﬁcial intelligence ethic guideline trustworthy ai guideline 2019 11 lipton mythos model interpretability commun acm 61 10 2018 12 adadi berrada peeking inside survey explainable artiﬁcial intelligence xai ieee access 6 2018 http doi 13 nunes jannach systematic review taxonomy explanation decision support recommender system user model interact 27 2017 14 arrieta ser bennetot tabik barbado garcia molina benjamin chatila herrera explainable artiﬁcial intelligence xai concept taxonomy opportunity challenge toward responsible ai inf fusion 58 2020 15 felzmann villaronga lutz transparency trust transparency requirement artiﬁcial intelligence legal norm contextual concern big data soc 6 1 2019 16 gilpin testart fruchter adebayo explaining explanation society nip workshop ethical social governance issue ai 2018 pp ab 17 hoffman mueller klein litman metric explainable ai challenge prospect corr ab 2018 18 preece harborne braines tomsett chakraborty stakeholder explainable ai corr ab 2018 19 weller transparency motivation challenge samek montavon vedaldi hansen müller ed explainable ai preting explaining visualizing deep learning springer 2019 pp 20 páez pragmatic turn explainable artiﬁcial intelligence xai mind mach 29 2019 21 cheng wang zhang connell gray harper zhu explaining algorithm ui strategy help expert stakeholder proceeding 2019 chi conference human factor computing system association computing machinery new york ny usa 2019 pp 22 abdul vermeulen wang lim kankanhalli trend trajectory explainable accountable intelligible system hci research agenda proceeding 2018 conference human factor computing system chi association computing machinery new york ny usa 2018 pp 23 dictionary stakeholder 2020 accessed 30 july 2020 24 hind wei campbell codella dhurandhar c ramamurthy varshney ted teaching ai explain sion proceeding 2019 conference ai ethic society association computing machinery new york ny usa 2019 pp 25 anjomshoae främling najjar explanation model prediction contextual importance utility explainable transparent autonomous agent system springer 2019 pp 26 atzmueller towards design explicative system transparent interpretable explainable analytics perspective social interaction context information proceeding 2019 workshop affective computing context awareness ambient intelligence afcai 2019 pp 27 baaj poli ouerdane some insight towards uniﬁed semantic representation explanation explainable artiﬁcial intelligence proceeding 2019 workshop interactive natural language technology explainable artiﬁcial intelligence association tational linguistics 2019 pp 28 balog radlinski arakelyan transparent scrutable explainable user model personalized recommendation proceeding international acm sigir conference research development information retrieval association computing machinery new york ny usa 2019 pp 29 binns van kleek veale lyngs zhao shadbolt reducing human percentage perception justice algorithmic decision proceeding 2018 conference human factor computing system chi association computing machinery new york ny usa 2018 pp 30 chakraborti sreedharan grover kambhampati plan explanation model reconciliation international conference interaction hri ieee 2019 pp 31 chen yan wang user evaluation recommendation explanation acm trans interact intell syst 9 4 2019 32 cotter cho rader explaining news feed algorithm analysis news feed fyi blog proceeding 2017 chi conference extended abstract human factor computing system chi ea association computing machinery new york ny usa 2017 pp 33 darlington aspect intelligent system explanation univers control autom 1 2 2013 34 ehrlich kirk patterson rasmussen ross gruen taking advice intelligent system sword explanation proceeding international conference intelligent user interface iui association computing machinery new york ny usa 2011 pp 35 freitas comprehensible classiﬁcation model position paper acm sigkdd explor newsl 15 1 2014 2594475 36 gregor benbasat explanation intelligent system theoretical foundation implication practice mi q 23 4 1999 37 hois junk achieve explainability transparency human ai interaction international conference interaction hci springer 2019 pp 38 kizilcec much information effect transparency trust algorithmic interface proceeding 2016 conference human factor computing system chi association computing machinery new york ny usa 2016 pp 39 nagulendra vassileva providing awareness explanation control personalized ﬁltering social networking site inf syst front 18 1 2016 40 papenmeier englebienne seifert model accuracy explanation ﬁdelity inﬂuence user trust ai proceeding ijcai 2019 workshop explainable artiﬁcial intelligence xai 2019 pp 41 pierrard poli hudelot new approach explainable multiple organ annotation data proceeding ijcai 2019 workshop explainable artiﬁcial intelligence xai 2019 pp 42 putnam riegel conati towards personalized xai case study intelligent tutoring system proceeding ijcai 2019 workshop explainable artiﬁcial intelligence xai 2019 pp 43 rader cotter cho explanation mechanism supporting algorithmic transparency proceeding 2018 conference human factor computing system chi association computing machinery new york ny usa 2018 pp 44 rosenfeld richardson explainability system auton agent syst 33 6 2019 45 sato nagatani sonoda zhang ohkuma context style explanation recommender system inf process 27 2019 http doi 19 langer oster speith et al artiﬁcial intelligence 296 2021 103473 46 vig sen riedl tagsplanations explaining recommendation using tag proceeding international conference intelligent user interface iui association computing machinery new york ny usa 2009 pp 47 watt lécué local score dependent model explanation time dependent covariates proceeding ijcai 2019 workshop explainable artiﬁcial intelligence xai 2019 pp 48 zhou hu li yu chen physiological indicator user trust machine learning inﬂuence enhanced international conference machine learning knowledge extraction springer 2019 pp 49 herlocker konstan riedl explaining collaborative ﬁltering recommendation proceeding 2000 acm conference computer supported cooperative work cscw association computing machinery new york ny usa 2000 pp 50 cramer evers ramlal van someren rutledge stash aroyo wielinga effect transparency trust acceptance art recommender user model interact 18 5 2008 455 51 byrne counterfactuals explainable artiﬁcial intelligence xai evidence human reasoning proceeding national joint conference artiﬁcial intelligence 2019 pp 52 de laat algorithmic based machine learning big data transparency restore accountability philos technol 31 4 2018 53 floridi cowl beltrametti chatila chazerand dignum luetge madelin pagallo rossi schafer valcke vayena ethical framework good ai society opportunity risk principle recommendation mind mach 28 4 2018 54 lepri oliver letouzé pentland vinck fair transparent accountable algorithmic process philos technol 31 4 2018 55 mathews explainable artiﬁcial intelligence application nlp biomedical malware classiﬁcation literature review intelligent puting proceeding computing conference springer 2019 pp 56 mittelstadt allo taddeo wachter floridi ethic algorithm mapping debate big data soc 3 2 2016 http doi 57 pieters explanation trust tell user security ai ethic inf technol 13 1 2011 010 58 ra van gerven haselager explanation method deep learning user value concern challenge explainable interpretable model computer vision machine learning springer 2018 pp 59 riedl artiﬁcial intelligence machine learning hum behav emerg technol 1 1 2019 60 robbins misdirected principle catch explicability ai mind mach 29 4 2019 61 sheh different xai different hri aaai fall symposium aaai press 2017 pp 62 sheh monteath deﬁning explainable ai requirement analysis künstl intell 32 4 2018 0559 63 sokol flach explainability fact sheet framework systematic assessment explainable approach proceeding 2020 ence fairness accountability transparency association computing machinery new york ny usa 2020 pp 64 sokol flach one explanation doe not ﬁt künstl intell 34 2 2020 65 sridharan meadow towards theory explanation collaboration künstl intell 33 4 2019 1007 66 vellido importance interpretability visualization machine learning application medicine health care neural comput appl 2019 67 wang yang abdul lim designing explainable ai proceeding 2019 conference human factor computing system chi association computing machinery new york ny usa 2019 pp 68 lee jain cha ojha kusbit procedural justice algorithmic fairness proc acm interact 3 2019 http doi 69 doran schulz besold doe explainable ai really mean new conceptualization perspective ceur workshop proceeding vol 2071 ceur 2018 pp 70 krishnan interpretability critical examination interpretability problem machine learning philos technol 2019 http doi 71 peddoju saravanan suresh explainable classiﬁcation using clustering deep learning model proceeding ijcai 2019 workshop explainable artiﬁcial intelligence xai 2019 pp 72 rajani mooney using explanation improve ensembling visual question answering system proceeding ijcai 2017 workshop explainable artiﬁcial intelligence xai 2017 pp 73 zhou chen towards trustworthy teaming uncertainty proceeding ijcai 2019 workshop explainable artiﬁcial intelligence xai 2019 pp 74 anjomshoae najjar calvaresi främling explainable agent robot result systematic literature review proceeding international conference autonomous agent multiagent system aamas international foundation autonomous agent multiagent system richland sc usa 2019 pp 75 fox long magazzeni explainable planning proceeding ijcai 2017 workshop explainable artiﬁcial intelligence xai 2017 pp 76 jasanoff virtual visible actionable data assemblage sightlines justice big data soc 4 2 2017 2053951717724477 77 friedrich zanker taxonomy generating explanation recommender system ai mag 32 3 2011 78 holzinger langs denk zatloukal müller causability explainability artiﬁcial intelligence medicine wiley interdiscip rev data min knowl discov 9 4 2019 79 sevastjanova beck ell turkay henkin butt keim going beyond visualization verbalization complementary medium explain machine learning model vi workshop visualization ai explainability visxai 2018 pp 80 sørmo cassens aamodt explanation goal artif intell rev 24 2 2005 10 81 zerilli knott maclaurin gavaghan transparency algorithmic human double standard philos technol 32 4 2019 82 lucic haned de rijke contrastive explanation large error retail forecasting prediction monte carlo simulation ceedings ijcai 2019 workshop explainable artiﬁcial intelligence xai 2019 pp 83 dam tran ghose explainable software analytics proceeding international conference software engineering new idea emerging result association computing machinery new york ny usa 2018 pp 20 langer oster speith et al artiﬁcial intelligence 296 2021 103473 84 de winter explanation software engineering pragmatic point view mind mach 20 2 2010 010 85 juozapaitis koul fern erwig explainable reinforcement learning via reward decomposition proceeding ijcai 2019 workshop explainable artiﬁcial intelligence xai 2019 pp 86 michael machine coaching proceeding ijcai 2019 workshop explainable artiﬁcial intelligence xai 2019 pp 87 sokol flach conversational explanation machine learning prediction counterfactual statement proceeding international joint conference artiﬁcial intelligence 2018 pp 88 wicaksono sammut sheh towards explainable tool creation robot proceeding ijcai 2017 workshop explainable artiﬁcial intelligence xai 2017 pp 89 eiter saribatur schüller abstraction unsolvability reason problem proceeding ijcai 2019 shop explainable artiﬁcial intelligence xai 2019 pp 90 kulesza stumpf wong burnett perona ko oberst debugging naive bayes text classiﬁcation acm trans interact intell syst 1 1 2011 91 hoffman klein mueller explaining explanation explainable ai proc hum factor ergon soc ann meet 62 2018 http doi 92 nothdurft heinroth minker impact explanation dialogue trust international conference interaction hci springer 2013 pp 93 brinton framework explanation machine learning decision proceeding ijcai 2017 workshop explainable artiﬁcial gence xai 2017 pp 94 tintarev explanation recommendation proceeding 2007 acm conference recommender system association computing machinery new york ny usa 2007 pp 95 weber hong goel explaining citation recommendation abstract full text proceeding ijcai 2019 workshop explainable artiﬁcial intelligence xai 2019 pp 96 gilpin bau yuan bajwa specter kagal explaining explanation overview interpretability machine learning ieee international conference data science advanced analytics dsaa 2018 pp 97 ho biased sample reverse engineering ranking algorithm facebook graph application programming interface big data soc 7 1 2020 98 hohman head caruana deline drucker gamut design probe understand data scientist understand machine learning model proceeding 2019 conference human factor computing system chi association computing machinery new york ny usa 2019 pp 99 veale binns fairer machine learning real world mitigating discrimination without collecting sensitive data big data soc 4 2 2017 100 zednik solving black box problem normative framework explainable artiﬁcial intelligence philos technol 2019 10 101 goodman flaxman european union regulation algorithmic right explanation ai mag 38 3 2017 102 sklar azhar explanation argumentation proceeding international conference interaction hai association computing machinery new york ny usa 2018 pp 103 lage lifschitz amir exploring computational user model agent policy summarization proceeding ijcai 2019 workshop explainable artiﬁcial intelligence xai 2019 pp 104 dahl appraising technology positive prospect philos technol 31 4 2018 0275 105 ghosh malioutov meel interpretable classiﬁcation rule relaxed logical form proceeding ijcai 2019 workshop explainable artiﬁcial intelligence xai 2019 pp 106 stuart nersessian peeking inside black box new kind scientiﬁc visualization mind mach 29 1 2019 1007 107 clos wiratunga massie towards explainable text classiﬁcation jointly learning lexicon modiﬁer term proceeding ijcai 2017 workshop explainable artiﬁcial intelligence xai 2017 pp 108 zhu liapis risi bidarra youngblood explainable ai designer perspective ieee conference computational intelligence game cig ieee 2018 pp 109 clinciu hastie survey explainable ai terminology proceeding workshop interactive natural language technology explainable artiﬁcial intelligence 2019 association computational linguistics 2019 pp 110 henin le métayer towards generic framework explanation method proceeding ijcai 2019 workshop explainable artiﬁcial intelligence xai 2019 pp 111 madumal miller sonenberg vetere grounded interaction protocol explainable artiﬁcial intelligence proceeding international conference autonomous agent multiagent system aamas international foundation autonomous agent multiagent system richland sc usa 2019 pp 112 olson neal li wong counterfactual state atari agent via generative deep learning proceeding ijcai 2019 workshop explainable artiﬁcial intelligence xai 2019 pp 113 zeng miao leung chin building explainable artiﬁcial intelligence argumentation proceeding aaai conference artiﬁcial intelligence aaai press 2018 pp 114 madumal miller sonenberg vetere explainable reinforcement learning causal lens proceeding ijcai 2019 workshop explainable artiﬁcial intelligence xai 2019 pp 115 ribeiro singh guestrin trust explaining prediction any classiﬁer proceeding acm sigkdd international conference knowledge discovery data mining association computing machinery new york ny usa 2016 pp 116 endsley autonomy hum factor hum factor ergon soc 59 1 2017 117 lee see trust automation designing appropriate reliance hum factor 46 1 2004 30392 118 parasuraman riley human automation use misuse disuse abuse hum factor hum factor ergon soc 39 2 1997 119 hoff bashir trust automation hum factor 57 3 2014 120 parasuraman manzey complacency bias human use automation attentional integration hum factor 52 3 2010 121 kunze summerskill marshall filtness automation transparency implication uncertainty communication interaction interface ergonomics 62 3 2019 21 langer oster speith et al artiﬁcial intelligence 296 2021 103473 122 samek wiegand müller explainable artiﬁcial intelligence understanding visualizing interpreting deep learning model corr arxiv 1708 ab 2017 123 montavon samek müller method interpreting understanding deep neural network digit signal process 73 2018 http doi 124 becker ackermann lapuschkin müller samek interpreting explaining deep neural network classiﬁcation audio signal corr ab 2018 125 lapuschkin binder montavon muller samek analyzing classiﬁers ﬁsher vector deep neural network proceeding ieee conference computer vision pattern recognition 2016 pp 126 caruana lou gehrke koch sturm elhadad intelligible model healthcare predicting pneumonia risk hospital readmission proceeding acm sigkdd international conference knowledge discovery data mining association computing machinery new york ny usa 2015 pp 127 baum hermann speith machine ethic machine explainability back international symposium artiﬁcial intelligence mathematics isaim 2018 pp 128 luetge german ethic code automated connected driving philos technol 30 2017 0284 129 purkiss perrewé gillespie mayes ferris implicit source bias employment interview judgment decision organ behav hum decis process 101 2 2006 130 caliskan bryson narayanan semantics derived automatically language corpus contain bias science 356 2017 131 guidotti monreale ruggieri turini giannotti pedreschi survey method explaining black box model acm comput surv 51 5 2019 132 venkatesh morris davis davis user acceptance information technology toward uniﬁed view manag inf syst q 27 3 2003 133 mcleod trust zalta ed stanford encyclopedia philosophy fall 2015 edition metaphysics research stanford university 2015 pp 134 raji smart white mitchell gebru hutchinson theron barnes closing ai accountability gap deﬁning framework internal algorithmic auditing proceeding 2020 conference fairness accountability transparency fat association computing machinery new york ny usa 2020 pp 135 matthias responsibility gap ascribing responsibility action learning automaton ethic inf technol 6 3 2004 http doi 136 deci olafsen ryan theory work organization state science ann rev organ psychol organ behav 4 1 2017 137 longoni bonezzi morewedge resistance medical artiﬁcial intelligence consum 46 4 2019 jcr 138 keil explanation understanding annu rev psychol 57 1 2006 139 bonnefon shariff rahwan social dilemma autonomous vehicle science 352 2016 140 buchanan shortliffe expert system mycin experiment stanford heuristic programming project 1984 141 dhaliwal benbasat use effect system explanation theoretical foundation framework empirical ation inf syst 7 3 1996 142 köhl baum langer oster speith bohlender explainability requirement ieee international ments engineering conference 2019 pp 143 de regt understanding scientiﬁc understanding oxford university press 2017 144 baumberger beisbart brun understanding overview recent debate epistemology philosophy science baumberger ammon ed explaining understanding new perspective epistemology philosophy science routledge 2017 pp 145 malfatti understanding testimony erkenntnis 2019 146 baumberger type understanding nature relation knowledge conceptus 40 98 2014 2014 147 lambert whether answer explanation only yield scientiﬁc understanding brittan ed causality method modality vol 48 springer dordrecht netherlands 1991 pp 148 lombrozo carey functional explanation function explanation cognition 99 2 2006 2004 149 chi de leeuw chiu lavancher eliciting improves understanding cogn sci 18 3 1994 10 150 mayer cognition instruction historic meeting within educational psychology educ psychol 84 4 1992 10 151 mueller hoffman clancey emrey klein explanation system literature synopsis key idea publication bibliography explainable ai corr ab 2019 152 kelp understanding phenomenon synthese 192 12 2015 153 feltovich coulson spiro learner mi understanding important diﬃcult concept challenge smart machine education smart machine education coming revolution educational technology mit press cambridge usa 2001 pp 154 rouse morris looking black box prospect limit search mental model psychol bull 100 3 1986 155 rozenblit keil misunderstood limit folk science illusion explanatory depth cogn sci 26 5 2002 1207 156 kuhn people know psychol sci 12 1 2001 157 kulesza stumpf burnett yang kwan wong much little right way explanation impact end user mental model ieee symposium visual language human centric computing ieee 2013 pp 158 tullio dey chalecki fogarty work ﬁeld study user interacting intelligent system proceeding 2007 conference human factor computing system chi association computing machinery new york ny usa 2007 pp 159 mitchell wu zaldivar barnes vasserman hutchinson spitzer raji gebru model card model reporting proceeding 2019 conference fairness accountability transparency association computing machinery new york ny usa 2019 pp 160 langer könig fitili information sword role computer experience information applicant reaction towards novel technology personnel selection comput hum behav 81 2018 22 langer oster speith et al artiﬁcial intelligence 296 2021 103473 161 newman fast harmon eliminating bias fair algorithmic reductionism procedural justice human resource decision organ behav hum decis process 160 2020 162 bazire brézillon understanding context using dey kokinov leake turner ed modeling using context springer 2005 pp 163 dourish talk talk context pers ubiquitous comput 8 1 2004 164 bobocel zdaniuk explanation used foster organizational justice handbook organizational justice 2005 pp 165 folger cropanzano fairness theory justice accountability advance organizational justice vol 1 2001 pp 166 shaw wild colquitt justify excuse review effect explanation appl psychol 88 3 2003 167 brockner wiesenfeld integrative framework explaining reaction decision interactive effect outcome procedure psychol bull 120 2 1996 168 wang harper zhu factor inﬂuencing perceived fairness algorithmic algorithm outcome development procedure individual difference proceeding 2020 conference human factor computing system chi association computing ery new york ny usa 2020 pp 169 lind van den bos fairness work toward general theory uncertainty management organ behav 24 2002 http doi 02 24006 170 colquitt chertkoff explaining injustice interactive effect explanation outcome fairness perception task motivation manag 28 5 2002 171 liu li task complexity review conceptualization framework int ind ergon 42 6 2012 2012 172 wilkenfeld functional explaining new approach philosophy explanation synthese 191 2014 173 wilkenfeld plunkett lombrozo depth deference attribute understanding philos stud 173 2016 http doi 174 lombrozo instrumental value explanation philos compass 6 8 2011 175 williams lombrozo explanation prior knowledge interact guide learning cogn psychol 66 1 2013 cogpsych 176 lombrozo rehder function biological kind classiﬁcation cogn psychol 65 4 2012 002 177 hempel explanation aspect scientiﬁc explanation free press 1965 pp 178 salmon scientiﬁc explanation causal structure world princeton university press 1984 179 gärdenfors knowledge flux modeling dynamic epistemic state mit press 1988 180 wachter mittelstadt russell counterfactual explanation without opening black box automated decision gdpr harv law technol 31 2 2018 181 craver explaining brain oxford university press oxford 2007 182 pearl causality model reasoning inference edition cambridge university press 2009 183 spirtes glymour scheines causation prediction search edition mit press 2001 184 borsboom cramer kali brain disorder not really network structure block reductionism psychopathology research behav brain sci 42 2018 185 lombrozo simplicity probability causal explanation cogn psychol 55 3 2007 186 vasilyeva wilkenfeld lombrozo contextual utility affect perceived quality explanation psychon bull rev 24 2017 187 bellotti edward intelligibility accountability human consideration system interact 16 2001 188 hartley bendixen educational research internet age examining role individual characteristic educ 30 9 2001 189 kauffman review predictive factor student success satisfaction online learning learn technol 23 2015 http doi 190 mcnamara kintsch songer kintsch good text always better interaction text coherence background knowledge level understanding learning text cogn instr 14 1 1996 191 goldberg language individual difference search universal personality lexicon wheeler ed review personality social psychology vol 2 edition sage publication 1981 pp 192 cacioppo petty need cognition pers soc psychol 42 1 1982 193 haugtvedt petty personality persuasion need cognition moderate persistence resistance attitude change pers soc psychol 63 2 1992 194 debacker crowson inﬂuence need closure learning teaching educ psychol rev 21 2009 1007 195 webster kruglanski individual difference need cognitive closure pers soc psychol 67 6 1994 1037 196 fernbach sloman louis shube explanation ﬁends foe mechanistic detail determines understanding preference consum 39 5 2012 197 hasher zacks working memory comprehension aging review new view psychology learning motivation elsevier 1988 pp 198 ackerman lauterman taking reading comprehension exam screen paper metacognitive analysis learning text time pressure comput hum behav 28 5 2012 199 prewett johnson saboe elliott coovert managing workload interaction review empirical study comput hum behav 26 5 2010 200 starcke wolf markowitsch brand anticipatory stress inﬂuences decision making explicit risk condition behav neurosci 122 6 2008 201 lupien maheu tu fiocco schramek effect stress stress hormone human cognition implication ﬁeld brain cognition brain cogn 65 3 2007 202 hancock process automation transition multitask system ieee trans syst man part syst hum 37 4 2007 203 chazette karras schneider want explanation analyzing role explainability emerging aspect requirement ieee international requirement engineering conference 2019 pp 23 langer oster speith et al artiﬁcial intelligence 296 2021 103473 204 chazette schneider explainability requirement challenge recommendation requir eng 25 4 2020 205 arya bellamy chen dhurandhar hind hoffman houde liao lu c mourad pedemonte raghavendra richards sattigeri shanmugam singh varshney wei zhang one explanation doe not ﬁt toolkit taxonomy ai explainability technique corr ab 2019 206 woodward scientiﬁc explanation zalta ed stanford encyclopedia philosophy winter 2019 edition metaphysics research stanford university 2019 pp 207 hall harborne tomsett galetic nottle preece systematic method understand requirement explainable ai xai system proceeding ijcai 2019 workshop explainable artiﬁcial intelligence xai 2019 pp 208 miller howe sonenberg explainable ai beware inmate running asylum learnt stop worrying love social behavioural science proceeding ijcai 2017 workshop explainable artiﬁcial intelligence xai 2017 pp 209 kim rudin shah bayesian case model generative approach reasoning prototype classiﬁcation advance neural information processing system 2014 pp 210 kim khanna koyejo example not enough learn criticize criticism interpretability advance neural information processing system 2016 pp 211 carroll rosson paradox active user interfacing thought cognitive aspect interaction mit press cambridge usa 1987 pp 24