received august 5 2018 accepted september 4 2018 date publication september 17 2018 date current version october 12 digital object identifier peeking inside survey explainable artificial intelligence xai amina adadi mohammed berrada computer interdisciplinary physic laboratory sidi mohammed ben abdellah university fez 30050 morocco corresponding author amina adadi abstract dawn fourth industrial revolution witnessing fast widespread adoption artiﬁcial intelligence ai daily life contributes accelerating shift towards algorithmic society however even unprecedented advancement key impediment use system often lack transparency indeed nature system allows powerful prediction not directly explained issue ha triggered new debate explainable ai xai research ﬁeld hold substantial promise improving trust transparency system recognized sine qua non ai continue making steady progress without disruption survey provides entry point interested researcher practitioner learn key aspect young rapidly growing body research related xai lens literature review existing approach regarding topic discus trend surrounding sphere present major research trajectory index term explainable artiﬁcial intelligence interpretable machine learning model introduction context nowadays artiﬁcial intelligence ai democratized everyday life put phenomenon number international data corporation idc forecast global investment ai grow 12 billion dollar 2017 billion dollar 2021 1 meanwhile statistic portal statista expects revenue ai market worldwide grow 480 billion dollar 2017 trillion dollar 2021 2 gartner identiﬁes ai inescapable technology among gartner top 10 strategic technology trend along immersive experience digital twin tinuous adaptive security shaping next generation digital business model ecosystem 3 consequently proliferation ai signiﬁcant impact society indeed ai ha already become ubiquitous become accustomed ai making decision u daily life product movie recommendation netﬂix amazon friend suggestion facebook tailored advertisement google search result page however decision disease si important know reason behind critical decision crucial need explaining ai outcome becomes fully apparent problematically though appear powerful term result prediction ai algorithm suffer opacity difﬁcult get insight internal nism work especially machine learning ml algorithm compound problem entrusting important decision system not explain present obvious danger address issue explainable artiﬁcial intelligence xai proposes make shift towards transparent ai aim create suite technique produce explainable model whilst maintaining high performance level xai landscape dynamic xai ha gaining increasing attention recently growing dynamic around ﬁeld ha reﬂected several scientiﬁc event example annual international conference series dedicated exclusively topic include fairness accountability transparency shop kdd 4 icml workshop human interpretability machine learning whi 5 topic ha also become key concern panel cussions speciﬁc session major conference nip 2016 workshop interpretable ml complex tems 6 ijcai 2017 2018 workshop 52138 2018 ieee translation content mining permitted academic research only personal use also permitted requires ieee permission see information volume 6 2018 adadi berrada peeking inside survey xai explainable artiﬁcial intelligence 7 xci 2017 able computational intelligence 8 ijcnn 2017 explainability learning machine 9 year 2018 ﬂourishing wide range dedicated workshop topic 2018 workshop explainable artiﬁcial intelligence 10 icaps 2018 workshop explainable ai planning 11 hri 2018 workshop explainable robotic system 12 acm intelligent user interface iui 2018 workshop explainable smart system ex 2018 13 ipmu 2018 advance explainable artiﬁcial gence 14 ﬁnally iccbr 2018 organize xcbr first workshop reasoning nation intelligent system 15 analysis xai landscape lead identify key player inﬂuencers behind intense dynamic indeed two prominent actor pursuing xai research group academic operating acronym 4 ii civilian military researcher funded defense advanced research project agency darpa 16 meaning fairness accountability transparency multiple artiﬁcial intelligence machine learning computer science legal social science policy application primarily focused promoting enabling explainability fairness algorithmic system social commercial impact 500 participant 70 paper held ﬁfth annual event ary 2018 brings together annually researcher er interested fairness accountability transparency system group darpa launched xai program 2017 aim developing new technique capable making intelligent system explainable program includes 11 project continue running darpa funded researcher seem primarily interested increasing explainability sophisticated pattern recognition model needed security application even though darpa funded u department defense program involves researcher drawn various academic tions diverse corporate team increasing interest xai ha also observed industrial community company cutting edge contributing make ai explainable include driverless ai product 17 microsoft next generation azure azure ml kyndi xai platform government ﬁnancial service fico credit risk push state xai even fico running explainable machine learning challenge xml challenge 18 goal challenge identify new approach creating machine learning based ai model high accuracy 1 2 3 explainability hand cognilytica ha ined ai positioning matrix capm market ai product proposed chart xai technology arguably identiﬁed implementation beyond threshold actual technology 19 contribution organization motivated preceding concern observation article collect share ﬁndings comprehensive survey xai fact considering xai ﬁeld term research propose step back holistic view present advancement research ﬁeld order chart path toward promising suitable direction future research unlike study focus speciﬁc dimension explainability work advocate multidisciplinary nature studied ﬁeld introduces major aspect domain explainability different perspective key claim paper issue explaining system scientiﬁcally interesting increasingly important hence necessity providing ﬁrm basis lens literature ground discussion aim help interested researcher quickly effectively grasp important facet topic clear idea key aspect related body research sense make three contribution propose comprehensive background regarding main concept motivation implication enabling explainability intelligent system based literature analysis 381 paper provide organized overview existing xai approach identify discus future research opportunity potential trend ﬁeld accordingly remainder survey organized follows section ii present preliminary background section iii survey latest development xai ﬁeld organizes surveyed approach according four spectives section iv discus research direction open problem gathered distilled literature survey finally section v concludes survey ii background understanding xai contextual definition xai research ﬁeld aim make ai system result understandable human term wa ﬁrst coined 2004 van lent et al 20 describe ability system explain behavior entity simulation game application term relatively new problem ability ha existed since researcher studied explanation expert system 21 however pace progress towards resolving problem ha slowed ai reached inﬂection point tacular advance ml since focus ai research ha shifted towards implementing model algorithm volume 6 2018 52139 adadi berrada peeking inside survey xai figure google trend result research interest artificial intelligence emphasis predictive power ability explain decision process ha taken back seat recently xai topic ha received renewed attention academia practitioner figure 1 illustrates able resurgence xai term research interest using google trend research topic direct result unstoppable penetration across try crucial impact critical ce without able provide detailed information chain reasoning lead certain decision recommendation prediction action made fore social ethical legal pressure call new ai technique capable making decision explainable understandable technically no standard generally accepted deﬁnition explainable ai actually xai term tends refer movement initiative effort made response ai transparency trust concern formal technical concept thus put some clariﬁcation around trend quote some xai deﬁnitions seen calling according darpa 16 xai aim produce explainable model maintaining high level learning performance prediction accuracy enable human user understand appropriately trust effectively manage emerging generation artiﬁcially intelligent partner goal enabling explainability ml stated 4 ensure algorithmic decision well any data driving decision explained stakeholder term fico 19 organizer xml challenge see xai innovation towards opening ml challenge create model technique accurate provide good trustworthy explanation satisﬁes customer need interested researcher believe important gain deep understanding xai concept beyond colloquial deﬁnitions primary goal shallow fact propose explore big picture key concept shaping xai landscape performing scan literature detailed next section conducted linguistic search identify record relevant term across research nities strongly relate concept xai goal analysis gain insight research community approach explainability detect main concept contribute deﬁne notion result word cloud shown figure 2 provides intuitive grasp xai scope allows drawing big picture research ﬁeld highlighting important related concept important term ordered according frequency appearance keywords surveyed paper ﬁltering technical term like deep learning decision tree sensitive analysis etc figure xai word cloud table 1 cast insight sample relevant common related xai concept believe help deﬁne contextually studied ﬁeld detailed table 1 xai not monolithic concept reﬂects several distinct related notion explainability closely related concept interpretability interpretable 52140 volume 6 2018 adadi berrada peeking inside survey xai table key related concept xai system explainable operation stood human note even though explainable keyword xai appellation ml community term interpretable used explainable figure 3 conﬁrms observation show trend regarding use two term scientiﬁc public setting volume 6 2018 52141 adadi berrada peeking inside survey xai figure google trend result comparing use explainable interpretable according context furthermore noted none mentioned variation term understandable comprehensible intelligible enough speciﬁc enable formalization implicitly depend user expertise preference contextual variable xai centered challenge demystifying black box also implies responsible ai help duce transparent model happen without ing ai model accuracy thus ai general ml speciﬁcally often tradeoff must made accuracy interpretability obvious link data science ﬁeld arises accuracy closely tied quality quantity training data rarely literature come across term social ence derivative yet explanation form social interaction clearly ha psychological cognitive philosophical projection based conducted analysis idea social science human behavior not ciently visible ﬁeld finally xai part new generation ai gy called third wave ai one objective ambition wave precisely generate algorithm explain ultimately culminates quest reaching human intelligence level goal known agi based term analysis built uniﬁed structured view main concept related xai ﬁeld illustrated figure 4 believe aiming holism approaching xai concept help researcher quickly initiated topic context moreover knowing main keywords used ﬁeld variation term relatively referring concept represent helpful prerequisite conduct relevant fruitful research using xai need application opportunity 1 need xai commercial beneﬁts ethic concern regulatory consideration xai essential user understand appropriately trust effectively manage ai result based explored literature need explaining ai system may stem least four reason although may appear overlap four reason standpoint capture different motivation explainability explain justify past several year seen multiple controversy enabled system yielding biased discriminatory result 37 38 implies increasing need nation ensure ai based decision not made erroneously talk explanation sion generally mean need reason justiﬁcations particular outcome rather description inner working logic reasoning behind making process general using xai system provides required information justify result particularly unexpected decision made also ensures auditable provable way defend algorithmic decision fair ethical lead building trust furthermore henceforth ai need provide justiﬁcations order compliance legislation instance right explanation regulation included figure schematic view xai related concept 52142 volume 6 2018 adadi berrada peeking inside survey xai general data protection regulation gdpr come effect across eu may 25 2018 39 b explain control explainability not important justifying decision also help prevent thing going wrong indeed understanding system behavior provides greater visibility unknown vulnerability ﬂaws help rapidly identify correct error low criticality tions debugging thus enabling enhanced control c explain improve another reason building explainable model need continuously improve model explained understood one easily improved user know system produced speciﬁc put also know make smarter thus xai could foundation ongoing iteration ment human machine explain discover asking explanation helpful tool learn new fact gather information thus gain knowledge only explainable system useful example given alphago zero 40 excel game go much better human player would desirable machine explain learned strategy knowledge u come no surprise future xai model taught u new hidden law biology chemistry physic conclude explainability powerful tool justifying ai based decision help verify prediction improving model gaining new insight problem hand lead towards trustworthy ai system figure 5 figure reason xai even though academic practitioner approve importance xai not everyone agrees pressing need greater interpretability ai system regard value xai wa called question recently google research director norvig 41 noted human not good explaining decision either claimed credibility ai system result could gauged simply observing output time ai powerhouse company researcher ha stressed indeed important point certainly explainability essential property however not always necessity fact ing every ai system explain every decision could result le efﬁcient system forced design choice bias towards explainable le capable versatile outcome furthermore making ai system explainable undoubtedly expensive require considerable resource development ai system way interrogated practice thus important think explanation useful need explainability depends degree functional opacity caused complexity ai algorithm low no high level interpretability required b degree resistance application domain error ha high tance unexpected error acceptable ai system targeted advertising example relatively low level interpretability could sufﬁce consequence going wrong negligible hand interpretability diagnosis system would signiﬁcantly higher any error could not only harm patient also deter adoption system therefore any domain cost making wrong prediction high present potential application domain xai approach 2 xai application domain interestingly xai bring signiﬁcant beneﬁt large range domain relying ai system herein explore some potential domain need research work explainable model transportation automated vehicle hold promise decreasing ﬁc death providing enhanced mobility also pose challenge addressing explainability ai decision autonomous vehicle make decision based classify object scene front car suddenly act abnormally some misclassiﬁcation problem consequence dangerous not possibility already happening recently uber killed woman arizona wa ﬁrst known fatality involving fully autonomous vehicle information reported anonymous source claimed car software registered object front vehicle treated way would plastic bag tumbleweed carried wind 42 only able system clarify ambiguous circumstance situation eventually prevent happening transportation potential application domain xai work towards explaining driving vehicle behavior volume 6 2018 52143 adadi berrada peeking inside survey xai ha already started 43 44 long way go b healthcare medical diagnosis model responsible human life conﬁdent enough treat patient instructed model artiﬁcial neural network ann wa trained predict pneumonia patient admitted hospital treated outpatient tial ﬁndings indicated neural net far accurate classical statistical method however sive test turned neural net inferred pneumonia patient asthma lower risk dying not admitted medically tuitive however reﬂected real pattern training patient pneumonia usually ted not only hospital directly icu treated aggressively survived 37 wa decided ai system wa dangerous use clinically only interpreting model discover crucial problem avoid recently researcher conducted preliminary work aiming make clinical system explainable 37 45 47 ing number work conﬁrms challenge interest applying xai approach healthcare domain c legal criminal justice ai ha potential better ass risk recidivism reduce cost associated crime incarceration however using criminal decision model predict risk recidivism court make sure model behaves equitable honest nondiscriminatory manner loomis wisconsin 48 case challenged use proprietary closed source risk assessment software sentencing loomis prison case alleged software correctional offender agement proﬁling alternative sanction compas 49 violates process right taking gender race account algorithm used considered trade secret causal audit process wa not clearly known judge transparency decision made sity critical domain yet work made towards making automated decision making legal system explainable 49 51 finance ﬁnancial service beneﬁts using ai tool include improvement related activity access investment advice customer service however tool also pose question around data security fair lending indeed ﬁnancial industry highly regulated loan issuer required law make fair decision thus one signiﬁcant challenge using system credit score model harder provide needed reason code borrower explanation denied credit especially basis denial output opaque ml algorithm some credit bureau agency equifax experian working ing research project generate automated reason code make ai score decision explainable auditor friendly 52 e military originally current famous xai initiative made military researcher 16 growing visibility xai topic due largely call research solicitation dapra project unsurprisingly ai military arena also suffers ai explainability problem report mit nology review knight 53 delf challenge relying autonomous system military operation healthcare domain often involves life death decision lead similar type ethical legal dilemma academic ai research community well represented application domain dapra ambitious xai program along some research initiative study explainability domain 54 line work made discussed domain conﬁrms need xai however work only infancy hearty research effort yet done moreover xai ﬁnd interesting application others domain like cybersecurity education entertainment ernment image recognition etc interesting chart tial harm automated wa presented future privacy forum 55 depicts various sphere life automated cause injury providing automated explanation turn trustful process includes employment insurance social beneﬁt housing differential pricing good service enabling xai technical challenge clearly awareness demand explainability growing various domain hence question use xai not systematic simply not everyone using xai fact bringing interpretability ai system challenging technical issue explainability intelligent tems ha run gamut traditional expert system totally explainable inﬂexible hard use deep neural network dnn effective virtually impossible see inside look back expert system 80 would consider scrutable system inference engine leveraged knowledge base make assertion could explain using chain reasoning led assertion 56 explanation capability frequently signiﬁcant beneﬁt provided expert system system completely built subject matter 52144 volume 6 2018 adadi berrada peeking inside survey xai expertise powerful somewhat inﬂexible moreover though signiﬁcant progress wa made ability period solid principle established however explainability problem wa not considered completely solved 57 modern machine learning algorithm go opposite end spectrum yielding system capable working purely observation creating tations world base prediction nevertheless complexity bestows extraordinary predictive ability ml algorithm also make result algorithm produce hard understand indeed ml rithms difﬁcult interpret structure way working ml algorithm intrinsically sider interaction input feature make disaggregating function human able form difﬁcult take example dnn successful contemporary ml model dnn ha generic nonlinear structure consisting many hidden layer numerous number neuron per layer tecture help produce prediction ple level linear transformation activation single linear transformation may interpreted looking weight input feature output class multiple layer interaction every layer imply disentangling super complicated nested structure difﬁcult task potentially even questionable one 58 another perception ml interpretability technical lenge explained hall gill 59 introduction machine learning interpretability book present mathematical problem interpretable ml multiplicity good model 60 mentioned given complicated structure ml model set input variable prediction target complex machine learning algorithm produce multiple accurate model taking similar not internal pathway network detail explanation also change across multiple accurate model systematic instability make automated generated explanation difﬁcult arguing interpretability challenging issue doe not mean technique level opacity indeed algorithm interpretable others often tradeoff accuracy interpretability accurate model usually not explainable example deep neural net boosted tree random forest support vector machine interpretable model usually le accurate example linear logistic regression rather static tradeoff accuracy along pretability dynamic target ongoing research try reach discussed section furthermore beyond technical challenge given goal nature implication xai progress towards overcoming challenge only achieved interdisciplinary collaboration expertise theory different research ﬁelds combined method technique developed multiple perspective move research forward standpoint enabling gy method xai potentially belong four basic research area data science algorithm data hungry need data produce better prediction decision backward path target produce better explanation justiﬁcation eventually also depends data data science core element ity process ii artiﬁcial learning generate explanation need computational process claim using computational process explain interesting work trail iii human science produce artiﬁcial explanation worth ﬁrst model human explain decision behavior 33 therefore approaching theory human ence lead innovative explainable model iv human computer interaction hci user understanding trust system partly depends way interacts machine given hci 61 core interest ogy entail understanding better empowering user technique research ﬁeld help developing transparent system focusing tried propose extensive background regarding xai deﬁning concept xai exposing motivation behind reemergence identifying segment ket result promising ﬁnally presenting some potential research area could potentially contribute overcome technical challenge related xai system next section aim capture researcher attention growing research body xai literature survey iii review related survey despite fact volume research interpretable explainable ai quickly expanding holistic survey systematic classiﬁcation research work missing indeed according literature review paper ﬁeld two inescapable position paper 25 62 try formalize concept explainability former attempted provide taxonomy desideratum method interpretability research lipton work not survey provides solid discussion might constitute interpretability lens literature survey kim tried deﬁne onomies best practice interpretability rigorous science main contribution paper taxonomy interpretability evaluation author shifted focus only one dimension expandability surement survey abdul et al 61 analyzed sizable ature explainable research based 289 core paper volume 6 2018 52145 adadi berrada peeking inside survey xai figure surveyed article year 12 412 citing paper built citation network however work focus mainly setting hci research agenda explainability recent survey guidotti et al 63 reviewed method explaining model large scale including data mining well machine learning presented detailed taxonomy explainability method according type problem faced even though survey considered holism term model discus model emphasized only interpretability mechanism ignoring explainability dimension evaluation hence detailed technical overview surveyed method make hard get quick understanding explanation method space finally dosilovic et al 64 proposed general overview topic conference paper presented advance explainability machine learning model supervised learning paradigm particular focus dnn contrast existing survey focus particular aspect explainability survey provides sive organized overview xai research contribution different perspective target holism clarity exploring exposing explainable approach space holistic survey conducted extensive literature review ing relevant paper six major academic database scopus ieeexplore acm digital library google scholar citeseer library sciencedirect addition preprints posted arxiv keyword based search wa used select paper consists searching index keywords based common variation literature term intelligible interpretable transparency black box understandable comprehensible explainable union set term related ai including artiﬁcial intelligence intelligent system machine learning term referring ml algorithm deep learning classiﬁer decision tree mainly interested recent advance ﬁeld research wa restricted article published 2004 gathered paper scanned based title abstract keywords determine relevant article analysis list selected paper wa largely completed afterwards using ward forward snowballing strategy consists using reference list selected paper citation paper identify additional paper 65 ﬁnal list paper includes 381 paper publication timeline paper shown figure 6 illustrates recent exponential increase paper ﬁeld next present brief pointer relevant work approaching different perspective broad stroke describe xai space along four main ax one spanning spectrum together shape holistically xai research landscape indeed aim present comprehensive holistic analysis state art ﬁeld projecting work four complementary ax not intend enumerate surveyed paper however try meet two criterion paper included discussion deemed signiﬁcant work received high citation level b good coverage corresponding axis axis xai method taxonomy explainability strategy quest make ai system explainable several nation method strategy proposed tively short period especially ml algorithm axis propose overview existing interpretability method 52146 volume 6 2018 adadi berrada peeking inside survey xai majority work discus explainability ml rithms thus interpretability term usually used based conducted survey literature arrive classify method according three criterion plexity interpretability ii scoop interpretability iii level dependency used ml model note since explainability ai still emerging ﬁeld class method belonging proposed omy neither mutually exclusive exhaustive however good yardstick compare contrast across multiple method following subsection describe main tures class give example current research complexity related method complexity model directly related interpretability generally complex model difﬁcult interpret explain thus straightforward way get interpretable would design algorithm inherently intrinsically interpretable many paper support classic approach name letham et al 66 proposed model called bayesian rule list brl based decision tree author claimed preliminary interpretable model provide concise vincing capability gain domain expert trust caruana et al 37 described application learning method based generalized additive model monia problem proved intelligibility model case study real medical data xu et al 67 introduced attention based model automatically learns describe content image showed visualization model able pret result ustun rudin 68 presented sparse linear model creating scoring system called slim result work highlight interpretability ity proposed system provide user qualitative understanding due high level sparsity small integer coefﬁcients common challenge hinders usability class method tradeoff interpretability accuracy 69 noted breiman 70 accuracy generally requires complex prediction method simple interpretable function not make accurate predictor sense intrinsic interpretable model come cost accuracy alternative approach interpretability machine learning construct high complex uninterpretable model high accuracy subsequently use separate set technique perform could deﬁne reverse engineering process provide needed tions without altering even knowing inner work original model class method offer explanation 25 though could signiﬁcantly complex costly recent work done xai ﬁeld belong class includes natural language explanation 71 visualization learned model 72 explanation example 73 approach presented detail next subsection light conclude overall utility value interpretability depends nature prediction task long model accurate task us reasonably restricted number internal component intrinsic interpretable model sufﬁcient otherwise prediction target involved complex highly accurate el considering interpretation model necessary also noted literature group intrinsic method complex uninterpretable model method aim modify internal structure complex model not primarily interpretable ically mitigated opacity thus improve interpretability 74 used method may either component add additional capability component belong model architecture 75 76 part loss function 77 part architecture structure term operation layer 78 79 scoop related method interpretability implies understanding automated model support two variation according scoop pretability understanding entire model behavior standing single prediction studied literature contribution made direction accordingly distinguish two subclass global pretability ii local interpretability 1 global interpretability global interpretability facilitates understanding whole logic model follows entire reasoning leading different possible outcome class method helpful ml model crucial inform population level decision drug consumption trend climatic change 80 case global effect estimate would helpful many explanation possible idiosyncrasy work propose globally interpretable model include aforementioned additive model predicting nia risk 37 rule set generated sparse bayesian generative model 66 however model usually speciﬁcally structured thus limited predictability serve interpretability yang et al 80 proposed global model interpretation via recursive partitioning called girp build global interpretation tree wide range ml model based local explanation experiment author highlighted method discover whether ular ml model behaving reasonable way overﬁt some unreasonable pattern et al 81 proposed supervised approach information extraction provides global volume 6 2018 52147 adadi berrada peeking inside survey xai deterministic interpretation work support idea representation learning successfully combined traditional bootstrapping yielding model interpretable nguyen et al 82 proposed approach based vation preferred input neuron neural learned prior form deep generator network produce global interpretable model image recognition activation maximization nique wa previously used erhan et al 83 even though multitude technique used ture enable global interpretability arguably global model interpretability hard achieve practice especially model exceed handful parameter analogically human focus effort only part model order comprehend whole local interpretability readily applicable 2 local interpretability explaining reason speciﬁc decision single diction mean interpretability occurring locally scoop interpretability used generate individual explanation generally justify model made ciﬁc decision instance several explored paper pose local explanation method provide next overview explanation method examined reviewed paper ribeiro et al 84 proposed lime local interpretable explanation model approximate model locally neighborhood any diction interest newer related highly anticipated work creator lime called anchor 85 extends lime using decision rule vein loco 86 another popular technique generating local explanation model offer local variable importance measure another attempt produce local explanation made baehrens et al 87 work author presented method capable explaining local decision taken arbitrary nonlinear classiﬁcation algorithm using local gradient characterize data point ha moved change predicted label following line work ﬁnd set work using similar method image classiﬁcation model 88 91 actually common approach understanding decision image classiﬁcation system ﬁnding region image particularly inﬂuential ﬁnal classiﬁcation also called sensitivity map saliency map pixel attribution map 92 approach use occlusion technique culations gradient assign importance value individual pixel meant reﬂect inﬂuence ﬁnal classiﬁcation based decomposition model prediction individual contribution feature sikonja kononenko 93 proposed explain model prediction one instance measuring difference original prediction one made omitting set feature recent work use decomposition explain locally include 94 95 several different technique obtaining 96 100 recent work lundberg lee 101 shown equivalence among technique introduced promising newer nique solid theoretical support called shapely tions uniﬁes local approach interesting promising line work focusing combining strength beneﬁts local global interpretability example include 102 104 four possible combination standard global model interpretability answer doe model make tions ii global model interpretability modular level identiﬁes part model inﬂuence prediction iii local interpretability group prediction indicates model make speciﬁc decision group instance iv ﬁnally usual local interpretability single prediction used justify model make speciﬁc decision instance 105 another observation noted reviewed literature local explanation used od generate explanation dnns however even though approach developed explain neural network author usually underline approach tially adopted explain any kind model mean agnostic model another way classify explanation method explored next model related method another important way classify model interpretability technique whether model agnostic meaning applied any type ml algorithm model speciﬁc meaning technique applicable only single type class algorithm 1 interpretability interpretability method limited ciﬁc model class intrinsic method deﬁnition drawback practice require particular type interpretation limited term choice model provide potentially expense using predictive representative model therefore ha recent surge est interpretability method 2 interpretability method not tied particular type ml model word class method separate prediction explanation interpretation usually generally used interpret ann could local global interpretable model est improving interpretability ai model large amount method developed recently using range technique statistic machine learning data 52148 volume 6 2018 adadi berrada peeking inside survey xai science since reviewed paper lie mostly class present herein overview studied work grouped technique broadly fall four technique type visualization ii knowledge extraction iii inﬂuence method iv explanation visualization natural idea understand ml model especially dnn visualize representation explore pattern den inside neural unit unsurprisingly consistent body research investigates way help diverse visualization technique order see inside black box visualization technique essentially applied supervised learning model amongst reviewed literature popular visualization technique surrogate el ii partial dependence plot pdp iii individual conditional expectation ice surrogate model surrogate model simple model used explain plex model speciﬁcally interpretable model like linear model decision tree trained prediction original model order interpret latter however almost no ical guarantee simple surrogate model highly representative complex model tioned lime 84 approach prescribed method building local surrogate model around single observation bastani et al 106 used surrogate model approach extract decision tree represents model behavior another remarkable work thiagarajan et al 107 posed approach building treeview visualization using surrogate model ii partial dependence plot pdp pdp graphical representation help visualizing average partial relationship one input ables prediction model work use pdp understand supervised learning model include green kern 108 used pdps understand relationship predictor conditional average treatment effect voter mobilization experiment prediction made bayesian additive regression tree chipman et al 109 ecological literature elith et al 110 rely stochastic gradient ing used pdps understand different environmental factor inﬂuence distribution particular freshwater berk bleich 51 demonstrated advantage using random forest associated pdps accurately model relationship asymmetric tion cost often arise criminal justice setting recently welling et al 111 proposed methodology called est floor visualize interpret random forest model proposed technique rely feature contribution method rather pdp argued author tages forest floor pdp interaction not masked averaging thus possible locate tions not visualized given projection iii individual conditional expectation ice ice plot extend pdp whereas pd plot provide coarse view model working ice plot reveal tions individual difference disaggregating pdp output recent work use ice rather cal pdp instance goldstein et al 112 introduced ice technique proved advantage pdp later casalicchio et al 113 proposed local feature importance based approach us partial importance pi individual conditional importance ici plot visual tool b knowledge extraction difﬁcult explain ml model work especially model based ann indeed cited multilayer feedforward network universal tor however since learning algorithm modify cell hidden layer may constitute interesting internal tations task extracting explanation network therefore extract comprehensible form edge acquired ann training encoded internal representation explored literature several work propose method extract knowledge embedded ann mainly rely two technique rule extraction ii model distillation rule extraction one effort gain insight highly complex model use rule extraction 114 116 work supporting technique propose approach provide symbolic comprehensible description knowledge learned network training extracting rule imate process ann utilizing input output ann way kind knowledge used traditional artiﬁcial intelligence expert system survey ra 74 taken cation rule extraction strategy proposed earlier 117 118 proposed three mode extract rule agogical rule extraction b decompositional rule extraction c eclectic decompositional approach focus extracting rule level individual unit within trained ann view underlying ann one transparency 93 95 pedagogical approach treat trained ann view underlying ann opaque orthogonal rule tion algorithm osre 119 successful ical methodology often applied biomedicine third type eclectic hybrid approach rule extraction incorporates element decompositionnel agogical technique 120 volume 6 2018 52149 adadi berrada peeking inside survey xai ii model distillation another technique fall knowledge extraction category model distillation distillation model pression transfer information dark knowledge deep network teacher shallow network dent 121 122 model compression wa originally posed reduce computational cost model runtime ha later applied interpretability tan et al 49 investigated model distillation used distill complex model ent model che et al 123 introduced paper approach called interpretable mimic learning learn interpretable phenotype feature ing robust prediction mimicking performance deep learning model recent work xu et al 124 presented darksight visualization method interpreting prediction classiﬁer data set way inspired notion dark knowledge posed method combine idea knowledge distillation dimension reduction visualization dnn interested researcher also consult 125 127 detail technique c influence method type technique estimate importance evance feature changing input internal ponents recording much change affect model performance inﬂuence technique often visualized reviewed literature three alternative method obtain input variable relevance sensitivity analysis ii relevance propagation iii feature importance sensitivity analysis sensitivity refers ann output inﬂuenced input weight perturbation 128 used verify whether model behavior output remain stable data intentionally perturbed change simulated data visualizing result sensitivity analysis sa considered agnostic explanation technique since ing model stability data change time enhance trust machine learning result sa ha increasingly used explaining ann general dnn classiﬁcation image particular 129 130 however important note sa doe not produce explanation function value rather variation purpose performing sa thus usually not actually explain relationship found instead sa generally used test model bility trustworthiness either tool ﬁnd remove unimportant input attribute starting point some powerful explanation technique decomposition ii relevance propagation lrp another technique compute relevance wa proposed 131 relevance propagation algorithm lrp redistributes prediction function backwards starting output layer network backpropagating input layer key property redistribution process referred relevance conservation contrast sa method explains prediction relative state maximum uncertainty identiﬁes property pivotal prediction rooster iii feature importance variable importance quantiﬁes contribution input variable feature prediction complex ml model increase model prediction error calculated permuting feature order measure feature tance permuting value important feature increase model error permuting value unimportant feature ignored model thus keep model error unchanged based technique fisher et al 132 proposed version feature importance called model class reliance mcr tioned work 113 proposed local version feature importance called sfimp shapley ture importance loco 86 use local feature importance well explanation explanation technique select particular instance dataset explain behavior machine learning model explanation mostly make any ml model pretable slight difference method explanation method interpret model selecting instance dataset not acting feature transforming model based conducted review identiﬁed two ing interpretability technique prototype criticism ii counterfactuals explanation prototype criticism prototype selection representative instance data 133 135 thus item membership determined similarity prototype lead overgeneralization avoid advantage exception shown also called criticism instance not well represented prototype kim 136 developed unsupervised rithm automatically ﬁnding prototype critic dataset called applied unlabeled data ﬁnds prototype critic characterize dataset whole ii counterfactuals explanation wachter et al 137 presented concept unconditional counterfactual explanation novel type explanation automated decision counterfactual explanation describe minimum condition would led alternative decision bank loan approved without need describe full logic algorithm focus 52150 volume 6 2018 adadi berrada peeking inside survey xai explaining single prediction contrast adversarial example emphasis reversing prediction not explaining 138 proposed classiﬁcation technique based study actual literature research butions class method actively growing new technique regularly proposed finally worth note main advantage method ﬂexibility model explanation representation level nevertheless although interpretability technique convenient often rely surrogate model approximation degrade accuracy explanation vide interpretation technique tend use model interpreted directly leading potentially accurate explanation figure pseudo ontology xai method taxonomy sum figure 7 key distinction drawn current thinking term explaining ml based ai system true transparency interpretable model sion tree rule linear model tions additional technique used lighten darkness complex model dnn either generating local explanation particular input ally explaining entire model local explanation focus data provide individual explanation provide trust model outcome global explanation focus model provide understanding decision process connotes some sense understanding mechanism model work thus term trustworthiness local explanation faithful global explanation unarguably popular class explainability od class type method usually used ann model model dent consequently technique rable possible compare behavior model different type agnostic model technique table 2 summarizes various explainability technique listed far together some good reference projection detailed method taxonomy form useful reference reader gain knowledge recent xai technique axis xai measurement evaluating explanation model model class equally interpretable kim 62 questioned interoperability measurement uation issue indeed despite growing body research produce interpretable ml method work evaluating method quantifying relevance only 5 studied paper focus issue probably due subjective nature explainability however given number existing interpretability od need comparing validating quantifying thus evaluating method arises kim established baseline evaluation approach proposed three major type interpretability evaluation put explanation application let end user typically domain expert test type evaluates quality explanation context ii ducting simpliﬁed evaluation experiment run lay human rather domain expert type appropriate goal test general notion quality explanation iii type doe not involve human appropriate class model regularizers already validated via grounded experiment based evaluation classiﬁcation mohseni ragan 148 presented evaluation benchmark evaluating instance explanation image textual data demonstrated comparing explanation result classiﬁcation model benchmark annotation possible ate quality appropriateness local explanation thus showed evaluation could used measure qualify local explanation earlier huysmans et al 149 investigated decision tree decision table propositional rule oblique rule order understand interpretable end performed experiment compare found overall decision tree decision table interpretable different task made tree table desirable backhaus seiffert 150 suggested quantitative sures compare ml method capability offer interpretation number machine learning method learned spectral data wa considered testing et al 151 argued quantifying interpretability implies deﬁning term alignment set concept proposed general framework called network dissection quantifying interpretability latent representation ann fying hidden unit semantics any given neural net aligning concept volume 6 2018 52151 adadi berrada peeking inside survey xai table summary explainability technique bau et al 152 different perception ity see latent property inﬂuenced different manipulable factor number input complexity model even user interface affect different measurable outcome end user ability trust debug model ran work related manipulation measurement model interpretability interesting experiment sists changing factor thought make model le interpretable measuring change affect people decision making focused two factor number input whether model ent ﬁnding experiment stipulates participant presented transparent minimum input model better able simulate model prediction however not ﬁnd signiﬁcant difference participant trust prediction error paul claim 153 based fact considerable number method proposed improving evaluating interpretability topic model discus idea topic modeling human feedback automated metric could applied evaluating ml interpretability recent work gilpin et al 154 proposed ological approach evaluating interpretability ml el according taxonomy distinguishes three type explainability emulate processing explain tation network common factor directly impact quality explainability approached different point study human next axis propose discus detail factor highlighting work focusing impact ai explainability axis xai perception human loop explain understand two different action ing depends mainly explained original model explanation made interpretability method understanding depends addition element receiving explanation explainee 52152 volume 6 2018 adadi berrada peeking inside survey xai word human explainable ml model ha represents challenge designing xai implies communicating complex computational process human requires addition ml expertise hci skill well furthermore since explanation human action ha long studied philosophy psychology thus ﬁelds consulted order simulate human tion process take inspiration developed model ﬁelds keeping human loop determinant factor overall explainability value however conducted literature review ha identiﬁed dearth work focusing human factor impact xai axis survey work discussed role human two perspective ﬁrst one focus produce explanation simulate human cognitive process ii second one focus produce explanation explanation work miller 33 perhaps signiﬁcant attempt articulating link human science xai paper miller 33 provided survey research philosophy psychology cognitive ence study explanation topic author noted latter could valuable resource progress ﬁeld xai highlighted three major ﬁndings explanation contrastive people not ask event e happened rather event e happened instead some event ii explanation selective focus one two possible cause not cause recommendation iii explanation social conversation interaction transfer knowledge implying explainer must able leverage mental model explainee engaging explanation process asserted imperative take account three point goal build useful xai call using social science model xai wa made 155 author paper argue ing literature xai method based developer intuition rather focused intended user based light literature survey demonstrate social science aspect rarely undertaken current xai research present some key result human science ﬁeld relevant xai going back interpretability taxonomy od worth note point pretability technique analogous human way explaining decision noted lipton 25 extent might consider human interpretable interpretability applies furthermore example based explanation agnostic method 136 explicitly inspired cognitive science human ing speciﬁcally human reasoning often using representative example basis categorization similarly kim method use tative example explain cluster data explanation early work bauer baldes 156 proposed interface allows user gain deeper insight knowledge represented ml el towards intelligible transparent ml model recently zhu et al 157 noted existing work focus new explaining method not usability cal interpretability efﬁcacy real user introduced derived research area called explainable ai er xaid proposed approach facilitating game designer niques xaid tamagnini et al 158 proposed rivelo pedagogical visual analytics interface enables expert user binary classiﬁers interactively exploring set explanation abdul et al 61 investigated hci research help develop practical explainable system cacy end user author performed sizable literature analysis set hci research agenda explainability also pointed relevant work attempt make explanation understandable interface textual form visual explanation amongst agnostic method visualization technique indeed method produce ter explanation see nately some technique belonging method produce visualization visually interesting not fully understandable human viewer recent work hohman et al 159 survey role visual analytics deep learning presented author edge importance producing visualization tations dnn human understandable expose work attempt produce visualization axis xai antithesis explain predict far presented work support xai ferent perspective beaten path presenting synthesized idea propose axis expose work challenge typical approach adjust intuitive belief conjecture previous ﬁndings regarding xai seeking holistic aim propose structured proposal respect triad thesis antithesis synthesis work explanation always important bunt et al 160 raised question importance consequently anticipated usage explanation technique within system support user making sion based study found generally opaque intelligent system positively perceived despite lack meaningful accessible explanation noted some user interested accessing mation dominant response application volume 6 2018 52153 adadi berrada peeking inside survey xai sufﬁciently transparent cost viewing explanation would outweigh beneﬁt xai thus not yet ready penetrate market kind intelligent system much little right work proposed kulesza et al 161 presented ﬁndings regarding explanation impact end user mental model interestingly suggest completeness tant soundness explanation increasing completeness via certain information type helped user mental model surprisingly perception tradeoff attending explanation also found trary explanation would u believe oversimpliﬁcation problem ness wa low user experienced mental demand lost trust explanation thereby reducing likelihood user pay attention explanation even though work holliday et al 162 entitled user trust intelligent system journey time work conﬁrmed based experimental study explanation impact trust work challenge typical approach considered trust intelligent system only captured single quantitative measure conclusion task research work ml interpretability agreed contribute towards rigorous notion ity 62 contrast wave thought offert 163 suggested work know see better understanding deﬁciencies intuitive notion interpretability needed well consider interpretability precisely term not order identify impaired intuitive consideration wang et al 164 proposed work trading pretability accuracy oblique treed sparse additive model sacriﬁces certain degree interpretability accuracy order achieve entirely sufﬁcient accuracy statistical standpoint shmueli 165 debated explain predict dilemma giving special emphasis machine learning ﬁeld finally taking machine learning model prediction important explanation yarkoni westfall 166 argued psychology increased focus prediction rather explanation ultimately lead u greater understanding behavior named work choosing prediction explanation ogy lesson machine learning iv discussion due broad spectrum xai approach almost impossible perform exhaustive survey xai work also inconceivable unthinkable include 381 studied paper work thus synthesis evance concern only subset work wa detailed survey mentioned selection criterion mostly based popularity impact proposal supplement axis made sure include fresh work order give interested researcher idea recent trend proposed review wa underpinned solid ground cover aspect related xai topic background section deliberately include venue signiﬁcant attention indeed due youth studied domain rapid growth turn source also important review highly inﬂuential impactful ﬁeld conclude survey compilation main ﬁndings well interesting fact previous study parallel discus some research direction open problem distilled surveyed work towards formalism xai multifaceted objective not addressed singular disciplinary effort however synergetic use method different research horizon must done soundly integrated way word ﬁeld progress supported standalone research community stage advancement mainly engaged towards formalism term systematic deﬁnitions depending background researcher use synonymously concept semantically different 25 refers notion different name 113 132 consensus deﬁnitions must done order enable easier transfer result information ii abstraction given number research proposal sufﬁcient material effort consolidation form generic explainable framework would guide production explainable approach instead isolated interpretability method though technical relevance remain only fragment whole solution larger technical operation ml algorithm come vein abstracted explanation generation another potential venue dosilovic et al 64 discussed work utility abstraction ﬁnding property erating hypothesis process important future artiﬁcial general intelligence agi system iii formalizing quantifying guidotti et al 63 randhar et al 167 puri et al 168 varshney et al 169 tend base proposal detailed problem formulation becomes invalid soon method ity explained model change progress ﬁeld imperative generalize expansibility lem formulation rigorous way irrespective changing factor variable direct effect advance state art explainability classifying qualifying evaluating indeed amount existing ity method literature ﬁrst area future work developing formalized rigorous evaluation metric 52154 volume 6 2018 adadi berrada peeking inside survey xai method otherwise risk forced explain explanation observed literature no clear way quantify explainability related line work infancy represents opportunity challenge time teaming not enough explain model user ha understand however even accurate explanation developing understanding could require tary answer question user would likely thus explainability only happen throw interaction human machine envisioning interactive explanation tems support many different action presenting initial explanation user potential research path pursue order advance xai ﬁeld partnership motivates naturally use hci human science discipline nevertheless cussed lack literature around able system take account two dimension two keen observation made respectively miller 33 abdul et al 61 attest lack social ences human behavioral study not enough impact explainable ai ii stream research explainable system hci community tend relatively isolated challenge link result hci empirical study human science ries order drive added value explainability approach hopefully contribute explainable model consequently adaptive explainable model would make appearance offering explanation would adapt according environment change user proﬁle level expertise domain knowledge cultural background interest preference contextual variable nation request setting justiﬁcation teaching audit machine human teaming expected spark icant research ai explanability era internet thing iot also waiting ing another research body focusing machine explanation conceiving explanation machine consumption drive some consideration worth ther research ultimately however likely future explainable approach especially adaptive one need provide kind explanation explainability method composition work explainability tends advance quantitatively pretability method reﬂected huge proliferation pretability technique way make deﬁning taxonomy interpretability method challenging task comparatively little attention given approach discussed potential combining different interpretability method achieve powerful explanation indeed literature seen some technique used complementary others sensitive analysis visualization not treat disparate interpretability method elementary composable building block could synergistically create new added value technique believe rich area future research hence enabling composability xai tially contribute effectively solve optimization issue ﬁeld making explainability accuracy move direction furthermore eventual combination could also concern actual interpretability method focusing ml model classical solution explainability related expert tems exactly combine element cal explainable expert system present interpretable ml method topic debate instance preece 57 argues element earlier work expert system offer route making progress towards xai today others explainable intelligent system existing work literature focus ability machine learning one type ai however issue also confront intelligent tems particularly explainable ai planning ii able agent beginning gain recognition promising derived ﬁeld xai planning important area ai used domain learning not option planner mostly concerned establishing correctness quality given plan respect model adding ity implies translate produced plan step pddl plan human understandable form thus intuitively explainable ai planning xaip mostly algorithm dent serve debugging system expert user explainability opportunity arise ai planning wa recently explored fox et al 170 described some initial analysis issue proposed roadmap achieving effective explainability based idea fact contrast ml ai planning potentially favorably disposed explainable planner eventually trusted ii planner allow easy tion human iii planner relatively transparent fact main issue enabling explainability planner mainly related gap planning algorithm human problem solving introductory work include 171 172 tion dozen paper wa discussed recently dedicated session icaps workshop 11 initial work open number future direction xaip begin need full formulation able planning problem open problem include sibility explainability case planning uncertainty explainability task motion planning robotics revision rich ai planning literature identify work could contribute xaip also serve useful starting point progress subﬁeld volume 6 2018 52155 adadi berrada peeking inside survey xai related thread work researcher looked idea explainable agent agent supposed represent human behavior work area mainly focus behavior explanation generation agent could explain reason behind action signiﬁcant earlier work proposed approach tual agent training system 173 174 recent work investigated possibility ability advanced autonomous agent like socially tive robot 175 178 economic perspective signiﬁcant beneﬁts gain front foot investing explainability today indeed due social ethical pressure xai could turn competitive differentiator drive real business value moreover technical challenge explainability involving tradeoff accuracy interpretability affect signiﬁcantly cost xai product curiously literature economical perspective xai area le research yet no le important encouraging economic interpretation essential address several issue explainability cost estimation variation algorithm propriety revealing trade secret predicting xai market evolution amongst work found literature akyol et al 179 proposed ﬁrst attempt quantitatively analyze cost transparency pot ml algorithm work igami 180 connection machine learning econometrics proposed perspective structural econometrics explainable ai noted relaxing implicit econometric assumption would make result economically interpretable discussed outlook no mean exhaustive give lead exploration different spectives based compilation existent study erature hope proposed direction inspire new research improve current state art xai conclusion xai vital interdisciplinary research ﬁeld ai ecosystem spirit holism presented paper comprehensive background regarding ﬁeld taking inspiration assimilate familiarize new topic focused five w cover aspect related xai moreover interest mapping broad landscape around xai research survey ha thoroughly reviewed portfolio explainability approach organized different perspective finding showed xai not labcoat research ﬁeld impact spanning large range application domain however seen evidence throughout work lack formalism term problem tion clear unambiguous deﬁnitions furthermore ha noted human role not sufﬁciently studied existing explainability approach essence attention devoted interpreting ml model letting promising ai system explainability ha concluded considerable effort required future tackle challenge open issue xai reference 1 international data corporation idc 2018 worldwide semiannual cognitive artiﬁcial intelligence system spending guide accessed jun 6 2018 online available 2 statista 2018 revenue artiﬁcial intelligence ai market worldwide 2016 accessed jun 6 2018 online able 3 gartner 2017 top 10 strategic technology trend accessed jun 6 2018 online available 4 barocas friedler hardt kroll wallach workshop series fairness accountability transparency machine learning accessed jun 6 2018 online available 5 kim varshney weller 2018 workshop human interpretability machine learning whi online available 6 wilson kim herlands 2016 proceeding nip 2016 workshop interpretable machine learning complex system online available 7 aha darrell pazzani reid sammut stone proc workshop explainable ai xai ijcai 2017 8 farina reed proc xci explainable comput intell workshop 2017 9 guyon et proc ijcnn explainability learn 2017 10 chander et proc ai 2018 11 biundo langley magazzeni smith proc icaps workshop explainable ai planning 2018 12 graaf malle dragan ziemke proc hri workshop explainable robot 2018 13 komatsu said proc acm intell interface iui workshop explainable smart syst ex 2018 14 alonso castiello mencar magdalena proc ipmu adv explainable artif 2018 15 agudo aha garcia proc iccbr workshop reasoning explanation intell syst xcbr 2018 16 gunning explainable artiﬁcial intelligence xai defense advanced research project agency darpa accessed jun 6 2018 online available intelligence 17 hall kurka bartz 2018 using driverless ai accessed jun 6 2018 online available 18 cognilytica 2018 cognilytica ai positioning matrix capm accessed jun 6 2018 online available 19 fico 2018 explainable machine learning challenge accessed jun 6 2018 online available 20 van lent fisher mancuso explainable artiﬁcial intelligence system tactical behavior proc conf innov appl artif 2004 pp 21 swartout moore explanation expert tems survey univ southern california los angeles ca usa tech 1988 22 doran schulz besold 2017 doe explainable ai really mean new conceptualization online available 23 koh liang 2017 understanding diction via inﬂuence online available 24 bojarski et al 2017 explaining deep neural network trained learning steer online available 52156 volume 6 2018 adadi berrada peeking inside survey xai 25 lipton mythos model interpretability proc icml workshop hum interpretability mach 2016 pp 26 andrzejak langner zabala interpretable model distributed data via merging decision tree proc cidm singapore apr 2013 pp 27 piltaver luštrek gam sibility classiﬁcation design validation proc itis šmarješke toplice slovenia 2014 pp 28 weld bansal 2018 challenge crafting intelligible online available 29 suman mall sukumaran satpathy extracting state model software component object vol 9 no 3 pp 2010 30 dignum responsible artiﬁcial intelligence designing ai human value itu ict discovery vol 1 pp 2017 31 danjuma 2015 performance evaluation machine learning algorithm life expectancy lung cancer online available 32 chen machine learning data science mathematical putational mathematical problem data science 2015 pp 33 miller 2017 explanation artiﬁcial intelligence insight social online available 34 prabhakar powerful limited darpa perspective ai proc darpa accessed jun 6 2018 online available 35 baum survey artiﬁcial general intelligence project ethic risk policy global catastrophic risk working paper 2017 online available 36 gill artiﬁcial super intelligence beyond rhetoric ai vol 31 no 2 pp 2016 37 caruana lou gehrke koch sturm elhadad intelligible model healthcare predicting pneumonia risk pital readmission proc acm sigkdd int conf knowl discovery data mining 2015 pp 38 howard zhang horvitz addressing bias machine learning algorithm pilot study emotion recognition gent system proc adv robot social impact arso mar 2017 pp 39 2016 european union general data protection regulation gdpr accessed jun 6 2018 online available 40 silver et mastering game go without human knowledge nature vol 550 no 7676 pp 2017 41 norvig google approach artiﬁcial intelligence machine learning unsw sydney nsw australia accessed jun 6 2018 online available 42 mcfarland 2018 uber shuts operation arizona cnn accessed jun 6 2018 online available 43 bojarski et al 2016 end end learning online available 44 haspiel et al 2018 explanation expectation trust ing automated vehicle online available 45 holzinger biemann pattichis kell 2017 need build explainable ai system medical domain online available 46 katuwal chen 2016 machine learning model pretability precision medicine online available 47 che purushotham khemani liu interpretable deep model icu outcome prediction proc amia annu 2017 pp 48 lightbourne damned lie criminal sentencing using based tool 15 duke law technol tech 2017 pp accessed jun 6 49 tan caruana hooker lou 2018 detecting bias model using transparent model online able 50 howell framework addressing fairness consequential machine learning proc fat 2018 pp 51 berk bleich statistical procedure forecasting criminal behavior comparative assessment criminol public policy vol 12 no 3 pp 2013 52 equifax 2018 equifax launch neurodecision technology accessed jun 6 2018 online available 53 knight 2017 military want autonomous machine explain mit technology review accessed jun 6 2018 online available 54 henelius puolamäki ukkonen 2017 interpreting siﬁers attribute interaction online available 55 future privacy forum 2017 unfairness algorithm distilling harm automated accessed jun 6 2018 online available 56 ka need user model generating expert system explanation penn library tech 1988 57 preece asking ai explainability intelligent perspective challenge intell syst accounting finance vol 25 no 1 pp 2018 58 tan sim gale improving interpretability deep neural network stimulated learning proc ieee workshop autom speech recognit understand asru 2015 pp 59 hall gill introduction machine learning interpretability newton usa reilly medium 2018 60 breiman 2001 statistical modeling two culture statistical science accessed jun 6 2018 online available 61 abdul vermeulen wang lim kankanhalli trend trajectory explainable accountable intelligible system hci research agenda proc sigchi conf hum factor comput syst chi 2018 582 62 kim 2018 towards rigorous science interpretable machine online available 63 guidotti monreale turini pedreschi giannotti 2018 survey method explaining black box online available 64 došilovič brčić hlupić explainable artiﬁcial ligence survey proc int conv inf commun electron microelectron mipro may 2018 pp 65 wohlin guideline snowballing systematic literature study replication software engineering proc int conf eval assessment softw eng ease 2014 art no 38 66 letham rudin mccormick madigan interpretable classiﬁers using rule bayesian analysis building better stroke prediction model ann appl vol 9 no 3 pp 2015 67 xu et show attend tell neural image caption generation visual attention proc int conf mach learn icml 2015 pp 68 ustun rudin supersparse linear integer model optimized medical scoring system mach vol 102 no 3 pp 2015 69 sarkar accuracy interpretability machine learning applied safer gambling proc ceur workshop 2016 pp 70 breiman statistical modeling two culture comment rejoinder author stat vol 16 no 3 pp 2001 71 krening harrison feigh isbell riedl thomaz learning explanation using sentiment advice rl ieee trans cogn develop vol 9 no 1 pp mar 2016 72 mahendran vedaldi understanding deep image tations inverting proc ieee conf comput vi pattern recognit cvpr jun 2015 pp 73 mikolov sutskever chen corrado dean tributed representation word phrase compositionality proc adv neural inf process syst nip 2013 pp 74 ra van gerven haselager 2018 explanation method deep learning user value concern online available volume 6 2018 52157 adadi berrada peeking inside survey xai 75 santoro et al 2017 simple neural network module relational online available 76 palm paquet winther 2017 recurrent tional network complex relational online available 77 dong su zhu zhang improving interpretability deep neural network semantic information proc ieee conf comput vi pattern recognit cvpr mar 2017 pp 78 louizos shalit mooij sontag zemel welling causal effect inference deep model proc adv neural inf process syst nip 2017 pp 79 goudet et al 2017 learning functional causal model generative neural online available 80 yang rangarajan ranka 2018 global model pretation via recursive online available 81 nagesh surdeanu 2018 representation learning global online available 82 nguyen dosovitskiy yosinski brox clune thesizing preferred input neuron neural network via deep generator network proc adv neural inf process syst nip 2016 pp 83 erhan courville bengio understanding representation learned deep architecture dept informatique recherche tionnelle univ montreal montreal qc canada tech 1355 2010 84 ribeiro singh guestrin trust explaining prediction any classiﬁer proc acm sigkdd int conf knowl discovery data mining 2016 pp 85 ribeiro singh guestrin anchor explanation proc aaai conf artif 2018 pp 86 lei g sell rinaldo tibshirani wasserman predictive inference regression amer stat published online available 87 baehrens schroeter harmeling kawanabe hansen müller explain individual classiﬁcation decision mach learn vol 11 no 6 pp 2010 88 simonyan vedaldi zisserman 2013 deep inside volutional network visualising image classiﬁcation model saliency online available 89 zeiler fergus visualizing understanding tional network proc eur conf comput vi zurich switzerland springer 2014 pp 90 zhou khosla lapedriza oliva torralba learning deep feature discriminative localization proc ieee conf put vi pattern jun 2016 pp 91 sundararajan taly yan 2017 axiomatic attribution deep online available 92 smilkov thorat kim viégas wattenberg 2017 smoothgrad removing noise adding online available 93 kononenko explaining classiﬁcations individual instance ieee trans knowl data vol 20 no 5 pp may 2008 94 montavon lapuschkin binder samek müller explaining nonlinear classiﬁcation decision deep taylor position pattern vol 65 pp may 2017 95 bach binder müller samek controlling tory heatmap resolution semantics via decomposition depth proc ieee int conf image process icip 2016 pp 96 fong vedaldi 2017 interpretable explanation black box meaningful online available 97 dabkowski gal real time image saliency black box classiﬁers proc adv neural inf process 2017 pp 98 kindermans et learning explain neural network patternnet patternattribution proc int conf learn 2018 pp accessed jun 6 2018 online available 99 shrikumar greenside shcherbina kundaje 2016 not black box interpretable deep learning propagating activation online available 100 ross hughes right right reason training differentiable model constraining explanation proc int joint conf artif 2017 pp 101 lundberg lee uniﬁed approach interpreting model prediction proc adv neural inf process 2017 pp 102 guidotti monreale ruggieri pedreschi turini giannotti 2018 local explanation black box sion online available 103 linsley scheibler eberhardt serre 2018 attention network visual online available 104 seo huang yang liu interpretable convolutional neural network dual local global attention review rating prediction proc acm conf recommender syst recsys 2017 pp 105 molnar 2018 interpretable machine learning guide making black box model explainable accessed jun 6 2018 online able 106 bastani kim bastani 2017 interpretability via model online available 107 thiagarajan kailkhura sattigeri ramamurthy 2016 treeview peeking deep neural network via online available 108 green kern modeling heterogeneous treatment effect experiment using bayesian additive regression tree proc annu summer meeting soc political 2010 pp 109 chipman george mcculloch bart bayesian additive regression tree appl vol 4 no 1 pp 2010 110 elith leathwick hastie working guide boosted regression tree animal vol 77 no 4 pp 2008 111 welling refsgaard brockhoff clemmensen 2016 forest ﬂoor visualization random online available 112 goldstein kapelner bleich pitkin peeking inside black box visualizing statistical learning plot individual tional expectation comput graph vol 24 no 1 pp 2015 doi 113 casalicchio molnar bischl 2018 visualizing feature importance black box online available 114 johansson könig niklasson truth extraction opaque model using genetic programming proc flair 2004 pp 115 aung et comparing analytical decision support model boolean rule extraction case study ovarian tumour nancy proc int symp neural netw berlin germany springer 2007 pp 116 hailesilassie 2017 rule extraction algorithm deep neural work online available 117 andrew diederich tickle survey critique technique extracting rule trained artiﬁcial neural network vol 8 no 6 pp 1995 118 gopikrishna evaluation rule extraction algorithm int data mining knowl manage process vol 4 no 3 pp 2014 119 etchells lisboa orthogonal rule extraction osre trained neural network practical efﬁcient approach ieee trans neural vol 17 no 2 pp mar 2006 120 barakat diederich eclectic support vector machine int comput vol 2 no 1 pp 2005 121 sadowski collado whiteson baldi deep learning dark knowledge dark matter proc nip workshop phys mach learn pmlr vol 42 2015 pp 122 hinton vinyals dean 2015 distilling knowledge neural online available 123 che purushotham khemani liu 2015 distilling knowledge deep network application healthcare online available 52158 volume 6 2018 adadi berrada peeking inside survey xai 124 xu park yi sutton 2018 interpreting deep classiﬁer visual distillation dark online available 125 tan interpretable approach detect bias model proc conf ai ethic 2017 pp 126 tan caruana hooker lou 2018 auditing box model using transparent model distillation side online available 127 tan caruana hooker gordo 2018 transparent model online available 128 zhang wallace 2016 sensitivity analysis tioners guide convolutional neural network sentence online available 129 cortez embrechts using sensitivity analysis tion technique open black box data mining model inf vol 225 pp mar 2013 130 cortez embrechts opening black box data mining model using sensitivity analysis proc ieee symp comput intell data mining cidm apr 2011 pp 131 bach binder montavon klauschen müller samek explanation classiﬁer sion relevance propagation plo one vol 10 no 7 2015 132 fisher rudin dominici 2018 model class reliance variable importance measure any machine learning model class rashomon online available 133 bien tibshirani prototype selection interpretable cation ann appl vol 5 no 4 pp 2011 134 kim rudin shah bayesian case model ative approach reasoning prototype classiﬁcation proc adv neural inf process 2014 pp 135 gurumoorthy dhurandhar cecchi 2017 todash fast interpretable prototype online available 136 kim khanna koyejo example not enough learn criticize criticism interpretability proc conf neural inf process syst nip 2016 pp 137 wachter mittelstadt russell 2017 counterfactual nation without opening black box automated decision online available 138 yuan zhu li 2017 adversarial ples attack defense deep online available 139 schetinin et conﬁdent interpretation bayesian decision tree ensemble clinical application ieee trans inf technol vol 11 no 3 pp may 2007 140 hara hayashi 2016 making tree ensemble online available 141 tan hooker well 2016 tree space prototype another look making tree ensemble online able 142 gibbon et computerized adaptive nostic screening tool depression clin psychiatry vol 74 no 7 pp 2013 143 garcía fernández herrera enhancing effectiveness interpretability decision tree rule induction classiﬁers evolutionary training set selection imbalanced problem appl soft vol 9 no 4 pp 2009 144 wang rudin falling rule list proc int conf artif intell statist aistats san diego ca usa jmlr w cp 2015 pp 145 su wei varshney malioutov 2015 pretable boolean rule learning online available 146 malioutov varshney emad dash learning pretable classiﬁcation rule boolean compressed sensing parent data mining big small data springer 2017 pp 147 mishra sturm dixon local interpretable agnostic explanation music content analysis proc ismir 2017 pp 148 mohseni ragan 2018 evaluation benchmark local explanation machine online able 149 huysmans dejaeger mues vanthienen baesens empirical evaluation comprehensibility decision table tree rule based predictive model decis support vol 51 no 1 pp 2011 150 backhaus seiffert quantitative measurement model interpretability analysis spectral data proc ieee symp comput intell data mining cidm 2013 pp 151 goldstein hofman vaughan wallach 2018 manipulating measuring model online available 152 bau zhou khosla oliva torralba 2017 network dissection quantifying interpretability deep visual online available 153 paul interpretable machine learning lesson topic eling proc chi workshop mach 2016 pp 154 gilpin bau yuan bajwa specter kagal 2018 explaining explanation approach uating interpretability machine online available 155 miller howe sonenberg explainable ai beware inmate running asylum proc ijcai workshop explainable ai xai 2017 pp 156 bauer baldes interface machine learning proc int conf intell interface 2005 pp 157 zhu liapis risi bidarra youngblood able ai designer perspective proc ieee conf comput intell game cig 2018 pp 158 tamagnini krause dasgupta bertini interpreting box classiﬁers using visual explanation proc workshop data 2017 art no 6 159 hohman kahng pienta chau 2018 visual analytics deep learning interrogative survey next online available 160 bunt lount lauzon explanation always important study deployed intelligent interactive system proc acm int conf intell interface 2012 pp 161 kulesza stumpf burnett yang kwan wong much little right way explanation impact end user mental model proc ieee symp vi lang comput 2013 pp 162 holliday wilson stumpf user trust intelligent system journey time proc int conf intell user interface 2016 pp 163 offert 2017 know see visualization intuitive online available 164 wang fujimaki motohashi trading ity accuracy oblique treed sparse additive model proc acm sigkdd int conf knowl discovery data mining 2015 pp 165 shmueli explain predict stat vol 25 no 3 pp 2010 166 yarkoni westfall choosing prediction explanation psychology lesson machine learning perspect psychol vol 12 no 6 pp 2017 167 dhurandhar iyengar lu shanmugam 2017 tip typifying interpretability online available 168 puri gupta agarwal verma krishnamurthy 2017 magix model agnostic globally interpretable online available 169 varshney khanduri sharma zhang varshney 2018 interpretability machine learning answer using distributed detection data fusion online available 170 fox long magazzeni explainable planning proc ijcai workshop xai 2017 pp 171 chakraborti sreedharan zhang kambhampati plan explanation model reconciliation moving beyond explanation soliloquy proc ijcai 2017 pp 172 zhang sreedharan kulkarni chakraborti zhuo kambhampati plan explicability predictability robot task planning proc icra may 2017 pp volume 6 2018 52159 adadi berrada peeking inside survey xai 173 harbers van den bosch meyer design evaluation explainable bdi agent proc int conf web intell agent 2010 pp 174 harbers van den bosch meyer agent virtual training presented prolearn doctoral consortium 2009 175 langley meadow sridharan choi explainable agency intelligent autonomous system proc aaai 2017 pp 176 kaptein broekens hindriks neerincx role emotion cognitive agent proc int conf affect comput intell interact workshop demo aciiw 2017 pp 177 neerincx van der waa kaptein van diggelen using perceptual cognitive explanation enhanced team performance proc int conf eng psychol cogn ergonom epce 2018 pp 178 garcia robb liu laskov patron hastie explain natural language interface scrutable autonomous robot proc explainable robot syst workshop hri 2018 179 akyol langbort basar 2016 price transparency strategic machine online available 180 igami 2017 artiﬁcial intelligence structural estimation nomic interpretation deep blue bonanza online available amina adadi received degree puter engineering national school applied science fez 2012 degree computer science sidi mohammed ben abdellah university fez morocco current research interest include artiﬁcial intelligence machine learning semantic web service mohammed berrada received degree computer science faculty science sidi mohammed ben abdellah sity fez morocco currently professor computer science manager department national school applied science fez current research interest include artiﬁcial intelligence prise architecture web service 52160 volume 6 2018