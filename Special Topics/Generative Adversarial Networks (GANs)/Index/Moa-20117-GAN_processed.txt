least square generative adversarial network xudong qing haoran raymond zhen stephen paul computer science city university hong kong mathematics information technology education university hong kong information system city university hong kong optical imagery analysis learning northwestern polytechnical university xudonmao itqli raylau steve abstract unsupervised learning generative adversarial work gans ha proven hugely successful regular gans hypothesize discriminator classiﬁer moid cross entropy loss function however found loss function may lead vanishing gradient lem learning process overcome lem propose paper least square tive adversarial network lsgans adopt least square loss function discriminator show minimizing objective function lsgan yield mizing pearson divergence two beneﬁts lsgans regular gans first lsgans able generate higher quality image regular gans second lsgans perform stable learning process evaluate lsgans lsun datasets experimental result show image generated lsgans better quality one generated regular gans also conduct two comparison ments lsgans regular gans illustrate stability lsgans introduction deep learning ha launched profound reformation even applied many task image classiﬁcation 7 object detection 27 segmentation 18 task obviously fall scope supervised learning mean lot labeled data vided learning process compared supervised learning however unsupervised learning task generative model obtain limited impact deep ing although some deep generative model rbm 8 dbm 28 vae 14 proposed model face difﬁculty intractable function ﬁculty intractable inference turn restricts effectiveness model recently generative adversarial network gans 6 demonstrated impressive performance vised learning task unlike deep generative model usually adopt approximation method intractable function inference gans not require any mation trained tiable network basic idea gans ously train discriminator generator tor aim distinguish real sample generated sample generator try generate fake sample real possible making discriminator believe fake sample real data far plenty work shown gans play signiﬁcant role iou task image generation 21 image resolution 16 learning 29 spite great progress gans image ation quality generated image gans still ited some realistic task regular gans adopt moid cross entropy loss function discriminator 6 argue loss function however lead problem vanishing gradient updating tor using fake sample correct side decision boundary still far real data figure 1 b show use fake sample genta update generator making discriminator believe real data cause almost no ror correct side real data side decision boundary however sample still far real data want pull close real data based observation propose least square generative adversarial network lsgans adopt least square loss function 2794 0 2 4 6 8 10 0 2 4 6 8 10 fake sample real sample sigmoid decision boundary least square decision boundary 0 2 4 6 8 10 0 2 4 6 8 10 fake sample real sample fake sample updating g sigmoid decision boundary 0 2 4 6 8 10 0 2 4 6 8 10 fake sample real sample fake sample updating g least square decision boundary b c figure illustration different behavior two loss function decision boundary two loss function note decision boundary go across real data distribution successful gans learning otherwise learning process saturated b decision boundary sigmoid cross entropy loss function orange area side real sample blue area side fake sample get small error fake sample magenta updating g correct side decision boundary c decision boundary least square loss function penalizes fake sample magenta result force generator generate sample toward decision boundary inator idea simple yet powerful least square loss function able move fake sample toward decision boundary least square loss function penalizes sample lie long way correct side decision boundary figure 1 c show least square loss function penalize fake sample genta pull toward decision boundary even though correctly classiﬁed based erty lsgans able generate sample closer real data another beneﬁt lsgans improved stability learning process generally speaking training gans difﬁcult issue practice instability gans learning 25 recently several paper pointed instability gans learning partially caused objective function 2 19 24 speciﬁcally minimizing objective function regular gan suffers vanishing gradient make hard update generator gans relieve problem lsgans penalize sample based distance decision boundary generates gradient update generator cently arjovsky et al 2 proposed method uate stability gans learning excluding batch malization 11 following method evaluating stability ﬁnd lsgans also able converge relatively good state without batch normalization contribution paper summarized follows propose lsgans adopt least square loss function discriminator show ing objective function lsgan yield ing pearson divergence evaluate lsgans lsun datasets experimental result demonstrate lsgans generate realistic image ular gans two comparison experiment ating training stability also conducted prove stability lsgans apply conditional lsgans chinese acter generation evaluate handwritten nese character dataset 3740 class proposed model able generate readable chinese character rest paper organized follows section 2 brieﬂy review related work generative adversarial work proposed method introduced section 3 experimental result presented section finally conclude paper section 5 related work generative adversarial network gans posed goodfellow et al 6 explained ory gans learning based game theoretic scenario showing powerful capability unsupervised task gans applied many speciﬁc task like age generation 4 image 16 text age synthesis 26 image image translation 12 combining traditional content loss adversarial loss generative adversarial network 16 achieve performance task image reed et al 26 proposed model thesize image given text description based ditional gans 20 isola et al 12 also used tional gans transfer image one representation another addition unsupervised learning task gans also show potential learning task salimans et al 29 proposed framework learning discriminator not only output probability input image real 2795 data also output probability belonging class despite great success gans achieved proving quality generated image still challenge lot work proposed improve ity image gans radford et al 25 ﬁrst duced convolutional layer gans architecture posed network architecture called deep convolutional erative adversarial network dcgans denton et al 5 proposed another framework called laplacian pyramid generative adversarial network lapgans structed laplacian pyramid generate image starting image man et al 29 proposed technique called feature ing get better convergence idea make erated sample match statistic real data imizing mean square error intermediate layer discriminator another critical issue gans stability ing process many work proposed address problem analyzing objective function gans 2 3 19 23 24 viewing discriminator energy function 33 used architecture prove stability gans learning make tor discriminator balanced metz et al 19 created unrolled objective function enhance erator che et al 3 incorporated reconstruction ule use distance real sample structed sample regularizer get stable ents nowozin et al 23 pointed objective original gan 6 related divergence special case divergence estimation generalized arbitrary 22 arjovsky et al 2 extended analyzing property four ferent divergence distance two distribution concluded wasserstein distance nicer shannon divergence qi 24 proposed gan whose loss function based assumption real sample smaller loss fake sample proved loss function ha ent almost everywhere method section ﬁrst review formulation gans brieﬂy next present lsgans along eﬁts section finally two model architecture gans introduced generative adversarial network learning process gans train inator generator g simultaneously target g learn distribution pg data g start sampling input variable z uniform gaussian tribution pz z map input variable z data space g z θg differentiable network hand classiﬁer x θd aim nize whether image training data minimax objective gans formulated follows min g max vgan g x log x z log 1 g z 1 least square generative adversarial work viewing discriminator classiﬁer regular gans adopt sigmoid cross entropy loss function stated section 1 updating generator loss function cause problem vanishing gradient ples correct side decision boundary still far real data remedy problem propose least square generative adversarial work lsgans suppose use coding scheme discriminator b label fake data real data respectively objective tions lsgans deﬁned follows min vlsgan x x z g z min g vlsgan g z g z 2 c denotes value g want believe fake data beneﬁts lsgans beneﬁts lsgans derived two aspect first unlike regular gans cause almost no loss sample lie long way correct side cision boundary figure 1 b lsgans penalize sample even though correctly classiﬁed figure 1 c update generator parameter discriminator ﬁxed decision boundary ﬁxed result penalization make generator erate sample toward decision boundary hand decision boundary go across manifold real data successful gans learning otherwise learning process saturated thus moving ated sample toward decision boundary lead making closer manifold real data second penalizing sample lying long way decision boundary generate gradient dating generator turn relief problem 2796 b figure 2 sigmoid cross entropy loss function b least square loss function vanishing gradient allows lsgans perform stable learning process beneﬁt also derived another perspective shown figure 2 least square loss function ﬂat only one point sigmoid cross entropy loss function saturate x relatively large relation pearson divergence original gan paper 6 author ha shown minimizing equation 1 yield minimizing shannon divergence c g kl pdata pdata pg 2 kl pg pdata pg 2 4 3 also explore relation lsgans consider following extension equation 2 min vlsgan x x z g z min g vlsgan g x x z g z 4 note adding term x x 2 vlsgan g doe not change optimal value since term doe not contain parameter ﬁrst derive optimal discriminator ﬁxed g x bpdata x apg x pdata x pg x 5 proof equation 5 found appendix following equation use pd denote pdata simplicity reformulate vlsgan g equation 4 follows deconv 256 bn deconv 256 bn fc bn z 1024 deconv 256 bn deconv 256 bn deconv 128 bn deconv 64 bn deconv 3 fc 1 conv 512 bn conv 256 bn least square loss conv 128 bn conv 64 b figure model architecture k k c stride denotes layer nel c output ﬁlters stride layer bn mean layer followed batch normalization layer fc n note layer n output node activation layer omitted generator b discriminator g x g z x x x apg x pd x pg x x apg x pd x pg x z x pd x b pd x pg x pd x pg x z x pg x b pd x pg x pd x pg x z x b pd x pg x pd x pg x dx z x b pd x pg x b pg x pd x pg x dx 6 set b 1 b 2 g z x x pd x pg x pd x pg x dx pearson pd 7 pearson pearson divergence thus ing equation 4 yield minimizing pearson divergence pd pg b c satisfy condtions b 1 b 2 parameter selection one method determine value b c tion 2 satisfy condition 1 2 minimizing equation 2 yield minimizing 2797 generated image 112 112 lsgans b generated image 112 112 dcgans b generated image 64 64 dcgans reported 25 figure generated image pearson divergence pd pg ample setting b 1 c 0 get following objective function min vlsgan x x z g z 1 min g vlsgan g z g z 8 another method make g generate sample real possible setting c example using binary coding scheme get following objective function min vlsgan x x z g z min g vlsgan g z g z 9 practice observe equation 8 equation 9 show similar performance thus either one selected following section use equation 9 train model model architecture ﬁrst model designed shown figure 3 motivated vgg model 30 pared architecture 25 two lutional layer added top two deconvolutional layer architecture discriminator identical one 25 except usage least square loss function following dcgans relu activation leakyrelu activation used generator discriminator respectively second model designed task lot class example chinese character chinese character ﬁnd training gans multiple class not able generate readable character reason multiple class input only one class output stated 9 ministic relationship input output one way solve problem use conditional gans 20 cause conditioning label information creates terministic relationship input output however directly conditioning encoding label vector thousand class infeasible term memory cost computational time cost use linear mapping layer reduce dimensionality label vector 2798 church outdoor b dining room c kitchen conference room figure generated image different scene datasets generator label vector concatenated noise input layer discriminator label vector catenated convolutional layer layer layer concatenated determined ically experiment section ﬁrst present detail datasets implementation next present result tive evaluation quantitative evaluation lsgans compare stability lsgans lar gans two comparison experiment finally uate lsgans handwritten chinese character dataset contains 3740 class datasets implementation detail evaluate lsgans three datasets lsun 32 15 17 implementation proposed model based public tion using tensorflow 1 lsun learning rate set learning rate set following dcgans adam optimizer set implementation available qualitative evaluation train lsgans dcgans network architecture figure 3 resolution 112 112 dataset generated image two method presented figure compared age generated dcgans texture detail texture bed image generated lsgans exquisite image generated lsgans look sharper also train lsgans four scene datasets cluding church dining room kitchen conference room result lsgans trained four scene datasets shown figure 5 1 2799 lsgans without bn g using adam b regular gans without bn g using adam c lsgans without bn g using rmsprop regular gans without bn g using rmsprop figure comparison experiment excluding batch normalization bn quantitative evaluation inception score train lsgans dcgans network chitecture use model randomly generate 50 000 image calculating inception score 29 evaluated inception score lsgans gans shown table observe tion score vary different trained model reported inception score table 1 averaged 10 different trained model lsgans dcgans quantitative evaluation inception score lsgans show comparable performance dcgans method inception score dcgan reported 10 dcgan lsgan table inception score human subjective study evaluate performance lsgans duct human subjective study using generated bedroom image 112 112 lsgans dcgans network architecture randomly construct image pair one image lsgans one dcgans ask amazon mechanical turk notators judge image look realistic vote totally dcgans get vote lsgans get vote lsgans get vote gans stability comparison stated section one beneﬁt lsgans improved stability present two comparison periments compare stability lsgans regular gans one follow comparison method 2 based network architecture presented 25 two tures designed compare stability ﬁrst one exclude batch normalization generator bng short second one exclude batch malization generator discriminator bngd short pointed 2 selection optimizer critical model performance thus evaluate two architecture two optimizers adam 13 sprop 31 summary four training setting 1 bng adam 2 bng rmsprop 3 bngd adam 4 bngd rmsprop train model dataset using regular gans lsgans separately following four major observation first bng adam chance lsgans generate tively good quality image test 10 time 5 succeeds generate relatively good quality image regular gans never observe successful learning ular gans suffer severe degree mode collapse generated image lsgans regular gans shown figure second bngd rmsprop figure 6 show lsgans generate higher quality image regular gans slight degree mode lapse third lsgans regular gans similar formances bng rmsprop bngd adam speciﬁcally bng rmsprop lsgans regular gans able generate relatively good image bngd adam slight degree mode collapse last rmsprop performs stable adam since regular gans learn generate relatively good image bng rmsprop fail learn adam another experiment evaluate gaussian mixture distribution dataset designed literature 19 train lsgans regular gans mixture 2800 step 0 step step step step target regular gans lsgans figure dynamic result gaussian kernel estimation lsgans regular gans ﬁnal column show real data distribution real generated real generated figure generated image handwritten chinese character lsgans row 1 row 2 image column belong class character row 3 row 4 also condition generated character readable 8 gaussian dataset using simple network architecture generator discriminator contain three layer figure 7 show dynamic result gaussian kernel density estimation see ular gans suffer mode collapse starting step generate sample around single valid mode data distribution lsgans learn gaussian mixture distribution successfully suggestion practice learning process lsgans task difﬁcult train observe lsgans learn erate good quality image successfully ﬁrst several training epoch sometimes suffer mode collapse last although lsgans may suffer mode collapse last still select good model middle training process also observe quality ated image lsgans may shift good bad training process based two vations suggest keep record generated image every thousand hundred iteration select model manually checking image quality handwritten chinese character also train conditional lsgan model described section handwritten chinese character dataset contains 3740 class lsgans learn generate readable chinese character successfully some domly selected character shown figure two major observation figure first generated character lsgans readable second get correct label generated image label vector used application data mentation conclusion future work paper proposed least square erative adversarial network lsgans tal result show lsgans generate higher quality age regular gans two comparison experiment evaluating stability also conducted result demonstrate lsgans perform stable lar gans furthermore propose conditional lsgan model chinese character generation ated handwritten chinese character dataset 3740 class based present ﬁndings hope extend lsgans complex datasets imagenet future instead pulling generated sample toward decision boundary designing method pull ated sample toward real data directly also worth investigation acknowledgment work supported research grant project number 9360153 special grant account number 9610367 city university hong kong 2801 reference 1 abadi agarwal barham et al flow machine learning heterogeneous tems 2015 2 arjovsky chintala bottou wasserstein gan 2017 3 che li jacob bengio li mode larized generative adversarial network 2016 4 chen duan houthooft schulman sutskever abbeel infogan interpretable representation learning information maximizing generative adversarial net advance neural information processing system nip page 2016 5 denton chintala szlam fergus deep erative image model using laplacian pyramid ial network advance neural information processing system nip page 2015 6 goodfellow mirza xu ozair courville bengio erative adversarial net advance neural information processing system nip page 2014 7 zhang ren sun deep residual ing image recognition computer vision pattern recognition cvpr 2016 8 hinton salakhutdinov reducing ity data neural network science 313 5786 507 2006 9 hornik stinchcombe white multilayer forward network universal approximators neural work 2 5 july 1989 10 huang li poursaeed hopcroft belongie stacked generative adversarial network 2016 11 ioffe szegedy generative adversarial synthesis proceeding international ence machine learning icml 2015 12 isola zhu zhou efros translation conditional adversarial network 2016 13 kingma ba adam method stochastic optimization 2014 14 kingma welling variational bayes international conference learning tions iclr 2014 15 krizhevsky learning multiple layer feature tiny image tech report 2009 16 ledig theis huszar caballero ham acosta aitken tejani totz wang shi single image ing generative adversarial network 2016 17 liu yin wang wang icdar 2011 chinese handwriting recognition competition ceedings 2011 international conference document analysis recognition icdar page 2011 18 long shelhamer darrell fully convolutional network semantic segmentation computer vision pattern recognition cvpr 2015 19 metz poole pfau unrolled generative adversarial network 2016 20 mirza osindero conditional generative sarial net 2014 21 nguyen yosinski bengio dosovitskiy clune plug play generative network ditional iterative generation image latent space 2016 22 nguyen wainwright jordan ing divergence functionals likelihood ratio vex risk minimization ieee transaction information theory 56 11 2010 23 nowozin cseke tomioka training generative neural sampler using variational divergence imization 2016 24 qi generative adversarial network lipschitz density 2017 25 radford metz chintala unsupervised resentation learning deep convolutional generative versarial network international conference learning representation iclr 2015 26 reed akata yan logeswaran schiele lee generative adversarial synthesis proceeding international conference chine learning icml 2016 27 ren girshick sun faster ward object detection region proposal work advance neural information processing tems 28 page 2015 28 salakhutdinov hinton deep boltzmann machine proceeding international conference artiﬁcial intelligence statistic volume 5 page 2009 29 salimans goodfellow zaremba cheung ford chen chen improved technique ing gans advance neural information processing tems nip page 2016 30 simonyan zisserman deep convolutional network image recognition international conference learning representation iclr 2015 31 tieleman hinton lecture divide gradient running average recent magnitude coursera neural network machine learning 2012 32 yu seff zhang song funkhouser xiao lsun construction image dataset using deep learning human loop 2015 33 zhao mathieu lecun tive adversarial network 2016 2802