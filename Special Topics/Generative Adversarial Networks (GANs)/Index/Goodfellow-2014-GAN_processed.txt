generative adversarial net ian jean mehdi mirza bing xu david sherjil aaron courville yoshua epartement informatique et de recherche erationnelle e de eal eal qc abstract propose new framework estimating generative model via ial process simultaneously train two model generative model g capture data distribution discriminative model estimate probability sample came training data rather ing procedure g maximize probability making mistake framework corresponds minimax game space arbitrary function g unique solution exists g recovering training data distribution equal 1 2 everywhere case g deﬁned multilayer perceptrons entire system trained backpropagation no need any markov chain unrolled approximate inference work either training generation sample experiment demonstrate potential framework qualitative quantitative evaluation generated sample 1 introduction promise deep learning discover rich hierarchical model 2 represent probability distribution kind data encountered artiﬁcial intelligence application natural image audio waveform containing speech symbol natural language corpus far striking success deep learning involved discriminative model usually map rich sensory input class label 14 20 striking success primarily based backpropagation dropout algorithm using piecewise linear unit 17 8 9 particularly gradient deep generative model le impact due difﬁculty approximating many intractable probabilistic computation arise maximum likelihood estimation related strategy due difﬁculty leveraging beneﬁts piecewise linear unit generative context propose new generative model estimation procedure sidestep difﬁculties 1 proposed adversarial net framework generative model pitted adversary discriminative model learns determine whether sample model distribution data distribution generative model thought analogous team counterfeiter trying produce fake currency use without detection discriminative model analogous police trying detect counterfeit currency competition game drive team improve method counterfeit indistiguishable genuine article goodfellow research scientist google work earlier udem student work visiting e de eal ecole polytechnique ozair visiting e de eal indian institute technology delhi bengio cifar senior fellow code hyperparameters available 1 framework yield speciﬁc training algorithm many kind model optimization algorithm article explore special case generative model generates sample passing random noise multilayer perceptron discriminative model also multilayer perceptron refer special case adversarial net case train model using only highly successful backpropagation dropout algorithm 16 sample generative model using only forward propagation no approximate inference markov chain necessary 2 related work recently work deep generative model focused model provided parametric speciﬁcation probability distribution function model trained ing log likelihood family model perhaps succesful deep boltzmann machine 25 model generally intractable likelihood function therefore require numerous approximation likelihood gradient difﬁculties motivated development generative machine not explicitly represent likelihood yet able erate sample desired distribution generative stochastic network 4 example generative machine trained exact backpropagation rather numerous proximations required boltzmann machine work extends idea generative machine eliminating markov chain used generative stochastic network work backpropagates derivative generative process using observation lim 0 f x ϵ x unaware time developed work kingma welling 18 rezende et al 23 developed general stochastic backpropagation rule allowing one agate gaussian distribution ﬁnite variance backpropagate covariance parameter well mean backpropagation rule could allow one learn tional variance generator treated hyperparameter work kingma welling 18 rezende et al 23 use stochastic backpropagation train variational coder vaes like generative adversarial network variational autoencoders pair differentiable generator network second neural network unlike generative adversarial network ond network vae recognition model performs approximate inference gans require differentiation visible unit thus not model discrete data vaes require differentiation hidden unit thus not discrete latent variable like approach exist 12 22 le closely related method previous work ha also taken approach using discriminative criterion train generative model 29 13 approach use criterion intractable deep generative model method difﬁcult even approximate deep model involve ratio tie not approximated using variational approximation lower bound ity estimation nce 13 involves training generative model learning weight make model useful discriminating data ﬁxed noise distribution using previously trained model noise distribution allows training sequence model increasing quality seen informal competition mechanism similar spirit formal petition used adversarial network game key limitation nce discriminator deﬁned ratio probability density noise distribution model distribution thus requires ability evaluate backpropagate density some previous work ha used general concept two neural network compete relevant work predictability minimization 26 predictability minimization hidden unit neural network trained different output second network predicts value hidden unit given value hidden unit work differs predictability minimization three important way 1 work competition network sole training criterion sufﬁcient train network predictability minimization only regularizer encourages hidden unit neural network tistically independent accomplish some task not primary training criterion 2 nature competition different predictability minimization two network output compared one network trying make output similar trying make 2 output different output question single scalar gans one network produce rich high dimensional vector used input another network attempt choose input network doe not know process 3 speciﬁcation learning process different predictability minimization described optimization problem objective function minimized learning approach minimum objective function gans based minimax game rather optimization problem value function one agent seek maximize seek minimize game terminates saddle point minimum respect one player strategy maximum respect player strategy generative adversarial network ha sometimes confused related concept ial example 28 adversarial example example found using optimization directly input classiﬁcation network order ﬁnd example similar data yet misclassiﬁed different present work adversarial example not mechanism training generative model instead adversarial example primarily analysis tool showing neural network behave intriguing way often conﬁdently sifying two image differently high conﬁdence even though difference imperceptible human observer existence adversarial example doe suggest generative adversarial network training could inefﬁcient show possible make modern discriminative network conﬁdently recognize class without emulating any attribute class 3 adversarial net adversarial modeling framework straightforward apply model multilayer perceptrons learn generator distribution pg data x deﬁne prior input noise variable pz z represent mapping data space g z θg g differentiable function represented multilayer perceptron parameter θg also deﬁne second multilayer perceptron x θd output single scalar x represents probability x came data rather pg train maximize probability assigning correct label training example sample simultaneously train g minimize log 1 g z word g play following minimax game value function v g min g max v g x log x z log 1 g z 1 next section present theoretical analysis adversarial net essentially showing training criterion allows one recover data generating distribution g given enough capacity limit see figure 1 le formal pedagogical explanation approach practice must implement game using iterative numerical approach optimizing completion inner loop training computationally prohibitive ﬁnite datasets would result overﬁtting instead alternate k step optimizing one step optimizing result maintained near optimal solution long g change slowly enough procedure formally presented algorithm practice equation 1 may not provide sufﬁcient gradient g learn well early learning g poor reject sample high conﬁdence clearly different training data case log 1 g z saturates rather training g minimize log 1 g z train g maximize log g z objective function result ﬁxed point dynamic g provides much stronger gradient early learning 4 theoretical result generator g implicitly deﬁnes probability distribution pg distribution sample g z obtained z therefore would like algorithm 1 converge good estimator pdata given enough capacity training time result section done parametric setting represent model inﬁnite capacity studying convergence space probability density function show section minimax game ha global optimum pg pdata show section algorithm 1 optimizes eq 1 thus obtaining desired result 3 x z x z x z x z b c figure 1 generative adversarial net trained simultaneously updating discriminative distribution blue dashed line discriminates sample data generating distribution black dotted line px generative distribution pg g green solid line lower horizontal line domain z sampled case uniformly horizontal line part domain upward arrow show mapping x g z imposes distribution pg transformed sample g contract region high density expands region low density pg consider adversarial pair near convergence pg similar pdata partially accurate classiﬁer b inner loop algorithm trained discriminate sample data converging x pdata x pdata x x c update g gradient ha guided g z ﬂow region likely classiﬁed data several step training g enough capacity reach point not improve pg pdata discriminator unable differentiate two distribution x 1 algorithm 1 minibatch stochastic gradient descent training generative adversarial net number step apply discriminator k hyperparameter used k 1 least expensive option experiment number training iteration k step sample minibatch noise sample z 1 z noise prior pg z sample minibatch example x 1 x data generating distribution pdata x update discriminator ascending stochastic gradient 1 x h log x log 1 g z end sample minibatch noise sample z 1 z noise prior pg z update generator descending stochastic gradient 1 x log 1 g z end update use any standard learning rule used tum experiment global optimality pg pdata ﬁrst consider optimal discriminator any given generator proposition g ﬁxed optimal discriminator g x pdata x pdata x pg x 2 4 proof training criterion discriminator given any generator g maximize quantity v g v g z x pdata x log x dx z z pz z log 1 g z dz z x pdata x log x pg x log 1 x dx 3 any b 0 0 function log b log 1 achieves maximum 0 1 discriminator doe not need deﬁned outside supp pdata pg concluding proof note training objective interpreted maximizing timating conditional probability p indicates whether x come pdata 1 pg 0 minimax game eq 1 reformulated c g max v g log g x log 1 g g z 4 log g x log 1 g x log pdata x pdata x pg x log pg x pdata x pg x theorem global minimum virtual training criterion c g achieved only pg pdata point c g achieves value proof pg pdata g x 1 2 consider eq 2 hence inspecting eq 4 g x 1 2 ﬁnd c g log 1 2 log 1 2 see best possible value c g reached only pg pdata observe 2 2 4 subtracting expression c g v g g obtain c g 4 kl pdata pdata pg 2 kl pg pdata pg 2 5 kl divergence recognize previous expression shannon divergence model distribution data generating process c g 4 2 jsd pdata 6 since divergence two distribution always zero iff equal shown 4 global minimum c g only solution pg pdata generative model perfectly replicating data distribution convergence algorithm 1 proposition g enough capacity step algorithm 1 discriminator allowed reach optimum given g pg updated improve criterion log g x log 1 g x pg converges pdata proof consider v g u pg function pg done criterion note u pg convex pg subderivatives supremum convex function include derivative function point maximum attained word f x fα x fα x convex x every α x β arg fα x equivalent computing gradient descent update pg optimal given responding supd u pg convex pg unique global optimum proven thm 1 therefore sufﬁciently small update pg pg converges px concluding proof practice adversarial net represent limited family pg distribution via function g z θg optimize θg rather pg proof not apply however excellent mance multilayer perceptrons practice suggests reasonable model use despite lack theoretical guarantee 5 model mnist tfd dbn 3 138 2 1909 66 stacked cae 3 121 2110 50 deep gsn 5 214 1890 29 adversarial net 225 2 2057 26 table 1 parzen estimate reported number mnist mean likelihood sample test set standard error mean computed across example tfd computed standard error across fold dataset different σ chosen using validation set fold tfd σ wa cross validated fold mean fold computed mnist compare model rather binary version dataset 5 experiment trained adversarial net range datasets including mnist 21 toronto face database tfd 27 19 generator net used mixture rectiﬁer linear activation 17 8 sigmoid activation discriminator net used maxout 9 activation dropout 16 wa applied training discriminator net theoretical framework permit use dropout noise intermediate layer generator used noise input only bottommost layer generator network estimate probability test set data pg ﬁtting gaussian parzen window sample generated g reporting distribution σ parameter gaussians wa obtained cross validation validation set procedure wa duced breuleux et al 7 used various generative model exact likelihood not tractable 24 3 4 result reported table method estimating likelihood ha somewhat high variance doe not perform well high dimensional space best method available knowledge advance generative model sample not estimate likelihood directly motivate research evaluate model figure 2 3 show sample drawn generator net training make no claim sample better sample generated existing method believe sample least competitive better generative model literature highlight potential adversarial framework 6 advantage disadvantage new framework come advantage disadvantage relative previous modeling work disadvantage primarily no explicit representation pg x must synchronized well g training particular g must not trained much without updating order avoid helvetica scenario g collapse many value z value x enough diversity model pdata much negative chain boltzmann machine must kept date learning step advantage markov chain never needed only backprop used obtain gradient no inference needed learning wide variety function incorporated model table 2 summarizes comparison generative adversarial net generative modeling approach aforementioned advantage primarily computational adversarial model may also gain some statistical advantage generator network not updated directly data ples only gradient ﬂowing discriminator mean component input not copied directly generator parameter another advantage adversarial work represent sharp even degenerate distribution method based markov chain require distribution somewhat blurry order chain able mix mode 7 conclusion future work framework admits many straightforward extension 6 b c figure 2 visualization sample model rightmost column show nearest training example neighboring sample order demonstrate model ha not memorized training set sample fair random draw not unlike visualization deep generative model image show actual sample model distribution not conditional mean given sample hidden unit moreover sample uncorrelated sampling process doe not depend markov chain mixing mnist b tfd c fully connected model convolutional discriminator deconvolutional generator figure 3 digit obtained linearly interpolating coordinate z space full model conditional generative model p x c obtained adding c input g learned approximate inference performed training auxiliary network predict z given similar inference net trained algorithm 15 advantage inference net may trained ﬁxed generator net generator net ha ﬁnished training one approximately model conditionals p x subset index x training family conditional model share parameter essentially one use adversarial net implement stochastic extension deterministic 10 learning feature discriminator inference net could improve mance classiﬁers limited labeled data available efﬁciency improvement training could accelerated greatly devising better method coordinating g determining better distribution sample z training paper ha demonstrated viability adversarial modeling framework suggesting research direction could prove useful 7 deep directed graphical model deep undirected graphical model generative autoencoders adversarial model training inference needed training inference needed training mcmc needed approximate partition function gradient enforced tradeoff mixing power reconstruction generation synchronizing discriminator generator helvetica inference learned approximate inference variational inference inference learned approximate inference sampling no difﬁculties requires markov chain requires markov chain no difﬁculties evaluating p x intractable may approximated ai intractable may approximated ai not explicitly represented may approximated parzen density estimation not explicitly represented may approximated parzen density estimation model design model need designed work desired inference scheme some inference scheme support similar model family gans careful design needed ensure multiple property any differentiable function theoretically permitted any differentiable function theoretically permitted table 2 challenge generative modeling summary difﬁculties encountered different approach deep generative modeling major operation involving model acknowledgment would like acknowledge patrice marcotte olivier delalleau kyunghyun cho guillaume alain jason yosinski helpful discussion yann dauphin shared parzen window uation code u would like thank developer 11 theano 6 1 particularly eric bastien rushed theano feature speciﬁcally beneﬁt project naud bergeron provided support l ex typesetting would also like thank cifar canada research chair funding compute canada calcul ebec providing computational resource ian goodfellow supported 2013 google fellowship deep learning finally would like thank le trois brasseurs stimulating creativity reference 1 bastien lamblin pascanu bergstra goodfellow bergeron bouchard bengio 2012 theano new feature speed improvement deep learning unsupervised feature learning nip 2012 workshop 2 bengio 2009 learning deep architecture ai publisher 3 bengio mesnil dauphin rifai 2013 better mixing via deep representation icml 13 4 bengio yosinski j deep generative stochastic network trainable backprop icml 14 5 bengio alain yosinski j deep generative stochastic work trainable backprop proceeding international conference machine learning icml 14 6 bergstra breuleux bastien lamblin pascanu desjardins turian bengio 2010 theano cpu gpu math expression compiler proceeding python scientiﬁc computing conference scipy oral presentation 7 breuleux bengio vincent 2011 quickly generating representative sample process neural computation 23 8 8 glorot bordes bengio 2011 deep sparse rectiﬁer neural network aistats 2011 8 9 goodfellow mirza courville bengio maxout network icml 2013 10 goodfellow mirza courville bengio deep boltzmann machine nip 2013 11 goodfellow lamblin dumoulin mirza pascanu bergstra bastien bengio machine learning research library arxiv preprint 12 gregor danihelka mnih blundell wierstra 2014 deep autoregressive network icml 2014 13 gutmann hyvarinen 2010 estimation new estimation principle unnormalized statistical model proceeding thirteenth international conference artiﬁcial intelligence statistic aistats 10 14 hinton deng dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury b deep neural network acoustic modeling speech recognition ieee signal processing magazine 29 6 15 hinton dayan frey neal 1995 algorithm unsupervised neural network science 268 16 hinton srivastava krizhevsky sutskever salakhutdinov improving neural network preventing feature detector technical report 17 jarrett kavukcuoglu ranzato lecun 2009 best architecture object recognition proc international conference computer vision iccv 09 page ieee 18 kingma welling 2014 variational bayes proceeding tional conference learning representation iclr 19 krizhevsky hinton 2009 learning multiple layer feature tiny image technical report university toronto 20 krizhevsky sutskever hinton 2012 imagenet classiﬁcation deep convolutional neural network nip 2012 21 lecun bottou bengio haffner 1998 learning applied document recognition proceeding ieee 86 11 22 mnih gregor 2014 neural variational inference learning belief network technical report arxiv preprint 23 rezende mohamed wierstra 2014 stochastic backpropagation approximate inference deep generative model technical report 24 rifai bengio dauphin vincent 2012 generative process sampling contractive icml 12 25 salakhutdinov hinton 2009 deep boltzmann machine aistats 2009 page 455 26 schmidhuber j 1992 learning factorial code predictability minimization neural computation 4 6 27 susskind anderson hinton 2010 toronto face dataset technical report utml tr toronto 28 szegedy zaremba sutskever bruna erhan goodfellow fergus 2014 intriguing property neural network iclr 29 tu z 2007 learning generative model via discriminative approach computer vision pattern recognition cvpr 07 ieee conference page ieee 9