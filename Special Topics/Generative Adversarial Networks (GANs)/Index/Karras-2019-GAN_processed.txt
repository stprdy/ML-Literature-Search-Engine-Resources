generator architecture generative adversarial network tero karras nvidia tkarras samuli laine nvidia slaine timo aila nvidia taila abstract propose alternative generator architecture erative adversarial network borrowing style transfer literature new architecture lead automatically learned unsupervised separation attribute pose identity trained human face stochastic variation generated image freckle hair enables intuitive control synthesis new generator improves term traditional distribution quality metric lead demonstrably better interpolation property also ter disentangles latent factor variation quantify interpolation quality disentanglement propose two new automated method applicable any tor architecture finally introduce new highly varied dataset human face introduction resolution quality image produced ative method especially generative adversarial network gan 21 seen rapid improvement recently 28 41 4 yet generator continue operate black box despite recent effort 2 understanding various aspect image synthesis process origin stochastic feature still lacking property tent space also poorly understood commonly demonstrated latent space interpolation 12 48 34 provide no quantitative way compare different generator motivated style transfer literature 26 generator architecture way expose novel way control image synthesis process generator start learned constant input adjusts style image convolution layer based latent code therefore directly controlling strength image feature different scale combined noise injected directly network architectural change lead automatic unsupervised separation attribute pose identity stochastic variation freckle hair generated image enables intuitive specific mixing interpolation operation not modify discriminator loss function any way work thus orthogonal ongoing discussion gan loss function regularization 23 41 4 37 40 33 generator embeds input latent code mediate latent space ha profound effect factor variation represented network input latent space must follow probability density training data argue lead some degree unavoidable entanglement intermediate latent space free restriction therefore allowed disentangled previous method estimating gree latent space disentanglement not directly plicable case propose two new automated rics perceptual path length linear separability quantifying aspect generator using rics show compared traditional generator tecture generator admits linear le entangled representation different factor variation finally present new dataset human face ffhq offer much higher quality cover considerably wider variation existing resolution datasets appendix made dataset publicly available along source code trained accompanying video found link generator traditionally latent code provided ator input layer first layer forward network figure depart design omitting input layer altogether starting learned constant instead figure right given tent code z input latent space z ping network f z first produce w ure left simplicity set dimensionality 1 4401 normalize pixelnorm pixelnorm conv conv conv pixelnorm pixelnorm upsample normalize fc fc fc fc fc fc fc fc b b b b const adain adain adain adain upsample conv conv conv style style style style noise latent latent mapping network synthesis network traditional b generator figure traditional generator 28 feed latent code though input layer only first map input diate latent space w control generator adaptive instance normalization adain convolution layer gaussian noise added convolution evaluating nonlinearity stand learned affine transform b applies learned scaling factor noise input mapping network f consists 8 layer synthesis work g consists 18 layer two resolution output last layer converted rgb using separate 1 1 convolution similar karras et al 28 generator ha total trainable parameter compared traditional generator space 512 mapping f implemented using mlp decision analyze section learned affine transformation specialize w style yb control adaptive instance normalization adain 26 16 20 15 operation convolution layer synthesis network adain operation defined adain xi xi xi σ xi yb 1 feature map xi normalized separately scaled biased using corresponding scalar ponents style thus dimensionality twice number feature map layer comparing approach style transfer compute spatially invariant style vector w instead example image choose reuse word style similar network architecture already used feedforward style transfer 26 unsupervised image translation 27 domain mixture 22 pared general feature transforms 35 53 adain particularly well suited purpose due efficiency compact representation method ffhq baseline progressive gan 28 b tuning incl bilinear c add mapping style remove traditional input e add noise input f mixing regularization table fréchet inception distance fid various generator sign lower better paper calculate fids using image drawn randomly training set report lowest distance encountered course training finally provide generator direct mean generate stochastic detail introducing explicit noise put image consisting related gaussian noise feed dedicated noise age layer synthesis network noise image broadcasted feature map using learned scaling factor added output sponding convolution illustrated figure plication adding noise input discussed tions quality generated image studying property generator demonstrate experimentally redesign doe not promise image quality fact improves considerably table 1 give fréchet inception distance fid 24 ous generator architecture 28 new ffhq dataset appendix result datasets given supplement baseline configuration progressive gan setup karras et al 28 inherit network hyperparameters cept stated otherwise first switch improved baseline b using bilinear operation 58 longer training tuned hyperparameters tailed description training setup hyperparameters included supplement improve new line adding mapping network adain erations c make surprising observation work no longer benefit feeding latent code first convolution layer therefore simplify ture removing traditional input layer starting image synthesis learned 4 4 512 constant tensor find quite remarkable synthesis network able produce meaningful result even though receives input only style control adain tions finally introduce noise input e improve result well novel mixing regularization f decorrelates neighboring style enables grained control generated imagery section evaluate method using two different loss tions rely 23 4402 figure uncurated set image produced erator config f ffhq dataset used variation truncation trick 38 4 31 ψ resolution 42 please see accompanying video result ffhq us configuration saturating loss 21 regularization 40 47 13 configuration found choice give best result contribution not modify loss function observe generator e improves fids quite significantly traditional generator b 20 corroborating imagenet ments made parallel work 5 4 figure 2 show rated set novel image generated ffhq dataset using generator confirmed fids age quality high even accessory eyeglass hat get successfully synthesized figure avoided sampling extreme region w using truncation trick 38 4 31 appendix b detail trick performed w instead note generator allows applying truncation selectively low resolution only detail not affected fids paper computed without cation trick only use illustrative purpose figure 2 video image generated 10242 resolution prior art much work gan architecture ha focused improving discriminator using multiple inators 17 43 10 multiresolution discrimination 55 51 57 work generator side ha mostly focused exact distribution input latent space 4 shaping input latent space via gaussian mixture el 3 clustering 44 encouraging convexity 48 recent conditional generator feed class identifier separate embedding network large number layer generator 42 latent still vided though input layer author considered feeding part latent code multiple generator layer 8 4 parallel work chen et al 5 self modulate generator using adains similarly work not consider intermediate latent space noise input property generator generator architecture make possible control image synthesis via modification style view mapping network affine formation way draw sample style learned distribution synthesis network way generate novel image based collection style effect style localized network fying specific subset style expected affect only certain aspect image see reason localization let u consider adain operation eq 1 first normalizes channel zero mean unit variance only applies scale bias based style new statistic dictated style modify relative importance feature subsequent convolution operation not depend original statistic malization thus style control only one convolution overridden next adain operation style mixing encourage style localize employ mixing regularization given percentage image generated using two random latent code instead one training generating image ply switch one latent code another operation refer style mixing randomly selected point synthesis network specific run two latent code mapping network responding control style applies fore crossover point regularization technique prevents network assuming adjacent style correlated table 2 show enabling mixing regularization training improves localization considerably indicated 4403 source source b coarse style source b middle style source b fine b figure two set image generated respective latent code source b rest image generated copying specified subset style source b taking rest source copying style corresponding coarse spatial resolution 42 82 brings aspect pose general hair style face shape eyeglass source b color eye hair lighting finer facial feature resemble instead copy style middle resolution 162 322 b inherit smaller scale facial feature hair style eye b pose general face shape eyeglass preserved finally copying fine style 642 10242 b brings mainly color scheme microstructure 4404 mixing number latents testing regularization 1 2 3 4 e 0 50 f 90 100 table fids ffhq network trained enabling mixing regularization different percentage training example stress test trained network randomizing 1 4 latents crossover point mixing regularization prof tolerance adverse operation significantly bel e f refer configuration table 1 generated image b stochastic variation c standard deviation figure example stochastic variation two generated image b different realization input noise overall appearance almost identical individual hair placed differently c standard deviation pixel 100 different realization highlighting part age affected noise main area hair ettes part background also interesting tic variation eye reflection global aspect identity pose unaffected stochastic variation improved fids scenario multiple latents mixed test time figure 3 present example image sized mixing two latent code various scale see subset style control meaningful attribute image stochastic variation many aspect human portrait regarded stochastic exact placement hair stubble freckle skin pore any ized without affecting perception image long follow correct distribution let u consider traditional generator implement stochastic variation given only input work input layer network need invent way generate pseudorandom number earlier activation whenever needed b c figure effect noise input different layer generator noise applied layer b no noise c noise fine er only 642 10242 noise coarse layer only 42 322 see artificial omission noise lead featureless painterly look coarse noise cause curling hair appearance larger background feature fine noise brings finer curl hair finer background detail skin pore sumes network capacity hiding periodicity erated signal difficult not always successful idenced commonly seen repetitive pattern generated image architecture sidestep issue altogether adding noise convolution figure 4 show stochastic realization derlying image produced using generator different noise realization see noise affect only stochastic aspect leaving overall composition aspect identity intact figure 5 illustrates effect applying stochastic variation ferent subset layer since effect best seen animation please consult accompanying video demonstration changing noise input one layer lead stochastic variation matching scale find interesting effect noise appears tightly localized network hypothesize any point generator pressure introduce new tent soon possible easiest way network create stochastic variation rely noise provided fresh set noise available every layer thus no incentive generate stochastic effect earlier activation leading localized effect 4405 distribution b mapping c mapping feature training set z feature w feature figure illustrative example two factor variation age feature masculinity hair length example training set some combination long haired male missing b force mapping z image feature become curved forbidden combination disappears z prevent sampling invalid combination c learned mapping z w able undo much warping separation global effect stochasticity previous section well accompanying video demonstrate change style global fects changing pose identity etc noise affect only consequential stochastic variation differently combed hair beard observation line style transfer erature ha established spatially invariant statistic gram matrix mean variance etc reliably encode style image 19 36 spatially varying feature encode specific instance generator style affect entire image complete feature map scaled biased value therefore global effect pose lighting background style controlled coherently meanwhile noise added independently pixel thus ideally suited controlling stochastic variation network tried control pose using noise would lead spatially inconsistent decision would penalized discriminator thus network learns use global local channel appropriately without explicit guidance disentanglement study various definition disentanglement 50 46 1 6 18 common goal latent space consists linear subspace control one factor ation however sampling probability tion factor z need match corresponding sity training data illustrated figure 6 cludes factor fully disentangled typical datasets input latent major benefit generator architecture intermediate latent space w doe not support artificial datasets designed disentanglement study 39 18 tabulate combination predetermined factor variation uniform frequency thus hiding problem pling according any fixed distribution sampling sity induced learned piecewise continuous ping f z mapping adapted unwarp w factor variation become linear posit pressure generator easier generate realistic image based gled representation based entangled tion expect training yield le gled w unsupervised setting factor variation not known advance 9 32 45 7 25 30 6 unfortunately metric recently proposed ing disentanglement 25 30 6 18 require encoder work map input image latent code metric purpose since baseline gan lack encoder possible add extra network purpose 7 11 14 want avoid investing effort component not part actual solution end describe two new way quantifying tanglement neither requires encoder known factor variation therefore computable any image dataset generator perceptual path length noted laine 34 interpolation tor may yield surprisingly change image example feature absent either endpoint may appear middle linear interpolation path sign latent space entangled factor ation not properly separated quantify effect measure drastic change image undergoes perform interpolation latent space intuitively le curved latent space result perceptually smoother transition highly curved latent space basis metric use pairwise image distance 59 calculated weighted difference two 54 embeddings weight fit metric agrees human tual similarity judgment subdivide latent space terpolation path linear segment define total perceptual length segmented path sum ceptual difference segment reported age distance metric natural definition perceptual path length would limit sum infinitely fine subdivision practice approximate using small subdivision epsilon ǫ average perceptual path length latent space z possible endpoint therefore lz e h 1 slerp g slerp ǫ 2 z 0 1 g generator g network evaluates 4406 method path length full end bility b traditional generator z generator w e add noise input w mixing 50 w f mixing 90 w table perceptual path length separability score various generator architecture ffhq lower better perform measurement z traditional network w based one making network resistant style mixing appears distort intermediate latent space w somewhat size mixing make difficult w efficiently encode factor variation span multiple scale ceptual distance resulting image slerp denotes spherical interpolation 52 propriate way interpolating normalized input latent space 56 concentrate facial feature instead background crop generated image contain only face prior evaluating pairwise image metric metric quadratic 59 divide compute expectation taking sample computing average perceptual path length w carried similar fashion lw e h 1 lerp f f g lerp f f ǫ 3 only difference interpolation happens w space vector w not normalized any fashion use linear interpolation lerp table 3 show length substantially shorter generator noise input dicating w perceptually linear yet measurement fact slightly biased favor put latent space w indeed disentangled tened mapping z may contain region not input manifold thus badly reconstructed generator even point mapped input manifold whereas input latent space z ha no region definition therefore expected restrict measure path endpoint 0 1 obtain smaller lw lz not affected indeed observe table table 4 show path length affected ping network see traditional generator benefit mapping network ditional depth generally improves perceptual path length well fids interesting lw improves traditional generator lz becomes considerably worse lustrating claim input latent space indeed arbitrarily entangled gans method fid path length full end bility b traditional 0 z traditional 8 z traditional 8 w 0 z 1 w 2 w f 8 w table effect mapping network ffhq number method name indicates depth mapping network see fid separability path length benefit mapping network hold ditional generator architecture furthermore deeper mapping network generally performs better shallow one linear separability latent space sufficiently disentangled possible find direction vector consistently spond individual factor variation propose another metric quantifies effect measuring well point separated two distinct set via linear hyperplane set corresponds specific binary attribute image order label generated image train auxiliary classification network number binary attribute distinguish male female face test classifier architecture discriminator use 28 trained using dataset retains 40 attribute available original celeba dataset measure separability one attribute generate image z z classify using auxiliary classification network sort sample according classifier confidence remove least confident half yielding labeled vector attribute fit linear svm predict bel based point z traditional w classify point plane compute conditional entropy h x class predicted svm class termined classifier tell much additional information required determine true class sample given know side plane lie low value suggests consistent latent space direction corresponding factor variation calculate final separability score exp p h enumerates 40 attribute similar inception score 49 exponentiation brings value logarithmic linear domain easier compare table 3 4 show w consistently better arable z suggesting le entangled representation 4407 figure ffhq dataset offer lot variety term age ethnicity viewpoint lighting image background furthermore increasing depth mapping network improves image quality separability w line hypothesis synthesis network herently favor disentangled input representation estingly adding mapping network front traditional generator result severe loss separability z prof situation intermediate latent space w fid improves well show even tional generator architecture performs better duce intermediate latent space doe not follow distribution training data conclusion based result parallel work chen et al 5 becoming clear traditional gan erator architecture every way inferior design true term established quality metric believe investigation ration attribute stochastic effect well linearity intermediate latent space prove fruitful improving understanding controllability gan synthesis note average path length metric could easily used regularizer training perhaps some variant linear separability metric could act one general expect method directly shaping intermediate latent space training provide esting avenue future work acknowledgement thank jaakko lehtinen david luebke tuomas kynkäänniemi discussion helpful ments janne hellsten tero kuosmanen pekka jänis compute infrastructure help code release ffhq dataset collected new dataset human face ffhq consisting age 10242 resolution figure 7 dataset includes vastly variation 28 term age ethnicity image background also ha much ter coverage accessory eyeglass sunglass hat etc image crawled flickr thus ψ 1 ψ ψ ψ 0 ψ ψ figure effect truncation trick function style scale fade ψ face converge mean face ffhq face similar trained network terpolation towards never seems cause artifact applying negative scaling style get corresponding opposite interesting various attribute ten flip opposite including viewpoint glass age coloring hair length often gender heriting bias website automatically aligned 29 cropped only image permissive license collected various automatic filter used prune set finally mechanical turk allowed u remove occasional statue painting photo photo made dataset publicly available truncation trick w consider distribution training data clear area low density poorly represented thus likely difficult generator learn nificant open problem generative modeling technique however known drawing latent vector cated 38 4 otherwise shrunk 31 sampling space tends improve average image quality although some amount variation lost follow similar strategy begin compute center mass w w z f z case ffhq point represents sort average face ure 8 ψ 0 scale deviation given w center w ψ w w ψ brock et al 4 observe only subset network amenable truncation even orthogonal ularization used truncation w space seems work reliably even without change loss function 4408 reference 1 achille soatto emergence ance disentangling deep representation corr 2017 6 2 bau zhu strobelt zhou tenenbaum freeman torralba gan dissection visualizing understanding generative adversarial network proc iclr 2019 1 3 weinshall gaussian mixture tive adversarial network diverse datasets pervised clustering image corr 2018 3 4 brock donahue simonyan large scale gan training high fidelity natural image synthesis corr 2018 1 3 8 5 chen lucic houlsby gelly self modulation generative adversarial network corr 2018 3 8 6 chen li grosse duvenaud ing source disentanglement variational autoencoders corr 2018 6 7 chen duan houthooft schulman sutskever abbeel infogan interpretable representation ing information maximizing generative adversarial net corr 2016 6 8 denton chintala szlam fergus deep generative image model using laplacian pyramid versarial network corr 2015 3 9 desjardins courville bengio gling factor variation via generative entangling corr 2012 6 10 doan monteiro albuquerque mazoure rand pineau hjelm online adaptative lum learning gans corr 2018 3 11 donahue krähenbühl darrell adversarial ture learning corr 2016 6 12 dosovitskiy springenberg brox learning generate chair convolutional neural network corr 2014 1 13 drucker cun improving generalization mance using double backpropagation ieee transaction neural network 3 6 1992 3 14 dumoulin belghazi poole lamb arjovsky mastropietro courville adversarially learned ference proc iclr 2017 6 15 dumoulin perez schucher strub vries courville bengio tions distill transformation 2 16 dumoulin shlens kudlur learned tation artistic style corr 2016 2 17 durugkar gemp mahadevan generative adversarial network corr 2016 3 18 eastwood williams framework quantitative evaluation disentangled representation proc iclr 2018 6 19 gatys ecker bethge image style transfer using convolutional neural network proc cvpr 2016 6 20 ghiasi lee kudlur dumoulin shlens exploring structure arbitrary neural artistic stylization network corr 2017 2 21 goodfellow mirza xu farley ozair courville bengio generative adversarial network nip 2014 1 3 22 hao yu mixgan ing concept different domain mixture generation corr 2018 2 23 gulrajani ahmed arjovsky dumoulin courville improved training wasserstein gans corr 2017 1 2 24 heusel ramsauer unterthiner nessler hochreiter gans trained two update rule converge local nash equilibrium proc nip page 2017 2 25 higgins matthey pal burgess glorot botvinick mohamed lerchner learning basic visual concept constrained variational framework proc iclr 2017 6 26 huang belongie arbitrary style transfer adaptive instance normalization corr 2017 1 2 27 huang liu belongie kautz timodal unsupervised translation corr 2018 2 28 karras aila laine lehtinen progressive growing gans improved quality stability tion corr 2017 1 2 7 8 29 kazemi sullivan one millisecond face alignment ensemble regression tree proc cvpr 2014 8 30 kim mnih disentangling factorising proc icml 2018 6 31 kingma dhariwal glow generative flow invertible convolution corr 2018 3 8 32 kingma welling variational bayes iclr 2014 6 33 kurach lucic zhai michalski gelly gan landscape loss architecture regularization normalization corr 2018 1 34 laine metric exploring latent space generative model iclr workshop poster 2018 1 6 35 li fang yang wang lu yang universal style transfer via feature transforms proc nip 2017 2 36 li wang liu hou demystifying neural style transfer corr 2017 6 37 lucic kurach michalski gelly quet gans created equal study corr 2017 1 38 marchesi megapixel size image creation using generative adversarial network corr 2017 3 8 4409 39 matthey higgins hassabis ner dsprites disentanglement testing sprite dataset 2017 6 40 mescheder geiger nowozin ing method gans actually converge corr 2018 1 3 41 miyato kataoka koyama yoshida tral normalization generative adversarial network corr 2018 1 42 miyato koyama cgans projection inator corr 2018 3 43 mordido yang meinel ing dynamic ensemble discriminator corr 2018 3 44 mukherjee asnani lin kannan gan latent space clustering generative adversarial work corr 2018 3 45 rezende mohamed wierstra stochastic backpropagation approximate inference deep tive model proc icml 2014 6 46 ridgeway survey inductive bias factorial corr 2016 6 47 ross improving adversarial bustness interpretability deep neural network ularizing input gradient corr 2017 3 48 sainburg thielk theilman migliori tner generative adversarial interpolative autoencoding versarial training latent space interpolation encourage convex latent distribution corr 2018 1 3 49 salimans goodfellow zaremba cheung radford chen improved technique training gans nip 2016 7 50 schmidhuber learning factorial code predictability minimization neural computation 4 6 1992 6 51 sharma barratt ermon pande improved training curriculum gans corr 2018 3 52 shoemake animating rotation quaternion curve proc siggraph 85 1985 7 53 siarohin sangineto sebe whitening oring transform gans corr 2018 2 54 simonyan zisserman deep tional network image recognition corr 2014 6 55 wang liu zhu tao kautz catanzaro image synthesis semantic manipulation conditional gans corr 2017 3 56 white sampling generative network note effective technique corr 2016 7 57 zhang goodfellow metaxas odena generative adversarial network corr 2018 3 58 zhang making convolutional network 2019 2 59 zhang isola efros shechtman wang unreasonable effectiveness deep feature tual metric proc cvpr 2018 6 7 4410