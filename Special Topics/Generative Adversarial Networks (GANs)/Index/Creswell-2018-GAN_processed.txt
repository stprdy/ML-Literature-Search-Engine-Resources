53 ieee signal processing magazine january 2018 antonia creswell tom white vincent dumoulin kai arulkumaran biswa sengupta anil bharath deep learning visual understanding part 2 generative adversarial network overview g enerative adversarial network gans provide way learn deep representation without extensively annotated training data achieve deriving tion signal competitive process involving pair network representation learned gans may used variety application including image synthesis semantic image editing style transfer image superresolution classification aim review article provide overview gans signal processing community drawing familiar analogy concept possible addition identifying different method training structing gans also point remaining challenge theory application introduction gans emerging technique semisupervised unsupervised learning achieve implicitly modeling distribution data proposed 2014 1 characterized training pair work competition common analogy apt visual data think one network art forger art expert forger known gan erature generator g creates forgery aim making realistic image expert known tor receives forgery real authentic image aim tell apart see figure 1 trained neously competition crucially generator ha no direct access real only way learns interaction criminator discriminator ha access synthetic sample sample drawn stack real image error signal discriminator provided simple ground truth knowing whether image came real stack generator error signal via discriminator used train generator leading toward able produce forgery better quality network represent generator discriminator typically implemented multilayer network consisting digital object identifier date publication 9 january 2018 authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 54 ieee signal processing magazine january 2018 convolutional fully connected layer generator discriminator network must differentiable though not necessary directly invertible one er generator network mapping some tion space called latent space space data shall focus image may express mally r z g g x r z z sample latent space r x x image denotes number dimension basic gan discriminator network may ilarly characterized function map image data probability image real data distribution rather generator distribution 0 1 x fixed generator g discriminator may trained classify image either training data real close one fixed generator fake close zero discriminator optimal may frozen generator g may continue trained lower accuracy discriminator generator distribution able match real data distribution perfectly criminator maximally confused predicting input practice discriminator might not trained optimal explore training process depth section training top interesting academic problem related training constructing gans motivation behind ing gans may not necessarily generator inator per se representation embodied either pair network used variety subsequent task explore application representation section application preliminary terminology generative model learn capture statistical distribution training data allowing u synthesize sample learned distribution top synthesizing novel data ples may used downstream task tic image editing 2 data augmentation 3 style transfer 4 also interested using representation model learn task classification 5 image retrieval 6 occasionally refer fully connected convolutional layer deep network generalization trons spatial filter bank nonlinear postprocessing case network weight learned backpropagation 7 notation gan literature generally deal multidimensional tor often represents vector probability space ic latent space z field signal processing common represent vector bold lowercase symbol adopt convention emphasize multidimensional nature variable accordingly commonly refer p x data representing probability density function random vector x lie x use p x g denote distribution vector produced generator work gan use calligraphic symbol g denote generator discriminator network tively network set parameter weight h g h learned optimization training synthetic data sample generator trained map noise sample synthetic data sample fool discriminator noise source real data sample real fake z x discriminator trained distinguish real data sample synthesized sample figure two model learned training process gan discriminator h generator g h typically implemented neural network could implemented any form differentiable system map data one space another see article text detail authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 55 ieee signal processing magazine january 2018 system training requires some clear objective function following usual notation use jg g h h jd g h h refer objective function generator discriminator tively choice notation reminds u two objective function sense codependent evolving eter set g h h network iteratively updated shall explore section training finally note multidimensional gradient used update use g dh denote gradient operator respect weight generator parameter dh denote gradient operator respect weight discriminator expected gradient indicated notation e capturing data distribution central problem signal processing statistic density estimation obtaining explicit parametric data real world key motivation behind gans gan literature term data generating distribution often used refer underlying probability density probability mass function observation data gans learn implicitly computing some sort similarity distribution candidate model distribution corresponding real data see figure 2 bother density estimation answer lie heart problem visual ence including image categorization visual object detection recognition object tracking object registration principle bayes theorem inference problem computer vision addressed estimating tional density function possibly indirectly form model learns joint distribution variable interest observed data difficulty face likelihood function image data ficult construct gans explicitly provide way evaluating density function pair suitable capacity generator implicitly capture distribution data related work one may view principle generative model ing comparison standard technique signal ing data analysis example signal processing make wide use idea representing signal weighted combination basis function fixed basis function underlie standard technique wavelet sentations approach constructing basis tions traced back hotelling 8 transform rooted pearson observation principal component minimize reconstruction error according minimum squared error criterion despite wide use standard principal component analysis pca doe not overt statistical model observed data though ha shown base pca may derived maximum likelihood parameter tion problem despite wide adoption pca basis tions emerge eigenvectors covariance matrix observation input data mapping representation space back signal image space linear shallow linear mapping limiting complexity model hence data represented independent component analysis ica provides another level sophistication signal component no longer need orthogonal mixing coefficient used blend component together construct example data merely considered statistically independent ica ha various formulation differ objective function used estimating signal component tive model express signal image generated component recent innovation explored ica noise contrastive estimation nce may seen approaching spirit gans 9 objective function learning independent component compare statistic applied 0 1 2 0 1 2 0 1 2 0 1 2 pg x pdata x pg x pdata x sample produced generator sample real data figure gan training generator encouraged produce distribution sample p x g match real data p x data propriately parameterized trained gan distribution nearly identical representation embodied gans captured learned parameter weight generator discriminator network authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 56 ieee signal processing magazine january 2018 noise produced candidate generative model 10 original nce approach not include update generator comparison made gans standard tool signal processing pca ica rier wavelet representation latent space gans analogy coefficient space commonly refer transform space set gans apart standard tool signal processing level complexity el map vector latent space image space generator network contain nonlinearities almost arbitrary depth many extraordinarily complex regard deep model modern e generative image modeling grouped explicit implicit density model explicit density model either tractable change variable model autoregressive model intractable directed model trained variational ence undirected model trained using markov chain implicit density model capture statistical distribution data generative process make use either ancestral sampling 11 markov sampling gans fall directed implicit model category detailed view relevant paper found 12 gan architecture fully connected gans first gan architecture used fully connected neural work generator discriminator 1 type architecture wa applied relatively simple image data set mnist handwritten digit natural image toronto face data set tfd convolutional gans going fully connected convolutional neural network cnns natural extension given cnns extremely well suited image data early experiment conducted suggested wa difficult train generator inator network using cnns level capacity tional power used vised learning laplacian pyramid ial network lapgan 13 offered one solution problem posing generation process using multiple scale image decomposed laplacian amid conditional convolutional gan trained produce layer given one additionally radford et al 5 posed family network tures called deep convolutional gan dcgan allows training pair deep convolutional generator discriminator network dcgans make use strided fractionally strided convolution allow spatial downsampling pling operator learned training operator handle change sampling rate tions key requirement mapping image space possibly dimensional latent space image space discriminator ther detail dcgan ture training presented section training extension synthesizing image two dimension wu et al 14 presented gans able synthetic data sample synthetic data sample generator must learn create conditional image sample generator must learn create conditional image sample noise source noise source class category class category real data sample real data sample real fake real fake discriminator trained distinguish real data synthesized sample conditional class c discriminator output estimated class label decision authenticity z z c c c b figure 3 conditional gan proposed mirza et al 15 performs image synthesis discriminator performs discrimination real fake image b infogan 16 hand ha discriminator network also estimate class label authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 57 ieee signal processing magazine january 2018 synthesize data sample using metric convolution wu et al 14 synthesized novel object including chair table car addition also sented method map image version object portrayed image conditional gans mirza et al 15 extended gan framework conditional setting making generator criminator network figure 3 conditional gans advantage able provide better resentations multimodal data generation parallel drawn conditional gans infogan 16 decomposes noise source incompressible source latent code attempting discover latent factor variation maximizing mutual information latent code generator output latent code used discover object class purely unsupervised ion although not strictly necessary latent code categorical representation learned infogan appear semantically meaningful dealing complex gled factor image appearance including variation pose lighting emotional content facial image 16 gans inference model original formulation gans lacked way map given observation x vector latent gan literature often referred inference mechanism several technique proposed invert tor pretrained gans 17 18 independently proposed adversarially learned inference ali 19 bidirectional gans bigans 20 provide simple effective extension introducing inference network discriminator examine joint data latent pair formulation generator consists two network encoder inference network jointly trained fool discriminator discriminator receives pair x z vector see figure 4 ha determine pair constitutes genuine tuple consisting real image sample encoding fake image sample corresponding input generator ideally model output referred reconstruction similar input cally fidelity reconstructed data sample synthesized using poor fidelity sample may improved additional adversarial cost tion data sample reconstruction 21 adversarial autoencoders autoencoders network composed encoder decoder learn map data internal latent tation learn deterministic ping via encoder data space image latent representation space mapping via decoder latent space back data space composition two mapping result reconstruction two mapping trained reconstructed image close possible original autoencoders reminiscent filter bank widely used image signal processing x x z ε synthetic data sample generator must learn create image sample noise source encoding data sample encoder must learn map real image sample latent space discriminator receives tuples z x real data sample real fake figure structure 19 20 consists three network one serf discriminator another map noise vector latent space image space decoder depicted generator g figure final network encoder depicted e mapping image space latent space authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 58 ieee signal processing magazine january 2018 however autoencoders generally learn nonlinear mapping direction implemented deep work possible architecture used implement autoencoders remarkably flexible training pervised backpropagation applied reconstructed image original learn parameter encoder decoder suggested previously one often want latent space useful organization additionally one may want perform ancestral sampling 11 encoder adversarial training provides route achieve two goal specifically adversarial training may applied latent space desired prior distribution latent space gan result combined loss function 22 reflects reconstruction error measure different distribution prior produced candidate encoding network approach akin variational autoencoder vae 23 gan play role leibler kl term loss function mescheder et al 24 unified vaes adversarial ing form adversarial variational bayes avb framework similar idea presented 12 avb try optimize criterion vaes us sarial training objective rather kl divergence training gans introduction training gans involves finding parameter discriminator maximize classification accuracy finding parameter generator maximally fuse discriminator training process summarized figure cost training evaluated using value function v g h depends generator nator training involves solving maxminv g g h log log v 1 x x e e g p p x x g data h training parameter one model ed parameter fixed goodfellow et al 1 show fixed generator unique optimal discriminator p p p x x x x g data data h also show generator g optimal p p x x g data equivalent optimal nator predicting sample drawn x word generator optimal discriminator maximally confused not distinguish real sample one fake ideally discriminator trained optimal respect current generator generator updated however practice discriminator might not trained optimal rather may only trained small number iteration generator updated ously discriminator alternate ing training criterion typically used generator using max log z g g rather min log 1 z g g despite theoretical existence unique solution gan training challenging often unstable several reason 5 25 26 one approach improving gan training draw sample noise source draw sample real image draw sample noise source estimated expected gradient estimated expected gradient update discriminator parameter update generator parameter z z x θg update θg θd update θd jθd θd θg jθg θg θd figure main loop gan training novel data sample xl may drawn passing random sample z generator network ent discriminator may updated k time updating generator authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 59 ieee signal processing magazine january 2018 ass empirical symptom might experienced training symptom include getting pair model converge 5 generative model collapsing generate lar sample different input 25 discriminator loss converging quickly zero 26 viding no reliable path gradient update generator several author suggested heuristic approach address issue 1 25 discussed next section early attempt explain gan training unstable proposed goodfellow man et al 1 25 observed gradient descent method typically used updating parameter generator discriminator propriate solution mization problem posed gan training actually constitutes saddle point man et al provided simple example show 25 however stochastic gradient descent often used update neural network oped programming environment make easy construct update work using stochastic gradient descent although early theoretical treatment 1 showed generator optimal p p x x g data neat result strong underlying intuition real data sample reside manifold sits space possible representation instance color image sample size n n 3 pixel value 0 r 3 space may call dimensionality n 3 2 dimension taking value zero maximum measurable pixel intensity data sample support pdata however constitute manifold real data associated some particular problem typically pying small part total space x similarly sample produced generator also occupy only small portion x arjovsky et al 26 showed support p x g p x data lie space ing x consequence p x g p x data may no overlap exists nearly trivial criminator capable distinguishing real sample p x x data fake sample p x x g 100 accuracy case discriminator error quickly converges zero parameter generator may only updated via criminator happens gradient used ing parameter generator also converge zero may no longer useful update generator arjovsky et explanation account several symptom related gan training 26 goodfellow et al 1 also showed optimal training g equivalent minimizing j divergence p x g p x data not mal update may le meaningful inaccurate theoretical insight ha motivated research cost function based alternative distance several explored section alternative training trick one first major improvement training gans generating image dcgan architecture posed radford et al 5 work wa result extensive exploration cnn architecture previously used computer vision resulted set guideline constructing training generator discriminator section convolutional gans alluded importance strided fractionally ed convolution 27 key nents architectural design allows generator discriminator learn good upsampling downsampling operation may contribute ments quality image synthesis specifically training batch normalization 28 wa recommended use work stabilize training deeper model another suggestion wa minimize number fully nected layer used increase feasibility training deeper model finally radford et al 5 showed using leaky tifying linear unit relus activation function intermediate layer discriminator gave superior mance using regular relus later salimans et al 25 proposed heuristic approach stabilizing training gans first ture matching change objective generator slightly increase amount information available specifically discriminator still trained distinguish real fake sample generator trained match criminator expected intermediate activation feature fake sample expected intermediate activation real sample second minibatch discrimination add extra input discriminator feature encodes distance given sample minibatch sample intended prevent mode collapse discriminator easily tell generator producing output third trick heuristic averaging penalizes network parameter deviate running average ous value help convergence equilibrium fourth virtual batch normalization reduces dependency one sample sample minibatch culating batch statistic normalization sample placed within reference minibatch fixed ning training finally label smoothing make target discriminator instead one smoothing discriminator classification boundary hence preventing overly confident discriminator would provide weak gradient erator sønderby et al 29 advanced idea challenging representation learned gans may used variety application including image synthesis semantic image editing style transfer image superresolution classification authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 60 ieee signal processing magazine january 2018 discriminator adding noise sample feeding criminator sønderby et al 29 argued label smoothing bias mal discriminator technique instance noise move manifold real fake sample closer together time preventing discriminator ily finding discrimination boundary completely separate real fake ples practice implemented adding gaussian noise synthesized real image annealing standard deviation time process wa dently proposed arjovsky et al 26 alternative formulation first part section considers oretic interpretation generalization gans ond part look alternative cost function aim directly address problem vanishing gradient generalization gan cost function nowozin et al 30 showed gan training may alized minimize not only j divergence estimate referred gences include divergence measure nowozin et al showed may approximated applying fenchel conjugate desired sample drawn distribution generated sample passing sample criminator 30 provide list fenchel conjugate commonly used well activation function may used final layer generator network depending choice derived generalized cost function training generator criminator nowozin et al 30 observe raw form maximizing generator objective likely lead weak gradient especially start training proposed alternative cost function updating tor le likely saturate beginning training nowozin et al proposed discriminator trained derivative ratio real fake data distribution estimated generator trained only estimate minimized uehara et al 31 extend criminator step ratio distribution real fake data predicted generator step directly minimized alternative also covered goodfellow 12 alternative cost function prevent vanishing gradient arjovsky et al 32 proposed wasserstein gan wgan gan alternative cost function derived approximation wasserstein distance unlike nal gan cost function wgan likely provide gradient useful updating generator cost function derived wgan relies discriminator refer critic continuous function practically may mented simply clipping parameter discriminator however recent research 33 suggested weight clipping adversely reduces capacity criminator model forcing learn simpler function gulrajani et al 33 proposed improved method training discriminator wgan penalizing norm discriminator gradient respect data sample training rather performing eter clipping brief comparison gan variant gans allow u synthesize novel data sample random noise considered difficult train due partially vanishing gradient gan model discussed article require careful hyperparameter tuning model selection training however perhaps easier model train adversarial autoencoder aae wgan aae relatively easy train adversarial loss applied fairly simple distribution lower dimension image data wgan 33 designed easier train using different formulation training objective doe not suffer vanishing gradient problem wgan may also trained successfully even without batch normalization also le sensitive choice earities used convolutional layer sample synthesized using gan wgan may belong any class present training data conditional gans provide approach synthesizing sample specified content evident various visualization technique ure 6 organization latent space harbor some meaning vanilla gans not provide inference model allow data sample mapped latent sentations bigans ali provide mechanism map image data latent space inference however reconstruction quality suggests not necessarily faithfully encode decode sample recent opment show ali may recover encoded data sample faithfully 21 however model share lot common avb aae autoencoders similar vaes latent space regularized using sarial training rather encoded sample prior structure latent space gans build representation data trained produce structured geometric tor space different domain quality shared neural network model including vaes 23 well linguistic model 34 general domain data modeled mapped vector space set gans apart standard tool signal processing level complexity model map vector latent space image space authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 61 ieee signal processing magazine january 2018 ha fewer dimension data space forcing model discover teresting structure data sent efficiently latent space originating end generator network data level resentation latent space highly structured may support mantic operation 5 example include rotation face trajectory latent space well image analogy effect adding visual attribute eyeglass onto bare face vanilla gan model generator map data latent space space modeled many gan model encoder additionally support inverse mapping 19 20 becomes powerful od exploring using structured latent space gan network encoder collection labeled image mapped latent space analyzed discover concept vector represent attribute smiling wearing vector applied scaled offset latent space influence behavior generator figure 6 similar using encoding process model distribution latent sample gurumurthy et al 35 propose modeling latent space mixture ians learning mixture component maximize likelihood generated data sample data ing distribution application gans discovering new application adversarial training deep network active area research examine computer vision application appeared ture subsequently refined application chosen highlight some different approach using based representation image manipulation analysis characterization not fully reflect potential breadth application gans using gans image classification place within broader context machine learning provides useful quantitative assessment feature extracted vised learning image synthesis remains core gan capability especially ful generated image subject constraint superresolution 36 38 offer example ing approach supplemented adversarial loss component achieve result finally translation demonstrates gans offer solution family task require automatically converting input image output image classification regression gan training complete neural network reused downstream task example output convolutional layer discriminator used feature extractor simple linear model fitted top feature using modest quantity image label pair 5 25 quality unsupervised representation dcgan network assessed applying larized classifier feature vector extracted trained discriminator 5 good classification score achieved using approach supervised pervised data set even disjoint nal training data quality data representation may improved adversarial training includes jointly learning ence mechanism ali 19 representation tor wa built using last three hidden layer ali encoder similar classifier yet achieved misclassification rate significantly lower dcgan 19 additionally ali ha achieved art classification result label information incorporated training routine labeled training data limited supply adversarial training may also used synthesize training sample shrivastava et al 39 use gans refine synthetic image maintaining annotation information training model only synthetic image no real training data shrivastava et al 39 achieved performance task similarly good result obtained gaze estimation prediction figure example applying smile vector ali model 19 first image example unsmiling woman last example woman smiling z value first image inferred last interpolating along vector connects give z value may passed generator synthesize novel sample note implication displacement vector latent space traverse smile intensity image space figure used courtesy tom white gans build representation data trained produce structured geometric vector space different domain authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 62 ieee signal processing magazine january 2018 using spatiotemporal gan architecture 40 some case model trained synthetic data not generalize well applied real data 3 bousmalis et al 3 propose address problem adapting synthetic sample source domain match target domain using adversarial training additionally liu et al 41 propose using multiple one per tied weight synthesize pair responding image sample different domain quality generated sample hard quantitatively judge across model classification task likely remain important quantitative tool performance assessment gans even new diverse application computer vision explored image synthesis much recent gan research focus improving quality utility capability lapgan model introduced cascade convolutional work within laplacian pyramid framework generate image fashion 13 similar approach used huang et al 42 gans operating intermediate representation rather image lapgan also extended conditional version gan model g network receive additional label information input technique ha proved useful common practice improve image quality idea gan conditioning wa later extended incorporate natural language example reed et al 43 used gan ture synthesize image text description one might describe reverse captioning example given text caption bird white some black head wing long orange beak trained gan generate several plausible image match description addition conditioning text description generative adversarial network gawwn ditions image location 44 gawwn system supported active interface large image could built incrementally textual description part supplied bounding box figure 7 conditional gans not only allow u synthesize novel sample ic attribute also allow u op tool intuitively editing image changing hairstyle person image making wear e editing image appear younger 35 additional application gans image editing include work zhu brock et al 2 45 translation conditional adversarial network well suited ing input image output image recurring theme computer graphic image processing computer vision model offer tion family problem 46 addition learning mapping input image output image model also construct loss function train mapping model ha demonstrated effective result different problem computer vision previously required rate machinery including semantic segmentation generating map aerial photo colorization black white image wang et al present similar idea using gans first synthesize map similar depth map map image natural scene cyclegan 4 extends work introducing cycle consistency loss attempt preserve original image cycle translation reverse translation mulation matching pair image no longer needed training make data preparation much simpler open technique larger family application example artistic style transfer 47 render natural image style artist picasso monet simply trained unpaired collection painting natural image figure 8 superresolution superresolution allows image generated image trained model ring detail upsampling srgan bird completely black bird bright blue man orange jacket black pant black cap wearing sunglass skiing head right leg belly beak figure example image synthesis using gawwn gawwn image conditioned text description image location specified either keypoint bounding box figure reproduced 44 permission authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 63 ieee signal processing magazine january 2018 model 36 extends earlier effort adding adversarial loss component constrains image reside manifold natural image srgan generator conditioned image infers natural image 4 upscaling factor unlike gan application sarial loss one component larger loss function also includes perceptual loss pretrained classifier regularization loss encourages spatially coherent image context adversarial loss constrains overall tion manifold natural image producing perceptually convincing solution customizing application often pered availability relevant curated training data set however srgan straightforward customizing specific domain new training image pair easily constructed downsampling corpus image important consideration practice since inferred detail gan generates vary depending domain image used training set discussion open question gans attracted considerable attention due ability leverage vast amount unlabeled data much res ha made alleviate some challenge related training evaluating gans still remain several open challenge mode collapse articulated section training gans common problem gans involves generator collapsing produce small family similar sample partial collapse worst case producing simply single sample complete lapse 26 48 diversity generator increased practical hack balance distribution sample produced discriminator real fake batch employing multiple gans cover different mode probability distribution 49 yet another solution alleviate mode collapse alter distance measure used compare statistical butions arjovsky 32 proposed compare distribution based wasserstein distance rather divergence dcgan 5 distance gan 50 metz et al 51 proposed unrolling discriminator several step letting calculate update current generator several step using unrolled criminators update generator using normal minimax objective normal discriminator only train update one step generator ha access criminator would update usual one step generator objective discriminator simply assign low probability generator previous output forcing generator move resulting either convergence endless cycle mode ping however unrolled objective generator prevent discriminator focusing previous update update generation foresight criminator would responded training point gan hessian loss function becomes indefinite optimal solution therefore lie finding saddle point rather local minimum deep learning large ber optimizers depend only first derivative loss function converging saddle point gans requires good initialization invoking stable manifold theorem nonlinear system theory lee et al 52 showed select initial point optimizer random gradient descent would not converge saddle probability one also see 25 53 additionally mescheder et al 54 argued convergence gan objective function suffers presence zero real part jacobian matrix well eigenvalue large imaginary part ening gan training yet due existence monet monet zebra summer photo photo horse zebra horse winter summer winter photo monet horse zebra winter summer b c figure cyclegan model learns image image translation two unordered image collection shown example al image mapping monet painting landscape photo b zebra horse c summer winter photo yosemite national park figure reproduced 4 permission authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 64 ieee signal processing magazine january 2018 optimizers not hope lost unfortunately method complexity scale cubically quadratically dimension parameter therefore another line question lie applying scaling order optimizers adversarial training fundamental problem existence rium gan using result bayesian nonparametrics arora et al 48 connects existence equilibrium finite mixture neural mean tain capacity no equilibrium might exist closely related note ha also argued gan training appear converged trained distribution could still far away target distribution alleviate issue arora et al 48 propose new measure called neural net distance evaluating generative model one gauge fidelity sample synthesized erative model use likelihood estimation gan trained using one methodology compared another model comparison question not only relevant gans also probabilistic model general theis 55 argued evaluating gans using ent measure lead conflicting conclusion quality synthesized sample decision select one measure another depends application conclusion explosion interest gans driven not only potential learn deep highly nonlinear mapping latent space data space back also potential make use vast quantity unlabeled image data remain closed deep representation learning within tie gan training many opportunity opments theory algorithm power deep network vast opportunity new application acknowledgment would like thank david valuable feedback previous revision article antonia creswell acknowledges support engineering physical ences research council doctoral training scholarship author antonia creswell received degree imperial college london biomedical engineering 2011 currently degree student biologically inspired computer vision group imperial college london focus research improving training erative adversarial network applying visual search learning representation unlabeled source image data tom white tom received degree mathematics university georgia degree medium art science massachusetts institute technology currently senior lecturer school design victoria university wellington new zealand current research focus exploring growing use structive machine learning computational design ative potential human designer working collaboratively artificial neural network exploration design idea prototyping vincent dumoulin received degree physic computer science university montréal canada doctoral candidate montréal institute learning algorithm sion yoshua bengio aaron courville working learning approach generative modeling kai arulkumaran received degree computer science university cambridge united kingdom 2012 degree biomedical engineering imperial college london 2014 currently candidate department bioengineering wa research intern twitter magic pony microsoft research research focus deep reinforcement learning computer vision visuomotor control biswa sengupta biswasengupta received honor degree electrical computer engineering 2004 degree theoretical computer science 2005 university york united kingdom received second degree neural behavioral science 2007 max planck institute biological cybernetics germany degree theoretical neuroscience 2011 university cambridge united kingdom received training bayesian statistic differential geometry university college london university cambridge leading cortexica vision system chief scientist currently visiting scientist imperial college london also leading research noah ark lab huawei technology united kingdom anil bharath received eng degree electronic electrical engineering university college london 1988 degree signal processing imperial college london 1993 currently reader department bioengineering academic fellow imperial data science institute low institution engineering technology wa academic visitor signal processing group university cambridge cofounder cortexica vision system research interest deep architecture visual inference reference 1 goodfellow mirza xu ozair courville bengio generative adversarial net proc advance neural information processing system 2014 pp 2 zhu krähenbühl shechtman efros generative visual manipulation natural image manifold proc european conf computer vision 2016 pp 3 bousmalis silberman dohan erhan krishnan unsupervised domain adaptation generative adversarial work proc ieee conf computer vision pattern recognition 2016 pp 4 zhu park isola efros 2017 unpaired translation using adversarial network proc int conf computer vision online available authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 65 ieee signal processing magazine january 2018 5 radford metz chintala unsupervised representation learning deep convolutional generative adversarial network proc int conf learning representation workshop track 2016 6 creswell bharath adversarial training sketch retrieval proc european conf computer vision workshop amsterdam netherlands 2016 pp 7 lecun bengio hinton deep learning nature vol 521 no 7553 pp 2015 8 hotelling analysis complex statistical variable principal ponents educ vol 24 no 6 pp 417 1933 9 goodfellow distinguishability criterion estimating generative el proc int conf learning representation workshop track 2015 10 gutmann hyvärinen estimation new estimation principle unnormalized statistical model artif intell vol 1 no 2 6 2010 11 bengio yao alain vincent generalized denoising encoders generative model proc advance neural information processing system 2013 pp 12 goodfellow 2016 nip 2016 tutorial generative adversarial network proc neural information processing system conf online available http 13 denton chintala szlam fergus deep generative image model using laplacian pyramid adversarial network proc advance neural information processing system 2015 pp 14 wu zhang xue freeman tenenbaum learning listic latent space object shape via modeling proc advance neural information processing system 2016 pp 15 mirza osindero conditional generative adversarial net arxiv preprint 2014 16 chen duan houthooft schulman sutskever abbeel infogan interpretable representation learning information maximizing tive adversarial net proc advance neural information processing system 2016 pp 17 creswell bharath inverting generator generative sarial network proc neural information processing system workshop adversarial training 2016 18 lipton tripathi precise recovery latent vector generative adversarial network proc int conf learning representation workshop track 2017 19 dumoulin belghazi poole mastropietro lamb arjovsky courville adversarially learned inference proc int conf learning representation 2017 20 donahue krähenbühl darrell adversarial feature learning proc int conf learning representation 2017 21 li liu chen pu chen henao carin towards understanding adversarial learning joint distribution matching proc advance neural information processing system 2017 22 makhzani shlens jaitly goodfellow 2016 adversarial encoders proc int conf learning representation online available http 23 kingma welling variational bayes proc int conf learning representation 2014 24 mescheder nowozin geiger 2017 adversarial variational bayes unifying variational autoencoders generative adversarial network online available 25 salimans goodfellow zaremba cheung radford chen improved technique training gans proc advance neural information processing system 2016 pp 26 arjovsky bottou towards principled method training tive adversarial network proc neural information processing system conf workshop adversarial training 2016 27 shelhamer long darrell fully convolutional network semantic segmentation ieee trans pattern anal mach vol 39 no 4 pp 2017 28 ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift proc int conf machine learning 2015 pp 29 sønderby caballero theis shi huszár amortised map inference image proc int conf learning representation 2017 30 nowozin cseke tomioka training generative neural sampler using variational divergence minimization proc advance neural information processing system 2016 pp 31 uehara sato suzuki nakayama matsuo generative adversarial net density ratio estimation perspective arxiv preprint 2016 32 arjovsky chintala bottou wasserstein gan proc int conf machine learning 2017 pp 33 gulrajani ahmed arjovsky dumoulin courville improved training wasserstein gans proc advance neural information processing system 2017 34 mikolov chen corrado dean efficient estimation word representation vector space proc int conf learning representation 2013 35 gurumurthy sarvadevabhatla radhakrishnan deligan generative adversarial network diverse limited data proc ieee conf computer vision pattern recognition 2017 pp 36 ledig theis huszár caballero aitken tejani totz wang shi single image using generative adversarial network proc ieee conf computer vision pattern recognition 2017 pp 37 yu porikli face image discriminative generative network proc european conf computer vision 2016 pp 38 yu porikli hallucinating unaligned noisy face image transformative discriminative autoencoders proc ieee conf computer vision pattern recognition 2017 pp 39 shrivastava pfister tuzel susskind wang webb learning simulated unsupervised image adversarial training proc ieee conf computer vision pattern recognition 2016 pp 2116 40 zhang lim zhao feng deep future gaze gaze anticipation egocentric video using adversarial network proc ieee conf computer vision pattern recognition 2017 pp 41 liu tuzel coupled generative adversarial network proc advance neural information processing system 2016 pp 42 huang li poursaeed hopcroft belongie stacked tive adversarial network proc ieee conf computer vision pattern recognition 2016 43 reed akata yan logeswaran schiele lee 2016 generative adversarial text image synthesis proc int conf machine learning online available 44 reed akata mohan tenka schiele lee learning draw proc advance neural information processing system 2016 pp 45 brock lim ritchie weston neural photo editing introspective adversarial network proc int conf learning representation 2017 46 isola zhu zhou efros translation conditional adversarial network proc ieee conf computer vision pattern recognition 2016 pp 47 li wand precomputed texture synthesis markovian generative adversarial network proc european conf computer vision 2016 pp 48 arora ge liang zhang generalization um generative adversarial net gans proc int conf machine learning 2017 pp 49 tolstikhin gelly bousquet schölkopf adagan boosting generative model arxiv preprint online available 50 zhao mathieu lecun 2017 generative ial network proc int conf learning representation online available 51 metz poole pfau 2017 unrolled generative adversarial network proc int conf learning representation online available 52 lee simchowitz jordan recht gradient descent only converges minimizers proc conf learning theory 2016 pp 53 pemantle nonconvergence unstable point urn model stochastic approximation ann vol 18 no 2 pp apr 1990 54 mescheder nowozin geiger 2017 numerics gans proc advance neural information processing system conf online available 55 theis van den oord bethge note evaluation ative model proc int conf learning representation sp authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply