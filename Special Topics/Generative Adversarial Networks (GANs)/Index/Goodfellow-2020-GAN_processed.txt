november 2020 vol 63 no 11 communication acm 139 generative adversarial network ian goodfellow jean mehdi mirza bing xu david sherjil ozair aaron courville yoshua bengio abstract generative adversarial network kind artificial ligence algorithm designed solve generative ing problem goal generative model study collection training example learn probability distribution generated generative adversarial network gans able generate example estimated probability distribution generative model based deep learning common gans among successful generative model cially term ability generate realistic resolution image gans successfully applied wide variety task mostly research setting continue present unique challenge research opportunity based game theory approach generative modeling based optimization introduction current approach developing artificial gence based primarily machine learning widely used successful form machine learning date supervised learning supervised learning algorithm given dataset pair example input example put learn associate input output thus learning mapping input output ples input example typically complicated data object like image natural language sentence audio waveform output example often relatively simple common kind supervised learning classification output integer code tifying specific category photo might recognized coming category 0 containing cat category 1 taining dog supervised learning often able achieve greater human accuracy training process complete thus ha integrated many product service unfortunately learning process still fall far short human ability supervised learning definition relies human supervisor provide output example input example worse existing approach vised learning often require million training example exceed human performance human might able learn perform task acceptably small number example order reduce amount human sion required learning number example required learning many researcher today study unsupervised learning often using generative model overview paper describe one particular approach unsupervised learning via generative modeling called generative adversarial network briefly review original version paper entitled generative adversarial network wa published advance neural information processing system 27 nip 2014 application gans identify core research problem related convergence game necessary make gans reliable technology generative modeling goal supervised learning relatively straightforward specify supervised learning algorithm essentially goal learn accurately associate new input example correct output instance object recognition algorithm may associate photo dog some kind dog category identifier unsupervised learning le clearly defined branch machine learning many different unsupervised ing algorithm pursuing many different goal broadly speaking goal unsupervised learning learn thing useful examining dataset containing unlabeled input example clustering dimensionality reduction common example unsupervised learning another approach unsupervised learning generative modeling generative modeling training example x drawn unknown distribution pdata x goal generative modeling algorithm learn pmodel x approximates pdata x closely possible straightforward way learn approximation pdata explicitly write function pmodel x θ controlled eters θ search value parameter make pdata pmodel similar possible particular popular approach generative modeling probably mum likelihood estimation consisting minimizing divergence pdata pmodel common approach estimating mean parameter gaussian distribution taking mean set tions one example maximum likelihood estimation approach based explicit density function trated figure explicit density modeling ha worked well traditional statistic using simple functional form probability tributions usually applied small number variable recently rise machine learning general deep learning particular researcher become interested learning model make use relatively complicated functional form deep neural work used generate data corresponding density function may computationally intractable traditionally two dominant approach confronting intractability problem 1 carefully design model tractable density function 2 design learning algorithm based research highlight 140 communication acm november 2020 vol 63 no 11 computationally tractable approximation intractable density function kingma approach proved difficult many application erating realistic high resolution image researcher remain unsatisfied result far motivates research improve two path also suggests third path could useful besides taking point x input returning mate probability generating point generative model useful able generate sample distribution pmodel illustrated figure many model represent density function also generate sample density function some case ing sample expensive only approximate method generating sample tractable some generative model avoid entire issue ing tractable density function learn only tractable sample generation process called implicit tive model gans fall category prior duction gans state art deep implicit generative model wa generative stochastic ble approximately generating sample via incremental process based markov chain gans introduced order create deep implicit generative model wa able generate true sample model distribution single generation step without need tal generation process approximate nature sampling markov chain today popular approach generative eling probably gans variational belief net 26 none approach relies markov chain reason interest gans today not succeeded original goal generative modeling without markov chain rather succeded generating ity image proven useful several task straightforward generation described section 5 generative adversarial network generative adversarial network based game sense game theory two machine learning model typically implemented using neural network one network called generator defines pmodel x itly generator not necessarily able evaluate sity function pmodel some variant gans evaluation density function possible any tractable density model sampling tractable differntiable could trained gan generator done danihelka learned model training data generated sample figure goal many generative model illustrated study collection training example learn generate example come probability distribution gans learn without using explicit representation density function one advantage gan framework may applied model density function computationally intractable sample shown sample imagenet including one labeled model use actual imagenet data illustrate goal hypothetical perfect model would attain x p x figure many approach generative modeling based density estimation observing several training example random variable x inferring density function p x generates training data approach illustrated several data point real number line used fit gaussian density function explains observed sample contrast common approach gans implicit model infer probability distribution p x without necessarily representing density function explicitly november 2020 vol 63 no 11 communication acm 141 et not required instead generator able draw sample distribution pmodel erator defined prior distribution p z vector z serf input generator function g z θ g θ g set learnable parameter defining generator strategy game input vector z thought source randomness otherwise deterministic tem analogous seed pseudorandom number erator prior distribution p z typically relatively unstructured distribution gaussian distribution uniform distribution hypercube sample z distribution noise main role generator learn tion g z transforms unstructured noise z realistic sample player game discriminator discriminator examines sample x return some mate x θ whether x real drawn training distribution fake drawn pmodel running erator original formulation gans estimate consists probability input real rather fake assuming real distribution fake tion sampled equally often formulation arjovsky et exist generally speaking level verbal intuitive description discriminator try dict whether input wa real fake player incurs cost j g θ g θ generator j θ g θ discriminator player attempt minimize cost roughly speaking tor cost encourages correctly classify data real fake generator cost encourages generate sample discriminator incorrectly classifies real many different specific formulation cost possible far popular formulation seem form roughly original version gans j wa defined negative discriminator assigns label given input discriminator word discriminator trained like regular binary classifier original work gans offered two version cost ator one version today called minimax gan defined cost j g yielding minimax game straightforward analyze theoretically defines cost generator flipping sign tor cost another approach gan generator cost defined ping discriminator label word tor tried minimize negative discriminator assigns wrong label later help avoid gradient saturation training model think gans bit like counterfeiter police counterfeiter make fake money police try arrest counterfeiter continue allow spending legitimate money competition counterfeiter police lead realistic counterfeit money eventually counterfeiter duce perfect fake police not tell difference real fake money one complication analogy generator learns via discriminator real data fake data dataset generator random latent variable random index dataset discriminator discriminator figure training gans involves training generator network discriminator network process involves real data drawn dataset fake data created continuously generator throughout training process discriminator trained much like any classifier defined deep neural network shown left discriminator shown data training set case discriminator trained assign data real class shown right training process also involves fake data fake data constructed first sampling random vector z prior distribution latent variable model generator used produce sample x g z function g simply function represented neural network transforms random unstructured z vector structured data intended statistically indistinguishable training data discriminator classifies fake data discriminator trained assign data fake class backpropagation algorithm make possible use derivative discriminator output respect discriminator input train generator generator trained fool discriminator word make discriminator assign input real class training process discriminator thus much any binary classifier exception data fake class come distribution change constantly generator learns rather fixed distribution learning process generator somewhat unique not given specific target output rather simply given reward producing output fool constantly changing opponent gradient counterfeiter mole among police reporting specific method police use detect fake process illustrated figure figure 4 show cartoon giving some intution process work research highlight 142 communication acm november 2020 vol 63 no 11 demonstrated metz et argmin operation difficult work way popular approach regard situation game two player much game theory literature concerned game discrete finite action space convex loss property simplifying gans require use game theory setting not yet cost action policy continuous regardless whether consider action choosing specific parameter vector θ g whether consider action generating sample x goal machine learning algorithm context find local nash point local minimum player cost respect player parameter local move no player reduce cost assuming player parameter not change common training algorithm simply use optimizer repeatedly take simultaneous step player incrementally minimizing er cost respect player parameter end training process gans often able produce realistic sample even complicated ets containing image example shown figure high level one reason gan framework cesful may involves little approximation many approach generative modeling must approximate intractable density function gans not involve any situation not straightforward model mization problem player cost function player parameter player may control only parameter possible reduce tion optimization goal minimize x z x z c b x z x z figure illustration basic intuition behind gan training process illustrated fitting gaussian distribution example understand goal generator learning simple scaling inverse cumulative distribution function data generating distribution gans trained simultaneously updating discriminator function blue dashed line discriminates sample data generating distribution black dotted line px generative distribution pmodel green solid line lower horizontal line domain z sampled case uniformly horizontal line part domain upward arrow show mapping x g z imposes distribution pmodel transformed sample g contract region high density expands region low density pmodel consider pair adversarial network initialization pmodel initialized unit gaussian example defined randomly initialized deep neural network b suppose trained convergence g held fixed practice trained simultaneously purpose building intuition see g fixed would converge c suppose gradually train g sample x generated g flow direction increasing order arrive region likely classified data meanwhile estimate updated response update nash equilibrium neither player improve payoff pmodel pdata discriminator unable differentiate two distribution constant function show point equally likely come either distribution practice g typically optimized simultaneous gradient step not necessary optimal every step shown intuitive cartoon see ref fedus et nagarajan realistic discussion gan equilibration process figure image sample progressive depicting person doe not exist wa imagined gan training photo celebrity november 2020 vol 63 no 11 communication acm 143 spurious nash equilibrium whether learning rithm converges nash doe many case practical interest theoretical question open best learning algorithm seem empirically often fail converge theoretical work answer question ongoing work design ter cost model training algorithm better vergence property gan topic article focused summary core design siderations algorithmic property gans many topic potential interest not ered due space consideration article discussed using gans approximate distribution p x also extended conditional 25 erate sample corresponding some input drawing ples conditional distribution p x gans related moment optimal quirk gans made especially clear connection mmd optimal transport may used train generative model pmodel ha port only thin manifold may actually assign zero likelihood training data gans struggle generate discrete data algorithm need propagate gradient discriminator output generator problem gradually like generative model gans used fill gap missing gans proven effective learning classify data using labeled training evaluating performance generative model including gans difficult research area 31 32 33 gans seen way machine learning learn cost function rather minimizing cost function gans seen way supervising machine learning asking approximation true underlying task only real error statistical error sampling finite amount training data rather measuring true underlying distribution failure learning algorithm converge exactly optimal parameter many generative modeling strategy would introduce source error also source approximation error based markov chain optimization bound true cost rather cost etc difficult give much specific guidance ing detail gans gans active research area specific advice quickly becomes date figure 6 show quickly capability gans progressed year since introduction convergence gans central theoretical result presented original gan space density function pmodel tor function only one local nash rium pmodel pdata possible optimize directly sity function algorithm consists mizing convergence inner loop making small gradient step pmodel outer loop converges nash equilibrium however theoretical model local move directly density function space may not relevant gans trained practice using local move parameter space generator function among set function representable neural network finite number parameter parameter represented finite number bit many different theoretical model interesting study whether nash equilibrium whether any figure illustration progress gan capability course approximately three year following introduction gans gans rapidly become capable due change gan algorithm improvement underlying deep learning algorithm improvement underlying deep learning software hardware infrastructure rapid progress mean infeasible any single document summarize gan capability any specific set best practice continue evolve rapidly enough any comprehensive survey quickly becomes date figure reproduced permission brundage et individual result ref karras et liu radford et respectively 2014 2015 2016 2017 research highlight 144 communication acm november 2020 vol 63 no 11 produce any output machine learning algorithm recognizes acceptable rather asking produce specific example output gans thus great learning situation many possible correct answer predicting many possible future happen video gans el used learn transform data one domain data another domain even without any labeled pair example domain zhu et example studying collection photo zebra collection photo horse gans turn photo horse photo gans used ence simulate experiment would costly run even traditional software gans used create fake data train machine learning model either real data would hard would privacy concern associated real model called network used domain gans used variety interactive digital medium effect end goal produce compelling gans even used solve variational inference problem used approach generative gans learn useful embedding vector discover concept like gender human face without conclusion gans kind generative model based game theory great practical success term generating realistic data especially image currently still difficult train gans become reliable ogy necessary design model cost training algorithm possible find good nash ria consistently reference arjovsky chintala bottou wasserstein gan arxiv preprint 2017 arora ge liang zhang generalization equilibrium generative adversarial net gans arxiv preprint 2017 wu williams greene preserving generative deep neural network support clinical data sharing biorxiv 2017 159756 bengio alain yosinski deep generative stochastic network trainable backprop icml 2014 2014 brundage avin clark toner eckersley garfinkel dafoe scharre zeitzoff filar anderson roff allen steinhardt flynn héigeartaigh beard belfield farquhar lyle crootof evans page bryson yampolskiy amodei malicious use artificial intelligence forecasting prevention mitigation arxiv 2018 danihelka lakshminarayanan uria wierstra dayan comparison maximum likelihood training real nvps arxiv preprint 2017 de oliveira paganini nachman learning particle physic example generative adversarial network physic synthesis computing software big science 1 1 2017 4 deng dong socher li li imagenet hierarchical image database 2009 fedus goodfellow dai maskgan better text generation via filling international conference learning representation 2018 fedus rosca lakshminarayanan dai mohamed goodfellow many path equilibrium gans not need decrease divergence every step international conference learning representation 2018 frey graphical model machine learning digital communication mit press boston 1998 ganin lempitsky unsupervised domain adaptation backpropagation international conference machine learning 2015 goodfellow mirza xu ozair courville bengio generative adversarial net ghahramani welling cortes lawrence weinberger ed advance neural information processing system 27 curran associate boston 2014 karras aila laine lehtinen progressive growing gans improved quality stability variation corr 2017 kingma welling encoding variational bayes proceeding international conference learning representation iclr 2014 li swersky zemel generative moment matching network corr 2015 liu tuzel coupled generative adversarial network lee sugiyama luxburg guyon garnett ed advance neural information processing system 29 curran associate boston 2016 lucic kurach michalski gelly bousquet gans created equal study arxiv preprint 2017 mathieu couprie lecun deep video prediction beyond mean square error arxiv preprint 2015 mescheder nowozin geiger adversarial variational bayes unifying variational autoencoders generative adversarial network arxiv preprint 2017 mescheder nowozin geiger numerics gans advance neural information processing system 2017 metz poole pfau unrolled generative adversarial network arxiv preprint 2016 mirza osindero conditional generative adversarial net arxiv preprint 2014 nagarajan kolter gradient descent gan optimization locally stable guyon luxburg bengio wallach fergus vishwanathan garnett ed advance neural information processing system 30 curran associate boston 2017 odena olah shlens conditional image synthesis auxiliary classifier gans arxiv preprint 2016 oord li babuschkin simonyan vinyals kavukcuoglu driessche lockhart cobo stimberg et al parallel wavenet fast speech synthesis arxiv preprint 2017 radford metz chintala unsupervised representation learning deep convolutional generative adversarial network arxiv preprint 2015 ratliff burden sastry characterization computation local nash equilibrium continuous game communication control computing allerton 2013 annual allerton conference ieee 2013 salimans goodfellow zaremba cheung radford chen improved technique training gans advance neural information processing system 2016 shrivastava pfister tuzel susskind wang webb learning simulated unsupervised image adversarial training theis van den oord bethge note evaluation generative model nov 2015 unterthiner nessler klambauer heusel ramsauer hochreiter coulomb gans provably optimal nash equilibrium via potential field arxiv preprint 2017 wu burda salakhutdinov grosse quantitative analysis generative model arxiv preprint 2016 yeh chen lim johnson semantic image inpainting perceptual contextual loss arxiv preprint 2016 zhu park isola efros unpaired translation using adversarial network arxiv preprint 2017 ian goodfellow written google brain jean mehdi mirza bing xu david sherjil ozair aaron courville yoshua bengio université de montréal final submitted copyright held publication right licensed acm