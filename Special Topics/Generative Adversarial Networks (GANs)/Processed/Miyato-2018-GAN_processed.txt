published conference paper iclr 2018 spectral normalization generative adversarial network takeru toshiki masanori yuichi miyato kataoka yyoshida network university institute informatics abstract one challenge study generative adversarial network bility training paper propose novel weight normalization nique called spectral normalization stabilize training discriminator new normalization technique computationally light easy incorporate existing implementation tested efﬁcacy spectral normalization dataset experimentally conﬁrmed spectrally normalized gans capable generating image better equal quality relative previous training stabilization technique code chainer tokui et 2015 generated image pretrained el available projection 1 introduction generative adversarial network gans goodfellow et 2014 enjoying considerable success framework generative model recent year ha applied numerous type task datasets radford et 2016 salimans et 2016 ho ermon 2016 li et 2017 nutshell gans framework produce model distribution mimic given target distribution consists generator produce model distribution nator distinguishes model distribution target concept consecutively train model distribution discriminator turn goal reducing difference tween model distribution target distribution measured best discriminator possible step training gans drawing attention machine learning community not only ability learn highly structured probability distribution also theoretically interesting aspect example nowozin et 2016 uehara et 2016 mohamed minarayanan 2017 revealed training discriminator amount training good estimator density ratio model distribution target perspective open door method implicit model mohamed lakshminarayanan 2017 tran et 2017 used carry variational optimization without direct knowledge density function persisting challenge training gans performance control discriminator high dimensional space density ratio estimation discriminator often inaccurate unstable training generator network fail learn multimodal structure target distribution even worse support model distribution support target distribution disjoint exists discriminator perfectly distinguish model distribution target arjovsky bottou 2017 discriminator produced situation training generator come complete stop derivative discriminator respect input turn motivates u introduce some form restriction choice discriminator paper propose novel weight normalization method called spectral normalization stabilize training discriminator network normalization enjoys following favorable property 1 16 feb 2018 published conference paper iclr 2018 lipschitz constant only tuned algorithm doe not require intensive tuning only satisfactory performance implementation simple additional computational cost small fact normalization method also functioned well even without tuning lipschitz constant only hyper parameter study provide explanation effectiveness spectral normalization gans regularization technique weight tion salimans kingma 2016 weight clipping arjovsky et 2017 gradient penalty rajani et 2017 also show absence complimentary regularization technique batch normalization weight decay feature matching discriminator spectral malization improve sheer quality generated image better weight normalization gradient penalty 2 method section lay theoretical groundwork proposed method let u consider simple discriminator made neural network following form input x f x θ w w l w w 1 θ w 1 w l w learning parameter set w l w al activation function omit bias term layer simplicity ﬁnal output discriminator given x θ f x θ 2 activation function corresponding divergence distance measure user choice standard formulation gans given min g max v g min max g taken set generator discriminator tions respectively conventional form v g goodfellow et 2014 given log x log 1 qdata data distribution pg model generator distribution learned adversarial optimization tivation function used expression some continuous function range 0 1 sigmoid function known ﬁxed generator g optimal discriminator form v g given g x qdata x qdata x pg x machine learning community ha pointing recently function space discriminator selected crucially affect performance gans number work hara et 2016 qi 2017 gulrajani et 2017 advocate importance lipschitz continuity assuring boundedness statistic example optimal discriminator gans standard formulation take form g x qdata x qdata x pg x sigmoid f x f x log qdata x pg x 3 derivative x 1 qdata x x 1 pg x x 4 unbounded even incomputable prompt u introduce some regularity condition derivative f x particularly successful work array qi 2017 arjovsky et 2017 gulrajani et 2017 proposed method control lipschitz constant discriminator adding regularization term deﬁned input example would follow footstep search discriminator set continuous function arg max v g 5 2 published conference paper iclr 2018 mean smallest value x any x norm norm input based regularization allow relatively easy formulation based sample also suffer fact not impose regularization space outside support generator data distribution without introducing somewhat heuristic mean method would introduce paper called spectral normalization method aim skirt issue normalizing weight matrix using technique devised yoshida miyato 2017 spectral normalization spectral normalization control lipschitz constant discriminator function f literally constraining spectral norm layer g hin deﬁnition lipschitz norm equal suph σ h σ spectral norm matrix matrix norm σ max h max 6 equivalent largest singular value therefore linear layer g h wh norm given suph σ h suph σ w σ w lipschitz norm activation function equal 1 1 use inequality observe following bound hl σ w l 7 spectral normalization normalizes spectral norm weight matrix w satisﬁes lipschitz constraint σ w 1 wsn w w 8 normalize w l using 8 appeal inequality 7 fact σ wsn w 1 see bounded would like emphasize difference spectral normalization spectral norm regularization introduced yoshida miyato 2017 unlike method spectral norm regularization penalizes spectral norm adding explicit regularization term objective function method fundamentally different method not make attempt set spectral norm designated value moreover reorganize derivative normalized cost function rewrite objective function 12 see method augmenting cost function sample data dependent regularization function spectral norm regularization hand imposes sample data independent regularization cost function like regularization lasso fast approximation spectral norm σ w mentioned spectral norm σ w use regularize layer criminator largest singular value naively apply singular value decomposition compute σ w round algorithm algorithm become computationally heavy instead use power iteration method estimate σ w golub van der vorst 2000 yoshida miyato 2017 power iteration method estimate spectral norm small additional computational time relative full computational cost vanilla gans please see appendix detail method algorithm 1 summary actual spectral normalization algorithm example relu jarrett et 2009 nair hinton 2010 glorot et 2011 leaky relu maas et 2013 satisﬁes condition many popular activation function satisfy constraint some predeﬁned k well 3 published conference paper iclr 2018 gradient analysis spectrally normalized weight wsn w respect wij wsn w 1 σ w eij 1 σ w 2 w w 1 σ w eij 1 ij σ w 2 w 9 1 σ w 1 ij wsn 10 eij matrix whose j entry 1 zero everywhere else respectively ﬁrst left right singular vector h hidden layer network transformed wsn derivative v g calculated respect w discriminator given g 1 σ w e e δt wsnh 1 11 1 σ w e 1 12 δ g wsnh λ ˆ e δt wsnh ˆ e represents empirical tation 0 ˆ e δht 1 some k would like comment implication 12 ﬁrst term ˆ e derivative weight without normalization light second term expression seen regularization term penalizing ﬁrst singular component adaptive regularization coefﬁcient λ positive δ wsnh pointing similar direction prevents column space w concentrating one particular direction course training word spectral normalization prevents transformation layer becoming sensitive one direction also use spectral normalization devise new parametrization model namely split layer map two separate able component spectrally normalized map spectral norm constant turn parametrization ha merit promotes performance gans see appendix e 3 spectral normalization v regularization technique weight normalization introduced salimans kingma 2016 method normalizes norm row vector weight matrix mathematically equivalent requiring weight weight normalization wwn wwn 2 wwn 2 σt wwn 2 min di 13 σt singular value matrix therefore scaler frobenius normalization requires sum squared singular value normalization however inadvertently impose much stronger constraint matrix intended wwn weight normalized matrix dimension di norm ﬁxed unit vector h maximized wwn σt wwn 0 2 mean wwn rank one similar thing said frobenius normalization see appendix detail using wwn corresponds using only one feature discriminate model probability distribution target order retain much norm input possible hence make discriminator sensitive one would hope make norm wwnh large weight normalization however come cost reducing rank hence number feature used discriminator thus conﬂict interest weight normalization desire use many feature possible distinguish generator distribution target distribution former interest often reign many case inadvertently diminishing number feature used discriminator consequently algorithm would produce rather arbitrary model distribution spectrum ha multiplicity would looking subgradients however probability happening zero almost surely would continue discussion without giving erations event 4 published conference paper iclr 2018 match target distribution only select feature weight clipping arjovsky et 2017 also suffers pitfall spectral normalization hand not suffer conﬂict interest note lipschitz constant linear operator determined only maximum singular value word spectral norm independent rank thus unlike weight normalization spectral normalization allows parameter matrix use many feature possible satisfying local constraint spectral normalization leaf freedom choosing number singular component feature feed next layer discriminator brock et al 2016 introduced orthonormal regularization weight stabilize training gans work brock et al 2016 augmented adversarial objective function adding following term tw f 14 seems serve purpose spectral normalization orthonormal regularization mathematically quite different spectral normalization orthonormal ization destroys information spectrum setting singular value one hand spectral normalization only scale spectrum maximum one gulrajani et al 2017 used gradient penalty method combination wgan work placed constant discriminator augmenting objective function regularizer reward function local constant 1 discrete set point form ˆ x x 1 x generated interpolating sample x generative distribution sample x data distribution rather forward approach doe not suffer problem mentioned regarding effective dimension feature space approach ha obvious weakness heavily dependent support current generative distribution matter course generative tion support gradually change course training destabilize effect regularization fact empirically observed high learning rate destabilize performance contrary spectral normalization regularizes function operator space effect regularization stable respect choice batch training spectral normalization doe not easily destabilize aggressive learning rate moreover requires computational cost spectral normalization power iteration computation requires one whole round ward backward propagation appendix section compare computational cost two method number update 4 experiment order evaluate efﬁcacy approach investigate reason behind efﬁcacy conducted set extensive experiment unsupervised image generation torralba et 2008 coates et 2011 compared method normalization technique see method fare large dataset also applied method dataset imagenet russakovsky et 2015 well section structured follows first discus objective function used train architecture describe optimization setting used experiment explain two performance measure image evaluate image produced trained generator finally summarize result imagenet architecture discriminator generator used convolutional neural network also evaluation spectral norm convolutional weight w treated operator matrix dimension dout dinhw trained parameter generator batch normalization ioffe szegedy 2015 refer reader table 3 appendix section detail architecture since conducting convolution discretely spectral norm depend size stride padding however answer only differ some predeﬁned 5 published conference paper iclr 2018 method used following standard objective function adversarial loss v g e x log x e z log 1 g z 15 z latent variable p z standard normal distribution n 0 g rdz deterministic generator function set dz 128 experiment update g used alternate cost proposed goodfellow et al 2014 z log g z used goodfellow et al 2014 bengio 2017 update used original cost deﬁned 15 also tested performance algorithm hinge loss given vd ˆ g e x min 0 x e z h min 0 ˆ g z 16 vg g ˆ e z h ˆ g z 17 respectively discriminator generator optimizing objective equivalent minimizing reverse kl divergence kl type loss ha already proposed used lim ye 2017 tran et al 2017 algorithm based hinge loss also showed good performance evaluated inception score fid wasserstein gans gradient penalty gulrajani et 2017 used following objective function v g x z g z eˆ x xd ˆ x 2 regularization term one introduced appendix section quantitative assessment generated example used inception score salimans et 2016 echet inception distance fid heusel et 2017 please see appendix detail score result section report accuracy spectral normalization use abbreviation gan spectrally normalized gans training dependence rithm performance hyperparmeters optimizer also compare performance quality algorithm technique criminator network including weight clipping arjovsky et 2017 gulrajani et 2017 bn ioffe szegedy 2015 layer normalization ln ba et 2016 weight normalization wn salimans kingma 2016 orthonormal regularization thonormal brock et 2016 order evaluate efﬁcacy gradient penalty also applied gradient penalty term standard adversarial loss gans 15 would refer method weight clipping followed original work arjovsky et al 2017 set clipping constant c convolutional weight layer gradient penalty set λ 10 suggested gulrajani et al 2017 orthonormal ized weight randomly selected orthonormal operator trained gans objective function augmented regularization term used brock et al 2016 comparative study throughout excluded multiplier parameter γ weight normalization method well batch normalization layer normalization method wa done order prevent method overtly violating lipschitz condition experimented different multiplier parameter fact not able achieve any improvement optimization used adam optimizer kingma ba 2015 experiment tested 6 setting 1 ndis number update discriminator per one update generator 2 learning rate α ﬁrst second order momentum parameter adam list detail setting table 1 appendix section 6 setting b c setting used previous representative work purpose setting e f evaluate performance algorithm implemented aggressive learning rate detail architecture convolutional network deployed generator discriminator refer reader table 3 appendix section number update gan generator experiment unless otherwise noted firstly inspected spectral norm layer training make sure spectral normalization procedure indeed serving purpose see figure 9 6 published conference paper iclr 2018 table 1 setting tested experiment hyperparameter setting following gulrajani et al 2017 bengio 2017 radford et al 2016 respectively setting α ndis 5 1 1 5 e 5 f 5 weight clip bn ln wn orthonormal sn 0 1 2 3 4 5 6 7 8 inception score b c e f weight clip ln wn orthonormal sn 0 1 2 3 4 5 6 7 8 9 inception score b c e f b figure 1 inception score different method hyperparameters higher better spectral norm layer ﬂoats around region throughout training please see appendix detail figure 1 2 show inception score method setting see spectral normalization relatively robust aggressive learning rate momentum eters fails train good gans high learning rate high momentum parameter orthonormal regularization performed poorly setting e performed slightly better method optimal setting result suggests method robust method respect change ting training also optimal performance weight normalization wa inferior spectral normalization consists diverse example best score spectral normalization better almost method table 2 show inception score different method optimal setting 10 dataset see performed better almost contemporary optimal setting performed even better hinge loss 17 training number iteration fell behind orthonormal regularization detailed comparison orthonormal regularization spectral normalization please see section figure 6 show image produced generator trained weight malization spectral normalization consistently better gans weight normalization term quality generated image precise mentioned section 3 set image generated spectral normalization wa clearer diverse image produced weight normalization also see failed train good gans high learning rate high momentum e f generated image also ran twice time longer iteration not seem converge yet still elongated training sequence still completes original iteration size optimal setting setting b ndis 1 computationally light 7 published conference paper iclr 2018 weight clip bn ln wn orthonormal sn 10 2 fid b c e f weight clip ln wn orthonormal sn 10 1 10 2 fid b c e f b figure 2 fids different method hyperparameters lower better table 2 inception score fids unsupervised image generation radford et 2016 experimented yang et al 2017 yang et 2017 bengio 2017 gulrajani et 2017 method inception score fid real data weight clipping batch norm layer norm weight norm orthonormal orthonormal update update eq 17 eq 17 update orthonormal eq 17 eq 17 et resnet batch normalization layer normalization shown figure 12 appendix section also compared algorithm multiple benchmark method summarized result bottom half table also tested performance method resnet based gans used gulrajani et al 2017 please note method listed thereof different optimization method architecture model please see table 4 5 appendix section detail network architecture implementation algorithm wa able perform better almost predecessor performance resnet experiment trained architecture multiple random seed weight initialization produced model different parameter generated 5000 image 10 time computed average inception score model value resnet table mean standard deviation score computed set model trained different seed 8 published conference paper iclr 2018 0 13 26 index 2 layer 1 0 32 63 0 0 0 2 0 4 0 6 0 8 1 0 layer 2 0 64 127 0 0 0 2 0 4 0 6 0 8 1 0 layer 3 0 64 127 0 0 0 2 0 4 0 6 0 8 1 0 layer 4 0 128 255 0 0 0 2 0 4 0 6 0 8 1 0 layer 5 0 128 255 0 0 0 2 0 4 0 6 0 8 1 0 layer 6 0 256 511 0 0 0 2 0 4 0 6 0 8 1 0 layer 7 wc wn sn 0 13 26 index 2 layer 1 0 32 63 0 0 0 2 0 4 0 6 0 8 1 0 layer 2 0 64 127 0 0 0 2 0 4 0 6 0 8 1 0 layer 3 0 64 127 0 0 0 2 0 4 0 6 0 8 1 0 layer 4 0 128 255 0 0 0 2 0 4 0 6 0 8 1 0 layer 5 0 128 255 0 0 0 2 0 4 0 6 0 8 1 0 layer 6 0 256 511 0 0 0 2 0 4 0 6 0 8 1 0 layer 7 wc wn sn b figure 3 squared singular value weight matrix trained different method weight ping wc weight normalization wn spectral normalization sn scaled singular value largest singular value equal wn sn calculated singular value normalized weight matrix analysis singular value analysis weight discriminator figure 3 show squared singular value weight matrix ﬁnal discriminator produced method using parameter yielded best inception score predicted section 3 singular value ﬁrst ﬁfth layer trained weight clipping weight normalization concentrate component weight matrix layer tend rank deﬁcit hand singular value weight matrix layer trained spectral ization broadly distributed goal distinguish pair probability distribution nonlinear data manifold embedded high dimensional space rank ciencies lower layer especially fatal output lower layer gone only set rectiﬁed linear transformation mean tend lie space linear part marginalizing many feature input distribution space result oversimpliﬁed discriminator actually conﬁrm effect phenomenon generated image especially figure image generated spectral normalization diverse complex generated weight normalization training time slightly slower weight normalization 110 computational time signiﬁcantly faster mentioned tion 3 slower method need calculate gradient gradient norm computational time almost vanilla gans relative computational cost power iteration 18 negligible compared cost forward backward propagation image size larger 48 48 please see figure 10 appendix section actual computational time comparison orthonormal regularization order highlight difference spectral normalization orthonormal tion conducted additional set experiment explained section 3 orthonormal regularization different method destroys spectral information put equal emphasis feature dimension including one shall weeded training process see extent possibly detrimental effect experimented increasing 9 published conference paper iclr 2018 relative size feature map dimension inception score orthonormal figure 4 effect performance induced change feature map dimension ﬁnal layer width highlighted region represents standard deviation result multiple seed weight initialization orthonormal regularization doe not perform well large feature map dimension possibly design force discriminator use dimension including one unnecessary setting optimizers used setting c wa optimal orthonormal regularization iteration 10 12 14 16 18 20 22 inception score orthnormal figure 5 learning curve conditional image generation term inception score gans gans orthonormal regularization imagenet mension feature space 6 especially ﬁnal layer conv training spectral normalization prefers relatively small feature space dimension 100 see figure setting training selected parameter orthonormal regularization performed optimally ﬁgure 4 show result experiment predicted formance orthonormal regularization deteriorates increase dimension feature map ﬁnal layer hand doe not falter modiﬁcation architecture thus least perspective may method robust respect change network architecture image generation imagenet show method remains effective large high dimensional dataset also applied method training conditional gans dataset 1000 class consisting approximately 1300 image compressed pixel regarding adversarial loss conditional gans used practically formulation used mirza osindero 2014 except replaced standard gans loss hinge loss 17 please see appendix detail experimental setting precisely simply increased input dimension output dimension factor figure 4 relative size implies layer structure original 10 published conference paper iclr 2018 gans without normalization gans layer normalization collapsed beginning training failed produce any meaningful image gans orthonormal normalization brock et al 2016 spectral normalization hand wa able produce image inception score orthonormal normalization however plateaued around iteration sn kept improving even afterward figure 5 knowledge research ﬁrst kind succeeding produce decent image imagenet dataset single pair tor generator figure 7 measure degree followed footstep odena et al 2017 computed intra odena et al 2017 pair dently generated gans image class see intra suffering le intra ensure superiority method not limited within speciﬁc setting also pared performance orthonormal regularization conditional gans projection discriminator miyato koyama 2018 well standard unconditional gans experiment achieved better performance orthonormal regularization setting see figure 13 appendix section 5 conclusion paper proposes spectral normalization stabilizer training gans apply tral normalization gans image generation task generated example diverse conventional weight normalization achieve better comparative inception score tive previous study method imposes global regularization discriminator opposed local regularization introduced possibly used combination future work would like investigate method stand amongst method theoretical basis experiment algorithm larger complex datasets acknowledgment would like thank member preferred network particularly maeda eiichi matsumoto masaki watanabe keisuke yahata insightful comment discussion also would like thank anonymous reviewer commenters openreview forum insightful discussion reference martin arjovsky eon bottou towards principled method training generative adversarial network iclr martin arjovsky soumith chintala eon bottou wasserstein generative adversarial network icml pp devansh arpit yingbo zhou bhargava u kota venu govindaraju normalization propagation metric technique removing internal covariate shift deep network icml pp jimmy lei ba jamie ryan kiros geoffrey e hinton layer normalization arxiv preprint andrew brock theodore lim james ritchie nick weston neural photo editing introspective adversarial network arxiv preprint adam coates andrew ng honglak lee analysis network unsupervised feature learning aistats pp harm de vries florian strub emie mary hugo larochelle olivier pietquin aaron c courville ulating early visual processing language nip pp dc dowson bv landau echet distance multivariate normal distribution journal multivariate analysis 12 3 vincent dumoulin jonathon shlens manjunath kudlur learned representation artistic style iclr xavier glorot antoine bordes yoshua bengio deep sparse rectiﬁer neural network aistats pp gene h golub henk van der vorst eigenvalue computation century journal tional applied mathematics 123 1 ian goodfellow jean mehdi mirza bing xu david sherjil ozair aaron courville yoshua bengio generative adversarial net nip pp 2014 11 published conference paper iclr 2018 ishaan gulrajani faruk ahmed martin arjovsky vincent dumoulin aaron courville improved training wasserstein gans arxiv preprint kaiming xiangyu zhang shaoqing ren jian sun deep residual learning image recognition cvpr pp martin heusel hubert ramsauer thomas unterthiner bernhard nessler unter klambauer sepp hochreiter gans trained two update rule converge nash equilibrium arxiv preprint jonathan ho stefano ermon generative adversarial imitation learning nip pp sergey ioffe christian szegedy batch normalization accelerating deep network training reducing internal covariate shift icml pp kevin jarrett koray kavukcuoglu marc aurelio ranzato yann lecun best architecture object recognition iccv pp kui jia dacheng tao shenghua gao xiangmin xu improving training deep neural network via singular value bounding cvpr diederik kingma jimmy ba adam method stochastic optimization iclr jiwei li monroe tianlin shi alan ritter dan jurafsky adversarial learning neural dialogue generation emnlp pp jae hyun lim jong chul ye geometric gan arxiv preprint andrew l maas awni hannun andrew ng rectiﬁer nonlinearities improve neural network acoustic model icml workshop deep learning audio speech language processing mehdi mirza simon osindero conditional generative adversarial net arxiv preprint takeru miyato masanori koyama cgans projection discriminator iclr shakir mohamed balaji lakshminarayanan learning implicit generative model nip workshop adversarial training vinod nair geoffrey e hinton rectiﬁed linear unit improve restricted boltzmann machine icml pp sebastian nowozin botond cseke ryota tomioka training generative neural sampler using variational divergence minimization nip pp augustus odena christopher olah jonathon shlens conditional image synthesis auxiliary classiﬁer gans icml pp qi generative adversarial network lipschitz density arxiv preprint alec radford luke metz soumith chintala unsupervised representation learning deep convolutional generative adversarial network iclr olga russakovsky jia deng hao su jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg li imagenet large scale visual recognition challenge international journal computer vision 115 3 masaki saito eiichi matsumoto shunta saito temporal generative adversarial net singular value clipping iccv tim salimans diederik p kingma weight normalization simple reparameterization accelerate ing deep neural network nip pp tim salimans ian goodfellow wojciech zaremba vicki cheung alec radford xi chen improved technique training gans nip pp christian szegedy wei liu yangqing jia pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich going deeper convolution cvpr pp seiya tokui kenta oono shohei hido justin clayton chainer open source framework deep learning proceeding workshop machine learning system learningsys ninth annual conference neural information processing system nip antonio torralba rob fergus william freeman 80 million tiny image large data set metric object scene recognition ieee transaction pattern analysis machine intelligence 30 11 dustin tran rajesh ranganath david blei deep hierarchical implicit model arxiv preprint masatoshi uehara issei sato masahiro suzuki kotaro nakayama yutaka matsuo generative adversarial net density ratio estimation perspective nip workshop adversarial training david yoshua bengio improving generative adversarial network denoising feature matching iclr sitao xiang hao li effect batch normalization weight normalization generative adversarial network arxiv preprint jianwei yang anitha kannan dhruv batra devi parikh layered recursive generative sarial network image generation iclr yuichi yoshida takeru miyato spectral norm regularization improving generalizability deep learning arxiv preprint 2017 12 published conference paper iclr 2018 b figure 6 generated image different method weight normalization spectral normalization 13 published conference paper iclr 2018 figure 7 pixel image generated trained dataset inception score 14 published conference paper iclr 2018 algorithm spectral normalization let u describe shortcut section detail begin vector u randomly initialized weight no multiplicity dominant singular value u not orthogonal ﬁrst left singular appeal principle power method produce ﬁrst left right singular vector following update rule v u 18 approximate spectral norm w pair singular vector σ w utw 19 use sgd updating w change w update would small hence change largest singular value implementation took advantage fact reused u computed step algorithm initial vector subsequent step fact recycle procedure one round power iteration wa sufﬁcient actual experiment achieve satisfactory performance algorithm 1 appendix summarizes computation spectrally normalized weight matrix w approximation note procedure computationally cheap even comparison calculation forward backward propagation neural network please see figure 10 actual computational time without spectral normalization algorithm 1 sgd spectral normalization initialize ul l 1 l random vector sampled isotropic bution update layer l apply power iteration method unnormalized weight w l vl w l w l 20 ul 21 calculate wsn spectral norm w l sn w l w w l σ w l ut l w vl 22 update w l sgd dataset dm learning rate α w l l lℓ w l sn w l dm 23 b experimental setting performance measure inception score introduced originally salimans et al 2016 xn n exp e dkl p p approximated 1 n pn p p trained inception convolutional neural network szegedy et 2015 would refer ception model short work salimans et al 2016 reported score strongly correlated subjective human judgment image quality following procedure salimans et al 2016 bengio 2017 calculated score randomly generated 5000 example trained generator evaluate ability generate natural image repeated experiment 10 time reported average standard deviation inception score echet inception distance heusel et 2017 another measure quality generated example us order information ﬁnal layer inception model applied practice safe assume u generated uniform distribution sphere not orthogonal ﬁrst singular vector happen probability 0 15 published conference paper iclr 2018 example chet distance dowson landau 1982 distance two distribution assuming multivariate gaussian distribution f 2 trace 24 mean covariance sample q p respectively output ﬁnal layer inception model softmax echet inception distance fid two distribution image distance computed echet inception distance true distribution generated distribution empirically 10000 5000 sample multiple repetition experiment not exhibit any notable variation score image generation comparative study experimented recent resnet architecture gulrajani et al 2017 well standard cnn additional set experiment used adam optimization used hyper parameter used gulrajani et al 2017 α 0 ndis 5 doubled feature map generator original modiﬁcation achieved better result note doubled dimension feature map experiment however mance deteriorated image generation imagenet image used set experiment resized 128 128 pixel detail architecture given table generator network conditional gans used ditional batch normalization cbn dumoulin et 2017 de vries et 2017 namely replaced standard batch normalization layer cbn conditional label information 1 1000 optimization used adam hyperparameters used resnet dataset trained network generator update applied linear decay learning rate iteration rate would 0 end network architecture table 3 standard cnn model used experiment image generation slope lrelu function network set z 0 dense mg 512 deconv bn 256 relu deconv bn 128 relu deconv bn 64 relu conv 3 tanh generator mg 4 svhn mg 6 rgb image x conv 64 lrelu conv 64 lrelu conv 128 lrelu conv 128 lrelu conv 256 lrelu conv 256 lrelu conv 512 lrelu dense b discriminator 32 svhn 48 16 published conference paper iclr 2018 bn relu conv bn relu conv figure 8 block architecture tor removed bn layer resblock table 4 resnet architecture dataset use similar tectures one used gulrajani et al 2017 z 0 dense 4 4 256 resblock 256 resblock 256 resblock 256 bn relu conv 3 tanh generator rgb image x resblock 128 resblock 128 resblock 128 resblock 128 relu global sum pooling dense b discriminator table 5 resnet architecture dataset z 0 dense 6 6 512 resblock 256 resblock 128 resblock 64 bn relu conv 3 tanh generator rgb image x resblock 64 resblock 128 resblock 256 resblock 512 resblock 1024 relu global sum pooling dense b discriminator 17 published conference paper iclr 2018 table 6 resnet architecture image generation imagenet dataset generator tional gans replaced usual batch normalization layer resblock conditional batch normalization layer model projection discriminator used architecture used miyato koyama 2018 please see paper detail z 0 dense 4 4 1024 resblock 1024 resblock 512 resblock 256 resblock 128 resblock 64 bn relu conv 3 tanh generator rgb image x resblock 64 resblock 128 resblock 256 resblock 512 resblock 1024 resblock 1024 relu global sum pooling dense b discriminator tional gans rgb image x resblock 64 resblock 128 resblock 256 concat embed h resblock 512 resblock 1024 resblock 1024 relu global sum pooling dense c discriminator conditional gans computational ease embedded integer label 0 1000 128 dimension concatenating vector output termediate layer 18 published conference paper iclr 2018 c appendix result accuracy spectral normalization figure 9 show spectral norm layer discriminator course training setting optimizer c table 1 throughout training fact not deviate part exception 6 convolutional layer largest rank deviate beginning training norm layer stabilizes around 1 some iteration 0 20000 40000 60000 80000100000 update σ w figure 9 spectral norm seven convolutional layer standard cnn course training cifar training time wn sn vanilla 0 5 10 15 20 25 30 35 40 45 second 100 generator update image 32 3 wn sn vanilla 0 20 40 60 80 100 second 100 generator update b image 48 3 figure 10 computational time 100 update set ndis 5 effect ndis spectral normalization weight normalization figure 11 show effect ndis performance weight normalization spectral ization result shown figure 11 follows setting except value ndis wn performance deteriorates larger ndis amount computing minimax better accuracy sn doe not suffer unintended effect 19 published conference paper iclr 2018 12 5 10 20 ndis 2 3 4 5 6 7 inception score inception score 10000 generator update sn wn figure 11 effect ndis spectral normalization weight normalization shaded region represents variance result different seed generated image layer normalization batch normalization figure 12 generated image layer norm batch norm 20 published conference paper iclr 2018 image generation imagenet iteration 10 11 12 13 14 15 16 inception score orthnormal unconditional gans iteration 10 15 20 25 30 inception score orthnormal b conditional gans projection discriminator figure 13 learning curve term inception score gans orthonormal regularization imagenet ﬁgure show result standard unconditional gans ﬁgure b show result conditional gans trained projection tor miyato koyama 2018 spectral normalization v regularization technique section dedicated comparative study spectral normalization regularization method discriminator particular show contemporary regularization including weight normalization weight clipping implicitly impose constraint weight matrix place unnecessary restriction search space discriminator speciﬁcally show weight normalization weight clipping unwittingly favor weight matrix force trained discriminator largely dependent select feature rendering algorithm able match model distribution target distribution only low dimensional feature space weight normalization frobenius normalization weight normalization introduced salimans kingma 2016 method normalizes norm row vector weight wwn wt 1 wt 2 wt wi wi 25 wi wi ith row vector wwn w respectively still another technique regularize weight matrix use frobenius norm wfn 26 p tr w tw qp j ij originally regularization technique invented goal improving tion performance supervised training salimans kingma 2016 arpit et 2016 however recent work ﬁeld gans salimans et 2016 xiang li 2017 found another raison etat regularizer discriminator succeeded improving performance original original literature weight normalization wa introduced method reparametrization form wwn wt 1 wt 2 γdo wt γi learned course training work deal case γi 1 ass method lipschitz constraint 21 published conference paper iclr 2018 method fact render trained discriminator some scribed k achieve desired effect certain extent however weight normalization 25 imposes following implicit restriction choice wwn wwn 2 wwn 2 σt wwn 2 min di 27 σt singular value matrix equation hold pmin di σt wwn 2 tr wwn w wn pdo wi wt restriction norm ﬁxed unit vector h maximized wwn σt wwn 0 2 mean wwn rank one using w corresponds using only one feature discriminate model probability distribution target similarly frobenius normalization requires wfn wfn wfn 2 1 argument follows see critical problem two regularization method order retain much norm input possible hence make discriminator sensitive one would hope make norm wwnh large weight normalization however come cost reducing rank hence number feature used discriminator thus conﬂict interest weight normalization desire use many feature possible distinguish generator distribution target distribution former interest often reign many case inadvertently diminishing number feature used discriminator consequently algorithm would produce rather arbitrary model distribution match target distribution only select feature spectral normalization hand not suffer conﬂict interest note lipschitz constant linear operator determined only maximum singular value word spectral norm independent rank thus unlike weight normalization spectral normalization allows parameter matrix use many feature possible satisfying local constraint spectral normalization leaf freedom choosing number singular component feature feed next layer discriminator see visually refer reader figure 14 note spectral normalization allows wider range choice weight normalization 0 25 49 index sn wn figure 14 visualization difference spectral normalization red weight normalization blue possible set singular value possible set singular value plotted increasing order weight normalization blue spectral normalization red set singular value permitted spectral normalization condition scaled wwn spectral norm exactly deﬁnition weight normalization area blue curve bound note range choice weight normalization small summary weight normalization frobenius normalization favor skewed distribution lar value making column space weight matrix lie approximately low dimensional vector space hand spectral normalization doe not compromise number feature dimension used discriminator fact experimentally show gans 22 published conference paper iclr 2018 trained spectral normalization generate synthetic dataset wider variety higher inception score gans trained two regularization method weight clipping still another regularization technique weight clipping introduced arjovsky et al 2017 training wasserstein gans weight clipping simply truncates element weight matrix absolute value bounded prescribed constant c unfortunately weight clipping suffers problem weight normalization frobenius normalization weight clipping truncation value c value ﬁxed unit vector x maximized rank w one training favor discriminator use only select feature gulrajani et al 2017 refers problem capacity underuse problem also reported training wgan weight clipping slower original dcgan radford et 2016 singular value clipping singular value constraint one direct straightforward way controlling spectral norm clip singular ues saito et 2017 jia et 2017 approach however computationally heavy one need implement singular value decomposition order compute singular value similar le obvious approach parametrize w follows train discriminator constrained parametrization w usv subject u tu v tv max sii k 28 u v diagonal matrix however not simple task train model remaining absolutely faithful parametrization constraint spectral normalization hand carry update relatively low tional cost without compromising normalization constraint wgan gradient penalty recently gulrajani et al 2017 introduced technique enhance stability training wasserstein gans arjovsky et 2017 work endeavored place constraint 5 discriminator augmenting adversarial loss function following regularizer function λ e ˆ x xd ˆ x 2 29 λ 0 balancing coefﬁcient ˆ x ˆ x ϵx 1 x 30 ϵ 0 1 x x g z z 31 using augmented objective function gulrajani et al 2017 succeeded training gan based resnet et 2016 impressive performance advantage method comparison spectral normalization impose local constraint directly discriminator function without rather normalization suggest method le likely underuse capacity network structure time type method penalizes gradient sample point ˆ x suffers obvious problem not able regularize function point outside support current generative distribution fact generative distribution support gradually change course training destabilize effect regularization contrary spectral normalization regularizes function effect regularization stable respect choice batch fact observed experiment high learning rate destabilize performance training spectral normalization doe not falter aggressive learning rate 23 published conference paper iclr 2018 moreover requires computational cost spectral normalization power iteration computation requires one whole round forward backward propagation figure 10 compare computational cost two method number update said one shall not rule possibility gradient penalty compliment spectral normalization vice versa two method regularizes discriminator completely different mean experiment section actually conﬁrmed tion reparametrization spectral normalization improves quality generated example baseline only e reparametrization motivated spectral normalization take advantage regularization effect spectral normalization saw develop another algorithm let u consider another parametrization weight matrix discriminator given w γ wsn 32 γ scalar variable learned parametrization compromise straint layer interest give freedom model keeping model becoming degenerate reparametrization need control lipschitz condition mean gradient penalty gulrajani et 2017 indeed think analogous version reparametrization replacing wsn 32 w normalized criterion extension form not new salimans kingma 2016 originally introduced weight normalization order derive reparametrization form 32 wsn replaced 32 wwn vectorized experiment comparison reparametrization different normalization method part addendum experimentally compare reparametrizations derived two different normalization method weight normalization spectral normalization tested reprametrization method training discriminator architecture network used cnn used previous section cnn used architecture provided gulrajani et 2017 table 7 8 summarize result see method signiﬁcantly improves inception score baseline regular cnn slightly improves score resnet based cnn figure 15 show learning curve critic loss train validation set b ception score different reparametrization method see beneﬁcial effect spectral normalization learning curve discriminator well verify ﬁgure discriminator spectral normalization overﬁts le training dataset criminator without reparametrization weight normalization effect overﬁtting observed inception score well ﬁnal score spectral normalization better others best inception score achieved course training spectral normalization achieved whereas spectral normalization vanilla normalization achieved respectively f gradient general normalization method let u denote w w normalized weight n w scalar normalized coefﬁcient spectral norm frobenius norm general write derivative loss implement method based code provided author jani et 2017 24 published conference paper iclr 2018 method inception score fid standard cnn baseline frobenius norm weight norm spectral norm resnet gulrajani et al 2017 resnet baseline spectral norm spectral norm feature map table 7 inception score different reparametrization mehtods without label pervisions reported inception score fid frobenius normalization training collapsed early stage method resnet inception score fid gulrajani et al 2017 baseline spectral norm spectral norm feature map table 8 inception score fids different reparametrization method label supervision auxiliary classiﬁer odena et 2017 respect unnormalized weight w follows g w 1 n w w w w n w 33 1 n w w v w v w n 34 α w v n 35 α w λ trace w v w gradient w v calculated ˆ e δ g wh h hidden node network transformed w ˆ e represents empirical expectation n w derivative g w 1 ˆ e ˆ e w w 36 n w σ w g w 1 σ w ˆ e ˆ e w 1 37 notice least case n w n w point gradient given w v 38 25 published conference paper iclr 2018 0 20000 40000 60000 80000 100000 update generator critic loss train valid wn train wn valid sn train sn valid critic loss 0 20000 40000 60000 80000 100000 update generator inception score wn sn b inception score figure 15 learning curve critic loss b inception score different reparametrization method weight normalization wn spectral normalization gp sn parametrization free 26