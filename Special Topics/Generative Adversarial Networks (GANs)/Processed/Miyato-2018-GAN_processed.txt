publish confer paper iclr spectral normal gener adversari network takeru toshiki masanori yuichi miyato kataoka yyoshida network univers institut informat abstract one challeng studi gener adversari network biliti train thi paper propos novel weight normal niqu call spectral normal stabil train discrimin new normal techniqu comput light easi incorpor exist implement test efﬁcaci spectral normal dataset experiment conﬁrm spectral normal gan capabl gener imag better equal qualiti rel previou train stabil techniqu code chainer tokui et gener imag pretrain el avail http project introduct gener adversari network gan goodfellow et enjoy consider success framework gener model recent year ha appli numer type task dataset radford et saliman et ho ermon li et nutshel gan framework produc model distribut mimic given target distribut consist gener produc model distribut nator distinguish model distribut target concept consecut train model distribut discrimin turn goal reduc differ tween model distribut target distribut measur best discrimin possibl step train gan draw attent machin learn commun onli abil learn highli structur probabl distribut also theoret interest aspect exampl nowozin et uehara et moham minarayanan reveal train discrimin amount train good estim densiti ratio model distribut target thi perspect open door method implicit model moham lakshminarayanan tran et use carri variat optim without direct knowledg densiti function persist challeng train gan perform control discrimin high dimension space densiti ratio estim discrimin often inaccur unstabl dure train gener network fail learn multimod structur target distribut even wors support model distribut support target distribut disjoint exist discrimin perfectli distinguish model distribut target arjovski bottou onc discrimin produc thi situat train gener come complet stop becaus deriv discrimin respect input turn thi motiv us introduc form restrict choic discrimin thi paper propos novel weight normal method call spectral normal stabil train discrimin network normal enjoy follow favor properti feb publish confer paper iclr lipschitz constant onli tune algorithm doe requir intens tune onli satisfactori perform implement simpl addit comput cost small fact normal method also function well even without tune lipschitz constant onli hyper paramet thi studi provid explan effect spectral normal gan regular techniqu weight tion saliman kingma weight clip arjovski et gradient penalti rajani et also show absenc complimentari regular techniqu batch normal weight decay featur match discrimin spectral maliz improv sheer qualiti gener imag better weight normal gradient penalti method thi section lay theoret groundwork propos method let us consid simpl discrimin made neural network follow form input x f x θ w w l w w θ w w l w learn paramet set w l w al activ function omit bia term layer simplic ﬁnal output discrimin given x θ f x θ activ function correspond diverg distanc measur user choic standard formul gan given min g max v g min max g taken set gener discrimin tion respect convent form v g goodfellow et given log x log qdata data distribut pg model gener distribut learn adversari optim tivat function use thi express continu function rang sigmoid function known ﬁxed gener g optim discrimin thi form v g given g x qdata x qdata x pg x machin learn commun ha point recent function space discrimin select crucial affect perform gan number work hara et qi gulrajani et advoc import lipschitz continu assur bounded statist exampl optim discrimin gan abov standard formul take form g x qdata x qdata x pg x sigmoid f x f x log qdata x pg x deriv x qdata x x pg x x unbound even incomput thi prompt us introduc regular condit deriv f x particularli success work thi array qi arjovski et gulrajani et propos method control lipschitz constant discrimin ad regular term deﬁn input exampl would follow footstep search discrimin set continu function arg max v g publish confer paper iclr mean smallest valu x ani x norm norm input base regular allow rel easi formul base sampl also suffer fact impos regular space outsid support gener data distribut without introduc somewhat heurist mean method would introduc thi paper call spectral normal method aim skirt thi issu normal weight matric use techniqu devis yoshida miyato spectral normal spectral normal control lipschitz constant discrimin function f liter constrain spectral norm layer g hin deﬁnit lipschitz norm equal suph σ h σ spectral norm matrix matrix norm σ max h max equival largest singular valu therefor linear layer g h wh norm given suph σ h suph σ w σ w lipschitz norm activ function equal use inequ observ follow bound hl σ w l spectral normal normal spectral norm weight matrix w satisﬁ lipschitz constraint σ w wsn w w normal w l use appeal inequ fact σ wsn w see bound abov would like emphas differ spectral normal spectral norm regular introduc yoshida miyato unlik method spectral norm regular penal spectral norm ad explicit regular term object function method fundament differ method make attempt set spectral norm design valu moreov reorgan deriv normal cost function rewrit object function see method augment cost function sampl data depend regular function spectral norm regular hand impos sampl data independ regular cost function like regular lasso fast approxim spectral norm σ w mention abov spectral norm σ w use regular layer crimin largest singular valu naiv appli singular valu decomposit comput σ w round algorithm algorithm becom comput heavi instead use power iter method estim σ w golub van der vorst yoshida miyato power iter method estim spectral norm veri small addit comput time rel full comput cost vanilla gan pleas see appendix detail method algorithm summari actual spectral normal algorithm exampl relu jarrett et nair hinton glorot et leaki relu maa et satisﬁ condit mani popular activ function satisfi constraint predeﬁn k well publish confer paper iclr gradient analysi spectral normal weight wsn w respect wij wsn w σ w eij σ w w w σ w eij ij σ w w σ w ij wsn eij matrix whose j entri zero everywher els respect ﬁrst left right singular vector h hidden layer network transform wsn deriv v g calcul respect w discrimin given g σ w e e δt wsnh σ w e δ g wsnh λ ˆ e δt wsnh ˆ e repres empir tation ˆ e δht k would like comment implic ﬁrst term ˆ e deriv weight without normal thi light second term express seen regular term penal ﬁrst singular compon adapt regular coefﬁcient λ posit δ wsnh point similar direct thi prevent column space w concentr one particular direct cours train word spectral normal prevent transform layer becom sensit one direct also use spectral normal devis new parametr model name split layer map two separ abl compon spectral normal map spectral norm constant turn thi parametr ha merit promot perform gan see appendix e spectral normal vs regular techniqu weight normal introduc saliman kingma method normal norm row vector weight matrix mathemat thi equival requir weight weight normal wwn wwn wwn σt wwn min di σt singular valu matrix therefor scaler thi frobeniu normal requir sum squar singular valu normal howev inadvert impos much stronger constraint matrix intend wwn weight normal matrix dimens di norm ﬁxed unit vector h maxim wwn σt wwn mean wwn rank one similar thing said frobeniu normal see appendix detail use wwn correspond use onli one featur discrimin model probabl distribut target order retain much norm input possibl henc make discrimin sensit one would hope make norm wwnh larg weight normal howev thi come cost reduc rank henc number featur use discrimin thu conﬂict interest weight normal desir use mani featur possibl distinguish gener distribut target distribut former interest often reign mani case inadvert diminish number featur use discrimin consequ algorithm would produc rather arbitrari model distribut spectrum ha multipl would look subgradi howev probabl thi happen zero almost sure would continu discuss without give erat event publish confer paper iclr match target distribut onli select featur weight clip arjovski et also suffer pitfal spectral normal hand suffer conﬂict interest note lipschitz constant linear oper determin onli maximum singular valu word spectral norm independ rank thu unlik weight normal spectral normal allow paramet matrix use mani featur possibl satisfi local constraint spectral normal leav freedom choos number singular compon featur feed next layer discrimin brock et al introduc orthonorm regular weight stabil train gan work brock et al augment adversari object function ad follow term tw f thi seem serv purpos spectral normal orthonorm regular mathemat quit differ spectral normal becaus orthonorm izat destroy inform spectrum set singular valu one hand spectral normal onli scale spectrum maximum one gulrajani et al use gradient penalti method combin wgan work place constant discrimin augment object function regular reward function local constant discret set point form ˆ x x x gener interpol sampl x gener distribut sampl x data distribut thi rather forward approach doe suffer problem mention abov regard effect dimens featur space approach ha obviou weak heavili depend support current gener distribut matter cours gener tion support gradual chang cours train thi destabil effect regular fact empir observ high learn rate destabil perform contrari spectral normal regular function oper space effect regular stabl respect choic batch train spectral normal doe easili destabil aggress learn rate moreov requir comput cost spectral normal power iter becaus comput requir one whole round ward backward propag appendix section compar comput cost two method number updat experi order evalu efﬁcaci approach investig reason behind efﬁcaci conduct set extens experi unsupervis imag gener torralba et coat et compar method normal techniqu see method fare larg dataset also appli method dataset imagenet russakovski et well thi section structur follow first discuss object function use train architectur describ optim set use experi explain two perform measur imag evalu imag produc train gener final summar result imagenet architectur discrimin gener use convolut neural network also evalu spectral norm convolut weight w treat oper matrix dimens dout dinhw train paramet gener batch normal ioff szegedi refer reader tabl appendix section detail architectur sinc conduct convolut discret spectral norm depend size stride pad howev answer onli differ predeﬁn publish confer paper iclr method use follow standard object function adversari loss v g e x log x e z log g z z latent variabl p z standard normal distribut n g rdz determinist gener function set dz experi updat g use altern cost propos goodfellow et al z log g z use goodfellow et al bengio updat use origin cost deﬁn also test perform algorithm hing loss given vd ˆ g e x min x e z h min ˆ g z vg g ˆ e z h ˆ g z respect discrimin gener optim object equival minim revers kl diverg kl thi type loss ha alreadi propos use lim ye tran et al algorithm base hing loss also show good perform evalu incept score fid wasserstein gan gradient penalti gulrajani et use follow object function v g x z g z eˆ x xd ˆ x regular term one introduc appendix section quantit assess gener exampl use incept score saliman et echet incept distanc fid heusel et pleas see appendix detail score result thi section report accuraci spectral normal use abbrevi gan spectral normal gan dure train depend rithm perform hyperparmet optim also compar perform qualiti algorithm techniqu crimin network includ weight clip arjovski et gulrajani et bn ioff szegedi layer normal ln ba et weight normal wn saliman kingma orthonorm regular thonorm brock et order evalu efﬁcaci gradient penalti also appli gradient penalti term standard adversari loss gan would refer thi method weight clip follow origin work arjovski et al set clip constant c convolut weight layer gradient penalti set λ suggest gulrajani et al orthonorm ize weight randomli select orthonorm oper train gan object function augment regular term use brock et al compar studi throughout exclud multipli paramet γ weight normal method well batch normal layer normal method thi wa done order prevent method overtli violat lipschitz condit experi differ multipli paramet fact abl achiev ani improv optim use adam optim kingma ba experi test set ndi number updat discrimin per one updat gener learn rate α ﬁrst second order momentum paramet adam list detail set tabl appendix section set b c set use previou repres work purpos set e f evalu perform algorithm implement aggress learn rate detail architectur convolut network deploy gener discrimin refer reader tabl appendix section number updat gan gener experi unless otherwis note firstli inspect spectral norm layer dure train make sure spectral normal procedur inde serv purpos see figur publish confer paper iclr tabl set test experi hyperparamet set follow gulrajani et al bengio radford et al respect set α ndi e f weight clip bn ln wn orthonorm sn incept score b c e f weight clip ln wn orthonorm sn incept score b c e f b figur incept score differ method hyperparamet higher better spectral norm layer ﬂoat around region throughout train pleas see appendix detail figur show incept score method set see spectral normal rel robust aggress learn rate momentum eter fail train good gan high learn rate high momentum paramet orthonorm regular perform poorli set e perform slightli better method optim set result suggest method robust method respect chang ting train also optim perform weight normal wa inferior spectral normal consist divers exampl best score spectral normal better almost method tabl show incept score differ method optim set dataset see perform better almost contemporari optim set perform even better hing loss train number iter fell behind orthonorm regular detail comparison orthonorm regular spectral normal pleas see section figur show imag produc gener train weight maliz spectral normal consist better gan weight normal term qualiti gener imag precis mention section set imag gener spectral normal wa clearer divers imag produc weight normal also see fail train good gan high learn rate high momentum e f gener imag also ran twice time longer iter becaus seem converg yet still thi elong train sequenc still complet befor origin iter size becaus optim set set b ndi comput light publish confer paper iclr weight clip bn ln wn orthonorm sn fid b c e f weight clip ln wn orthonorm sn fid b c e f b figur fid differ method hyperparamet lower better tabl incept score fid unsupervis imag gener radford et experi yang et al yang et bengio gulrajani et method incept score fid real data weight clip batch norm layer norm weight norm orthonorm orthonorm updat updat eq eq updat orthonorm eq eq et resnet batch normal layer normal shown figur appendix section also compar algorithm multipl benchmark method summar result bottom half tabl also test perform method resnet base gan use gulrajani et al pleas note method list thereof differ optim method architectur model pleas see tabl appendix section detail network architectur implement algorithm wa abl perform better almost predecessor perform resnet experi train architectur multipl random seed weight initi produc model differ paramet gener imag time comput averag incept score model valu resnet tabl mean standard deviat score comput set model train differ seed publish confer paper iclr index layer layer layer layer layer layer layer wc wn sn index layer layer layer layer layer layer layer wc wn sn b figur squar singular valu weight matric train differ method weight ping wc weight normal wn spectral normal sn scale singular valu largest singular valu equal wn sn calcul singular valu normal weight matric analysi singular valu analysi weight discrimin figur show squar singular valu weight matric ﬁnal discrimin produc method use paramet yield best incept score predict section singular valu ﬁrst ﬁfth layer train weight clip weight normal concentr compon weight matric layer tend rank deﬁcit hand singular valu weight matric layer train spectral izat broadli distribut goal distinguish pair probabl distribut nonlinear data manifold embed high dimension space rank cienci lower layer especi fatal output lower layer gone onli set rectiﬁ linear transform mean tend lie space linear part margin mani featur input distribut space result oversimpliﬁ discrimin actual conﬁrm effect thi phenomenon gener imag especi figur imag gener spectral normal divers complex gener weight normal train time slightli slower weight normal comput time signiﬁcantli faster mention tion slower method becaus need calcul gradient gradient norm comput time almost vanilla gan becaus rel comput cost power iter neglig compar cost forward backward propag imag size larger pleas see figur appendix section actual comput time comparison orthonorm regular order highlight differ spectral normal orthonorm tion conduct addit set experi explain section orthonorm regular differ method destroy spectral inform put equal emphasi featur dimens includ one shall weed train process see extent possibl detriment effect experi increas publish confer paper iclr rel size featur map dimens incept score orthonorm figur effect perform induc chang featur map dimens ﬁnal layer width highlight region repres standard deviat result multipl seed weight initi orthonorm regular doe perform well larg featur map dimens possibl becaus design forc discrimin use dimens includ one unnecessari set optim use set c wa optim orthonorm regular iter incept score orthnorm figur learn curv condit imag gener term incept score gan gan orthonorm regular imagenet mension featur space especi ﬁnal layer conv train spectral normal prefer rel small featur space dimens see figur set train select paramet orthonorm regular perform optim ﬁgure show result experi predict formanc orthonorm regular deterior increas dimens featur map ﬁnal layer hand doe falter thi modiﬁc architectur thu least thi perspect may method robust respect chang network architectur imag gener imagenet show method remain effect larg high dimension dataset also appli method train condit gan dataset class consist approxim imag compress pixel regard adversari loss condit gan use practic formul use mirza osindero except replac standard gan loss hing loss pleas see appendix detail experiment set precis simpli increas input dimens output dimens factor figur rel size impli layer structur origin publish confer paper iclr gan without normal gan layer normal collaps begin train fail produc ani meaning imag gan orthonorm normal brock et al spectral normal hand wa abl produc imag incept score orthonorm normal howev plateau around iter sn kept improv even afterward figur knowledg research ﬁrst kind succeed produc decent imag imagenet dataset singl pair tor gener figur measur degre follow footstep odena et al comput intra odena et al pair dentli gener gan imag class see intra suffer less intra ensur superior method limit within speciﬁc set also pare perform orthonorm regular condit gan project discrimin miyato koyama well standard uncondit gan experi achiev better perform orthonorm regular set see figur appendix section conclus thi paper propos spectral normal stabil train gan appli tral normal gan imag gener task gener exampl divers convent weight normal achiev better compar incept score tive previou studi method impos global regular discrimin oppos local regular introduc possibl use combin futur work would like investig method stand amongst method theoret basi experi algorithm larger complex dataset acknowledg would like thank member prefer network particularli maeda eiichi matsumoto masaki watanab keisuk yahata insight comment discuss also would like thank anonym review comment openreview forum insight discuss refer martin arjovski eon bottou toward principl method train gener adversari network iclr martin arjovski soumith chintala eon bottou wasserstein gener adversari network icml pp devansh arpit yingbo zhou bhargava u kota venu govindaraju normal propag metric techniqu remov intern covari shift deep network icml pp jimmi lei ba jami ryan kiro geoffrey e hinton layer normal arxiv preprint andrew brock theodor lim jame ritchi nick weston neural photo edit introspect adversari network arxiv preprint adam coat andrew ng honglak lee analysi network unsupervis featur learn aistat pp harm de vri florian strub emi mari hugo larochel olivi pietquin aaron c courvil ulat earli visual process languag nip pp dc dowson bv landau echet distanc multivari normal distribut journal multivari analysi vincent dumoulin jonathon shlen manjunath kudlur learn represent artist style iclr xavier glorot antoin bord yoshua bengio deep spars rectiﬁ neural network aistat pp gene h golub henk van der vorst eigenvalu comput centuri journal tional appli mathemat ian goodfellow jean mehdi mirza bing xu david sherjil ozair aaron courvil yoshua bengio gener adversari net nip pp publish confer paper iclr ishaan gulrajani faruk ahm martin arjovski vincent dumoulin aaron courvil improv train wasserstein gan arxiv preprint kaim xiangyu zhang shaoq ren jian sun deep residu learn imag recognit cvpr pp martin heusel hubert ramsauer thoma unterthin bernhard nessler unter klambauer sepp hochreit gan train two updat rule converg nash equilibrium arxiv preprint jonathan ho stefano ermon gener adversari imit learn nip pp sergey ioff christian szegedi batch normal acceler deep network train reduc intern covari shift icml pp kevin jarrett koray kavukcuoglu marc aurelio ranzato yann lecun best architectur object recognit iccv pp kui jia dacheng tao shenghua gao xiangmin xu improv train deep neural network via singular valu bound cvpr diederik kingma jimmi ba adam method stochast optim iclr jiwei li monro tianlin shi alan ritter dan jurafski adversari learn neural dialogu gener emnlp pp jae hyun lim jong chul ye geometr gan arxiv preprint andrew l maa awni hannun andrew ng rectiﬁ nonlinear improv neural network acoust model icml workshop deep learn audio speech languag process mehdi mirza simon osindero condit gener adversari net arxiv preprint takeru miyato masanori koyama cgan project discrimin iclr shakir moham balaji lakshminarayanan learn implicit gener model nip workshop adversari train vinod nair geoffrey e hinton rectiﬁ linear unit improv restrict boltzmann machin icml pp sebastian nowozin botond cseke ryota tomioka train gener neural sampler use variat diverg minim nip pp augustu odena christoph olah jonathon shlen condit imag synthesi auxiliari classiﬁ gan icml pp qi gener adversari network lipschitz densiti arxiv preprint alec radford luke metz soumith chintala unsupervis represent learn deep convolut gener adversari network iclr olga russakovski jia deng hao su jonathan kraus sanjeev satheesh sean zhiheng huang andrej karpathi aditya khosla michael bernstein alexand berg li imagenet larg scale visual recognit challeng intern journal comput vision masaki saito eiichi matsumoto shunta saito tempor gener adversari net singular valu clip iccv tim saliman diederik p kingma weight normal simpl reparameter acceler ing deep neural network nip pp tim saliman ian goodfellow wojciech zaremba vicki cheung alec radford xi chen improv techniqu train gan nip pp christian szegedi wei liu yangq jia pierr sermanet scott reed dragomir anguelov dumitru erhan vincent vanhouck andrew rabinovich go deeper convolut cvpr pp seiya tokui kenta oono shohei hido justin clayton chainer open sourc framework deep learn proceed workshop machin learn system learningsi ninth annual confer neural inform process system nip antonio torralba rob fergu william freeman million tini imag larg data set metric object scene recognit ieee transact pattern analysi machin intellig dustin tran rajesh ranganath david blei deep hierarch implicit model arxiv preprint masatoshi uehara issei sato masahiro suzuki kotaro nakayama yutaka matsuo gener adversari net densiti ratio estim perspect nip workshop adversari train david yoshua bengio improv gener adversari network denois featur match iclr sitao xiang hao li effect batch normal weight normal gener adversari network arxiv preprint jianwei yang anitha kannan dhruv batra devi parikh layer recurs gener sarial network imag gener iclr yuichi yoshida takeru miyato spectral norm regular improv generaliz deep learn arxiv preprint publish confer paper iclr b figur gener imag differ method weight normal spectral normal publish confer paper iclr figur pixel imag gener train dataset incept score publish confer paper iclr algorithm spectral normal let us describ shortcut section detail begin vector u randomli initi weight multipl domin singular valu u orthogon ﬁrst left singular appeal principl power method produc ﬁrst left right singular vector follow updat rule v u approxim spectral norm w pair singular vector σ w utw use sgd updat w chang w updat would small henc chang largest singular valu implement took advantag thi fact reus u comput step algorithm initi vector subsequ step fact thi recycl procedur one round power iter wa sufﬁcient actual experi achiev satisfactori perform algorithm appendix summar comput spectral normal weight matrix w thi approxim note thi procedur veri comput cheap even comparison calcul forward backward propag neural network pleas see figur actual comput time without spectral normal algorithm sgd spectral normal initi ul l l random vector sampl isotrop bution updat layer l appli power iter method unnorm weight w l vl w l w l ul calcul wsn spectral norm w l sn w l w w l σ w l ut l w vl updat w l sgd dataset dm learn rate α w l l lℓ w l sn w l dm b experiment set perform measur incept score introduc origin saliman et al xn n exp e dkl p p approxim n pn p p train incept convolut neural network szegedi et would refer ception model short work saliman et al report thi score strongli correl subject human judgment imag qualiti follow procedur saliman et al bengio calcul score randomli gener exampl train gener evalu abil gener natur imag repeat experi time report averag standard deviat incept score echet incept distanc heusel et anoth measur qualiti gener exampl use order inform ﬁnal layer incept model appli practic safe assum u gener uniform distribut sphere orthogon ﬁrst singular vector becaus thi happen probabl publish confer paper iclr exampl chet distanc dowson landau distanc two distribut assum multivari gaussian distribut f trace mean covari sampl q p respect output ﬁnal layer incept model befor softmax echet incept distanc fid two distribut imag distanc comput echet incept distanc true distribut gener distribut empir sampl multipl repetit experi exhibit ani notabl variat thi score imag gener compar studi experi recent resnet architectur gulrajani et al well standard cnn thi addit set experi use adam optim use veri hyper paramet use gulrajani et al α ndi doubl featur map gener origin becaus thi modiﬁc achiev better result note doubl dimens featur map experi howev manc deterior imag gener imagenet imag use thi set experi resiz pixel detail architectur given tabl gener network condit gan use dition batch normal cbn dumoulin et de vri et name replac standard batch normal layer cbn condit label inform optim use adam hyperparamet use resnet dataset train network gener updat appli linear decay learn rate iter rate would end network architectur tabl standard cnn model use experi imag gener slope lrelu function network set z dens mg deconv bn relu deconv bn relu deconv bn relu conv tanh gener mg svhn mg rgb imag x conv lrelu conv lrelu conv lrelu conv lrelu conv lrelu conv lrelu conv lrelu dens b discrimin svhn publish confer paper iclr bn relu conv bn relu conv figur block architectur tor remov bn layer resblock tabl resnet architectur dataset use similar tectur one use gulrajani et al z dens resblock resblock resblock bn relu conv tanh gener rgb imag x resblock resblock resblock resblock relu global sum pool dens b discrimin tabl resnet architectur dataset z dens resblock resblock resblock bn relu conv tanh gener rgb imag x resblock resblock resblock resblock resblock relu global sum pool dens b discrimin publish confer paper iclr tabl resnet architectur imag gener imagenet dataset gener tional gan replac usual batch normal layer resblock condit batch normal layer model project discrimin use architectur use miyato koyama pleas see paper detail z dens resblock resblock resblock resblock resblock bn relu conv tanh gener rgb imag x resblock resblock resblock resblock resblock resblock relu global sum pool dens b discrimin tional gan rgb imag x resblock resblock resblock concat emb h resblock resblock resblock relu global sum pool dens c discrimin condit gan comput eas embed integ label dimens befor concaten vector output termedi layer publish confer paper iclr c appendix result accuraci spectral normal figur show spectral norm layer discrimin cours train set optim c tabl throughout train fact deviat part except convolut layer largest rank deviat begin train norm thi layer stabil around iter updat σ w figur spectral norm seven convolut layer standard cnn dure cours train cifar train time wn sn vanilla second gener updat imag wn sn vanilla second gener updat b imag figur comput time updat set ndi effect ndi spectral normal weight normal figur show effect ndi perform weight normal spectral izat result shown figur follow set except valu ndi wn perform deterior larger ndi amount comput minimax better accuraci sn doe suffer thi unintend effect publish confer paper iclr ndi incept score incept score gener updat sn wn figur effect ndi spectral normal weight normal shade region repres varianc result differ seed gener imag layer normal batch normal figur gener imag layer norm batch norm publish confer paper iclr imag gener imagenet iter incept score orthnorm uncondit gan iter incept score orthnorm b condit gan project discrimin figur learn curv term incept score gan orthonorm regular imagenet ﬁgure show result standard uncondit gan ﬁgure b show result condit gan train project tor miyato koyama spectral normal vs regular techniqu thi section dedic compar studi spectral normal regular method discrimin particular show contemporari regular includ weight normal weight clip implicitli impos constraint weight matric place unnecessari restrict search space discrimin speciﬁc show weight normal weight clip unwittingli favor weight matric thi forc train discrimin larg depend select featur render algorithm abl match model distribut target distribut onli veri low dimension featur space weight normal frobeniu normal weight normal introduc saliman kingma method normal norm row vector weight wwn wt wt wt wi wi wi wi ith row vector wwn w respect still anoth techniqu regular weight matrix use frobeniu norm wfn p tr w tw qp j ij origin regular techniqu invent goal improv tion perform supervis train saliman kingma arpit et howev recent work ﬁeld gan saliman et xiang li found anoth raison etat regular discrimin succeed improv perform origin origin literatur weight normal wa introduc method reparametr form wwn wt wt γdo wt γi learn cours train thi work deal case γi assess method lipschitz constraint publish confer paper iclr method fact render train discrimin scribe k achiev desir effect certain extent howev weight normal impos follow implicit restrict choic wwn wwn wwn σt wwn min di σt singular valu matrix abov equat hold becaus pmin di σt wwn tr wwn w wn pdo wi wt thi restrict norm ﬁxed unit vector h maxim wwn σt wwn mean wwn rank one use w correspond use onli one featur discrimin model probabl distribut target similarli frobeniu normal requir wfn wfn wfn argument abov follow see critic problem two regular method order retain much norm input possibl henc make discrimin sensit one would hope make norm wwnh larg weight normal howev thi come cost reduc rank henc number featur use discrimin thu conﬂict interest weight normal desir use mani featur possibl distinguish gener distribut target distribut former interest often reign mani case inadvert diminish number featur use discrimin consequ algorithm would produc rather arbitrari model distribut match target distribut onli select featur spectral normal hand suffer conﬂict interest note lipschitz constant linear oper determin onli maximum singular valu word spectral norm independ rank thu unlik weight normal spectral normal allow paramet matrix use mani featur possibl satisfi local constraint spectral normal leav freedom choos number singular compon featur feed next layer discrimin see thi visual refer reader figur note spectral normal allow wider rang choic weight normal index sn wn figur visual differ spectral normal red weight normal blue possibl set singular valu possibl set singular valu plot increas order weight normal blue spectral normal red set singular valu permit spectral normal condit scale wwn spectral norm exactli deﬁnit weight normal area blue curv bound note rang choic weight normal small summari weight normal frobeniu normal favor skew distribut lar valu make column space weight matric lie approxim low dimension vector space hand spectral normal doe compromis number featur dimens use discrimin fact experiment show gan publish confer paper iclr train spectral normal gener synthet dataset wider varieti higher incept score gan train two regular method weight clip still anoth regular techniqu weight clip introduc arjovski et al train wasserstein gan weight clip simpli truncat element weight matric absolut valu bound abov prescrib constant c unfortun weight clip suffer problem weight normal frobeniu normal weight clip truncat valu c valu ﬁxed unit vector x maxim rank w one train favor discrimin use onli select featur gulrajani et al refer thi problem capac underus problem also report train wgan weight clip slower origin dcgan radford et singular valu clip singular valu constraint one direct straightforward way control spectral norm clip singular ue saito et jia et thi approach howev comput heavi becaus one need implement singular valu decomposit order comput singular valu similar less obviou approach parametr w follow train discrimin thi constrain parametr w usv subject u tu v tv max sii k u v diagon matrix howev simpl task train thi model remain absolut faith thi parametr constraint spectral normal hand carri updat rel low tional cost without compromis normal constraint wgan gradient penalti recent gulrajani et al introduc techniqu enhanc stabil train wasserstein gan arjovski et work endeavor place constraint discrimin augment adversari loss function follow regular function λ e ˆ x xd ˆ x λ balanc coefﬁcient ˆ x ˆ x ϵx x ϵ x x g z z use thi augment object function gulrajani et al succeed train gan base resnet et impress perform advantag method comparison spectral normal impos local constraint directli discrimin function without rather normal thi suggest method less like underus capac network structur time thi type method penal gradient sampl point ˆ x suffer obviou problem abl regular function point outsid support current gener distribut fact gener distribut support gradual chang cours train thi destabil effect regular contrari spectral normal regular function effect regular stabl respect choic batch fact observ experi high learn rate destabil perform train spectral normal doe falter aggress learn rate publish confer paper iclr moreov requir comput cost spectral normal power iter becaus comput requir one whole round forward backward propag figur compar comput cost two method number updat said one shall rule possibl gradient penalti compliment spectral normal vice versa becaus two method regular discrimin complet differ mean experi section actual conﬁrm tion reparametr spectral normal improv qualiti gener exampl baselin onli e reparametr motiv spectral normal take advantag regular effect spectral normal saw abov develop anoth algorithm let us consid anoth parametr weight matrix discrimin given w γ wsn γ scalar variabl learn thi parametr compromis straint layer interest give freedom model keep model becom degener thi reparametr need control lipschitz condit mean gradient penalti gulrajani et inde think analog version reparametr replac wsn w normal criterion extens thi form new saliman kingma origin introduc weight normal order deriv reparametr form wsn replac wwn vector experi comparison reparametr differ normal method thi part addendum experiment compar reparametr deriv two differ normal method weight normal spectral normal test reprametr method train discrimin architectur network use cnn use previou section cnn use architectur provid gulrajani et tabl summar result see method signiﬁcantli improv incept score baselin regular cnn slightli improv score resnet base cnn figur show learn curv critic loss train valid set b ception score differ reparametr method see beneﬁci effect spectral normal learn curv discrimin well verifi ﬁgure discrimin spectral normal overﬁt less train dataset crimin without reparametr weight normal effect overﬁt observ incept score well ﬁnal score spectral normal better best incept score achiev cours train spectral normal achiev wherea spectral normal vanilla normal achiev respect f gradient gener normal method let us denot w w normal weight n w scalar normal coefﬁcient spectral norm frobeniu norm gener write deriv loss implement method base code provid author jani et http publish confer paper iclr method incept score fid standard cnn baselin frobeniu norm weight norm spectral norm resnet gulrajani et al resnet baselin spectral norm spectral norm featur map tabl incept score differ reparametr mehtod without label pervis report incept score fid frobeniu normal becaus train collaps earli stage method resnet incept score fid gulrajani et al baselin spectral norm spectral norm featur map tabl incept score fid differ reparametr method label supervis auxiliari classiﬁ odena et respect unnorm weight w follow g w n w w w w n w n w w v w v w n α w v n α w λ trace w v w gradient w v calcul ˆ e δ g wh h hidden node network transform w ˆ e repres empir expect n w deriv g w ˆ e ˆ e w w n w σ w g w σ w ˆ e ˆ e w notic least case n w n w point thi gradient given w v publish confer paper iclr updat gener critic loss train valid wn train wn valid sn train sn valid critic loss updat gener incept score wn sn b incept score figur learn curv critic loss b incept score differ reparametr method weight normal wn spectral normal gp sn parametr free