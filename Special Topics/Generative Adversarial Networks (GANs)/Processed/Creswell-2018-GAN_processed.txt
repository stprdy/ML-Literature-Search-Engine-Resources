ieee signal process magazin januari antonia creswel tom white vincent dumoulin kai arulkumaran biswa sengupta anil bharath deep learn visual understand part gener adversari network overview g ener adversari network gan provid way learn deep represent without extens annot train data achiev thi deriv tion signal competit process involv pair network represent learn gan may use varieti applic includ imag synthesi semant imag edit style transfer imag superresolut classif aim thi review articl provid overview gan signal process commun draw familiar analog concept possibl addit identifi differ method train struct gan also point remain challeng theori applic introduct gan emerg techniqu semisupervis unsupervis learn achiev thi implicitli model distribut data propos character train pair work competit common analog apt visual data think one network art forger art expert forger known gan eratur gener g creat forgeri aim make realist imag expert known tor receiv forgeri real authent imag aim tell apart see figur train neousli competit crucial gener ha direct access real onli way learn interact crimin discrimin ha access synthet sampl sampl drawn stack real imag error signal discrimin provid simpl ground truth know whether imag came real stack gener error signal via discrimin use train gener lead toward abl produc forgeri better qualiti network repres gener discrimin typic implement multilay network consist digit object identifi date public januari author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee signal process magazin januari convolut fulli connect layer gener discrimin network must differenti though necessari directli invert one er gener network map tion space call latent space space data shall focu imag may express thi malli r z g g x r z z sampl latent space r x x imag denot number dimens basic gan discrimin network may ilarli character function map imag data probabl imag real data distribut rather gener distribut x fix gener g discrimin may train classifi imag either train data real close one fix gener fake close zero discrimin optim may frozen gener g may continu train lower accuraci discrimin gener distribut abl match real data distribut perfectli crimin maxim confus predict input practic discrimin might train optim explor train process depth section train top interest academ problem relat train construct gan motiv behind ing gan may necessarili gener inat per se represent embodi either pair network use varieti subsequ task explor applic represent section applic preliminari terminolog gener model learn captur statist distribut train data allow us synthes sampl learn distribut top synthes novel data ple may use downstream task tic imag edit data augment style transfer also interest use represent model learn task classif imag retriev occasion refer fulli connect convolut layer deep network gener tron spatial filter bank nonlinear postprocess case network weight learn backpropag notat gan literatur gener deal multidimension tor often repres vector probabl space ic latent space z field signal process common repres vector bold lowercas symbol adopt thi convent emphas multidimension natur variabl accordingli commonli refer p x data repres probabl densiti function random vector x lie x use p x g denot distribut vector produc gener work gan use calligraph symbol g denot gener discrimin network tive network set paramet weight h g h learn optim dure train synthet data sampl gener train map nois sampl synthet data sampl fool discrimin nois sourc real data sampl real fake z x discrimin train distinguish real data sampl synthes sampl figur two model learn dure train process gan discrimin h gener g h typic implement neural network could implement ani form differenti system map data one space anoth see articl text detail author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee signal process magazin januari system train requir clear object function follow usual notat use jg g h h jd g h h refer object function gener discrimin tive choic notat remind us two object function sens codepend evolv eter set g h h network iter updat shall explor thi section train final note multidimension gradient use updat use g dh denot gradient oper respect weight gener paramet dh denot gradient oper respect weight discrimin expect gradient indic notat e captur data distribut central problem signal process statist densiti estim obtain explicit parametr data real world thi key motiv behind gan gan literatur term data gener distribut often use refer underli probabl densiti probabl mass function observ data gan learn implicitli comput sort similar distribut candid model distribut correspond real data see figur whi bother densiti estim answer lie heart problem visual enc includ imag categor visual object detect recognit object track object registr principl bay theorem infer problem comput vision address estim tional densiti function possibl indirectli form model learn joint distribut variabl interest observ data difficulti face likelihood function imag data ficult construct gan explicitli provid way evalu densiti function pair suitabl capac gener implicitli captur distribut data relat work one may view principl gener model ing comparison standard techniqu signal ing data analysi exampl signal process make wide use idea repres signal weight combin basi function fix basi function underli standard techniqu wavelet sentat approach construct basi tion trace back hotel transform root pearson observ princip compon minim reconstruct error accord minimum squar error criterion despit wide use standard princip compon analysi pca doe overt statist model observ data though ha shown base pca may deriv maximum likelihood paramet tion problem despit wide adopt pca basi tion emerg eigenvector covari matrix observ input data map represent space back signal imag space linear shallow linear map limit complex model henc data repres independ compon analysi ica provid anoth level sophist signal compon longer need orthogon mix coeffici use blend compon togeth construct exampl data mere consid statist independ ica ha variou formul differ object function use dure estim signal compon tive model express signal imag gener compon recent innov explor ica nois contrast estim nce thi may seen approach spirit gan object function learn independ compon compar statist appli pg x pdata x pg x pdata x sampl produc gener sampl real data figur dure gan train gener encourag produc distribut sampl p x g match real data p x data propriat parameter train gan distribut nearli ident represent embodi gan captur learn paramet weight gener discrimin network author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee signal process magazin januari nois produc candid gener model origin nce approach includ updat gener comparison made gan standard tool signal process pca ica rier wavelet represent latent space gan analog coeffici space commonli refer transform space set gan apart standard tool signal process level complex el map vector latent space imag space becaus gener network contain nonlinear almost arbitrari depth thi mani extraordinarili complex regard deep model modern es gener imag model group explicit implicit densiti model explicit densiti model either tractabl chang variabl model autoregress model intract direct model train variat enc undirect model train use markov chain implicit densiti model captur statist distribut data gener process make use either ancestr sampl markov sampl gan fall direct implicit model categori detail view relev paper found gan architectur fulli connect gan first gan architectur use fulli connect neural work gener discrimin thi type architectur wa appli rel simpl imag data set mnist handwritten digit natur imag toronto face data set tfd convolut gan go fulli connect convolut neural network cnn natur extens given cnn extrem well suit imag data earli experi conduct suggest wa difficult train gener inat network use cnn level capac tional power use vise learn laplacian pyramid ial network lapgan offer one solut thi problem pose gener process use multipl scale imag decompos laplacian amid condit convolut gan train produc layer given one abov addit radford et al pose famili network ture call deep convolut gan dcgan allow train pair deep convolut gener discrimin network dcgan make use stride fraction stride convolut allow spatial downsampl pling oper learn dure train oper handl chang sampl rate tion key requir map imag space possibl dimension latent space imag space discrimin ther detail dcgan ture train present section train extens synthes imag two dimens wu et al present gan abl synthet data sampl synthet data sampl gener must learn creat condit imag sampl gener must learn creat condit imag sampl nois sourc nois sourc class categori class categori real data sampl real data sampl real fake real fake discrimin train distinguish real data synthes sampl condit class c discrimin output estim class label decis authent z z c c c b figur condit gan propos mirza et al perform imag synthesi discrimin perform discrimin real fake imag b infogan hand ha discrimin network also estim class label author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee signal process magazin januari synthes data sampl use metric convolut wu et al synthes novel object includ chair tabl car addit also sent method map imag version object portray imag condit gan mirza et al extend gan framework condit set make gener crimin network figur condit gan advantag abl provid better resent multimod data gener parallel drawn condit gan infogan decompos nois sourc incompress sourc latent code attempt discov latent factor variat maxim mutual inform latent code gener output thi latent code use discov object class pure unsupervis ion although strictli necessari latent code categor represent learn infogan appear semant meaning deal complex gled factor imag appear includ variat pose light emot content facial imag gan infer model origin formul gan lack way map given observ x vector latent gan literatur thi often refer infer mechan sever techniqu propos invert tor pretrain gan independ propos adversari learn infer ali bidirect gan bigan provid simpl effect extens introduc infer network discrimin examin joint data latent pair thi formul gener consist two network encod infer network jointli train fool discrimin discrimin receiv pair x z vector see figur ha determin pair constitut genuin tupl consist real imag sampl encod fake imag sampl correspond input gener ideal model output refer reconstruct similar input calli fidel reconstruct data sampl synthes use poor fidel sampl may improv addit adversari cost tion data sampl reconstruct adversari autoencod autoencod network compos encod decod learn map data intern latent tation learn determinist ping via encod data space imag latent represent space map via decod latent space back data space composit two map result reconstruct two map train reconstruct imag close possibl origin autoencod reminisc filter bank wide use imag signal process x x z ε synthet data sampl gener must learn creat imag sampl nois sourc encod data sampl encod must learn map real imag sampl latent space discrimin receiv tupl z x real data sampl real fake figur structur consist three network one serv discrimin anoth map nois vector latent space imag space decod depict gener g figur final network encod depict e map imag space latent space author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee signal process magazin januari howev autoencod gener learn nonlinear map direct implement deep work possibl architectur use implement autoencod remark flexibl train pervis backpropag appli reconstruct imag origin learn paramet encod decod suggest previous one often want latent space use organ addit one may want perform ancestr sampl encod adversari train provid rout achiev two goal specif adversari train may appli latent space desir prior distribut latent space gan thi result combin loss function reflect reconstruct error measur differ distribut prior produc candid encod network thi approach akin variat autoencod vae gan play role leibler kl term loss function mesched et al unifi vae adversari ing form adversari variat bay avb framework similar idea present avb tri optim criterion vae use sarial train object rather kl diverg train gan introduct train gan involv find paramet discrimin maxim classif accuraci find paramet gener maxim fuse discrimin thi train process summar figur cost train evalu use valu function v g h depend gener nator train involv solv maxminv g g h log log v x x e e g p p x x g data h dure train paramet one model ed paramet fix goodfellow et al show fix gener uniqu optim discrimin p p p x x x x g data data h also show gener g optim p p x x g data equival optim nator predict sampl drawn x word gener optim discrimin maxim confus distinguish real sampl one fake ideal discrimin train optim respect current gener gener updat howev practic discrimin might train optim rather may onli train small number iter gener updat ousli discrimin altern ing train criterion typic use gener use max log z g g rather min log z g g despit theoret exist uniqu solut gan train challeng often unstabl sever reason one approach improv gan train draw sampl nois sourc draw sampl real imag draw sampl nois sourc estim expect gradient estim expect gradient updat discrimin paramet updat gener paramet z z x θg updat θg θd updat θd jθd θd θg jθg θg θd figur main loop gan train novel data sampl xl may drawn pass random sampl z gener network ent discrimin may updat k time befor updat gener author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee signal process magazin januari ass empir symptom might experienc dure train symptom includ get pair model converg gener model collaps gener veri lar sampl differ input discrimin loss converg quickli zero vide reliabl path gradient updat gener sever author suggest heurist approach address issu discuss next section earli attempt explain whi gan train unstabl propos goodfellow man et al observ gradient descent method typic use updat paramet gener discrimin propriat solut mizat problem pose gan train actual constitut saddl point man et al provid simpl exampl show thi howev stochast gradient descent often use updat neural network ope program environ make easi construct updat work use stochast gradient descent although earli theoret treatment show gener optim p p x x g data veri neat result strong underli intuit real data sampl resid manifold sit space possibl represent instanc color imag sampl size n n pixel valu r space may call dimension n dimens take valu zero maximum measur pixel intens data sampl support pdata howev constitut manifold real data associ particular problem typic py veri small part total space x similarli sampl produc gener also occupi onli small portion x arjovski et al show support p x g p x data lie space ing x consequ thi p x g p x data may overlap exist nearli trivial crimin capabl distinguish real sampl p x x data fake sampl p x x g accuraci thi case discrimin error quickli converg zero paramet gener may onli updat via crimin thi happen gradient use ing paramet gener also converg zero may longer use updat gener arjovski et explan account sever symptom relat gan train goodfellow et al also show optim train g equival minim js diverg p x g p x data mal updat may less meaning inaccur thi theoret insight ha motiv research cost function base altern distanc sever explor section altern train trick one first major improv train gan gener imag dcgan architectur pose radford et al thi work wa result extens explor cnn architectur previous use comput vision result set guidelin construct train gener discrimin section convolut gan allud import stride fraction ed convolut key nent architectur design thi allow gener discrimin learn good upsampl downsampl oper may contribut ment qualiti imag synthesi specif train batch normal wa recommend use work stabil train deeper model anoth suggest wa minim number fulli nect layer use increas feasibl train deeper model final radford et al show use leaki tifi linear unit relu activ function intermedi layer discrimin gave superior manc use regular relu later saliman et al propos heurist approach stabil train gan first ture match chang object gener slightli increas amount inform avail specif discrimin still train distinguish real fake sampl gener train match crimin expect intermedi activ featur fake sampl expect intermedi activ real sampl second minibatch discrimin add extra input discrimin featur encod distanc given sampl minibatch sampl thi intend prevent mode collaps discrimin easili tell gener produc output third trick heurist averag penal network paramet deviat run averag ou valu help converg equilibrium fourth virtual batch normal reduc depend one sampl sampl minibatch culat batch statist normal sampl place within refer minibatch fix ning train final label smooth make target discrimin instead one smooth discrimin classif boundari henc prevent overli confid discrimin would provid weak gradient erat sønderbi et al advanc idea challeng represent learn gan may use varieti applic includ imag synthesi semant imag edit style transfer imag superresolut classif author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee signal process magazin januari discrimin ad nois sampl befor feed crimin sønderbi et al argu label smooth bias mal discrimin techniqu instanc nois move manifold real fake sampl closer togeth time prevent discrimin ili find discrimin boundari complet separ real fake ple practic thi implement ad gaussian nois synthes real imag anneal standard deviat time process wa dentli propos arjovski et al altern formul first part thi section consid oret interpret gener gan ond part look altern cost function aim directli address problem vanish gradient gener gan cost function nowozin et al show gan train may aliz minim onli js diverg estim refer genc includ diverg measur nowozin et al show may approxim appli fenchel conjug desir sampl drawn distribut gener sampl pass sampl crimin provid list fenchel conjug commonli use well activ function may use final layer gener network depend choic deriv gener cost function train gener crimin nowozin et al observ raw form maxim gener object like lead weak gradient especi start train propos altern cost function updat tor less like satur begin train nowozin et al propos discrimin train deriv ratio real fake data distribut estim gener train onli estim minim uehara et al extend crimin step ratio distribut real fake data predict gener step directli minim altern also cover goodfellow altern cost function prevent vanish gradient arjovski et al propos wasserstein gan wgan gan altern cost function deriv approxim wasserstein distanc unlik nal gan cost function wgan like provid gradient use updat gener cost function deriv wgan reli discrimin refer critic continu function practic thi may ment simpli clip paramet discrimin howev recent research suggest weight clip advers reduc capac crimin model forc learn simpler function gulrajani et al propos improv method train discrimin wgan penal norm discrimin gradient respect data sampl dure train rather perform eter clip brief comparison gan variant gan allow us synthes novel data sampl random nois consid difficult train due partial vanish gradient gan model discuss thi articl requir care hyperparamet tune model select train howev perhap easier model train adversari autoencod aae wgan aae rel easi train becaus adversari loss appli fairli simpl distribut lower dimens imag data wgan design easier train use differ formul train object doe suffer vanish gradient problem wgan may also train success even without batch normal also less sensit choic eariti use convolut layer sampl synthes use gan wgan may belong ani class present train data condit gan provid approach synthes sampl specifi content evid variou visual techniqu ure organ latent space harbor mean vanilla gan provid infer model allow data sampl map latent sentat bigan ali provid mechan map imag data latent space infer howev reconstruct qualiti suggest necessarili faith encod decod sampl veri recent opment show ali may recov encod data sampl faith howev thi model share lot common avb aae autoencod similar vae latent space regular use sarial train rather encod sampl prior structur latent space gan build represent data train produc structur geometr tor space differ domain thi qualiti share neural network model includ vae well linguist model gener domain data model map vector space set gan apart standard tool signal process level complex model map vector latent space imag space author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee signal process magazin januari ha fewer dimens data space forc model discov terest structur data sent effici thi latent space origin end gener network data thi level resent latent space highli structur may support mantic oper exampl includ rotat face trajectori latent space well imag analog effect ad visual attribut eyeglass onto bare face vanilla gan model gener map data latent space space model mani gan model encod addit support invers map thi becom power od explor use structur latent space gan network encod collect label imag map latent space analyz discov concept vector repres attribut smile wear vector appli scale offset latent space influenc behavior gener figur similar use encod process model distribut latent sampl gurumurthi et al propos model latent space mixtur ian learn mixtur compon maxim likelihood gener data sampl data ing distribut applic gan discov new applic adversari train deep network activ area research examin comput vision applic appear ture subsequ refin applic chosen highlight differ approach use base represent imag manipul analysi character fulli reflect potenti breadth applic gan use gan imag classif place within broader context machin learn provid use quantit assess featur extract vise learn imag synthesi remain core gan capabl especi ful gener imag subject constraint superresolut offer exampl ing approach supplement adversari loss compon achiev result final translat demonstr gan offer solut famili task requir automat convert input imag output imag classif regress gan train complet neural network reus downstream task exampl output convolut layer discrimin use featur extractor simpl linear model fit top featur use modest quantiti imag label pair qualiti unsupervis represent dcgan network assess appli lariz classifi featur vector extract train discrimin good classif score achiev use thi approach supervis pervis data set even disjoint nal train data qualiti data represent may improv adversari train includ jointli learn enc mechan ali represent tor wa built use last three hidden layer ali encod similar classifi yet achiev misclassif rate significantli lower dcgan addit ali ha achiev art classif result label inform incorpor train routin label train data limit suppli adversari train may also use synthes train sampl shrivastava et al use gan refin synthet imag maintain annot inform train model onli synthet imag real train data shrivastava et al achiev perform task similarli good result obtain gaze estim predict figur exampl appli smile vector ali model first imag exampl unsmil woman last exampl woman smile z valu first imag infer last interpol along vector connect give z valu may pass gener synthes novel sampl note implic displac vector latent space travers smile intens imag space figur use courtesi tom white gan build represent data train produc structur geometr vector space differ domain author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee signal process magazin januari use spatiotempor gan architectur case model train synthet data gener well appli real data bousmali et al propos address thi problem adapt synthet sampl sourc domain match target domain use adversari train addit liu et al propos use multipl one per tie weight synthes pair respond imag sampl differ domain becaus qualiti gener sampl hard quantit judg across model classif task like remain import quantit tool perform assess gan even new divers applic comput vision explor imag synthesi much recent gan research focus improv qualiti util capabl lapgan model introduc cascad convolut work within laplacian pyramid framework gener imag fashion similar approach use huang et al gan oper intermedi represent rather imag lapgan also extend condit version gan model g network receiv addit label inform input thi techniqu ha prove use common practic improv imag qualiti thi idea gan condit wa later extend incorpor natur languag exampl reed et al use gan ture synthes imag text descript one might describ revers caption exampl given text caption bird white black head wing long orang beak train gan gener sever plausibl imag match descript addit condit text descript gener adversari network gawwn dition imag locat gawwn system support activ interfac larg imag could built increment textual descript part suppli bound box figur condit gan onli allow us synthes novel sampl ic attribut also allow us op tool intuit edit imag chang hairstyl person imag make wear es edit imag appear younger addit applic gan imag edit includ work zhu brock et al translat condit adversari network well suit ing input imag output imag recur theme comput graphic imag process comput vision model offer tion thi famili problem addit learn map input imag output imag model also construct loss function train thi map thi model ha demonstr effect result differ problem comput vision previous requir rate machineri includ semant segment gener map aerial photo color black white imag wang et al present similar idea use gan first synthes map similar depth map map imag natur scene cyclegan extend thi work introduc cycl consist loss attempt preserv origin imag cycl translat revers translat thi mulat match pair imag longer need train thi make data prepar much simpler open techniqu larger famili applic exampl artist style transfer render natur imag style artist picasso monet simpli train unpair collect paint natur imag figur superresolut superresolut allow imag gener imag train model ring detail upsampl srgan thi bird complet black thi bird bright blue man orang jacket black pant black cap wear sunglass ski head right leg belli beak figur exampl imag synthesi use gawwn gawwn imag condit text descript imag locat specifi either keypoint bound box figur reproduc permiss author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee signal process magazin januari model extend earlier effort ad adversari loss compon constrain imag resid manifold natur imag srgan gener condit imag infer natur imag upscal factor unlik gan applic sarial loss one compon larger loss function also includ perceptu loss pretrain classifi regular loss encourag spatial coher imag thi context adversari loss constrain overal tion manifold natur imag produc perceptu convinc solut custom applic often pere avail relev curat train data set howev srgan straightforward custom specif domain new train imag pair easili construct downsampl corpu imag thi import consider practic sinc infer detail gan gener vari depend domain imag use train set discuss open question gan attract consider attent due abil leverag vast amount unlabel data much ress ha made allevi challeng relat train evalu gan still remain sever open challeng mode collaps articul section train gan common problem gan involv gener collaps produc small famili similar sampl partial collaps worst case produc simpli singl sampl complet laps divers gener increas practic hack balanc distribut sampl produc discrimin real fake batch employ multipl gan cover differ mode probabl distribut yet anoth solut allevi mode collaps alter distanc measur use compar statist bution arjovski propos compar distribut base wasserstein distanc rather diverg dcgan distanc gan metz et al propos unrol discrimin sever step let calcul updat current gener sever step use unrol crimin updat gener use normal minimax object normal discrimin onli train updat one step gener ha access crimin would updat usual one step gener object discrimin simpli assign low probabl gener previou output forc gener move result either converg endless cycl mode ping howev unrol object gener prevent discrimin focus previou updat updat gener foresight crimin would respond train point gan hessian loss function becom indefinit optim solut therefor lie find saddl point rather local minimum deep learn larg ber optim depend onli first deriv loss function converg saddl point gan requir good initi invok stabl manifold theorem nonlinear system theori lee et al show select initi point optim random gradient descent would converg saddl probabl one also see addit mesched et al argu converg gan object function suffer presenc zero real part jacobian matrix well eigenvalu larg imaginari part thi ene gan train yet due exist monet monet zebra summer photo photo hors zebra hors winter summer winter photo monet hors zebra winter summer b c figur cyclegan model learn imag imag translat two unord imag collect shown exampl al imag map monet paint landscap photo b zebra hors c summer winter photo yosemit nation park figur reproduc permiss author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee signal process magazin januari optim hope lost unfortun method complex scale cubic quadrat dimens paramet therefor anoth line question lie appli scale order optim adversari train fundament problem exist rium gan use result bayesian nonparametr arora et al connect exist equilibrium finit mixtur neural mean tain capac equilibrium might exist close relat note ha also argu gan train appear converg train distribut could still far away target distribut allevi thi issu arora et al propos new measur call neural net distanc evalu gener model one gaug fidel sampl synthes er model use likelihood estim gan train use one methodolog compar anoth model comparison question onli relev gan also probabilist model gener thei argu evalu gan use ent measur lead conflict conclus qualiti synthes sampl decis select one measur anoth depend applic conclus explos interest gan driven onli potenti learn deep highli nonlinear map latent space data space back also potenti make use vast quantiti unlabel imag data remain close deep represent learn within tie gan train mani opportun opment theori algorithm power deep network vast opportun new applic acknowledg would like thank david hi valuabl feedback previou revis articl antonia creswel acknowledg support engin physic enc research council doctor train scholarship author antonia creswel receiv degre imperi colleg london biomed engin current degre student biolog inspir comput vision group imperi colleg london focu research improv train er adversari network appli visual search learn represent unlabel sourc imag data tom white tom receiv hi degre mathemat univers georgia hi degre media art scienc massachusett institut technolog current senior lectur school design victoria univers wellington new zealand hi current research focus explor grow use structiv machin learn comput design ativ potenti human design work collabor artifici neural network dure explor design idea prototyp vincent dumoulin receiv hi degre physic comput scienc univers montréal canada doctor candid montréal institut learn algorithm sion yoshua bengio aaron courvil work learn approach gener model kai arulkumaran receiv hi degre comput scienc univers cambridg unit kingdom hi degre biomed engin imperi colleg london current candid depart bioengin wa research intern twitter magic poni microsoft research hi research focu deep reinforc learn comput vision visuomotor control biswa sengupta biswasengupta receiv hi honor degre electr comput engin hi degre theoret comput scienc univers york unit kingdom receiv hi second degre neural behavior scienc max planck institut biolog cybernet germani hi degre theoret neurosci univers cambridg unit kingdom receiv train bayesian statist differenti geometri univers colleg london univers cambridg befor lead cortexica vision system chief scientist current visit scientist imperi colleg london also lead research noah ark lab huawei technolog unit kingdom anil bharath receiv hi eng degre electron electr engin univers colleg london degre signal process imperi colleg london current reader depart bioengin academ fellow imperi data scienc institut low institut engin technolog wa academ visitor signal process group univers cambridg cofound cortexica vision system hi research interest deep architectur visual infer refer goodfellow mirza xu ozair courvil bengio gener adversari net proc advanc neural inform process system pp zhu krähenbühl shechtman efro gener visual manipul natur imag manifold proc european conf comput vision pp bousmali silberman dohan erhan krishnan unsupervis domain adapt gener adversari work proc ieee conf comput vision pattern recognit pp zhu park isola efro unpair translat use adversari network proc int conf comput vision onlin avail http author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee signal process magazin januari radford metz chintala unsupervis represent learn deep convolut gener adversari network proc int conf learn represent workshop track creswel bharath adversari train sketch retriev proc european conf comput vision workshop amsterdam netherland pp lecun bengio hinton deep learn natur vol pp hotel analysi complex statist variabl princip ponent educ vol pp goodfellow distinguish criteria estim gener el proc int conf learn represent workshop track gutmann hyvärinen estim new estim principl unnorm statist model artif intel vol bengio yao alain vincent gener denois encod gener model proc advanc neural inform process system pp goodfellow nip tutori gener adversari network proc neural inform process system conf onlin avail http denton chintala szlam fergu deep gener imag model use laplacian pyramid adversari network proc advanc neural inform process system pp wu zhang xue freeman tenenbaum learn listic latent space object shape via model proc advanc neural inform process system pp mirza osindero condit gener adversari net arxiv preprint chen duan houthooft schulman sutskev abbeel infogan interpret represent learn inform maxim tive adversari net proc advanc neural inform process system pp creswel bharath invert gener gener sarial network proc neural inform process system workshop adversari train lipton tripathi precis recoveri latent vector gener adversari network proc int conf learn represent workshop track dumoulin belghazi pool mastropietro lamb arjovski courvil adversari learn infer proc int conf learn represent donahu krähenbühl darrel adversari featur learn proc int conf learn represent li liu chen pu chen henao carin toward understand adversari learn joint distribut match proc advanc neural inform process system makhzani shlen jaitli goodfellow adversari encod proc int conf learn represent onlin avail http kingma well variat bay proc int conf learn represent mesched nowozin geiger adversari variat bay unifi variat autoencod gener adversari network onlin avail http saliman goodfellow zaremba cheung radford chen improv techniqu train gan proc advanc neural inform process system pp arjovski bottou toward principl method train tive adversari network proc neural inform process system conf workshop adversari train shelham long darrel fulli convolut network semant segment ieee tran pattern anal mach vol pp ioff szegedi batch normal acceler deep network train reduc intern covari shift proc int conf machin learn pp sønderbi caballero thei shi huszár amortis map infer imag proc int conf learn represent nowozin cseke tomioka train gener neural sampler use variat diverg minim proc advanc neural inform process system pp uehara sato suzuki nakayama matsuo gener adversari net densiti ratio estim perspect arxiv preprint arjovski chintala bottou wasserstein gan proc int conf machin learn pp gulrajani ahm arjovski dumoulin courvil improv train wasserstein gan proc advanc neural inform process system mikolov chen corrado dean effici estim word represent vector space proc int conf learn represent gurumurthi sarvadevabhatla radhakrishnan deligan gener adversari network divers limit data proc ieee conf comput vision pattern recognit pp ledig thei huszár caballero aitken tejani totz wang shi singl imag use gener adversari network proc ieee conf comput vision pattern recognit pp yu porikli face imag discrimin gener network proc european conf comput vision pp yu porikli hallucin veri unalign noisi face imag transform discrimin autoencod proc ieee conf comput vision pattern recognit pp shrivastava pfister tuzel susskind wang webb learn simul unsupervis imag adversari train proc ieee conf comput vision pattern recognit pp zhang lim zhao feng deep futur gaze gaze anticip egocentr video use adversari network proc ieee conf comput vision pattern recognit pp liu tuzel coupl gener adversari network proc advanc neural inform process system pp huang li poursae hopcroft belongi stack tive adversari network proc ieee conf comput vision pattern recognit reed akata yan logeswaran schiel lee gener adversari text imag synthesi proc int conf machin learn onlin avail http reed akata mohan tenka schiel lee learn draw proc advanc neural inform process system pp brock lim ritchi weston neural photo edit introspect adversari network proc int conf learn represent isola zhu zhou efro translat condit adversari network proc ieee conf comput vision pattern recognit pp li wand precomput textur synthesi markovian gener adversari network proc european conf comput vision pp arora ge liang zhang gener um gener adversari net gan proc int conf machin learn pp tolstikhin gelli bousquet schölkopf adagan boost gener model arxiv preprint onlin avail http zhao mathieu lecun gener ial network proc int conf learn represent onlin avail http metz pool pfau unrol gener adversari network proc int conf learn represent onlin avail http lee simchowitz jordan recht gradient descent onli converg minim proc conf learn theori pp pemantl nonconverg unstabl point urn model stochast approxim ann vol pp apr mesched nowozin geiger numer gan proc advanc neural inform process system conf onlin avail http thei van den oord bethg note evalu ativ model proc int conf learn represent sp author licens use limit queen mari univers london download march utc ieee xplore restrict appli