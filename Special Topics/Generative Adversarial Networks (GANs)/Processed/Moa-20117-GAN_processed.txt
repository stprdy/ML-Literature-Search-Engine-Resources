least squar gener adversari network xudong qing haoran raymond zhen stephen paul comput scienc citi univers hong kong mathemat inform technolog educ univers hong kong inform system citi univers hong kong optic imageri analysi learn northwestern polytechn univers xudonmao itqli raylau steve abstract unsupervis learn gener adversari work gan ha proven huge success regular gan hypothes discrimin classiﬁ moid cross entropi loss function howev found thi loss function may lead vanish gradient lem dure learn process overcom lem propos thi paper least squar tive adversari network lsgan adopt least squar loss function discrimin show minim object function lsgan yield mize pearson diverg two beneﬁt lsgan regular gan first lsgan abl gener higher qualiti imag regular gan second lsgan perform stabl dure learn process evalu lsgan lsun dataset experiment result show imag gener lsgan better qualiti one gener regular gan also conduct two comparison ment lsgan regular gan illustr stabil lsgan introduct deep learn ha launch profound reform even appli mani task imag classiﬁc object detect segment task obvious fall scope supervis learn mean lot label data vide learn process compar supervis learn howev unsupervis learn task gener model obtain limit impact deep ing although deep gener model rbm dbm vae propos model face difﬁculti intract function ﬁculti intract infer turn restrict effect model recent gener adversari network gan demonstr impress perform vise learn task unlik deep gener model usual adopt approxim method intract function infer gan requir ani mation train tiabl network basic idea gan ousli train discrimin gener tor aim distinguish real sampl gener sampl gener tri gener fake sampl real possibl make discrimin believ fake sampl real data far plenti work shown gan play signiﬁc role iou task imag gener imag resolut learn spite great progress gan imag ation qualiti gener imag gan still ite realist task regular gan adopt moid cross entropi loss function discrimin argu thi loss function howev lead problem vanish gradient updat tor use fake sampl correct side decis boundari still far real data figur b show use fake sampl genta updat gener make discrimin believ real data caus almost ror becaus correct side real data side decis boundari howev sampl still far real data want pull close real data base thi observ propos least squar gener adversari network lsgan adopt least squar loss function fake sampl real sampl sigmoid decis boundari least squar decis boundari fake sampl real sampl fake sampl updat g sigmoid decis boundari fake sampl real sampl fake sampl updat g least squar decis boundari b c figur illustr differ behavior two loss function decis boundari two loss function note decis boundari go across real data distribut success gan learn otherwis learn process satur b decis boundari sigmoid cross entropi loss function orang area side real sampl blue area side fake sampl get veri small error fake sampl magenta updat g correct side decis boundari c decis boundari least squar loss function penal fake sampl magenta result forc gener gener sampl toward decis boundari inat idea simpl yet power least squar loss function abl move fake sampl toward decis boundari becaus least squar loss function penal sampl lie long way correct side decis boundari figur c show least squar loss function penal fake sampl genta pull toward decis boundari even though correctli classiﬁ base thi erti lsgan abl gener sampl closer real data anoth beneﬁt lsgan improv stabil learn process gener speak train gan difﬁcult issu practic becaus instabl gan learn recent sever paper point instabl gan learn partial caus object function speciﬁc minim object function regular gan suffer vanish gradient make hard updat gener gan reliev thi problem becaus lsgan penal sampl base distanc decis boundari gener gradient updat gener centli arjovski et al propos method uat stabil gan learn exclud batch maliz follow thi method evalu stabil ﬁnd lsgan also abl converg rel good state without batch normal contribut thi paper summar follow propos lsgan adopt least squar loss function discrimin show ing object function lsgan yield ing pearson diverg evalu lsgan lsun dataset experiment result demonstr lsgan gener realist imag ular gan two comparison experi ate train stabil also conduct prove stabil lsgan appli condit lsgan chines acter gener evalu handwritten nese charact dataset class propos model abl gener readabl chines charact rest thi paper organ follow section brieﬂi review relat work gener adversari work propos method introduc section experiment result present section final conclud paper section relat work gener adversari network gan pose goodfellow et al explain ori gan learn base game theoret scenario show power capabl unsupervis task gan appli mani speciﬁc task like age gener imag text age synthesi imag imag translat combin tradit content loss adversari loss gener adversari network achiev perform task imag reed et al propos model thesiz imag given text descript base dition gan isola et al also use tional gan transfer imag one represent anoth addit unsupervis learn task gan also show potenti learn task saliman et al propos framework learn discrimin onli output probabl input imag real data also output probabl belong class despit great success gan achiev prove qualiti gener imag still challeng lot work propos improv iti imag gan radford et al ﬁrst duce convolut layer gan architectur pose network architectur call deep convolut er adversari network dcgan denton et al propos anoth framework call laplacian pyramid gener adversari network lapgan struct laplacian pyramid gener imag start imag man et al propos techniqu call featur ing get better converg idea make erat sampl match statist real data imiz mean squar error intermedi layer discrimin anoth critic issu gan stabil ing process mani work propos address thi problem analyz object function gan view discrimin energi function use architectur prove stabil gan learn make tor discrimin balanc metz et al creat unrol object function enhanc erat che et al incorpor reconstruct ule use distanc real sampl struct sampl regular get stabl ent nowozin et al point object origin gan relat diverg special case diverg estim gener arbitrari arjovski et al extend thi analyz properti four ferent diverg distanc two distribut conclud wasserstein distanc nicer shannon diverg qi propos gan whose loss function base assumpt real sampl smaller loss fake sampl prove thi loss function ha ent almost everywher method thi section ﬁrst review formul gan brieﬂi next present lsgan along eﬁt section final two model architectur gan introduc gener adversari network learn process gan train inat gener g simultan target g learn distribut pg data g start sampl input variabl z uniform gaussian tribut pz z map input variabl z data space g z θg differenti network hand classiﬁ x θd aim nize whether imag train data minimax object gan formul follow min g max vgan g x log x z log g z least squar gener adversari work view discrimin classiﬁ regular gan adopt sigmoid cross entropi loss function state section updat gener thi loss function caus problem vanish gradient ple correct side decis boundari still far real data remedi thi problem propos least squar gener adversari work lsgan suppos use code scheme discrimin b label fake data real data respect object tion lsgan deﬁn follow min vlsgan x x z g z min g vlsgan g z g z c denot valu g want believ fake data beneﬁt lsgan beneﬁt lsgan deriv two aspect first unlik regular gan caus almost loss sampl lie long way correct side cision boundari figur b lsgan penal sampl even though correctli classiﬁ figur c updat gener paramet discrimin ﬁxed decis boundari ﬁxed result penal make gener erat sampl toward decis boundari hand decis boundari go across manifold real data success gan learn otherwis learn process satur thu move ate sampl toward decis boundari lead make closer manifold real data second penal sampl lie long way decis boundari gener gradient date gener turn reliev problem b figur sigmoid cross entropi loss function b least squar loss function vanish gradient thi allow lsgan perform stabl dure learn process thi beneﬁt also deriv anoth perspect shown figur least squar loss function ﬂat onli one point sigmoid cross entropi loss function satur x rel larg relat pearson diverg origin gan paper author ha shown minim equat yield minim shannon diverg c g kl pdata pdata pg kl pg pdata pg also explor relat lsgan consid follow extens equat min vlsgan x x z g z min g vlsgan g x x z g z note ad term x x vlsgan g doe chang optim valu sinc thi term doe contain paramet ﬁrst deriv optim discrimin ﬁxed g x bpdata x apg x pdata x pg x proof equat found appendix follow equat use pd denot pdata simplic reformul vlsgan g equat follow deconv bn deconv bn fc bn z deconv bn deconv bn deconv bn deconv bn deconv fc conv bn conv bn least squar loss conv bn conv b figur model architectur k k c stride denot layer nel c output ﬁlter stride layer bn mean layer follow batch normal layer fc n note layer n output node activ layer omit gener b discrimin g x g z x x x apg x pd x pg x x apg x pd x pg x z x pd x b pd x pg x pd x pg x z x pg x b pd x pg x pd x pg x z x b pd x pg x pd x pg x dx z x b pd x pg x b pg x pd x pg x dx set b b g z x x pd x pg x pd x pg x dx pearson pd pearson pearson diverg thu ing equat yield minim pearson diverg pd pg b c satisfi condtion b b paramet select one method determin valu b c tion satisfi condit minim equat yield minim gener imag lsgan b gener imag dcgan b gener imag dcgan report figur gener imag pearson diverg pd pg ampl set b c get follow object function min vlsgan x x z g z min g vlsgan g z g z anoth method make g gener sampl real possibl set c exampl use binari code scheme get follow object function min vlsgan x x z g z min g vlsgan g z g z practic observ equat equat show similar perform thu either one select follow section use equat train model model architectur ﬁrst model design shown figur motiv vgg model pare architectur two lution layer ad top two deconvolut layer architectur discrimin ident one except usag least squar loss function follow dcgan relu activ leakyrelu activ use gener discrimin respect second model design task lot class exampl chines charact chines charact ﬁnd train gan multipl class abl gener readabl charact reason multipl class input onli one class output state minist relationship input output one way solv thi problem use condit gan caus condit label inform creat terminist relationship input output howev directli condit encod label vector thousand class infeas term memori cost comput time cost use linear map layer reduc dimension label vector church outdoor b dine room c kitchen confer room figur gener imag differ scene dataset gener label vector concaten nois input layer discrimin label vector caten convolut layer layer layer concaten determin ical experi thi section ﬁrst present detail dataset implement next present result tive evalu quantit evalu lsgan compar stabil lsgan lar gan two comparison experi final uat lsgan handwritten chines charact dataset contain class dataset implement detail evalu lsgan three dataset lsun implement propos model base public tion use tensorflow lsun learn rate set learn rate set follow dcgan adam optim set implement avail http qualit evalu train lsgan dcgan network architectur figur resolut dataset gener imag two method present figur compar age gener dcgan textur detail textur bed imag gener lsgan exquisit imag gener lsgan look sharper also train lsgan four scene dataset clude church dine room kitchen confer room result lsgan train four scene dataset shown figur lsgan without bn g use adam b regular gan without bn g use adam c lsgan without bn g use rmsprop regular gan without bn g use rmsprop figur comparison experi exclud batch normal bn quantit evalu incept score train lsgan dcgan network chitectur use model randomli gener imag calcul incept score evalu incept score lsgan gan shown tabl observ tion score vari differ train model report incept score tabl averag differ train model lsgan dcgan thi quantit evalu incept score lsgan show compar perform dcgan method incept score dcgan report dcgan lsgan tabl incept score human subject studi evalu perform lsgan duct human subject studi use gener bedroom imag lsgan dcgan network architectur randomli construct imag pair one imag lsgan one dcgan ask amazon mechan turk notat judg imag look realist vote total dcgan get vote lsgan get vote lsgan get vote gan stabil comparison state section one beneﬁt lsgan improv stabil present two comparison periment compar stabil lsgan regular gan one follow comparison method base network architectur present two ture design compar stabil ﬁrst one exclud batch normal gener bng short second one exclud batch maliz gener discrimin bngd short point select optim critic model perform thu evalu two architectur two optim adam sprop summari four train set bng adam bng rmsprop bngd adam bngd rmsprop train abov model dataset use regular gan lsgan separ follow four major observ first bng adam chanc lsgan gener tive good qualiti imag test time succe gener rel good qualiti imag regular gan never observ success learn ular gan suffer sever degre mode collaps gener imag lsgan regular gan shown figur second bngd rmsprop figur show lsgan gener higher qualiti imag regular gan slight degre mode laps third lsgan regular gan similar formanc bng rmsprop bngd adam speciﬁc bng rmsprop lsgan regular gan abl gener rel good imag bngd adam slight degre mode collaps last rmsprop perform stabl adam sinc regular gan learn gener rel good imag bng rmsprop fail learn adam anoth experi evalu gaussian mixtur distribut dataset design literatur train lsgan regular gan mixtur step step step step step target regular gan lsgan figur dynam result gaussian kernel estim lsgan regular gan ﬁnal column show real data distribut real gener real gener figur gener imag handwritten chines charact lsgan row row imag column belong class charact row row also thi condit gener charact readabl gaussian dataset use simpl network architectur gener discrimin contain three layer figur show dynam result gaussian kernel densiti estim see ular gan suffer mode collaps start step gener sampl around singl valid mode data distribut lsgan learn gaussian mixtur distribut success suggest practic dure learn process lsgan task difﬁcult train observ lsgan learn erat good qualiti imag success ﬁrst sever train epoch sometim suffer mode collaps last although lsgan may suffer mode collaps last still select good model middl train process also observ qualiti ate imag lsgan may shift good bad dure train process base abov two vation suggest keep record gener imag everi thousand hundr iter select model manual check imag qualiti handwritten chines charact also train condit lsgan model describ section handwritten chines charact dataset contain class lsgan learn gener readabl chines charact success domli select charact shown figur two major observ figur first gener charact lsgan readabl second get correct label gener imag label vector use applic data mentat conclus futur work thi paper propos least squar er adversari network lsgan tal result show lsgan gener higher qualiti age regular gan two comparison experi evalu stabil also conduct result demonstr lsgan perform stabl lar gan furthermor propos condit lsgan model chines charact gener ate handwritten chines charact dataset class base present ﬁnding hope extend lsgan complex dataset imagenet futur instead pull gener sampl toward decis boundari design method pull ate sampl toward real data directli also worth investig acknowledg thi work support research grant project number special grant account number citi univers hong kong refer abadi agarw barham et al flow machin learn heterogen tem arjovski chintala bottou wasserstein gan che li jacob bengio li mode lariz gener adversari network chen duan houthooft schulman sutskev abbeel infogan interpret represent learn inform maxim gener adversari net advanc neural inform process system nip page denton chintala szlam fergu deep er imag model use laplacian pyramid ial network advanc neural inform process system nip page goodfellow mirza xu ozair courvil bengio er adversari net advanc neural inform process system nip page zhang ren sun deep residu ing imag recognit comput vision pattern recognit cvpr hinton salakhutdinov reduc iti data neural network scienc hornik stinchcomb white multilay forward network univers approxim neural work juli huang li poursae hopcroft belongi stack gener adversari network ioff szegedi gener adversari synthesi proceed intern enc machin learn icml isola zhu zhou efro translat condit adversari network kingma ba adam method stochast optim kingma well variat bay intern confer learn tion iclr krizhevski learn multipl layer featur tini imag tech report ledig thei huszar caballero ham acosta aitken tejani totz wang shi singl imag ing gener adversari network liu yin wang wang icdar chines handwrit recognit competit ceed intern confer document analysi recognit icdar page long shelham darrel fulli convolut network semant segment comput vision pattern recognit cvpr metz pool pfau unrol gener adversari network mirza osindero condit gener sarial net nguyen yosinski bengio dosovitskiy clune plug play gener network dition iter gener imag latent space nguyen wainwright jordan ing diverg function likelihood ratio vex risk minim ieee transact inform theori nowozin cseke tomioka train gener neural sampler use variat diverg imiz qi gener adversari network lipschitz densiti radford metz chintala unsupervis resent learn deep convolut gener versari network intern confer learn represent iclr reed akata yan logeswaran schiel lee gener adversari synthesi proceed intern confer chine learn icml ren girshick sun faster ward object detect region propos work advanc neural inform process tem page salakhutdinov hinton deep boltzmann machin proceed intern confer artiﬁci intellig statist volum page saliman goodfellow zaremba cheung ford chen chen improv techniqu ing gan advanc neural inform process tem nip page simonyan zisserman veri deep convolut network imag recognit intern confer learn represent iclr tieleman hinton lectur divid gradient run averag recent magnitud coursera neural network machin learn yu seff zhang song funkhous xiao lsun construct imag dataset use deep learn human loop zhao mathieu lecun tive adversari network