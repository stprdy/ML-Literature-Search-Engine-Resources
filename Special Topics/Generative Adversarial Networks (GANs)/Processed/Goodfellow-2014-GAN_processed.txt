gener adversari net ian jean mehdi mirza bing xu david sherjil aaron courvil yoshua epart informatiqu et de recherch erationnel e de eal eal qc abstract propos new framework estim gener model via ial process simultan train two model gener model g captur data distribut discrimin model estim probabl sampl came train data rather ing procedur g maxim probabl make mistak thi framework correspond minimax game space arbitrari function g uniqu solut exist g recov train data distribut equal everywher case g deﬁn multilay perceptron entir system train backpropag need ani markov chain unrol approxim infer work dure either train gener sampl experi demonstr potenti framework qualit quantit evalu gener sampl introduct promis deep learn discov rich hierarch model repres probabl distribut kind data encount artiﬁci intellig applic natur imag audio waveform contain speech symbol natur languag corpora far strike success deep learn involv discrimin model usual map rich sensori input class label strike success primarili base backpropag dropout algorithm use piecewis linear unit particularli gradient deep gener model less impact due difﬁculti approxim mani intract probabilist comput aris maximum likelihood estim relat strategi due difﬁculti leverag beneﬁt piecewis linear unit gener context propos new gener model estim procedur sidestep difﬁculti propos adversari net framework gener model pit adversari discrimin model learn determin whether sampl model distribut data distribut gener model thought analog team counterfeit tri produc fake currenc use without detect discrimin model analog polic tri detect counterfeit currenc competit thi game drive team improv method counterfeit indistiguish genuin articl goodfellow research scientist googl thi work earlier udem student thi work visit e de eal ecol polytechniqu ozair visit e de eal indian institut technolog delhi bengio cifar senior fellow code hyperparamet avail http thi framework yield speciﬁc train algorithm mani kind model optim algorithm thi articl explor special case gener model gener sampl pass random nois multilay perceptron discrimin model also multilay perceptron refer thi special case adversari net thi case train model use onli highli success backpropag dropout algorithm sampl gener model use onli forward propag approxim infer markov chain necessari relat work recent work deep gener model focus model provid parametr speciﬁc probabl distribut function model train ing log likelihood thi famili model perhap succes deep boltzmann machin model gener intract likelihood function therefor requir numer approxim likelihood gradient difﬁculti motiv develop gener machin explicitli repres likelihood yet abl erat sampl desir distribut gener stochast network exampl gener machin train exact backpropag rather numer proxim requir boltzmann machin thi work extend idea gener machin elimin markov chain use gener stochast network work backpropag deriv gener process use observ lim f x ϵ x unawar time develop thi work kingma well rezend et al develop gener stochast backpropag rule allow one agat gaussian distribut ﬁnite varianc backpropag covari paramet well mean backpropag rule could allow one learn tional varianc gener treat hyperparamet thi work kingma well rezend et al use stochast backpropag train variat coder vae like gener adversari network variat autoencod pair differenti gener network second neural network unlik gener adversari network ond network vae recognit model perform approxim infer gan requir differenti visibl unit thu model discret data vae requir differenti hidden unit thu discret latent variabl like approach exist less close relat method previou work ha also taken approach use discrimin criterion train gener model approach use criteria intract deep gener model method difﬁcult even approxim deep model becaus involv ratio tie approxim use variat approxim lower bound iti estim nce involv train gener model learn weight make model use discrimin data ﬁxed nois distribut use previous train model nois distribut allow train sequenc model increas qualiti thi seen inform competit mechan similar spirit formal petit use adversari network game key limit nce discrimin deﬁn ratio probabl densiti nois distribut model distribut thu requir abil evalu backpropag densiti previou work ha use gener concept two neural network compet relev work predict minim predict minim hidden unit neural network train differ output second network predict valu hidden unit given valu hidden unit thi work differ predict minim three import way thi work competit network sole train criterion sufﬁcient train network predict minim onli regular encourag hidden unit neural network tistic independ accomplish task primari train criterion natur competit differ predict minim two network output compar one network tri make output similar tri make output differ output question singl scalar gan one network produc rich high dimension vector use input anoth network attempt choos input network doe know process speciﬁc learn process differ predict minim describ optim problem object function minim learn approach minimum object function gan base minimax game rather optim problem valu function one agent seek maxim seek minim game termin saddl point minimum respect one player strategi maximum respect player strategi gener adversari network ha sometim confus relat concept ial exampl adversari exampl exampl found use optim directli input classiﬁc network order ﬁnd exampl similar data yet misclassiﬁ thi differ present work becaus adversari exampl mechan train gener model instead adversari exampl primarili analysi tool show neural network behav intrigu way often conﬁdent sifi two imag differ high conﬁdenc even though differ impercept human observ exist adversari exampl doe suggest gener adversari network train could inefﬁci becaus show possibl make modern discrimin network conﬁdent recogn class without emul ani attribut class adversari net adversari model framework straightforward appli model multilay perceptron learn gener distribut pg data x deﬁn prior input nois variabl pz z repres map data space g z θg g differenti function repres multilay perceptron paramet θg also deﬁn second multilay perceptron x θd output singl scalar x repres probabl x came data rather pg train maxim probabl assign correct label train exampl sampl simultan train g minim log g z word g play follow minimax game valu function v g min g max v g x log x z log g z next section present theoret analysi adversari net essenti show train criterion allow one recov data gener distribut g given enough capac limit see figur less formal pedagog explan approach practic must implement game use iter numer approach optim complet inner loop train comput prohibit ﬁnite dataset would result overﬁt instead altern k step optim one step optim thi result maintain near optim solut long g chang slowli enough procedur formal present algorithm practic equat may provid sufﬁcient gradient g learn well earli learn g poor reject sampl high conﬁdenc becaus clearli differ train data thi case log g z satur rather train g minim log g z train g maxim log g z thi object function result ﬁxed point dynam g provid much stronger gradient earli learn theoret result gener g implicitli deﬁn probabl distribut pg distribut sampl g z obtain z therefor would like algorithm converg good estim pdata given enough capac train time result thi section done parametr set repres model inﬁnit capac studi converg space probabl densiti function show section thi minimax game ha global optimum pg pdata show section algorithm optim eq thu obtain desir result x z x z x z x z b c figur gener adversari net train simultan updat discrimin distribut blue dash line discrimin sampl data gener distribut black dot line px gener distribut pg g green solid line lower horizont line domain z sampl thi case uniformli horizont line abov part domain upward arrow show map x g z impos distribut pg transform sampl g contract region high densiti expand region low densiti pg consid adversari pair near converg pg similar pdata partial accur classiﬁ b inner loop algorithm train discrimin sampl data converg x pdata x pdata x x c updat g gradient ha guid g z ﬂow region like classiﬁ data sever step train g enough capac reach point improv becaus pg pdata discrimin unabl differenti two distribut x algorithm minibatch stochast gradient descent train gener adversari net number step appli discrimin k hyperparamet use k least expens option experi number train iter k step sampl minibatch nois sampl z z nois prior pg z sampl minibatch exampl x x data gener distribut pdata x updat discrimin ascend stochast gradient x h log x log g z end sampl minibatch nois sampl z z nois prior pg z updat gener descend stochast gradient x log g z end updat use ani standard learn rule use tum experi global optim pg pdata ﬁrst consid optim discrimin ani given gener proposit g ﬁxed optim discrimin g x pdata x pdata x pg x proof train criterion discrimin given ani gener g maxim quantiti v g v g z x pdata x log x dx z z pz z log g z dz z x pdata x log x pg x log x dx ani b function log b log achiev maximum discrimin doe need deﬁn outsid supp pdata pg conclud proof note train object interpret maxim timat condit probabl p indic whether x come pdata pg minimax game eq reformul c g max v g log g x log g g z log g x log g x log pdata x pdata x pg x log pg x pdata x pg x theorem global minimum virtual train criterion c g achiev onli pg pdata point c g achiev valu proof pg pdata g x consid eq henc inspect eq g x ﬁnd c g log log see thi best possibl valu c g reach onli pg pdata observ subtract thi express c g v g g obtain c g kl pdata pdata pg kl pg pdata pg kl diverg recogn previou express shannon diverg model distribut data gener process c g jsd pdata sinc diverg two distribut alway zero iff equal shown global minimum c g onli solut pg pdata gener model perfectli replic data distribut converg algorithm proposit g enough capac step algorithm discrimin allow reach optimum given g pg updat improv criterion log g x log g x pg converg pdata proof consid v g u pg function pg done abov criterion note u pg convex pg subderiv supremum convex function includ deriv function point maximum attain word f x fα x fα x convex x everi α x β arg fα x thi equival comput gradient descent updat pg optim given respond supd u pg convex pg uniqu global optima proven thm therefor sufﬁcient small updat pg pg converg px conclud proof practic adversari net repres limit famili pg distribut via function g z θg optim θg rather pg proof appli howev excel manc multilay perceptron practic suggest reason model use despit lack theoret guarante model mnist tfd dbn stack cae deep gsn adversari net tabl parzen estim report number mnist mean likelihood sampl test set standard error mean comput across exampl tfd comput standard error across fold dataset differ σ chosen use valid set fold tfd σ wa cross valid fold mean fold comput mnist compar model rather binari version dataset experi train adversari net rang dataset includ mnist toronto face databas tfd gener net use mixtur rectiﬁ linear activ sigmoid activ discrimin net use maxout activ dropout wa appli train discrimin net theoret framework permit use dropout nois intermedi layer gener use nois input onli bottommost layer gener network estim probabl test set data pg ﬁtting gaussian parzen window sampl gener g report thi distribut σ paramet gaussian wa obtain cross valid valid set thi procedur wa duce breuleux et al use variou gener model exact likelihood tractabl result report tabl thi method estim likelihood ha somewhat high varianc doe perform well high dimension space best method avail knowledg advanc gener model sampl estim likelihood directli motiv research evalu model figur show sampl drawn gener net train make claim sampl better sampl gener exist method believ sampl least competit better gener model literatur highlight potenti adversari framework advantag disadvantag thi new framework come advantag disadvantag rel previou model work disadvantag primarili explicit represent pg x must synchron well g dure train particular g must train much without updat order avoid helvetica scenario g collaps mani valu z valu x enough divers model pdata much neg chain boltzmann machin must kept date learn step advantag markov chain never need onli backprop use obtain gradient infer need dure learn wide varieti function incorpor model tabl summar comparison gener adversari net gener model approach aforement advantag primarili comput adversari model may also gain statist advantag gener network updat directli data ple onli gradient ﬂow discrimin thi mean compon input copi directli gener paramet anoth advantag adversari work repres veri sharp even degener distribut method base markov chain requir distribut somewhat blurri order chain abl mix mode conclus futur work thi framework admit mani straightforward extens b c figur visual sampl model rightmost column show nearest train exampl neighbor sampl order demonstr model ha memor train set sampl fair random draw unlik visual deep gener model imag show actual sampl model distribut condit mean given sampl hidden unit moreov sampl uncorrel becaus sampl process doe depend markov chain mix mnist b tfd c fulli connect model convolut discrimin deconvolut gener figur digit obtain linearli interpol coordin z space full model condit gener model p x c obtain ad c input g learn approxim infer perform train auxiliari network predict z given thi similar infer net train algorithm advantag infer net may train ﬁxed gener net gener net ha ﬁnish train one approxim model condit p xs subset indic x train famili condit model share paramet essenti one use adversari net implement stochast extens determinist learn featur discrimin infer net could improv manc classiﬁ limit label data avail efﬁcienc improv train could acceler greatli devis better method coordin g determin better distribut sampl z dure train thi paper ha demonstr viabil adversari model framework suggest research direct could prove use deep direct graphic model deep undirect graphic model gener autoencod adversari model train infer need dure train infer need dure train mcmc need approxim partit function gradient enforc tradeoff mix power reconstruct gener synchron discrimin gener helvetica infer learn approxim infer variat infer infer learn approxim infer sampl difﬁculti requir markov chain requir markov chain difﬁculti evalu p x intract may approxim ai intract may approxim ai explicitli repres may approxim parzen densiti estim explicitli repres may approxim parzen densiti estim model design model need design work desir infer scheme infer scheme support similar model famili gan care design need ensur multipl properti ani differenti function theoret permit ani differenti function theoret permit tabl challeng gener model summari difﬁculti encount differ approach deep gener model major oper involv model acknowledg would like acknowledg patric marcott olivi delalleau kyunghyun cho guillaum alain jason yosinski help discuss yann dauphin share hi parzen window uation code us would like thank develop theano particularli eric bastien rush theano featur speciﬁc beneﬁt thi project naud bergeron provid support l ex typeset would also like thank cifar canada research chair fund comput canada calcul ebec provid comput resourc ian goodfellow support googl fellowship deep learn final would like thank le troi brasseur stimul creativ refer bastien lamblin pascanu bergstra goodfellow bergeron bouchard bengio theano new featur speed improv deep learn unsupervis featur learn nip workshop bengio learn deep architectur ai publish bengio mesnil dauphin rifai better mix via deep represent icml bengio yosinski j deep gener stochast network trainabl backprop icml bengio alain yosinski j deep gener stochast work trainabl backprop proceed intern confer machin learn icml bergstra breuleux bastien lamblin pascanu desjardin turian bengio theano cpu gpu math express compil proceed python scientiﬁc comput confer scipi oral present breuleux bengio vincent quickli gener repres sampl process neural comput glorot bord bengio deep spars rectiﬁ neural network aistat goodfellow mirza courvil bengio maxout network icml goodfellow mirza courvil bengio deep boltzmann machin nip goodfellow lamblin dumoulin mirza pascanu bergstra bastien bengio machin learn research librari arxiv preprint gregor danihelka mnih blundel wierstra deep autoregress network icml gutmann hyvarinen estim new estim principl unnorm statist model proceed thirteenth intern confer artiﬁci intellig statist aistat hinton deng dahl moham jaitli senior vanhouck nguyen sainath kingsburi b deep neural network acoust model speech recognit ieee signal process magazin hinton dayan frey neal algorithm unsupervis neural network scienc hinton srivastava krizhevski sutskev salakhutdinov improv neural network prevent featur detector technic report jarrett kavukcuoglu ranzato lecun best architectur object recognit proc intern confer comput vision iccv page ieee kingma well variat bay proceed tional confer learn represent iclr krizhevski hinton learn multipl layer featur tini imag technic report univers toronto krizhevski sutskev hinton imagenet classiﬁc deep convolut neural network nip lecun bottou bengio haffner learn appli document recognit proceed ieee mnih gregor neural variat infer learn belief network technic report arxiv preprint rezend moham wierstra stochast backpropag approxim infer deep gener model technic report rifai bengio dauphin vincent gener process sampl contract icml salakhutdinov hinton deep boltzmann machin aistat page schmidhub j learn factori code predict minim neural comput susskind anderson hinton toronto face dataset technic report utml tr toronto szegedi zaremba sutskev bruna erhan goodfellow fergu intrigu properti neural network iclr tu z learn gener model via discrimin approach comput vision pattern recognit cvpr ieee confer page ieee