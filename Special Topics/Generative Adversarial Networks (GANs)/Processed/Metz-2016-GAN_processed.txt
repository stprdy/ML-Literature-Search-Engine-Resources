publish confer paper iclr unrol gener adversari network luke googl brain lmetz ben stanford univers pool david pfau googl deepmind pfau jascha googl brain jaschasd abstract introduc method stabil gener adversari network gan deﬁn gener object respect unrol optim discrimin thi allow train adjust use optim crimin gener object ideal infeas practic use current valu discrimin often unstabl lead poor solut show thi techniqu solv common problem mode collaps stabil train gan complex recurr gener increas divers coverag data distribut gener introduct use deep neural network gener model complex data ha made great advanc recent year thi success ha achiev surpris divers train loss model architectur includ denois autoencod vincent et variat toencod kingma well rezend et gregor et kulkarni et burda et kingma et gener stochast network alain et diffus probabilist model et autoregress model thei bethg van den oord et b real preserv transform dinh et helmholtz machin dayan et bornschein et gener ial network gan goodfellow et gener adversari network deep gener model train maxim log likelihood lower bound log likelihood gan take radic differ approach doe requir infer explicit calcul data likelihood instead two model use solv minimax game tor sampl data discrimin classiﬁ data real gener theori model capabl model arbitrarili complex probabl distribut use optim discrimin given class gener origin gan propos goodfellow et al minim diverg data distribut gener extens gener thi wider class diverg nowozin et sonderbi et pool et abil train extrem ﬂexibl gener function without explicitli comput hood perform infer target diverg made gan extrem success imag gener odena et saliman et radford et imag super resolut ledig et ﬂexibl gan framework ha also enabl number success extens techniqu instanc structur tion reed et b odena et train energi base model zhao et combin gan loss mutual inform loss chen et done member googl brain resid program complet part googl brain internship may publish confer paper iclr practic howev gan suffer mani issu particularli dure train one common failur mode involv gener collaps produc onli singl sampl small famili veri similar sampl anoth involv gener discrimin oscil dure train rather converg ﬁxed point addit one agent becom much power learn signal agent becom useless system doe learn train gan mani trick must employ care select architectur radford et minibatch discrimin saliman et nois inject saliman et sonderbi et even trick set hyperparamet train success gener veri small practic onc converg gener model produc gan train procedur normal cover whole distribut dumoulin et che et even target cover diverg kl addit becaus intract comput gan train loss becaus approxim measur perform parzen window estim suffer major ﬂaw thei et evalu gan perform challeng current human judgement sampl qualiti one lead metric evalu gan practic thi metric doe take account mode drop number mode greater number sampl one visual fact mode drop problem gener help visual sampl qualiti model choos focu onli common mode common mode correspond deﬁnit typic sampl addit gener model abl alloc express power mode doe cover would attempt cover mode differenti optim mani optim scheme includ sgd rmsprop tieleman hinton adam kingma ba consist sequenc differenti updat paramet gradient backpropag unrol optim updat similar fashion backpropag recurr neural network paramet output optim thu includ differenti way anoth object maclaurin et thi idea wa ﬁrst suggest minimax problem pearlmutt siskind zhang lesser provid theoret analysi experiment result differenti singl step gradient ascent simpl matrix game differenti unrol optim wa ﬁrst scale deep network maclaurin et wa use hyperparamet optim recent belang mccallum han et andrychowicz et backpropag optim procedur context unrel gan minimax game thi work address challeng unstabl optim mode collaps gan unrol optim discrimin object dure train method gener adversari network gan learn problem ﬁnd optim paramet g gener function g z θg minimax object g argmin θg max θd f θg θd argmin θg f θg θg θg argmax θd f θg θd f commonli chosen f θg θd log x θd log g z θg θd x data variabl z latent variabl pdata data distribut discrimin θd x output estim probabl sampl x come data distribut θd θg discrimin gener paramet gener function g θg z transform sampl latent space sampl data space publish confer paper iclr minimax loss eq optim discrimin x known smooth function gener probabl pg x goodfellow et x pdata x pdata x pg x gener loss eq rewritten directli term pg x eq rather θg θg similarli smooth function pg x smooth guarante typic lost x θd g z θg drawn parametr famili nonetheless suggest true gener object eq often well behav desir target direct optim explicitli solv optim discrimin paramet θg everi updat step gener g comput infeas discrimin base neural network therefor thi minimax optim problem typic solv altern gradient descent θg ascent θd optim solut g ﬁxed point iter learn dynam alli f θg θd convex θg concav θd altern gradient descent ascent trust region updat guarante converg ﬁxed point certain addit weak tion juditski et howev practic f θg θd typic veri far convex θg concav θd updat constrain appropri way result gan train suffer mode collaps undamp oscil problem detail section order address difﬁculti introduc surrog object function fk θg θd train gener close resembl true gener object f θg θg unrol gan local optimum discrimin paramet express ﬁxed point iter optim procedur θd θk ηk df θk dθk θg lim ηk learn rate schedul clariti express eq full batch steepest gradient ascent equat sophist optim similarli unrol ment unrol adam kingma ba unrol k step creat surrog object updat gener fk θg θd f θk θg θd k thi object correspond exactli standard gan object k correspond true gener object function f θg g adjust number unrol step k thu abl interpol standard gan train dynam associ patholog costli gradient descent true gener loss paramet updat gener discrimin paramet updat use thi surrog loss θg dfk θg θd dθg θd η df θg θd dθd clariti use full batch steepest gradient descent ascent stepsiz η abov periment instead use minibatch adam updat gradient eq requir propag optim process eq clear descript differenti publish confer paper iclr θd θg θg θd θd θg sgd θd gradient θg gradient θg θd forward pass θg θd θg θd sgd unrol sgd gradient figur illustr comput graph unrol gan unrol step gener updat equat involv backpropag gener gradient blue arrow unrol optim step k unrol optim use gradient fk respect θk describ equat indic green arrow discrimin updat equat doe depend unrol optim red arrow gradient descent given algorithm maclaurin et though practic use automat differenti packag mean thi step doe need program explicitli pictori represent updat provid figur import distinguish thi approach suggest goodfellow et sever updat step discrimin paramet run befor singl updat step gener approach updat step model still gradient descent ascent respect ﬁxed valu model paramet rather surrog loss describ eq perform k step discrimin updat singl step gener updat correspond updat gener paramet θg use onli ﬁrst term eq miss gradient term better understand behavior surrog loss fk θg θd examin gradient respect gener paramet θg dfk θg θd dθg θk θg θd θk θg θd θg θd dθk θg θd dθg standard gan train correspond exactli updat gener paramet use onli ﬁrst term thi gradient θk θg θd paramet result discrimin updat step optim gener ani ﬁxed discrimin delta function x discrimin assign highest data probabl therefor standard gan train gener updat step partial collaps toward delta function second term captur discrimin would react chang gener reduc tendenc gener engag mode collaps instanc second term reﬂect gener collaps toward delta function discrimin react assign lower probabl state increas gener loss therefor discourag gener collaps may improv stabil k θk goe local optimum f therefor second term eq goe danskin gradient unrol surrog loss fk θg θd respect θg thu ident gradient standard gan loss f θg θd k k take k impli standard gan discrimin also fulli optim gener updat two extrem fk θg θd captur addit inform respons discrimin chang gener publish confer paper iclr consequ surrog loss gan thought game discrimin gener g agent take turn take action updat paramet nash equilibrium reach optim action evalu probabl ratio pdata x pg x x gener move x eq optim gener action move mass maxim thi ratio initi move g move much mass parametr famili updat step permit singl point maxim ratio probabl densiti action take quit simpl track point extent allow parametr famili updat step assign low data probabl uniform probabl everywher els thi cycl g move follow repeat forev converg depend rate chang two agent thi similar situat simpl matrix game like match penni altern gradient descent ascent ﬁxed learn rate known converg singh et bowl veloso unrol case howev thi undesir behavior longer occur g action take account respond particular g tri make step hard time respond thi extra inform help gener spread mass make next step less effect instead collaps point principl surrog loss function could use case unrol optim thi known lead converg game gradient descent ascent fail zhang lesser howev motiv use surrog gener loss section unrol inner two nest min max function doe appli use surrog discrimin loss addit common discrimin overpow gener train gan give inform g allow see futur may thu help two model balanc experi thi section demonstr improv mode coverag stabil appli thi techniqu ﬁve dataset increas complex evalu gener model notori hard problem thei et de facto standard gan literatur ha becom sampl qualiti evalu human evalu heurist incept score exampl saliman et evalu metric reason job captur sampl qualiti fail captur sampl divers ﬁrst experi divers easili evalu via visual inspect later experi thi case use varieti method quantifi coverag sampl measur individu strongli suggest unrol reduc improv stabil none alon conclus believ taken togeth howev provid extrem compel evid advantag unrol stochast optim must choos minibatch use unrol updat eq experi ﬁxed minibatch minibatch unrol step found signiﬁcantli impact result use ﬁxed minibatch experi thi section provid refer implement thi techniqu gan mixtur gaussian dataset illustr impact discrimin unrol train simpl gan architectur mixtur gaussian arrang circl detail list architectur hyperparamet see appendix figur show dynam thi model time without unrol gener rotat around valid mode data distribut never abl spread mass ad unrol step g quickli learn spread probabl mass system converg data distribut appendix b perform experi thi toy dataset explor unrol compar histor averag compar use unrol discrimin updat publish confer paper iclr figur unrol discrimin stabil gan train toy mixtur gaussian dataset column show heatmap gener distribut increas number train step ﬁnal column show data distribut top row show train gan unrol step gener quickli spread converg target distribut bottom row show standard gan train gener rotat mode data distribut never converg ﬁxed distribut onli ever assign signiﬁc probabl mass singl data mode onc figur unrol gan train increas stabil rnn gener convolut crimin train mnist top row wa run unrol step bottom row standard gan unrol step imag sampl gener indic number train step gener without backpropag gener case ﬁnd unrol object perform better patholog model mismatch gener discrimin evalu abil thi approach improv trainabl look tradit challeng famili model train recurr neural network rnn thi experi tri gener mnist sampl use lstm hochreit schmidhub mnist digit pixel imag timestep gener lstm output one column thi imag timestep ha output entir sampl use convolut neural network discrimin see appendix c full model train detail unlik previous success gan model symmetri gener discrimin thi task result complex power balanc result seen figur onc without unrol model quickli collaps rotat sequenc singl mode instead rotat spatial cycl like blob run unrol step gener dispers appear cover whole data distribut exampl publish confer paper iclr discrimin size unrol step size compar g mode gener kl model size compar g mode gener kl model tabl unrol gan cover discret mode model dataset data mode correspond combin three mnist digit digit combin number mode cover given differ number unrol step two differ architectur revers kl diverg model data also given standard error provid measur mode manifold collaps use augment mnist gan suffer two differ type model collaps collaps subset data mode collaps within data distribut experi isol effect use artiﬁci construct dataset demonstr unrol larg rescu type collaps discret mode collaps explor degre gan drop discret mode dataset use techniqu similar one che et construct dataset stack three randomli chosen mnist digit construct rgb imag differ mnist digit color channel thi new dataset ha distinct mode correspond combin ten mnist class three channel train gan thi dataset gener sampl train model sampl experi comput predict class label color channel use mnist classiﬁ evalu perform use two metric number mode gener produc least one sampl kl diverg model expect data distribut within thi discret label space kl diverg estim tractabl tween gener sampl data distribut class data distribut uniform distribut class present tabl number unrol step increas mode coverag vers kl diverg improv contrari che et found reason size model one use section cover mode even without unrol use smaller convolut gan model detail model use provid appendix observ addit interest effect thi experi beneﬁt unrol increas discrimin size reduc believ unrol effect increas capac discrimin unrol discrimin better react ani speciﬁc way gener produc sampl discrimin weak posit impact unrol thu larger manifold collaps addit discret mode examin effect unrol model continu fold get thi quantiti construct dataset consist color mnist digit unlik previou experi singl mnist digit wa chosen assign singl matic color perfect gener one abl recov distribut color use gener digit use color mnist digit gener also ha model digit make task sufﬁcient complex gener unabl perfectli solv color digit sampl normal distribut detail thi dataset provid appendix examin distribut color sampl gener train gan also true exampl section lack divers ate color almost invis use onli visual inspect sampl sampl found appendix publish confer paper iclr unrol step js diverg layer size js diverg layer size js diverg layer size tabl unrol gan better model continu distribut gan train model domli color mnist digit color drawn gaussian distribut js genc data model distribut digit color report along standard error js diverg unrol step larger model lead better js diverg step step step step figur visual percept sampl qualiti divers veri similar model train differ number unrol step actual sampl divers higher unrol step pane show sampl gener train model step unrol order recov color gan assign digit use cluster pick foreground color background perform thi transform train data gener imag next ﬁt gaussian kernel densiti estim distribut digit color final comput js diverg model data distribut color result found tabl sever model size detail model provid appendix gener best perform model unrol step larger model perform better smaller model take unrol step seem hurt thi measur divers suspect thi due introduc oscillatori dynam train take unrol step howev lead improv perform unrol imag model test techniqu tradit convolut gan architectur task similar use radford et saliman et previou experi test model standard gan train algorithm would converg thi section improv standard model reduc tendenc engag mode collaps ran conﬁgur thi model vari number unrol step conﬁgur wa run time differ random seed full train detail see appendix sampl conﬁgur found figur obviou differ visual qualiti across model conﬁgur visual inspect howev provid onli poor measur sampl divers train unrol discrimin expect gener divers sampl close resembl underli data distribut introduc two techniqu examin sampl divers infer via optim pairwis distanc distribut publish confer paper iclr unrol step step step step step averag mse percent best rank tabl gan train unrol better abl match imag train set standard gan like due mode drop standard gan result show mse train imag best reconstruct model given number unrol step fraction train imag best reconstruct given model given ﬁnal column best reconstruct found optim latent represent z produc closest match pixel output g z θg result averag run model differ random seed infer via optim sinc likelihood tractabl comput gan typic test take sampl comput imag pixel space train data low et revers measur abil gener model gener imag look like speciﬁc sampl train data thi gener random sampl model would need exponenti larg number sampl instead treat ﬁnding nearest neighbor xnearest target imag xtarget optim task znearest argmin z z θg xnearest g znearest θg thi concept backpropag gener imag ha wide use visual featur discrimin network simonyan et yosinski et nguyen et ha appli explor visual manifold gan zhu et appli thi techniqu model train optim random start use lbfg optim typic use similar set style transfer johnson et champandard result compar averag mean squar error xnearest xtarget pixel space found tabl addit comput percent imag certain conﬁgur achiev lowest loss compar conﬁgur zero step case poor reconstruct less time doe obtain lowest error conﬁgur take unrol step result signiﬁc improv mse take unrol step result modest improv continu reduc reconstruct mse visual see thi compar result optim process step conﬁgur figur select imag differ behavior appar sort data absolut valu fraction differ mse step model thi highlight exampl either step model accur ﬁt data exampl appendix g show comparison model initi use differ random seed mani zero step imag fuzzi deﬁn suggest imag gener standard gan gener model come drop mode unrol step ad outlin becom clear well deﬁn model cover distribut thu recreat sampl pairwis distanc second complementari approach compar statist data sampl correspond statist sampl gener variou model one particularli simpl relev statist distribut pairwis distanc random pair sampl case mode collaps greater probabl mass concentr smaller volum distribut distanc skew toward smaller distanc sampl random pair imag model well train data comput histogram distanc sampl pair illustr figur standard gan zero unrol step ha probabl mass skew toward smaller intersampl distanc compar publish confer paper iclr data step step step step data step step step step figur train set imag accur reconstruct use gan train unrol standard step gan like due mode drop standard gan raw data left optim imag reach thi target follow unrol step reconstruct mse list sampl random imag select train set correspond best reconstruct model found via tion shown eight imag largest absolut fraction differ gan train unrol step real data number unrol step increas histogram intersampl distanc increasingli come resembl data distribut thi evid support unrol decreas mode collaps behavior gan discuss thi work develop method stabil gan train reduc mode collaps deﬁn gener object respect unrol optim discrimin strate applic thi method sever task either rescu unstabl train reduc tendenc model drop region data distribut main drawback thi method comput cost train step increas linearli number unrol step tradeoff better approxim true gener loss comput requir make thi estim depend architectur one unrol step enough unstabl model rnn case need stabil train initi posit result suggest may sufﬁcient perturb train gradient direct singl unrol step perturb thi comput efﬁcient investig requir method present bridg gap theoret practic result train gan believ develop better updat rule gener discrimin import line work gan train thi work onli consid small fraction design space instanc approach could extend unrol g updat well let discrimin react gener would move also possibl unrol sequenc g updat thi would make updat recurs g could react maxim perform g alreadi updat acknowledg would like thank laurent dinh david dohan vincent dumoulin liam fedu ishaan rajani julian ibarz eric jang matthew johnson marc lanctot augustu odena gabriel pereyra publish confer paper iclr pairwis norm distribut data step data step data step norm data step probabl densiti figur number unrol step gan train increas distribut pairwis distanc model sampl close resembl distribut data plot histogram pairwis distanc randomli select sampl red line give pairwis distanc data ﬁve blue line plot repres model train differ random seed vertic line median distribut colin raffel sam schoenholz ayush sekhari jon shlen dale schuurman insight convers well rest googl brain team publish confer paper iclr refer guillaum alain yoshua bengio li yao jason yosinski eric saizheng zhang pascal vincent gsn gener stochast network arxiv preprint marcin andrychowicz misha denil sergio gomez matthew w hoffman david pfau tom schaul nando de freita learn learn gradient descent gradient descent arxiv preprint david belang andrew mccallum structur predict energi network arxiv preprint jorg bornschein samira shabanian asja fischer yoshua bengio bidirect helmholtz machin arxiv preprint michael bowl manuela veloso multiag learn use variabl learn rate artiﬁci intellig yuri burda roger gross ruslan salakhutdinov import weight autoencod arxiv preprint alex champandard semant style transfer turn doodl ﬁne artwork arxiv preprint tong che yanran li athul paul jacob yoshua bengio wenji li mode regular gener adversari network arxiv preprint arxiv xi chen yan duan rein houthooft john schulman ilya sutskev pieter abbeel gan interpret represent learn inform maxim gener adversari net arxiv preprint john danskin theori applic weapon alloc problem ume springer scienc busi media peter dayan geoffrey e hinton radford neal richard zemel helmholtz machin neural comput laurent dinh david krueger yoshua bengio nice independ compon mation arxiv preprint laurent dinh jascha sami bengio densiti estim use real nvp arxiv preprint vincent dumoulin ishmael belghazi ben pool alex lamb martin arjovski olivi etro aaron courvil adversari learn infer arxiv preprint xavier glorot yoshua bengio understand difﬁculti train deep feedforward neural network jmlr w cp proceed thirteenth intern confer artiﬁci intellig statist aistat volum pp may ian goodfellow jean mehdi mirza bing xu david sherjil ozair aaron courvil yoshua bengio gener adversari net ghahramani well cort lawrenc weinberg ed advanc neural inform cess system pp curran associ url http karol gregor ivo danihelka alex grave daan wierstra draw recurr neural network imag gener proceed intern confer machin ing pp url http tian han yang lu zhu ying nian wu altern gener network url http publish confer paper iclr sepp hochreit urgen schmidhub long memori neural novemb issn doi url http sergey ioff christian szegedi batch normal acceler deep network train reduc intern covari shift proceed intern confer machin learn icml lill franc juli pp url http justin johnson alexandr alahi li perceptu loss style transfer arxiv preprint anatoli juditski arkadi nemirovski et al first order method nonsmooth convex optim gener purpos method optim machin learn pp diederik kingma jimmi ba adam method stochast optim arxiv preprint diederik p kingma max well variat bay url http diederik kingma tim saliman max well improv variat infer invers autoregress ﬂow teja kulkarni whitney pushmeet kohli joshua tenenbaum deep convolut invers graphic network arxiv preprint christian ledig luca thei ferenc huszar jose caballero andrew aitken alykhan tejani hann totz zehan wang wenzh shi singl imag use gener adversari network url http dougal maclaurin david duvenaud ryan adam hyperparamet tion revers learn anh nguyen alexey dosovitskiy jason yosinski thoma brox jeff clune synthes prefer input neuron neural network via deep gener network arxiv preprint sebastian nowozin botond cseke ryota tomioka train gener neural sampler use variat diverg minim arxiv preprint augustu odena christoph olah jonathon shlen condit imag synthesi iari classiﬁ gan arxiv preprint barak pearlmutt jeffrey mark siskind ad function framework lambda ultim backpropag acm tran program lang march issn doi url http ben pool alexand alemi jascha anelia angelova improv gener object gan arxiv preprint alec radford luke metz soumith chintala unsupervis represent learn deep convolut gener adversari network arxiv preprint scott reed zeynep akata santosh mohan samuel tenka bernt schiel honglak lee ing draw nip scott reed zeynep akata xinchen yan lajanugen logeswaran bernt schiel honglak lee gener adversari synthesi proceed intern enc machin learn publish confer paper iclr danilo jimenez rezend shakir moham daan wierstra stochast backpropag variat infer deep latent gaussian model intern confer machin learn cites tim saliman ian goodfellow wojciech zaremba vicki cheung alec radford xi chen improv techniqu train gan arxiv preprint karen simonyan andrea vedaldi andrew zisserman deep insid convolut network sualis imag classiﬁc model salienc map arxiv preprint satind singh michael kearn yishay mansour nash converg gradient dynam game proceed sixteenth confer uncertainti artiﬁci intellig pp morgan kaufmann publish jascha eric weiss niru maheswaranathan surya ganguli deep vise learn use nonequilibrium thermodynam proceed intern confer machin learn pp url http casper kaae sonderbi jose caballero luca thei wenzh shi ferenc huszar amortis map infer imag url http thei bethg gener imag model use spatial lstm advanc ral inform process system dec url http thei van den oord bethg note evalu gener model ternat confer learn represent apr url http tieleman hinton lectur divid gradient run averag recent magnitud coursera neural network machin learn aron van den oord nal kalchbrenn koray kavukcuoglu pixel recurr neural network arxiv preprint url http aron van den oord nal kalchbrenn oriol vinyal lass espeholt alex grave ray kavukcuoglu condit imag gener pixelcnn decod arxiv preprint pascal vincent hugo larochel isabel lajoi yoshua bengio manzagol stack denois autoencod learn use represent deep network local denois criterion mach learn decemb issn url http jason yosinski jeff clune anh nguyen thoma fuch hod lipson understand neural network deep visual arxiv preprint chongji zhang victor r lesser learn polici predict proceed aaai confer artiﬁci intellig junbo zhao michael mathieu yann lecun gener adversari network arxiv preprint zhu philipp uhl eli shechtman alexei efro gener visual tion natur imag manifold proceed european confer comput vision eccv publish confer paper iclr appendix gaussian train detail network architectur experiment detail experi section follow dataset sampl mixtur gaussian standard deviat mean equal space around circl radiu gener network consist fulli connect network hidden layer size relu activ follow linear project dimens weight initi orthogon scale discrimin network ﬁrst scale input factor roughli scale follow layer fulli connect network relu activ linear layer size act logit gener minim lg log x log g z discrimin minim ld x g z x sampl data distribut z n network optim use adam kingma ba learn rate network train altern updat gener discrimin one step consist either g updat b mixtur gaussian experi effect time delay histor averag anoth comparison look wa regard histor averag base approach centli similarli inspir approach use saliman et stabil train studi look take ensembl discrimin time first look take ensembl last n step shown figur updat step number ensembl figur histor averag doe visibl increas stabil mixtur gaussian task row correspond ensembl discrimin consist indic number immedi preced discrimin column correspond differ number train step explor thi idea ran experi ensembl discrimin differ period replac discrimin ensembl exampl sampl rate would take step replac discrimin result seen figur observ given longer longer time delay model becom less less stabl hypothes thi due initi shape discrimin loss surfac train discrimin estim probabl densiti onli accur region wa train ﬁxing thi discrimin remov feedback gener exploit publish confer paper iclr updat step step ensembl figur introduc longer time delay discrimin ensembl result biliti probabl distribut window visual x axi number weight updat axi mani step skip discrimin updat select ensembl discrimin discrimin abil move result gener abl exploit ﬁxed area poor perform older discrimin ensembl new discrimin compens thi lead system diverg effect second gradient second factor analyz effect backpropag learn signal roll equat turn thi backpropag unrol troduc stop gradient call comput graph unrol step stop gradient place updat signal correspond onli ﬁrst term equat look conﬁgur without stop gradient vanilla unrol gan stop gradient stop gradient take averag k unrol step instead take ﬁnal valu result see figur initi observ differ unrol without second gradient requir unrol step becom stabl discrimin unrol converg second gradient term becom zero due simplic problem suspect discrimin nearli converg everi gener step second gradient term wa thu irrelev test thi modiﬁ dynam perform ﬁve gener step discrimin updat result shown figur discrimin kept equilibrium success train achiev half mani unrol step use term gradient onli includ ﬁrst term c rnn mnist train detail network architectur experi section follow mnist dataset scale gener ﬁrst scale nois vector unit fulli connect layer relu activ thi fed initi state lstm hochreit schmidhub run step correspond number column mnist result sequenc tivat project fulli connect layer output tanh activ function weight initi via xavier initi glorot bengio forget bia lstm initi discrimin network feed input convolut follow lution follow convolut convolut stride radford et leaki rectiﬁ use leak batch normal appli layer ioff szegedi result tensor ﬂatten linear project perform singl scalar publish confer paper iclr unrol gan updat step unrol step unrol gan without second gradient updat step unrol step figur discrimin remain nearli optimum dure learn perform nearli ident without second gradient term equat shown figur discrimin lag behind gener backpropag unrol aid converg gener network minimis lg log g z discrimin minim ld log x log g z network train adam kingma ba learn rate network train altern updat gener discrimin step one step consist network updat train detail network architectur discrimin gener encod follow tion kernel size batch normal discrimin use leaki relu leak gener use standard relu gener network deﬁn number output stride input z fulli connect reshap imag transpos convolut transpos convolut transpos convolut convolut publish confer paper iclr unrol gan g step per updat step unrol step unrol gan g step per without second gradient updat step unrol step figur backpropag unrol process aid converg crimin doe fulli converg gener updat take gener step per discrimin step unrol greatli increas stabil requir onli unrol step converg without second gradient requir unrol step also see figur discrimin network deﬁn number output stride input x g convolut convolut convolut flatten fulli connect gener network minimis lg log g z discrimin minim ld log x g z network train adam gener learn rate discrimin learn rate network train altern updat gener discrimin step one step consist network updat publish confer paper iclr e class mnist number output stride input z fulli connect reshap imag transpos convolut transpos convolut transpos convolut convolut discrimin network parametr size x deﬁn follow test use x number output stride input x g convolut x convolut x convolut x flatten fulli connect f color mnist dataset dataset gener thi dataset ﬁrst took mnist digit scale imag sampl color c normal distribut gener color digit final add small amount pixel independ nois sampl normal distribut result valu clipe visual thi gener imag sampl seen ﬁgure onc veri hard visual see differ sampl divers compar size model figur right sampl data distribut middl sampl size model look ahead step worst divers left sampl size model look ahead step divers model model use thi section parametr variabl x control capac valu architectur use experi use valu gener network deﬁn publish confer paper iclr number output stride input z fulli connect x reshap imag x transpos convolut x transpos convolut x transpos convolut x convolut discrimin network deﬁn number output stride input x g convolut x convolut x convolut x flatten fulli connect g optim base visual exampl model base optim perform run differ seed unrol step conﬁgur bellow comparison run index ideal thi would mani mani comparison space efﬁcienc group run index run publish confer paper iclr data step step step step data step step step step figur sampl differ random seed publish confer paper iclr data step step step step data step step step step figur sampl differ random seed publish confer paper iclr data step step step step data step step step step figur sampl differ random seed publish confer paper iclr data step step step step data step step step step figur sampl differ random seed publish confer paper iclr data step step step step data step step step step figur sampl differ random seed