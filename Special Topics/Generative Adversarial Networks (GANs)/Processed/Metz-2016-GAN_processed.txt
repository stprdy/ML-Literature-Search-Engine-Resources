published conference paper iclr 2017 unrolled generative adversarial network luke google brain lmetz ben stanford university poole david pfau google deepmind pfau jascha google brain jaschasd abstract introduce method stabilize generative adversarial network gans deﬁning generator objective respect unrolled optimization discriminator allows training adjusted using optimal criminator generator objective ideal infeasible practice using current value discriminator often unstable lead poor solution show technique solves common problem mode collapse stabilizes training gans complex recurrent generator increase diversity coverage data distribution generator 1 introduction use deep neural network generative model complex data ha made great advance recent year success ha achieved surprising diversity training loss model architecture including denoising autoencoders vincent et 2010 variational toencoders kingma welling 2013 rezende et 2014 gregor et 2015 kulkarni et 2015 burda et 2015 kingma et 2016 generative stochastic network alain et 2015 diffusion probabilistic model et 2015 autoregressive model theis bethge 2015 van den oord et b real preserving transformation dinh et 2014 2016 helmholtz machine dayan et 1995 bornschein et 2015 generative ial network gans goodfellow et 2014 generative adversarial network deep generative model trained maximizing log likelihood lower bound log likelihood gans take radically different approach doe not require inference explicit calculation data likelihood instead two model used solve minimax game tor sample data discriminator classiﬁes data real generated theory model capable modeling arbitrarily complex probability distribution using optimal discriminator given class generator original gan proposed goodfellow et al minimizes divergence data distribution generator extension generalize wider class divergence nowozin et 2016 sonderby et 2016 poole et 2016 ability train extremely ﬂexible generating function without explicitly computing hood performing inference targeting divergence made gans extremely successful image generation odena et 2016 salimans et 2016 radford et 2015 image super resolution ledig et 2016 ﬂexibility gan framework ha also enabled number successful extension technique instance structured tion reed et b odena et 2016 training energy based model zhao et 2016 combining gan loss mutual information loss chen et 2016 done member google brain residency program completed part google brain internship 1 12 may 2017 published conference paper iclr 2017 practice however gans suffer many issue particularly training one common failure mode involves generator collapsing produce only single sample small family similar sample another involves generator discriminator oscillating training rather converging ﬁxed point addition one agent becomes much powerful learning signal agent becomes useless system doe not learn train gans many trick must employed careful selection architecture radford et 2015 minibatch discrimination salimans et 2016 noise injection salimans et 2016 sonderby et 2016 even trick set hyperparameters training successful generally small practice converged generative model produced gan training procedure normally not cover whole distribution dumoulin et 2016 che et 2016 even targeting covering divergence kl additionally intractable compute gan training loss approximate measure performance parzen window estimate suffer major ﬂaws theis et 2016 evaluation gan performance challenging currently human judgement sample quality one leading metric evaluating gans practice metric doe not take account mode dropping number mode greater number sample one visualizing fact mode dropping problem generally help visual sample quality model choose focus only common mode common mode correspond deﬁnition typical sample additionally generative model able allocate expressive power mode doe cover would attempted cover mode differentiating optimization many optimization scheme including sgd rmsprop tieleman hinton 2012 adam kingma ba 2014 consist sequence differentiable update parameter gradient backpropagated unrolled optimization update similar fashion backpropagation recurrent neural network parameter output optimizer thus included differentiable way another objective maclaurin et 2015 idea wa ﬁrst suggested minimax problem pearlmutter siskind 2008 zhang lesser 2010 provided theoretical analysis experimental result differentiating single step gradient ascent simple matrix game differentiating unrolled optimization wa ﬁrst scaled deep network maclaurin et 2015 wa used hyperparameter optimization recently belanger mccallum 2015 han et 2016 andrychowicz et 2016 backpropagate optimization procedure context unrelated gans minimax game work address challenge unstable optimization mode collapse gans unrolling optimization discriminator objective training 2 method generative adversarial network gan learning problem ﬁnd optimal parameter g generator function g z θg minimax objective g argmin θg max θd f θg θd 1 argmin θg f θg θg 2 θg argmax θd f θg θd 3 f commonly chosen f θg θd log x θd 0 log 1 g z θg θd 4 x data variable z latent variable pdata data distribution discriminator θd x 0 1 output estimated probability sample x come data distribution θd θg discriminator generator parameter generator function g θg z transforms sample latent space sample data space 2 published conference paper iclr 2017 minimax loss eq 4 optimal discriminator x known smooth function generator probability pg x goodfellow et 2014 x pdata x pdata x pg x 5 generator loss eq 2 rewritten directly term pg x eq 5 rather θg θg similarly smooth function pg x smoothness guarantee typically lost x θd g z θg drawn parametric family nonetheless suggest true generator objective eq 2 often well behaved desirable target direct optimization explicitly solving optimal discriminator parameter θg every update step generator g computationally infeasible discriminator based neural network therefore minimax optimization problem typically solved alternating gradient descent θg ascent θd optimal solution g ﬁxed point iterative learning dynamic ally f θg θd convex θg concave θd alternating gradient descent ascent trust region update guaranteed converge ﬁxed point certain additional weak tions juditsky et 2011 however practice f θg θd typically far convex θg concave θd update not constrained appropriate way result gan training suffers mode collapse undamped oscillation problem detailed section order address difﬁculties introduce surrogate objective function fk θg θd training generator closely resembles true generator objective f θg θg unrolling gans local optimum discriminator parameter expressed ﬁxed point iterative optimization procedure θd 6 θk ηk df θk dθk 7 θg lim 8 ηk learning rate schedule clarity expressed eq 7 full batch steepest gradient ascent equation sophisticated optimizers similarly unrolled ments unroll adam kingma ba 2014 unrolling k step create surrogate objective update generator fk θg θd f θk θg θd 9 k 0 objective corresponds exactly standard gan objective k corresponds true generator objective function f θg g adjusting number unrolling step k thus able interpolate standard gan training dynamic associated pathology costly gradient descent true generator loss parameter update generator discriminator parameter update using surrogate loss θg dfk θg θd dθg 10 θd η df θg θd dθd 11 clarity use full batch steepest gradient descent ascent stepsize η periments instead use minibatch adam update gradient eq 10 requires propagating optimization process eq clear description differentiation 3 published conference paper iclr 2017 θd θg θg θd θd 1 θg sgd θd gradient θg gradient θg θd forward pas θg θd θg θd 2 sgd unrolling sgd gradient figure 1 illustration computation graph unrolled gan 3 unrolling step generator update equation 10 involves backpropagating generator gradient blue arrow unrolled optimization step k unrolled optimization us gradient fk respect θk described equation 7 indicated green arrow discriminator update equation 11 doe not depend unrolled optimization red arrow gradient descent given algorithm 2 maclaurin et 2015 though practice use automatic differentiation package mean step doe not need programmed explicitly pictorial representation update provided figure important distinguish approach suggested goodfellow et 2014 several update step discriminator parameter run single update step generator approach update step model still gradient descent ascent respect ﬁxed value model parameter rather surrogate loss describe eq performing k step discriminator update single step generator update corresponds updating generator parameter θg using only ﬁrst term eq 12 missing gradient term better understand behavior surrogate loss fk θg θd examine gradient respect generator parameter θg dfk θg θd dθg θk θg θd θk θg θd θg θd dθk θg θd dθg 12 standard gan training corresponds exactly updating generator parameter using only ﬁrst term gradient θk θg θd parameter resulting discriminator update step optimal generator any ﬁxed discriminator delta function x discriminator assigns highest data probability therefore standard gan training generator update step partial collapse towards delta function second term capture discriminator would react change generator reduces tendency generator engage mode collapse instance second term reﬂects generator collapse towards delta function discriminator reacts assigns lower probability state increasing generator loss therefore discourages generator collapsing may improve stability k θk go local optimum f 0 therefore second term eq 12 go 0 danskin 1967 gradient unrolled surrogate loss fk θg θd respect θg thus identical gradient standard gan loss f θg θd k 0 k take k imply standard gan discriminator also fully optimized generator update two extreme fk θg θd capture additional information response discriminator change generator 4 published conference paper iclr 2017 consequence surrogate loss gans thought game discriminator generator g agent take turn taking action updating parameter nash equilibrium reached optimal action evaluate probability ratio pdata x pg x x generator move x eq 5 optimal generator action move mass maximize ratio initial move g move much mass parametric family update step permit single point maximizes ratio probability density action take quite simple track point extent allowed parametric family update step assign low data probability uniform probability everywhere else cycle g moving following repeat forever converge depending rate change two agent similar situation simple matrix game like matching penny alternating gradient descent ascent ﬁxed learning rate known not converge singh et 2000 bowling veloso 2002 unrolled case however undesirable behavior no longer occurs g action take account respond particular g try make step hard time responding extra information help generator spread mass make next step le effective instead collapsing point principle surrogate loss function could used case unrolled optimization known lead convergence game gradient descent ascent fails zhang lesser 2010 however motivation using surrogate generator loss section unrolling inner two nested min max function doe not apply using surrogate discriminator loss additionally common discriminator overpower generator training gan giving information g allowing see future may thus help two model balanced 3 experiment section demonstrate improved mode coverage stability applying technique ﬁve datasets increasing complexity evaluation generative model notoriously hard problem theis et 2016 de facto standard gan literature ha become sample quality evaluated human evaluated heuristic inception score example salimans et 2016 evaluation metric reasonable job capturing sample quality fail capture sample diversity ﬁrst 2 experiment diversity easily evaluated via visual inspection later experiment not case use variety method quantify coverage sample measure individually strongly suggestive unrolling reducing improving stability none alone conclusive believe taken together however provide extremely compelling evidence advantage unrolling stochastic optimization must choose minibatches use unrolling update eq experimented ﬁxed minibatch minibatches unrolling step found not signiﬁcantly impact result use ﬁxed minibatches experiment section provide reference implementation technique gan mixture gaussians dataset illustrate impact discriminator unrolling train simple gan architecture mixture 8 gaussians arranged circle detailed list architecture hyperparameters see appendix figure 2 show dynamic model time without unrolling generator rotates around valid mode data distribution never able spread mass adding unrolling step g quickly learns spread probability mass system converges data distribution appendix b perform experiment toy dataset explore unrolling compare historical averaging compare using unrolled discriminator update 5 published conference paper iclr 2017 figure 2 unrolling discriminator stabilizes gan training toy mixture gaussians dataset column show heatmap generator distribution increasing number training step ﬁnal column show data distribution top row show training gan 10 unrolling step generator quickly spread converges target distribution bottom row show standard gan training generator rotates mode data distribution never converges ﬁxed distribution only ever assigns signiﬁcant probability mass single data mode figure 3 unrolled gan training increase stability rnn generator convolutional criminator trained mnist top row wa run 20 unrolling step bottom row standard gan 0 unrolling step image sample generator indicated number training step generator without backpropagating generator case ﬁnd unrolled objective performs better pathological model mismatched generator discriminator evaluate ability approach improve trainability look traditionally challenging family model train recurrent neural network rnns experiment try generate mnist sample using lstm hochreiter schmidhuber 1997 mnist digit pixel image timestep generator lstm output one column image 28 timesteps ha output entire sample use convolutional neural network discriminator see appendix c full model training detail unlike previously successful gan model no symmetry generator discriminator task resulting complex power balance result seen figure without unrolling model quickly collapse rotates sequence single mode instead rotating spatially cycle like blob running unrolling step generator disperses appears cover whole data distribution example 6 published conference paper iclr 2017 discriminator size unrolling step 0 1 5 10 size compared g mode generated kl model size compared g mode generated kl model table 1 unrolled gans cover discrete mode modeling dataset data mode corresponding combination three mnist digit 103 digit combination number mode covered given different number unrolling step two different architecture reverse kl divergence model data also given standard error provided measure mode manifold collapse using augmented mnist gans suffer two different type model collapse collapse subset data mode collapse within data distribution experiment isolate effect using artiﬁcially constructed datasets demonstrate unrolling largely rescue type collapse discrete mode collapse explore degree gans drop discrete mode dataset use technique similar one che et 2016 construct dataset stacking three randomly chosen mnist digit construct rgb image different mnist digit color channel new dataset ha distinct mode corresponding combination ten mnist class three channel train gan dataset generate sample trained model sample experiment compute predicted class label color channel using mnist classiﬁer evaluate performance use two metric number mode generator produced least one sample kl divergence model expected data distribution within discrete label space kl divergence estimated tractably tween generated sample data distribution class data distribution uniform distribution class presented table 1 number unrolling step increased mode coverage verse kl divergence improve contrary che et 2016 found reasonably sized model one used section covered mode even without unrolling use smaller convolutional gan model detail model used provided appendix observe additional interesting effect experiment beneﬁts unrolling increase discriminator size reduced believe unrolling effectively increase capacity discriminator unrolled discriminator better react any speciﬁc way generator producing sample discriminator weak positive impact unrolling thus larger manifold collapse addition discrete mode examine effect unrolling modeling continuous fold get quantity constructed dataset consisting colored mnist digit unlike previous experiment single mnist digit wa chosen assigned single matic color perfect generator one able recover distribution color used generate digit use colored mnist digit generator also ha model digit make task sufﬁciently complex generator unable perfectly solve color digit sampled normal distribution detail dataset provided appendix examine distribution color sample generated trained gan also true example section lack diversity ated color almost invisible using only visual inspection sample sample found appendix 7 published conference paper iclr 2017 unrolling step 0 1 5 10 j divergence layer size j divergence layer size j divergence layer size table 2 unrolled gans better model continuous distribution gans trained model domly colored mnist digit color drawn gaussian distribution j gence data model distribution digit color reported along standard error j divergence unrolling step larger model lead better j divergence 0 step 1 step 5 step 10 step figure 4 visual perception sample quality diversity similar model trained different number unrolling step actual sample diversity higher unrolling step pane show sample generated training model 0 1 5 10 step unrolling order recover color gan assigned digit used 2 cluster pick foreground color background performed transformation training data generated image next ﬁt gaussian kernel density estimator distribution digit color finally computed j divergence model data distribution color result found table 2 several model size detail model provided appendix general best performing model unrolled step larger model perform better smaller model taking 1 unrolling step seems hurt measure diversity suspect due introducing oscillatory dynamic training taking unrolling step however lead improved performance unrolling image modeling test technique traditional convolutional gan architecture task similar used radford et 2015 salimans et 2016 previous experiment tested model standard gan training algorithm would not converge section improve standard model reducing tendency engage mode collapse ran 4 conﬁgurations model varying number unrolling step 0 1 5 conﬁguration wa run 5 time different random seed full training detail see appendix sample 4 conﬁgurations found figure no obvious difference visual quality across model conﬁgurations visual inspection however provides only poor measure sample diversity training unrolled discriminator expect generate diverse sample closely resemble underlying data distribution introduce two technique examine sample diversity inference via optimization pairwise distance distribution 8 published conference paper iclr 2017 unrolling step 0 step 1 step 5 step 10 step average mse percent best rank table 3 gans trained unrolling better able match image training set standard gans likely due mode dropping standard gan result show mse training image best reconstruction model given number unrolling step fraction training image best reconstructed given model given ﬁnal column best reconstruction found optimizing latent representation z produce closest matching pixel output g z θg result averaged 5 run model different random seed inference via optimization since likelihood not tractably computed gans typically tested taking sample computing image pixel space training data low et 2014 reverse measure ability generative model generate image look like speciﬁc sample training data generating random sample model would need exponentially large number sample instead treat ﬁnding nearest neighbor xnearest target image xtarget optimization task znearest argmin z z θg 2 13 xnearest g znearest θg 14 concept backpropagating generate image ha widely used visualizing feature discriminative network simonyan et 2013 yosinski et 2015 nguyen et 2016 ha applied explore visual manifold gans zhu et 2016 apply technique model trained optimize 3 random start using lbfgs optimizer typically used similar setting style transfer johnson et 2016 champandard 2016 result comparing average mean squared error xnearest xtarget pixel space found table addition compute percent image certain conﬁguration achieves lowest loss compared conﬁgurations zero step case poor reconstruction le 1 time doe obtain lowest error 4 conﬁgurations taking 1 unrolling step result signiﬁcant improvement mse taking 10 unrolling step result modest improvement continues reduce reconstruction mse visually see compare result optimization process 0 1 5 10 step conﬁgurations figure select image difference behavior apparent sort data absolute value fractional difference mse 0 10 step model 1 2 highlight example either 0 10 step model not accurately ﬁt data example appendix g show comparison model initialized using different random seed many zero step image fuzzy deﬁned suggesting image not generated standard gan generative model come dropped mode unrolling step added outline become clear well deﬁned model cover distribution thus recreate sample pairwise distance second complementary approach compare statistic data sample corresponding statistic sample generated various model one particularly simple relevant statistic distribution pairwise distance random pair sample case mode collapse greater probability mass concentrated smaller volume distribution distance skewed towards smaller distance sample random pair image model well training data compute histogram distance sample pair illustrated figure 6 standard gan zero unrolling step ha probability mass skewed towards smaller intersample distance compared 9 published conference paper iclr 2017 data 0 step 1 step 5 step 10 step data 0 step 1 step 5 step 10 step figure 5 training set image accurately reconstructed using gans trained unrolling standard 0 step gan likely due mode dropping standard gan raw data left optimized image reach target follow 0 1 5 10 unrolling step reconstruction mse listed sample random 1280 image selected training set corresponding best reconstruction model found via tion shown eight image largest absolute fractional difference gans trained 0 10 unrolling step real data number unrolling step increased histogram intersample distance increasingly come resemble data distribution evidence support unrolling decreasing mode collapse behavior gans 4 discussion work developed method stabilize gan training reduce mode collapse deﬁning generator objective respect unrolled optimization discriminator strated application method several task either rescued unstable training reduced tendency model drop region data distribution main drawback method computational cost training step increase linearly number unrolling step tradeoff better approximating true generator loss computation required make estimate depending architecture one unrolling step enough unstable model rnn case needed stabilize training some initial positive result suggesting may sufﬁcient perturb training gradient direction single unrolling step perturbs computationally efﬁcient investigation required method presented bridge some gap theoretical practical result training gans believe developing better update rule generator discriminator important line work gan training work only considered small fraction design space instance approach could extended unroll g updating well letting discriminator react generator would move also possible unroll sequence g update would make update recursive g could react maximize performance g already updated acknowledgment would like thank laurent dinh david dohan vincent dumoulin liam fedus ishaan rajani julian ibarz eric jang matthew johnson marc lanctot augustus odena gabriel pereyra 10 published conference paper iclr 2017 pairwise norm distribution data 0 step data 1 step data 5 step norm data 10 step probability density figure 6 number unrolling step gan training increased distribution pairwise distance model sample closely resembles distribution data plot histogram pairwise distance randomly selected sample red line give pairwise distance data ﬁve blue line plot represents model trained different random seed vertical line median distribution colin raffel sam schoenholz ayush sekhari jon shlens dale schuurmans insightful conversation well rest google brain team 11 published conference paper iclr 2017 reference guillaume alain yoshua bengio li yao jason yosinski eric saizheng zhang pascal vincent gsns generative stochastic network arxiv preprint marcin andrychowicz misha denil sergio gomez matthew w hoffman david pfau tom schaul nando de freitas learning learn gradient descent gradient descent arxiv preprint david belanger andrew mccallum structured prediction energy network arxiv preprint jorg bornschein samira shabanian asja fischer yoshua bengio bidirectional helmholtz machine arxiv preprint michael bowling manuela veloso multiagent learning using variable learning rate artiﬁcial intelligence 136 2 yuri burda roger grosse ruslan salakhutdinov importance weighted autoencoders arxiv preprint alex champandard semantic style transfer turning doodle ﬁne artwork arxiv preprint tong che yanran li athul paul jacob yoshua bengio wenjie li mode regularized generative adversarial network arxiv preprint arxiv xi chen yan duan rein houthooft john schulman ilya sutskever pieter abbeel gan interpretable representation learning information maximizing generative adversarial net arxiv preprint john danskin theory application weapon allocation problem ume springer science business medium peter dayan geoffrey e hinton radford neal richard zemel helmholtz machine neural computation 7 5 laurent dinh david krueger yoshua bengio nice independent component mation arxiv preprint laurent dinh jascha samy bengio density estimation using real nvp arxiv preprint vincent dumoulin ishmael belghazi ben poole alex lamb martin arjovsky olivier etro aaron courville adversarially learned inference arxiv preprint xavier glorot yoshua bengio understanding difﬁculty training deep feedforward neural network jmlr w cp proceeding thirteenth international conference artiﬁcial intelligence statistic aistats 2010 volume 9 pp may ian goodfellow jean mehdi mirza bing xu david sherjil ozair aaron courville yoshua bengio generative adversarial net ghahramani welling cortes lawrence weinberger ed advance neural information cessing system 27 pp curran associate url karol gregor ivo danihelka alex graf daan wierstra draw recurrent neural network image generation proceeding international conference machine ing pp url tian han yang lu zhu ying nian wu alternating generator network url 12 published conference paper iclr 2017 sepp hochreiter urgen schmidhuber long memory neural 9 8 1780 november issn doi url sergey ioffe christian szegedy batch normalization accelerating deep network training reducing internal covariate shift proceeding international conference machine learning icml 2015 lille france july 2015 pp url justin johnson alexandre alahi li perceptual loss style transfer arxiv preprint anatoli juditsky arkadi nemirovski et al first order method nonsmooth convex optimization general purpose method optimization machine learning pp diederik kingma jimmy ba adam method stochastic optimization arxiv preprint diederik p kingma max welling variational bayes url http diederik kingma tim salimans max welling improving variational inference inverse autoregressive ﬂow tejas kulkarni whitney pushmeet kohli joshua tenenbaum deep convolutional inverse graphic network arxiv preprint christian ledig lucas theis ferenc huszar jose caballero andrew aitken alykhan tejani hannes totz zehan wang wenzhe shi single image using generative adversarial network url dougal maclaurin david duvenaud ryan adam hyperparameter tion reversible learning anh nguyen alexey dosovitskiy jason yosinski thomas brox jeff clune synthesizing preferred input neuron neural network via deep generator network arxiv preprint sebastian nowozin botond cseke ryota tomioka training generative neural sampler using variational divergence minimization arxiv preprint augustus odena christopher olah jonathon shlens conditional image synthesis iary classiﬁer gans arxiv preprint barak pearlmutter jeffrey mark siskind ad functional framework lambda ultimate backpropagator acm trans program lang 30 2 march issn doi url ben poole alexander alemi jascha anelia angelova improved generator objective gans arxiv preprint alec radford luke metz soumith chintala unsupervised representation learning deep convolutional generative adversarial network arxiv preprint scott reed zeynep akata santosh mohan samuel tenka bernt schiele honglak lee ing draw nip scott reed zeynep akata xinchen yan lajanugen logeswaran bernt schiele honglak lee generative adversarial synthesis proceeding international ence machine learning 13 published conference paper iclr 2017 danilo jimenez rezende shakir mohamed daan wierstra stochastic backpropagation variational inference deep latent gaussian model international conference machine learning citeseer tim salimans ian goodfellow wojciech zaremba vicki cheung alec radford xi chen improved technique training gans arxiv preprint karen simonyan andrea vedaldi andrew zisserman deep inside convolutional network sualising image classiﬁcation model saliency map arxiv preprint satinder singh michael kearns yishay mansour nash convergence gradient dynamic game proceeding sixteenth conference uncertainty artiﬁcial intelligence pp morgan kaufmann publisher jascha eric wei niru maheswaranathan surya ganguli deep vised learning using nonequilibrium thermodynamics proceeding international conference machine learning pp url casper kaae sonderby jose caballero lucas theis wenzhe shi ferenc huszar amortised map inference image url theis bethge generative image modeling using spatial lstms advance ral information processing system 28 dec url theis van den oord bethge note evaluation generative model ternational conference learning representation apr url tieleman hinton lecture divide gradient running average recent magnitude coursera neural network machine learning aron van den oord nal kalchbrenner koray kavukcuoglu pixel recurrent neural network arxiv preprint url aron van den oord nal kalchbrenner oriol vinyals lasse espeholt alex graf ray kavukcuoglu conditional image generation pixelcnn decoder arxiv preprint pascal vincent hugo larochelle isabelle lajoie yoshua bengio manzagol stacked denoising autoencoders learning useful representation deep network local denoising criterion mach learn december issn url jason yosinski jeff clune anh nguyen thomas fuchs hod lipson understanding neural network deep visualization arxiv preprint chongjie zhang victor r lesser learning policy prediction proceeding aaai conference artiﬁcial intelligence junbo zhao michael mathieu yann lecun generative adversarial network arxiv preprint zhu philipp uhl eli shechtman alexei efros generative visual tion natural image manifold proceeding european conference computer vision eccv 2016 14 published conference paper iclr 2017 appendix gaussian training detail network architecture experimental detail experiment section follows dataset sampled mixture 8 gaussians standard deviation mean equally spaced around circle radius generator network consists fully connected network 2 hidden layer size 128 relu activation followed linear projection 2 dimension weight initialized orthogonal scaling discriminator network ﬁrst scale input factor 4 roughly scale followed 1 layer fully connected network relu activation linear layer size 1 act logit generator minimizes lg log x log 1 g z discriminator minimizes ld x 1 g z x sampled data distribution z n 0 network optimized using adam kingma ba 2014 learning rate network trained alternating update generator discriminator one step consists either g updating b mixture gaussian experiment effect time delay historical averaging another comparison looked wa regard historical averaging based approach cently similarly inspired approach used salimans et 2016 stabilize training study looked taking ensemble discriminator time first looked taking ensemble last n step shown figure 1 5 20 0 50 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 update step number ensemble figure historical averaging doe not visibly increase stability mixture gaussians task row corresponds ensemble discriminator consists indicated number immediately preceding discriminator column correspond different number training step explore idea ran experiment ensemble 5 discriminator different period replacing discriminator ensemble example sample rate 100 would take 500 step replace 5 discriminator result seen figure observe given longer longer time delay model becomes le le stable hypothesize due initial shape discriminator loss surface training discriminator estimate probability density only accurate region wa trained ﬁxing discriminator removing feedback generator exploitation 15 published conference paper iclr 2017 1 10 100 0 1000 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 update step step ensemble figure introducing longer time delay discriminator ensemble result bility probability distribution not window visualized x axis number weight update axis many step skip discriminator update selecting ensemble 5 discriminator discriminator ability move result generator able exploit ﬁxed area poor performance older discriminator ensemble new discriminator compensate leading system diverge effect second gradient second factor analyzed effect backpropagating learning signal rolling equation turn backpropagation unrolling troducing stop gradient call computation graph unrolling step stop gradient place update signal corresponds only ﬁrst term equation looked 3 conﬁgurations without stop gradient vanilla unrolled gan stop gradient stop gradient taking average k unrolling step instead taking ﬁnal value result see figure initially observed no difference unrolling without second gradient required 3 unrolling step become stable discriminator unrolled convergence second gradient term becomes zero due simplicity problem suspect discriminator nearly converged every generator step second gradient term wa thus irrelevant test modiﬁed dynamic perform ﬁve generator step discriminator update result shown figure discriminator kept equilibrium successful training achieved half many unrolling step using term gradient only including ﬁrst term c rnn mnist training detail network architecture experiment section follows mnist dataset scaled 1 generator ﬁrst scale noise vector 256 unit fully connected layer relu activation fed initial state lstm hochreiter schmidhuber 1997 run 28 step corresponding number column mnist resulting sequence tivations projected fully connected layer 28 output tanh activation function weight initialized via xavier initialization glorot bengio 2010 forget bias lstm initialized discriminator network feed input convolution 16 followed lution 32 followed convolution 32 convolution stride radford et 2015 leaky rectiﬁers used leak batch normalization applied layer ioffe szegedy 2015 resulting tensor ﬂattened linear projection performed single scalar 16 published conference paper iclr 2017 unrolled gan 0 1 3 5 10 0 20 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 update step unrolling step unrolled gan without second gradient 0 1 3 5 10 0 20 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 update step unrolling step figure discriminator remains nearly optimum learning performance nearly identical without second gradient term equation shown figure discriminator lag behind generator backpropagating unrolling aid convergence generator network minimises lg log g z discriminator minimizes ld log x log 1 g z network trained adam kingma ba 2014 learning rate network trained alternating updating generator discriminator step one step consists 1 network update training detail network architecture discriminator generator encoder follows tions kernel size batch normalization discriminator us leaky relu leak generator us standard relu generator network deﬁned number output stride input z 0 fully connected 4 4 512 reshape image transposed convolution 256 2 transposed convolution 128 2 transposed convolution 64 2 convolution 1 3 1 17 published conference paper iclr 2017 unrolled gan 5 g step per 0 1 3 5 10 0 20 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 update step unrolling step unrolled gan 5 g step per without second gradient 0 1 3 5 10 0 20 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 update step unrolling step figure backpropagating unrolling process aid convergence criminator doe not fully converge generator update taking 5 generator step per discriminator step unrolling greatly increase stability requiring only 5 unrolling step converge without second gradient requires 10 unrolling step also see figure discriminator network deﬁned number output stride input x g convolution 64 2 convolution 128 2 convolution 256 2 flatten fully connected 1 generator network minimises lg log g z discriminator minimizes ld log x g z network trained adam generator learning rate discriminator learning rate network trained alternating updating generator discriminator step one step consists 1 network update 18 published conference paper iclr 2017 e 1000 class mnist number output stride input z 0 fully connected 4 4 64 reshape image transposed convolution 32 2 transposed convolution 16 2 transposed convolution 8 2 convolution 3 1 discriminator network parametrized size x deﬁned follows test used x number output stride input x g convolution 8 x 2 convolution 16 x 2 convolution 32 x 2 flatten fully connected 1 f colored mnist dataset dataset generate dataset ﬁrst took mnist digit scaled 0 image sample color c normally distributed generate colored digit 1 finally add small amount pixel independent noise sampled normal distribution resulting value cliped 1 visualized generates image sample seen ﬁgure hard visually see difference sample diversity comparing 128 512 sized model figure right sample data distribution middle sample size model 0 look ahead step worst diversity left sample size model 10 look ahead step diversity model model used section parametrized variable x control capacity value architecture used experiment used 1 value generator network deﬁned 19 published conference paper iclr 2017 number output stride input z 0 fully connected 4 4 512 x reshape image x transposed convolution 256 x 2 transposed convolution 128 x 2 transposed convolution 64 x 2 convolution 3 1 discriminator network deﬁned number output stride input x g convolution 64 x 2 convolution 128 x 2 convolution 256 x 2 flatten fully connected 1 g optimization based visualization example model based optimization performed 5 run different seed unrolling step conﬁguration bellow comparison run index ideally would many many comparison space efﬁciency grouped run index run 20 published conference paper iclr 2017 data 0 step 1 step 5 step 10 step data 0 step 1 step 5 step 10 step figure sample different random seed 21 published conference paper iclr 2017 data 0 step 1 step 5 step 10 step data 0 step 1 step 5 step 10 step figure sample different random seed 22 published conference paper iclr 2017 data 0 step 1 step 5 step 10 step data 0 step 1 step 5 step 10 step figure sample different random seed 23 published conference paper iclr 2017 data 0 step 1 step 5 step 10 step data 0 step 1 step 5 step 10 step figure sample different random seed 24 published conference paper iclr 2017 data 0 step 1 step 5 step 10 step data 0 step 1 step 5 step 10 step figure sample different random seed 25