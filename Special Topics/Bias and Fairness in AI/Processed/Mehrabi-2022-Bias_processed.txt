survey bias fairness machine learning ninareh mehrabi fred morstatter nripsuta saxena kristina lerman aram galstyan widespread use artificial intelligence ai system application everyday life accounting fairness ha gained significant importance designing engineering system ai system used many sensitive environment make important decision thus crucial ensure decision not reflect discriminatory behavior toward certain group population recently some work ha developed traditional machine learning deep learning address challenge different subdomains commercialization system researcher becoming aware bias application contain attempting address survey investigated different application shown bias various way listed different source bias affect ai application created taxonomy fairness definition machine learning researcher defined order avoid existing bias ai system addition examined different domain subdomains ai showing researcher observed regard unfair outcome method way tried address still many future direction solution taken mitigate problem bias ai system hoping survey motivate researcher tackle issue near future observing existing work respective field cc concept computing methodology intelligence tions artificial intelligence additional key word phrase fairness bias artificial intelligence machine learning deep learning natural language processing representation learning 1 introduction machine learning algorithm penetrated every aspect life algorithm make movie recommendation suggest product buy date increasingly used scenario loan 113 hiring decision 19 39 clear benefit algorithmic unlike people machine not become tired bored 45 119 take account order magnitude factor people however like people algorithm vulnerable bias render decision unfair 6 121 context fairness absence any prejudice favoritism toward individual group based inherent acquired characteristic thus unfair algorithm one whose decision skewed toward particular group people canonical example come tool used court united state make pretrial detention release decision software correctional offender management profiling alternative sanction compas measure risk person recommit another crime judge use compas decide whether release offender keep prison investigation software found bias 1 author address usc information science institute 4676 admiralty way suite 1001 marina del rey ca 90292 material based upon work supported defense advanced research project agency darpa agreement no 25 jan 2022 2 mehrabi et al compas likely higher false positive rate offender caucasian offender falsely predicting higher risk recommitting crime recidivism similar finding made area ai system judge beauty pageant winner wa biased facial recognition software digital camera overpredicts asian biased prediction stem hidden neglected bias data algorithm survey identify two potential source unfairness machine learning arise bias data arise algorithm review research investigating bias data skew learned machine learning algorithm nuance way algorithm work prevent making fair data unbiased furthermore observe biased algorithmic outcome might impact user experience thus generating feedback loop data algorithm user perpetuate even amplify existing source bias begin review several highly visible case unfair machine learning algorithm led suboptimal discriminatory outcome section section 3 describe different type source bias occur within loop mentioned next section 4 present different way concept fairness ha operationalized studied literature discus way two concept coupled last focus different family machine learning approach fairness manifest differently one current tackling section 5 followed potential area future work domain section 6 2 example algorithmic unfairness popularity ai machine learning past decade prolific spread different application safety fairness constraint become significant issue researcher engineer machine learning used court ass probability defendant recommits crime used different medical field childhood welfare system 35 autonomous vehicle application direct effect life harm society not designed engineered correctly consideration fairness 123 ha list application way ai system affect daily life inherent bias existence bias ai chatbots employment matching flight routing automated legal aid immigration algorithm search advertising placement algorithm 67 discus example bias real world creep ai robotic system bias face recognition application voice recognition search engine therefore important researcher engineer concerned downstream application potential harmful effect modeling algorithm system system demonstrate discrimination compas exemplar discriminatory system addition discriminatory behavior wa also evident algorithm would deliver advertisement promoting job science technology engineering math stem field 88 advertisement wa designed deliver ments way however le woman compared men saw advertisement due would result younger woman considered valuable subgroup expensive show advertisement optimization algorithm would deliver ad discriminatory way although original pure intention wa bias 2 3 survey bias fairness machine learning 3 facial recognition system 128 recommender system 140 also largely studied evaluated many case shown discriminative towards certain population subgroup order able address bias issue application important u know bias coming prevent enumerated bias compas widely used commercial risk assessment software addition bias also contains performance issue compared human compared human judgment study wa discovered not any better normal human 46 also interesting note although compas us 137 feature only 7 presented people study 46 argues compas not any better simple logistic regression model making decision think responsibly recognize application tool subsequent decision affect people life therefore considering fairness constraint crucial task designing engineering type sensitive tool another similar study investigating source group unfairness unfairness across different group defined later author 145 compared savry tool used risk assessment framework includes human intervention process automatic machine learning method order see one accurate fair conducting type study done frequently prior releasing tool order avoid harm assessment tool interesting direction researcher taken introducing tool ass amount fairness tool system example aequitas 136 toolkit let user test model regard several bias fairness metric different population subgroup aequitas produce report obtained data help data scientist machine learning researcher policymakers make conscious decision avoid harm damage toward certain population ai fairness 360 another toolkit developed ibm order help moving fairness research algorithm industrial setting create benchmark fairness algorithm get evaluated environment fairness researcher share idea 11 type toolkits helpful learner researcher people working industry move towards developing fair machine learning application away discriminatory behavior 3 bias data algorithm user experience ai system algorithm data driven require data upon trained thus data tightly coupled functionality algorithm system case underlying training data contains bias algorithm trained learn bias reflect prediction result existing bias data affect algorithm using data producing biased outcome algorithm even amplify perpetuate existing bias data addition algorithm display biased behavior due certain design choice even data not biased outcome biased algorithm fed system affect user decision result biased data training future algorithm example imagine web search engine put specific result top list user tend interact top result pay little attention list 92 interaction user item collected web search engine data used make future decision information presented based popularity user interest result result top become popular not nature result due biased interaction placement result algorithm 92 loop capturing feedback bias data algorithm user 4 mehrabi et al data algorithm user interaction behavioral bias content production bias ranking bias emergent bias aggregation bias longitudinal data fallacy fig example bias definition placed data algorithm user interaction feedback loop interaction illustrated figure use loop categorize definition bias section type bias bias exist many shape form some lead unfairness different stream learning task 144 author talk source bias machine learning categorization description order motivate future solution source bias introduced paper 120 author prepare complete list different type bias corresponding definition exist different cycle data origin collection processing reiterate important source bias introduced two paper also add some work existing research paper additionally introduce different categorization definition paper according data algorithm user interaction loop data algorithm section talk bias data used ml training algorithm might result biased algorithmic outcome 1 measurement bias measurement reporting bias arises choose utilize measure particular feature 144 example type bias wa observed recidivism risk prediction tool compas prior arrest arrest used proxy variable measure level riskiness crime viewed mismeasured proxy partly due fact minority community controlled policed frequently higher arrest rate however one not conclude people coming minority group higher arrest rate therefore dangerous difference group assessed controlled 144 2 omitted variable bias omitted variable occurs one important variable left model 38 114 131 example case would someone survey bias fairness machine learning 5 design model predict relatively high accuracy annual percentage rate customer stop subscribing service soon observes majority user canceling subscription without receiving any warning designed model imagine reason canceling subscription appearance new strong competitor market offer solution half price appearance competitor wa something model wa not ready therefore considered omitted variable 3 representation bias representation bias arises sample population data collection process 144 sample lack diversity population missing subgroup anomaly lack geographical diversity datasets like imagenet shown figure 3 4 result demonstrable bias towards western culture 4 aggregation bias aggregation bias ecological fallacy arises false conclusion drawn individual observing entire population example type bias seen clinical aid tool consider diabetes patient apparent morbidity difference across ethnicity gender specifically level widely used diagnose monitor diabetes differ complex way across gender ethnicity therefore model ignores individual difference likely not ethnic gender group population 144 true even represented equally training data any general assumption subgroup within population result aggregation bias 0 50 100 150 200 250 300 0 50 100 150 200 250 300 multivariate linear regression clusterwise linear regression cluster regression data 0 50 100 150 200 250 300 0 50 100 150 200 250 300 multivariate linear regression clusterwise linear regression cluster regression data fig illustration bias data red line show regression mlr entire population dashed green line regression subgroup solid green line unbiased regression subgroup equal size mlr show positive relationship outcome independent variable b regression show almost no relationship le balanced data relationship variable within subgroup however remain credit nazanin alipourfard simpson paradox simpson paradox type aggregation bias arises analysis heterogeneous data 18 paradox arises association observed aggregated data disappears revers data disaggregated underlying subgroup fig 2 one example type paradox arose gender bias lawsuit university admission uc berkeley 16 analyzing graduate school admission data seemed like wa bias toward woman smaller fraction admitted graduate program compared male counterpart however admission data wa separated analyzed department woman applicant equality some case even small advantage 6 mehrabi et al fig fraction country represented iso code open image imagenet image datasets datasets u great britain represent top location 142 shreya shankar fig geographic distribution country open image data set sample almost one third data wa 60 data wa six represented country across north america europe 142 shreya shankar men paradox happened woman tended apply department lower admission rate gender simpson paradox ha observed variety domain including biology 37 psychology 81 astronomy 109 computational social science 91 b modifiable areal unit problem statistical bias geospatial analysis arises modeling data different level spatial aggregation 56 bias result different trend learned data aggregated different spatial scale 5 sampling bias sampling bias similar representation bias arises due random sampling subgroup consequence sampling bias trend estimated one population may not generalize data collected new population intuition consider example figure left plot represents data collected study three subgroup uniformly sampled fig 2 suppose next time study wa conducted one subgroup wa sampled frequently rest fig 2 b positive trend found regression model first study almost completely disappears solid red line plot right although subgroup trend dashed green line unaffected 6 longitudinal data fallacy researcher analyzing temporal data must use longitudinal ysis track cohort time learn behavior instead temporal data often modeled survey bias fairness machine learning 7 using analysis combine diverse cohort single time point heterogeneous cohort bias analysis leading different conclusion longitudinal analysis example analysis bulk reddit data 10 revealed comment length decreased time average however bulk data represented snapshot population reality contained different cohort joined reddit different year data wa disaggregated cohort comment length within cohort wa found increase time 7 linking bias linking bias arises network attribute obtained user connection activity interaction differ misrepresent true behavior user 120 104 author show social network biased toward node only considering link network not considering content behavior user network 153 also show user interaction significantly different social link pattern based feature method interaction time difference bias network result many factor network sampling shown 59 111 change network measure cause different type problem algorithm user algorithm modulate user behavior any bias algorithm might introduce bias user behavior section talk bias result algorithmic outcome affect user behavior consequence 1 algorithmic bias algorithmic bias bias not present input data added purely algorithm 9 algorithmic design choice use certain optimization function regularization choice applying regression model data whole considering subgroup general use statistically biased estimator algorithm 44 contribute biased algorithmic decision bias outcome algorithm 2 user interaction bias user interaction bias type bias not only observant web also get triggered two user interface user imposing biased behavior interaction 9 type bias influenced type subtypes presentation ranking bias presentation bias presentation bias result information presented 9 example web user only click content see seen content get click everything else get no click could case user doe not see information web 9 b ranking bias idea result relevant important result attraction click others bias affect search engine 9 crowdsourcing application 93 3 popularity bias item popular tend exposed however popularity metric subject example fake review social bot 117 instance type bias seen search engine 71 117 recommendation system popular object would presented public presentation may not result good quality instead may due biased factor 4 emergent bias emergent bias occurs result use interaction real user bias arises result change population cultural value societal knowledge usually some time completion design 53 type bias likely observed user interface since interface tend reflect capacity characteristic habit prospective user design 53 type bias divided subtypes discussed detail 53 8 mehrabi et al 5 evaluation bias evaluation bias happens model evaluation 144 includes use inappropriate disproportionate benchmark evaluation application adience benchmark benchmark used evaluation facial recognition system biased toward skin color gender 24 serve example type bias 144 user data many data source used training ml model any inherent bias user might reflected data generate furthermore user behavior algorithm any bias present algorithm might introduce bias data generation process list several important type bias 1 historical bias historical bias already existing bias issue world seep data generation process even given perfect sampling feature selection 144 example type bias found 2018 image search result searching woman ceo ultimately resulted fewer female ceo image due fact only 5 fortune 500 ceo would cause search result biased towards male ceo 144 search result course reflecting reality whether not search algorithm reflect reality issue worth considering 2 population bias population bias arises statistic demographic representative user characteristic different user population platform original target population 120 population bias creates data example type bias arise different user demographic different social platform woman likely use pinterest facebook instagram men active online forum like reddit twitter example statistic related social medium use among young adult according gender race ethnicity parental educational background found 64 3 bias subtype selection sampling bias subject research select example type bias observed opinion poll measure enthusiasm political candidate enthusiastic supporter likely complete poll 4 social bias social bias happens others action affect judgment 9 example type bias case want rate review item low score influenced high rating change scoring thinking perhaps harsh 9 151 5 behavioral bias behavioral bias arises different user behavior across platform text different datasets 120 example type bias observed 108 author show difference emoji representation among platform result different reaction behavior people sometimes even leading communication error 6 temporal bias temporal bias arises difference population behavior time 120 example observed twitter people talking particular topic start using hashtag some point capture attention continue discussion event without using hashtag 120 146 7 content production bias content production bias arises structural lexical semantic syntactic difference content generated user 120 example type bias seen 118 difference use language across different gender 4 survey bias fairness machine learning 9 age group discussed difference use language also seen across within country population existing work try categorize bias definition group definition falling solely data user interaction however due existence feedback loop phenomenon 36 definition intertwined need categorization closely model situation feedback loop not only existent data algorithm also algorithm user interaction 29 inspired paper modeled categorization bias definition shown figure 1 grouped definition arrow loop thought effective emphasize fact definition intertwined one consider affect cycle address accordingly data bias example multiple way discriminatory bias seep data instance using unbalanced data create bias underrepresented group 170 analyzes some example bias exist data algorithm offer some recommendation suggestion toward mitigating issue example bias machine learning data 24 author show datasets like adience imbalanced contain mainly adience bias analysis towards group underrepresented data another instance way use analyze data create bias not consider different subgroup data 24 author also show considering only female group not enough also need use race subdivide gender group female male male female only case clearly observe bias towards female previously male would compromise female would hide underlying bias towards subgroup popular datasets serve base developed algorithm tool also harmful downstream application based datasets instance imagenet 135 open image 86 two widely used datasets 142 researcher showed datasets suffer representation bias advocate need incorporate geographic diversity inclusion creating datasets addition author 105 write existing representational bias different knowledge base widely used natural language processing nlp application different commonsense reasoning task example data bias medical application data bias dangerous sensitive application example medical domain many instance data studied used skewed toward certain dangerous quences underrepresented community 98 showed exclusion resulted misclassification clinical study became advocate sequencing genome diverse population data prevent harm underrepresented population author 143 studied genotype dataset found individual openly shared genotype public repository 87 european only 58 2 asian 50 2 african study conducted 54 state uk biobank large widely used genetic dataset may not represent sampling population researcher found evidence healthy volunteer selection bias 150 ha example study existing bias data used medical domain 157 also look 10 mehrabi et al algorithm data utilized medical field writes artificial intelligence health care ha not impacted patient equally discrimination similar bias discrimination also source unfairness discrimination considered source unfairness due human prejudice stereotyping based sensitive attribute may happen intentionally unintentionally bias considered source unfairness due data collection sampling measurement although bias also seen source unfairness due human prejudice stereotyping algorithmic fairness literature intuitive categorize according existing research area survey mainly focus concept relevant algorithmic fairness issue 99 133 152 contain broad information discrimination theory involve multidisciplinary concept legal theory economics social science referenced interested reader explainable discrimination difference treatment outcome amongst different group justified explained via some attribute some case situation difference justified explained not considered illegal discrimination hence called explainable 77 instance author 77 state uci adult dataset 7 widely used dataset fairness domain male average higher annual income female however average female work fewer hour male per week work hour per week attribute used explain low income need considered make decision without considering working hour male female end averaging income lead reverse discrimination since would cause male employee get lower salary female therefore explainable discrimination acceptable legal explained attribute like working hour 77 author present methodology quantify explainable illegal discrimination data argue method not take explainable part discrimination account may result outcome introduce reverse discrimination equally harmful undesirable explain quantify measure discrimination data classifier decision directly considers illegal explainable discrimination unexplainable discrimination contrast explainable discrimination able discrimination discrimination toward group unjustified therefore considered illegal author 77 also present local technique removing only illegal unexplainable discrimination allowing only explainable difference decision preprocessing niques change training data contains no unexplainable discrimination expect classifier trained preprocessed data not capture illegal unexplainable discrimination unexplainable discrimination consists direct indirect discrimination 1 direct discrimination direct discrimination happens protected attribute individual explicitly result outcome toward 164 typically some trait identified law illegal discriminate usually trait considered protected sensitive attribute computer science literature list some protected attribute provided table 3 specified fair housing equal credit opportunity act fha ecoa 30 2 indirect discrimination indirect discrimination individual appear treated based seemingly neutral attribute however protected group individual still get treated unjustly result implicit effect protected attribute survey bias fairness machine learning 11 residential zip code person used decision making process loan application however still lead racial discrimination redlining despite fact zip code appears attribute may correlate race population residential area 130 164 source discrimination 1 systemic discrimination systemic discrimination refers policy custom behavior part culture structure organization may perpetuate discrimination certain subgroup population 40 132 found employer overwhelmingly preferred competent candidate culturally similar shared similar periences hobby happen belong overwhelmingly certain subgroup may result discrimination competent candidate not belong subgroup 2 statistical discrimination statistical discrimination phenomenon use average group statistic judge individual belonging group usually occurs employer law enforcement officer use individual obvious recognizable characteristic proxy either hidden characteristic may actually relevant outcome 124 4 algorithmic fairness fighting bias discrimination ha long history philosophy psychology recently however order able fight discrimination achieve fairness one first define fairness philosophy psychology tried define concept fairness long computer science fact no universal definition fairness exists show difficulty solving problem 138 different preference outlook different culture lend preference different way looking fairness make harder come single definition acceptable everyone situation indeed even computer science work proposing new fairness constraint algorithm ha come west lot paper use datasets problem show constraint perform still no clear agreement constraint appropriate problem broadly fairness absence any prejudice favoritism towards individual group based intrinsic acquired trait context 139 even though fairness incredibly desirable quality society surprisingly difficult achieve practice challenge mind many fairness definition proposed address different algorithmic bias discrimination issue discussed previous section definition fairness 17 author studied fairness definition political philosophy tried tie learning author 70 studied history fairness definition area education 149 author listed explained some definition used fairness algorithmic classification problem 139 author studied general public perception some fairness definition computer science literature reiterate provide some widely used definition along explanation inspired 149 definition 1 equalized odds definition equalized odds provided 63 state predictor ˆ satisfies equalized odds respect protected attribute outcome ˆ independent conditional p ˆ p ˆ mean probability person positive class correctly assigned positive outcome 12 mehrabi et al probability person negative class incorrectly assigned positive outcome protected unprotected group member 149 word equalized odds definition state protected unprotected group equal rate true positive false positive definition 2 equal opportunity binary predictor ˆ satisfies equal opportunity respect p ˆ p ˆ 63 mean probability person positive class assigned positive outcome equal protected unprotected female male group member 149 word equal opportunity definition state protected unprotected group equal true positive rate definition 3 demographic parity also known statistical parity predictor ˆ satisfies graphic parity p ˆ 0 p ˆ 1 48 87 likelihood positive outcome 149 regardless whether person protected female group definition 4 fairness awareness algorithm fair give similar prediction similar individual 48 87 word any two individual similar respect similarity inverse distance metric defined particular task receive similar outcome definition 5 fairness unawareness algorithm fair long any protected attribute not explicitly used process 61 87 definition 6 treatment equality treatment equality achieved ratio false negative false positive protected group category 15 definition 7 test fairness score x test fair reflects likelihood recidivism irrespective individual group membership value p 34 word test fairness definition state any predicted probability score people protected unprotected group must equal probability correctly belonging positive class 149 definition 8 counterfactual fairness predictor ˆ counterfactually fair any text x p ˆ u ˆ u any value attainable 87 counterfactual fairness definition based intuition decision fair towards individual actual world counterfactual world individual belonged different demographic definition 9 fairness relational domain notion fairness able capture relational structure only taking attribute individual consideration taking account social organizational connection individual 50 definition 10 conditional statistical parity set legitimate factor l predictor ˆ satisfies conditional statistical parity p ˆ 0 p ˆ 1 41 conditional statistical parity state people protected unprotected female male group equal probability assigned positive outcome given set legitimate factor l 149 fairness definition fall different type follows survey bias fairness machine learning 13 name reference group subgroup individual demographic parity 87 48 conditional statistical parity 41 equalized odds 63 equal opportunity 63 treatment equality 15 test fairness 34 subgroup fairness 79 80 fairness unawareness 87 61 fairness awareness 48 counterfactual fairness 87 table categorizing different fairness notion group subgroup individual type 1 individual fairness give similar prediction similar individual 48 87 2 group fairness treat different group equally 48 87 3 subgroup fairness subgroup fairness intends obtain best property group individual notion fairness different notion us order obtain better outcome pick group fairness constraint like equalizing false positive asks whether constraint hold large collection subgroup 79 80 important note according 83 impossible satisfy some fairness straints except highly constrained special case 83 author show inherent incompatibility two condition calibration balancing positive negative class not satisfied simultaneously unless certain constraint therefore important take context application fairness definition need used consideration use accordingly 141 another important aspect consider time temporal analysis impact definition may individual group 95 author show current fairness definition not always helpful not promote improvement sensitive actually harmful analyzed time some case also show measurement error also act favor fairness definition therefore show temporal modeling measurement important evaluation fairness criterion introduce new range challenge toward direction also important pay attention source bias type trying solve question 5 method fair machine learning numerous attempt address bias artificial intelligence order achieve fairness stem domain ai section enumerate different domain ai work ha produced community combat bias unfairness method table 2 provides overview different area focus upon survey section largely useful take view ally method target bias algorithm fall three category 1 technique try transform data underlying discrimination removed 43 algorithm allowed modify training data used 11 2 technique try modify change learning algorithm order remove discrimination model training process 43 14 mehrabi et al allowed change learning procedure machine learning model used training either incorporating change objective function imposing constraint 11 14 3 performed training accessing holdout set wa not involved training model 43 algorithm only treat learned model black box without any ability modify training data learning algorithm only used label assigned model initially get reassigned based function phase 11 14 example some existing work categorization type shown table method not limited general machine learning technique ai popularity expanded different domain natural language processing deep learning learning fair representation 42 97 112 learning fair word embeddings 20 58 169 debiasing method proposed different ai application domain method try avoid unethical interference sensitive protected attribute process others target exclusion bias trying include user sensitive group addition some work try satisfy one fairness notion method disparate learning process dlps try satisfy notion treatment disparity impact disparity allowing protected attribute training phase avoiding prediction time 94 list protected sensitive attribute provided table point attribute not affect outcome decision housing loan credit card 30 according law some existing work try treat sensitive attribute noise disregard effect some causal method use causal graph disregard some path causal graph result sensitive attribute affecting outcome decision different method technique discussed different targeting different problem different area machine learning detail expand horizon reader bias affect system try help researcher carefully look various new problem concerning potential place discrimination bias affect outcome system unbiasing data every dataset result several design decision made data curator decision consequence fairness resulting dataset turn affect resulting algorithm order mitigate effect bias data some general method proposed advocate good practice using data datasheets would act like supporting document data reporting dataset creation method characteristic motivation skews 13 55 12 proposes similar approach nlp application similar suggestion ha proposed model 110 author 66 also propose label like nutrition label food order better categorize data task addition general technique some work ha targeted specific type bias example 81 ha proposed method test case simpson paradox data 3 4 proposed method discover simpson paradox data automatically causal model graph also used some work detect direct discrimination data along prevention technique modifies data prediction would absent direct discrimination 163 62 also worked preventing discrimination data mining targeting direct indirect simultaneous effect approach messaging 74 preferential sampling 75 76 disparate impact removal 51 also aim remove bias data survey bias fairness machine learning 15 area reference classification 78 106 57 85 147 63 159 154 69 25 155 122 49 73 75 regression 14 1 pca 137 community detection 104 clustering 31 8 graph embedding 22 causal inference 96 164 165 160 116 115 162 82 127 161 variational auto encoders 97 5 112 42 adversarial learning 90 156 word embedding 20 169 58 23 166 coreference resolution 168 134 language model 21 sentence embedding 100 machine translation 52 semantic role labeling 167 named entity recognition 101 table list paper targeting talking bias fairness different area attribute fha ecoa race color national origin religion sex familial status disability exercised right ccpa marital status recipient public assistance age table list protected attribute specified fair housing equal credit opportunity act fha ecoa 30 fair machine learning address issue variety method proposed satisfy some fairness definition new definition depending application fair classification since classification canonical task machine learning widely used different area direct contact human important type method fair absent bias harm some population therefore certain method proposed 57 78 85 106 satisfy certain definition fairness classification instance 147 author try satisfy subgroup fairness classification equality opportunity equalized odds 63 disparate treatment disparate impact 2 159 16 mehrabi et al equalized odds 154 method try not only satisfy some fairness constraint also stable toward change test set 69 author 155 propose general framework learning fair classifier framework used formulating classification fairness guarantee another work 25 author propose three different modification existing naive bayes classifier classification 122 take new approach fair classification imposing fairness constraint multitask learning mtl framework addition imposing fairness training approach benefit minority group focusing maximizing average accuracy group opposed maximizing accuracy whole without attention accuracy across different group similar work 49 author propose decoupled classification system separate classifier learned group use transfer learning reduce issue le data minority group 73 author propose achieve fair classification mitigating dependence classification outcome sensitive attribute utilizing wasserstein distance measure 75 author propose preferential sampling p method create discrimination free train data set learn classifier discrimination free dataset classifier no discrimination 102 author propose bias mitigation strategy utilizes attention mechanism classification provide interpretability algorithm reference community detection 104 word embedding 23 optimized 27 data 76 classification 159 regression 14 classification 78 classification 155 adversarial learning 90 classification 63 word embedding 20 classification 125 classification 102 table algorithm categorized appropriate group based processing fair regression 14 proposes fair regression method along evaluating measure introduced price fairness pof measure introduce three fairness penalty follows individual fairness definition individual fairness stated 14 every cross pair 𝑥 𝑦 model 𝑤is penalized differently treat 𝑥and weighted function different group sampled formally operationalized 𝑤 𝑆 1 𝑥𝑖 𝑦𝑖 𝑥𝑗 𝑦𝑗 𝑑 𝑦𝑖 𝑦𝑗 2 survey bias fairness machine learning 17 group fairness average two group instance similar label weighted nearness label instance 14 𝑤 𝑆 1 𝑥𝑖 𝑦𝑖 𝑥𝑗 𝑦𝑗 𝑑 𝑦𝑖 𝑦𝑗 2 hybrid fairness hybrid fairness requires positive negatively labeled cross pair treated similarly average two group 14 𝑤 𝑆 𝑥𝑖 𝑦𝑖 𝑥𝑗 𝑦𝑗 𝑑 𝑦𝑖 𝑦𝑗 2 𝑥𝑖 𝑦𝑖 𝑥𝑗 𝑦𝑗 𝑑 𝑦𝑖 𝑦𝑗 2 addition previous work 1 considers fair regression problem formulation regard two notion fairness statistical demographic parity bounded group loss 2 us decision tree satisfy disparate impact treatment regression task addition classification structured prediction 167 author studied semantic model famous dataset imsitu realized only 33 agent role cooking image man rest 67 cooking image woman agent imsitu training set also noticed addition existing bias dataset model would amplify bias training dataset bias magnified man filling only 16 cooking image observation author paper 167 show structured prediction model risk leveraging social bias therefore propose calibration algorithm called rba reducing bias amplification rba technique debiasing model calibrating prediction structured prediction idea behind rba ensure model prediction follow distribution training data study two case object visual semantic role labeling classification show method amplify existing bias data fair pca 137 author show vanilla pca exaggerate error reconstruction one group people different group equal size propose fair method create representation similar richness different make indistinguishable hide dependence sensitive protected attribute show vanilla pca labeled face wild lfw dataset 68 ha lower reconstruction error rate men woman face even sampling done equal weight gender intend introduce dimensionality reduction technique maintains similar fidelity different group population dataset therefore introduce fair pca define fair dimensionality reduction algorithm definition fair pca optimization function follows 𝐴and 𝐵denote two subgroup 𝑈𝐴and 𝑈𝐵denote matrix whose row correspond row 𝑈that contain member subgroup 𝐴and 𝐵given 𝑚data point 𝑅𝑛 𝑟𝑎𝑛𝑘 𝑈 1 𝐴 𝑈𝐴 1 𝐵 𝑈𝐵 proposed algorithm process listed 1 relax fair pca objective semidefinite program sdp solve 2 solve linear program would reduce rank solution conditional random field crf 18 mehrabi et al community inequality online community social network also potentially another place bias discrimination affect population example online community user fewer number friend follower face disadvantage heard online social medium 104 addition existing method community detection method amplify bias ignoring user network wrongfully assigning irrelevant small community 104 author show type bias exists perpetuated existing community detection method propose new attributed community detection method called clan mitigate harm toward disadvantaged group online social community clan process considers network structure alongside node attribute address exclusion bias indicated 1 detect community using modularity value step using only network ture 2 train classifier classify user minor group putting one major group using node attribute step using node attribute fair method domain similar community detection also proposed graph embedding 22 clustering 8 31 causal approach fairness causal model ascertain causal relationship variable using causal graph one represent causal relationship variable node graph edge graph model used remove unwanted causal dependence outcome sensitive attribute gender race designing system policy 96 many researcher used causal model graph solve concern machine learning 33 96 author discus detail subject causality importance designing fair algorithm ha much research discrimination discovery moval us causal model graph order make decision irrespective sensitive attribute group individual instance 164 author propose framework detects direct indirect discrimination data along removal technique 165 extension previous work 160 give nice overview previous work done area author along discussing discrimination solving using previous method addition targeting direct indirect nation expanding previous work generalizing author 116 propose similar pathway approach fair inference using causal graph would restrict certain problematic discriminative pathway causal graph flexibly given any set constraint hold effect identified observed distribution 32 author introduce counterfactual fairness definition extension counterfactual fairness definition 87 propose method achieve extending work 116 115 author extended formalization algorithmic fairness previous work setting learning optimal policy subject constraint based definition fairness describe several strategy learning optimal policy modifying some existing strategy value search based some fairness consideration 162 author only target discrimination discovery no removal finding instance similar another instance observing change protected attribute change outcome decision declare existence discrimination 82 author define following two notion discrimination proxy follows unresolved discrimination variable v causal graph exhibit unresolved discrimination exists directed path v not blocked resolving variable v 82 survey bias fairness machine learning 19 proxy discrimination variable v causal graph exhibit potential proxy discrimination exists directed path v blocked proxy variable v not proxy 82 proposed method prevent avoid also show no observational criterion determine whether predictor exhibit unresolved discrimination therefore causal reasoning framework need incorporated 127 instead using usual risk difference author propose causal risk difference 2 causal discrimination discovery define 𝑝𝑐 2 𝑝𝑐 2 í 𝑑𝑒𝑐 í 𝑅𝐷𝑐not close zero mean bias decision value due group membership causal discrimination covariates not accounted analysis omitted variable bias 𝑅𝐷𝑐then becomes causal discrimination measure discrimination discovery 161 another work type us causal network discrimination discovery fair representation learning variational auto encoders learning fair representation avoiding unfair ence sensitive attribute ha introduced many different research paper example variational fair autoencoder introduced 97 treat sensitive variable nuisance variable removing information variable get fair representation use maximum mean discrepancy regularizer obtain invariance rior distribution latent variable adding maximum mean discrepancy mmd penalty lower bound vae architecture satisfies proposed model variational fair autoencoder similar work not targeting fairness specifically ha introduced 72 5 author also propose debiased vae architecture called learns sensitive latent variable bias model skin tone gender etc propose algorithm top using latent variable debias system like facial detection system 112 author model task optimization objective would minimize loss mutual information encoding sensitive variable relaxed version assumption shown equation use order learn fair representation show adversarial training unnecessary some case even equation 1 c sensitive variable z encoding 𝑚𝑖𝑛 𝑞 l 𝑞 𝑥 𝜆𝐼 𝑧 𝑐 1 42 author introduce flexibly fair representation learning disentanglement disentangles information multiple sensitive attribute flexible fair variational autoencoder not only flexible respect downstream task label also flexible respect sensitive attribute address demographic parity notion fairness target multiple sensitive attribute any subset combination adversarial learning 90 author present framework mitigate bias model learned data stereotypical association propose model trying maximize accuracy predictor time minimize ability adversary predict protected sensitive variable stereotyping variable z model consists two predictor shown figure model predictor trained predict given help approach like stochastic gradient descent model try learn weight w minimizing some loss function lp ˆ 𝑦 output layer passed adversary another network network try predict adversary 20 mehrabi et al may different input depending fairness definition needing achieved instance order satisfy demographic parity adversary would try predict protected variable z using only predicted label ˆ 𝑌passed input preventing adversary learning goal predictor similarly achieve equality odds adversary would get true label addition predicted label ˆ satisfy equality opportunity given class would only select instance adversary 156 take interesting different direction toward solving fairness issue using adversarial network introducing fairgan generates synthetic data free discrimination similar real data use newly generated synthetic data fairgan debiased instead real data training testing not try remove discrimination dataset unlike many existing approach instead generate new datasets similar real one debiased preserve good data utility architecture fairgan model shown figure fairgan consists two component generator 𝐺𝐷𝑒𝑐which generates fake data conditioned protected attribute 𝑃𝐺 𝑥 𝑦 𝑠 𝑃𝐺 𝑥 𝑃𝐺 𝑠 𝑃𝐺 𝑠 𝑃𝑑𝑎𝑡𝑎 𝑠 two discriminator trained differentiate real data denoted 𝑃𝑑𝑎𝑡𝑎 𝑥 𝑦 𝑠 generated fake data denoted 𝑃𝐺 𝑥 𝑦 𝑠 pz noise p protected attribute gdec generator pg pdata discriminator discriminator real fake 0 1 fig structure fairgan proposed 156 fig architecture adversarial network proposed 90 brian hu zhang addition achieving fairness constraint statistical parity 𝑃𝐺 𝑥 1 𝑃𝐺 𝑥 0 training emphasizes differentiation two type survey bias fairness machine learning 21 synthetic generated model sample 𝑃𝐺 𝑥 1 𝑃𝐺 𝑥 0 indicating synthetic sample unprotected protected group denotes protected sensitive variable adapted notation 156 fair nlp word embedding 20 author noticed using word embeddings word analogy test man would mapped computer programmer woman would mapped bias toward woman triggered author propose method debias word embeddings proposing method respect embeddings word debiases embeddings word following step notice step 2 ha two different option depending whether target hard debiasing soft debiasing would use either step 1 identify gender subspace identifying direction embedding capture bias 20 2 hard debiasing soft debiasing hard debiasing neutralize equalize neutralize put away gender subspace word make sure word removed zeroed gender subspace 20 equalize make word equidistant equality set gendered word 20 b soft bias correction try move little possible retain similarity original embedding much possible reducing gender bias controlled parameter 20 following footstep author future work attempted tackle problem 169 generating version glove called try retain gender information some word embedding learned dimension ensuring sion free gender effect approach primarily relies glove base model gender protected attribute however recent paper 58 argues debiasing technique state many recent work debiasing word embeddings superficial technique hide bias actually remove recent work 23 took new direction proposed preprocessing method discovery problematic document training corpus bias tried debias system perturbing removing document efficiently training corpus recent work 166 author target bias elmo contextualized word vector attempt analyze mitigate observed bias embeddings show corpus used training elmo ha significant gender skew male entity nearly three time common female entity automatically lead gender bias pretrained contextualized embeddings propose following two method mitigating existing bias using pretrained embeddings downstream task coreference resolution 1 data augmentation approach 2 neutralization approach coreference resolution 168 paper show coreference system gender bias introduce benchmark called winobias focusing gender bias coreference resolution addition introduce technique remove bias existing coreferencing method combination using debiasing technique general approach follows first generate auxiliary datasets using approach replace male entity female entity way around train model combination original auxiliary datasets use solution combination debiasing technique generate word embeddings 22 mehrabi et al also point source gender bias coreference system propose solution show first source bias come training data propose solution generates auxiliary data set swapping male female entity another case arises resource bias word embeddings bias proposed solution replace glove debiased embedding method last another source bias come unbalanced gender list balancing count list solution proposed another work 134 author also show existence gender bias three coreference resolution system observing many occupation system resolve pronoun biased fashion preferring one gender language model 21 author introduce metric measuring gender bias generated text language model based recurrent neural network trained text corpus along measuring bias training text use equation 2 𝑤is any word corpus 𝑓is set gendered word belong female category woman 𝑚to male category measure bias using mean absolute standard deviation proposed metric along fitting univariate linear regression model analyzing effectiveness metric measuring bias 𝑏𝑖𝑎𝑠 𝑤 𝑙𝑜𝑔 𝑃 𝑃 2 language model also introduce regularization loss term would minimize projection embeddings trained encoder onto embedding gender subspace following soft debiasing technique introduced 20 finally evaluate effectiveness method reducing gender bias conclude stating order reduce bias compromise perplexity also point effectiveness bias metric metric sentence encoder 100 author extend research detecting bias word ding technique sentence embedding try generalize technique using word embedding association test weat 26 context sentence coder introducing new sentence encoding technique sentence encoder association test seat used sentence encoding technique cbow gpt elmo bert find although wa varying evidence bias sentence encoders using seat recent method like bert immune bias said not claiming model state sophisticated bias discovery technique may used case thereby encouraging future work area machine translation 52 author noticed translating word friend following two sentence english spanish achieved different case word translated way work hospital friend nurse work hospital friend doctor sentence friend translated female version spanish friend amiga result not reflecting expectation second sentence friend wa translated amigo male version friend spanish doctor stereotypical male nurse female model pick bias stereotype reflects performance solve author 52 build approach leverage fact machine translation us word embeddings use existing debiasing method word embedding apply machine translation pipeline not only helped mitigate existing bias survey bias fairness machine learning 23 system also boosted performance system one blue score 126 author show google translate system suffer gender bias making sentence taken bureau labor statistic dozen language gender neutral including yoruba hungarian chinese translating english showing google translate show favoritism toward male stereotypical field stem job 148 author annotated analyzed europarl dataset 84 large political multilingual dataset used machine translation discovered exception youngest age group represents only small percentage total amount sentence male data available age group also looked entire dataset showed sentence produced male speaker furthermore mitigate issue improve morphological agreement machine translation augmented every sentence tag english source side identifying gender speaker helped system case not always work ha suggested integrating speaker information way named entity recognition 101 author investigate type existing bias various named entity recognition ner system particular observed context entity tagged person entity john person john going school female name opposed male name tagged entity not tagged formalize observation author propose six different evaluation metric would measure amount bias among different gender ner system curated templated sentence pertaining human action applied metric name census data incorporated template six introduced measure aim demonstrate certain type bias serve specific purpose showing various result follows error unweighted type error author wanted recognize portion entity tagged anything person entity male v female demographic group could entity not tagged tagged entity location í error weighted type error similar unweighted case except author considered frequency popularity name could penalize popular name tagged wrongfully í í 𝑛 𝑓𝑟𝑒𝑞𝑓 indicates frequency name particular year female census data likewise 𝑓𝑟𝑒𝑞𝑚 indicates frequency name particular year male census data error unweighted type error entity tagged entity location city notice error doe not count entity not tagged í 𝑃𝐸𝑅𝑆𝑂𝑁 name not tagged 24 mehrabi et al error weighted error similar unweighted case except frequency taken consideration í 𝑃𝐸𝑅𝑆𝑂𝑁 í 𝑛 error unweighted type error report entity not tagged notice even entity tagged entity error type would not consider í error weighted error similar unweighted case frequency taken consideration í í 𝑛 author also investigate data ner system trained find data also biased toward female gender not including versatile name represent female name comparison different mitigation algorithm field algorithmic fairness relatively new area research work still need done improvement said already paper propose fair ai algorithm bias mitigation technique compare different mitigation algorithm using different benchmark datasets fairness domain instance author 65 propose geometric solution learn fair representation remove correlation protected unprotected feature proposed approach control fairness accuracy via adjustable parameter work author evaluate performance approach different benchmark datasets compas adult german compare various different approach fair learning algorithm considering fairness accuracy measure 65 72 158 159 addition ibm ai fairness 360 toolkit 11 ha implemented many current fair learning algorithm ha demonstrated some result demo utilized interested user compare different method regard different fairness measure 6 challenge opportunity fairness research many definition approach fairness literature study area anything complete fairness algorithmic bias still hold number research opportunity section provide pointer outstanding challenge fairness research overview opportunity development understudied problem challenge several remaining challenge addressed fairness literature among 1 synthesizing definition fairness several definition would constitute fairness machine learning perspective proposed literature definition cover wide range use case result somewhat disparate view fairness nearly impossible understand one fairness solution would fare different definition fairness synthesizing definition one remains open research problem since make evaluation system unified comparable survey bias fairness machine learning 25 unified fairness definition framework also help incompatibility issue some current fairness definition 2 equality equity definition presented literature mostly focus equality ensuring individual group given amount resource attention outcome however little attention ha paid equity concept individual group given resource need succeed 60 103 operationalizing definition studying augments contradicts existing definition fairness remains exciting future direction 3 searching unfairness given definition fairness possible identify instance unfairness particular dataset inroad toward problem made area data bias detecting instance simpson paradox arbitrary datasets 3 however unfairness may require consideration due variety definition nuance detecting one fig heatmap depicting distribution previous work fairness grouped domain fairness definition opportunity work taxonomized summarized current state research algorithmic bias particular focus machine learning even area alone research broad subareas natural language processing representation learning community detection seen effort make methodology fair nevertheless every area ha not received amount attention research community figure 7 provides overview ha done different area address fairness definition type domain some area community detection subgroup level received no attention literature could fertile future research area 7 conclusion survey introduced problem adversely affect ai system term bias unfairness issue viewed primarily two dimension data algorithm trated problem demonstrate fairness important issue showed example 26 mehrabi et al potential harm unfairness application judicial system face recognition promoting algorithm went definition fairness bias proposed researcher stimulate interest reader provided some work done different area term addressing bias may affect ai system different method domain ai general machine learning deep learning natural language processing subdivided field analysis subdomain work done address fairness constraint hope expand horizon reader think deeply working system method ensure ha low likelihood causing potential harm bias toward particular group expansion ai use world important researcher take issue seriously expand knowledge field survey categorized created taxonomy ha done far address different issue different domain regarding fairness issue possible future work direction taken address existing problem bias ai discussed previous section 8 acknowledgment material based upon work supported defense advanced research project agency darpa agreement no would like thank organizer speaker attendee 2019 summer school bias discrimination ai would like also thank brian hu zhang shreya shankar 9 appendix datasets fairness research aside existence bias datasets datasets specifically used address bias fairness issue machine learning also some datasets introduced target issue bias previously observed older existing datasets list some widely known datasets characteristic discussed survey uci adult dataset uci adult dataset also known census income dataset contains information extracted 1994 census data people attribute age occupation education race sex indicating whether income person exceeds not used study want compare gender race inequality based people annual income various study 7 german credit dataset german credit dataset contains 1000 credit record ing attribute personal status sex credit score credit amount housing status etc used study gender inequality issue 47 winobias winobias dataset follows winograd format ha 40 occupation sentence referenced human pronoun two type challenge sentence dataset requiring linkage gendered pronoun either male female stereotypical occupation wa used coreference resolution study certify system ha gender bias case towards stereotypical occupation 168 community crime dataset community crime dataset gather mation different community united state related several factor highly influence some common crime robbery murder rape data includes crime data obtained 1990 u lemas survey 1995 fbi unified crime report also contains data 1990 u census survey bias fairness machine learning 27 compas dataset compas dataset contains record defendant broward county indicating jail prison time demographic criminal history compas risk score 2013 2014 89 recidivism juvenile justice dataset recidivism juvenile justice dataset contains juvenile offender age committed crime year 2002 2010 completed prison sentence 2010 catalonia juvenile justice system 145 pilot parliament benchmark dataset pilot parliament benchmark dataset also known ppb contains image 1270 individual national parliament three european iceland finland sweden three african rwanda senegal south africa country benchmark wa released gender race balance diversity representativeness 24 diversity face dataset diversity face dif image dataset collected fairness research face recognition dif large dataset containing one million annotation face image also diverse dataset diverse facial feature different craniofacial distance skin color facial symmetry contrast age pose gender resolution along diverse area ratio 107 dataset name reference size area uci adult dataset 7 income record social german credit dataset 47 credit record financial pilot parliament benchmark dataset 24 image facial image winobias 168 sentence coreference resolution community crime dataset 129 crime record social compas dataset 89 crime record social recidivism juvenile justice dataset 28 crime record social diversity face dataset 107 1 million image facial image table widely used datasets fairness domain additional information datasets including size area concentration reference 1 alekh agarwal miroslav dudik zhiwei steven wu fair regression quantitative definition based algorithm international conference machine learning 2 sina aghaei mohammad javad azizi phebe vayanos learning optimal fair decision tree discriminative proceeding aaai conference artificial intelligence vol 33 3 nazanin alipourfard peter g fennell kristina lerman trust trend discovering simpson paradox social data proceeding eleventh acm international conference web search data mining acm 4 nazanin alipourfard peter g fennell kristina lerman using simpson paradox discover interesting pattern behavioral data twelfth international aaai conference web social medium 5 alexander amini ava soleimany wilko schwarting sangeeta bhatia daniela ru uncovering mitigating algorithmic bias learned latent structure 2019 6 julia angwin jeff larson surya mattu lauren kirchner machine bias software used across country predict future criminal biased black propublica 2016 7 asuncion newman uci machine learning repository mlr 8 arturs backurs piotr indyk krzysztof onak baruch schieber ali vakilian tal wagner scalable fair clustering proceeding international conference machine learning proceeding machine learning research vol 97 kamalika chaudhuri ruslan salakhutdinov pmlr long beach california usa 28 mehrabi et al 9 ricardo bias web commun acm 61 6 may 2018 3209581 10 samuel barbosa dan cosley amit sharma roberto averaging gone wrong using aware analysis better understand behavior april 2016 11 rachel ke bellamy kuntal dey michael hind samuel c hoffman stephanie houde kalapriya kannan pranay lohia jacquelyn martino sameep mehta aleksandra mojsilovic et al ai fairness 360 extensible toolkit detecting understanding mitigating unwanted algorithmic bias arxiv preprint 2018 12 emily bender batya friedman data statement natural language processing toward mitigating system bias enabling better science transaction association computational linguistics 6 2018 13 misha benjamin paul gagnon negar rostamzadeh chris pal yoshua bengio alex shee towards standardization data license montreal data license 14 richard berk hoda heidari shahin jabbari matthew joseph michael kearns jamie morgenstern seth neel aaron roth convex framework fair regression 15 richard berk hoda heidari shahin jabbari michael kearns aaron roth fairness criminal justice risk assessment state art sociological method research 0049124118782533 16 peter j bickel eugene hammel j william connell sex bias graduate admission data berkeley science 187 4175 1975 17 rdp binns fairness machine learning lesson political philosophy journal machine learning research 2018 18 colin r blyth simpson paradox principle amer statist assoc 67 338 1972 19 miranda bogen aaron rieke help wanted examination hiring algorithm equity technical report bias technical report upturn 20 tolga bolukbasi chang james zou venkatesh saligrama adam kalai man computer programmer woman homemaker debiasing word embeddings advance neural information processing system 21 shikha bordia samuel bowman identifying reducing gender bias language model proceeding 2019 conference north american chapter association computational linguistics student research workshop 22 avishek bose william hamilton compositional fairness constraint graph embeddings international conference machine learning 23 brunet colleen ashton anderson richard zemel understanding origin bias word embeddings proceeding international conference machine learning proceeding machine learning research vol 97 kamalika chaudhuri ruslan salakhutdinov pmlr long beach california usa 24 joy buolamwini timnit gebru gender shade intersectional accuracy disparity commercial gender classification proceeding conference fairness accountability transparency proceeding machine learning research vol 81 sorelle friedler christo wilson pmlr new york ny usa 25 toon calder sicco verwer three naive bayes approach classification data mining knowledge discovery 21 2 2010 26 aylin caliskan joanna j bryson arvind narayanan semantics derived automatically language corpus contain bias science 356 6334 2017 27 flavio calmon dennis wei bhanukiran vinzamuri karthikeyan natesan ramamurthy kush r varshney optimized discrimination prevention advance neural information processing system 30 guyon luxburg bengio wallach fergus vishwanathan garnett curran associate 28 manel capdevila marta ferrer eulália luque la reincidencia en el delito en la justicia de menores centro de estudios jurídicos formación especializada generalitat de catalunya documento no publicado 2005 29 allison jb chaney brandon stewart barbara e engelhardt algorithmic confounding mendation system increase homogeneity decrease utility proceeding acm conference recommender system acm 30 jiahao chen nathan kallus xiaojie mao geoffry svacha madeleine udell fairness unawareness assessing disparity protected class unobserved proceeding conference fairness accountability transparency acm survey bias fairness machine learning 29 31 xingyu chen brandon fain liang lyu kamesh munagala proportionally fair clustering international conference machine learning 32 chiappa counterfactual fairness aaai conference artificial intelligence 33 chiappa isaac causal bayesian network viewpoint fairness kosta pierson slamanig krenn ed privacy identity management fairness accountability transparency age big data privacy identity ifip advance information communication technology vol springer cham 34 alexandra chouldechova fair prediction disparate impact study bias recidivism prediction instrument big data 5 2 2017 35 alexandra chouldechova diana oleksandr fialko rhema vaithianathan case study decision making child maltreatment hotline screening decision proceeding conference fairness accountability transparency proceeding machine learning research vol 81 sorelle friedler christo wilson pmlr new york ny usa 36 alexandra chouldechova aaron roth frontier fairness machine learning arxiv preprint 2018 37 john chuang olivier rivoire stanislas leibler simpson paradox synthetic microbial system science 323 5911 2009 38 kevin clarke phantom menace omitted variable bias econometric research conflict management peace science 22 4 2005 39 lee cohen zachary lipton yishay mansour efficient candidate screening multiple test implication fairness 40 united state equal employment opportunity commission eeoc compliance manual washington equal employment opportunity commission 1992 41 sam emma pierson avi feller sharad goel aziz huq algorithmic decision making cost fairness proceeding acm sigkdd international conference knowledge discovery data mining acm 42 elliot creager david madras jacobsen marissa wei kevin swersky toniann pitassi richard zemel flexibly fair representation learning disentanglement international conference machine learning 43 brian alessandro cathy neil tom lagatta conscientious classification data scientist guide classification big data 5 2 2017 44 david danks alex john london algorithmic bias autonomous system ijcai 45 shai danziger jonathan levav liora extraneous factor judicial decision proceeding national academy science 108 17 2011 46 julia dressel hany farid accuracy fairness limit predicting recidivism science advance 4 1 2018 arxiv 47 dheeru dua casey graff uci machine learning repository 48 cynthia dwork moritz hardt toniann pitassi omer reingold richard zemel fairness awareness proceeding innovation theoretical computer science conference cambridge massachusetts itcs 12 acm new york ny usa 49 cynthia dwork nicole immorlica adam tauman kalai max leiserson decoupled classifier efficient machine learning proceeding conference fairness accountability transparency proceeding machine learning research vol 81 sorelle friedler christo wilson pmlr new york ny usa 50 golnoosh farnadi behrouz babaki lise getoor fairness relational domain proceeding 2018 conference ai ethic society new orleans la usa aies 18 acm new york ny usa 51 michael feldman sorelle friedler john moeller carlos scheidegger suresh venkatasubramanian certifying removing disparate impact proceeding acm sigkdd international conference knowledge discovery data mining sydney nsw australia kdd 15 association computing machinery new york ny usa 52 joel escudé font marta r equalizing gender bias neural machine translation word embeddings technique arxiv preprint 2019 53 batya friedman helen nissenbaum bias computer system acm trans inf syst 14 3 july 1996 30 mehrabi et al 54 anna fry thomas j littlejohns cathie sudlow nicola doherty ligia adamska tim sprosen rory collins naomi e allen comparison sociodemographic characteristic uk biobank participant general population american journal epidemiology 186 9 06 2017 http arxiv 55 timnit gebru jamie morgenstern briana vecchione jennifer wortman vaughan hanna wallach hal daumé iii kate crawford datasheets datasets 56 gehlke katherine biehl certain effect grouping upon size correlation coefficient census tract material amer statist assoc 29 1934 57 naman goel mohammad yaghini boi faltings machine learning convex fairness criterion aaai conference artificial intelligence 58 hilum gonen yoav goldberg lipstick pig debiasing method cover systematic gender bias word embeddings not remove arxiv preprint 2019 59 sandra ning wang alejandro rivero javier yamir moreno assessing bias sample large online network social network 38 2014 60 susan gooden race social equity nervous area government routledge 61 nina muhammad bilal zafar krishna p gummadi adrian weller case process fairness learning feature selection fair decision making nip symposium machine learning law vol 1 2 62 hajian methodology direct indirect discrimination prevention data mining ieee transaction knowledge data engineering 25 7 july 2013 http 63 moritz hardt eric price nati srebro et al equality opportunity supervised learning advance neural information processing system 64 eszter hargittai whose space difference among user social network site journal communication 13 1 10 2007 arxiv 65 yuzi keith burghardt kristina lerman geometric solution fair representation proceeding conference ai ethic society 66 sarah holland ahmed hosny sarah newman joshua joseph kasia chmielinski dataset nutrition label framework drive higher data quality standard arxiv preprint 2018 67 ayanna howard jason borenstein ugly truth robot creation problem bias social inequity science engineering ethic 24 5 2018 68 gary b huang marwan mattar tamara berg eric labeled face wild database forstudying face recognition unconstrained environment 69 lingxiao huang nisheeth vishnoi stable fair classification international conference machine learning 70 ben hutchinson margaret mitchell 2019 50 year test un fairness lesson machine learning proceeding conference fairness accountability transparency acm 71 introna nissenbaum defining web politics search engine computer 33 1 jan 2000 72 ayush jaiswal yue wu wael abdalmageed premkumar natarajan unsupervised adversarial invariance 73 ray jiang aldo pacchiano tom stepleton heinrich jiang silvia chiappa wasserstein fair classification 74 kamiran calder classifying without discriminating 2009 international conference computer control communication 75 faisal kamiran toon calder classification no discrimination preferential sampling proc machine learning conf belgium netherlands citeseer 76 faisal kamiran toon calder data preprocessing technique classification without discrimination knowledge information system 33 1 01 oct 2012 77 faisal kamiran e explainable discrimination classification springer berlin heidelberg berlin heidelberg 78 toshihiro kamishima shotaro akaho hideki asoh jun sakuma classifier prejudice remover regularizer joint european conference machine learning knowledge discovery database springer 79 michael kearns seth neel aaron roth zhiwei steven wu preventing fairness gerrymandering auditing learning subgroup fairness international conference machine learning survey bias fairness machine learning 31 80 michael kearns seth neel aaron roth zhiwei steven wu empirical study rich subgroup fairness machine learning proceeding conference fairness accountability transparency acm 81 rogier kievit willem eduard frankenhuis lourens waldorp denny borsboom simpson paradox psychological science practical guide frontier psychology 4 2013 513 82 niki kilbertus mateo rojas carulla giambattista parascandolo moritz hardt dominik janzing bernhard schölkopf avoiding discrimination causal reasoning advance neural information processing system 83 jon kleinberg sendhil mullainathan manish raghavan inherent fair determination risk score arxiv preprint 2016 84 philipp koehn europarl parallel corpus statistical machine translation mt summit vol 5 85 emmanouil krasanakis eleftherios symeon papadopoulos yiannis kompatsiaris adaptive sensitive reweighting mitigate bias classification proceeding 2018 world wide web conference lyon france www 18 international world wide web conference steering committee republic canton geneva switzerland 86 ivan krasin tom duerig neil alldrin vittorio ferrari sami alina kuznetsova hassan rom jasper uijlings stefan popov andreas veit et al openimages public dataset image classification dataset available 2 3 2017 87 matt j kusner joshua loftus chris russell ricardo silva counterfactual fairness advance neural information processing system 30 guyon luxburg bengio wallach fergus vishwanathan garnett curran associate 88 anja lambrecht catherine e tucker algorithmic bias empirical study apparent discrimination display stem career ad empirical study apparent discrimination display stem career ad march 9 2018 2018 89 j larson mattu l kirchner j angwin compas analysis github available google scholar 2016 90 blake lemoine brian zhang mitchell mitigating unwanted bias adversarial learning 2018 91 kristina lerman computational social scientist beware simpson paradox behavioral data journal computational social science 1 1 2018 92 kristina lerman tad hogg leveraging position bias improve peer recommendation plo one 9 6 2014 93 kristina lerman tad hogg leveraging position bias improve peer recommendation plo one 9 6 2014 94 zachary c lipton alexandra chouldechova julian mcauley doe mitigating ml disparate impact require disparate treatment stat 1050 2017 19 95 lydia liu sarah dean esther rolf max simchowitz moritz hardt delayed impact fair machine learning proceeding international conference machine learning 96 joshua r loftus chris russell matt j kusner ricardo silva causal reasoning algorithmic fairness arxiv preprint 2018 97 christos louizos kevin swersky yujia li max welling richard zemel variational fair autoencoder stat 1050 2016 4 98 arjun manrai birgit funke heidi rehm morten olesen bradley maron peter szolovits david margulies joseph loscalzo isaac kohane genetic misdiagnoses potential health parity new england journal medicine 375 7 2016 arxiv pmid 27532831 99 ray marshall economics racial discrimination survey journal economic literature 12 3 1974 100 chandler may alex wang shikha bordia samuel r bowman rachel rudinger measuring social bias sentence encoders arxiv preprint 2019 101 ninareh mehrabi thamme gowda fred morstatter nanyun peng aram galstyan man person woman location measuring gender bias named entity recognition arxiv preprint 2019 102 ninareh mehrabi umang gupta fred morstatter greg ver steeg aram galstyan attributing fair decision attention intervention arxiv preprint 2021 103 ninareh mehrabi yuzhong huang fred morstatter statistical equity fairness classification objective arxiv preprint 2020 104 ninareh mehrabi fred morstatter nanyun peng aram galstyan debiasing community detection importance node arxiv preprint 2019 32 mehrabi et al 105 ninareh mehrabi pei zhou fred morstatter jay pujara xiang ren aram galstyan lawyer dishonest quantifying representational harm commonsense knowledge resource proceeding 2021 conference empirical method natural language processing association computational linguistics online punta cana dominican republic 106 aditya krishna menon robert c williamson cost fairness binary classification proceeding conference fairness accountability transparency proceeding machine learning research vol 81 sorelle friedler christo wilson pmlr new york ny usa 107 michele merler nalini ratha rogerio feris john r smith diversity face arxiv preprint 2019 108 hannah jean miller jacob shuo chang isaac johnson loren terveen brent hecht 2016 blissfully happy ready tofight varying interpretation emoji tenth international aaai conference web social medium 109 minchev g matijevic dw hogg g guiglion steinmetz f anders c chiappini martig queiroz c scannapieco paradox galactic archaeology arxiv preprint 2019 110 margaret mitchell simone wu andrew zaldivar parker barnes lucy vasserman ben hutchinson elena spitzer inioluwa deborah raji timnit gebru model card model reporting proceeding conference fairness accountability transparency atlanta ga usa fat 19 acm new york ny usa 111 fred morstatter jürgen pfeffer huan liu kathleen carley sample good enough comparing data twitter streaming api twitter firehose international aaai conference weblogs social medium icwsm aaai press 112 daniel moyer shuyang gao rob brekelmans aram galstyan greg ver steeg invariant representation without adversarial training advance neural information processing system 113 amitabha mukerjee rita biswas kalyanmoy deb amrit p mathur evolutionary algorithm bank loan management international transaction operational research 9 5 2002 114 david b mustard reexamining criminal behavior importance omitted variable bias review economics statistic 85 1 2003 115 razieh nabi daniel malinsky ilya shpitser learning optimal fair policy arxiv preprint 2018 116 razieh nabi ilya shpitser fair inference outcome aaai conference artificial intelligence 117 azadeh nematzadeh giovanni luca ciampaglia filippo menczer alessandro flammini algorithmic popularity bias hinders promotes quality arxiv preprint 2017 118 nguyen rilana gravel rudolf berend trieschnigg theo meder 2013 old think study language age twitter proceeding seventh international aaai conference weblogs social medium icwsm aaai press 119 anne keeffe michael mccarthy routledge handbook corpus linguistics routledge 120 alexandra olteanu carlos castillo fernando diaz emre kiciman social data bias methodological pitfall ethical boundary 2016 121 cathy neil weapon math destruction big data increase inequality threatens democracy crown publishing group new york ny usa 122 luca oneto michele doninini amon elder massimiliano pontil taking advantage multitask learning fair classification proceeding 2019 conference ai ethic society 123 osonde osoba william welser iv intelligence image risk bias error artificial intelligence rand corporation 124 edmund phelps statistical theory racism sexism american economic review 62 4 1972 125 geoff pleiss manish raghavan felix wu jon kleinberg kilian q weinberger fairness calibration advance neural information processing system 30 guyon luxburg bengio wallach fergus vishwanathan garnett curran associate http 126 marcelo prate pedro h avelar luís c lamb assessing gender bias machine translation case study google translate neural computing application 2018 127 bilal qureshi faisal kamiran asim karim salvatore ruggieri causal discrimination discovery propensity score analysis arxiv preprint 2016 survey bias fairness machine learning 33 128 inioluwa deborah raji joy buolamwini actionable auditing investigating impact publicly naming biased performance result commercial ai product 129 redmond community crime unnormalized data set uci machine learning repository website ic uci html 2011 130 willy e rice race gender redlining discriminatory access loan credit insurance historical empirical analysis consumer sued lender insurer federal state court san diego rev 33 1996 583 131 stephanie k riegg causal inference omitted variable bias financial aid research assessing solution review higher education 31 3 2008 132 lauren rivera hiring cultural matching case elite professional service firm american sociological review 77 6 2012 133 andrea romei salvatore ruggieri multidisciplinary survey discrimination analysis 134 rachel rudinger jason naradowsky brian leonard benjamin van durme gender bias coreference resolution proceeding 2018 conference north american chapter association computational linguistics human language technology volume 2 short paper association computational linguistics new orleans louisiana 135 olga russakovsky jia deng hao su jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein et al imagenet large scale visual recognition challenge international journal computer vision 115 3 2015 136 pedro saleiro benedict kuester abby stevens ari anisfeld loren hinkson jesse london rayid ghani aequitas bias fairness audit toolkit arxiv preprint 2018 137 samira samadi uthaipon tantipongpipat jamie morgenstern mohit singh santosh vempala price fair pca one extra dimension proceeding international conference neural information processing system montr 233 al canada nip 18 curran associate usa 138 nripsuta ani saxena perception fairness proceeding 2019 conference ai ethic society honolulu hi usa aies 19 acm new york ny usa 3314314 139 nripsuta ani saxena karen huang evan defilippis goran radanovic david c parkes yang liu fairness definition fare examining public attitude towards algorithmic definition fairness proceeding 2019 conference ai ethic society acm 140 tobias schnabel adith swaminathan ashudeep singh navin chandak thorsten joachim mendations treatment debiasing learning evaluation international conference machine learning 141 andrew selbst danah boyd sorelle friedler suresh venkatasubramanian janet vertesi fairness abstraction sociotechnical system proceeding conference fairness accountability transparency acm 142 shreya shankar yoni halpern eric breck james atwood jimbo wilson sculley no classification without representation assessing geodiversity issue open data set developing world stat 1050 2017 22 143 richard shaw manuel corpas bias personal genomics 144 harini suresh john v guttag framework understanding unintended consequence machine learning arxiv preprint 2019 145 songül tolan marius miron emilia gómez carlos castillo machine learning may lead unfairness evidence risk assessment juvenile justice catalonia 2019 146 zeynep tufekci big question social medium big data representativeness validity methodological pitfall eighth international aaai conference weblogs social medium 147 berk ustun yang liu david parkes fairness without harm decoupled classifier preference guarantee proceeding international conference machine learning proceeding machine learning research vol 97 kamalika chaudhuri ruslan salakhutdinov pmlr long beach california usa 148 eva vanmassenhove christian hardmeier andy way getting gender right neural machine translation proceeding 2018 conference empirical method natural language processing 149 sahil verma julia rubin fairness definition explained 2018 international workshop software fairness fairware ieee 150 selwyn vickers mona fouad moon chen enhancing minority participation clinical trial empact laying groundwork improving minority clinical trial accrual cancer 120 2014 34 mehrabi et al 151 ting wang dashun wang amazon rating might mislead story herding effect big data 2 4 2014 152 steven l willborn disparate impact model discrimination theory limit ul rev 34 1984 799 153 christo wilson bryce boe alessandra sala krishna pn puttaswamy ben zhao user interaction social network implication proceeding acm european conference computer system acm 154 blake woodworth suriya gunasekar mesrob ohannessian nathan srebro learning predictor arxiv preprint 2017 155 yongkai wu lu zhang xintao wu classification criterion convexity bound 156 depeng xu shuhan yuan lu zhang xintao wu fairgan generative adversarial network 2018 ieee international conference big data big data ieee 157 irene chen peter szolovits marzyeh ghassemi ai help reduce disparity general medical mental health care ama journal ethic 21 02 2019 158 muhammad bilal zafar isabel valera manuel gomez rodriguez krishna p gummadi fairness beyond disparate treatment disparate impact learning classification without disparate mistreatment proceeding international conference world wide web 159 muhammad bilal zafar isabel valera manuel gomez rodriguez krishna p gummadi fairness constraint mechanism fair classification arxiv preprint 2015 160 lu zhang xintao wu learning causal framework international journal data science analytics 4 1 01 aug 2017 161 lu zhang yongkai wu xintao wu discrimination discovery using causal network social cultural behavioral modeling kevin xu david reitter dongwon lee nathaniel osgood springer international publishing cham 162 lu zhang yongkai wu xintao wu situation discrimination discovery causal inference approach proceeding international joint conference artificial intelligence new york new york usa ijcai 16 aaai press 163 lu zhang yongkai wu xintao wu achieving data release proceeding acm sigkdd international conference knowledge discovery data mining acm 164 lu zhang yongkai wu xintao wu causal framework discovering removing direct indirect discrimination proceeding international joint conference artificial intelligence 165 zhang wu wu causal discrimination discovery removal criterion bound algorithm ieee transaction knowledge data engineering 2018 166 jieyu zhao tianlu wang mark yatskar ryan cotterell vicente ordonez chang gender bias contextualized word embeddings proceeding 2019 conference north american chapter association computational linguistics human language technology volume 1 long short paper 167 jieyu zhao tianlu wang mark yatskar vicente ordonez chang men also like shopping reducing gender bias amplification using constraint proceeding 2017 conference empirical method natural language processing 168 jieyu zhao tianlu wang mark yatskar vicente ordonez chang gender bias coreference resolution evaluation debiasing method 169 jieyu zhao yichao zhou zeyu li wei wang chang learning word embeddings proceeding 2018 conference empirical method natural language processing 170 james zou londa schiebinger ai sexist racist time make fair nature publishing group