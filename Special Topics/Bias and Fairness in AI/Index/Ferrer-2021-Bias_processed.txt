72 ieee ieee technology society magazine feature operating large scale impacting large group people automated system make sequential sometimes contestable decision automated decision impact range nomena credit score insurance payouts health evaluation form automation become problematic place certain group people systematic disadvantage case legally defined unfair unequal treatment vidual group based certain protected teristics also known protected attribute income education gender ethnicity unfair treatment caused automated decision usually taken intelligent agent system topic digital discrimination arises digital discrimination prevalent diverse range field risk assessment system ing credit score 1 2 digital discrimination becoming serious lem decision delegated system increasingly based artificial intelligence ai technique machine learning although significant amount research ha undertaken different disciplinary angle understand computer science law none field able resolve problem term instance putational method verify certify data bias discrimination ai perspective xavier ferrer tom van nuenen jose mark cot√© natalia criado king college london london digital object identifier date current version 3 june set algorithm not account tural ethical complexity not distinguish bias discrimination term technical inflection predicated legal ethical principle 3 article propose synergistic approach allows u explore bias discrimination ai supplementing technical literature social legal ethical perspective critical vey synthesis related literature compare evaluate sometimes contradictory tie within field discus discipline might collaborate resolve problem also highlight number interdisciplinary challenge attest address discrimination ai bias discrimination technical literature area tion typically refers related issue bias yet despite playing important role discriminatory process bias doe not necessarily lead ination bias mean deviation standard sometimes necessary identify existence some statistical pattern data language used 4 5 classifying finding difference instance would impossible without bias article follow common nition bias used literature focus problematic instance bias may lead crimination making system three main cause bias distinguished 4 follows authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 73 june 2021 bias modeling bias may deliberately introduced smoothing ization parameter mitigate compensate bias data called algorithmic processing bias introduced modeling case usage objective category make subjective judgment called algorithmic focus bias bias training algorithm learn make sion prediction based data set often contain past decision data set used training purpose reflects existing dice algorithm likely learn make biased decision moreover data not correctly represent characteristic different population representing unequal ground truth may result biased algorithmic decision bias usage algorithm result bias used situation not intended algorithm utilized predict particular outcome given ulation lead inaccurate result applied different form transfer context bias potential interpretation algorithm output lead biased action called interpretation bias significant amount literature focus form bias may may not lead natory outcome relationship bias discrimination not always clear stood literature assumes system free bias not discriminate hence reducing inating bias reduces eliminates potential discrimination however whether algorithm considered discriminatory not depends context deployed task intended perform instance consider sible case algorithmic bias usage algorithm biased toward hiring young people first glance considered algorithm discriminating older people however biased algorithm only considered criminate context intended deployed doe not justify hiring young people older people therefore statistically ist approach estimating ratio younger older people hired insufficient attest whether algorithm discriminating considering socially politically fraught context remains ethically unclear need draw line biased discriminating outcome therefore ai technical researcher often 1 use discrimination bias equivalent 2 focus measuring bias without actually attending problem whether not discrimination aim gle some issue measuring bias ass whether algorithm free bias need analyze entirety algorithmic process entail first confirming algorithm underlying assumption modeling not biased second training test data not include bias prejudice finally adequate make decision specific context task often not however not access information number issue prevent analysis data used train model instance typically protected since contains personal information rendering task attesting training bias sible access algorithm source code might also restricted general public removing possibility identifying modeling bias common algorithm valuable private asset company third specific algorithm deployed might unknown auditor depending available ent type bias attesting might possible term process term metric used measure procedural versus relational approach distinguish two general approach measure bias 1 procedural approach focus identifying bias process algorithm 6 2 relational approach focus identifying preventing biased decision data set algorithmic output although ensuring unbiased outcome useful attest whether specific rithm ha discriminatory impact population focusing algorithmic process help yield insight reason happened first place authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 74 feature ieee technology society magazine procedural approach focus identifying bias algorithmic ante hoc intervention hard implement two main reason 1 ai algorithm often sophisticated complex since addition trained huge data set usually make use unsupervised learning structure might prove difficult trace understand neural network 2 source code algorithm rarely available cedural approach become beneficial progress explainable ai 6 able understand process behind algorithmic discriminatory decision help u understand possible problem algorithm code behavior thus act accordingly toward creation nondiscriminatory algorithm current literature nondiscriminatory ai promotes introduction explanation model inherently interpretable model decision tree association rule causal reasoning counterfactual explanation provide coarse approximation tem behaves explaining weight ship variable segment model 7 11 note however attesting rithmic process free bias doe not ensure nondiscriminatory algorithmic output since crimination arise consequence bias training usage 12 procedural approach attend rithmic process relational approach measure bias data set algorithmic output approach popular literature not require insight algorithmic process besides evaluating bias data available looking statistical parity mentation compare algorithmic outcome obtained two different subpopulation data set 13 make use counterfactual tive explanation 8 11 14 shown promising result aiding provision able model make decision inscrutable system intelligible developer user asking question x instead bias only located testing time one example approach local able explanation lime make use adversarial learning generate terfactual explanation 6 approach ate correlation algorithmic input biased output identify feature may lead biased action affect protected lations 15 since implementation often ignore context algorithm deployed decision whether biased output result case discrimination often left user ass 9 bias metric metric measuring bias organized three different category 1 statistical measure 2 measure 3 causal ing review 16 offer extensive description some metric discus intuition behind common type rics used literature statistical measure attest bias represent intuitive notion bias focus ing relationship association algorithm predicted outcome different input demographic distribution subject actual outcome achieved ures include first group fairness also named tical parity requires equal quantity group distinct individual receive possible algorithmic outcome instance four five applicant advantaged group given mortgage ratio applicant protected group obtain gage well second predictive parity satisfied protected unprotected group equal positive predictive probability individual correctly classified belonging positive class finally principle well calibration state probability estimate vided algorithm properly adjusted real value despite popularity statistical metric ha shown statistical definition insufficient estimate absence bias algorithmic outcome often assume availability verified come necessary estimate often ignore attribute classified subject sitiveones 17 similarity measure hand focus defining similarity value individual causal discrimination example ures stating classifier not biased duce classification any two subject nonprotected attribute complex bias metric based similarity measure authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 75 june 2021 individual fairness awareness 17 state fairness hold tance distribution output viduals distance two individual estimated mean similarity metric complexity using metric consists accurately defining similarity measure rectly represents complexity situation question often impossible task eralize moreover similarity measure individual suffer implicit bias expert resulting biased similarity estimator finally definition based causal reasoning assume bias attested mean directed causal graph graph attribute presented node joined edge mean tions represent relation attribute 10 exploring graph effect different protected attribute algorithm put assessed analyzed causal fairness approach limited assumption valid causal graph able describe problem constructed not always feasible due sometimes unknown complex relation attribute impact output attesting addressing discrimination first step explored related literature identify discriminatory output determining group whose algorithmic output going compared technical approach select population interest vary either 1 consider subpopulation already defined 9 18 2 selected mean heuristic gate individual share one protected proxy attribute protected group fairtest detecting bias data set protected attribute encoded legislation see legal perspective section usually include attribute sex gender ethnicity proxy ute attribute strongly correlated protected attribute weightlifting ability strongly lated gender however process selecting individual group based attribute trivial since group often result intersection multiple protected proxy attribute see social perspective section 1 protected potentially taged group selected implementation apply different bias metric see bias metric section compare identify relevant ences algorithm outcome different group difference consequence protected attribute likely algorithm decision considered discriminatory alleviate contextual problem whether algorithmic outcome may form case tion approach often incorporate explanatory ute attribute gender age specific context deemed acceptable ate even lead apparent discrimination protected attribute 18 some relevant approach ibm ai fairness 360 contains technique developed ibm research community help detect mitigate bias machine learning model throughout ai application lifecycle google offer interactive visual interface allows researcher investigate model performance range feature data set optimization strategy despite effort parameterizing context uncertainty technical implementation tive dimension separate bias discrimination remains challenge response some approach base implementation various tion law focus relationship tected attribute decision outcome instance court rule castaneda rule used general often arguably adequate prima facie evidence legal spective section detail rule approach intervene problematic bias focus 1 removing protected attribute data attempt impede algorithm using protected attribute make natory decision fairness blindness 12 17 2 debiasing algorithm output 19 issue removing protected attribute input data often result significant loss accuracy algorithm 17 moreover excluded attribute often correlated proxy attribute remain data set ing bias may still present certain residential 2 3 authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 76 feature ieee technology society magazine area specific demographic play role proxy variable ethnicity approach also criticized alter model world ai make use instead ing ai perceives act bias 17 broader level debiasing algorithm output requires specific definition context difficult achieve technical perspective only myriad lingering question remains answered much bias doe algorithm need encode consider output criminating reflect peculiarity data algorithm data often reflects inequity time short clearer definition relation algorithmic bias discrimination needed argue definition only provided approach take legal social ethical consideration account response next section engage cally related work legal social ethical perspective legal perspective legislation designed prevent discrimination particular group people share one protected protected name antidiscrimination law antidiscrimination law vary across country instance european antidiscrimination lation organized directive directive discrimination ground race ethnic origin chapter 3 eu ter fundamental right antidiscrimination law u described title vii civil right act 1964 federal state statute plemented court decision instance title vii prohibits discrimination employment basis race sex national origin religion equal pay act prohibits wage disparity based sex employer union main issue trial related tion consist determining 20 1 relevant population affected discrimination case group compared 2 discrimination measure formalizes group disparate treatment disparate impact 18 21 3 threshold constitutes prima facie evidence tion note three issue coincide problem explored technical approach presented earlier respect last point no strict threshold ha laid pean union rule equal employment opportunity commission 1978 state job selection rate protected group le tion rate unprotected group sometimes used prima facie evidence adverse impact castaneda rule state number people protected group selected relevant population not smaller 3 ard deviation number expected random selection also used 21 although law relieve discriminatory issue complex scenario arise instance hildebrandt koops 22 mention legally gray area price discrimination consumer different graphical area offered different price based difference average income recent regulation general data protection regulation gdpr offered framework alleviate some enforcement problem antidiscrimination law include clause automated ing related procedural regularity ability introducing right explanation individual obtain meaningful explanation logic involved automated decision ing take place however solution often assume scenario seen may difficult achieve technically even achieved may not sarily provide answer sought ass whether discrimination present not generally current law badly equipped address algorithmic crimination 21 leese 23 instance note antidiscrimination framework typically follow establishment causal chain indicator theoretical level sex race representation population scrutiny analytics however create aggregate individual profile prone production arbitrary category instead real community even data subject granted procedural relational tions question remains point tial bias reasonably considered form discrimination authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 77 june 2021 social perspective digital discrimination not only technical nomenon regulated law one also need considered perspective rigorously understood defining tutes discrimination matter understanding particular social historical condition idea inform need reevaluated ing implementation context bias usage defined form challenge any kind eralist ai solution one complication highlighted social spective potential digital discrimination reinforce existing social inequality point becomes increasingly pressing multiple tie experience exclusion subordination start phenomenon called tionality 24 one example formed ple way race gender interact class labor market effectively generating new identity category legislation perspective crimination law applied tion experienced population share one protected attribute however problem exponentially grow complexity also considering proxy variable intersection different feature 15 cultural ideological level call transparency ai system need seen ideal much form truth duction 25 furthermore no standard evaluation methodology exists among ai researcher ethically ass bias classification explanation classification serf different function different context 14 arguably assessed differently different people way data set defined curated instance depends tions value creator 26 conducting set experimental study elicit people response range algorithmic decision narios explanation decision binns et al 27 find strong split respondent some find general idea algorithmic ination immoral others resist imputing morality computer system altogether computer job 27 although algorithmic implicates dimension justice claim objectivity may also preclude public awareness dimension given differing stance discrimination society providing explanation public geted algorithmic system key allows individual make mind evaluation system hildebrand koops 22 instance call smart transparency designing cal infrastructure responsible ing way allows individual anticipate respond profiled context public evaluation also becomes important question moral standard encoded ai erations discrimination expected readily shared widely differing range citizen 28 although framework always criticized reductionist approach complexity social value keeping account kind value important society go some way helping establish discrimination defined ethical perspective finally need bring ethical tive tasioulas argues discrimination doe not need unlawful unfair 29 yet moral standard historically dynamic ously evolving due technological development explains law encoded social morality often lag behind technical development light discriminatory risk benefit ai might pose moral standard need reassessed ble new definition discriminatory impact say one famous attempt address tion robotics derives fiction isaac asimov three law robotics recently ai munity ha attempted codify ethical principle ai asilomar ai however principle criticized vague mainly due level abstraction making not necessarily helpful 29 grounded detailed framework ai ethic recently proposed standard defined ieee global initiative ethic autonomous intelligent aim provide incubation space new solution relevant ethical implementation intelligent technology another noteworthy 4 5 authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 78 feature ieee technology society magazine contribution presented 29 stating ethical question related usage ai organized three interconnected level first level involves law govern activity including public standard backed public institution enforcement mechanism claim morally binding citizen virtue formal enactment some effort discussed legal perspective section seen ples however evades problem not socially entrenched standard ern life legal standard rely not only law discourage people wrongful ior also moral standard instilled u childhood reinforced society second level social morality around ai definition morality problematic involves potential infinity reference point well cultivation emotional response guilt indignation effect human consciousness cognition 29 third final level includes individual engagement ai individual ciations still need exercise moral judgment instance devising code practice however level ationalized extent technical ai point view not yet clear open challenge addressing attesting digital discrimination remedying corresponding deficiency remain problem technical legal social ethical reason technically number practical limit accomplished particularly regarding ability automatically determine relationship bias crimination translate social reality machine code current legislation poorly equipped address classificatory complexity arising algorithmic discrimination social quality differing attitude toward computation obfuscate distinction bias discrimination ethical perspective ing moral standard need reassessed quently updated light risk benefit ai might pose sum design evaluation ai system rooted different perspective concern goal see table 1 posit existence defined path perspective would misleading needed instead ity distinction concerning desirable ai implementation dialogical orientation toward design process finding solution crimination ai requires robust collaboration conclude summarizing believe some important challenge advance research solution attesting avoiding tion ai much bias much whether biased decision considered discriminatory not depends many factor context ai going deployed group compared decision factor like tradeoff meritocratic value simplify problem technical tions tend borrow definition legal ature threshold constitute prima facie evidence discrimination use general rule attest algorithmic discrimination yet not addressed simply encoding legal social ethical context nontrivial bias discrimination different ontological status former may seem easy define term programmatic tions latter involves host social ethical issue challenging resolve ist framework critical ai literacy another challenge need ment critical ai literacy noted need take account ai sion making system extent literacy system targeted improved part entail knowledge table summary challenge different perspective authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 79 june 2021 particularity attribute used data set well ability compare explanation decision moral rule ing choice however not solely technical exercise system render algorithmically constructed data subject challenge could addressed approach sider technical dimension plex social context system deployed building public confidence greater democratic participation ai system requires ongoing development not explainable ai better interaction method sociotechnical platform tool public engagement increase critical public standing agency ai third ai not seen potential lem causing discrimination also great nity mitigate existing issue fact ai pick discrimination suggests made aware instance ai could help spot digital form crimination assist acting upon aim become reality would need explored work better understanding social ethical legal principle well dialogically constructed solution knowledge incorporated ai system two way achieve goal 1 using approach like machine learning actually look vious case discrimination try spot future 2 using ai operationalizes legal ples mentioned normative approach include nondiscrimination norm part edge ai system influence decision making would instance facilitate ai system realizing knowledge gathered learned resulting discriminatory decision deployed specific context hence ai system could alert expert human proactively address issue acknowledgment work wa supported engineering physical science research council epsrc grant part discovering attesting digital discrimination dadd see reference 1 neil weapon math destruction big data increase inequality threatens democracy portland usa broadway book 2017 2 pedreshi ruggieri f turini aware data mining proc proc acm sigkdd int conf knowl discovery data mining kdd 2008 pp 3 criado digital discrimination algorithmic regulation oxford oup 2019 4 danks london algorithmic bias autonomous system proc ijcai 2017 pp 5 ferrer et discovering categorising language bias reddit proc int aaai conf web social medium icwsm 2020 6 mueller et explanation system literature synopsis key idea publication bibliography explainable ai 2019 online available 7 guidotti et survey method explaining black box model acm comput vol 51 no 5 pp 2018 8 guidotti et factual counterfactual explanation black box decision making ieee intell vol 34 no 6 pp 2019 9 ruggieri pedreschi f turini integrating induction deduction finding evidence discrimination ai law vol 18 no 1 pp 2010 10 kilbertus et avoiding discrimination causal reasoning proc nip 2017 pp 11 byrne counterfactuals explainable artificial intelligence xai evidence human reasoning proc ijcai 2019 pp 12 calder zliobaite unbiased computational process lead discriminative decision procedure discrimination privacy information society berlin germany springer 2013 pp 13 criado ferrer normative approach attest digital discrimination proc advancing towards sdgs artif intell fair equitable world workshop eur conf artif intell ecai 2020 14 miller explanation artificial intelligence insight social science artif vol 267 pp 2019 15 et beyond distributive fairness algorithmic decision making proc aaai 2018 online available authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 80 feature ieee technology society magazine 16 verma rubin fairness definition explained proc fairware may 2018 pp 17 dwork et fairness awareness proc itcs new york ny usa acm 2012 pp 18 feldman et certifying removing disparate impact proc new york ny usa acm 2015 pp 19 bolukbasi et man computer programmer woman homemaker debiasing word embeddings proc adv neural inf process 2016 pp 20 romei ruggieri multidisciplinary survey discrimination analysis knowl eng vol 29 no 5 2014 art no 582638 21 barocas selbst big data disparate impact california law vol 104 no 3 pp jun 2016 online available 22 hildebrandt koops challenge ambient law legal protection profiling era mod law vol 73 no 3 pp 2010 23 leese new profiling algorithm black box failure safeguard european union secur dialogue vol 45 no 5 pp 2014 24 walby armstrong strid intersectionality multiple inequality social theory sociology vol 46 no 2 pp apr 2012 25 ananny crawford seeing without knowing limitation transparency ideal application algorithmic accountability new medium vol 20 no 3 pp mar 2018 26 van nuenen et transparency assessing discriminatory artificial intelligence computer vol 53 no 11 pp 2020 27 binns et reducing human percentage perception justice algorithmic decision proc chi new york ny usa acm 2018 377 28 curry mullins whitehouse good cooperate testing theory 60 society current vol 60 no 1 pp 2019 29 tasioulas first step towards ethic robot artificial intelligence practical ethic vol 7 no 1 xavier ferrer received degree informatics artificial intelligence institute spanish national research council barcelona spain universitat autonoma de barcelona uab barcelona currently research associate digital discrimination department informatics king college london london research interest related natural language processing machine learning fairness tom van nuenen received degree cultural study tilburg university tilburg netherlands currently research associate department informatics king college london london visiting scholar digital humanity university california berkeley berkeley ca usa focus use mixed method identify solve social ethical development question related big data artificial intelligence ai machine learning jose currently reader associate professor department informatics director kcl cybersecurity centre king college london london ha principal investigator number large project funded epsrc including discovering attesting digital discrimination dadd project secure ai assistant sais project research interest intersection artificial intelligence ai computer interaction cybersecurity strong focus ai security ethic privacy mark cot√© currently senior lecturer data culture society department digital humanity king college london london principal investigator pi investigator ci range ukri grant namely engineering physical science research council art humanity research council involved research critical interdisciplinary method focusing social cultural dimension big data algorithm machine learning natalia criado currently senior lecturer computer science department informatics ukri centre doctoral training safe trusted ai king college london london research interest computational norm normative multiagent system application multiagent system data science artificial intelligence enhance cybersecurity privacy direct question comment article xavier ferrer king college london london authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply