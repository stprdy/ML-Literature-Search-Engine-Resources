deep neuroevolution genetic algorithm competitive alternative training deep neural network reinforcement learning felipe petroski vashisht madhavan edoardo conti joel lehman kenneth stanley jeff clune uber ai lab jeffclune abstract deep artiﬁcial neural network dnns ically trained via learning gorithms namely backpropagation evolution strategy e rival rithms policy ents challenging deep reinforcement learning rl problem however e ered algorithm form stochastic gradient descent via ation similar tion gradient raise question whether evolutionary gorithms work dnn scale demonstrate evolve weight dnn simple based genetic algorithm ga performs well hard deep rl problem including atari humanoid locomotion deep ga cessfully evolves network four lion free parameter largest neural network ever evolved traditional evolutionary gorithm result 1 expand sense scale gas operate 2 gest intriguingly some case following gradient not best choice optimizing performance 3 make immediately able multitude neuroevolution technique improve performance demonstrate latter showing combining dnns novelty search encourages exploration task deceptive sparse reward function solve problem algorithm dqn e ga fail additionally deep ga faster e dqn train atari hour one desktop hour distributed 720 core enables compact encoding technique introduction recent trend machine learning ai research old algorithm work remarkably well combined sufﬁcient computing resource data ha story 1 backpropagation applied deep ral network supervised learning task puter vision krizhevsky et 2012 voice nition seide et 2011 2 backpropagation deep neural network combined traditional reinforcement learning algorithm watkins dayan 1992 mnih et 2015 policy gradient pg method sehnke et 2010 mnih et 2016 3 evolution strategy e applied reinforcement learning mark salimans et 2017 one common theme method including e involves gradient approximation similar ﬁnite difference williams 1992 wierstra et 2008 man et 2017 historical trend raise question whether similar story play method gas paper investigates question testing mance simple ga hard deep reinforcement learning rl benchmark including atari 2600 bellemare et 2013 brockman et 2016 mnih et 2015 manoid locomotion mujoco simulator todorov et 2012 schulman et 2015 2017 brockman et 2016 compare performance ga contemporary algorithm applied deep rl dqn mnih et 2015 method mnih et 2016 policy gradient method e one might pect gas perform far worse method simple not follow gradient ingly found gas turn competitive gorithm rl performing better some domain worse others roughly well overall dqn e adding new family algorithm box deep rl problem also validate ness learning gas comparing performance random search r ga always performs random search interestingly discovered 20 apr 2018 genetic algorithm competitive alternative training deep neural network reinforcement learning some atari game random search outperforms ful deep rl algorithm dqn game e suggesting local optimum dle point noisy gradient estimate some force impeding progress problem method note although deep neural network often not struggle local optimum supervised learning pascanu et 2014 local optimum remain issue rl reward signal may deceptively encourage agent perform action prevent discovering globally optimal behavior like e deep rl algorithm ga ha unique beneﬁts gas prove slightly faster e discussed low ga e thus substantially faster speed policy gradient method explore two distinct ga implementation 1 version gpus cpu 2 distributed version many cpu across many chine single modern desktop 4 gpus 48 cpu core ga train atari hour training comparable performance take day dqn day speedup enables individual researcher single albeit expensive desktop start using main formerly reserved lab only ate perhaps quickly any rl algorithm given substantial distributed computation 720 cpu core across dozen machine ga e train atari hour also beneﬁcial via new technique introduce even network trained gas encoded thousand byte yielding compact encoding method overall unexpectedly competitive performance ga random search suggests structure search space some domain not amenable search realization open new search direction exploit region search might appropriate motivates research new kind hybrid algorithm background high level rl problem challenge agent imize some notion cumulative reward total counted without supervision accomplish goal sutton barto 1998 host traditional rl gorithms perform well small tabular state space ton barto 1998 however scaling problem learning act directly pixel wa challenging rl algorithm harnessed tional power deep neural network dnns thus alyzing ﬁeld deep reinforcement learning deep rl mnih et 2015 three broad family deep ing algorithm shown promise rl problem far method dqn mnih et 2015 icy gradient method sehnke et 2010 mnih et 2016 trpo schulman et 2015 ppo man et 2017 recently evolution strategy e salimans et 2017 deep algorithm approximate optimal q function dnns yielding policy given state choose action maximizes watkins dayan 1992 mnih et 2015 hessel et 2017 policy gradient method directly learn ters dnn policy output probability taking action state team openai recently experimented simpliﬁed version natural tion strategy wierstra et 2008 speciﬁcally one learns mean distribution parameter not variance found algorithm refer simply evolution strategy e competitive dqn difﬁcult rl benchmark problem much faster training time faster time many cpu available due better tion salimans et 2017 method considered method calculate approximate gradient dnn optimize parameter via stochastic gradient though not require differentiating reward function simulator dqn culates gradient loss dnn tion approximator via backpropagation policy gradient sample behavior stochastically current policy reinforce perform well via stochastic ent ascent e doe not calculate gradient analytically approximates gradient reward function rameter space salimans et 2017 wierstra et 2008 test whether truly method ga perform well challenging deep rl task ﬁnd gas perform surprisingly well thus considered new addition set algorithm deep rl problem method genetic algorithm purposefully test extremely simple ga set baseline well evolutionary rithms work rl problem expect future work reveal adding legion enhancement exist gas fogel stayton 1994 haupt haupt 2004 clune et 2011 mouret doncieux 2009 lehman ley stanley et 2009 mouret clune 2015 improve performance deep rl task genetic algorithm holland 1992 eiben et 2003 evolves population p n individual neural genetic algorithm competitive alternative training deep neural network reinforcement learning work parameter vector θ often called genotype ery generation θi evaluated producing ﬁtness score aka reward f θi ga variant performs cation selection wherein top individual become parent next generation produce next tion following process repeated n time ent selected uniformly random replacement mutated applying additive gaussian noise eter vector θ σϵ ϵ 0 priate value σ wa determined empirically periment described supplementary information si table n th individual unmodiﬁed copy best individual previous generation technique called elitism reliably try select true elite presence noisy evaluation evaluate top 10 individual per generation 30 additional episode counting frame one consumed training one highest mean score designated elite historically gas often involve crossover combining parameter multiple parent produce offspring simplicity not include new tion evaluated process repeat g erations some stopping criterion met si algorithm 1 provides pseudocode version open source code hyperparameter conﬁgurations experiment available hyperparameters also listed si table hyperparameters ﬁxed atari game chosen set 36 hyperparameters tested six game terix enduro gravitar kangaroo seaquest venture ga implementation traditionally store individual parameter vector θ approach scale poorly ory network transmission cost large population large deeper wider neural network propose novel method store large parameter vector compactly representing parameter vector initialization seed plus list random seed produce series mutation produced θ θ reconstructed innovation wa critical efﬁcient implementation distributed deep si fig 2 show eq 1 describes method θn ψ τn σε τn 1 θn offspring ψ τn ministic mutation function τ encoding θn ing list mutation seed φ φ deterministic initialization function ε τn 0 deterministic gaussian number ator input seed τn produce vector length case ε τn large precomputed table indexed using seed si sec provides encoding detail including seed could smaller technique advantageous size compressed representation increase linearly ber generation often order thousand dent size network often order million doe course require computation struct dnn weight vector competitive agent evolve little ten generation enabling compressed representation parameter neural work thousand byte sion compression rate depends number erations practice always substantial atari ﬁnal network compressible resents state art encoding large network pactly however not general network compression technique not compress arbitrary network instead only work network evolved one motivation choosing e versus icy gradient method faster time tributed computation owing better parallelization man et 2017 found distributed deep ga not only preserve beneﬁt slightly prof upon si sec describes faster e importantly gas also use gpus speed forward pas dnns especially large one making possible train single top implementation one ern desktop train atari hour take hour 720 distributed core distributed gpu training would speed training large population size novelty search one beneﬁt training deep neural network gas enables u immediately take advantage rithms previously developed neuroevolution munity demonstration experiment novelty search n lehman stanley wa signed deceptive domain mization mechanism converge local optimum n avoids local optimum ignoring reward function evolution instead rewarding agent performing haviors never performed novel surprisingly often outperform algorithm utilize reward signal result demonstrated maze navigation simulated biped locomotion task lehman stanley apply n see form combined dnns deceptive based rl problem call image hard maze refer ga optimizes novelty n requires behavior characteristic bc describes behavior policy bc π behavioral tance function bcs any two policy dist bc πi bc πj genetic algorithm competitive alternative training deep neural network reinforcement learning generation member population probability p bc stored archive novelty policy deﬁned average distance k 25 nearest neighbor sorted behavioral distance population archive novel individual thus determined based behavioral distance current previously seen individual ga otherwise proceeds normal substituting novelty ness reward reporting plotting purpose only identify individual highest reward per tion algorithm presented si algorithm 2 experiment experiment focus performance ga challenging problem validated fectiveness deep rl algorithm e salimans et 2017 include learning play atari directly pixel mnih et 2015 schulman et 2017 mnih et 2016 bellemare et 2013 tinuous control problem involving simulated humanoid robot learning walk brockman et 2016 schulman et 2017 salimans et 2017 todorov et 2012 also tested maze domain ha clear local optimum image hard maze study well algorithm avoid deception lehman stanley atari image hard maze experiment record best agent found multiple independent domly initialized ga run 5 atari 10 image hard maze atari stochastic ﬁnal score run calculated taking elite across generation averaging score achieves 200 independent evaluation ﬁnal score main median ﬁnal run score humanoid comotion detail si sec atari training deep neural network play atari mapping rectly pixel action wa celebrated feat arguably launched deep rl era expanded derstanding difﬁculty rl domain machine learning could tackle mnih et 2015 test performance dnns evolved simple ga pare dnns trained major family deep rl gorithms e model experiment e paper salimans et al 2017 since inspired study due limited computational resource compare result 13 atari game some chosen game e performs well frostbite gravitar kangaroo venture zaxxon poorly amidar enduro skiing seaquest remaining game chosen ale bellemare et 2013 set alphabetical order assault asterix asteroid atlantis facilitate comparison result reported salimans et al 2017 keep number game frame agent experience course ga run constant one billion frame frame limit result differing number generation per independent ga run si sec table 3 policy ent quality different run may see frame some game agent life longer training agent evaluated full episode capped frame include multiple life ﬁtness sum episode reward ﬁnal atari game score following identical dqn mnih et 2015 1 data preprocessing 2 network tecture 3 stochastic environment start episode 30 random initial operation use larger dqn architecture mnih et al 2015 consisting 3 convolutional layer 32 64 64 channel followed hidden layer 512 unit convolutional layer use 8 8 4 4 3 3 ﬁlters stride 4 2 1 respectively hidden er followed rectiﬁer nonlinearity relu network contains parameter interestingly many past assumed simple ga would fail scale result ga implementation fair comparison algorithm difﬁcult tion procedure algorithm realize ferent computation speed sample efﬁciency another consideration whether agent evaluated random start random number action regime trained start randomly sampled human play test generalization nair et 2015 not database human start sample agent evaluated random start possible pare result algorithm random start true dqn e not include result human start also attempt control number frame seen training dqn far slower run present result literature train fewer frame requires day computation hour computation needed e ga train frame many variant dqn could compare including rainbow hessel et 2017 gorithm combine many different recent improvement dqn van hasselt et 2016 wang et 2015 schaul et 2015 sutton barto 1998 bellemare et 2017 fortunato et 2017 however choose compare ga original vanilla dqn algorithm partly cause also introduce vanilla ga without many modiﬁcations improvement previously developed haupt haupt 2004 genetic algorithm competitive alternative training deep neural network reinforcement learning likely surprise many simple ga able train deep neural network play many atari game roughly well dqn e table 1 among 13 game tried dqn e ga duced best score 3 game produced best score skiing ga produced score higher any algorithm date aware including dqn variant rainbow dqn paper hessel et 2017 some game ga formance advantage dqn e erable frostbite venture skiing video cies evolved ga viewed http comparison ga performs better e dqn 6 game 13 table 1 4 ga also performs worse many game ing theme deep rl different family rithms perform differently across different domain man et 2017 however comparison liminary computational resource needed gather sufﬁcient sample size see algorithm signiﬁcantly different per game instead key takeaway tend perform roughly similarly doe well different game performance not plateau ga run test whether ga improves given additional putation thus run ga six time longer frame game score improves table 1 score ga outperforms e dqn 7 8 7 13 game parisons respectively si table 4 game ga performance still ha not converged frame si fig 3 leaving open question well ga ultimately perform run even longer knowledge parameter neural network largest neural network ever evolved simple one remarkable fact quickly ga ﬁnds performing individual employ large ulation size run last relatively generation min 348 max si table 3 many game ga ﬁnds solution better dqn only one ten generation speciﬁcally median ga performance higher ﬁnal dqn performance 1 1 3 5 11 29 generation skiing venture frostbite oids gravitar zaxxon respectively similar result hold e 1 2 3 7 12 25 ga tions needed outperform e skiing frostbite amidar asterix asteroid venture respectively number generation required beat 1 1 1 1 1 2 52 enduro frostbite kangaroo skiing venture gravitar amidar respectively generation ga tends make change controlled σ parameter vector see method ga outperforms dqn e generation especially doe ﬁrst generation round selection suggests many policy exist near origin precise near region random tion function generates policy raise question ga anything random search answer question evaluate many policy domly generated ga initialization function φ report best score gave random search mately amount frame computation ga compared performance table 1 every game ga outperformed random search niﬁcantly game fig 3 p future p value via wilcoxon test proved performance suggests ga performing healthy optimization generation surprisingly given celebrated impressive dqn e 13 game random search actually outperforms dqn 3 frostbite skiing venture e 3 amidar frostbite skiing 6 enduro frostbite gravitar kangaroo skiing venture ingly some policy produced random search not trivial degenerate policy instead appear quite sophisticated consider following example game frostbite requires agent perform long sequence jump row iceberg ing different direction avoiding enemy tionally collecting food build igloo brick brick si fig 4 only igloo built agent ter igloo receive large payoff ﬁrst two life policy found random search completes series 17 action jumping 4 row iceberg moving different direction avoiding enemy back three time construct igloo only igloo built agent immediately move towards enters point get large reward repeat entire process harder level time also gathering food thus earning bonus point video policy sulted high score le 1 hour random search average score 797 produced dqn day optimization one may think random search found lucky open loop sequence tions overﬁt particular stochastic environment markably found policy actually generalizes initial condition achieving median score 95 bootstrapped median conﬁdence interval 200 different test environment 30 random initial standard testing cedure hessel et 2017 mnih et 2015 genetic algorithm competitive alternative training deep neural network reinforcement learning dqn e r ga ga frame time forward pass backward pass 0 0 0 0 operation u u u u u u amidar 978 112 264 143 263 377 assault 649 714 814 asterix asteroid atlantis enduro 729 95 36 60 80 frostbite 797 370 191 gravitar 473 805 304 431 476 764 kangaroo 94 seaquest 503 798 850 skiing venture 163 760 23 488 969 zaxxon table atari simple genetic algorithm competitive dqn policy gradient evolution gy e shown game score higher better comparing performance algorithm inherently challenging see main text attempt facilitate comparison showing estimate amount computation operation sum forward backward neural network pass data efﬁciency number game frame training episode long time algorithm take run e dqn ga perform best 3 3 4 3 game respectively surprisingly random search often ﬁnds policy superior dqn e see text discussion note dramatic difference speed algorithm much faster ga e data efﬁciency favor dqn score dqn hessel et al 2017 e salimans et al 2017 dqn e not provide error bar not reported original literature ga random search error bar visualized si fig 3 time approximate depend variety factor found ga run slightly faster e average symbol indicates state art performance ga score bolded best not prevent bolding column example success r versus dqn e suggest many atari game seem hard based low performance leading deep rl algorithm may not hard think instead algorithm some reason performing poorly task tually quite easy result suggest time best search strategy not follow gradient instead conduct dense search local hood select best point found subject return discussion sec 5 image hard maze experiment seek demonstrate beneﬁt gas working dnn scale algorithm developed improve gas immediately taken shelf improve dnn training example algorithm novelty search n popular evolutionary method exploration rl lehman stanley n wa originally motivated hard maze domain lehman stanley staple roevolution community demonstrates problem cal optimum aka deception reinforcement learning robot receives reward closer get goal crow ﬂies problem deceptive ily getting closer goal lead agent permanently get stuck one map deceptive trap fig 1 left optimization algorithm not conduct sufﬁcient ploration suffer fate n solves problem ignores reward encourages agent visit new place lehman stanley original version problem involves only put radar sensor sense wall two continuous put speed forward backward rotation making solvable small neural network ten connection want demonstrate beneﬁts n scale deep neural network introduce new version domain called image hard maze like many atari game show bird view world agent form 84 84 pixel image fig 1 left change make problem easier some way fully observable harder others genetic algorithm competitive alternative training deep neural network reinforcement learning figure image hard maze domain result left small wheeled robot must navigate goal bird view pixel input robot start bottom left corner facing right right novelty search train deep neural network avoid local optimum stymie algorithm ga solely optimizes reward ha no incentive explore get stuck local optimum trap ga optimizing novelty encouraged ignore reward explore whole map enabling eventually ﬁnd goal e performs even worse ga discussed main text dqn also fail solve task e performance mean θ policy iteration plotted ga performance individual per generation plotted dqn not number evaluation per iteration evolutionary algorithm plot ﬁnal median reward dashed line si fig 5 show behavior algorithm training much neural network must learn process pixel input take action si sec ha additional experimental detail conﬁrm result held small neural work original version task also hold visual version task deep neural network parameter network processing pixel novelty search able solve task ﬁnding goal fig 1 ga optimizes reward only expected get stuck local optimum trap 2 si fig 5 thus fails solve problem fig 1 signiﬁcantly underperforming p result conﬁrm able use exploration method novelty search solve sort deception even problem involving learning directly pixel largest neural network optimized novelty search date three order magnitude companion paper conti et 2017 also demonstrate similar ﬁnding hybridizing novelty search e create show help deep neural network avoid tion challenging rl benchmark domain expected e also fails solve task cu solely maximizing reward fig 1 si fig 5 also test dqn policy gradient problem not source code able obtain source code ha similar formance wu et 2017 only difference synchronous instead asynchronous iments modiﬁed reward domain step reward negative change distance goal since last plotting purpose record ﬁnal distance goal reward dard algorithm provides information doe not remove deception dqn requires discrete output discretize two uous output ﬁve equally sized bin enable possible output combination learns 52 25 also expected dqn fail solve problem fig 1 si fig 5 default exploration mechanism not enough ﬁnd global optimum given tive reward function domain dqn drawn expected trap unclear reason even though visit trap 2 often early training converges ting stuck different part maze course ration technique could added control tially make perform well only sought show deep ga allows algorithm oped neural network harnessed hard problem require dnns future work interesting combine n deep ga domain including atari robotics domain importantly demonstration suggests algorithm enhance gas bined dnns perhaps promising combine notion diversity novelty quality high performing seeking collect set performing yet interestingly different policy mouret clune 2015 lehman stanley cully et 2015 pugh et 2016 result also motivate future research combining deep rl algorithm dqn novelty search quality diversity algorithm humanoid locomotion ga wa also able solve challenging continuous control benchmark humanoid locomotion brockman et 2016 ha validated modern powerful rithms trpo e ga duce robot could walk well took time longer perform slightly worse e si sec surprising gas previously performed well robot locomotion task clune et 2011 huizinga et 2016 future research required understand discussion surprising success ga r domain thought require least some degree gradient genetic algorithm competitive alternative training deep neural network reinforcement learning tion suggests some heretofore aspect search space imply densely sampling region around origin sufﬁcient some case ﬁnd far better solution found method even far putation time suggesting gradient not point solution optimization sue interfere ﬁnding saddle point noisy gradient estimate ga result suggest sampling region around good solution ten sufﬁcient ﬁnd even better solution quence discovery possible many challenging domain result turn implies distribution solution increasing quality unexpectedly dense not need follow gradient ﬁnd another exclusive hypothesis gas e improved performance due temporally tended exploration osband et 2016 meaning plore consistently since action episode tion set mutated parameter improves exploration plappert et 2017 help exploration two reason 1 agent take action ha distribution action time visit state make easier learn whether icy state advantageous 2 agent also likely correlated action across state way go mutation internal tions affect action taken many state similarly perhaps interesting result sometimes actually worse follow gradient sample locally parameter space better solution scenario probably doe not hold domain even region domain sometimes hold hold expands conceptual understanding viability different kind search operator reason ga might outperform method local optimum present jump ter space whereas gradient method not without tional optimization trick momentum although note e utilized modern adam optimizer experiment kingma ba 2014 includes mentum one unknown question whether cal search better early search ce switching search later allows progress would impossible prohibitively computationally expensive ga make another known question promise simultaneously ing ga method modern algorithm deep rl policy gradient evolution strategy still know little ultimate promise gas versus competing algorithm training deep neural work reinforcement learning problem additionally used extremely simple ga many technique invented improve ga performance eiben et 2003 haupt haupt 2004 including crossover holland 1992 deb myburgh 2016 indirect ing stanley 2007 stanley et 2009 clune et 2011 encouraging quality diversity mouret clune 2015 pugh et 2016 name moreover many technique invented dramatically improve training dnns backpropagation ual network et 2015 selu relu activation function krizhevsky et 2012 klambauer et 2017 lstms grus hochreiter schmidhuber 1997 cho et 2014 regularization hoerl kennard 1970 dropout srivastava et 2014 annealing learning rate schedule robbins monro 1951 hypothesize many technique also improve lution large dnns some enhancement may improve ga mance humanoid locomotion example indirect encoding allows genomic parameter affect tiple weight ﬁnal neural network way similar convolution tied weight far ity ha shown dramatically improve performance data efﬁciency evolving robot gait clune et 2011 result found hyperneat gorithm stanley et 2009 ha indirect coding abstract power developmental biology stanley 2007 particularly promising direction humanoid locomotion atari investigating interesting learn domain deep ga tends perform well poorly understand also gas could help main architecture search liu et 2017 ikkulainen et 2017 training limited precision binary neural network conclusion work introduces deep ga competitively train deep neural network challenging rl task coding technique enables efﬁcient distributed training compact network encoding found ga fast enabling training atari gle desktop distributed 720 cpu mented gas surprisingly competitive lar algorithm deep reinforcement learning problem dqn e especially challenging atari domain also showed interesting algorithm developed neuroevolution community mediately tested deep neural network ing deep novelty search solve deceptive game interesting see ture research investigate potential limit gas especially combined technique known genetic algorithm competitive alternative training deep neural network reinforcement learning improve ga performance generally result tinue story started backprop extended e old simple algorithm plus modern amount computation perform amazingly well raise question old algorithm revisited acknowledgement thank member uber ai lab ful suggestion throughout course work ticular zoubin ghahramani peter dayan noah goodman thomas miconi theofanis karaletsos also thank justin pinkul mike deats cody yancey entire opusstack team uber providing resource nical support thanks also david ha many helpful suggestion improved paper reference bellemare naddaf veness bowling arcade learning environment evaluation form general agent artif intell jair bellemare dabney munos butional perspective reinforcement learning arxiv preprint brockman cheung pettersson schneider schulman tang zaremba openai gym cho van enboer bahdanau gio property neural machine lation approach arxiv preprint clune stanley pennock ofria performance indirect encoding across uum regularity ieee transaction evolutionary computation conti madhavan petroski lehman ley clune improving exploration lution strategy deep reinforcement learning via population agent arxiv preprint cully clune tarapore mouret robot adapt like animal nature doi deb myburgh breaking rier optimization using customized lutionary algorithm gecco eiben smith others introduction lutionary computing volume springer fogel stayton effectiveness crossover simulated evolutionary optimization biosystems fortunato azar piot menick osband graf mnih munos hassabis pietquin others noisy network exploration arxiv preprint glorot bengio understanding difﬁculty training deep feedforward neural network icai haupt haupt practical genetic algorithm john wiley son zhang ren sun deep residual learning image recognition arxiv preprint hessel modayil van hasselt schaul ostrovski dabney horgan piot azar silver rainbow combining improvement deep forcement learning arxiv preprint hochreiter schmidhuber long ory neural computation hoerl kennard ridge regression biased estimation nonorthogonal problem technometrics holland genetic algorithm scientiﬁc american huizinga mouret clune doe aligning notypic genotypic modularity improve evolution neural network gecco ioffe szegedy batch normalization ing deep network training reducing internal covariate shift icml kingma ba adam method stochastic mization arxiv preprint klambauer unterthiner mayr hochreiter neural network arxiv preprint krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural network nip lehman stanley abandoning objective lution search novelty alone evolutionary computation lehman stanley evolving diversity virtual creature novelty search local competition gecco isbn doi 10 lehman chen clune stanley e traditional tor arxiv preprint liu simonyan vinyals fernando kavukcuoglu hierarchical representation efﬁcient architecture search arxiv preprint miikkulainen liang meyerson rawal fink francon raju navruzyan duffy hodjat evolving deep neural network arxiv preprint genetic algorithm competitive alternative training deep neural network reinforcement learning mnih kavukcuoglu silver rusu veness bellemare graf riedmiller fidjeland ostrovski others control deep reinforcement learning nature mnih badia mirza graf lillicrap harley silver kavukcuoglu asynchronous method deep reinforcement learning icml mouret clune illuminating search space ping elite arxiv mouret doncieux overcoming bootstrap problem evolutionary robotics using behavioral sity proceeding ieee congress ary computation nair srinivasan blackwell alcicek fearon de maria panneershelvam suleyman beattie petersen others massively parallel od deep reinforcement learning arxiv preprint osband blundell pritzel van roy deep exploration via bootstrapped dqn nip pascanu dauphin ganguli bengio saddle point problem optimization arxiv preprint plappert houthooft dhariwal sidor chen chen asfour abbeel andrychowicz parameter space noise exploration arxiv preprint pugh soros stanley quality sity new frontier evolutionary computation issn robbins monro stochastic approximation method annals mathematical statistic salimans ho chen sidor sutskever lution strategy scalable alternative ment learning arxiv salimans goodfellow zaremba cheung ford chen improved technique training gans nip salimans ho chen sutskever tion strategy scalable alternative reinforcement learning arxiv preprint schaul quan antonoglou silver prioritized experience replay arxiv preprint schulman levine abbeel jordan moritz trust region policy optimization icml schulman wolski dhariwal radford klimov proximal policy optimization algorithm arxiv preprint sehnke osendorfer uckstieß graf peter schmidhuber policy dients neural network seide li yu conversational speech tion using deep neural network interspeech 2011 srivastava hinton krizhevsky sutskever salakhutdinov dropout simple way prevent neural network overﬁtting journal chine learning research stanley compositional pattern producing network novel abstraction development genetic ming evolvable machine special issue mental system stanley ambrosio gauci indirect encoding evolving scale neural network artiﬁcial life sutton barto reinforcement learning introduction volume mit press cambridge todorov erez tassa mujoco physic gine control intelligent robot system iros 2012 international ence van hasselt guez silver deep reinforcement learning double aaai wang schaul hessel van hasselt tot de freitas dueling network tures deep reinforcement learning arxiv preprint watkins dayan machine learning wierstra schaul peter schmidhuber ural evolution strategy evolutionary computation cec 2008 ieee world congress tional intelligence ieee congress williams simple statistical rithms connectionist reinforcement learning chine learning wu mansimov grosse liao ba able method deep reinforcement learning using approximation nip genetic algorithm competitive alternative training deep neural network reinforcement learning supplementary information ga faster e ga faster e two main reason 1 ery generation e must calculate update neural network parameter vector doe via weighted age across many salimans et al 2017 offspring random θ perturbation weighted ness averaging operation slow large neural work large number latter required healthy optimization not required deep 2 e requires require virtual batch malization generate diverse policy amongst offspring necessary accurate ﬁnite difference approximation salimans et 2016 virtual batch malization requires additional forward pass ence random set observation chosen start compute layer normalization statistic used manner batch normalization ioffe szegedy 2015 found random ga rameter perturbation generate sufﬁciently diverse policy without virtual batch normalization thus avoid additional forward pass network algorithm 1 simple genetic algorithm input mutation function ψ population size n number selected individual policy initialization routine φ ﬁtness function g 1 2 g generation 1 n next generation population g 1 φ n 0 initialize random dnn else k uniformrandom 1 select parent pg ψ k mutate parent end evaluate fi f pg end sort pg descending order fi g 1 set elite candidate c 1 10 else set elite candidate c 1 9 elite end set elite 1 30 f θ pg elite pg elite only include elite end return elite algorithm 2 novelty search input mutation function ψ population size n number selected individual policy initialization routine φ empty archive archive insertion probability p elty function η behavior characteristic function bc g 1 2 g generation 2 n next generation population g 1 φ n 0 initialize random dnn else k uniformrandom 1 select parent pg ψ k mutate parent end bcg bc pg end copy pg 1 1 bcg 1 1 1 n next generation population evaluate fi η bcg bcg 1 add bcg probability p end end sort pg descending order fi end return elite hyperparameters use xavier initialization glorot bengio 2010 policy initialization function φ bias weight set zero connection weight drawn standard normal distribution variance nin number incoming connection neuron hyperparameter humanoid image atari locomotion hard maze population size n mutation power σ truncation size 625 61 20 number trial 5 1 1 archive probability table hyperparameters population size incremented account elite many unusual number found via preliminary hyperparameter search domain additional information deep ga compact encoding method compact encoding technique based ple seed need only long enough generate unique mutation vector per offspring per parent any given parent produce x offspring τn genetic algorithm competitive alternative training deep neural network reinforcement learning game minimum median maximum generation generation generation amidar 1325 1364 1541 assault 501 707 1056 asterix 494 522 667 asteroid 1096 1209 1261 atlantis 507 560 580 enduro 348 348 348 frostbite 889 1016 1154 gravitar 1706 1755 1834 kangaroo 688 787 862 seaquest 660 678 714 skiing 933 1237 1281 venture 527 606 680 zaxxon 765 810 823 table number generation ga reached frame eq 1 small x number special case need one unique seed n per θ vector generation 0 thus encoded n bit reason seed vastly smaller search space size not every point search space possible offspring θn only need able generate offspring randomly not need able reach any point search space one random step however perform n dom mutation produce θn process reach many point search space however truly able reach every point search space need set mutation vector span search space meaning need seed least bit use function h θ τ map given θ τn new seed applies ψ τn ε h τn 2 note case two notion seed encoding series small τ seed new seed τ generated parent θ previous seed additional experimental detail image hard maze temporal context current frame previous three frame input timestep following mnih et al 2015 output remain original hard maze problem formulation lehman stanley unlike atari domain image hard maze ment deterministic doe not need multiple tions policy following lehman stanley bc x position robot end episode 400 timesteps behavioral distance function squared euclidean distance ﬁnal x tions simulator ignores forward backward motion would result robot penetrating wall preventing robot sliding along wall although rotational motor command still usual effect situation humanoid locomotion tested ga challenging continuous control problem speciﬁcally humanoid locomotion test mujoco environment openai gym todorov et 2012 brockman et 2016 volves simulated humanoid robot learning walk ing problem ha validated modern powerful rithms mnih et 2016 trpo schulman et 2015 e salimans et 2017 problem involves mapping vector 376 scalar describe state humanoid position locity angle 17 joint torque robot receives scalar reward combination four component timestep get positive reward standing velocity positive x direction negative reward energy expends hard impact ground four term summed every timestep episode calculate total reward stabilize training normalize dimension input subtracting mean dividing standard deviation computed executing dom policy environment also applied ing mutation power σ decreasing generation resulted small performance boost end training full set ters listed si table experiment ran 5 independent randomly tialized run report median run elite selection routine not reevaluate offspring 30 time like atari experiment ran experiment atari experiment improved evaluation method experiment completed not computational source experiment changed tocol not believe change would qualitatively alter result also used normalized column initialization routine salimans et al 2017 instead xavier initialization found perform itatively similarly determining ﬁtness agent evaluate mean 5 independent episode generation plotting purpose only uate elite 30 time genetic algorithm competitive alternative training deep neural network reinforcement learning 𝜃 𝜙 𝜏 𝜃 𝜃 𝜃 𝜓 𝜃 𝜏 𝜃 𝜃 𝜎𝜀𝜏 𝜓 𝜃 𝜃 1 𝜃 1 1 𝜓 𝜃 2 𝜃 2 2 𝜓 𝜃 23 𝜃 23 23 weight 𝜏 𝜏 𝜏 𝜏 𝜏 𝜏 𝜏 𝜏 𝜏 lineage encoding figure visual representation deep ga encoding method randomly initialized parameter vector produced initialization function φ seeded mutation function ψ seeded applies mutation result ﬁnal parameter vector θg result series mutation recreating θg done applying mutation step order thus knowing series seed τg produced series mutation enough information reconstruct θg initialization mutation function deterministic since τ small 28 bit long number generation low order hundred thousand large neural network parameter vector stored compactly architecture ha two hidden layer tanh activation function architecture one conﬁguration ﬁle included source code released salimans et al 2017 architecture described paper similar smaller 64 neuron per layer salimans et 2017 although relatively shallow deep learning standard much smaller atari dnns architecture still contains parameter order magnitude greater largest ral network evolved robotics task aware contained huizinga et 2016 fore 800 parameter clune et 2011 many sumed evolution would fail larger scale network hundred thousand million weight paper previous work ha called problem solved score salimans et 2017 ga achieves median level tions however requires far computation e e requires generation median formance surpass threshold not clear ga requires much computation cially given quickly ga found policy atari domain also surprising ga doe not excel domain given gas formed well past robot control task clune et 2011 ga need far computation domain interesting nevertheless doe ally solve producing agent walk score considering fast discovery performing solution atari clearly ga advantage versus method depends domain standing dependence important target future research dqn e r ga ga dqn 6 6 3 6 7 e 7 7 3 6 8 7 6 6 6 7 r 10 10 7 13 13 ga 7 7 7 0 13 ga 6 5 6 0 0 table comparison algorithm 13 atari game value represents many game algorithm listed top column produce higher score algorithm listed left row ga beat dqn 7 game meaning frame many paper including report number frame used training however bit unclear literature meant exactly term hope introduce some terminology lend clarity confusing issue improve reproducibility ability compare algorithm fairly imagine emitted frame training gest calling game one could every frame indeed due frame skip atari paper exactly repeat previous action skipped frame resulting frame suggest calling frame training frame frame algorithm trained paper genetic algorithm competitive alternative training deep neural network reinforcement learning port game frame used algorithm via personal communication scientist openai deepmind conﬁrmed accurately reporting ber frame game frame not training frame used dqn e mnih et al 2015 mnih et al 2016 salimans et al 2017 respectively one additional clariﬁcation paper mentioned ga paper input network atari current framet three previous frame three previous frame ing frame set meaning count game frame input network following game framet genetic algorithm competitive alternative training deep neural network reinforcement learning 0 2 4 6 200 400 600 800 1000 amidar 0 2 4 6 1000 2000 3000 4000 5000 assault 0 2 4 6 0 5000 10000 15000 20000 asterix 0 2 4 6 1000 1500 2000 2500 3000 3500 4000 4500 asteroid 0 2 4 6 0 200000 400000 600000 800000 1000000 1200000 atlantis 0 2 4 6 0 200 400 600 enduro 0 2 4 6 0 2000 4000 6000 8000 frostbite 0 2 4 6 200 400 600 800 1000 gravitar 0 2 4 6 0 2000 4000 6000 8000 10000 12000 14000 kangaroo 0 2 4 6 number game frame 0 1000 2000 3000 4000 5000 6000 seaquest 0 2 4 6 number game frame 14000 12000 10000 8000 6000 skiing 0 2 4 6 number game frame 0 250 500 750 1000 1250 1500 venture 0 2 4 6 number game frame 0 5000 10000 15000 20000 25000 zaxxon ga r dqn e figure ga random search performance across generation atari 2600 game performance ga random search compared dqn e depends game plot ﬁnal score dashed line dqn e not performance value across training trained different number game frame si table 1 ga r report median 95 bootstrapped conﬁdence interval median across 5 experiment current elite per run score elite mean 30 independent episode genetic algorithm competitive alternative training deep neural network reinforcement learning figure example individual frostbite found random search see main text tion behavior policy ﬁnal score episode far higher score produced dqn e although not high score found ga table 1 figure different algorithm explore deceptive age hard maze time traditional gorithms not exhibit sufﬁcient exploration avoid local optimum going trap 2 shown fig 1 contrast ga optimizing novelty only explores entire vironment ultimately ﬁnds goal evolutionary gorithms ga e blue cross represent population e red cross represent top ga spring orange dot represent ﬁnal position ga elite current mean e policy black cross entry archive 3 evolutionary algorithm number evaluation e ga many ping point revisit location due poor exploration giving illusion fewer evaluation dqn plot position agent episode prior checkpoint listed plot ing e signiﬁcantly underperforms ga p 8 10 run get stuck near trap 1 not deception instead seemingly not reliably learn pas small bottleneck corridor phenomenon ha never served gas hard maze suggesting e least hyperparameters qualitatively ferent gas regard lehman et 2017 believe difference occurs e optimizes average ward population sampled probability distribution even maximum ﬁtness agent sampled bution higher along corridor e not move direction average population lower policy sampled distribution crash wall experience fate companion paper investigate interesting difference e gas lehman et 2017 note however even e moved bottleneck 2 10 run solely got stuck trap 2