see discussion stats author profile publication designing neural network neuroevolution article nature machine intelligence january 2019 doi citation 619 read 4 author including kenneth stanley university central florida 217 publication citation see profile jeff clune university british columbia 153 publication citation see profile content following page wa uploaded jeff clune 18 march user ha requested enhancement downloaded file review article http ai lab san francisco ca usa central florida orlando fl usa wyoming laramie wy usa technology san francisco ca usa university texas austin austin tx usa kstanley jeffclune remarkable consider human brain product evolution natural process without intelligent oversight forethought although artificial neural network seen great progress recent year remain tant shadow great cognitive masterpiece natural evolution get called artificial general intelligence agi roughly mean artificial intelligence ai smart human current neural network research largely focused field deep learning deep reinforcement learning field dominant method training neural network efficient algorithm calculating loss function ent combined stochastic gradient descent sgd modify neural network weight greedily reduce loss method ha proven remarkably effective supervised learning ha also produced impressive reinforcement learning result alternative approach draw inspiration logical process produced human brain train neural network evolutionary algorithm field called roevolution although not work combining neural network evolutionary computation wa called neuroevolution cally use term broadly encompass effort enables important capability typically unavailable approach capability neural network include learning building block example activation tions hyperparameters example learning rate architecture example number neuron per layer many layer layer connect even rule learning neuroevolution ha fundamental ferences traditional approach example maintains population solution search grant ingly distinct benefit drawback finally lution research ha recently developed largely isolation neural network research range unique interesting powerful technique invented tion community provide exciting resource inspiration hybridization deep learning deep reinforcement ing machine learning community ha also surge interest lately hybridizing idea neuroevolution mainstream machine learning highlight emerging direction describe effort conclude ing future research direction advance harness idea neuroevolution including combination deep learning deep reinforcement learning could catalyse progress towards ambitious goal creating agi goal share broader machine intelligence munity work neuroevolution community u ha spent many year field review naturally reflects experience interest developed long period thus rather dispassionate historical account work area review exposition particular area concentrated think essential believe innovation continue many great researcher contributed field although hope captured key contribution many possible entire field result idea greater justice classic neuroevolution major inspiration investigation neuroevolution evolution brain nature notion cial neural network wa well established researcher began ask whether rough abstraction brain might evolved artificially evolutionary algorithm rithms also well established time borrow inspiration evolution nature breed fitter example designing neural network neuroevolution kenneth jeff joel risto much recent machine learning ha focused deep learning neural network weight trained variant stochastic gradient descent alternative approach come field neuroevolution harness ary algorithm optimize neural network inspired fact natural brain product tionary process neuroevolution enables important capability typically unavailable approach including learning neural network building block example activation function hyperparameters architecture even algorithm learning neuroevolution also differs deep learning deep reinforcement learning maintaining population solution search enabling extreme exploration massive parallelization finally neuroevolution research ha recently developed largely isolation neural network research ha developed many unique effective technique effective machine learning area review look several key aspect modern neuroevolution including computing benefit novelty diversity power indirect encoding field contribution architecture search hope inspire renewed interest field meet potential increasing computation available today highlight many idea provide exciting resource inspiration hybridization deep learning deep reinforcement learning machine learning community explain neuroevolution could prove critical tool pursuit artificial general intelligence nature machine intelligence vol 1 january 2019 24 review article nature machine intelligence candidate population many tion crossover population gradually evolves increasing level fitness researcher saw algorithm nity optimize neural network some extent researcher intrigued potential alternative tion also motivated nature achievement evolution remain unmatched artificial system although first researcher focused evolving only weight small network alternative tion interest quickly turned ambitious possibility evolving topology architecture neural even dynamic learning evolved rule updating weight alternative reinforcement learning early algorithm evolving network topology simply mutated weight stored connection focus turned quickly sophisticated encoding describing manipulating indirect encoding also became popular genome formula generating network rather direct description network new representation gained popularity graphical program cartesian genetic implicit encoding connectivity analogue genetic inspired genetic regulatory network scope attribute evolve broadened need algorithmic advancement support ambitious end example shift evolving fixed topology increasingly complex one created new challenge like crossing structure combining structure two parent network create parsimonious offspring network different topology tecting complex structure dying population allowing enough time weight optimized reveal true potential one approach gained considerable traction addressing challenge neuroevolution augmenting topology neat addressed problem crossing able topology historical marking tell crossover operator part two neural network similar thus swapped prevented premature extinction augmented tures mechanism called speciation solving problem made evolving increasingly complex topology effective early success field often concerned evolving neural network controller robot known evolutionary one prominent success wa produce first running gait sony aibo another wa evolving neural network morphology robot could move around real notable accomplishment outside lutionary robotics include helping discover neat accurate measurement yet mass top quark wa achieved tevatron particle neuroevolution also enabled some innovative video game concept ing new content real time game allowing player train character part game neuroevolution ha also used study open question evolutionary biology origin ity modularity hierarchy found biological network like neural network animal although impressive especially day ful application involved tiny neural network modern standard composed hundred thousand connection instead million connection commonly seen modern deep neural work dnn research natural question whether evolution task evolving large dnns address next new era neuroevolution scale intriguing historical pattern many classic machine ing algorithm perform qualitatively different far better scaled take advantage vast computing resource available example deep learning algorithm idea train neural network namely coupled optimization trick example important architectural motif example long memory unit lstms 31 known 2012 algorithm not perform well neural network however combined faster modern computer ing speedup provided graphic processing unit gpus paired large datasets algorithm produced great improvement performance generated recent excitement investment research recent year ha similarly shown neuroevolution algorithm also perform far better scaled take advantage modern computing resource described next scientist found neuroevolution competitive alternative based method training deep neural network reinforcement learning problem result important also foreshadow potential neuroevolution make impact across spectrum neural network optimization problem modern scale including case architecture search differentiation used conventional deep learning not clear solution reinforcement learning involves ai agent learning trial error environment without direct supervision instead try different action sequence receive infrequent reward action must learn sparse feedback future action maximize type learning lenging supervised learning correct output input given training main challenge ing mapping way generalizes reinforcement learning contrast requires exploring environment try discover optimal action take including figuring action lead rewarding event sometimes relevant action reward generate separated long time horizon known although rithms existed decade train reinforcement learning agent problem input ha recently surge progress interest deep ment learning involves dnns learn sense act state space example raw visual stream involve thousand pixel value per frame video result particularly large impact deep forcement learning algorithm learn play many different atari video learn make simulated robot surprise many salimans et showed recent form evolution strategy classic evolutionary algorithm 39 called natural evolutionary strategy ne 40 performs competitively best deep reinforcement learning algorithm including deep dqn 3 policy gradient method example ne ref 38 directly evolves weight dnn size used dqn atari work four million weight parameter must learned surprise wa evolutionary algorithm could compete method parameter space however ne interpreted based method estimate gradient parameter space take step direction many not conclude work pure evolutionary algorithm operate dnn scale changed result simple genetic algorithm wa also competitive dqn evolution strategy atari game outperformed many moreover subset game genetic algorithm even performed later powerful version genetic algorithm entirely contains nature machine intelligence vol 1 january 2019 25 review article nature machine intelligence population agent dnn parameter vector independently mutated reproduce mance better relative others population salimans et et additionally showed tion algorithm run quickly original dqn algorithm parallelizable one ha sufficient computing some case compared some algorithm example dqn not ary algorithm le sample efficient extremely parallelizable run far faster real wall clock time example hour instead day albeit cost ing computing ref 38 ref 42 also show directly evolving weight neural network solve continuous control problem robot learning walk specifically salimans et showed ne performs well enabling humanoid robot controlled neural network walk faster term wall clock time competing reinforcement learning rithms owing evolution allowing better parallellization et showed ga also perform well task although worse sample complexity mania et showed fied neuroevolution variant evolution strategy training layer neural network learning linear mapping state action produce result simulated robot control task outperforming complex modern version policy gradient method example trust region policy proximal policy deep deterministic policy neuroevolution performs well controlling robot not surprising given long history success field evolutionary interest also growing way hybridize method deep learning neuroevolution lehman et recently introduced safe mutation output gradient based insight evaluating policy often expensive ning entire episode physic simulator video game see performs task example evaluating output neural network often inexpensive one need only conduct forward pass network saved reference situation neuroevolution random mutation made policy mapping input action represented dnn some mutation may no effect behaviour policy network others might major thus usually strophic consequence policy example always outputting action insight behind safe mutation keep reference library state action incurring only slight cost forward backward pas use gradient information scale magnitude mutation make change policy reference set neither large small lehman et show approach improves mance evolutionary algorithm including enabling ful evolution weight network hundred layer unprecedented another hybridization ha posed run variant reinforcement learning engine behind crossover mutation operator within lution still another direction ha shown perform well combine style evolutionary algorithm search directly space neural network parameter style policy gradient algorithm search space action change neural network parameter via backpropagation make profitable action likely ing random parameter perturbation drive consistent tion like evolutionary algorithm reinforcing successful action weight parameter via exciting success described far achieved simple neuroevolution algorithm however neuroevolution community ha invented many sophisticated technique greatly improve performance ple algorithm many based observation evolution natural computational instantiation exciting engine modern technique attempt recreate creativity algorithmically search better ral network discus work ha already begun port many idea including encourage sity novelty intrinsic enhancement improving performance important idea covered article include indirect encoding method encoding large evolution network trained gradient descent continuing test best idea neuroevolution munity scale deep neural network modern amount computing power data likely yield considerable tional advance moreover combining idea deep learning deep reinforcement learning research area continue deliver many breakthrough next section describes consider exciting idea neuroevolution community hope aging researcher experiment dnn scale blend idea traditional machine learning novelty behavioural diversity hallmark natural evolution amazing diversity plex functional organism ha intricate machinery life massive collaborative union cell form animal sort including human addition interesting right massive parallel tion way life wa probably critical evolution human intelligence diversity make innovation thus similar drive towards diversity important ing neuroevolution possible route ai reason neuroevolution evolutionary computation whole long focused indeed adapting population solution evolutionary algorithm naturally suited parallel exploration diverse solution initial work diversity neuroevolution focused encouraging diversity genetic space space goal circumventing local optimum idea search ha converged local optimum aging exploration away optimum may enough uncover new promising gradient improvement representative approach include new individual replaces one genetically similar explicit individual clustered genetic distance ished many member cluster although sometimes effective diversity often fails produce wide diversity different infinite way set neural network weight instantiate behaviour owing caling permuting redundant mapping example many different weight setting cause robot fall immediately word trivial generate diverse similarly behaving parameter vector escaping local optimum often requires exploration diverse biological evolution doe important human problem result limitation genetic diversity recent approach directly reward diversity research ha led related idea directly evolving desired quality generating representative involves evolutionary reward individual increasing ness diverging individual ified characterization behaviour domain way nature machine intelligence vol 1 january 2019 26 review article nature machine intelligence search organically push different individual population towards different exploring relevant ioural dimension optimizing performance one helpful step developing new algorithm explore breadth potential diversification technique break box wherein evolution viewed mainly optimizer biological evolution unlike optimization sense doe not strive towards any particular organism indeed one fundamental mechanism accumulation diverse novelty bringing question whether optimizing single optimal individual capture enables evolution discover rich complex behaviour alternate point view recognizes diversity mier product popular neuroevolution algorithm adopts perspective called novelty search 63 reward neural network only behaving way novel relative viduals produced earlier search word search rithm includes no pressure towards greater improvement according traditional fitness performance measure idea whole population spread generation evolution span wide range behaviour although gradient gence genetic space uninformative many different genome produce uninteresting behaviour gradient behavioural novelty often contains useful domain information word something new often requires learning skill respect constraint domain ple learning perform new skateboard trick requires ance coordination might gained riding around indeed some reinforcement learning domain searching only behavioural novelty outperforms although first applied small network novelty search ha recently demonstrated scale reinforcement learning problem improves providing another example idea neuroevolution community benefit modern amount computation building result novelty search general perspective evolution new expanding area neuroevolution research explores quality diversity quality diversity algorithm designed illuminate diversity possible solution evolution ha uncovered organism across le environmental niche example kind search quality objective creature speed discovering fast tah would not preclude discovering fast ant may locomote quickly relative creature similar morphology example early quality diversity algorithm include novelty search local competition archive phenotypic elite provide ent way integrate pressure perform well within sifying search nslc modifies evolutionary algorithm enabling optimizing population diverse locally optimal individual individual relative similar strategy simple ful algorithm subdivides space possible behaviour discrete niche containing single champion agent type found far competition enforced only locally mutation parent one niche produce new champion another niche enabling like effect becoming one niche may stepping stone success another product rithms often called repertoire collection diverse yet effective option rather single optimal solution result published nature wa applied discover diverse repertoire walking gait damaged robot could quickly recover searching best champion gait worked despite space quality diversity algorithm continues exciting area current research although much progress ha made roevolution algorithm remains considerable qualitative gap complexity nature discovers current product evolutionary algorithm gap hint breakthrough area yet made indirect encoding 100 trillion connection 100 billion human brain far exceeds size any modern neural network situated within expanse intricate architecture module pattern connectivity underpin human intelligence fascinating question astronomical structure lated within genetic code whose capacity only gene 3 billion base pair learning course critical part story still tremendous amount information encoded genome regarding overall tecture many neuron modular component module wired module rule govern learning occurs also part tion need encode component requires regularity reuse structural motif compression enables genome reasonably compact interestingly regularity provides powerful computational tages neural structure well example power lar structure familiar anyone experience deep learning success convolution particular regular pattern connectivity wherein feature detector situated many location layer convolution wa designed hand heuristic solution problem capturing feature different level simple regularity ha proven powerful become nearly tous across successful modern architecture deep however neuroevolution raise prospect fication powerful regularity need not fall ultimately hand human designer prospect connects naturally also potential compressed encoding describe vast tectures composed extensive regularity beyond convolution example larger palette regularity could include various symmetry bilateral radial instance well gradient along filter vary according regular principle becoming smaller towards periphery ultimately would ideal machine learning could discover pattern including convolution without requiring limited cleverness designer capability neuroscientist indirect encoding neuroevolution address prospect investigating potential artificial genetic encoding compactly capture regularity symmetry ture motivated compression dna nature research indirect encoding stretch back decade pattern formation later researcher explored evolvable encoding wide range structure blob artificial cell robot morphology neural including influential work bongard hornby popular modern indirect encoding neuroevolution positional network cppns function similarly neural network inspiration come instead developmental biology structure situated built within geometric space example early development embryo chemical gradient help define ax head tail front back left way structure arm leg situated correct position furthermore within structure substructure finger hand must placed within local coordinate nature machine intelligence vol 1 january 2019 27 review article nature machine intelligence system hand configuration happens biological system cell producing reacting diffusing cals called morphogens would extremely computationally expensive simulate cppns abstract process simple network function composition represented graph input layer primary ax example x ture input network serving base coordinate tem small set activation function abstract common structural motif within developing embryo composed yield complex pattern example gaussian function elicits equivalent symmetric chemical gradient sigmoid generates asymmetric one sine wave recall tion function composed within weighted network like special kind neural network fig b yield surprisingly complex structure node connection modern standard example cppn fig produce skull image picbreeder fig ref 91 traditionally cppns evolved algorithm allows architecture increasing complexity evolve starting simple initial form important note composing simple function doe not yield simple pattern example sine wave encodes strict repetition sine composed square variable sin yield repetition variation powerful ubiquitous cept seen across biology thus network function quickly becomes complex part explaining composition chemical gradient hierarchy real organism yield structure including brain complexity cppns used wide range application benefit tendency towards regular structure erating creating including form soft also method behind recent widely discussed result showing some image including generated cppns easily fool otherwise highly accurate ease implementation rich resultant tural space furthermore inspired research age creative innovation source canalization tendency indirect encoding nature yield robust easily adaptable developmental pathway even inspired fix some limitation tion dnns based idea providing coordinate space input beyond application perhaps important role cppns generate pattern weight neural network approach called hyperneat main idea generate pattern weight function geometry input output domain example input visual field weight projecting field generated function position source input neuron within field short source field target field weight connection two neuron expressed function f encoded cppn position source target neuron respectively either end weight w f x x x x 1 1 2 2 1 1 2 2 fig see formalism enable regular connectivity pattern consider simple symmetric function example input f cause weight projecting source neuron pattern weight target ric interestingly adding input inputting only induce generalized convolutional nectivity pattern many way extend encoding different neural architecture example multiple layer respective connectivity pattern generated separate cppn output encoding connectivity function geometry effect mean conveying critical problem structure tion algorithm example correlation weight nearby neuron only learned general pattern position neuron known perhaps ubiquitous heuristic modern deep neural work repeated pattern across geometry neural structure fact recent variant evolution combine neuroevolution indirect encoding sgd indeed discovered convolutional pattern without explicitly inputting convolutional coordinate frame effect reinventing ha et recently also introduced hypernetworks adapt hyperneat indirect encoding train weight entirely sgd ref 100 hypernetworks help enhance performance lstms language modelling task providing example idea tion fruitfully hybridized idea traditional machine learning community make hyperneat interesting convolution not only conceivable regular pattern connectivity could important neural network principle discover any pattern thereby exploit different kind regularity not accessible conventional neural network learning algorithm example position sensor motor neuron within quadruped body exploited efficiently evolve lar gait require regular connectivity pattern lated convolution another example hyperneat create repeated architectural motif repeated modular another great benefit hyperneat indirect encoding general enable large neural network evolved compact encoding connects back tion dna example hyperneat generate functional neural network million connection encoded cppns only dozen potential extreme compression inspired researcher subsequently oped indirect encoding generate pattern network geometry weight indirect encoding steenkiste et koutnik et among first system approach reinforcement learning directly pixel action interestingly although work strong impact field reinforcement learning learning play atari directly pixel hyperneat wa first system direct atari result research also continues indirect encoding neural network cartesian genetic today increasingly powerful computation present great tunities indirect encoding not only increasingly large ral network held memory decoding step wherein cppn generates weight neural network analogue embryogeny nature also accelerated parallelized newfound computational capacity also open sophisticated us indirect encoding nudge neuroevolution even closer natural evolution using cppn generate pattern coefficient plasticity instead static weight adaptive hyperneat 105 also variant hyperneat encode thus learn neural network architecture weight arbitrary resolution evolving neural network learn lifetime open broad new opportunity powerful discussed next moving field closer kind automatic design entire learning system could fundamentally alter landscape deep learning architecture search learning learn currently riencing surge interest within deep ha long nature machine intelligence vol 1 january 2019 28 review article nature machine intelligence attracted attention researcher neuroevolution natural evolution intrinsically powerful rithm evolution viewed outer loop search algorithm produced organism including human extraordinarily sophisticated learning capability seen running inner loop search thus natural researcher inspired evolution nature explored potential similar nested loop within cial system perhaps way structure powerful learning capability could arise much work within neuroevolution ha focused synaptic plasticity key mechanism behind neural learning early explored potential evolve local learning rule dictate weight connection change response tion level source target node famous rule hebbian roughly implement synaptic strengthening source target neuron fire together although hebbian rule expressed variety form simple expression ηxixj describing change cppn bias weight su bstrate cppn bias intensity weight cppn input output cppn skull pattern hyperneat cppn generates nn weight example hyperneat network view front left back right neural network x c e b fig 1 compositional network hyperneat cppns network heterogeneous activation function produce geometric pattern network shown cppn take x also take input distance x image centre queried many point output interpreted image pattern space b c cppn shown b actual architecture produce skull pattern shown c wa interactively evolved picbreeder online colour b distinguish component evolved network responsible key part image c based analysis function small image within node activation pattern computed specific node x ultimately combined network produce final image hyperneat cppns leverage capability output weight blue arrow every source target node location green arrow neural network e example hyperneat neural network evolved make quadruped robot run viewed either input neuron orange sphere output neuron blue sphere front pink sphere hidden neuron green red connection excitatory inhibitory thickness indicates connection strength note complex regular geometric weight pattern example outgoing connection input inhibitory upper hidden node excitatory towards lower hidden node regularity variation diffusion inhibitory connection output node varies across x ax nature machine intelligence vol 1 january 2019 29 review article nature machine intelligence weight neuron neuron j function activation xi xj sufficient show room evolution decide dynamic network coefficient η connection evolved degree plasticity connection optimized various local learning rule possible effect any able function determine dynamic synapse fact even possible indirect encoding generate pattern learning rule function geometry network ity mentioned adaptive furthermore plasticity connection made vary lifetime agent mechanism lation special neuromodulatory signal increase decrease plasticity individual connection capability erful assist kind learning similar reinforcement learning example locking weight turning plasticity yield high reward ing plasticity expected reward doe not materialize mechanism unfold concert recurrence inside work providing evolution rich playground designing complex plastic system learn soltoggio et pioneered evolving neuromodulatory plasticity others followed including showing benefit combining plasticity indirect see ref 10 comprehensive review geoning research area achievement far one exciting opportunity neuromodulation enables mitigation catastrophic forgetting grand challenge machine learning must solved create agi natural animal able learn variety different task across lifetime remember perform already learned task even not done long sharp contrast artificial neural network learn new skill erasing learned previous skill meaning forget neuromodulation offer tantalizing prospect turning plasticity only subset neural weight relevant task currently performed meaning knowledge stored weight different task left untouched ellefsen et showed combining neuromodulation technique promote functional modularity neural meaning different task performed different module within neural network mitigates catastrophic forgetting however effect wa limited neuromodulation ref 120 wa encoded separately connection making lenging optimization problem evolution learn shut connection module task performed velez et introduced idea neuromodulation enables evolution instead increase decrease plasticity different geometric region neural work addition ability easily upregulate late plasticity entire geometric region enabled evolution create neural network entirely eliminate catastrophic forgetting albeit simple neural network solving simple problem testing technique scale dnns exciting area future work yet another example neuroevolution could benefit modern computation additionally although promising work ha appeared recently deep learning combat catastrophic neuromodulation entirely different approach problem divorced neuroevolution coupled instead sgd providing another potential example porting idea neuroevolution community deep learning even indirect encoding much work far ing learning mechanism synaptic plasticity ha focused relatively small neural network solving relatively simple lem using plastic connection remember food within simple recent result showing roevolution algorithm optimizing million hint much larger plastic system could possible evolve full capability complex dynamic system yet explored setting exciting opportunity aside plasticity another aspect learning learn discovering right architecture learning much work neuroevolution ha also focused challenge architecture search network tectures become complex difficult design complexity matter neat represents early advance evolving network architecture along weight small scale recent work ha focused evolving deep neural evolve network architecture optimize weight gradient descent effect optimizing network purpose gradient descent many method inspired neat ability increase complexity generation adding layer instead individual neuron mization ha already led architecture improve state art several deep learning benchmark including vision language multitask learning many greatest achievement deep learning visual image processing task classification segmentation object detection neural network come dominate computer vision led substantial performance improvement different often innovation within computer vision domain come discovery ent better architecture type computer vision problem tends require specialized result ha proliferation neural network architecture image cessing architecture complex varied enough raise question whether better architecture could discovered automatically recent result showing particularly successful took inspiration neat evolved dnns starting small adding complexity mutation added entire layer neuron achieving impressive performance popular cifar image classification dataset variant improved performance evolving small neural network module repeatedly used larger blueprint wa inspired idea repeatedly stacking layer module make dnn idea ha proved successful architecture efficiency reason module evolved computationally cheaper task solved smaller blueprint ferred no evolution build larger network solve target computationally expensive task image cation cifar imagenet datasets despite massive effort army researcher year architecture famous computer vision benchmark method evolved architecture produced state art time tion cifar ha since slightly currently hold state art language processing another area deep learning ha large impact architectural need however differ vision whereas convolutional filter key architectural motif vision language common motif gated recurrent network node network complex neural network instead weighted sum followed nonlinearity typically includes memory cell connection write read forget gate structure make easier retain activation value indefinitely improving gradient flow network make possible perform well various sequence processing task even though idea nated wa not computation wa available scale ten million parameter train length large datasets started work well enough make difference application speech tion language understanding language nature machine intelligence vol 1 january 2019 30 review article nature machine intelligence interestingly structure lstm node ha remained tively constant since inception 25 year ago variation proposed not significantly improve standard past year however ha become possible search automatically better gated recurrent network design using example reinforcement learning fruitful approach encode structure gated rent node program evolved genetic gene describe network form given search space multiple memory cell reusable input different activation function ability struct multiple linear nonlinear path node tion came complex structure several time larger standard lstm node put together layered network trained example language ling task machine learning benchmark predicting next word large corpus even total number adjustable parameter 20 million evolved node perform 15 better standard lstm remarkably node design used another task music prediction doe not improve standard node structure evolved music prediction mance improves 12 56 word neuroevolution er complexity customized improves performance task another promising application architecture evolution multitask learning ha long known training neural network multiple task make learning task requirement data task help shape internal representation general generalize ter new input generally recent work ha shown retically practice combining gradient task improves requirement different task best combined architecture multitask learning network make large difference therefore good opportunity architecture search straightforward approach would evolve single shared architecture separate decoder put layer task alternate promising method cmtr coevolution module task based three principle 1 evolve custom network topology task 2 construct topology module whose training shared across task 3 evolve structure module method allows evolution discover general module specific topology omniglot character 50 ent alphabet example per recently emerged standard benchmark multitask learning cmtr approach improved state art 32 benchmark demonstrating benefit multitask training average formance alphabet wa 14 better training network similar size alphabet alone module ferent topology discovered used fig 2 topology varied lot alphabet remarkably topology sistent across multiple run similar topology discovered visually similar alphabet result suggest approach discovers useful common representation specialized way using task input 1 2 2 3 4 4 output input 1 2 1 3 4 output input 1 2 1 3 4 3 2 4 3 2 4 2 3 2 1 3 3 2 2 output 4 2 4 topology gurmukhi mujarati topology angelic invented alphabet b fig 2 sample evolved topology module omniglot multitask learning benchmark neuroevolution discovers set common building block module not shown identified number differentiation task making possible perform better task even limited training b topology task combine module represent range simple complex b similar alphabet gurmukhi left mujarati right similar topology structure consistently found different independent evolutionary run related neuroevolution technique similarly improve state art various vision language machine learning benchmark useful architectural specialization would difficult expensive discover hand demonstrating power evolution designing complex system nature machine intelligence vol 1 january 2019 31 review article nature machine intelligence several alternate approach architecture zation neural network besides evolution including optimizing some part network design using gradient descent ment learning bayesian parameter however evolution promising method exploring neural network structure broadly doe not require explicit dients additionally straightforwardly augmented novelty search quality diversity indirect encoding idea discussed could improve result discussed accomplished simple ary algorithm prospect combining ability evolve modulated plasticity network architecture yield unique new research opportunity begin resemble evolution brain kind technology produce scale combined gradient descent remains largely unexplored looking forward although neuroevolution ha long existed independent research area trend emerging wherein line roevolution deep learning concern neural work gradually blur idea neuroevolution infuse deep learning hybridization evolution gradient descent conceptual insight transferring one paradigm trend likely accelerate especially computational resource expand opening evolutionary ments focused small model discovery evolution neural network proved potent combination nature doe neuroevolution offer intriguing path pitulating some fruit evolving brain trend towards hybridization neuroevolution method encompasses many idea learning architecture search gradient descent also manifest unique original one example hyperparameters based learning evolved online population different another example evolutionary algorithm evolves way online trained gradient descent enabling automatic discovery shared feature transfer learning evolved policy evolves loss function policy gradient rithm enabling discovery internal reward signal erate learning new task evolution endows u innate desire enjoying eating intrinsic motivation curiosity improve ability learn survive method instantiate evolution tion recombination operator policy gan gradient descent also boost performance evolution safe mutation tor introduced ref 48 avoid overly disruptive mutation estimating sensitivity individual weight perturbation gradient computation side convergence neuroevolution deep learning importation conceptual approach neuroevolution without using any evolution example plastic neural network neural network encode online learning rule weight change without gradient descent response experience long studied within artificial life only recently similar plastic work reformulated learned gradient descent also enables impact explored much larger scale mentioned similar lation ha occurred drive trend probably evolution open playground testing proving idea imposes limitation inable model example doe not require model differentiable ultimately possible translate idea gradient descent effort may result greater ity efficiency neuroevolutionary idea also inspired progress area deep reinforcement learning example diversity need mirror similar insight novelty case older idea evolutionary tion reinvented independently deep learning offering opportunity diversity deeper standing example recent exciting resemble investigation subfield believe previous evolutionary work continue source synergistic inspiration future deep learning research additionally idea developed neuroevolution some case already ported improve subfields optimize sort network genetic regulatory network final critical opportunity neuroevolution lead effort construct algorithm main challenge create algorithm produce interesting increasingly complex discovery indefinitely inspiration evolution nature life earth wa discovered single run ha continued produce new form billion year learn program rithms similarly perpetually interesting could capture profoundly powerful aspect creativity nature direct purpose straightforward application endedness generate new artefact building clothing architecture without bound intriguing possibility complexity agi only possible discover process generates plex structure indefinitely furthermore ness may require only neural network body evolve together nature morphology evolve along neural network artificial ing form long run could fuel generating architecture learning rithms ultimately reach intelligence race neuroevolution research develop mechanism innovate without boundary consider grand challenge scientific many idea discussed herein invented time available computation made difficult try anything tiny neural network however computer vision year ago computation becoming available see original idea modern improvement finally take achieve grand ambition long evolution natural evolution gence expect neuroevolution play important role collective quest create ai algorithm endlessly innovate received 6 september 2018 accepted 12 november 2018 published online 7 january 2019 reference goodfellow bengio courville deep learning mit press cambridge 2016 lecun bengio hinton deep learning nature 521 2015 mnih et al control deep reinforcement learning nature 518 2015 sutton barto reinforcement learning introduction mit press cambridge 2018 rumelhart hinton williams learning representation error nature 323 1986 de jong evolutionary computation unified perspective mit press cambridge 2002 nature machine intelligence vol 1 january 2019 32 review article nature machine intelligence gruau automatic definition modular neural network adapt behav 3 1994 yao review evolutionary artificial neural network int intell syst 8 1993 floreano dürr mattiussi neuroevolution architecture learning evol intell 1 2008 soltoggio stanley risi born learn inspiration progress future evolved plastic artificial neural network neural netw 108 2018 dasgupta mcgregor designing neural network using structured genetic algorithm proc international workshop combination genetic algorithm neural network ieee 1992 pujol poli evolving topology weight neural network using dual representation appl intell j 8 1998 bongard pfeifer machine new specie ed hara pfeifer springer tokyo 2003 gruau genetic synthesis modular neural network proc international conference genetic algorithm ed forrest morgan kaufmann san francisco 1993 khan ahmad khan miller fast learning neural network using cartesian genetic programming neurocomputing 121 2013 turner j miller neuroevolution evolving heterogeneous artificial neural network evol intell 7 2014 mattiussi floreano analog genetic encoding evolution circuit network ieee trans evol comput 11 2006 stanley miikkulainen evolving neural network augmenting topology evol comput 10 2002 moriarty miikkulainen evolving obstacle avoidance behavior robot arm animal animats 4 proc international conference simulation adaptive behavior ed maes et al mit press cambridge 1996 nolfi floreano evolutionary robotics mit press cambridge 2000 hornby g et al evolving robust gait aibo proc ieee conference robotics automation ieee 2000 lipson pollack automatic design manufacture robotic lifeforms nature 406 2000 aaltonen et al measurement top quark mass dilepton event selected using neuroevolution cdf phys rev lett 102 2001 2009 togelius yannakakis stanley browne procedural content generation taxonomy survey ieee trans comput intell ai game 3 2011 stanley bryant miikkulainen neuroevolution nero video game ieee trans evol comput 9 2005 clune mouret lipson evolutionary origin modularity proc soc b 280 20122863 2013 huizinga mouret clune evolving neural network modular regular hyperneat plus connection cost technique proc genetic evolutionary computation conference gecco acm 2014 polyak some method speeding convergence iteration method ussr comput math math phys 4 1964 qian momentum term gradient descent learning algorithm neural netw 12 1999 lecun bottou bengio haffner learning applied document recognition proc ieee 86 1998 hochreiter schmidhuber long memory neural comput 9 1997 dahl yu deng acero deep neural network large vocabulary speech recognition ieee trans audio speech lang process 20 2012 hinton et al deep neural network acoustic modeling speech recognition shared view four research group ieee signal process mag 29 2012 krizhevsky sutskever hinton imagenet classification deep convolutional neural network advance neural information processing system 25 nip 2012 ed pereira et al nip 2012 lillicrap et al continuous control deep reinforcement learning preprint 2016 schulman levine abbeel jordan moritz trust region policy optimization mach learn 37 2015 schulman wolski dhariwal radford klimov proximal policy optimization algorithm preprint 2017 salimans ho chen x sutskever evolution strategy scalable alternative reinforcement learning preprint 2017 rechenberg simulationsmethoden der medizin und biologie springer hannover 1978 wierstra et al natural evolution strategy mach learn 15 2014 mnih et al asynchronous method deep reinforcement learning international conference machine learning pmlr 2016 et al deep neuroevolution genetic algorithm competitive alternative training deep neural network reinforcement learning preprint 2017 hessel et al rainbow combining improvement deep reinforcement learning proc 2018 aaai conference artificial intelligence aaai 2017 horgan et al distributed prioritized experience replay proc 2018 international conference learning representation openreview 2018 mania guy recht simple random search provides competitive approach reinforcement learning preprint 2018 clune stanley pennock ofria performance indirect encoding across continuum regularity ieee trans evol comput 15 2011 cully clune tarapore mouret robot adapt like animal nature 521 2015 lehman chen clune j stanley safe mutation deep recurrent neural network output gradient proc genetic evolutionary computation conference gecco acm 2018 gangwani peng genetic policy optimization proc 2018 international conference learning representation openreview 2018 fortunato et al noisy network exploration proc 2018 international conference learning representation openreview 2018 plappert et al parameter space noise exploration proc 2018 international conference learning representation openreview 2018 lehman et al surprising creativity digital evolution collection anecdote evolutionary computation artificial life research community preprint 2018 conti et al improving exploration evolutionary strategy deep reinforcement learning via population agent advance neural information processing system nip curran associate red hook 2018 stanton clune deep curiosity search exploration improves performance challenging deep reinforcement learning problem preprint 2018 stanley miikkulainen taxonomy artificial embryogeny artif life 9 2003 rawal miikkulainen node network evolving recurrent neural network preprint 2018 real aggarwal huang le regularized evolution image classifier architecture search preprint 2018 dawkins extended phenotype gene unit selection freeman oxford 1982 gould full house harvard univ press cambridge 2011 goldberg richardson genetic algorithm sharing multimodal function optimization proc international conference genetic algorithm erlbaum hillsdale 1987 mahfoud niching method genetic algorithm phd thesis univ illinois 1995 jong de analysis behavior class genetic adaptive system phd thesis univ michigan 1975 lehman j stanley abandoning objective evolution search novelty alone evol comput 19 2011 neyshabur salakhutdinov srebro normalized optimization deep neural network advance neural information processing system 28 nip 2015 mit press cambridge 2015 radcliffe genetic set recombination application neural network topology optimisation neural comput appl 1 1993 holekamp innovative problem solving wild spotted hyena proc soc b 279 2012 kanter change master binnovation entrepreneturship american corporation simon schuster new york 1984 mouret doncieux encouraging behavioral diversity evolutionary robotics empirical study evol comput 20 2012 mengistu lehman j clune evolvability search directly selecting evolvability order study produce proc genetic evolutionary computation conference gecco acm 2016 gravina liapis yannakakis surprise search beyond objective novelty proc genetic evolutionary computation conference gecco acm 2016 nature machine intelligence vol 1 january 2019 33 review article nature machine intelligence deb pratap agarwal meyarivan fast elitist multiobjective genetic algorithm ieee trans evolut comput 6 2002 zitzler thiele multiobjective evolutionary algorithm comparative case study strength pareto approach ieee trans evolut comput 3 1999 pugh soros b stanley quality diversity new frontier evolutionary computation front robot ai 3 40 2016 lehman j stanley evolving diversity virtual creature novelty search local competition proc annual conference genetic evolutionary computation gecco acm 2011 mouret clune illuminating search space mapping elite preprint 2015 brant stanley minimal criterion coevolution new approach search proc genetic evolutionary computation conference gecco acm 2017 hodjat shahrzad miikkulainen distributed novelty search proc international conference synthesis simulation living system alife xv mit press cambridge 2016 huizinga mouret clune doe aligning phenotypic genotypic modularity improve evolution neural network proc genetic evolutionary computation conference gecco acm 2016 meyerson miikkulainen discovering evolutionary stepping stone behavior domination proc genetic evolutionary computation conference gecco acm 2017 nguyen yosinski j clune understanding innovation engine automated creativity improved stochastic optimization via deep learning evol comput 24 2016 human brain number linearly primate brain front hum neurosci 3 31 2009 venter et al sequence human genome science 291 2001 striedter principle brain evolution sinauer associate sunderland 2005 russakovsky et al imagenet large scale visual recognition challenge int comput vision 115 2015 turing chemical basis morphogenesis phil trans soc b 237 1952 lindenmayer mathematical model cellular interaction development filament input theor biol 18 1968 bongard pfeifer r repeated structure dissociation genotypic phenotypic complexity artificial ontogeny proc genetic evolutionary computation conference gecco kaufmann 2001 hornby pollack creating component generative representation evolution artif life 8 2002 stanley compositional pattern producing network novel abstraction development genet program evol mach spec issue dev syst 8 2007 meinhardt model biological pattern formation academic london 1982 secretan et al picbreeder case study collaborative evolutionary exploration design space evol comput 19 2011 clune j lipson evolving object generative encoding inspired developmental biology proc european conference artificial life mit press cambridge 2011 cheney n maccurdy r clune j lipson unshackling evolution evolving soft robot multiple material powerful generative encoding proc genetic evolutionary computation conference gecco acm 2013 nguyen yosinski j clune deep neural network easily fooled high confidence prediction unrecognizable image ieee conference computer vision pattern recognition cvpr ieee 2015 huizinga stanley clune emergence canalization evolvability interactive evolutionary system artif life 24 2018 liu et al intriguing failing convolutional neural network coordconv solution proc 2018 conference neural information processing system nip curran associate red hook 2018 gauci j stanley autonomous evolution topographic regularity artificial neural network neural comput 22 2010 stanley ambrosio b gauci indirect encoding evolving neural network artif life 15 2009 fernando et al convolution evolution differentiable pattern producing network proc genetic evolutionary computation conference gecco acm 2016 ha dai le hypernetworks proc 2017 international conference learning representation vol 2 openreview 2017 van steenkiste koutník driessens schmidhuber encoding neuroevolution proc genetic evolutionary computation conference gecco acm 2016 koutnik gomez schmidhuber evolving neural network compressed weight space proc genetic evolutionary computation conference gecco acm 2010 hausknecht lehman miikkulainen stone neuroevolution approach general atari game playing ieee trans comput intell ai game 6 2014 turner j miller recurrent cartesian genetic programming artificial neural network genet program evol mach 18 2017 risi stanley indirectly encoding neural plasticity pattern local rule proc international conference simulation adaptive behavior springer new york 2010 risi stanley enhanced encoding evolving placement densty connectivity neuron artif life j 18 2012 schmidhuber evolutionary principle learning learning learn hook phd thesis technische univ münchen 1987 duan et al fast reinforcement learning via slow reinforcement learning preprint 2016 finn c abbeel levine fast adaptation deep network proc international conference machine learning pmlr 2017 miconi learning learn backpropagation hebbian plasticity preprint 2016 wang et al learning reinforcement learn preprint 2016 floreano urzelai evolutionary robot behavioral fitness neural netw 13 2000 floreano mondada evolution plastic neurocontrollers situated agent ieee trans syst man cybern 26 1996 hebb organization behavior neuropsychological theory wiley hoboken 1949 soltoggio bullinaria mattiussi dürr floreano evolutionary advantage neuromodulated plasticity dynamic scenario proc international conference artificial life alife xi ed bullock et al mit press cambridge 2008 risi stanley unified approach evolving plasticity neural geometry proc international joint conference neural network ieee 2012 tonelli mouret relationship generative encoding regularity learning ability evolving plastic artificial neural network plo one 8 2013 barnes underwood j fate association transfer theory exp psychol 58 97 1959 french catastrophic forgetting connectionist network trend cogn sci 3 1999 ellefsen mouret clune j bongard neural modularity help organism evolve learn new skill without forgetting old skill plo comput biol 11 2015 velez clune neuromodulation eliminate catastrophic forgetting simple neural network plo one 12 2017 kirkpatrick et al overcoming catastrophic forgetting neural network proc natl acad sci usa 114 2017 zenke f poole b ganguli continual learning synaptic intelligence proc international conference machine learning pmlr 2017 miikkulainen et al evolving deep neural network preprint 2017 zhang ren sun identity mapping deep residual network european conference computer vision springer berlin 2016 huang liu van der maaten weinberger densely connected convolutional network proc ieee conference computer vision pattern recognition cvpr ieee 2017 szegedy ioffe vanhoucke impact residual connection learning proc 2017 aaai conference artificial intelligence aaai 2017 real et al evolution image classifier proc international conference machine learning ed precup teh plmr 2017 nature machine intelligence vol 1 january 2019 34 review article nature machine intelligence zoph vasudevan shlens j le learning transferable architecture scalable image recognition proc ieee conference computer vision pattern recognition ieee 2018 zhang ren sun deep residual learning image recognition proc ieee conference computer vision pattern recognition ieee 2016 cubuk zoph mane vasudevan le autoaugment learning augmentation policy data preprint 2018 schmidhuber deep learning neural network overview neural netw 61 2015 greff srivastava koutník steunebrink schmidhuber lstm search space odyssey ieee trans neural netw learn syst 28 2017 melis dyer blunsom state art evaluation neural language model proc 2018 international conference learning representation openreview 2018 zoph b le neural architecture search reinforcement learning proc 2017 international conference learning representation openreview 2017 marcus santorini b marcinkiewicz building large annotated corpus english penn treebank comput linguist 19 1993 caruana multitask learning mach learn 28 1997 meyerson miikkulainen augmentation deep multitask learning intratask back proc international conference machine learning pmlr 2018 liang meyerson miikkulainen evolutionary architecture search deep multitask network proc genetic evolutionary computation conference gecco acm 2018 elsken metzen hutter neural architecture search survey preprint 2017 fernando et al pathnet evolution channel gradient descent super neural network preprint 2017 houthooft et al evolved policy gradient preprint 2018 wang xu yao x tao evolutionary generative adversarial network preprint 2018 jaderberg et al population based training neural network preprint 2017 jaderberg et al performance multiplayer game deep reinforcement learning preprint 2018 eysenbach gupta ibarz j levine diversity need learning skill without reward function preprint 2018 miconi clune j stanley differentiable plasticity training plastic neural network backpropagation proc international conference machine learning pmlr 2018 mordvintsev pezzotti schubert olah differentiable image parameterizations distill 3 2018 bansal pachocki sidor sutskever mordatch emergent complexity via competition proc 2018 international conference learning representation openreview 2018 silver et al mastering game go without human knowledge nature 550 2017 paredis coevolutionary computation artif life 2 1995 pollack blair land coevolution backgammon player proc international workshop artificial life synthesis simulation living system ed langton shimohara mit press cambridge 1996 potter de jong evolving neural network collaborative specie proc 1995 summer computer simulation conference society computer simulation 1995 rosin belew method competitive finding opponent worth beating proc 1995 international conference genetic algorithm morgan kaufmann burlington 1995 harrington pollack gene regulatory network evolution augmenting topology ieee trans evolut comput 19 2015 auerbach bongard relationship environmental morphological complexity evolved robot proc genetic evolutionary computation conference gecco acm 2012 pfeifer bongard body shape way think new view intelligence mit press cambridge 2006 howard et al evolving embodied intelligence material machine nat mach intell 2019 stanley lehman j soros last grand challenge never heard reilly online 2017 competing interest author declare no competing interest additional information reprint permission information available correspondence addressed publisher note springer nature remains neutral regard jurisdictional claim published map institutional affiliation springer nature limited 2019 nature machine intelligence vol 1 january 2019 35 view publication stats