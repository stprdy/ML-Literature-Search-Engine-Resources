476 ieee transaction artificial intelligence vol 2 no 6 december 2021 neuroevolution deep neural network current trend future challenge edgar galván peter mooney variety method applied tectural conﬁguration learning training artiﬁcial deep neural network dnn method play crucial role success failure dnn problem application evolutionary algorithm ea gaining momentum putationally feasible method automated optimization dnns neuroevolution term describes process automated conﬁguration training dnns using ea many work exist literature no comprehensive survey rently exist focusing exclusively strength limitation using neuroevolution approach dnns absence survey lead disjointed fragmented ﬁeld preventing dnns researcher potentiallyadoptingneuroevolutionarymethods research resulting lost opportunity wider application within deep learning problem article present comprehensive survey discussion evaluation using ea architectural conﬁguration training dnns article highlight pertinent current issue challenge neuroevolution identiﬁes multiple promising future research direction impact concept deep learning originated study artiﬁcial neural network anns anns achieved extraordinary result variety diverse application area numerous method applied architectural ration learning training artiﬁcial dnn method lem application recently ea gaining momentum computationally feasible method called neuroevolution automated conﬁguration learning training dnns article review 170 recent scientiﬁc paper describing major ea paradigm applied researcher conﬁguration optimization multiple dnns articulating clear understanding context feasibility neuroevolution researcher ai ea dnn beneﬁt article impact article come ing toward enhancing research capacity knowledge skill researcher currently working neuroevolution actively engaging considering becoming involved area index learning dl deep neural network dnns evolutionary algorithm ea machine learning neuroevolution manuscript received july 20 2020 revised december 9 2020 january 30 2021 accepted march 16 date publication march 22 2021 date current version december 13 work wa supported department computer science mu article wa recommended publication associate editor sanaz mostaghim upon evaluation reviewer comment corresponding author edgar galván author naturally inspired computation research group partment computer science maynooth university maynooth ireland color version one ﬁgures article available http digital object identiﬁer introduction eep learning dl algorithm 57 65 94 subset machine learning algorithm inspired deep erarchical structure human perception well production system algorithm achieved extraordinary result diverse area including computer vision 159 speech nition 58 115 board game 145 video game 114 mention design deep neural network dnns architecture along optimization eters training play crucial part success failure 105 demonstrated large number scientiﬁc work published recent year work classiﬁed one lowing two broad category method 6 34 sometimes referred neuroevolution 42 170 mentlearning rl method 158 two category also proposed specialized ature including monte carlo based simulation 119 random search 11 random search weight prediction 14 climbing 37 grid search 174 bayesian optimization 12 76 103 168 mutual information 161 162 173 rl method started gaining momentum thanks impressive result 7 16 101 179 181 182 recently ea method began yielding impressive result automatic ﬁguration dnns architecture 39 102 150 ha reported neuroevolution requires le computational time compared rl method 114 130 150 155 basically dnn feedforward artiﬁcial neural network ann many hidden layer layer constituting nonlinear information processing unit usually two hidden layer ann signiﬁes dnn adding layer unit within layer dnn represent function increasing complexity 57 evolutionary algorithm ea 6 34 also known lutionary computation system stochastic technique mimic basic principle life automatic algorithm popular proven competitive face challenging problem feature ities multiple local optimum nonlinear interaction variable 33 ea also proven yield competitive sults many problem ai approach even comparing well result achieved human perts 86 88 finding architecture often tedious process indeed lindauer 2021 ieee personal use permitted requires ieee permission see information authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply galván mooney neuroevolution deep neural network current trend future challenge 477 hutter 100 remark 300 work published area neural architecture search na almost third study corresponding neuroevolution dnns witnessed increased number publication 2017 date writing considered period 2009 present see fig 1 supplementary material along detail systematic literature search focus exclusively architecture search od dnns well approach training dnns particularly work considers landmark ea genetic algorithm gas 66 evolution strategy es 13 132 genetic programming gp 87 well recent ea variant differential evolution de 127 neuroevolution augmenting topology neat 149 grammatical evolution ge 135 furthermore consider main dl architecture classiﬁed liu et al 105 used neuroevolution including autoencoders aes 24 convolutional neural network cnns 90 deep belief network dbns 176 177 restricted boltzmann machine rbms 105 118 dl architecture ered study include recurrent neural network rnns 75 long memory lstm 52 previous ature review area include conducted floreano et al 42 yao 170 recent review provided stanley et al 148 darwish et al 23 baldominos et al 9 former work explains inﬂuence modern computational power scale allowing grand ambition neuroevolution dl many year ago achieved fulﬁlled darwish et al 23 deliver broader introduction overview swarm intelligence ea optimization hyperparameters architecture neural network data analytics baldominos et al 9 work discussesafeweasmethodsemployedindnns withparticular emphasis cnns contrast work article provides new contribution concentrating conﬁguration design neuroevolution approach dl consider ea approach applied dl particular speciﬁc conﬁguration purpose article delivers estimation work neuroevolution dnns rest article organized follows section ii provides some background dl ea section iii discus architecture dnns evolved efﬁciently using ea sectionivdiscussestrainingofdnnswitheaswhilesectionv set major challenge fertile avenue future work finally section vi concludes article ii background deep neural network deep learning dl emerged work hinton et al 65 studying anns become active research area 105 ann consists multiple simple connected unit neuron producing sequence vations process training ann may require long casual chain computational stage 138 dl algorithm class machine learning algorithm using multiple layer progressively extract higher level feature raw data input deep refers number transformation layer raw data dl subsequent level attempt learn order transform input data progressively abstract composite representation following section summarise neuroevolution dnns ha applied development wide range anns including not limited convolutional neural network autoencoders deep belief network recurrent neural network taxonomy architecture found 80 cnns 108 memory network 1 deep learning architecture convolutional neural work cnns cnns shown impressive performance processing data topology deep network sists set layer containing one plane unit plane receives input neighborhood plane previous layer idea connecting unit receptive ﬁelds date back perceptron animal visual cortex organization discovered hubel wiesel 70 input image convolved trainable kernel ﬁlters offset produce feature map ﬁlters include layer connection weight usually four pixel feature map form group passed function sigmoid function hyperbolic tangent function pixel produce additional feature map layer n plane normally used layer n feature detected layer called convolutional layer feature detected exact location le important convolutional layer followed another layer charge performing local averaging subsampling operation due high dimensionality nns input weight cnn classiﬁer may cause overﬁtting problem addressed using pooling process also called subsampling reducing overall size signal normally cnn trained usual backpropagation cedure proposed lecun et al 95 learning process cnn determined following three key element 1 sparse interaction reduces computational processing withkernelsthataresmallerthantheinputs 2 parametersharing referstolearningonesetofparametersinsteadoflearningoneset location ﬁnally 3 equivariance representation mean whenever input change output change manner 57 cnns ﬁrst successful dl tures applied face detection handwriting recognition image classiﬁcation speech recognition natural language processing recommender system 31 90 172 early evolution cnn architecture ha slow remarkable lenet 95 proposed late alexnet 90 proposed decade later similar two ﬁve convolutional layer respectively moreover also used kernel large receptive ﬁelds layer close input smaller ﬁlters closer output major difference latter used rectiﬁed linear unit activation function became standard designing cnns since alexnet use novel deeper model took simonyan zisserman 146 imagenet challenge proposed model known network authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 478 ieee transaction artificial intelligence vol 2 no 6 december 2021 proposed not only deeper use plex building block szegedy et al 159 proposed googlenet also known inception network used inception block also 2015 residual network resnet architecture proposed et al 64 imagenet challenge moreover multiple cnns variant proposed combining convolution ae 72 rbms 29 description variant network found 105 2 deep learning architecture autoencoders aes aes simple learning circuit designed transform input output minimum amount distortion ae consists combination encoder decoder function tation decoder function convert new tation back original form aes attempt preserve mation provide output make suitable data preprocessing iterative architecture dnns 91 despite work appearing 2020 author suggest still relatively little work exploring plication ofeastoneuralarchitecturesearch baldi 8 argued aes taken center stage deep architecture approach wa still little theoretical understanding aes deep architecture date interesting theoretical work started ﬁlling important gap studying using mutual information 161 173 choosing appropriate ae architecture order process speciﬁc dataset mean ae capable learning optimal representation data 17 encoding aes within some representation mean approach could broad enough consider ae variation 17 unsupervised feature learning approach aes attempt learn compact resentation input data whilst retaining important information representation representation pected completely reconstruct original input make initialization ae critical 67 whilst aes induce helpful useful representation input data only capable handling single sample not capable eling relationship pair sample input data 3 deep learning architecture deep belief network dbns dbns generative model implemented number way including rbms see section aes see section dbns suited problem feature extraction drawn tremendous attention cently 177 dbns traditional classiﬁers large number parameter require great deal training time 18 rbms stacked together considered dbn fundamental building block dbn rbms consisting one visible layer one hidden layer dbns applied classiﬁcation problem feature vector data sample used set value state visible variable lower layer dbn dbn trained generate probability distribution possible label input data offer good solution learn hierarchical feature representation data 4 dl architecture network type introduce network architecture namely recurrent ral network rnns restricted boltzmann machine rbms long short term memory lstm rnns cnns input vector eventually producing vector output number layer cnn determine amount computational step required rnns ﬂexible allow operation across sequence vector connection unit network form directed cycle creates internal state network allowing u exhibit dynamic temporal behavior internal hidden state allows rnn store information past efﬁciently rnns well suited sequential data prediction ha seen applied area statistical language modeling prediction however computational power rnns make difﬁcult train main reason difﬁculty due exploding vanishing gradient problem 75 although vanishing gradient ha addressed lstm gated rnns theory rnns make use information arbitrarily long sequence realistically limited considering only step rbms restricted boltzmann machine rbm work symmetrically connected unit designed make stochastic decision whether neural network rbm no connection hidden unit ple hidden layer learning occurs considering hidden activity single rbm data training higher level rbm 136 no communication connection layer restriction introduced boltzmann machine rbms probabilistic model using layer hidden binary variable unit model distribution visible layer variable rbms successfully applied problem involving high dimensional data image text 93 outlined fischer igel 41 rbms subject recent research proposed building block multilayer learning architecture dbns concept hidden neuron extract relevant feature data observation feature serve input another rbm stacking rbms allows network learn feature feature goal arriving representation 115 lstm lstm cial type rnns capable learning dependency work incredibly well large variety problem currently widely used basic unit within hidden layer lstm network called memory block containing one memory cell pair adaptive multiplicative gating unit gate input output cell block 52 lstm network possible circumvent problem vanishing error gradient network training process method error back propagation lstm network usually controlled recurrent gate error propagated back time potentially unlimited number virtual layer way learning take place lstm preserving memory thousand even million authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply galván mooney neuroevolution deep neural network current trend future challenge 479 time interval past network topology lstm developed accordance speciﬁcs problem recurrent neural network rnns long ory lstm emerged effective scalable model several learning problem related sequential data 59 gers schmidhuber 53 showed standard rnns fail learn presence time lag exceeding ﬁve ten step relevant input event target signal lstm not affected problem capable dealing minimal time lag excess 1000 step lstm clearly outperforms previous rnns not only regular language benchmark according previous research also language benchmark 53 evolutionary algorithm evolutionary algorithm ea 6 34 also known lutionary computation system refer set stochastic mization bioinspired algorithm use evolutionary principle build robust adaptive system ﬁeld ha origin four landmark evolutionary method genetic algorithm 66 55 evolution strategy 133 139 evolutionary programming 43 genetic programming 87 key element algorithm undoubtedly ﬂexibility allowing practitioner use element two different ea technique consequently boundary approach no longer distinct allowing holistic ea framework emerge ea work population tation potential solution particular problem potential solution commonly known individual represents point search space optimal solution lie population evolved mean genetic operator number generation produce better result problem individual evaluated using ﬁtness function determine good bad individual problem hand ﬁtness value assigned individual population probabilistically determines successful individual propagating part code future generation evolutionary process carried using genetic erators selection crossover mutation key operator used ea selection operator charge choosing one individual population based ﬁtness value multiple selection operator proposed one popular selection operator tournament selection best individual selected pool normally size population stochastic crossover also known recombination operator exchange material normally two selected individual operator charge exploiting search space stochastic mutation operator make random change gene individual charge exploring search space mutation operator important guarantee diversity population well recovering genetic material lost evolution evolutionary process repeated stopping condition reached maximum number generation ha executed population stage contains best evolved potential solution problem may also fig 1 nasnet search space 182 b scalable architecture image classiﬁcation consisting two repeated motif termed normal cell reduction cell left full outer structure omitting skip input clarity right detailed view skip input c example cell dotted red circle demarcates pairwise combination example mutation used b redrawn zoph et al 182 real et al 130 fig 2 genetic representation cgp individual encoding cnn architecture b phenotypic representation c cnn architecture deﬁned gene no 5 coloured black background genotype not expressed phenotype summation node c light yellow background performs max pooling lh input node no 3 get input tensor size redrawn suganuma et al 153 represent global optimal solution algorithm 1 show typical step considered ea main ea employed dnns genetic algorithm genetic programming evolution strategy others differential evolution grammatical lution neuroevolution augmenting topology fig 2 supplementary material show gas only ea method used training dnns rest conﬁguration dnns taxonomy algorithm found 44 160 1 evolutionary algorithm genetic algorithm gas ea wa introduced holland 66 highly authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 480 ieee transaction artificial intelligence vol 2 no 6 december 2021 algorithm 1 common ea process network design adapted 167 1 input reference dataset number generation number individual generation n mutation crossover probability pm pc 2 initialisation generating set randomised individual n n computing recognition accuracy 3 1 2 4 selection producing new generation n n russian roulette process n n 5 crossover pair performing crossover probability pc 6 mutation individual mt n n performing mutation probability pm 7 fitness evaluation computing ﬁtness recognition accuracy new individual mt n n 8 end 9 output set individual ﬁnal generation mt n n ﬁtness value popularised goldberg 55 subsequently achieving nary result well reaching multiple research community including machine learning neural network gas quentlydescribedasfunctionoptimizers butnowthetendencyis consider gas search algorithm able ﬁnd solution multiple form gas proposed specialized literature bitstring representation one predominant encoding representation used gas crossover main genetic operator mutation secondary operator reproduce offspring evolutionary search 2 evolutionary algorithm genetic programming gp ea subclass gas popularised koza 87 gp form automated programming individual domly created using functional terminal set required solve given problem multiple type gp proposed literature typical structure predominant form gp ea cartesian gp cgp 113 another form gp ha used neuroevolution dnns 152 153 3 evolutionary algorithm evolution strategy e ea introduced rechenberg 133 es generally applied representation mization problem e mutation main operator whereas crossover secondary optional operator historically two basic form e known µ λ µ λ µ refers size parent population whereas λ refers number offspring produced following generation selection applied former e offspring replace parent whereas latter form e selection applied offspring parent form population following generation covariance matrix proposed hansen 61 63 art adapts full covariance matrix normal search distribution 4 evolutionary algorithm evolutionary programming ea proposed fogel 43 little difference e ep difference lie lack use crossover ep whereas genetic operator rarely used e ep normally parent produce offspring whereas e number offspring produced genetic operator higher parent 5 ea others multiple algorithm used dnns notably differential evolution de grammatical evolution ge neuroevolution augmenting topology neat de differential evolution wa proposed price storn 127 de ha proven highly efﬁcient continuous search space often reported robust well achieving faster convergence speed compared optimization method 128 perturb population member scaled difference randomly selected distinct population member ge grammatical evolution ea proposed ryan et al 135 mapping process used generate genetic program using binary string select production rule form grammar deﬁnition ge seen special form gp one main difference unlike gp ge doe not perform evolutionary process program neat neuroevolution augmenting topology form ea proposed stanley miikkulainen 149 neat technique evolving neural network following three element crucial neat work 1 historical marking thatallowssolutionstobecrossedover 2 speciationthatallows deﬁning niche 3 starting minimal structure allowing u incrementally ﬁnd better solution iii evolving dnns architecture ea motivation recently ea started gaining momentum designing dnns architecture 39 42 84 102 112 130 131 149 167 popularity algorithm due fact method offering parallelized mechanism simultaneously explore multiple area search space offering mechanism escape local optimum algorithm inherently suited parallelization meaning potential solution taneously computed within acceptable time steady increase computing power including graphic processing unit contributing faster computational calculation ea critique despite popularity ea designing dnn tures also criticized slow learner well computationally expensive evaluate 33 authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply galván mooney neuroevolution deep neural network current trend future challenge 481 example using small ea 20 uals potential solution training set 50 000 sample one generation alone hundred thousand million generation require one million evaluation use ﬁtness function dl architecture cnns dufourq bassett 32 used ga evolve cnn one convolution one max pooling dropout among others author reported competitive result compared algorithm emnist dataset well fashion dataset desell 28 proposed algorithm based neat 149 evolve cnn architecture desell carried some modiﬁcations neat algorithm evolve cnn architecture selection crossover mutation whereas operator played important role produce behaved cnns mutation operator involving seven type operation seemed crucial result reported mnist dataset zoph et al 182 proposed nasnet search space deﬁned predetermined outer structure depicted fig 1 previous work resnet 64 densenet 68 outer structure composed convolutional cell called normal cell coloured pink fig 1 reduction cell coloured grey repeated many time former type cell return feature map dimension whereas latter return feature map height width reduced factor two cell type constrained architecture architecture normal cell different architecture reduced cell goal architecture search process wa discover architecture two type cell example shown fig 1 b real et al 130 proposed regularized evolution evolve image classiﬁer achieving superior accuracy method ﬁrst time author used ea ﬁxed length member encoding architecture cnns used nasnet search space 182 goal wa discover architecture normal cell reduction cell depicted fig 1 real et al 130 used modiﬁed version tournament selection two type tation drive evolution tournament selection see section ii wa modiﬁed newest genotype chosen older genotype mutation operator involved one two operation taking place individual hidden state mutation op mutation execute any type mutation ﬁrst random cell chosen pairwise combination stochastically selected see fig 1 c ﬁnally one two pair selected randomly hidden state replaced another hidden state constraint no loop formed op mutation differs only modifying operation used within selected hidden state fig 1 show two mutation operation work author used dataset test proposed evolution compared method random search achieved better accuracy result reducing putational time required algorithm compared two method author also used ﬁttest chromosome found algorithm retrained using imagenet dataset xie et al 167 proposed genetic cnn automatically learn structure cnns limited number layer well limited size operation convolutional ters xie et al 167 adopted ga binary representation represent part evolved network network composed various stage stage composed node represent convolutional operation binary encoding adopted xie et al 167 represents connection number ordered node representation allowed use crossover along roulette selection mutation deﬁned stage minimal unit apply crossover mutation even restriction author achieved competitive accuracy result using mnist datasets also demonstrating approach generalized using learned architecture dataset wa achieved cause approach wa able produce network alexnet 90 vggnet 146 network googlenet 159 highway network deep resnet 64 reported beneﬁcial applied computer vision problem real et al 131 used ea automatically optimize cnn architecture individual architecture encoded graph vertex representing tensor two resent spatial coordinate image third number channel activation function normalization 71 applied vertex eleven type mutation involving inserting layer removing layer modifying layer parameter used real et al 131 indicated crossover not improve result yielded mutation operator andreportedcompetitiveaverageaccuracyresults ﬁve independent run datasets compared algorithm including resnet 64 densenet 68 suganuma et al 153 used cartesian gp 113 ically design cnn architecture genotype encodes mation type connection node fig 2 picts idea type include convblock resblock max pooling average pooling summation concatenation vblock consists standard convolution processing followed batch normalization relu 118 whereas resblock convblock followed tensor summation cgp encoding scheme represents program directed acyclic graph grid nr row nc column fig 2 b provides example phenotype obtained fig 2 case grid deﬁned nr 2 nc corresponding cnn architecture depicted fig 2 c author used dataset evaluating cgp individual expensive adopted simple e see section ii author approach achieved competitive result compared method including resnet 64 author authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 482 ieee transaction artificial intelligence vol 2 no 6 december 2021 reported encouraging result using cgp automatically ure cnn architecture regardless sample size used work example cnn architecture produced cgp small dataset scenario wider compared architecture suganumaet al 151 extended work proposing rich initialization early termination former us resnet densely connected cnn densenet build cgp individual latter refers terminating individual evaluation accuracy poor reference curve built compared previous accuracy curve assunção et al 3 4 proposed denser hybrid anism gas dynamic structured ge 135 evolve dnns architecture outer layer proposed approach charge encoding macro structure dnns evolved mean gas dynamic structured ge charge inner layer encodes parameter dnns ing selection two form crossover three type mutation add replicate remove unit outer layer used multiple datasets cluding mnist rectangle similarly work 112 153 assunção et al 3 4 performed only ten epoch train dnns reported competitive result compared algorithm interestingly served ﬁtness increase time number hidden layer decreased suggesting two metric conﬂict optimising cnns architecture sun et al 157 proposed use ga encoding evolve mean selection crossover mutation unsupervised dnns learning meaningful sentations computer vision task approach included following two main part ﬁnding optimal architecture dnns desirable initialization weight activation function ii parameter value nection weight desirable initialization ﬁrst wa primarily achieved using encoding wa inspired work conducted zhao et al 178 captured element described one gene represents average 1000 parameter encoding exploitation achieved crossover reduced overcome problem sun et al used backpropagation part ii various part approach author demonstrated local search adopted part wa necessary order achieve promising result recently sun et al 156 proposed ga named ing deep cnns automatically discover cnn architecture inspired real et al 131 sun et al 157 proposed method evaluate ﬁtness individual population 30 independent run also used selection mutation crossover crossover wa not used study carried real et al 131 itation real work sun et al 157 use encoding convolutional pooling full connection layer using standard deviation average value connection weight able efﬁciently evaluate chromosome classiﬁcation error well number connection weight wa used evaluate chromosome along normal cnn deep architecture author restricted training ten epoch last epoch ﬁtness computed chromosome author reported highly encouraging result many case achieving better result compared algorithm benchmark datasets 163 van wyk bosman described neural ture search na method automate process ﬁnding optimal cnn architecture arbitrary image restoration work demonstrates feasibility performing na signiﬁcant memory computational time constraint dataset wa chosen evaluation author found conﬁgured architecture wa heavily overparameterized wa not case evolved nn performed task signiﬁcantly lower number total parameter sun et al 155 proposed encoding strategy built block namely resnet densenet used variable length ga allowing automatically evolve cnns architecture unrestricted depth sun et al 155 used selection crossover mutation evolve candidate solution employed repair mechanism produce valid cnns author used datasets compared result nine manually designed method four semiautomatic method ﬁve automatic method interestingly result outperformed method well semiautomatic method term classiﬁcation error rate evolutionary multiobjective optimization emo 21 25 explained section ha little used automatic conﬁguration dnns network well optimization hyperparameters work latter include recent approach proposed kim et al 81 author used speed accuracy two conﬂicting objective optimized mean emo use nondominated sorting geneticalgorithmii 26 using three classiﬁcation task including use mnist drowsy behaviour recognition datasets spired kim et al 81 study lu et al 106 used emo conﬂicting objective lu et al 106 empirically tested multiple computational complexity metric measure speed including number active node number active connection node operation mention lu et al 106 indicated latter metric wa accurate wa used second conﬂicting tive optimization moreover author used ingenious bitstring encoding allowed use homogeneous crossover mutation one change mutation operation normally adopted gas author used datasets achieving petitive result approach human expert conﬁgurations wang et al 164 explored ability differential evolution de automatically evolve architecture ters deep cnns method called decnn us de control evolution rate managed differential value decnn evolves architecture cnns authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply galván mooney neuroevolution deep neural network current trend future challenge 483 encoding strategy implemented use gle ip address represent one layer dnn ip address pushed sequence interface corresponding order layer dnns six mnist datasets used benchmark testing decnn performed competitively 12 competitor six benchmark martín et al 110 proposed evodeep ea designed ﬁnd best architecture optimize essary parameter train dnn us machine model order determine possible transition different kind layer allowing evodeep generate valid sequence layer output one layer ﬁts input requirement next layer tested mnist datasets elsken et al 38 proposed multiobjective optimization lutionary algorithm moea named lemonade employing inheritance mechanism based mate network morphism mutation operator speeding training dnns architecture better result reported test error number parameter compared nasnet search space architecture yang et al 169 propose continuous evolution strategy utilizing knowledge learned last evolution tion architecture search nondominated sort strategy adopted select several excellent architecture continuous evolutionary architecture search car provides series architecture model pareto front high efﬁciency result indicate car method give superior result benchmark datasets model shirakawa et al 144 propose general framework dynamic optimization network structure tion weight parametric distribution used generate network structure distribution parameter derstood network hyperparameters method shown computationally efﬁcient static optimization approach ﬂexible conventional tion approach methodology applied selection layer selection activation function adaptation stochastic network ﬁnally selection connection densenets author conclude proposed method capable learning layer size appropriate mix rate activation function within reasonable computational time optimizing weight architecture ann within single training run considering possible architecture subgraphs supergraph called architecture search na na used akimoto et al 1 develop generic optimization framework based stochasticrelaxationfor architecturesearch frameworkcan handlepracticallyanytypeofarchitecturevariationprovideditis possibletodeﬁneaparametricfamilyofprobabilitydistributions upon using adaptation mechanism tic natural gradient ascent improves optimization speed add robustness hyperparameter tuning experimental analysis indicates adaptive stochastic natural gradient method na achieves signiﬁcant speedup evolutionary convolutional autoencoders caes without promising performance awad et al 5 use de na de ha shown achieve excellent performance range na benchmark found best approach applying de parameter discrete categorical maintain population continuous space perform canonical de only using discretization copy individual order evaluate de shown perform well four recent na method including na baseline algorithm random search author conclude de ha good ability handle mixed data type space dl architecture autoencoders suganuma et al 152 used cartesian gp 113 adopting e technique using selection mutation operator only optimize dns architecture image restoration author used convolutional autoencoders caes built upon convolutional layer skip connection optimizing network conﬁned search space symmetric caes author achieved competitive result method need using adversarial training sophisticated loss function normally employed image restoration task luo et al 107 propose novel semisupervised ae called discriminant ae application fault diagnosis posed method ha different training process loss function traditional aes case discriminant ae capable extracting better representation raw data provided completely different loss function used representation extracted discriminant ae generate bigger difference sample class discriminant ae make full use label feature variable obtain optimal representation center group sample separated much possible ashfahani et al 2 propose devdan deep evolving denoising ae application data stream analytics devdan demonstrates proposal denoising ae variant traditional ae focused retrieving original input information noise perturbation devdan feature open structure capable initiating structure ning without presence preconﬁgured network structure devdan ﬁnd competitive network architecture compared method ten classiﬁcation datasets dl architecture deep belief network dbns offer promising solution learn ful hierarchical feature representation data provided chen et al 18 use dbns automatically extract feature image proposing evolutionary function array classiﬁer voter efacv classiﬁes feature image extracted dbn composed stacked rbms e used train efacv mainly used binary classiﬁcation problem multiclass classiﬁcation problem necessary multiple efacv efacv show fast computational speed reduction overall training time experiment performed mnist dataset liu et al 104 describe structured learning dnns based multiobjective tion propose multiobjective optimisation evolutionary algorithm moea dbn learning procedure use authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 484 ieee transaction artificial intelligence vol 2 no 6 december 2021 rbm train dn layer layer necessary remove unimportant unnecessary connection dnn move toward discovering optimal dnn connection structure sparse possible without lost representation experiment based mnist datasets different training sample indicate moea approach effective zhang et al 176 use dbns prognostic health management system aircraft aerospace design proposing modbne multiobjective dbns ensemble powerful multiobjective ea based decomposition integrated training dbns evolve multiple dbns simultaneously accuracy diversity two conﬂicting objective dbn composed stacked rbms trained unsupervised manner modbne evaluated compared prominent diagnostics benchmarking problem nasa turbofan engine degradation problem proposed approach structural parameter dbn strongly dependent complexity problem number training sample available approach worked outstandingly well comparison existing approach zhang et al 177 considered problem learning method idea assign misclassiﬁcation cost class appropriately author report study dbns network drawn lot attention researcher recently ances class input data problem disproportionate number class instance affect quality applied learning algorithm zhang et al 177 argue dbns well placed handle type imbalanced data problem evolutionary deep belief network proposed deal problem assigning differential misclassiﬁcation cost data class evaluated 58 popular knowledge evolutionary learning keel benchmark datasets network lstm rrn rbm shinozaki watanabe 143 propose optimization egy dnn structure parameter using ea dnn structure parameterized directed acyclic graph experiment carried phoneme recognition spoken digit detection conducted upon massively parallel computing platform using 62 computing graphic processing unit rbms used training phase ororbia et al 121 develop evolutionay exploration menting model examm designed devolve rnns using selection memory structure rnns well suited task performing prediction time series data examm wa designed select large number memory cell structure allowed evolutionary approach yield best performing rnn architecture peng et al 123 use lstm nn capable analyzing time series long time span order make prediction tackle vanishing gradient problem study us de identify hyperparameters lstm author claim ﬁrst time de ha used choose hyperparameters lstm forecasting application forecasting involves complex continuous nonlinear function de approach well suited type problem gonçalvez et al 56 introduced semantic learning machine slm shown outperform similar method wide range supervised learning problem slm described geometric semantic hill climber approach nns following 1 λ strategy search best nn architecture conﬁguration allows slm concentrate current best nn without drawing any penalty crucial aspect slm approach geometric semantic mutation elsaid et al 36 proposed approach based 35 called adaptive structure transfer learning strategy goal improving training time deep rnns author used statistical information topology source rnn topology weight distribution reported better performing rnns using half number genome compared nontransfer method summary evolving dnn architecture using ea ea method different representation used designing dnns ranging method including gas gp e using hybrid combining example use ga ge ingenious representation interesting proaches achieving extraordinary result conﬁgured network 130 commonplace approach some case employing hundred computer 131 using fewgpus 156 focused designing deep cnns ae rbm rnn lstm dbm also considered despite commending le research attention paper neuroevolution selected paper way order ﬁnd selection paper cinctly demonstrated use neuroevolution dnns table ordered alphabetical order surname summarises ea representation used representation individual genetic operator used ea parameter table outline computational resource used responding study attempting outline number gpus used calculation gpu day per run approximated sun et al 155 indicate benchmark datasets used experimental analysis finally table indicates neural network architecture ha evolved automatically using semiautomated approach whilst also indicating target dnn architecture every selected paper doe not report mation some paper omit detail computational resource others omit information number run interesting output table numerous difference approach used paper listed crossover omitted several study mostly due encoding adopted various researcher population size selection strategy ea change study mnist cifar clearly popular benchmark datasets see many example study using benchmark datasets speciﬁc application domain authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply galván mooney neuroevolution deep neural network current trend future challenge 485 table ea representation genetic operator parameter v alues used neuroevolution design dnns architecture includes datasets used corresponding computational effort gpu day automatic matic refer architecture evolved automatically using approach dash indicates information wa not reported iv training dnns ea motivation backpropagation ha one successful dominant method used training anns past number decade 134 simple effective elegant method applies stochastic gradient descent sgd weight ann goal minimize overall error however remarked morse stanley 117 widely held belief around 2006 wa backpropagation would suffer loss gradient within dnns turned false ha subsequently shown backpropagation sgd effective optimizing dnns even million connection 69 backpropagation sgd beneﬁt availability sufﬁcient training data availability computational power problem space many dimension success using sgd dnns still surprising practically speaking sgd highly susceptible local optimum 117 jin et al 73 kleinberg li 85 argue independently noise help sgn escape saddle point due randomness estimator choromanska et al 20 kawaguchi huang 77 esise independently presence multiple local optimum not problem similar best solution ea perform well presence saddle point wa discussed section critique no guarantee convergence solution computed using ea usually classiﬁed near optimal ea effect approximation gradient estimated individual population corresponding objective hand sgd authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 486 ieee transaction artificial intelligence vol 2 no 6 december 2021 computes exact gradient subsequently some researcher consider ea unsuitable dl task reason however ha demonstrated exact approximation obtained sgd not absolutely critical overall success dnns using approach lillicrap et al 99 demonstrated breaking precision gradient calculation ha no negative detrimental effect learning morse stanley 117 speculate reason lack research focus using evolutionary computation dnns wa not entirely related concern around gradient rather belief new approach dnn could actually emerge outside sgd dl architecture convolutional neural network et al 150 proposed method evolve weight convolutional dnns using simple ga population chromosome ﬁxed length proposed anism successfully evolved network four million free parameter some key element study conducted et al 150 successfully evolve large neural network include following 1 use selection mutation genetic operator only 2 use novel method store large parameter vector compactly representing initialization seed plus list random seed produce series mutation produced parameter vector 3 use computational setting including one modern computer 4 gpus 48 cpu core well 720 cpu core across dozen computer instead using optimization technique mean ﬁtness function et al 150 used novelty search 97 rewarding new behavior individual author used rl benchmark problem including atari 2600 10 114 hard maze 98 humanoid locomotion 15 strated proposed approach competitive algorithm problem including dqn 114 method 140 e 137 pawelczyk et al 122 focused encoding cnns random weight using ga main goal wa let ea learn number gradient learning iteration necessary achieve error using mnist dataset approach reported best result around 450 gradient learnt iteration compared 400 constant iteration yielded best overall result dl architecture autoencoders david greental 24 used ga ﬁxed length evolve weight value ae dnn chromosome wa evaluated using rms error training sample author used only ten individual 50 weight individual updated using agation half population randomly generated generation tested approach dataset compared approach traditional ae using svm reporting better classiﬁcation error dnn versus author indicated reason method produced better result wa gradient descent method backpropagation highly susceptible trapped local optimum ga method helped prevent fernando et al 40 introduced differentiable sion compositional pattern producing network called differentiable pattern producing network dppn dppn approach attempt combine advantage result learning nn optimization capability evolutionary approach dppn ha demonstrated rior result benchmark dataset mnist generic ea used optimization algorithm dppn result indicate dppns associated learning algorithm ability dramatically reduce number parameter larger nn author argue integration evolutionary learning allows optimization avoid becoming stuck local optimum point saddle point relevant work morse stanley 117 proposed limited evaluation lutionary algorithm leea using ga ﬁxed length representation evolve mean crossover mutation 1000 weight network inspired sgd leea compute error gradient single instance training set ﬁtness computed using small fraction training set leaa doe not generalize whole training sample author proposed following two approach using small batch instance ii using ﬁtness function considered performance current minibatch performance individual ancestor minibatches testing task function approximation time series prediction task author clared leea competitive approach even author not use dnns small artiﬁcial nn interesting note used dnn setting khadka tumer 79 remark deep rl method notoriously sensitive choice hyperparamaters often brittle convergence property method also challenged long time horizon sparse reward ea respond positively challenge use ﬁtness metric allows ea tolerate sparse reward distribution endure long time horizon however ea struggle perform well optimization large number parameter required author introduce evolutionary rl erl algorithm ea used evolve diverse experience train rl agent agent subjected mutation crossover operator create next generation agent erl described guide guide bias exploration ward state higher better return promoting diversity explored policy introduces redundancy stability recurrent neural network rnns see section porate memory nn storing information past within hidden state network kahdka et al 78 introduced new network architecture called ular memory unit mmu mmu disconnect memory authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply galván mooney neuroevolution deep neural network current trend future challenge 487 table ii ea representation operator parameter v alues neuroevolution dnn training datasets used computational effort gpu day indicates information wa not reported central computation operation without requiring costly memory management strategy neuroevolutionary method used train mmu architecture performance mmu approach gradient descent neuroevolution examined author ﬁnd neuroevolution repeatable generalizable across task mmu nn designed highly conﬁgurable characteristic exploited neuroevolutionary algorithm evolve work population size set 100 fraction elite set fully differentiable version mmu ﬁnd gradient descent performs better sequence recall task neuroevolutionperformssigniﬁcantly better gradient descent sequence classiﬁcation task cui et al 22 proposed use ea sgd speed training anns interestingly author use multiple sgd optimizers certain hyperparameters learning schedule used build ea individual showed different task see table ii proposed approach od speed training neural network meier et al 111 proposed gradient estimator considers surrogate gradient direction well random search direction author demonstrated proposed approach considerably improves gradient estimation capability employed e particularly used mnist dataset lesser degree rl task shahab grot 141 proposed use ea along sgd training neural model main motivation due fact ea inherently parallel suitable distributed training setup compared minibatch sgd important ha demonstrated latter fails reduce number training iteration beyond minibatch size 142 summary training dnns using ea early year neuroevolution wa thought method might exceed capability propagation 170 anns general dnns ular increasingly adopted use sgd backpropagation idea using ea training dnns instead ha almost abandoned dnn research community ea uinely different paradigm specifying search problem 117 provide exciting opportunity learning dnns comparing neuroevolutionary approach approach gradient descent author khadka et al 78 urge caution generation neuroevolution not readily rable gradient descent epoch despite fact ha argued ea compete search small problem well using nn nondifferentiable activation function 109 encouraging result achieved 54 116 126 inspired some researcher carry research training dnns includes work conducted david greental 24 fernando et al 40 using deep ae pawelczyk et al 122 et al 150 use deep cnns table ii structured similar way table table selected paper way order ﬁnd selection paper succinctly demonstrated use ea training dnns see mutation selection used selected work crossover omitted certain situation see greater diversity type benchmark datasets used greater focus datasets problem future work neuroevolution dnns ea dnns ea successfully used automatically designing artiﬁcial dnns described throughout article tiple algorithm proposed recent year including genetic cnn 167 evolution 131 evolving deep cnn 156 hierarchical evolution 102 despite success automatically conﬁguring dnns architecture common limitation method training time needed range day week order achieve competitive result based ec us efﬁcient model also known surrogate estimating ﬁtness value ea 74 hence ec considerably speed authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 488 ieee transaction artificial intelligence vol 2 no 6 december 2021 evolutionary search reducing number ﬁtness evaluation time correctly estimating ﬁtness value some potential solution adoption ea limited research discussed article dealt limited exception sun et al 154 demonstrate using ensemble member successfully used correctly estimate cnn accuracy considerably reduced training time 33 10 gpu day still achieving competitive accuracy result compared limitation unknown number training run necessary achieve good prediction performance combining sgd ea discussed section iv backpropagation one successful method currently used training anns backpropagation normally involves application sgd weight anns goal minimize overall error era dl began perhaps consider early date time use ea wa common wa abandoned thanks impressive result deep model trained sgd discussed section iv ha small number work appearing recently considered replacing sgd ea indeed begun observe trend combining technique yielding better result time speeding training process anns 117 129 another exciting promising area research mutation neutral theory seen numerous study used selection tation only drive evolution automatically ﬁnding suitable dnn architecture see section iii train dnn see section iv table ii present summary genetic operator used various researcher interestingly many searcher reported highly encouraging result using two genetic operator including work conducted real et al 130 131 using gas hundred gpus well work carried suganuma et al 153 employing cartesian gp using only gpus kimura neutral theory molecular evolution 82 83 state majority evolutionary change molecular level result random ﬁxation selectively neutral tations mutation one gene another neutral doe not affect phenotype thus mutation take place natural evolution neither advantageous disadvantageous survival individual reasonable extrapolate evolution ha managed produce amazing complexityandadaptationsseeninnature thenneutralityshould aid also ea however whether neutrality help hinders search ea not answered general one only answer question within context speciﬁc class problem neutral representation set operator 46 51 124 125 not aware any work neuroevolution dnns neutrality work 45 discus some venue research neutrality dnns some interesting encoding adopted researcher including suganuma work 153 see fig 2 allow measurement level neutrality present evolutionary search indicates whether presence beneﬁcial not certain problem dnns neutrality beneﬁcial takingintoconsiderationspeciﬁcclassesofproblems representation genetic operator also immediately positive impact training time needed evaluation potential ea candidate solution not necessary multiobjective optimization reported research result one objective variable ha used nn training classiﬁcation error cnns two objective rarely considered task becomes much difﬁcult 21 25 objective may conﬂict mo concerned simultaneous optimization one objective function function conﬂict set tradeoff solution among objective sought no single global optimum exists optimal tradeoff solution no objective improved without degrading one others idea captured pareto dominance relation solution x search space said another solution x least good objective strictly better least one objective important aspect emo 21 25 allows solution ranked according performance objective respect solution population emo one active research area ea 38 yet surprising see emo approach scarcely used automatic conﬁguration artiﬁcial dnns architecture learning dnns often conﬁguration artiﬁcial dnns requires simultaneously satisfying multiple objective reducing tional calculation training dataset attaining high accuracy emo offer elegant efﬁcient framework handle conﬂicting objective aware only work area 38 81 104 106 176 summarised section iii dnns fitness landscape analysis genetic operator work neuroevolution dnns used core genetic operator including selection mutation crossover ha also used work use operator summarised table ii use crossover sometimes difﬁcult adopt depending encoding used some variant proposed 156 study adopted standard crossover operator discussed 81 however no work area neuroevolution dnns focused attention explaining adoption particular netic operator particular problem notion ﬁtness landscape 165 ha u several decade nonmathematical aid ha proven authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply galván mooney neuroevolution deep neural network current trend future challenge 489 table iii common datasets used neuroevolution dnns powerful understanding evolutionary search viewing search space deﬁned set potential solution landscape heuristic algorithm ea thought navigating ﬁnd best solution essentially highest peak landscape height point search space represents abstract way ﬁtness solution associated point landscape therefore knowledge interface problem ea help researcher practitioner deﬁne genetic operator including mutation crossover connectivity structure landscape standardised neuroevolution study dnns section ii multiple dnns architecture posed specialized literature many research work reviewed article compared result yielded neuroevolution algorithm however unclear some technique better others type operator used representation adopted study type learning employed training due lack standardised study neuroevolution dnns difﬁcult draw ﬁnal conclusion help u identify element promising dnns lack standardised study mean not indicate ea paradigm associated genetic operator preferred automatic conﬁguration particular dnn architecture well training diversifying use benchmark problem dnns new large datasets combined increasingly powerful computational resource allowed dnns solve hard ing image classiﬁcation certainly considered primary benchmark evaluate dnns 180 benchmark datasets many included table iii used compare putational result experimental setup produced different research group believe success dnns provides opportunity expand dnns domain however successfully comprehensive benchmark datasets required without benchmark may difﬁcult make convincing argument support dnns problem beyond traditional domain image tion machine translation new benchmark problem must allow appropriate comparison performance testing na algorithm some step already taken ample ying et al 171 propose provides search space test na algorithm without incurring much compute cost wa proposed zela et al 175 adapts used search method dart proxyless na dong yang 30 extended different search space result multiple datasets critical benchmark datasets available freely stallkamp et al 147 argue niche area trafﬁc sign recognition difﬁcult compare published work study based different data consider classiﬁcation different way use proprietary data some case not publicly available make comparison result difﬁcult zhang et al 176 access data tic benchmarking problem related nasa system speciﬁc problem domain outside vision speech recognition language also benchmark datasets available may le zhang et al 177 use datasets keel also use dataset manufacturing drilling machine order obtain practical evaluation chen li 19 argue big data continues play vital role area predictive analytics new way thinking novel algorithmic approach needed due difﬁculty deﬁning big data benchmark datasets vi conclusion comprehensive survey neuroevolution approach dnns key aspect ea dl ha presented important issue challenge discussed area reader ing na method dl community reader encouraged use ea approach dnn work conﬁguration dnns not trivial problem often becoming tedious process ea competitive matic creation conﬁguration network situation poorly incorrectly conﬁgured network lead failure underutilization dnns acknowledgment author would like thank anonymous reviewer providing insightful comment article additional valuable comment provided many public mailing list including uai connectionists numerous nn dl neuroevolution expert provided additional valuable comment reference 1 akimoto shirakawa yoshinari uchida saito nishida adaptive stochastic natural gradient method ral architecture search proc int conf mach 2019 pp 2 ashfahani pratama lughofer ong devdan deep evolving denoising autoencoder neurocomputing vol 390 pp authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 490 ieee transaction artificial intelligence vol 2 no 6 december 2021 3 assunção lourenço machado ribeiro evolving topology large scale deep neural network castelli sekanina zhang cagnoni ed berlin germany springer 2018 pp 4 assunção lourenço machado ribeiro denser deep evolutionary network structured representation genet program evolvable vol 20 no 1 pp 2019 5 awad mallik hutter differential evolution neural architecture search proc workshop neural architecture search int conf learn representation 2020 6 bäck evolutionary algorithm theory practice evolution strategy evolutionary programming genetic algorithm oxford oxford univ press 1996 7 baker gupta naik raskar designing neural network architecture using reinforcement learning proc int conf learn representation conference track proceeding toulon france apr 2017 8 baldi autoencoders unsupervised learning deep architecture proc workshop unsupervised transfer learning 2012 pp 9 baldominos saez isasi automated evolutionary design neural network past present future neural comput vol 32 no 2 pp 2020 10 bellemare naddaf veness bowling arcade learning environment evaluation platform general agent proc int joint conf artif buenos aire argentina jul 2015 pp 11 bergstra bengio random search mization mach learn vol 13 pp 2012 12 bergstra yamins cox making science model search hyperparameter optimization hundred dimension vision architecture proc int conf int conf mach 2013 pp 13 beyer schwefel evolution strategy comprehensive duction nat comput int vol 1 no 1 pp 2002 14 brock lim ritchie weston smash model architecture search hypernetworks proc int conf learn representation conf track vancouver bc canada apr 30 may 3 2018 15 brockman et openai gym corr vol 2016 online available 16 cai chen zhang yu wang efﬁcient architecture searchbynetworktransformation innovative appl artiﬁcial aaai symp educational adv artiﬁcial 2018 pp 17 charte rivera martnez deljesus evoaaa evolutionary methodology automated neural autoencoder architecture search integr vol 27 pp 2020 18 chen liu wu jiang chen image classiﬁcation stacked restricted boltzmann machine evolutionary function array classiﬁcation voter proc ieee congr evol 2016 pp 19 chen lin big data deep learning challenge tives ieee access vol 2 pp 1 2014 20 choromanska henaff mathieu arous lecun loss surface multilayer network proc int conf artif intell 2015 pp 21 coello evolutionary optimization historical view ﬁeld ieee comput intell vol 1 no 1 pp 2006 22 cui zhang tüske picheny evolutionary stochastic gradient descent optimization deep neural work adv neural inf process vol 31 bengio wallach larochelle grauman garnett ed red hook ny usa curran associate 2018 pp 23 darwish hassanien da survey swarm evolutionary computing approach deep learning artif intell vol 53 no 3 pp 2019 24 david greental genetic algorithm evolving deep neural network proc companion publication annu conf genet evol 2014 pp 25 deb optimization using evolutionary algorithm new york ny usa wiley 2001 26 deb pratap agarwal meyarivan fast elitist multiobjective genetic algorithm ieee trans evol vol 6 no 2 pp apr 2002 27 deng dong socher li li imagenet hierarchical image database proc ieee conf comput vi pattern 2009 pp 28 desell large scale evolution convolutional neural network ing volunteer computing proc genet evol comput 2017 pp 29 desjardins bengio empirical evaluation tional rbms vision département informatique et de recherche opérationnelle université de montréal montreal qc canada tech 1327 2008 30 dong yang extending scope producible neural architecture search proc int conf learn representation addis ababa ethiopia apr 2020 31 santos gatti deep convolutional neural work sentiment analysis short text proc int conf comput linguistics tech paper dublin ireland 2014 pp 32 dufourq bassett eden evolutionary deep network efﬁcient machine learning proc pattern recognit assoc south afr robot mechatronics 2017 pp 33 eiben smith evolutionary computation tion thing nature vol 521 pp may 28 2015 34 eiben smith introduction evolutionary computing berlin germany 2003 35 elsaid karnas lyu krutz ororbia desell evolutionary transfer learning structural adaptation proc int conf appl evolutionary 2020 pp 36 elsaid karns lyu krutz ororbia desell ing neuroevolutionary transfer learning deep recurrent neural network adaptation proc genet evol comput 2020 pp 37 elsken metzen hutter simple efﬁcient architecture search convolutional neural network proc int conf learn representation vancouver bc canada apr 30 3 2018 2017 38 elsken metzen hutter efﬁcient neural architecture search via lamarckian evolution proc int conf learn representation new orleans la usa may 2019 39 elsken metzen hutter neural architecture search cham switzerland springer 2019 pp 40 fernando et convolution evolution differentiable tern producing network proc genet evol comput 2016 pp 41 fischer igel introduction restricted boltzmann chine progress pattern recognition image analysis computer vision application alvarez et ed berlin germany springer 2012 pp 42 floreano dürr mattiussi neuroevolution tures learning evol vol 1 no 1 pp 2008 43 fogel owen walsh artiﬁcial intelligence simulated evolution chichester wiley 1966 44 foster evolutionary computation nat rev vol 2 pp 2001 45 galván neuroevolution deep learning role neutrality corr vol 2021 online available 46 analysis effect neutrality problem hardness evolutionary algorithm thesis dept sch comput sci elect univ essex united kingdo colchester 2009 47 dignum poli effect constant neutrality performance problem hardness gp proc eur conf genet 2008 pp 48 poli empirical investigation neutrality affect evolutionary search proc annu conf genet evol 2006 pp 49 poli some step towards understanding neutrality affect evolutionary search proc int conf parallel problem solving springer 2006 pp 50 poli empirical investigation degree neutrality affect gp search proc mexican int conf artif intell adv artif 2009 pp 51 poli kattan neill brabazon neutrality evolutionary algorithm know evolving vol 2 no 3 pp 2011 52 gers learning forget continual prediction lstm proc iet 1999 pp authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply galván mooney neuroevolution deep neural network current trend future challenge 491 53 gers schmidhuber lstm recurrent network learn simple language ieee trans neural vol 12 no 6 pp 2001 54 goerick rodemann evolution strategy alternative gradient based learning proc int conf eng appl neural 1996 pp 55 goldberg genetic algorithm search optimization machine learning reading usa 1989 56 gonçalves seca castelli exploration tic learning machine neuroevolution algorithm dynamic training data use ensemble construction method deep learning spectives cham switzerland springer international publishing 2020 pp 57 goodfellow bengio courville deep learning cambridge usa mit press 2016 58 graf mohamed hinton speech recognition deep recurrent neural network proc ieee int conf speech signal 2013 pp 59 greff srivastava koutnk steunebrink ber lstm search space odyssey ieee trans neural netw learn vol 28 no 10 pp 2017 60 hajewski oliveira xing distributed evolution deep autoencoders corr vol 2020 online available 61 hansen mller koumoutsakos reducing time plexity derandomized evolution strategy covariance matrix adaptation evol vol 11 no 1 pp 2003 62 hansen ostermeier adapting arbitrary normal mutation distribution evolution strategy covariance matrix adaptation proc ieee int conf evol 1996 pp 63 hansen ostermeier completely derandomized adaptation evolution strategy evol vol 9 no 2 pp jun 2001 64 zhang ren sun deep residual learning image recognition proc ieee conf comput vi pattern 2016 pp 65 hinton osindero teh fast learning algorithm deep belief net neural vol 18 no 7 pp jul 2006 66 holland adaptation natural artiﬁcial system ductory analysis application biology control artiﬁcial intelligence cambridge usa mit press 1992 67 hong yu wan tao wang multimodal deep autoencoder human pose recovery ieee trans image vol 24 no 12 pp 2015 68 huang liu van der maaten weinberger densely connected convolutional network proc ieee conf comput vi pattern 2017 pp 69 huang et gpipe efﬁcient training giant neural network using pipeline parallelism proc adv neural inf process vol 32 pp 2019 70 hubel wiesel receptive ﬁelds binocular interaction functional architecture cat visual cortex j vol 160 no 1 pp 1962 71 ioffe szegedy batch normalization accelerating deep work training reducing internal covariate shift proc int conf mach 2015 pp 72 jarrett kavukcuoglu ranzato lecun best architecture object recognition proc ieee int conf comput 2009 pp 73 jin ge netrapalli kakade jordan escape saddle point efﬁciently proc int conf mach precup teh proc mach learn vol 70 2017 pp 74 jin evolutionary computation recent advance future challenge swarm evol vol 1 no 2 pp 2011 75 józefowicz zaremba sutskever empirical exploration recurrent network architecture proc int conf mach 2015 pp 76 kandasamy neiswanger schneider póczos xing neural architecture search bayesian optimisation optimal transport proc adv neural inf process syst 31 annu conf neural inf process 2018 pp 77 kawaguchi huang gradient descent ﬁnds global minimum generalizabledeepneuralnetworksofpracticalsizes allerton conf control 2019 pp 78 khadka chung tumer neuroevolution modular neural network deep memory problem evol vol 27 no 4 pp 2019 79 khadka tumer policy gradient ment learning proc int conf neural inf process 2018 pp 80 khan sohail zahoora qureshi survey recent architecture deep convolutional neural network artif intell vol 53 no 8 pp 2020 81 kim reddy yun seo nemo multiobjective optimization deep neural network speed accuracy icml 2017 automl workshop 2017 pp 82 kimura evolutionary rate molecular level nature vol 217 pp 1968 83 kimura neutral theory molecular evolution cambridge cambridge univ press 1983 84 kitano designing neural network using genetic algorithm graph generation system complex vol 4 pp 1990 85 kleinberg li yuan alternative view doe sgd escape local minimum int conf mach 2018 pp 86 koza keane streeter mydlowec yu lanza genetic programming iv routine machine gence vol berlin germany springer 2006 87 koza genetic programming programming computer mean natural selection cambridge usa mit press 1992 88 koza result produced genetic ming genet program evolvable vol 11 no pp 2010 89 krizhevsky nair hinton canadian institute advanced research 2021 accessed online available 90 krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural network commun acm vol 60 no 6 pp may 2017 91 autoencoders deep learning network proc ieee annu comput softw appl 2015 pp 92 larochelle erhan courville bergstra bengio empirical evaluation deep architecture problem many factor variation proc int conf mach 2007 pp 93 larochelle mandel pascanu bengio learning gorithms classiﬁcation restricted boltzmann machine mach learn vol 13 no 22 pp 2012 94 lecun bengio hinton deep learning nature vol 521 no 7553 pp 2015 95 lecun bottou bengio haffner learning applied document recognition proc ieee 1998 pp 96 lecun bottou bengio haffner ing applied document recognition proc ieee vol 86 no 11 pp 1998 97 lehman stanley novelty search problem tives genet programm theory vol 11 pp 2011 98 lehman stanley abandoning objective evolution search novelty alone evol vol 19 no 2 pp jun 2011 99 lillicrap cownden tweed akerman random feedback weight support learning deep neural network nature vol 7 no 13276 2016 100 bestpracticesforscientiﬁcresearchonneural architecture search mach learn vol 21 no 243 pp 2020 101 liu et progressive neural architecture search proc eur conf comput vision 2018 102 liu simonyan vinyals fernando kavukcuoglu hierarchical representation efﬁcient architecture search proc int conf learn representation 2018 pp 103 liu simonyan yang dart differentiable architecture search 2018 104 liu gong miao wang li structure learning deep neural network based multiobjective optimization ieee trans neural netw learn vol 29 no 6 pp jun authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 492 ieee transaction artificial intelligence vol 2 no 6 december 2021 105 liu wang liu zeng liu alsaadi survey deep neural network architecture application puting vol 234 pp 2017 106 genetic algorithm proc genet evol comput 2019 pp 427 107 luo li wang liang discriminant autoencoder feature extraction fault diagnosis chemometrics intell lab vol 192 2019 art no 103814 108 principe taxonomy neural memory network ieee trans neural netw learn vol 31 no 6 pp jun 2020 109 mandischer comparison evolution strategy agation neural network training neurocomputing vol 42 no 1 pp 2002 110 martin naranjo camacho evodeep new evolutionary approach automatic deep neural network parametrisation parallel distrib vol 117 pp 2018 111 meier mujika gauy steger improving ent estimation evolutionary strategy past descent direction corr vol 2019 online available 112 miikkulainen et evolving deep neural network corr vol 2017 online available 00548 113 miller cartesian genetic programming berlin germany springer 2011 pp 114 mnih et playing atari deep reinforcement learning corr vol 2013 online available 5602 115 mohamed sainath dahl ramabhadran hinton picheny deep belief network using discriminative feature phone recognition proc ieee int conf speech signal 2011 pp 116 montana davis training feedforward neural network using genetic algorithm proc int joint conf artif 1989 pp 117 morse stanley simple evolutionary optimization rival stochastic gradient descent neural network proc genet evol comput 2016 pp 118 nair hinton rectiﬁed linear unit improve restricted boltzmann machine proc int conf int conf mach 2010 pp 119 negrinho gordon deeparchitect automatically designing training deep architecture corr vol 2017 line available 120 netzer wang coates bissacco wu ng reading digit natural image unsupervised feature learning proc conf neural inf process 2011 online able 121 ororbia elsaid desell investigating recurrent neural network memory structure using proc genet evol comput 2019 pp 122 pawełczyk kawulok nalepa deep neural network proc genet evol comput conf companion 2018 pp 123 peng liu liu wang effective long memory differential evolution algorithm electricity price prediction energy vol 162 pp 2018 124 poli effect neutrality ﬁtness distance correlation phenotypic mutation rate problem hardness foundation genetic algorithm stephen et ed berlin germany springer 2007 pp 125 poli effect constant trality problem hardness ﬁtness distance correlation phenotypic mutation rate ieee trans evol vol 16 no 2 pp apr 2012 126 porto fogel fogel alternative neural network training method ieee expert intell syst vol 10 no 3 pp jun 1995 127 price storn lampinen differential practical approach global optimization berlin germany springer 2005 128 rahnamayan tizhoosh salama based differential evolution ieee trans evol vol 12 no 1 pp 2008 129 rajbhandari ruwase carbin chilimbi mizing cnns multicores scalability performance goodput proc int conf architectural support program lang oper 2017 pp 130 real aggarwal huang le regularized evolution image classiﬁer architecture search proc aaai conf artif aaai innovative appl artif intell iaai aaai symp edu adv artif 2019 pp 131 real et evolution image classiﬁers proc int conf mach 2017 pp 132 rechenberg evolutionsstrategien simulationsmethoden der medizin und biologie schneider ranft ed berlin germany springer 1978 pp 133 rechenberg evolution strategy nature way optimization optimization method application possibility limitation bergmann ed berlin germany springer 1989 pp 134 rumelhart hinton williams learning internal representation error propagation cambridge usa mit press 1986 pp 135 ryan collins neill grammatical evolution evolving gram arbitrary language genetic programming banzhaf poli schoenauer fogarty berlin germany springer 1998 pp 136 salakhutdinov hinton deep boltzmann machine proc int conf artif intell proc mach learn apr 2009 pp 137 salimans ho chen sidor sutskever evolution strategy scalable alternative reinforcement learning 2017 138 schmidhuber deep learning neural network overview neural vol 61 pp 2015 139 schwefel numerical optimization computer model hoboken nj usa wiley 1981 140 sehnke osendorfer rckstie graf peter huber policy gradient neural vol 23 no 4 pp 2010 141 shahab grot evolutionary distributed sgd proc genetics evol comput conf companion 2020 pp 142 shallue lee antognini frostig dahl measuring effect data parallelism neural network training mach learn vol 20 pp 2019 143 shinozaki watanabe structure discovery deep neural network based evolutionary algorithm proc ieee int conf speech signal 2015 pp 144 shirakawa iwata akimoto dynamic optimization neural network structure using probabilistic modeling proc aaai conf artif vol 32 no 1 2018 145 silver et mastering game go deep neural network tree search nature vol 529 no 7587 pp 2016 146 simonyan zisserman deep convolutional network image recognition proc int conf learn tations san diego ca usa may 2015 147 stallkamp schlipsing salmen igel man computer benchmarking machine learning algorithm trafﬁc sign recognition neural vol 32 pp 2012 2011 148 stanley clune lehman miikkulainen designing neural network neuroevolution nat mach vol 1 pp 2019 149 stanley miikkulainen evolving neural network augmenting topology evol vol 10 no 2 pp jun 2002 150 madhavan conti lehman stanley clune deep neuroevolution genetic algorithm competitive alternative training deep neural network reinforcement learning corr vol 2017 online available 151 suganuma kobayashi shirakawa nagao evolution deep convolutional neural network using cartesian genetic ming evol vol 28 no 1 pp authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply galván mooney neuroevolution deep neural network current trend future challenge 493 152 suganuma ozay okatani exploiting potential standard convolutional autoencoders image restoration ary search proc int conf mach stockholmsmässan stockholm sweden vol 80 jul 2018 pp 153 suganuma shirakawa nagao genetic programming approach designing convolutional neural network architecture proc genet evol comput 2017 pp 154 sun wang xue jin yen zhang assisted evolutionary deep learning using random based performance predictor ieee trans evol vol 24 no 2 pp apr 2020 155 sun xue zhang yen completely automated cnn architecture design based block ieee trans neural netw learn vol 31 no 4 pp apr 2020 156 sun xue zhang yen evolving deep convolutional neural network image classiﬁcation ieee trans evol vol 24 no 2 pp apr 2020 157 sun yen yi evolving unsupervised deep neural network learning meaningful representation ieee trans evol computation vol 23 no 1 pp 2019 158 sutton barto reinforcement learning introduction bradford book cambridge usa mit press 2018 159 szegedy et going deeper convolution proc ieee conf comput vi pattern 2015 pp 160 talbi taxonomy hybrid metaheuristics heuristic vol 8 no 5 pp 2002 161 tapia estevez information plane autoencoders proc int joint conf neural jul 2020 162 tishby zaslavsky deep learning information tleneck principle ieee inf theory workshop jerusalem israel apr 26 may 1 2015 pp doi 163 van wyk bosman evolutionary neural architecture search image restoration proc int joint conf neural 2019 pp 164 wang sun xue zhang hybrid differential evolution approach designing deep convolutional neural network image classiﬁcation proc adv artif intell australasian joint lecture note comput springer vol 11320 2018 pp 165 wright role mutation inbreeding crossbreeding selection evolution proc int congr 1932 pp 166 xiao rasul vollgraf novel image dataset benchmarking machine learning algorithm corr vol 2017 online available 07747 167 xie yuille genetic cnn proc ieee int conf comput 2017 pp 168 xie zheng liu lin snas stochastic neural ture search proc int conf learn representation new orleans la usa may 2019 169 yang et car continuous evolution efﬁcient neural chitecture search proc conf comput vision pattern recognition 2020 pp 170 yao evolving artiﬁcial neural network proc ieee vol 87 no 9 pp 1999 171 ying klein real christiansen murphy hutter towards reproducible neural architecture search int conf mach 2019 pp 172 ying chen eksombatchai hamilton leskovec graph convolutional neural network mender system proc acm sigkdd int conf knowl discov data mining 2018 pp 173 yu principe understanding autoencoders information theoretic concept neural network elsevier vol 117 pp 2019 174 zagoruyko komodakis wide residual network proc brit mach vi wilson hancock smith new york uk bmva press 2016 175 zela siems hutter benchmarking dissecting neural architecture search proc int conf learn representation addis ababa ethiopia apr 2020 176 zhang lim qin tan multiobjective deep belief network ensemble remaining useful life estimation prognostic ieee trans neural netw learn vol 28 no 10 pp 2017 177 zhang tan li hong deep belief network imbalanced classiﬁcation ieee trans neural netw learn vol 30 no 1 pp 2019 178 zhao zhang lu direct evolutionary feature extraction algorithm classifying high dimensional data proc nat conf artif 2006 pp 179 zhong yan liu practical network block design corr vol 2017 online available http 180 zhu et benchmarking analyzing deep neural network training proc ieee int symp workload characterization 2018 pp 181 zoph le neural architecture search reinforcement proc int conf learn representation conf track toulon france apr 2017 online available http 182 zoph vasudevan shlens le learning transferable architecture scalable image recognition ieee conf comput vision pattern recognition salt lake city ut usa ieee comput jun 2018 pp authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply