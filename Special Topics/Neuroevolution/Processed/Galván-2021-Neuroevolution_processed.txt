ieee transact artifici intellig vol decemb neuroevolut deep neural network current trend futur challeng edgar galván peter mooney varieti method appli tectur conﬁgur learn train artiﬁci deep neural network dnn method play crucial role success failur dnn problem applic evolutionari algorithm ea gain momentum putat feasibl method autom optim dnn neuroevolut term describ process autom conﬁgur train dnn use ea mani work exist literatur comprehens survey rentli exist focus exclus strength limit use neuroevolut approach dnn absenc survey lead disjoint fragment ﬁeld prevent dnn research potentiallyadoptingneuroevolutionarymethod research result lost opportun wider applic within deep learn problem thi articl present comprehens survey discuss evalu use ea architectur conﬁgur train dnn thi articl highlight pertin current issu challeng neuroevolut identiﬁ multipl promis futur research direct impact concept deep learn origin studi artiﬁci neural network ann ann achiev extraordinari result varieti divers applic area numer method appli architectur ration learn train artiﬁci dnn method lem applic recent ea gain momentum comput feasibl method call neuroevolut autom conﬁgur learn train dnn thi articl review recent scientiﬁc paper describ major ea paradigm appli research conﬁgur optim multipl dnn articul clear understand context feasibl neuroevolut research ai ea dnn beneﬁt thi articl impact thi articl come ing toward enhanc research capac knowledg skill research current work neuroevolut activ engag consid becom involv thi area index learn dl deep neural network dnn evolutionari algorithm ea machin learn neuroevolut manuscript receiv juli revis decemb januari accept march date public march date current version decemb thi work wa support depart comput scienc mu thi articl wa recommend public associ editor sanaz mostaghim upon evalu review comment correspond author edgar galván author natur inspir comput research group partment comput scienc maynooth univers maynooth ireland color version one ﬁgure thi articl avail http digit object identiﬁ introduct eep learn dl algorithm subset machin learn algorithm inspir deep erarch structur human percept well product system algorithm achiev extraordinari result divers area includ comput vision speech nition board game video game mention design deep neural network dnn architectur along optim eter train play crucial part success failur demonstr larg number scientiﬁc work publish recent year work classiﬁ one low two broad categori method sometim refer neuroevolut mentlearn rl method two categori also propos special atur includ mont carlo base simul random search random search weight predict climb grid search bayesian optim mutual inform rl method start gain momentum thank impress result recent ea method began yield impress result automat ﬁgurat dnn architectur ha report neuroevolut requir less comput time compar rl method basic dnn feedforward artiﬁci neural network ann mani hidden layer layer constitut nonlinear inform process unit usual two hidden layer ann signiﬁ dnn ad layer unit within layer dnn repres function increas complex evolutionari algorithm ea also known lutionari comput system stochast techniqu mimic basic principl life automat algorithm veri popular proven competit face challeng problem featur iti multipl local optima nonlinear interact variabl ea also proven yield competit sult mani problem ai approach even compar well result achiev human pert find architectur often veri tediou process inde lindauer ieee person use permit requir ieee permiss see http inform author licens use limit queen mari univers london download march utc ieee xplore restrict appli galván mooney neuroevolut deep neural network current trend futur challeng hutter remark work publish area neural architectur search na almost third studi correspond neuroevolut dnn wit increas number public date write consid period present see fig supplementari materi along detail systemat literatur search focu exclus architectur search od dnn well approach train dnn particularli thi work consid landmark ea genet algorithm ga evolut strategi ess genet program gp well recent ea variant differenti evolut de neuroevolut augment topolog neat grammat evolut ge furthermor consid main dl architectur classiﬁ liu et al use neuroevolut includ autoencod ae convolut neural network cnn deep belief network dbn restrict boltzmann machin rbm dl architectur ere thi studi includ recurr neural network rnn long memori lstm previou atur review area includ conduct floreano et al yao recent review provid stanley et al darwish et al baldomino et al former work explain inﬂuenc modern comput power scale allow grand ambit neuroevolut dl mani year ago achiev fulﬁll darwish et al deliv broader introduct overview swarm intellig ea optim hyperparamet architectur neural network data analyt baldomino et al work discussesafeweasmethodsemployedindnn withparticular emphasi cnn contrast work thi articl provid new contribut concentr conﬁgur design neuroevolut approach dl consid ea approach appli dl particular speciﬁc conﬁgur thi purpos thi articl deliv estim work neuroevolut dnn rest thi articl organ follow section ii provid background dl ea section iii discuss architectur dnn evolv efﬁcient use ea sectionivdiscussestrainingofdnnswitheaswhilesectionv set major challeng fertil avenu futur work final section vi conclud thi articl ii background deep neural network deep learn dl emerg work hinton et al studi ann becom veri activ research area ann consist multipl simpl connect unit neuron produc sequenc vation process train ann may requir long casual chain comput stage dl algorithm class machin learn algorithm use multipl layer progress extract higher level featur raw data input deep refer number transform layer raw data dl subsequ level attempt learn order transform input data progress abstract composit represent follow section summaris neuroevolut dnn ha appli develop wide rang ann includ limit convolut neural network autoencod deep belief network recurr neural network taxonomi architectur found cnn memori network deep learn architectur convolut neural work cnn cnn shown impress perform process data topolog deep network sist set layer contain one plane unit plane receiv input neighborhood plane previou layer thi idea connect unit recept ﬁeld date back perceptron anim visual cortex organ discov hubel wiesel input imag convolv trainabl kernel ﬁlter offset produc featur map ﬁlter includ layer connect weight usual four pixel featur map form group thi pass function sigmoid function hyperbol tangent function pixel produc addit featur map layer n plane normal use layer n featur detect layer call convolut layer onc featur detect exact locat less import convolut layer follow anoth layer charg perform local averag subsampl oper due high dimension nn input weight cnn classiﬁ may caus overﬁt thi problem address use pool process also call subsampl reduc overal size signal normal cnn train usual backpropag cedur propos lecun et al learn process cnn determin follow three key element spars interact reduc comput process withkernelsthataresmallerthantheinput parametershar referstolearningonesetofparametersinsteadoflearningoneset locat ﬁnalli equivari represent mean whenev input chang output chang manner cnn ﬁrst success dl ture appli face detect handwrit recognit imag classiﬁc speech recognit natur languag process recommend system earli evolut cnn architectur ha slow remark lenet propos late alexnet propos decad later veri similar two ﬁve convolut layer respect moreov also use kernel larg recept ﬁeld layer close input smaller ﬁlter closer output major differ latter use rectiﬁ linear unit activ function becam standard design cnn sinc alexnet use novel deeper model took simonyan zisserman imagenet challeng propos model known network author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact artifici intellig vol decemb propos onli deeper use plex build block szegedi et al propos googlenet also known incept network use incept block also residu network resnet architectur propos et al imagenet challeng moreov multipl cnn variant propos combin convolut ae rbm descript variant thi network found deep learn architectur autoencod ae ae simpl learn circuit design transform input output minimum amount distort ae consist combin encod decod function tation decod function convert new tation back origin form ae attempt preserv mation provid output make suitabl data preprocess iter architectur dnn despit thi work appear author suggest still rel littl work explor plicat ofeastoneuralarchitecturesearch baldi argu ae taken center stage deep architectur approach wa still veri littl theoret understand ae deep architectur date interest theoret work start ﬁlling thi import gap studi use mutual inform choos appropri ae architectur order process speciﬁc dataset mean ae capabl learn optim represent data encod ae within represent mean approach could broad enough consid ae variat unsupervis featur learn approach ae attempt learn compact resent input data whilst retain import inform represent thi represent pect complet reconstruct origin input thi make initi ae critic whilst ae induc veri help use represent input data onli capabl handl singl sampl capabl ele relationship pair sampl input data deep learn architectur deep belief network dbn dbn gener model implement number way includ rbm see section ae see section dbn suit problem featur extract drawn tremend attent centli dbn tradit classiﬁ veri larg number paramet requir great deal train time rbm stack togeth consid dbn fundament build block dbn rbm consist one visibl layer one hidden layer dbn appli classiﬁc problem featur vector data sampl use set valu state visibl variabl lower layer dbn dbn train gener probabl distribut possibl label input data offer good solut learn hierarch featur represent data dl architectur network type introduc network architectur name recurr ral network rnn restrict boltzmann machin rbm long short term memori lstm rnn cnn input vector eventu produc vector output number layer cnn determin amount comput step requir rnn ﬂexibl allow oper across sequenc vector connect unit network form direct cycl thi creat intern state network allow us exhibit dynam tempor behavior thi intern hidden state allow rnn store inform past efﬁcient rnn well suit sequenti data predict thi ha seen appli area statist languag model predict howev comput power rnn make veri difﬁcult train main reason thi difﬁculti due explod vanish gradient problem although vanish gradient ha address lstm gate rnn theori rnn make use inform arbitrarili long sequenc realist limit consid onli step rbm restrict boltzmann machin rbm work symmetr connect unit design make stochast decis whether neural network rbm connect hidden unit ple hidden layer learn occur consid hidden activ singl rbm data train higher level rbm commun connect layer thi restrict introduc boltzmann machin rbm probabilist model use layer hidden binari variabl unit model distribut visibl layer variabl rbm success appli problem involv high dimension data imag text outlin fischer igel rbm subject recent research propos build block multilay learn architectur dbn concept hidden neuron extract relev featur data observ featur serv input anoth rbm thi stack rbm allow network learn featur featur goal arriv represent lstm lstm cial type rnn capabl learn depend work incred well larg varieti problem current wide use basic unit within hidden layer lstm network call memori block contain one memori cell pair adapt multipl gate unit gate input output cell block lstm network possibl circumv problem vanish error gradient network train process method error back propag lstm network usual control recurr gate error propag back time potenti unlimit number virtual layer thi way learn take place lstm preserv memori thousand even million author licens use limit queen mari univers london download march utc ieee xplore restrict appli galván mooney neuroevolut deep neural network current trend futur challeng time interv past network topolog lstm develop accord speciﬁc problem recurr neural network rnn long ori lstm emerg effect scalabl model sever learn problem relat sequenti data ger schmidhub show standard rnn fail learn presenc time lag exceed ﬁve ten step relev input event target signal lstm affect thi problem capabl deal minim time lag excess step lstm clearli outperform previou rnn onli regular languag benchmark accord previou research also languag benchmark evolutionari algorithm evolutionari algorithm ea also known lutionari comput system refer set stochast mizat bioinspir algorithm use evolutionari principl build robust adapt system ﬁeld ha origin four landmark evolutionari method genet algorithm evolut strategi evolutionari program genet program key element algorithm undoubtedli ﬂexibl allow practition use element two differ ea techniqu consequ boundari approach longer distinct allow holist ea framework emerg ea work popul tation potenti solut particular problem potenti solut commonli known individu repres point search space optim solut lie popul evolv mean genet oper number gener produc better result problem individu evalu use ﬁtness function determin good bad individu problem hand ﬁtness valu assign individu popul probabilist determin success individu propag part code futur gener evolutionari process carri use genet erat select crossov mutat key oper use ea select oper charg choos one individu popul base ﬁtness valu multipl select oper propos one popular select oper tournament select best individu select pool normal size popul stochast crossov also known recombin oper exchang materi normal two select individu thi oper charg exploit search space stochast mutat oper make random chang gene individu charg explor search space mutat oper import guarante divers popul well recov genet materi lost dure evolut thi evolutionari process repeat stop condit reach maximum number gener ha execut popul thi stage contain best evolv potenti solut problem may also fig nasnet search space b scalabl architectur imag classiﬁc consist two repeat motif term normal cell reduct cell left full outer structur omit skip input clariti right detail view skip input c exampl cell dot red circl demarc pairwis combin exampl mutat use b redrawn zoph et al real et al fig genet represent cgp individu encod cnn architectur b phenotyp represent c cnn architectur deﬁn gene colour black background genotyp express phenotyp summat node c light yellow background perform max pool lh input node get input tensor size redrawn suganuma et al repres global optim solut algorithm show typic step consid ea main ea employ dnn genet algorithm genet program evolut strategi differenti evolut grammat lution neuroevolut augment topolog fig supplementari materi show ga onli ea method use train dnn rest conﬁgur dnn taxonomi algorithm found evolutionari algorithm genet algorithm ga thi ea wa introduc holland highli author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact artifici intellig vol decemb algorithm common ea process network design adapt input refer dataset number gener number individu gener n mutat crossov probabl pm pc initialis gener set randomis individu n n comput recognit accuraci select produc new gener n n russian roulett process n n crossov pair perform crossov probabl pc mutat individu mt n n perform mutat probabl pm fit evalu comput ﬁtness recognit accuraci new individu mt n n end output set individu ﬁnal gener mt n n ﬁtness valu popularis goldberg subsequ achiev nari result well reach multipl research commun includ machin learn neural network ga quentlydescribedasfunctionoptim butnowthetendencyi consid ga search algorithm abl ﬁnd solut multipl form ga propos special literatur bitstr represent one predomin encod represent use ga crossov main genet oper mutat secondari oper reproduc offspr evolutionari search evolutionari algorithm genet program gp thi ea subclass ga popularis koza gp form autom program individu domli creat use function termin set requir solv given problem multipl type gp propos literatur typic structur predomin form gp ea cartesian gp cgp anoth form gp ha use neuroevolut dnn evolutionari algorithm evolut strategi es ea introduc rechenberg ess gener appli represent mizat problem es mutat main oper wherea crossov secondari option oper histor two basic form es known µ λ µ λ µ refer size parent popul wherea λ refer number offspr produc follow gener befor select appli former es offspr replac parent wherea latter form es select appli offspr parent form popul follow gener covari matrix propos hansen art adapt full covari matrix normal search distribut evolutionari algorithm evolutionari program ea propos fogel littl differ es ep differ lie lack use crossov ep wherea thi genet oper rare use es ep normal parent produc offspr wherea es number offspr produc genet oper higher parent ea multipl algorithm use dnn notabl differenti evolut de grammat evolut ge neuroevolut augment topolog neat de differenti evolut wa propos price storn de ha proven highli efﬁcient continu search space often report robust well achiev faster converg speed compar optim method perturb popul member scale differ randomli select distinct popul member ge grammat evolut ea propos ryan et al map process use gener genet program use binari string select product rule form grammar deﬁnit ge seen special form gp one main differ unlik gp ge doe perform evolutionari process program themselv neat neuroevolut augment topolog form ea propos stanley miikkulainen neat techniqu evolv neural network follow three element crucial neat work histor mark thatallowssolutionstobecrossedov speciationthatallow deﬁn nich start minim structur allow us increment ﬁnd better solut iii evolv dnn architectur ea motiv recent ea start gain momentum design dnn architectur popular algorithm due fact method offer parallel mechan simultan explor multipl area search space offer mechan escap local optima algorithm inher suit parallel mean potenti solut taneous comput within accept time steadi increas comput power includ graphic process unit contribut faster comput calcul ea critiqu despit popular ea design dnn ture also critic slow learner well comput expens evalu author licens use limit queen mari univers london download march utc ieee xplore restrict appli galván mooney neuroevolut deep neural network current trend futur challeng exampl use small ea ual potenti solut train set sampl one gener alon hundr thousand million gener requir one million evalu use ﬁtness function dl architectur cnn dufourq bassett use ga evolv cnn one convolut one max pool dropout among author report competit result compar algorithm emnist dataset well fashion dataset desel propos algorithm base neat evolv cnn architectur desel carri modiﬁc neat algorithm evolv cnn architectur select crossov mutat wherea oper play import role produc behav cnn mutat oper involv seven type oper seem crucial result report mnist dataset zoph et al propos nasnet search space deﬁn predetermin outer structur depict fig previou work resnet densenet thi outer structur compos convolut cell call normal cell colour pink fig reduct cell colour grey repeat mani time former type cell return featur map dimens wherea latter return featur map height width reduc factor two cell type constrain architectur architectur normal cell differ architectur reduc cell goal architectur search process wa discov architectur two type cell exampl thi shown fig b real et al propos regular evolut evolv imag classiﬁ achiev superior accuraci method ﬁrst time author use ea ﬁxed length member encod architectur cnn use nasnet search space goal wa discov architectur normal cell reduct cell depict fig real et al use modiﬁ version tournament select two type tation drive evolut tournament select see section ii wa modiﬁ newest genotyp chosen older genotyp mutat oper involv one two oper take place onc individu hidden state mutat op mutat execut ani type mutat ﬁrst random cell chosen pairwis combin stochast select see fig c ﬁnalli one two pair select randomli thi hidden state replac anoth hidden state constraint loop form op mutat differ onli modifi oper use within select hidden state fig show two mutat oper work author use dataset test propos evolut compar method random search achiev better accuraci result reduc putat time requir algorithm compar two method author also use ﬁttest chromosom found algorithm retrain use imagenet dataset xie et al propos genet cnn automat learn structur cnn limit number layer well limit size oper convolut ter xie et al adopt ga binari represent repres part evolv network network compos variou stage stage compos node repres convolut oper binari encod adopt xie et al repres connect number order node thi represent allow use crossov along roulett select mutat deﬁn stage minim unit appli crossov mutat even restrict author achiev competit accuraci result use mnist dataset also demonstr approach gener use learn architectur dataset thi wa achiev caus approach wa abl produc network alexnet vggnet network googlenet highway network deep resnet report beneﬁci appli comput vision problem real et al use ea automat optim cnn architectur individu architectur encod graph vertic repres tensor two resent spatial coordin imag third number channel activ function normal appli vertic eleven type mutat involv insert layer remov layer modifi layer paramet use real et al indic crossov improv result yield mutat oper andreportedcompetitiveaverageaccuracyresult ﬁve independ run dataset compar algorithm includ resnet densenet suganuma et al use cartesian gp ical design cnn architectur genotyp encod mation type connect node fig pict thi idea type includ convblock resblock max pool averag pool summat concaten vblock consist standard convolut process follow batch normal relu wherea resblock convblock follow tensor summat cgp encod scheme repres program direct acycl graph grid nr row nc column fig b provid exampl phenotyp obtain fig case grid deﬁn nr nc correspond cnn architectur depict fig c author use dataset evalu cgp individu expens adopt simpl es see section ii author approach achiev competit result compar method includ resnet author author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact artifici intellig vol decemb report encourag result use cgp automat ure cnn architectur regardless sampl size use work exampl cnn architectur produc cgp small dataset scenario wider compar architectur suganumaet al extend thi work propos rich initi earli termin former use resnet dens connect cnn densenet build cgp individu latter refer termin individu evalu accuraci poor refer curv built compar previou accuraci curv assunção et al propos denser hybrid anism ga dynam structur ge evolv dnn architectur outer layer propos approach charg encod macro structur dnn evolv mean ga dynam structur ge charg inner layer encod paramet dnn ing select two form crossov three type mutat add replic remov unit outer layer use multipl dataset clude mnist rectangl similarli work assunção et al perform onli ten epoch train dnn report competit result compar algorithm interestingli serv ﬁtness increas time number hidden layer decreas suggest two metric conﬂict optimis cnn architectur sun et al propos use ga encod evolv mean select crossov mutat unsupervis dnn learn meaning sentat comput vision task approach includ follow two main part ﬁnding optim architectur dnn desir initi weight activ function ii paramet valu nection weight desir initi ﬁrst wa primarili achiev use encod wa inspir work conduct zhao et al captur element describ one gene repres averag paramet thi encod exploit achiev crossov reduc overcom thi problem sun et al use backpropag part ii variou part approach author demonstr local search adopt part wa necessari order achiev promis result recent sun et al propos ga name ing deep cnn automat discov cnn architectur inspir real et al sun et al propos method evalu ﬁtness individu popul independ run also use select mutat crossov crossov wa use studi carri real et al itat real work sun et al use encod convolut pool full connect layer use standard deviat averag valu connect weight abl efﬁcient evalu chromosom classiﬁc error well number connect weight wa use evalu chromosom along normal cnn deep architectur author restrict train ten epoch last epoch ﬁtness comput chromosom author report highli encourag result mani case achiev better result compar algorithm benchmark dataset van wyk bosman describ neural ture search na method autom process ﬁnding optim cnn architectur arbitrari imag restor work demonstr feasibl perform na signiﬁc memori comput time constraint dataset wa chosen evalu author found conﬁgur architectur wa heavili overparameter thi wa case evolv nn perform task signiﬁcantli lower number total paramet sun et al propos encod strategi built block name resnet densenet use variabl length ga allow automat evolv cnn architectur unrestrict depth sun et al use select crossov mutat evolv candid solut employ repair mechan produc valid cnn author use dataset compar result nine manual design method four semiautomat method ﬁve automat method interestingli result outperform method well semiautomat method term classiﬁc error rate evolutionari multiobject optim emo explain section ha littl use automat conﬁgur dnn network well optim hyperparamet work latter includ recent approach propos kim et al author use speed accuraci two conﬂict object optim mean emo use nondomin sort geneticalgorithmii use three classiﬁc task includ use mnist drowsi behaviour recognit dataset spire kim et al studi lu et al use emo conﬂict object lu et al empir test multipl comput complex metric measur speed includ number activ node number activ connect node oper mention lu et al indic latter metric wa accur wa use second conﬂict tive optim moreov author use ingeni bitstr encod allow use homogen crossov mutat one chang mutat oper normal adopt ga author use dataset achiev petit result approach human expert conﬁgur wang et al explor abil differenti evolut de automat evolv architectur ter deep cnn method call decnn use de control evolut rate manag differenti valu decnn evolv architectur cnn author licens use limit queen mari univers london download march utc ieee xplore restrict appli galván mooney neuroevolut deep neural network current trend futur challeng encod strategi implement use gle ip address repres one layer dnn thi ip address push sequenc interfac correspond order layer dnn six mnist dataset use benchmark test decnn perform veri competit competitor six benchmark martín et al propos evodeep ea design ﬁnd best architectur optim essari paramet train dnn use machin model order determin possibl transit differ kind layer allow evodeep gener valid sequenc layer output one layer ﬁt input requir next layer test mnist dataset elsken et al propos multiobject optim lutionari algorithm moea name lemonad employ inherit mechan base mate network morphism mutat oper speed train dnn architectur better result report test error number paramet compar nasnet search space architectur yang et al propos continu evolut strategi util knowledg learn last evolut tion architectur search nondomin sort strategi adopt select sever excel architectur thi continu evolutionari architectur search car provid seri architectur model pareto front high efﬁcienc result indic car method give superior result benchmark dataset model shirakawa et al propos gener framework dynam optim network structur tion weight parametr distribut use gener network structur distribut paramet derstood network hyperparamet thi method shown comput efﬁcient static optim approach ﬂexibl convent tion approach methodolog appli select layer select activ function adapt stochast network ﬁnalli select connect densenet author conclud propos method capabl learn layer size appropri mix rate activ function within reason comput time optim weight architectur ann within singl train run consid possibl architectur subgraph supergraph call architectur search na na use akimoto et al develop gener optim framework base stochasticrelaxationfor architecturesearch thi frameworkcan handlepracticallyanytypeofarchitecturevariationprovided possibletodeﬁneaparametricfamilyofprobabilitydistribut upon use adapt mechan tic natur gradient ascent improv optim speed add robust hyperparamet tune experiment analysi indic adapt stochast natur gradient method na achiev signiﬁc speedup evolutionari convolut autoencod cae without promis perform awad et al use de na de ha shown achiev excel perform rang na benchmark found best approach appli de paramet discret categor maintain popul continu space perform canon de onli use discret copi individu order evalu de shown perform veri well four recent na method includ na baselin algorithm random search author conclud de ha good abil handl mix data type space dl architectur autoencod suganuma et al use cartesian gp adopt es techniqu use select mutat oper onli optim dn architectur imag restor author use convolut autoencod cae built upon convolut layer skip connect optim network conﬁn search space symmetr cae author achiev competit result method need use adversari train sophist loss function normal employ imag restor task luo et al propos novel semisupervis ae call discrimin ae applic fault diagnosi pose method ha differ train process loss function tradit ae case discrimin ae capabl extract better represent raw data provid complet differ loss function use represent extract discrimin ae gener bigger differ sampl class discrimin ae make full use label featur variabl obtain optim represent center group sampl separ much possibl ashfahani et al propos devdan deep evolv denois ae applic data stream analyt devdan demonstr propos denois ae variant tradit ae focus retriev origin input inform nois perturb devdan featur open structur capabl initi structur ning without presenc preconﬁgur network structur devdan ﬁnd competit network architectur compar method ten classiﬁc dataset dl architectur deep belief network dbn offer promis solut learn ful hierarch featur represent data provid chen et al use dbn automat extract featur imag propos evolutionari function array classiﬁ voter efacv classiﬁ featur imag extract dbn compos stack rbm es use train efacv mainli use binari classiﬁc problem multiclass classiﬁc problem necessari multipl efacv efacv show fast comput speed reduct overal train time experi perform mnist dataset liu et al describ structur learn dnn base multiobject tion propos multiobject optimis evolutionari algorithm moea dbn learn procedur use author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact artifici intellig vol decemb rbm train dn layer layer necessari remov unimport unnecessari connect dnn move toward discov optim dnn connect structur spars possibl without lost represent experi base mnist dataset differ train sampl indic moea approach effect zhang et al use dbn prognost health manag system aircraft aerospac design propos modbn multiobject dbn ensembl power multiobject ea base decomposit thi integr train dbn evolv multipl dbn simultan accuraci divers two conﬂict object dbn compos stack rbm train unsupervis manner modbn evalu compar promin diagnost benchmark problem nasa turbofan engin degrad problem propos approach structur paramet dbn strongli depend complex problem number train sampl avail approach work outstandingli well comparison exist approach zhang et al consid problem learn method thi idea assign misclassiﬁc cost class appropri author report veri studi dbn network drawn lot attent research recent anc class input data problem disproportion number class instanc thi affect qualiti appli learn algorithm zhang et al argu dbn veri well place handl type imbalanc data problem evolutionari deep belief network propos deal problem assign differenti misclassiﬁc cost data class evalu popular knowledg evolutionari learn keel benchmark dataset network lstm rrn rbm shinozaki watanab propos optim egi dnn structur paramet use ea dnn structur parameter direct acycl graph experi carri phonem recognit spoken digit detect conduct upon massiv parallel comput platform use comput graphic process unit rbm use train phase ororbia et al develop evolutionay explor ment model examm design devolv rnn use select memori structur rnn well suit task perform predict time seri data examm wa design select larg number memori cell structur thi allow evolutionari approach yield best perform rnn architectur peng et al use lstm nn capabl analyz time seri long time span order make predict tackl vanish gradient problem studi use de identifi hyperparamet lstm author claim thi ﬁrst time de ha use choos hyperparamet lstm forecast applic forecast involv complex continu nonlinear function de approach well suit type problem gonçalvez et al introduc semant learn machin slm thi shown outperform similar method wide rang supervis learn problem slm describ geometr semant hill climber approach nn follow λ strategi search best nn architectur conﬁgur thi allow slm concentr current best nn without draw ani penalti thi crucial aspect slm approach geometr semant mutat elsaid et al propos approach base call adapt structur transfer learn strategi goal improv train time deep rnn author use statist inform topolog sourc rnn topolog weight distribut report better perform rnn use half number genom compar nontransf method summari evolv dnn architectur use ea ea method differ represent use design dnn rang method includ ga gp es use hybrid combin exampl use ga ge ingeni represent interest proach achiev extraordinari result conﬁgur network commonplac approach case employ hundr comput use fewgpu focus design deep cnn ae rbm rnn lstm dbm also consid despit commend less research attent paper neuroevolut select paper way order ﬁnd select paper cinctli demonstr use neuroevolut dnn tabl order alphabet order surnam summaris ea represent use represent individu genet oper use ea paramet tabl outlin comput resourc use respond studi attempt outlin number gpu use calcul gpu day per run approxim sun et al indic benchmark dataset use experiment analysi final tabl indic neural network architectur ha evolv automat use semiautom approach whilst also indic target dnn architectur everi select paper doe report mation paper omit detail comput resourc omit inform number run veri interest output thi tabl numer differ approach use paper list crossov omit sever studi mostli due encod adopt variou research popul size select strategi ea chang studi mnist cifar clearli popular benchmark dataset see mani exampl studi use benchmark dataset speciﬁc applic domain author licens use limit queen mari univers london download march utc ieee xplore restrict appli galván mooney neuroevolut deep neural network current trend futur challeng tabl ea represent genet oper paramet v alu use neuroevolut design dnn architectur includ dataset use correspond comput effort gpu day automat matic refer architectur evolv automat use approach dash indic inform wa report iv train dnn ea motiv backpropag ha one success domin method use train ann past number decad thi simpl effect eleg method appli stochast gradient descent sgd weight ann goal minim overal error howev remark mors stanley wide held belief around wa backpropag would suffer loss gradient within dnn thi turn fals ha subsequ shown backpropag sgd effect optim dnn even million connect backpropag sgd beneﬁt avail sufﬁcient train data avail comput power problem space mani dimens success use sgd dnn still surpris practic speak sgd highli suscept local optima jin et al kleinberg li argu independ nois help sgn escap saddl point due random estim choromanska et al kawaguchi huang esis independ presenc multipl local optima problem veri similar best solut ea perform veri well presenc saddl point wa discuss section critiqu guarante converg solut comput use ea usual classiﬁ near optim ea effect approxim gradient thi estim individu popul correspond object hand sgd author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact artifici intellig vol decemb comput exact gradient subsequ research consid ea unsuit dl task thi reason howev ha demonstr exact approxim obtain sgd absolut critic overal success dnn use thi approach lillicrap et al demonstr break precis gradient calcul ha neg detriment effect learn mors stanley specul reason lack research focu use evolutionari comput dnn wa entir relat concern around gradient rather belief new approach dnn could actual emerg outsid sgd dl architectur convolut neural network et al propos method evolv weight convolut dnn use simpl ga popul chromosom ﬁxed length propos anism success evolv network four million free paramet key element studi conduct et al success evolv larg neural network includ follow use select mutat genet oper onli use novel method store larg paramet vector compactli repres initi seed plu list random seed produc seri mutat produc paramet vector use comput set includ one modern comput gpu cpu core well cpu core across dozen comput instead use optim techniqu mean ﬁtness function et al use novelti search reward new behavior individu author use rl benchmark problem includ atari hard maze humanoid locomot strate propos approach competit algorithm problem includ dqn method es pawelczyk et al focus encod cnn random weight use ga main goal wa let ea learn number gradient learn iter necessari achiev error use mnist dataset approach report best result around gradient learnt iter compar constant iter yield best overal result dl architectur autoencod david greental use ga ﬁxed length evolv weight valu ae dnn chromosom wa evalu use rm error train sampl author use onli ten individu weight individu updat use agat half popul randomli gener gener test approach dataset compar approach tradit ae use svm report better classiﬁc error dnn versu author indic reason whi method produc better result wa becaus gradient descent method backpropag highli suscept trap local optima ga method help prevent thi fernando et al introduc differenti sion composit pattern produc network call differenti pattern produc network dppn dppn approach attempt combin advantag result learn nn optim capabl evolutionari approach dppn ha demonstr rior result benchmark dataset mnist gener ea use optim algorithm dppn result indic dppn associ learn algorithm abil dramat reduc number paramet larger nn author argu thi integr evolutionari learn allow optim avoid becom stuck local optima point saddl point relev work mors stanley propos limit evalu lutionari algorithm leea use ga ﬁxed length represent evolv mean crossov mutat weight network inspir sgd leea comput error gradient singl instanc train set ﬁtness comput use small fraction train set leaa doe gener whole train sampl author propos follow two approach use small batch instanc ii use ﬁtness function consid perform current minibatch perform individu ancestor minibatch test task function approxim time seri predict task author clare leea competit approach even author use dnn small artiﬁci nn interest note thi use dnn set khadka tumer remark deep rl method notori sensit choic hyperparamat often brittl converg properti method also challeng long time horizon spars reward ea respond veri posit challeng use ﬁtness metric allow ea toler spars reward distribut endur long time horizon howev ea struggl perform well optim larg number paramet requir author introduc evolutionari rl erl algorithm ea use evolv divers experi train rl agent agent subject mutat crossov oper creat next gener agent thi erl describ guid guid bias explor ward state higher better return promot divers explor polici introduc redund stabil recurr neural network rnn see section porat memori nn store inform past within hidden state network kahdka et al introduc new network architectur call ular memori unit mmu thi mmu disconnect memori author licens use limit queen mari univers london download march utc ieee xplore restrict appli galván mooney neuroevolut deep neural network current trend futur challeng tabl ii ea represent oper paramet v alu neuroevolut dnn train dataset use comput effort gpu day indic inform wa report central comput oper without requir costli memori manag strategi neuroevolutionari method use train mmu architectur perform mmu approach gradient descent neuroevolut examin author ﬁnd neuroevolut repeat generaliz across task mmu nn design highli conﬁgur thi characterist exploit neuroevolutionari algorithm evolv work popul size set fraction elit set fulli differenti version mmu ﬁnd gradient descent perform better sequenc recal task neuroevolutionperformssigniﬁcantli better gradient descent sequenc classiﬁc task cui et al propos use ea sgd speed train ann interestingli author use multipl sgd optim certain hyperparamet learn schedul use build ea individu show differ task see tabl ii propos approach od speed train neural network meier et al propos gradient estim consid surrog gradient direct well random search direct author demonstr propos approach consider improv gradient estim capabl employ es particularli use mnist dataset lesser degre rl task shahab grot propos use ea along sgd train neural model main motiv due fact ea inher parallel suitabl distribut train setup compar minibatch sgd thi import becaus ha demonstr latter fail reduc number train iter beyond minibatch size summari train dnn use ea earli year neuroevolut wa thought method might exceed capabl propag ann gener dnn ular increasingli adopt use sgd backpropag idea use ea train dnn instead ha almost abandon dnn research commun ea uin differ paradigm specifi search problem provid excit opportun learn dnn compar neuroevolutionari approach approach gradient descent author khadka et al urg caution gener neuroevolut readili rabl gradient descent epoch despit fact ha argu ea compet search small problem well use nn nondifferenti activ function encourag result achiev inspir research carri research train dnn thi includ work conduct david greental fernando et al use deep ae pawelczyk et al et al use deep cnn tabl ii structur similar way tabl tabl select paper way order ﬁnd select paper succinctli demonstr use ea train dnn see mutat select use select work crossov omit certain situat see greater divers type benchmark dataset use greater focu dataset problem futur work neuroevolut dnn ea dnn ea success use automat design artiﬁci dnn describ throughout thi articl tipl algorithm propos recent year includ genet cnn evolut evolv deep cnn hierarch evolut despit success automat conﬁgur dnn architectur common limit method train time need thi rang day week order achiev competit result base ec use efﬁcient model also known surrog estim ﬁtness valu ea henc ec consider speed author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact artifici intellig vol decemb evolutionari search reduc number ﬁtness evalu time correctli estim ﬁtness valu potenti solut adopt thi ea limit research discuss thi articl dealt limit except sun et al demonstr use ensembl member success use correctli estim cnn accuraci thi consider reduc train time gpu day still achiev competit accuraci result compar limit unknown number train run necessari achiev good predict perform combin sgd ea discuss section iv backpropag one success method current use train ann backpropag normal involv applic sgd weight ann goal minim overal error befor thi era dl began perhap consid earli date time use ea wa common wa abandon thank impress result deep model train sgd discuss section iv ha small number work appear recent consid replac sgd ea inde begun observ trend combin techniqu yield better result time speed train process ann thi anoth excit promis area research mutat neutral theori seen numer studi use select tation onli drive evolut automat ﬁnding suitabl dnn architectur see section iii train dnn see section iv tabl ii present summari genet oper use variou research interestingli mani searcher report highli encourag result use two genet oper includ work conduct real et al use ga hundr gpu well work carri suganuma et al employ cartesian gp use onli gpu kimura neutral theori molecular evolut state major evolutionari chang molecular level result random ﬁxation select neutral tation mutat one gene anoth neutral doe affect phenotyp thu mutat take place natur evolut neither advantag disadvantag surviv individu reason extrapol thi evolut ha manag produc amaz complexityandadaptationsseeninnatur thenneutralityshould aid also ea howev whether neutral help hinder search ea answer gener one onli answer thi question within context speciﬁc class problem neutral represent set oper awar ani work neuroevolut dnn neutral thi work discuss venu research neutral dnn interest encod adopt research includ suganuma work see fig allow measur level neutral present evolutionari search indic whether presenc beneﬁci certain problem dnn neutral beneﬁci takingintoconsiderationspeciﬁcclassesofproblem represent genet oper thi also immedi posit impact train time need becaus evalu potenti ea candid solut necessari multiobject optim report research result one object variabl ha use nn train classiﬁc error cnn two object rare consid task becom much difﬁcult object may conﬂict mo concern simultan optim one object function function conﬂict set tradeoff solut among object sought singl global optimum exist optim tradeoff solut object improv without degrad one thi idea captur pareto domin relat solut x search space said anoth solut x least good object strictli better least one object thi import aspect emo becaus allow solut rank accord perform object respect solut popul emo one activ research area ea yet surpris see emo approach scarc use automat conﬁgur artiﬁci dnn architectur learn dnn often conﬁgur artiﬁci dnn requir simultan satisfi multipl object reduc tional calcul train dataset attain high accuraci emo offer eleg efﬁcient framework handl conﬂict object awar onli work area summaris section iii dnn fit landscap analysi genet oper work neuroevolut dnn use core genet oper includ select mutat crossov ha also use work use oper summaris tabl ii use crossov sometim difﬁcult adopt depend encod use variant propos studi adopt standard crossov oper discuss howev work area neuroevolut dnn focus attent explain whi adopt particular netic oper particular problem notion ﬁtness landscap ha us sever decad nonmathemat aid ha proven author licens use limit queen mari univers london download march utc ieee xplore restrict appli galván mooney neuroevolut deep neural network current trend futur challeng tabl iii common dataset use neuroevolut dnn veri power understand evolutionari search view search space deﬁn set potenti solut landscap heurist algorithm ea thought navig ﬁnd best solut essenti highest peak landscap height point thi search space repres abstract way ﬁtness solut associ point landscap therefor knowledg interfac problem ea thi help research practition deﬁn genet oper includ mutat crossov connect structur landscap standardis neuroevolut studi dnn section ii multipl dnn architectur pose special literatur mani research work review thi articl compar result yield neuroevolut algorithm howev unclear whi techniqu better becaus type oper use becaus represent adopt studi becaus type learn employ dure train due lack standardis studi neuroevolut dnn difﬁcult draw ﬁnal conclus help us identifi element promis dnn lack standardis studi mean indic ea paradigm associ genet oper prefer automat conﬁgur particular dnn architectur well train diversifi use benchmark problem dnn new larg dataset combin increasingli power comput resourc allow dnn solv hard ing imag classiﬁc certainli consid primari benchmark evalu dnn benchmark dataset mani includ tabl iii use compar putat result experiment setup produc differ research group believ success dnn provid opportun expand dnn domain howev success comprehens benchmark dataset requir without benchmark may difﬁcult make convinc argument support dnn problem beyond tradit domain imag tion machin translat new benchmark problem must allow appropri comparison perform test na algorithm step alreadi taken ampl ying et al propos provid search space test na algorithm without incur much comput cost wa propos zela et al adapt use search method dart proxyless na dong yang extend differ search space result multipl dataset critic benchmark dataset avail freeli stallkamp et al argu nich area trafﬁc sign recognit difﬁcult compar publish work becaus studi base differ data consid classiﬁc differ way use proprietari data case publicli avail make comparison result difﬁcult zhang et al access data tic benchmark problem relat nasa system speciﬁc problem domain outsid vision speech recognit languag also benchmark dataset avail may less zhang et al use dataset keel also use dataset manufactur drill machin order obtain practic evalu chen li argu big data continu play vital role area predict analyt new way think novel algorithm approach need due difﬁculti deﬁn big data benchmark dataset vi conclus comprehens survey neuroevolut approach dnn key aspect ea dl ha present import issu challeng discuss thi area reader ing na method dl commun reader encourag use ea approach dnn work conﬁgur dnn trivial problem often becom tediou process ea competit matic creation conﬁgur network situat poorli incorrectli conﬁgur network lead failur underutil dnn acknowledg author would like thank anonym review provid veri insight comment thi articl addit valuabl comment provid mani public mail list includ uai connectionist numer nn dl neuroevolut expert provid addit valuabl comment refer akimoto shirakawa yoshinari uchida saito nishida adapt stochast natur gradient method ral architectur search proc int conf mach pp ashfahani pratama lughof ong devdan deep evolv denois autoencod neurocomput vol pp author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact artifici intellig vol decemb assunção lourenço machado ribeiro evolv topolog larg scale deep neural network castelli sekanina zhang cagnoni ed berlin germani springer pp assunção lourenço machado ribeiro denser deep evolutionari network structur represent genet program evolv vol pp awad mallik hutter differenti evolut neural architectur search proc workshop neural architectur search int conf learn represent bäck evolutionari algorithm theori practic evolut strategi evolutionari program genet algorithm oxford oxford univ press baker gupta naik raskar design neural network architectur use reinforc learn proc int conf learn represent confer track proceed toulon franc apr baldi autoencod unsupervis learn deep architectur proc workshop unsupervis transfer learn pp baldomino saez isasi autom evolutionari design neural network past present futur neural comput vol pp bellemar naddaf veness bowl arcad learn environ evalu platform gener agent proc int joint conf artif bueno air argentina jul pp bergstra bengio random search mizat mach learn vol pp bergstra yamin cox make scienc model search hyperparamet optim hundr dimens vision architectur proc int conf int conf mach pp beyer schwefel evolut strategi comprehens duction nat comput int vol pp brock lim ritchi weston smash model architectur search hypernetwork proc int conf learn represent conf track vancouv bc canada apr may brockman et openai gym corr vol onlin avail http cai chen zhang yu wang efﬁcient architectur searchbynetworktransform innov appl artiﬁci aaai symp educ adv artiﬁci pp chart rivera martnez deljesu evoaaa evolutionari methodolog autom neural autoencod architectur search integr vol pp chen liu wu jiang chen imag classiﬁc stack restrict boltzmann machin evolutionari function array classiﬁc voter proc ieee congr evol pp chen lin big data deep learn challeng tive ieee access vol pp choromanska henaff mathieu arou lecun loss surfac multilay network proc int conf artif intel pp coello evolutionari optim histor view ﬁeld ieee comput intel vol pp cui zhang tüske picheni evolutionari stochast gradient descent optim deep neural work adv neural inf process vol bengio wallach larochel grauman garnett ed red hook ny usa curran associ pp darwish hassanien da survey swarm evolutionari comput approach deep learn artif intel vol pp david greental genet algorithm evolv deep neural network proc companion public annu conf genet evol pp deb optim use evolutionari algorithm new york ny usa wiley deb pratap agarw meyarivan fast elitist multiobject genet algorithm ieee tran evol vol pp apr deng dong socher li li imagenet hierarch imag databas proc ieee conf comput vi pattern pp desel larg scale evolut convolut neural network ing volunt comput proc genet evol comput pp desjardin bengio empir evalu tional rbm vision département informatiqu et de recherch opérationnel université de montréal montreal qc canada tech dong yang extend scope produc neural architectur search proc int conf learn represent addi ababa ethiopia apr santo gatti deep convolut neural work sentiment analysi short text proc int conf comput linguist tech paper dublin ireland pp dufourq bassett eden evolutionari deep network efﬁcient machin learn proc pattern recognit assoc south afr robot mechatron pp eiben smith evolutionari comput tion thing natur vol pp may eiben smith introduct evolutionari comput berlin germani elsaid karna lyu krutz ororbia desel evolutionari transfer learn structur adapt proc int conf appl evolutionari pp elsaid karn lyu krutz ororbia desel ing neuroevolutionari transfer learn deep recurr neural network adapt proc genet evol comput pp elsken metzen hutter simpl efﬁcient architectur search convolut neural network proc int conf learn represent vancouv bc canada apr elsken metzen hutter efﬁcient neural architectur search via lamarckian evolut proc int conf learn represent new orlean la usa may elsken metzen hutter neural architectur search cham switzerland springer pp fernando et convolut evolut differenti tern produc network proc genet evol comput pp fischer igel introduct restrict boltzmann chine progress pattern recognit imag analysi comput vision applic alvarez et ed berlin germani springer pp floreano dürr mattiussi neuroevolut ture learn evol vol pp fogel owen walsh artiﬁci intellig simul evolut chichest wiley foster evolutionari comput nat rev vol pp galván neuroevolut deep learn role neutral corr vol onlin avail http analysi effect neutral problem hard evolutionari algorithm thesi dept sch comput sci elect univ essex unit kingdo colchest dignum poli effect constant neutral perform problem hard gp proc eur conf genet pp poli empir investig whi neutral affect evolutionari search proc annu conf genet evol pp poli step toward understand neutral affect evolutionari search proc int conf parallel problem solv springer pp poli empir investig degre neutral affect gp search proc mexican int conf artif intel adv artif pp poli kattan neill brabazon neutral evolutionari algorithm know evolv vol pp ger learn forget continu predict lstm proc iet pp author licens use limit queen mari univers london download march utc ieee xplore restrict appli galván mooney neuroevolut deep neural network current trend futur challeng ger schmidhub lstm recurr network learn simpl languag ieee tran neural vol pp goerick rodemann evolut strategi altern gradient base learn proc int conf eng appl neural pp goldberg genet algorithm search optim machin learn read usa gonçalv seca castelli explor tic learn machin neuroevolut algorithm dynam train data use ensembl construct method deep learn spectiv cham switzerland springer intern publish pp goodfellow bengio courvil deep learn cambridg usa mit press grave moham hinton speech recognit deep recurr neural network proc ieee int conf speech signal pp greff srivastava koutnk steunebrink ber lstm search space odyssey ieee tran neural netw learn vol pp hajewski oliveira xing distribut evolut deep autoencod corr vol onlin avail http hansen mller koumoutsako reduc time plexiti derandom evolut strategi covari matrix adapt evol vol pp hansen ostermei adapt arbitrari normal mutat distribut evolut strategi covari matrix adapt proc ieee int conf evol pp hansen ostermei complet derandom adapt evolut strategi evol vol pp jun zhang ren sun deep residu learn imag recognit proc ieee conf comput vi pattern pp hinton osindero teh fast learn algorithm deep belief net neural vol pp jul holland adapt natur artiﬁci system ductori analysi applic biolog control artiﬁci intellig cambridg usa mit press hong yu wan tao wang multimod deep autoencod human pose recoveri ieee tran imag vol pp huang liu van der maaten weinberg dens connect convolut network proc ieee conf comput vi pattern pp huang et gpipe efﬁcient train giant neural network use pipelin parallel proc adv neural inf process vol pp hubel wiesel recept ﬁeld binocular interact function architectur cat visual cortex j vol pp ioff szegedi batch normal acceler deep work train reduc intern covari shift proc int conf mach pp jarrett kavukcuoglu ranzato lecun best architectur object recognit proc ieee int conf comput pp jin ge netrap kakad jordan escap saddl point efﬁcient proc int conf mach precup teh proc mach learn vol pp jin evolutionari comput recent advanc futur challeng swarm evol vol pp józefowicz zaremba sutskev empir explor recurr network architectur proc int conf mach pp kandasami neiswang schneider póczo xing neural architectur search bayesian optimis optim transport proc adv neural inf process syst annu conf neural inf process pp kawaguchi huang gradient descent ﬁnd global minima generalizabledeepneuralnetworksofpracticals allerton conf control pp khadka chung tumer neuroevolut modular neural network deep memori problem evol vol pp khadka tumer polici gradient ment learn proc int conf neural inf process pp khan sohail zahoora qureshi survey recent architectur deep convolut neural network artif intel vol pp kim reddi yun seo nemo multiobject optim deep neural network speed accuraci icml automl workshop pp kimura evolutionari rate molecular level natur vol pp kimura neutral theori molecular evolut cambridg cambridg univ press kitano design neural network use genet algorithm graph gener system complex vol pp kleinberg li yuan altern view doe sgd escap local minima int conf mach pp koza kean streeter mydlowec yu lanza genet program iv routin machin genc vol berlin germani springer koza genet program program comput mean natur select cambridg usa mit press koza result produc genet ming genet program evolv vol pp krizhevski nair hinton canadian institut advanc research access onlin avail http krizhevski sutskev hinton imagenet classiﬁc deep convolut neural network commun acm vol pp may autoencod deep learn network proc ieee annu comput softw appl pp larochel erhan courvil bergstra bengio empir evalu deep architectur problem mani factor variat proc int conf mach pp larochel mandel pascanu bengio learn gorithm classiﬁc restrict boltzmann machin mach learn vol pp lecun bengio hinton deep learn natur vol pp lecun bottou bengio haffner learn appli document recognit proc ieee pp lecun bottou bengio haffner ing appli document recognit proc ieee vol pp lehman stanley novelti search problem tive genet programm theori vol pp lehman stanley abandon object evolut search novelti alon evol vol pp jun lillicrap cownden tweed akerman random feedback weight support learn deep neural network natur vol bestpracticesforscientiﬁcresearchonneur architectur search mach learn vol pp liu et progress neural architectur search proc eur conf comput vision liu simonyan vinyal fernando kavukcuoglu hierarch represent efﬁcient architectur search proc int conf learn represent pp liu simonyan yang dart differenti architectur search liu gong miao wang li structur learn deep neural network base multiobject optim ieee tran neural netw learn vol pp jun author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact artifici intellig vol decemb liu wang liu zeng liu alsaadi survey deep neural network architectur applic pute vol pp genet algorithm proc genet evol comput pp luo li wang liang discrimin autoencod featur extract fault diagnosi chemometr intel lab vol art princip taxonomi neural memori network ieee tran neural netw learn vol pp jun mandisch comparison evolut strategi agat neural network train neurocomput vol pp martin naranjo camacho evodeep new evolutionari approach automat deep neural network parametris parallel distrib vol pp meier mujika gauy steger improv ent estim evolutionari strategi past descent direct corr vol onlin avail http miikkulainen et evolv deep neural network corr vol onlin avail http miller cartesian genet program berlin germani springer pp mnih et play atari deep reinforc learn corr vol onlin avail http moham sainath dahl ramabhadran hinton picheni deep belief network use discrimin featur phone recognit proc ieee int conf speech signal pp montana davi train feedforward neural network use genet algorithm proc int joint conf artif pp mors stanley simpl evolutionari optim rival stochast gradient descent neural network proc genet evol comput pp nair hinton rectiﬁ linear unit improv restrict boltzmann machin proc int conf int conf mach pp negrinho gordon deeparchitect automat design train deep architectur corr vol line avail http netzer wang coat bissacco wu ng read digit natur imag unsupervis featur learn proc conf neural inf process onlin abl http ororbia elsaid desel investig recurr neural network memori structur use proc genet evol comput pp pawełczyk kawulok nalepa deep neural network proc genet evol comput conf companion pp peng liu liu wang effect long memori differenti evolut algorithm electr price predict energi vol pp poli effect neutral ﬁtness distanc correl phenotyp mutat rate problem hard foundat genet algorithm stephen et ed berlin germani springer pp poli effect constant traliti problem hard ﬁtness distanc correl phenotyp mutat rate ieee tran evol vol pp apr porto fogel fogel altern neural network train method ieee expert intel syst vol pp jun price storn lampinen differenti practic approach global optim berlin germani springer rahnamayan tizhoosh salama base differenti evolut ieee tran evol vol pp rajbhandari ruwas carbin chilimbi mize cnn multicor scalabl perform goodput proc int conf architectur support program lang oper pp real aggarw huang le regular evolut imag classiﬁ architectur search proc aaai conf artif aaai innov appl artif intel iaai aaai symp edu adv artif pp real et evolut imag classiﬁ proc int conf mach pp rechenberg evolutionsstrategien simulationsmethoden der medizin und biologi schneider ranft ed berlin germani springer pp rechenberg evolut strategi natur way optim optim method applic possibl limit bergmann ed berlin germani springer pp rumelhart hinton william learn intern represent error propag cambridg usa mit press pp ryan collin neill grammat evolut evolv gram arbitrari languag genet program banzhaf poli schoenauer fogarti berlin germani springer pp salakhutdinov hinton deep boltzmann machin proc int conf artif intel proc mach learn apr pp saliman ho chen sidor sutskev evolut strategi scalabl altern reinforc learn schmidhub deep learn neural network overview neural vol pp schwefel numer optim comput model hoboken nj usa wiley sehnk osendorf rckstie grave peter huber polici gradient neural vol pp shahab grot evolutionari distribut sgd proc genet evol comput conf companion pp shallu lee antognini frostig dahl measur effect data parallel neural network train mach learn vol pp shinozaki watanab structur discoveri deep neural network base evolutionari algorithm proc ieee int conf speech signal pp shirakawa iwata akimoto dynam optim neural network structur use probabilist model proc aaai conf artif vol silver et master game go deep neural network tree search natur vol pp simonyan zisserman veri deep convolut network imag recognit proc int conf learn tation san diego ca usa may stallkamp schlips salmen igel man comput benchmark machin learn algorithm trafﬁc sign recognit neural vol pp stanley clune lehman miikkulainen design neural network neuroevolut nat mach vol pp stanley miikkulainen evolv neural network augment topolog evol vol pp jun madhavan conti lehman stanley clune deep neuroevolut genet algorithm competit altern train deep neural network reinforc learn corr vol onlin avail http suganuma kobayashi shirakawa nagao evolut deep convolut neural network use cartesian genet ming evol vol pp author licens use limit queen mari univers london download march utc ieee xplore restrict appli galván mooney neuroevolut deep neural network current trend futur challeng suganuma ozay okatani exploit potenti standard convolut autoencod imag restor ari search proc int conf mach stockholmsmässan stockholm sweden vol jul pp suganuma shirakawa nagao genet program approach design convolut neural network architectur proc genet evol comput pp sun wang xue jin yen zhang assist evolutionari deep learn use random base perform predictor ieee tran evol vol pp apr sun xue zhang yen complet autom cnn architectur design base block ieee tran neural netw learn vol pp apr sun xue zhang yen evolv deep convolut neural network imag classiﬁc ieee tran evol vol pp apr sun yen yi evolv unsupervis deep neural network learn meaning represent ieee tran evol comput vol pp sutton barto reinforc learn introduct bradford book cambridg usa mit press szegedi et go deeper convolut proc ieee conf comput vi pattern pp talbi taxonomi hybrid metaheurist heurist vol pp tapia estevez inform plane autoencod proc int joint conf neural jul tishbi zaslavski deep learn inform tleneck principl ieee inf theori workshop jerusalem israel apr may pp doi van wyk bosman evolutionari neural architectur search imag restor proc int joint conf neural pp wang sun xue zhang hybrid differenti evolut approach design deep convolut neural network imag classiﬁc proc adv artif intel australasian joint lectur note comput springer vol pp wright role mutat inbreed crossbreed select evolut proc int congr pp xiao rasul vollgraf novel imag dataset benchmark machin learn algorithm corr vol onlin avail http xie yuill genet cnn proc ieee int conf comput pp xie zheng liu lin sna stochast neural ture search proc int conf learn represent new orlean la usa may yang et car continu evolut efﬁcient neural chitectur search proc conf comput vision pattern recognit pp yao evolv artiﬁci neural network proc ieee vol pp ying klein real christiansen murphi hutter toward reproduc neural architectur search int conf mach pp ying chen eksombatchai hamilton leskovec graph convolut neural network mender system proc acm sigkdd int conf knowl discov data mine pp yu princip understand autoencod inform theoret concept neural network elsevi vol pp zagoruyko komodaki wide residu network proc brit mach vi wilson hancock smith new york uk bmva press zela siem hutter benchmark dissect neural architectur search proc int conf learn represent addi ababa ethiopia apr zhang lim qin tan multiobject deep belief network ensembl remain use life estim prognost ieee tran neural netw learn vol pp zhang tan li hong deep belief network imbalanc classiﬁc ieee tran neural netw learn vol pp zhao zhang lu direct evolutionari featur extract algorithm classifi high dimension data proc nat conf artif pp zhong yan liu practic network block design corr vol onlin avail http zhu et benchmark analyz deep neural network train proc ieee int symp workload character pp zoph le neural architectur search reinforc proc int conf learn represent conf track toulon franc apr onlin avail http zoph vasudevan shlen le learn transfer architectur scalabl imag recognit ieee conf comput vision pattern recognit salt lake citi ut usa ieee comput jun pp author licens use limit queen mari univers london download march utc ieee xplore restrict appli