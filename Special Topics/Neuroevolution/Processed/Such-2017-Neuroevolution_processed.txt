deep neuroevolut genet algorithm competit altern train deep neural network reinforc learn felip petroski vashisht madhavan edoardo conti joel lehman kenneth stanley jeff clune uber ai lab jeffclun abstract deep artiﬁci neural network dnn ical train via learn gorithm name backpropag evolut strategi es rival rithm polici ent challeng deep reinforc learn rl problem howev es ere algorithm becaus form stochast gradient descent via ation similar tion gradient rais question whether evolutionari gorithm work dnn scale demonstr evolv weight dnn simpl base genet algorithm ga perform well hard deep rl problem includ atari humanoid locomot deep ga cess evolv network four lion free paramet largest neural network ever evolv tradit evolutionari gorithm result expand sens scale ga oper gest intriguingli case follow gradient best choic optim perform make immedi abl multitud neuroevolut techniqu improv perform demonstr latter show combin dnn novelti search encourag explor task decept spars reward function solv problem algorithm dqn es ga fail addit deep ga faster es dqn train atari hour one desktop hour distribut core enabl compact encod techniqu introduct recent trend machin learn ai research old algorithm work remark well combin sufﬁcient comput resourc data ha stori backpropag appli deep ral network supervis learn task puter vision krizhevski et voic nition seid et backpropag deep neural network combin tradit reinforc learn algorithm watkin dayan mnih et polici gradient pg method sehnk et mnih et evolut strategi es appli reinforc learn mark saliman et one common theme method includ es involv gradient approxim similar ﬁnite differ william wierstra et man et thi histor trend rais question whether similar stori play method ga thi paper investig question test manc simpl ga hard deep reinforc learn rl benchmark includ atari bellemar et brockman et mnih et manoid locomot mujoco simul todorov et schulman et brockman et compar perform ga contemporari algorithm appli deep rl dqn mnih et method mnih et polici gradient method es one might pect ga perform far wors method becaus simpl follow gradient ingli found ga turn competit gorithm rl perform better domain wors roughli well overal dqn es ad new famili algorithm box deep rl problem also valid ness learn ga compar perform random search rs ga alway perform random search interestingli discov apr genet algorithm competit altern train deep neural network reinforc learn atari game random search outperform ful deep rl algorithm dqn game es suggest local optima dle point noisi gradient estim forc imped progress problem method note although deep neural network often struggl local optima supervis learn pascanu et local optima remain issu rl becaus reward signal may decept encourag agent perform action prevent discov global optim behavior like es deep rl algorithm ga ha uniqu beneﬁt ga prove slightli faster es discuss low ga es thu substanti faster speed polici gradient method explor two distinct ga implement version gpu cpu distribut version mani cpu across mani chine singl modern desktop gpu cpu core ga train atari hour train compar perform take day dqn day thi speedup enabl individu research singl albeit expens desktop start use main formerli reserv lab onli ate perhap quickli ani rl algorithm given substanti distribut comput cpu core across dozen machin ga es train atari hour also beneﬁci via new techniqu introduc even network train ga encod veri thousand byte yield compact encod method overal unexpectedli competit perform ga random search suggest structur search space domain amen search realiz open new search direct exploit region search might appropri motiv research new kind hybrid algorithm background high level rl problem challeng agent imiz notion cumul reward total count without supervis accomplish goal sutton barto host tradit rl gorithm perform well small tabular state space ton barto howev scale problem learn act directli pixel wa challeng rl algorithm har tional power deep neural network dnn thu alyz ﬁeld deep reinforc learn deep rl mnih et three broad famili deep ing algorithm shown promis rl problem far method dqn mnih et ici gradient method sehnk et mnih et trpo schulman et ppo man et recent evolut strategi es saliman et deep algorithm approxim optim q function dnn yield polici given state choos action maxim watkin dayan mnih et hessel et polici gradient method directli learn ter dnn polici output probabl take action state team openai recent experi simpliﬁ version natur tion strategi wierstra et speciﬁc one learn mean distribut paramet varianc found thi algorithm refer simpli evolut strategi es competit dqn difﬁcult rl benchmark problem much faster train time faster time mani cpu avail due better tion saliman et method consid method calcul approxim gradient dnn optim paramet via stochast gradient though requir differenti reward function simul dqn culat gradient loss dnn tion approxim via backpropag polici gradient sampl behavior stochast current polici reinforc perform well via stochast ent ascent es doe calcul gradient analyt approxim gradient reward function ramet space saliman et wierstra et test whether truli method ga perform well challeng deep rl task ﬁnd ga perform surprisingli well thu consid new addit set algorithm deep rl problem method genet algorithm purpos test extrem simpl ga set baselin well evolutionari rithm work rl problem expect futur work reveal ad legion enhanc exist ga fogel stayton haupt haupt clune et mouret doncieux lehman ley stanley et mouret clune improv perform deep rl task genet algorithm holland eiben et evolv popul p n individu neural genet algorithm competit altern train deep neural network reinforc learn work paramet vector θ often call genotyp eri gener θi evalu produc ﬁtness score aka reward f θi ga variant perform cation select wherein top individu becom parent next gener produc next tion follow process repeat n time ent select uniformli random replac mutat appli addit gaussian nois eter vector θ σϵ ϵ priat valu σ wa determin empir periment describ supplementari inform si tabl n th individu unmodiﬁ copi best individu previou gener techniqu call elit reliabl tri select true elit presenc noisi evalu evalu top individu per gener addit episod count frame one consum dure train one highest mean score design elit histor ga often involv crossov combin paramet multipl parent produc offspr simplic includ new tion evalu process repeat g erat stop criterion met si algorithm provid pseudocod version open sourc code hyperparamet conﬁgur experi avail http hyperparamet also list si tabl hyperparamet ﬁxed atari game chosen set hyperparamet test six game terix enduro gravitar kangaroo seaquest ventur ga implement tradit store individu paramet vector θ thi approach scale poorli ori network transmiss cost larg popul larg deeper wider neural network propos novel method store larg paramet vector compactli repres paramet vector initi seed plu list random seed produc seri mutat produc θ θ reconstruct thi innov wa critic efﬁcient implement distribut deep si fig show eq describ method θn ψ τn σε τn θn offspr ψ τn minist mutat function τ encod θn ing list mutat seed φ φ determinist initi function ε τn determinist gaussian number ator input seed τn produc vector length case ε τn larg precomput tabl index use seed si sec provid encod detail includ seed could smaller thi techniqu advantag becaus size compress represent increas linearli ber gener often order thousand dent size network often order million doe cours requir comput struct dnn weight vector competit agent evolv littl ten gener enabl compress represent paramet neural work thousand byte sion compress rate depend number erat practic alway substanti atari ﬁnal network compress thi resent state art encod larg network pactli howev gener network compress techniqu becaus compress arbitrari network instead onli work network evolv one motiv choos es versu ici gradient method faster time tribut comput owe better parallel man et found distribut deep ga onli preserv thi beneﬁt slightli prove upon si sec describ whi faster es importantli ga also use gpu speed forward pass dnn especi larg one make possibl train singl top implement one ern desktop train atari hour take hour distribut core distribut gpu train would speed train larg popul size novelti search one beneﬁt train deep neural network ga enabl us immedi take advantag rithm previous develop neuroevolut muniti demonstr experi novelti search ns lehman stanley wa sign decept domain mizat mechan converg local optima ns avoid local optima ignor reward function dure evolut instead reward agent perform havior never perform befor novel surprisingli often outperform algorithm util reward signal result demonstr maze navig simul bipe locomot task lehman stanley appli ns see form combin dnn decept base rl problem call imag hard maze refer ga optim novelti ns requir behavior characterist bc describ behavior polici bc π behavior tanc function bc ani two polici dist bc πi bc πj genet algorithm competit altern train deep neural network reinforc learn gener member popul probabl p bc store archiv novelti polici deﬁn averag distanc k nearest neighbor sort behavior distanc popul archiv novel individu thu determin base behavior distanc current previous seen individu ga otherwis proce normal substitut novelti ness reward report plot purpos onli identifi individu highest reward per tion algorithm present si algorithm experi experi focu perform ga challeng problem valid fectiv deep rl algorithm es saliman et includ learn play atari directli pixel mnih et schulman et mnih et bellemar et tinuou control problem involv simul humanoid robot learn walk brockman et schulman et saliman et todorov et also test maze domain ha clear local optimum imag hard maze studi well algorithm avoid decept lehman stanley atari imag hard maze experi record best agent found multipl independ domli initi ga run atari imag hard maze becaus atari stochast ﬁnal score run calcul take elit across gener averag score achiev independ evalu ﬁnal score main median ﬁnal run score humanoid comot detail si sec atari train deep neural network play atari map rectli pixel action wa celebr feat arguabl launch deep rl era expand derstand difﬁculti rl domain machin learn could tackl mnih et test perform dnn evolv simpl ga pare dnn train major famili deep rl gorithm es model experi es paper saliman et al sinc inspir studi due limit comput resourc compar result atari game chosen becaus game es perform well frostbit gravitar kangaroo ventur zaxxon poorli amidar enduro ski seaquest remain game chosen ale bellemar et set alphabet order assault asterix asteroid atlanti facilit comparison result report saliman et al keep number game frame agent experi cours ga run constant one billion frame frame limit result differ number gener per independ ga run si sec tabl polici ent qualiti differ run may see frame game agent live longer dure train agent evalu full episod cap frame includ multipl live ﬁtness sum episod reward ﬁnal atari game score follow ident dqn mnih et data preprocess network tectur stochast environ start episod random initi oper use larger dqn architectur mnih et al consist convolut layer channel follow hidden layer unit convolut layer use ﬁlter stride respect hidden er follow rectiﬁ nonlinear relu network contain paramet interestingli mani past assum simpl ga would fail scale result ga implement fair comparison algorithm difﬁcult tion procedur algorithm realiz ferent comput speed sampl efﬁcienc anoth consider whether agent evalu random start random number action regim train start randomli sampl human play test gener nair et becaus databas human start sampl agent evalu random start possibl pare result algorithm random start true dqn es includ result human start also attempt control number frame seen dure train becaus dqn far slower run present result literatur train fewer frame requir day comput hour comput need es ga train frame mani variant dqn could compar includ rainbow hessel et gorithm combin mani differ recent improv dqn van hasselt et wang et schaul et sutton barto bellemar et fortunato et howev choos compar ga origin vanilla dqn algorithm partli caus also introduc vanilla ga without mani modiﬁc improv previous develop haupt haupt genet algorithm competit altern train deep neural network reinforc learn like surpris mani simpl ga abl train deep neural network play mani atari game roughli well dqn es tabl among game tri dqn es ga duce best score game produc best score ski ga produc score higher ani algorithm date awar includ dqn variant rainbow dqn paper hessel et game ga formanc advantag dqn es erabl frostbit ventur ski video cie evolv ga view http comparison ga perform better es dqn game tabl ga also perform wors mani game ing theme deep rl differ famili rithm perform differ across differ domain man et howev comparison liminari becaus comput resourc need gather sufﬁcient sampl size see algorithm signiﬁcantli differ per game instead key takeaway tend perform roughli similarli doe well differ game becaus perform plateau ga run test whether ga improv given addit putat thu run ga six time longer frame game score improv tabl score ga outperform es dqn game parison respect si tabl game ga perform still ha converg frame si fig leav open question well ga ultim perform run even longer knowledg thi paramet neural network largest neural network ever evolv simpl one remark fact quickli ga ﬁnd perform individu becaus employ larg ulat size run last rel gener min max si tabl mani game ga ﬁnd solut better dqn onli one ten gener speciﬁc median ga perform higher ﬁnal dqn perform gener ski ventur frostbit oid gravitar zaxxon respect similar result hold es ga tion need outperform es ski frostbit amidar asterix asteroid ventur respect number gener requir beat enduro frostbit kangaroo ski ventur gravitar amidar respect gener ga tend make chang control σ paramet vector see method ga outperform dqn es gener especi doe ﬁrst gener befor round select suggest mani polici exist near origin precis near region random tion function gener polici rais question ga anyth random search answer thi question evalu mani polici domli gener ga initi function φ report best score gave random search mate amount frame comput ga compar perform tabl everi game ga outperform random search niﬁcantli game fig p thi futur p valu via wilcoxon test prove perform suggest ga perform healthi optim gener surprisingli given celebr impress dqn es game random search actual outperform dqn frostbit ski ventur es amidar frostbit ski enduro frostbit gravitar kangaroo ski ventur ingli polici produc random search trivial degener polici instead appear quit sophist consid follow exampl game frostbit requir agent perform long sequenc jump row iceberg ing differ direct avoid enemi tional collect food build igloo brick brick si fig onli igloo built agent ter igloo receiv larg payoff ﬁrst two live polici found random search complet seri action jump row iceberg move differ direct avoid enemi back three time construct igloo onli onc igloo built agent immedi move toward enter point get larg reward repeat entir process harder level thi time also gather food thu earn bonu point video http polici sult veri high score less hour random search averag score produc dqn day optim one may think random search found lucki open loop sequenc tion overﬁt particular stochast environ markabl found thi polici actual gener initi condit achiev median score bootstrap median conﬁdenc interv differ test environ random initi standard test cedur hessel et mnih et genet algorithm competit altern train deep neural network reinforc learn dqn es rs ga ga frame time forward pass backward pass oper u u u u u u amidar assault asterix asteroid atlanti enduro frostbit gravitar kangaroo seaquest ski ventur zaxxon tabl atari simpl genet algorithm competit dqn polici gradient evolut gie es shown game score higher better compar perform algorithm inher challeng see main text attempt facilit comparison show estim amount comput oper sum forward backward neural network pass data efﬁcienc number game frame train episod long time algorithm take run es dqn ga perform best game respect surprisingli random search often ﬁnd polici superior dqn es see text discuss note dramat differ speed algorithm much faster ga es data efﬁcienc favor dqn score dqn hessel et al es saliman et al dqn es provid error bar becaus report origin literatur ga random search error bar visual si fig time approxim becaus depend varieti factor found ga run slightli faster es averag symbol indic state art perform ga score bold best prevent bold column exampl success rs versu dqn es suggest mani atari game seem hard base low perform lead deep rl algorithm may hard think instead algorithm reason perform poorli task tualli quit easi result suggest time best search strategi follow gradient instead conduct dens search local hood select best point found subject return discuss sec imag hard maze thi experi seek demonstr beneﬁt ga work dnn scale algorithm develop improv ga immedi taken shelf improv dnn train exampl algorithm novelti search ns popular evolutionari method explor rl lehman stanley ns wa origin motiv hard maze domain lehman stanley stapl roevolut commun demonstr problem cal optima aka decept reinforc learn robot receiv reward closer get goal crow ﬂie problem decept becaus ili get closer goal lead agent perman get stuck one map decept trap fig left optim algorithm conduct sufﬁcient plorat suffer thi fate ns solv thi problem becaus ignor reward encourag agent visit new place lehman stanley origin version thi problem involv onli put radar sensor sens wall two continu put speed forward backward rotat make solvabl small neural network ten connect becaus want demonstr beneﬁt ns scale deep neural network introduc new version domain call imag hard maze like mani atari game show bird view world agent form pixel imag fig left thi chang make problem easier way fulli observ harder becaus genet algorithm competit altern train deep neural network reinforc learn figur imag hard maze domain result left small wheel robot must navig goal thi bird view pixel input robot start bottom left corner face right right novelti search train deep neural network avoid local optima stymi algorithm ga sole optim reward ha incent explor get stuck local optimum trap ga optim novelti encourag ignor reward explor whole map enabl eventu ﬁnd goal es perform even wors ga discuss main text dqn also fail solv thi task es perform mean θ polici iter plot ga perform individu per gener plot becaus dqn number evalu per iter evolutionari algorithm plot ﬁnal median reward dash line si fig show behavior algorithm dure train much neural network must learn process thi pixel input take action si sec ha addit experiment detail conﬁrm result held small neural work origin version thi task also hold visual version thi task deep neural network paramet network process pixel novelti search abl solv task ﬁnding goal fig ga optim reward onli expect get stuck local optima trap si fig thu fail solv problem fig signiﬁcantli underperform p result conﬁrm abl use explor method novelti search solv thi sort decept even problem involv learn directli pixel thi largest neural network optim novelti search date three order magnitud companion paper conti et also demonstr similar ﬁnding hybrid novelti search es creat show help deep neural network avoid tion challeng rl benchmark domain expect es also fail solv task becaus cuse sole maxim reward fig si fig also test dqn polici gradient thi problem sourc code abl obtain sourc code ha similar formanc wu et onli differ synchron instead asynchron iment modiﬁ reward domain step reward neg chang distanc goal sinc last plot purpos record ﬁnal distanc goal reward dard algorithm provid inform doe remov decept becaus dqn requir discret output discret two uou output ﬁve equal size bin enabl possibl output combin learn also expect dqn fail solv thi problem fig si fig default explor mechan enough ﬁnd global optimum given tive reward function thi domain dqn drawn expect trap unclear reason even though visit trap often earli train converg ting stuck differ part maze cours ration techniqu could ad control tialli make perform well onli sought show deep ga allow algorithm ope neural network har hard problem requir dnn futur work interest combin ns deep ga domain includ atari robot domain importantli demonstr suggest algorithm enhanc ga bine dnn perhap promis combin notion divers novelti qualiti high perform seek collect set perform yet interestingli differ polici mouret clune lehman stanley culli et pugh et result also motiv futur research combin deep rl algorithm dqn novelti search qualiti divers algorithm humanoid locomot ga wa also abl solv challeng continu control benchmark humanoid locomot brockman et ha valid modern power rithm trpo es ga duce robot could walk well took time longer perform slightli wors es si sec surpris becaus ga previous perform well robot locomot task clune et huizinga et futur research requir understand whi discuss surpris success ga rs domain thought requir least degre gradient genet algorithm competit altern train deep neural network reinforc learn tion suggest heretofor aspect search space impli dens sampl region around origin sufﬁcient case ﬁnd far better solut found method even far putat time suggest gradient point solut optim sue interfer ﬁnding saddl point noisi gradient estim ga result suggest sampl region around good solut ten sufﬁcient ﬁnd even better solut quenc discoveri possibl mani challeng domain result turn impli distribut solut increas qualiti unexpectedli dens need follow gradient ﬁnd anoth exclus hypothesi ga es improv perform due tempor tend explor osband et mean plore consist sinc action episod tion set mutat paramet improv explor plappert et thi help explor two reason agent take action ha distribut action time visit state make easier learn whether ici state advantag agent also like correl action across state way go becaus mutat intern tion affect action taken mani state similarli perhap interest result sometim actual wors follow gradient sampl local paramet space better solut thi scenario probabl doe hold domain even region domain sometim hold hold expand conceptu understand viabil differ kind search oper reason ga might outperform method local optima present jump ter space wherea gradient method without tional optim trick momentum although note es util modern adam optim experi kingma ba includ mentum one unknown question whether cal search better earli search cess switch search later allow progress would imposs prohibit comput expens ga make anoth known question promis simultan ing ga method modern algorithm deep rl polici gradient evolut strategi still know veri littl ultim promis ga versu compet algorithm train deep neural work reinforc learn problem addit use extrem simpl ga mani techniqu invent improv ga perform eiben et haupt haupt includ crossov holland deb myburgh indirect ing stanley stanley et clune et encourag qualiti divers mouret clune pugh et name moreov mani techniqu invent dramat improv train dnn backpropag ual network et selu relu activ function krizhevski et klambauer et lstm gru hochreit schmidhub cho et regular hoerl kennard dropout srivastava et anneal learn rate schedul robbin monro hypothes mani techniqu also improv lution larg dnn enhanc may improv ga manc humanoid locomot exampl indirect encod allow genom paramet affect tipl weight ﬁnal neural network way similar convolut tie weight far iti ha shown dramat improv perform data efﬁcienc evolv robot gait clune et result found hyperneat gorithm stanley et ha indirect code abstract power development biolog stanley particularli promis direct humanoid locomot atari investig interest learn domain deep ga tend perform well poorli understand whi also ga could help main architectur search liu et ikkulainen et train limit precis binari neural network conclus work introduc deep ga competit train deep neural network challeng rl task code techniqu enabl efﬁcient distribut train compact network encod found ga fast enabl train atari gle desktop distribut cpu ment ga surprisingli competit lar algorithm deep reinforc learn problem dqn es especi challeng atari domain also show interest algorithm develop neuroevolut commun mediat test deep neural network ing deep novelti search solv decept game interest see ture research investig potenti limit ga especi combin techniqu known genet algorithm competit altern train deep neural network reinforc learn improv ga perform gener result tinu stori start backprop extend es old simpl algorithm plu modern amount comput perform amazingli well rais question old algorithm revisit acknowledg thank member uber ai lab ful suggest throughout cours thi work ticular zoubin ghahramani peter dayan noah goodman thoma miconi theofani karaletso also thank justin pinkul mike deat codi yancey entir opusstack team uber provid resourc nical support thank also david ha mani help suggest improv paper refer bellemar naddaf veness bowl arcad learn environ evalu form gener agent artif intel jair bellemar dabney muno bution perspect reinforc learn arxiv preprint brockman cheung pettersson schneider schulman tang zaremba openai gym cho van enboer bahdanau gio properti neural machin lation approach arxiv preprint clune stanley pennock ofria perform indirect encod across uum regular ieee transact evolutionari comput conti madhavan petroski lehman ley clune improv explor lution strategi deep reinforc learn via popul agent arxiv preprint culli clune tarapor mouret robot adapt like anim natur doi deb myburgh break rier optim use custom lutionari algorithm gecco eiben smith introduct lutionari comput volum springer fogel stayton effect crossov simul evolutionari optim biosystem fortunato azar piot menick osband grave mnih muno hassabi pietquin noisi network explor arxiv preprint glorot bengio understand difﬁculti train deep feedforward neural network icai haupt haupt practic genet algorithm john wiley son zhang ren sun deep residu learn imag recognit arxiv preprint hessel modayil van hasselt schaul ostrovski dabney horgan piot azar silver rainbow combin improv deep forcement learn arxiv preprint hochreit schmidhub long ori neural comput hoerl kennard ridg regress bias estim nonorthogon problem technometr holland genet algorithm scientiﬁc american huizinga mouret clune doe align notyp genotyp modular improv evolut neural network gecco ioff szegedi batch normal ing deep network train reduc intern covari shift icml kingma ba adam method stochast mizat arxiv preprint klambauer unterthin mayr hochreit neural network arxiv preprint krizhevski sutskev hinton imagenet classiﬁc deep convolut neural network nip lehman stanley abandon object lution search novelti alon evolutionari comput lehman stanley evolv divers virtual creatur novelti search local competit gecco isbn doi lehman chen clune stanley es tradit tor arxiv preprint liu simonyan vinyal fernando kavukcuoglu hierarch represent efﬁcient architectur search arxiv preprint miikkulainen liang meyerson rawal fink francon raju navruzyan duffi hodjat evolv deep neural network arxiv preprint genet algorithm competit altern train deep neural network reinforc learn mnih kavukcuoglu silver rusu veness bellemar grave riedmil fidjeland ostrovski control deep reinforc learn natur mnih badia mirza grave lillicrap harley silver kavukcuoglu asynchron method deep reinforc learn icml mouret clune illumin search space ping elit arxiv mouret doncieux overcom bootstrap problem evolutionari robot use behavior siti proceed ieee congress ari comput nair srinivasan blackwel alcicek fearon de maria panneershelvam suleyman beatti petersen massiv parallel od deep reinforc learn arxiv preprint osband blundel pritzel van roy deep explor via bootstrap dqn nip pascanu dauphin ganguli bengio saddl point problem optim arxiv preprint plappert houthooft dhariw sidor chen chen asfour abbeel andrychowicz paramet space nois explor arxiv preprint pugh soro stanley qualiti siti new frontier evolutionari comput issn robbin monro stochast approxim method annal mathemat statist saliman ho chen sidor sutskev lution strategi scalabl altern ment learn arxiv saliman goodfellow zaremba cheung ford chen improv techniqu train gan nip saliman ho chen sutskev tion strategi scalabl altern reinforc learn arxiv preprint schaul quan antonogl silver priorit experi replay arxiv preprint schulman levin abbeel jordan moritz trust region polici optim icml schulman wolski dhariw radford klimov proxim polici optim algorithm arxiv preprint sehnk osendorf uckstieß grave peter schmidhub polici dient neural network seid li yu convers speech tion use deep neural network interspeech srivastava hinton krizhevski sutskev salakhutdinov dropout simpl way prevent neural network overﬁt journal chine learn research stanley composit pattern produc network novel abstract develop genet ming evolv machin special issu mental system stanley ambrosio gauci indirect encod evolv scale neural network artiﬁci life sutton barto reinforc learn introduct volum mit press cambridg todorov erez tassa mujoco physic gine control intellig robot system iro intern enc van hasselt guez silver deep reinforc learn doubl aaai wang schaul hessel van hasselt tot de freita duel network ture deep reinforc learn arxiv preprint watkin dayan machin learn wierstra schaul peter schmidhub ural evolut strategi evolutionari comput cec ieee world congress tional intellig ieee congress william simpl statist rithm connectionist reinforc learn chine learn wu mansimov gross liao ba abl method deep reinforc learn use approxim nip genet algorithm competit altern train deep neural network reinforc learn supplementari inform whi ga faster es ga faster es two main reason eri gener es must calcul updat neural network paramet vector doe via weight age across mani saliman et al offspr random θ perturb weight ness thi averag oper slow larg neural work larg number latter requir healthi optim requir deep es requir requir virtual batch maliz gener divers polici amongst offspr necessari accur ﬁnite differ approxim saliman et virtual batch maliz requir addit forward pass enc random set observ chosen start comput layer normal statist use manner batch normal ioff szegedi found random ga ramet perturb gener sufﬁcient divers polici without virtual batch normal thu avoid addit forward pass network algorithm simpl genet algorithm input mutat function ψ popul size n number select individu polici initi routin φ ﬁtness function g g gener n next gener popul g φ n initi random dnn els k uniformrandom select parent pg ψ k mutat parent end evalu fi f pg end sort pg descend order fi g set elit candid c els set elit candid c elit end set elit f θ pg elit pg elit onli includ elit onc end return elit algorithm novelti search input mutat function ψ popul size n number select individu polici initi routin φ empti archiv archiv insert probabl p elti function η behavior characterist function bc g g gener n next gener popul g φ n initi random dnn els k uniformrandom select parent pg ψ k mutat parent end bcg bc pg end copi pg bcg n next gener popul evalu fi η bcg bcg add bcg probabl p end end sort pg descend order fi end return elit hyperparamet use xavier initi glorot bengio polici initi function φ bia weight set zero connect weight drawn standard normal distribut varianc nin number incom connect neuron hyperparamet humanoid imag atari locomot hard maze popul size n mutat power σ truncat size number trial archiv probabl tabl hyperparamet popul size increment account elit mani unusu number found via preliminari hyperparamet search domain addit inform deep ga compact encod method compact encod techniqu base ple seed need onli long enough gener uniqu mutat vector per offspr per parent ani given parent produc x offspr τn genet algorithm competit altern train deep neural network reinforc learn game minimum median maximum gener gener gener amidar assault asterix asteroid atlanti enduro frostbit gravitar kangaroo seaquest ski ventur zaxxon tabl number gener ga reach frame eq small x number special case need one uniqu seed n per θ vector gener thu encod n bit reason seed vastli smaller search space size becaus everi point search space possibl offspr θn onli need abl gener offspr randomli need abl reach ani point search space one random step howev becaus perform n dom mutat produc θn process reach mani point search space howev truli abl reach everi point search space need set mutat vector span search space mean need seed least bit use function h θ τ map given θ τn new seed appli ψ τn ε h τn note thi case two notion seed encod seri small τ seed new seed τ gener parent θ previou seed addit experiment detail imag hard maze tempor context current frame previou three frame input timestep follow mnih et al output remain origin hard maze problem formul lehman stanley unlik atari domain imag hard maze ment determinist doe need multipl tion polici follow lehman stanley bc x posit robot end episod timestep behavior distanc function squar euclidean distanc ﬁnal x tion simul ignor forward backward motion would result robot penetr wall prevent robot slide along wall although rotat motor command still usual effect situat humanoid locomot test ga challeng continu control problem speciﬁc humanoid locomot test mujoco environ openai gym todorov et brockman et volv simul humanoid robot learn walk ing thi problem ha valid modern power rithm mnih et trpo schulman et es saliman et thi problem involv map vector scalar describ state humanoid posit lociti angl joint torqu robot receiv scalar reward combin four compon timestep get posit reward stand veloc posit x direct neg reward energi expend hard impact ground four term sum everi timestep episod calcul total reward stabil train normal dimens input subtract mean divid standard deviat comput execut dom polici environ also appli ing mutat power σ decreas gener result small perform boost end train full set ter list si tabl experi ran independ randomli tializ run report median run dure elit select routin reevalu offspr time like atari experi becaus ran experi befor atari experi improv evalu method experi complet comput sourc experi chang tocol believ thi chang would qualit alter result also use normal column initi routin saliman et al instead xavier initi found perform similarli determin ﬁtness agent evalu mean independ episod gener plot purpos onli uat elit time genet algorithm competit altern train deep neural network reinforc learn 𝜃 𝜙 𝜏 𝜃 𝜃 𝜃 𝜓 𝜃 𝜏 𝜃 𝜃 𝜎𝜀𝜏 𝜓 𝜃 𝜃 𝜃 𝜓 𝜃 𝜃 𝜓 𝜃 𝜃 weight 𝜏 𝜏 𝜏 𝜏 𝜏 𝜏 𝜏 𝜏 𝜏 lineag encod figur visual represent deep ga encod method randomli initi paramet vector produc initi function φ seed mutat function ψ seed appli mutat result ﬁnal paramet vector θg result seri mutat recreat θg done appli mutat step order thu know seri seed τg produc thi seri mutat enough inform reconstruct θg initi mutat function determinist sinc τ small bit long number gener low order hundr thousand larg neural network paramet vector store compactli architectur ha two hidden layer tanh activ function thi architectur one conﬁgur ﬁle includ sourc code releas saliman et al architectur describ paper similar smaller neuron per layer saliman et although rel shallow deep learn standard much smaller atari dnn thi architectur still contain paramet order magnitud greater largest ral network evolv robot task awar contain huizinga et fore paramet clune et mani sume evolut would fail larger scale network hundr thousand million weight thi paper previou work ha call problem solv score saliman et ga achiev median abov level tion howev requir far comput es es requir gener median formanc surpass threshold clear whi ga requir much comput cialli given quickli ga found polici atari domain also surpris ga doe excel thi domain given ga form well past robot control task clune et ga need far comput thi domain interest nevertheless doe alli solv produc agent walk score consid veri fast discoveri perform solut atari clearli ga advantag versu method depend domain stand thi depend import target futur research dqn es rs ga ga dqn es rs ga ga tabl comparison algorithm atari game valu repres mani game algorithm list top column produc higher score algorithm list left row ga beat dqn game mean frame mani paper includ report number frame use dure train howev bit unclear literatur meant exactli thi term hope introduc terminolog lend clariti thi confus issu improv reproduc abil compar algorithm fairli imagin emit frame dure train gest call game one could everi frame inde due frame skip atari paper exactli thi repeat previou action skip frame result frame suggest call frame train frame frame algorithm train paper genet algorithm competit altern train deep neural network reinforc learn port game frame use algorithm via person commun scientist openai deepmind conﬁrm accur report ber frame game frame train frame use dqn es mnih et al mnih et al saliman et al respect one addit clariﬁc paper mention ga thi paper input network atari current framet three previou frame three previou frame ing frame set mean count game frame input network follow game framet genet algorithm competit altern train deep neural network reinforc learn amidar assault asterix asteroid atlanti enduro frostbit gravitar kangaroo number game frame seaquest number game frame ski number game frame ventur number game frame zaxxon ga rs dqn es figur ga random search perform across gener atari game perform ga random search compar dqn es depend game plot ﬁnal score dash line dqn es becaus perform valu across train becaus train differ number game frame si tabl ga rs report median bootstrap conﬁdenc interv median across experi current elit per run score elit mean independ episod genet algorithm competit altern train deep neural network reinforc learn figur exampl individu frostbit found random search see main text tion behavior thi polici ﬁnal score thi episod far higher score produc dqn es although high score found ga tabl figur differ algorithm explor decept age hard maze time tradit gorithm exhibit sufﬁcient explor avoid local optimum go trap shown fig contrast ga optim novelti onli explor entir viron ultim ﬁnd goal evolutionari gorithm ga es blue cross repres popul es red cross repres top ga spring orang dot repres ﬁnal posit ga elit current mean es polici black cross entri archiv evolutionari algorithm number evalu es ga mani ping point becaus revisit locat due poor explor give illus fewer evalu dqn plot posit agent episod prior checkpoint list abov plot ing es signiﬁcantli underperform ga p run get stuck near trap becaus decept instead seemingli becaus reliabl learn pass small bottleneck corridor thi phenomenon ha never serv ga hard maze suggest es least hyperparamet qualit ferent ga thi regard lehman et believ thi differ occur becaus es optim averag ward popul sampl probabl distribut even maximum ﬁtness agent sampl bution higher along corridor es move direct averag popul lower polici sampl distribut crash wall experi fate companion paper investig thi interest differ es ga lehman et note howev even es move thi bottleneck run becaus sole got stuck trap