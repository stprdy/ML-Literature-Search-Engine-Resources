darpa explainable artiﬁcial intelligence program david gunning david aha n dramatic success machine learning hasledtoanewwaveofaiapplications example transportation security medicine ﬁnance defense offer tremendous beneﬁts not explain decision action human user darpa explainable artiﬁcial intelligence xai program endeavor create ai system whose learned model decision understood appropriately trusted end user realizing goal requires method learning explainable model designing effective explanation interface understanding psychologic requirement effective explanation xai developer team addressing ﬁrst two challenge creating ml technique developing principle strategy interaction technique generating tive explanation another xai team addressing third challenge rizing extending applying psychologic theory explanation help xai evaluator deﬁne suitable evaluation framework developer team use test system xai team completed ﬁrst program may series ongoing evaluation developer team assessing well xam system explanation improve user derstanding user trust user task performance dvances machine learning ml technique promise produce ai system perceive learn decide act however unable explain decision action human user lack especially important department defense whose challenge require developing intelligent autonomous symbiotic system explainable ai essential user understand appropriately trust effectively manage artiﬁcially intelligent partner address darpa launched explainable artiﬁcial intelligence xai program may darpa deﬁnes explainable ai ai system explain rationale human user characterize strength weakness convey understanding behave future naming program explainable ai rather interpretable comprehensible transparent ai example reﬂects darpa objective create ai system use effective explanation also reﬂects xai team interest human psychology explanation draw vast body research expertise social science 44 ai magazine copyright 2019 association advancement artiﬁcial intelligence right reserved issn deep learning security early ai system predominantly logical symbolic performed some form logical ference could provide trace inference step became basis explanation wa substantial work making system explainable fell short user need comprehension example simply summarizing inner working system doe not yield ﬁcient explanation proved brittle complexity recent ai success due largely new ml niques construct model internal sentations include support vector machine random forest probabilistic graphical model inforcement learning rl deep learning dl neural network although model exhibit high performance opaque use ha creased ha research explainability perspective ml chakraborty et al 2017 ra et al 2018 cognitive psychology miller 2017 larly many workshop held cently ml example international conference machine learning conference neural formation processing system ai example ternational joint conference artiﬁcial intelligence hci example conference computer interaction intelligent user interface ferences special topic meeting related xai seems inherent tension ml performance example predictive accuracy explainability often method example dl least explainable explainable example decision tree least accurate figure 1 illustrates notional graph various ml technique darpa formulated xai program sioned three broad strategy improve explainability maintaining high level learning performance based promising research time ﬁgure 2 deep explanation interpretable model model induction deep explanation refers modiﬁed hybrid dl technique learn explainable feature representation include explanation tion facility several design choice might produce explainable representation example training data selection architectural layer loss function regularization optimization technique training sequence researcher used deconvolutional network visualize convolutional network layer technique existed associating semantic concept deep network node approach generating image caption could extended train second deep network generates explanation without explicitly identifying original network semantic feature interpretable model ml technique learn structured interpretable causal model early example included bayesian rule list letham et al 2015 bayesian program learning learning model causal relationship use stochastic grammar learn interpretable structure model induction refers technique ment any given ml black infer approximate explainable model example explanation system ribeiro et al 2016 inferred explanation serving analyzing behavior black box model darpa used strategy categorize portfolio new ml technique provide future practitioner wider range design option covering trade space xai concept approach xai program goal create suite new modiﬁed ml technique produce explainable model combined effective explanation technique enable end user understand ately trust effectively manage emerging eration ai system target xai end user depends decision recommendation duced ai system action taken therefore need understand system rationale example intelligence analyst receives ommendations big data analytics system need understand recommended certain activity investigation similarly operator task autonomous vehicle drive route need derstand system model propriately use future mission figure 3 illustrates xai concept provide user explanation enable understand system overall strength weakness convey understanding behave future different situation perhaps permit user correct system mistake concept pose interrelated search challenge 1 produce able model 2 design explanation interface 3 understand psychologic ments effective explanation ﬁrst two lenges addressed 11 xai research team developing new ml technique produce explainable model new principle strategy hci technique example zation language understanding language generation generate effective explanation third challenge focus another xai research team summarizing extending applying psychologic theory explanation xai program address two operationally evant challenge problem area ﬁgure 4 data analytics classiﬁcation event interest heterogeneous multimedia data autonomy decision policy autonomous system area represent two important ml problem category supervised learning rl department defense terests intelligence analysis autonomous system data analytics challenge wa motivated common problem intelligence analyst presented decision recommendation big data summer 2019 45 deep learning security analytics algorithm must decide report supporting evidence analysis pursue algorithm often produce false alarm must pruned subject concept drift furthermore algorithm often make mendations analyst must ass determine whether evidence support contradicts potheses effective explanation help confront issue autonomy challenge wa motivated need effectively manage ai partner example department defense seek semiautonomous system augment warﬁghter capability operator need understand behave determine best use future mission effective explanation better enable determination challenge problem area critical measure explanation effectiveness would convenient learned model explainability could measured automatically xai system explanation effectiveness must assessed according explanation aid human user requires psychologic experiment measure user satisfaction mental model task performance appropriate trust darpa formulated initial explanation evaluation framework includes tential measure explanation effectiveness ﬁgure 5 exploring reﬁning framework important part xai program research agenda xai program goal concept strategy lenges evaluation framework described program 2016 broad agency announcement figure 6 display xai program schedule consists two phase phase 1 18 month menced may 2017 includes initial technology demonstration xai system phase 2 30 month includes sequence evaluation challenge problem selected system developer xai evaluator ﬁrst formal evaluation xai system took place fall article describes developer team progress leading evaluation whose result presented xai program meeting winter xai program development progress figure 7 summarizes 11 xai technical area 1 developer team team florida institute human machine cognition ihmc developing psychologic model explanation three team pursuing challenge problem area autonomy data analytics three working only former ﬁve working only latter per strategy described ﬁgure 2 team investigating diverse range technique developing explainable model explanation interface naturalistic foundation xai objective ihmc team includes researcher macrocognition michigan technological university develop evaluate psychologically plausible model explanation develop actionable concept method measure metric explanatory reasoning ihmc team decision tree deep learning ensemble method random forest learning performance explainability learning technique statistical model svms aogs performance explainability neural net markov model mlns hbns srl crfs bayesian belief net graphical model figure learning performance versus explainability several category learning technique 46 ai magazine deep learning security investigating nature explanation must happen person satisﬁed attempt explain 1 working complex system 2 acted way given situation address question team ha formulated naturalistic model human explanatory reasoning providing guidance performer team method evaluating effectiveness xai system explanation team reviewed relevant literature philosophy science specialization within psychology criterion sized assessing goodness explanation team also collecting analyzing corpus case individual create receive explanation working complex system team developed measure explanation goodness user mental model example rectness completeness user task performance user make reasonably accurate judgment trust doubt system gain insight user must explore making process performance xai system especially boundary case including way deep neural network dnns spoofed methodology wa described series essay hoffman klein 2017 hoffman et al 2017 klein 2018 hoffman et al 2018 figure 8 illustrates ihmc model xai planation process highlighting measurement category assessing explanation effectiveness user ceives recommendation decision xai system along explanation could tested goodness versus preestablished criterion user satisfaction explanation contributes user mental model ai system could tested accuracy comprehension ai system recommendation user mental model may enable decrease user task performance could also measured process contribute user appropriate inappropriate trust ai system xai evaluator using model test developer team xai system evaluation xai program independent government tor naval research laboratory phase 1 laboratory ihmc help prepared evaluation framework team use template explainability model induction technique infer explainable model any model black box deep explanation modified deep learning technique learn explainable feature new approach create suite machine learning technique produce explainable model maintaining high level learning performance interpretable model technique learn structured interpretable causal model performance explainability decision tree deep learning ensemble method random forest learning technique statistical model svms aogs neural net markov model mlns hbns srl crfs bayesian belief net graphical model model experiment learning performance input unit hidden unit output unit 41 42 45 45 19 16 18 16 5 3 31 1 31 1 33 1 31 figure strategy developing explainable model summer 2019 47 deep learning security designing implementing phase 1 evaluation experiment select test problem problem challenge problem area data alytics autonomy apply new ml technique learn explainable model problem evaluate performance learned ml model table 1 combine learned model explanation interface create explainable learning system conduct experiment user perform speciﬁed task using explainable learning system measure explanation effectiveness employing ihmc model explanation process ﬁgure 8 explanation effectiveness measurement category table 1 evaluation include following mental condition 1 without explanation xai system used perform task without providing explanation user 2 explanation xai system used perform task generates explanation every recommendation decision make every action take 3 partial tion xai system used perform task generates only partial ablated explanation sess various explanation feature 4 control baseline nonexplainable system used perform task explainable learning system table 2 summarizes team technical proaches phase 1 test problem deeply explainable ai university california berkeley ucb team including researcher boston university university amsterdam kitware developing ai system human understandable virtue explicit structural interpretation hu et al 2017 provides post hoc park et al 2018 introspective ramanishka et al 2017 explanation ha predictive behavior allows appropriate trust huang et al 2018 key challenge deeply able ai dexai generate accurate explanation model behavior select useful user ucb addressing former creating implicit explicit explanation model plicitly present complex latent representation derstandable way build explicit structure inherently understandable dexai model create repertoire possible explanatory action action generated without any user model called reﬂexive second challenge ucb proposes rational explanation use model user belief deciding explanatory action select ucb also developing explanation interface based innovation informed iterative design principle ucb addressing challenge problem area autonomy dexai demonstrated hicle control using berkeley deep drive data set carla simulator kim canny 2017 strategy game scenario starcraft ii data learning process training data learned function output today cat p not something else succeed fail trust correct error user task training data new learning process explainable model explanation interface tomorrow understand understand not know succeed know fail know trust know erred cat ha fur whisker claw ha feature user task input unit hidden unit output unit 41 42 45 45 19 16 18 16 5 3 31 1 31 1 33 1 31 figure xai concept 48 ai magazine deep learning security analytics dexai demonstrated using visual question answering vqa ﬁltering task example using data set vqa task activity recognition task respectively xview distinct describable moment hendricks et al 2018 causal model explain learning goal charles river analytics cra team including researcher university chusetts brown university generate present causal explanation ml operation causal model explain learning camel proach camel explanation presented user narrative interactive intuitive interface camel includes causal probabilistic programming framework combine representation learning method causal modeling marazopoulou et al 2015 probabilistic programming language pfeffer 2016 describe complex rich phenomenon camel used describe ml system speciﬁc data characteristic inﬂuenced outcome changing factor would affect come generative probabilistic model represented probabilistic programming language naturally press causal relationship well suited task explaining ml system camel probe internal representation ml system discover represents natural domain concept build causal model effect ml system operation conducting experiment domain cepts systematically included removed cra ha applied approach dnns classiﬁcation rl learned us causal probabilistic model infer explanation system prediction tions inference large complex contain many interacting component camel composes explanatory narrative walk user interaction major concept inﬂuence ml system output camel explanation interface based cognitive system engineering design principle established learn model recommend explanation explainable model explanation interface analyst looking item interest massive multimedia data set explainable model explanation interface data analytics classification learning task autonomy reinforcement learning task action explanation ardupilot sitl simulation operator directing autonomous system accomplish series mission classifies item interest large data set learns decision policy simulated mission explains not recommended item analyst decides item report pursue explains behavior review operator decides future task delegate use explanation explain decision multimedia data two truck performing loading activity image army force research lab arducopter arducopter click image flight version arducopter download firmware firmware beta firmware pick provious firmware load custom firmware arducopter arducopter arducopter figure xai challenge problem area summer 2019 49 deep learning security hci technique allows user understand teract explanatory narrative engendering trust automation enabling effective teamwork cra addressing challenge problem area data analytics camel ha demonstrated using pedestrian detection using inria pedestrian data set harradon et al 2018 cra working toward activity recognition task using activitynet autonomy camel ha demonstrated atari game amidar cra working toward strating starcraft ii learning communicating explainable representation analytics autonomy university california los angeles ucla team including researcher oregon state university michigan state university developing interpretable model combine representational paradigm including interpretable dnns compositional graphical model graph model produce explanation three level compositionality causality utility ucla system includes performer executes task multimodal input data explainer explains perception cognitive reasoning cisions user performer output interpretable representation spatial temporal causal parse graph scene perception analytics task planning tonomy compositional probabilistic attributed interpretable grounded dnn feature image video explainer put explanatory parse graph dialogue process chai 2017 localizes relevant subgraph infers user intent system represents explanation three level 1 concept composition represented parse graph fragment depict information aggregated constituent context decision made node uncertainty decision explanation framework system take input current task make recommendation decision action system provides explanation user justifies recommendation decision action user make decision based explanation explainable model explanation interface task decision recommendation decision action explanation xai system measure explanation effectiveness user satisfaction clarity explanation user rating utility explanation user rating mental model understanding individual decision understanding overall model assessment prediction intervene prediction task performance doe explanation improve user decision task performance artificial decision task introduced diagnose user understanding trust assessment appropriate future use trust correctability extra credit identifying error correcting error continuous training figure evaluating explanation effectiveness 50 ai magazine deep learning security conﬁdence level 2 causal counterfactual soning realized extracting causal diagram pgs predict happen certain alternative action taken 3 utility explanation explain system made certain decision ucla addressing xai challenge problem area using common framework representation inference data analytics ucla demonstrated system using network video camera scene understanding event analysis autonomy ucla demonstrated scenario using robot cuting task virtual reality platform autonomous vehicle driving game engine acceptance testing deep adaptive program oregon state university osu developing tool explaining learned agent perform sequential decision making identifying best principle designing explanation user interface osu able agent model employ explainable deep adaptive program xdaps combine adaptive program deep rl explainability xdaps mers create agent writing program clude choice point represent decision automatically optimized via deep rl simulator interaction choice point deep rl attache trained deep decision neural network dnn yield high performance herently unexplainable initial xdap training xact train tion neural network qi li 2017 dnn provide sparse set explanation feature feature encode property dnn decision logic neural network not initially human interpretable address xact enables domain expert attach interpretable description xdap programmer annotate environment reward type cepts automatically embedded dnns annotation concept learning dnn decision explained via scriptions relevant annotation cepts understood via neural network saliency visualization tool osu gating utility saliency computation explaining sequential decision making osu explanation user interface allows user navigate thousand learned agent decision obtain visual natural language nl explanation design based information foraging theory ift allows user efﬁciently drill useful explanatory information any moment assessment rationale learned decision may efﬁciently identify ﬂaws agent decision making improve user trust osu addressing autonomy challenge lem area ha demonstrated xact scenario using strategy game engine pilot study informed explanation user terface design characterizing user navigate game play tend explain game cisions dodge et al 2018 common ground learning explanation palo alto research center parc team including researcher carnegie mellon university army cyber institute university edinburgh university michigan developing interactive sensemaking system explain learned pabilities xai system control simulated unmanned aerial system apr may jun jul aug sep oct nov dec jan feb mar apr may jun jul aug sep oct nov dec jan feb mar apr may jun jul aug sep oct nov dec jan feb mar apr may jun jul aug sep oct nov dec jan feb mar apr may kickoff progress report tech demo eval 1 result eval 2 result eval 3 result final refine test explainable learning system eval 3 deliver software library toolkits summarize current psychological theory explanation develop theoretical model explanation refine test model explanation support system development evaluation deliver final model explanation develop demonstrate explainable learning system eval 1 refine test explainable learning system eval 2 psychological model explanation 1 team explainable learning system 11 developer team evaluator nrl define evaluation framework prepare eval 1 eval 1 analyze result phase 1 technology demonstration phase 2 government evaluation prepare eval 2 eval 2 analyze result prepare eval 3 eval 3 analyze result accept software figure xai program schedule summer 2019 51 deep learning security xai system explanation communicate information us make decision whether understands thing work goal address parc common ground learning explanation cogle user establish mon ground term use tions meaning enabled parc introspective discourse model interleaf learning explaining process performing task natural world ing autonomous system requiring experience create enough knowledge reliable high mance cogle employ encode ai agent task knowledge nize knowledge level element higher lower level model action longer range local effect support framework informs guide learning testing xai system cogle multilayer architecture partition formation processing sensemaking cognitive modeling learning learning layer employ capacity constrained recurrent hierarchical dnns produce abstraction composition state action unmanned aerial system port understanding generalized pattern combine learned abstraction create hierarchical transparent policy match learned system cognitive layer bridge symbolic representation abstraction tions generalized pattern cogle explanation interface support performance review risk assessment training ﬁrst provides map trace unmanned aerial system mission action divide action decision ﬂight path explainable segment second interface tool enable user examine ass tem competency make prediction mission performance cogle demonstrated ardupilot simulator discretized abstract simulation test bed evaluated drone erators analyst evaluation help parc determine best develop priate domain understandable model explainable reinforcement learning carnegie mellon university creating new pline explainable rl enable dynamic machine interaction adaptation maximum team performance effort ha two goal velop new method learning inherently able rl policy develop strategy explain existing policy former carnegie mellon developing method improve model learning rl agent capture beneﬁts training data explainable model explanation interface cp area performer explainable model explanation interface ucb deep learning reflexive rational cra causal model induction narrative generation ucla pattern explanation autonomy osu adaptive program acceptance testing parc cognitive modeling interactive training cmu explainable rl xrl interaction analytics sri deep learning explanation raytheon bbn deep learning argumentation pedagogy utd probabilistic logic decision diagram tamu mimic learning interactive visualization rutgers model induction bayesian teaching new learning process psychological model explanation ihmc explanation quality user satisfaction user mental model trust mistrust appropriate trust appropriate use better performance user comprehension user performance explanation figure xai research team 52 ai magazine deep learning security approach ability visualize plan internal model space integrating beneﬁts approach simplicity higher ultimate performance include method incrementally add state action world model discovering relevant latent formation learn model via training complex optimal control policy learn general dl model directly integrate exploit rigid body physic kolter 2017 learn understandable predictive state representation using recurrent architecture hefny et al 2018 carnegie mellon also developing method explain action plan rl agent observed either online system log involves answering question agent choose particular action ing data responsible choice achieve carnegie mellon developed technique generate nl description agent behavior log detect outlier anomaly carnegie mellon also developed improvement traditional ﬂuence function method dl allowing xrl system precisely identify portion ing set inﬂuence policy outcome carnegie mellon addressing autonomy lenge problem area ha demonstrated xrl several scenario including openai gym atari game autonomous vehicle simulation mobile service robot educational software game deep representation explainable generative adversarial network sri international team including researcher university toronto university guelph university california san diego veloping explainable ml framework modal data analytics generates explanation justiﬁcations decision panied visualization input data used generate inference deep representation explanationxplainable generative adversarial network system employ dnn architecture inspired attentional model visual neuroscience identiﬁes retrieves present evidence user part explanation attentional mechanism provide user mean system probing collaboration user task performance goodness criterion test satisfaction test understanding test performance trust mistrust appropriate trust reliance system assessed receives may initially give way appropriate use enables revise improves assessed assessed engender explanation user mental model involves xai process xai measurement category figure initial model explanation process explanation effectiveness measurement category summer 2019 53 deep learning security us generative adversarial work gans learn understand data creating learning representation atory power gans made explainable using interpretable decoder map unsupervised ters onto representation involves generating visual evidence given text query using generation vicol et al 2018 part interpretable feature human pose bounding box evidence used search queried visual data system present explanation answer based visual concept extracted timodal data input knowledge base query given explanatory question provides tions visual evidence used decision sualizations system inner working explanation interface ensures highly intuitive explanation made possible tional module localize evidence used visual task initial study demonstrate explanation substantially improve user task performance sri addressing data analytics challenge problem area ha demonstrated using vqa multimodal qa task image video data set explainable question answering system raytheon bbn technology team including researcher georgia institute technology mit university texas austin oping system answer unrestricted nl question posed user multimedia data provides interactive explorable explanation derived answer explainable question answering system equas learns explainable dnn model internal structure example individual neuron aligned semantic concept example wheel handlebar zhou et al 2015 allows neural activation within network cision process translated nl explanation example object bicycle ha two wheel handlebar equas also us neural visualization technique highlight input region associated neuron inﬂuenced cisions express explanation equas retains index retrieves case training data support choice rejected alternative recognized ruled using contrastive language visualization example four explanation dalities map key element argument tion interactive pedagogy didactic statement visualization case rejection alternative choice equas explanation interface allows user explore explanation space populated explanation mode enables iterative guided collaborative interaction allowing user drill supporting evidence explanation category raytheon bbn addressing analytics lenge problem area ha demonstrated initial equas capability vqa task image ploring different explanation modality able user understand predict behavior underlying vqa system measure description ml model performance various measure problem area basis ml model given domain understand whether performance improved degraded relative nonexplainable baseline explanation effectiveness explanation goodness feature explanation assessed criterion explanation goodness explanation satisfaction user subjective rating explanation completeness usefulness accuracy satisfaction mental model understanding user understanding system ability predict system new situation user task performance success user performing task system designed support appropriate trust reliance user ability know not trust system recommendation decision table measurement category 54 ai magazine deep learning security team explainable model explanation interface challenge problem uc berkeley post hoc explanation training additional dl model reﬂexive explanation arise model autonomy vehicle control x carla strategy game starcraft ii explicit introspective explanation nmns rational explanation come reasoning user belief analytics visual qa ﬁltering task xview didemo etc reinforcement learning informative rollouts explicit modular agent charles river analytics experiment learned model team explainable causal probabilistic programming model interactive visualization based generation temporal spatial narrative causal probabilistic model autonomy atari starcraft ii analytics pedestrian detection inria activity recognition activitynet ucla interpretable representation spatial temporal causal model scene event interpretation analytics task plan autonomy explanation concept composition causal counterfactual reasoning utility explanation autonomy robot executing daily task vr platform autonomous vehicle driving game engine theory mind representation user belief user mental model agent explanation representation explanation model explanatory parse graph dialogue priority loss explanation analytics network video camera scene understanding event analysis oregon state xdaps combination adaptive program deep learning explainabilty provides visual nl explanation interlace acceptance testing test pilot based ift autonomy strategy based game engine designed support explanation starcraft ii parc architecture learning layer dnns cognitive layer cognitive model explanation layer hci interactive visualization state action policy value autonomy mavsim wrapper ardupilot simulation environment module test pilot reﬁne train system carnegie mellon university new scientiﬁc discipline xrl xrl work new algorithm representation interactive explanation dynamic system autonomy openai gym autonomy electrical grid mobile service robot educational software interaction improve performance sri multiple dl technique based mechanism compositional nmns gans dnn visualization analytics vqa visual gnome movieqa query evidence explains dnn decision generate nl justiﬁcations raytheon bbn semantic labeling dnn neuron comprehensive strategy based argumentation theory analytics vqa image video dnn audit trail construction nl generation class activation mapping dnn visualization utd tplms enables user explore correct underlying model well add background knowledge analytics infer activity multimodal data video text wet lab biology textually annotated cooking scene data set continued following page summer 2019 55 deep learning security tractable probabilistic logic model new deep explainable representation university texas dallas utd team cluding researcher ucla texas indian institute technology delhi developing uniﬁed approach xai using tractable probabilistic logic model tplms tplms family representation clude example decision tree binary decision diagram cutset network sentential decision gram arithmetic circuit tractable markov logic gogate domingo 2016 utd system extends tplms generate tions query result handle continuous variable complex constraint unseen entity compactly represent complex object parse tree list shape enable efﬁcient representation soning time scalable inference system us novel gorithms answer complex explanation query using technique including lifted inference tional inference combination fast increased learning accuracy us discriminative technique deriving algorithm compose nns support vector machine tplms using terpretability bias learn interpretable model approach extended handle situation utd explanation interface display terpretable representation multiple related explanation interactive component allows user debug model suggest alternative explanation utd addressing analytics challenge lem area ha demonstrated system ognizing human activity multimodal data video text textually annotated cooking scene data set transforming deep learning harness interpretability shallow model interactive system texas university tamu team including researcher washington state university developing interpretable dl framework us mimic learning leverage explainable shallow model facilitates domain interpretation visualization interaction mimic learning bridge gap deep shallow model enables interpretability system also mine informative pattern raw data enhance terpretability learning performance system interpretable learning algorithm tract knowledge dnns relevant explanation dl module connects module leveraging interpretability shallow model learning system output displayed user visualization including coordinated integrated view tamu system handle image du et al 2018 text gao et al 2017 data applied xai analytics challenge problem area provides effective interpretation detected inaccuracy diverse source maintaining competitive detection formance tamu system combine model transparency stance explanation interpretability generate nation easily comprehended user system ha deployed multiple task using data twitter facebook imagenet online health care forum news website model explanation optimal selection teaching example rutgers university extending bayesian teaching enable automatic explanation selecting data team explainable model explanation interface challenge problem texas mimic learning framework combine dl model prediction shallow model explanation interactive visualization multiple news using heat map topic modeling cluster show predictive feature analytics multiple task using data twitter facebook imagenet news website interpretable learning algorithm extract knowledge dnns relevant explanation rutgers select optimal trading example explain model decision based bayesian teaching explanation full model substructure user submitted example analytics image processing text corpus vqa movie event table summary explainable learning system developer approach selected phase 1 test problem 56 ai magazine deep learning security subset representative model ference rutgers approach allows explanationof inference any probabilistic generative tive model well inﬂuential dl model yang shafto 2017 rutgers also developing formal theory machine cooperation supporting interactive guided explanation complex compositional model common among core approach building model human learning foster explainability carefully controlled behavioral experiment quantify explainability explanation bayesian teaching input data set probabilistic model inference method return small subset example best explains model inference experiment unfamiliar image show explanation inference category speciﬁc image increase racy people reasoning model vong et al 2018 experiment familiar image category show explanation allow user accurately calibrate trust model prediction explanation complex model facilitated interactive guided explanation exploiting positionality cooperative modiﬁcations ml model rutgers provides generic approach fostering understanding via guided exploration teraction occurs interface expose model structure explains component aspect data rutgers approach ha demonstrated facilitate understanding large text corpus assessed human ability accurately summarize corpus short guided explanation rutgers addressing data analytics challenge problem area ha demonstrated approach image text combination example vqa structured simulation involving temporal causal structure conclusion future work darpa xai program developing evaluating wide variety new ml technique modiﬁed dl technique learn explainable feature method learn structured interpretable causal model model induction technique infer explainable model any model one year xai program initial technology tions result indicate three broad gy merit investigation provide future developer design option covering mance versus explainability trade space developer team xai system evaluated ass value explanation provide localizing contribution speciﬁc technique within trade space acknowledgment author thank xai development team speciﬁcally principle investigator innovative research contribution article trevor darrell ucb brian ruttenberg avi pfeffer cra zhu ucla alan fern osu mark steﬁk parc zico kolter carnegie mellon mohamed amer giedrius burachas sri tional bill ferguson raytheon bbn vibhav gogate utd xia ben hu tamu patrick shafto rutgers robert hoffman ihmc author owe special thanks marisa carrera exceptional technical support xai program tensive editing skill reference kolter z modular entiable rigid body physic engine paper presented neural information processing system deep reinforcement learning symposium long beach ca december chakraborty tomsett raghavendra harborne alzantot cerutti srivastava et al interpretability deep learning model survey sults presented ieee smart world congress 2017 workshop dais 2017 workshop distributed lytics infrastructure algorithm tion federation san francisco ca august dodge penney hilderbrand anderson burnett expert assessing explaining agent behavior strategy game proceeding 2018 chi conference human factor computing system new york association computing machinery du liu song hu x towards planation prediction guided feature version proceeding acm sigkdd international conference knowledge discovery data mining new york association computing machinery gao liu lawley hu x terpretable classiﬁcation framework information traction online healthcare forum journal healthcare engineering 2460174 gogate domingo probabilistic theorem proving communication acm 59 7 harradon druce ruttenberg b causal learning explanation deep neural network via autoencoded activation arxiv preprint ithaca ny cornell university library hefny marinho sun srinivasa gordon recurrent predictive state policy network ceedings international conference machine learning international machine learning society hendricks hu darrell akata z grounding visual explanation presented european conference computer vision eccv munich germany september hoffman miller mueller klein clancey j explaining explanation part 4 deep dive deep net ieee intelligent system 33 3 hoffman klein explaining explanation part 1 theoretical foundation ieee intelligent system 32 3 summer 2019 57 deep learning security hoffman mueller klein explaining explanation part 2 empirical foundation ieee intelligent system 32 4 hu andreas rohrbach darrell saenko learning reason module network visual question answering proceeding ieee ternational conference computer vision new york ieee huang bhatia abbeel dragan establishing appropriate trust via critical state sented annual international conference interaction workshop explainable robot behavior madrid spain october kim canny j interpretable learning car visualizing causal attention proceeding international conference computer sion new york ieee klein explaining explanation part 3 causal landscape ieee intelligent system 33 2 letham rudin mccormick madigan interpretable classiﬁers using rule bayesian analysis building better stroke prediction model annals applied statistic 9 3 marazopoulou maier jensen learning structure causal model relational temporal dependence proceeding conference uncertainty artiﬁcial intelligence association uncertainty artiﬁcial intelligence miller explanation artiﬁcial intelligence insight social science arxiv preprint ithaca ny cornell university library park hendricks akata rohrbach schiele darrell rohrbach multimodal nation justifying decision pointing evidence proceeding ieee conference computer vision pattern recognition new york ieee pfeffer practical probabilistic programming greenwich ct manning publication qi li learning explainable embeddings deep network paper presented nip workshop interpreting explaining visualizing deep learning long beach ca december ramanishka da zhang saenko visual saliency guided caption proceeding ieee conference computer vision pattern recognition new york ieee ra van gerven haselager 2018 explanation method deep learning user value concern challenge arxiv preprint ithaca ny cornell university library ribeiro singh guestrin 2016 trust explaining prediction any classiﬁer proceeding acm sigkdd international conference knowledge discovery data mining new york association computing machinery chai interactive learning quisition grounded verb semantics towards bot communication proceeding annual meeting association computational linguistics vol 1 stroudsburg pa association computation linguistics vicol tapaswi castrejon fidler towards understanding tric situation video ieee conference computer vision pattern recognition new york ieee vong sojitra reyes yang shafto bayesian teaching image category paper sented annual meeting cognitive science society cogsci madison wi july yang shafto explainable artiﬁcial telligence via bayesian teaching paper presented conference neural information processing system workshop teaching machine robot human long beach ca december zhou khosla lapedriza oliva torralba object detector emerge deep scene cnns paper presented international conference learning resentations san diego ca may david gunning program manager darpa formation innovation ofﬁce intergovernmental sonnel act paciﬁc northwest national lab gunning ha 30 year experience developing ai technology prior darpa tour managed pal project produced siri cpof project u army adopted system use iraq nistan gunning wa also program director parc senior research manager vulcan senior vice president set corporation vice president cycorp senior scientist air force research lab gunning hold computer science stanford university cognitive psychology university dayton david aha acting director nrl navy center applied research ai washington interest include goal reasoning xai reasoning machine learning among topic ha coorganized many event topic example xai workshop launched uci repository ml database served aaai councilor lead darpa xai uation team 58 ai magazine deep learning security