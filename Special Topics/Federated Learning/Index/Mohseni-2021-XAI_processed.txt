24 multidisciplinary survey framework design evaluation explainable ai system sina mohseni niloofar zarei texas university usa eric ragan university florida usa need interpretable accountable intelligent system grows along prevalence artificial intelligence ai application used everyday life explainable ai xai system intended explain reasoning behind system decision prediction researcher different discipline work together define design evaluate explainable system however scholar different discipline focus different objective fairly independent topic xai research pose challenge tifying appropriate design evaluation methodology consolidating knowledge across effort end article present survey framework intended share knowledge experience xai design evaluation method across multiple discipline aiming support diverse design goal evaluation method xai research thorough review xai related paper field machine learning sualization interaction present categorization xai design goal evaluation method categorization present mapping design goal different xai user group evaluation method finding develop framework design guideline paired evaluation method close iterative design evaluation cycle multidisciplinary xai team provide summarized table evaluation method recommendation different goal xai research cc concept computing design evaluation method computing methodology learning additional key word phrase explainable artificial intelligence xai interaction hci machine learning explanation transparency acm reference format sina mohseni niloofar zarei eric ragan multidisciplinary survey framework design evaluation explainable ai system acm trans interact intell syst 11 article 24 august 2021 45 page reviewing article wa managed special issue associate editor shixia liu daniel archambault tatiana von landesberger remco changcagatay turkay work article supported darpa xai program nsf award author address mohseni zarei langford building 3137 tamu college station tx 77840 email ragan university florida cse building gainesville fl 32611 email eragan permission make digital hard copy part work personal classroom use granted without fee provided copy not made distributed profit commercial advantage copy bear notice full citation first page copyright component work owned others acm must honored abstracting credit permitted copy otherwise republish post server redistribute list requires prior specific permission fee request permission permission 2021 association computing machinery acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al 1 introduction impressive application artificial intelligence ai machine learning become lent time tech giant like google facebook amazon collected analyzed enough personal data smartphones personal assistant device social medium model individual better people recent negative interference social medium bot political election 91 212 yet another sign susceptible life misuse ai big data 163 circumstance despite tech giant thirst advanced system others suggest holding fully unleashing ai critical application better understood rely demand predictable accountable ai grows task higher sensitivity social impact commonly entrusted ai service hence algorithm transparency essential factor holding organization responsible accountable product service communication information explainable artificial intelligence xai system possible solution toward accountable ai making possible explaining ai process logic end user 72 specifically explainable algorithm enable control oversight case adverse wanted effect biased social discrimination xai system defined intelligent system describes reasoning behind decision prediction ai explanation either explanation form model scription could benefit user many way improving safety fairness relying ai decision increasing impact advanced machine learning system era ha attracted much attention different community interpretability intelligent system ha also studied numerous context 69 167 study personalized agent dation system critical task medical analysis powergrid control ha added importance machine learning explanation ai transparency instance step toward goal legal right explanation ha established european union general data protection regulation gdpr commission current state regulation mainly focused user data protection privacy expected cover algorithmic transparency explanation requirement ai system 67 clearly addressing broad array definition expectation xai requires disciplinary research effort existing community different requirement often drastically different priority area specialization instance research domain machine learning seek design new interpretable model explain model hoc explainers along line different approach researcher visual analytics design study tool method data domain expert visualize complex model study interaction manipulate machine learning model contrast research interaction hci focus need user trust standing machine generated explanation psychology research also study fundamental human understanding interpretability structure explanation looking broad spectrum research xai evident scholar different ciplines different goal mind even though different aspect xai research following general goal ai interpretability researcher discipline use different measure metric evaluate xai goal example numerical analytic method employed chine learning field evaluate computational interpretability human interpretability evaluation commonly primary goal hci visualization munities regard although seems mismatch specific objective designing evaluating explainability interpretability convergence goal beneficial achieving full potential xai end article present survey framework intended share acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai knowledge experience xai design evaluation method across multiple discipline support diverse design goal evaluation method xai research thorough review xai related paper field machine learning visualization hci present categorization interpretable machine learning design goal evaluation method show mapping design goal different xai user group evaluation method finding develop framework design guideline paired evaluation method close iterative design evaluation loop multidisciplinary team provide summarized evaluation method different goal xai research lastly review recommendation xai design evaluation drawn literature review 2 background nowadays algorithm analyze user data affect process million ple matter like employment insurance rate loan rate even criminal justice 35 ever algorithm serve critical role many industry disadvantage result discrimination 44 196 unfair 163 instance recently news feed targeted advertising algorithm social medium attracted much attention aggravating lack information diversity social medium 23 significant part trouble could algorithmic recommender not low user choose recommended item instead present relevant content option address heer 75 suggests use shared representation task augmented machine learning model user knowledge reduce ative effect immature ai autonomous system present case study interactive system integrate proactive computational support interactive system bellotti edward 16 argue intelligent system not act behalf suggest user control system principle support accountability system user transparency provide essential information hidden prevents blind faith 218 key benefit algorithmic parency interpretability include user awareness 9 bias discrimination detection 45 196 interpretable behavior intelligent system 124 accountability user 46 furthermore considering growing body example discrimination legal aspect algorithmic researcher demanding investigating transparency accountability ai law mitigate adverse effect algorithmic 49 145 201 section review research background related xai system broad multidisciplinary perspective end relate summary position derived survey work field auditing inexplicable ai researcher audit algorithm study bias discrimination algorithmic decision making 184 study user awareness effect algorithm 58 auditing algorithm mechanism investigating algorithm functionality detect bias unwanted rithm behavior without need know specific design detail auditing method focus problematic effect result algorithmic system audit algorithm researcher feed new input algorithm review system output behavior researcher generate new data user account help script bot 44 sourcing 73 emulate real data real user auditing process bias detection among multiple algorithm auditing detect algorithm behaves differently another algorithm recent example auditing work eslami et al 59 analyzed user review three hotel booking website study user awareness acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al fig user interacts explainable interface send query interpretable machine learning receive model prediction explanation interpretable model interacts data generate explanation new prediction user query bias online rating algorithm example demonstrate auditing valuable yet intensive process could not scaled easily large number algorithm call new research effective solution toward algorithmic transparency explainable ai along method mentioned supporting transparency machine learning nation also become common approach achieve transparency many application social medium management human worker 116 197 199 xai system illustrated figure 1 able generate explanation describe reasoning behind machine learning decision prediction machine learning explanation enable user understand data processed aim bring awareness possible bias system malfunction example measure user perception justice intelligent binns et al 20 studied explanation system everyday task determining car surance rate loan application approval result highlight importance machine learning explanation user comprehension trust algorithmic system similar work studying knowledge social medium algorithm radar et al 170 ran sourced study see different type explanation affect user belief news feed rithmic transparency social medium platform study measured user awareness correctness accountability evaluate algorithmic transparency found tions caused user become aware system behavior stumpf et al 194 designed experiment investigate meaningful explanation interaction hold user accountable machine learning algorithm show explanation potential method supporting richer collaboration share intelligence recent advancement trend explainable ai research demand wide range goal algorithmic transparency call research across varied application area end review encourages perspective intelligibility transparency goal related survey guideline recent year survey position paper suggesting research direction highlighting challenge interpretable machine learning research 48 78 127 although review limited computer science literature summarize several relevant survey related topic xai across active discipline including social science survey model guideline section add value xai research best knowledge no existing comprehensive survey framework evaluation method explainable machine learning system social science survey research social science particularly important xai system understand people generate communicate understand explanation acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai taking account others thinking cognitive bias social expectation process explaining hoffman mueller klein reviewed key concept explanation intelligent system series essay identify people formulate accept explanation way generate identified purpose pattern causal reasoning 83 84 102 lastly focus deep neural network dnns examine theoretical empirical finding machine learning algorithm 79 work presented conceptual model process explaining xai context 85 framework includes specific step measure goodness explanation user satisfaction understanding explanation user trust reliance xai system effect curiosity search explanation system performance miller 142 suggests close collaboration machine learning researcher space xai social science would refine explainability ai people discus understanding replicating people generate select present explanation could improve interaction instance miller review people generate select explanation involved cognitive bias social expectation paper ing social science aspect xai system include study role algorithmic transparency explanation lawful ai 49 fair accountable algorithmic ce 117 hci survey many hci survey discus limitation challenge ai parency 208 interactive machine learning 6 others suggest set theoretical sign principle support intelligibility intelligent system accountability human user 16 90 recent survey abdul et al 1 presented thorough literature analysis find topic relationship among topic used visualization keywords topic model citation network present holistic view research effort wide range xai related domain privacy fairness intelligent agent system another work wang et al 204 explored theoretical underpinnings human proposed conceptual framework building xai system framework help choose better explanation present backed reasoning theory human cognitive bias focused xai interface design eiband et al 56 present participatory process integration transparency existing intelligent system using explanation another design framework xaid zhu et al 225 present approach facilitating game designer machine learning niques study investigates usability xai algorithm term well support game designer visual analytics survey survey visualization domain follow visual analytics goal understanding interacting machine learning system different visual analytics application 57 180 choo liu 34 reviewed challenge opportunity visual analytics explainable deep learning design recent paper hohman et al 88 provide excellent review categorization visual analytics tool deep learning cation cover various data visualization technique used deep visual analytics application also spinner et al 192 proposed xai pipeline map xai ce iterative workflow three stage model understanding diagnosis refinement operationalize framework designed explainer visual analytics system interactive interpretable machine learning instantiates step pipeline machine learning survey machine learning area guidotti et al 71 present comprehensive review classification machine learning interpretability technique also montavon et al 152 focus interpretability technique dnn model convolutional acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al fig diagram summarizing iterative literature selection review process achieve desired literature investigation breadth depth started 40 paper create reference table added 80 paper upward downward literature investigation improve review breath depth finally added another 80 paper related conference proceeding journal balance reference table neural network cnn zhang et al 221 review research interpretability technique six direction including visualization cnn representation diagnosing technique cnns approach transforming cnn representation interpretable graph building explainable model learning based model interpretability another work gilpin et al 64 review interpretability technique machine learning algorithm egorizes evaluation approach bridge gap machine learning hci community complementing existing work survey provides multidisciplinary categorization design goal evaluation method xai system result surveyed paper propose framework provides design evaluation plan multidisciplinary team designer building xai system unlike eiband et al 56 not make assumption adding transparency existing intelligent interface not limit ation xai system user mental model instead characterize design goal evaluation method compile unified framework multidisciplinary teamwork design framework ha similarity wang et 204 theoretical framework support design goal see section multidisciplinary work extends conceptual framework 1 including design interpretability algorithm part framework 2 pairing evaluation method design step close iterative design evaluation loop 3 survey method conducted survey existing research literature capture organize breadth design goal xai evaluation used structured iterative methodology find research categorize evaluation method presented research article marized figure 2 iterative paper selection process started selecting existing work top computer science conference journal across field hci visualization chine learning however since xai quite fast growing topic also wanted include arxiv preprints useful discussion workshop paper started 40 paper related xai topic across three research field including not limited research interpretable machine learning technique deep learning visualization interactive model visualization machine nation intelligent agent system explainable user interface explanatory debugging algorithmic transparency fairness used selective coding identify 10 main research attribute paper main attribute identified include research discipline social science hci visualization machine learning paper type interface design algorithm design evaluation paper application domain machine learning interpretability algorithmic fairness recommendation system transparency intelligent system intelligent interactive system agent explainable intelligent system acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai agent human explanation human trust machine learning model deep learning decision tree svm data modality image text tabular data explanation type graphical textual data visualization design goal model debugging user reliance bias mitigation evaluation type qualitative computational quantitative targeted user ai novice data expert ai expert evaluation measure user trust task performance user mental model second round collecting xai literature conducted upward downward ature investigation using google scholar search engine add 80 paper reference table narrowed search xai related topic keywords including not ited interpretability explainability intelligibility transparency algorithmic fairness trust mental model debugging machine learning intelligent system information performed axial coding organize literature started discussion proposed design evaluation categorization finally maintain reasonable literature coverage balance number paper category design goal evaluation measure added another 80 paper ence table conference selected xai related paper include not limited following chi iui hcomp sigdial ubicomp aies vi icwsm ijcai kdd aaai cvpr neurips conference journal included trend cognitive science transaction cognitive developmental system cognition journal transaction interactive gent system international journal study transaction visualization computer graphic transaction neural network learning system following review 226 paper categorization xai design goal evaluation od supported reference paper performing design evaluation xai system reference available online research community provide insight beyond discussion document table 2 show digest surveyed paper contains 42 paper design evaluation xai system later section 7 provide series table organize different evaluation method research paper example reference documenting analysis 69 paper total 4 xai terminology familiarize reader common xai concept terminology repeatedly erenced review following four subsection summarize characterization model explanation many related survey 2 207 report 38 200 also provide useful compilation terminology concept comprehensive report instance abdul et al 1 present citation graph diverse domain related explanation including ble intelligent system system software learnability later arrieta et al 11 present thorough review xai concept taxonomy arrive concept ble ai manifold multiple ai principle including model fairness explainability privacy similarly concept safe ai ha reviewed amodei et al 8 interest intelligent application autonomous vehicle 147 table 1 present tions 14 common term related survey topic organizes relation intelligible system transparent ai topic consider transparent ai system class intelligible system therefore property goal previously established intelligible system ideally transferable transparent ai system however challenge limitation ing transparency complex machine learning algorithm raise issue ensuring fairness algorithm not necessarily problematic intelligible system require closer attention research community 1 acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al table table common terminology related intelligible system transparent ai concept category description intelligible system main concept system understandable predictable user transparency explanation 1 16 207 understandability intelligibility intelligible system support user understanding system underlying function 11 123 predictability desired property intelligibility support building mental model system enables user predict system behavior 207 trustworthiness enabling positive user attitude toward system emerges knowledge experience emotion 82 85 reliability supporting user trust rely follow system advice higher performance 82 85 safety desired outcome improving safety reducing user unintended misuse due misperception unawareness 147 transparent ai main concept system provides information process 38 127 interpretable ai inherently model due low complexity machine learning algorithm 151 explainable ai practical approach supporting user understanding complex model providing explanation prediction 204 interpretability ability support user understanding comprehension model process prediction 11 127 explainability desired property ability explain underlying model reasoning accurate user comprehensible explanation 11 127 accountable ai allowing auditing documentation hold organization accountable product service 49 117 fair ai desired outcome enabling ethical fairness analysis model data used process 11 117 main concept shown gray related term main concept listed categorized desired outcome property practical approach explainable ai one particular practical approach intelligible system enable improved transparency note definition interpretation vary across literature table meant serve quick reference description presented table 1 meant introduction term though exact definition interpretation depend usage context research discipline quently researcher different discipline often use term interchangeably disregarding difference meaning 2 perhaps two generic term model transparent model center xai terminology ambiguity term refers complex chine learning model not 127 opposed transparent model simple enough 11 find accurate consistent separate transparency xai system described figure 1 interpretability internal machine learning model specifically table 1 show transparent ai could achieved either interpretable ai explainable ai approach example terminology ambiguity include term interpretability explainability often used synonym field machine learning example phrase interpretable machine learning technique often refers technique generating explanation model dnns 151 another example occasional case using term transparent system explainable system interchangeably hci research 56 others clarify explainability not equivalent transparency doe not require knowing flow bit ai process 49 acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai global local explanation one way classify explanation interpretation scale instance explanation could thorough describing entire machine learning model alternatively could only tially explain model could limited explaining individual input instance global explanation model explanation explanation type describes overall machine learning model work model visualization 130 131 decision rule 113 example planation falling category case interpretable approximation complex model serve model explanation tree regularization 213 recent example regularized plex model learn decision boundary model complexity explanation design main factor used choose different type global explanation contrast local explanation instance explanation aim explain relationship tween specific pair reasoning behind result individual user query type explanation thought le overwhelming novice suited investigating edge case model debugging data local explanation often make use saliency method 14 219 local approximation main model 172 173 saliency method also known attribution map sensitivity map use different approach based method method show feature input strongly influence model prediction local approximation model hand train interpretable model learned main model locally represent complex model behavior interpretable model explainers human interpretability machine learning model inversely proportional model size complexity complex model dnns high performance robustness world application not interpretable human user due large variable space linear regression model decision tree offer better interpretability limited performance data whereas random forest model ensemble hundred decision tree much higher performance le understandable tradeoff model pretability performance led researcher design method explain any machine learning algorithm dnns explainers 134 172 independent algorithm describe model prediction explaining certain decision ha made instead describing whole model however limitation explaining box model explainers uncertainty fidelity explainer discus fidelity explanation section furthermore although hoc explainers generally describe prediction made method lack explaining decision made explain user face complex intelligent system may demand different type explanatory information explanation type may require design review six common type explanation used xai system design explanation demonstrate holistic representation machine learning algorithm explain model work visual representation model graph 113 decision aries 135 common design example explanation however research show user may also able develop mental model algorithm based collection explanation multiple individual instance 133 acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al explanation describe prediction made particular input explanation aim communicate feature input data 172 logic model 113 173 ha led given prediction algorithm type explanation either model agnostic 134 172 model dependent 188 solution explanation help user understand reason specific output wa not output system 202 explanation also called contrastive explanation acterize reason difference model prediction user expected outcome feature importance feature attribution commonly used interpretability technique explanation explanation involve demonstration different algorithmic data change fect model output given new input 29 manipulation input 125 changing model eters 103 different scenario may automatically recommended system chosen exploration interactive user control domain data image text complex machine learning model dnns fewer parameter user directly tune examine trained model compared simpler data tabular data model explanation spell hypothetical adjustment input model would result different output 125 126 output interest technique ate counterfactual explanation considering model structure internal value not part explanation 203 method work actively user curiosity partial conception system allow evolving mental model system iterative testing explanation present user similar instance input generate similar output model also called explanation example explanation pick sample model training dataset similar original input model representation space 30 although popular easy achieve research show based explanation could misleading training datasets lack uniform distribution data 98 explain type machine learning explanation goal reveal new information derlying system survey mainly focus explanation though note research interpretable machine learning ha also studied purpose knowledge transfer object localization error detection 61 162 explanation designed using variety format different user group 216 visual explanation use visual element describe reasoning behind machine learning model tention map visual saliency form saliency heatmaps 190 219 example visual explanation widely used machine learning literature verbal explanation describe machine model reasoning word phrase natural language verbal explanation popular application like explanation decision list 113 form explanation ha also implemented recommendation system 17 77 robotics 176 plainable interface commonly make use multiple modality visual verbal numerical element explanation support user understanding 156 analytic explanation another approach view explore data machine learning model representation 88 alytic explanation commonly rely numerical metric data visualization visual analytics tool also allow researcher review model structure relation parameter plex deep model heatmap visualization 193 graph network 66 hierarchical cision tree visualization commonly used visualize analytic explanation interpretable acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai fig summary categorization xai design goal evaluation measure user group top different system design goal user group bottom common evaluation measure used user group notice similar xai goal different user group require different research objective design method implementation path algorithm recently hohman et al 87 implemented combination visualization ization communicate summarize key aspect model different perspective chromik et al 36 extends idea dark pattern active user interface design 68 machine learning explanation review possible way phrasing explanation implementation interface could deceive user benefit party review negative effect lack user attention explanation formation incorrect mental model even algorithmic anxiety 93 could among consequence deceptive presentation interaction machine learning explanation 5 categorization xai design goal evaluation method ideal xai system able answer user query meet xai concept goal 72 individual research effort focus designing studying xai system respect specific interpretability goal specific user similarly evaluating explanation strate verify effectiveness explainable system intended goal careful review analysis xai goal evaluation method literature recognized following two attribute significant purpose interdisciplinary organization xai design evaluation method design goal first attribute categorization design goal interpretable algorithm explainable interface xai research obtain xai design goal multiple research discipline machine learning data visualization hci better derstand difference various goal xai organize xai design goal three user group ai novice general ai product data expert expert data analytics domain expert ai expert machine learning model designer evaluation measure review evaluation method discus measure used uate machine learning explanation measure include user mental model user trust reliance explanation usefulness satisfaction task performance acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al computational measure review pay attention evaluation measure xai author believe category relatively le explored figure 3 present pairing xai design goal evaluation measure note user group used auxiliary dimension emphasize importance end user system goal overlap xai user group show similarity design evaluation od different targeted user group however similar xai goal different user group require different research objective design method implementation path help rize characterization along example literature table 2 present table xai evaluation literature emphasize importance design goal evaluation measure user type first review detail research focusing xai design goal section 6 including eight goal organized user group review evaluation measure method section 7 including six main measure method collected surveyed literature 6 xai design goal research effort explored many goal xai system kim 48 reviewed multiple priority xai system example including safety ethic user reliance scientific understanding later arrieta et al 11 presented thorough review xai nities different application domain accordingly different design choice explanation type scope level detail affected application domain design goal user type example machine learning expert might prefer highly detailed visualization deep model help optimize diagnose algorithm daily used ai product not expect fully detailed explanation every query personalized agent therefore xai system expected provide right type explanation right group user meaning efficient design xai system according user need level expertise end distinguish xai design goal based designated evaluation subject categorize three general group ai expert data expert ai novice emphasize separation group presented primarily organizational convenience goal not mutually exclusive across group specific priority case dependent any particular project xai design goal also extend broader goal responsible ai improving transparency explainability intelligent system note although overlap method used achieve goal research objective design approach substantially different among distinct research field user group instance even though leveraging interpretable model reduce machine learning model bias research goal ai expert bias mitigation also design goal ai novice avoid adverse effect rithmic respective domain setting however interpretability technique ai expert bias mitigation tool ai novice require different design method ments following subsection review eight design goal xai system organized user group ai novice ai novice refer use ai product daily life no little expertise machine learning system include intelligent application like personalized agent home assistant device social medium website smart system machine learning algorithm serve internal function apis enable specific feature embedded intelligent interface previous research show intuitive interface interaction design enhance user experience system improving comprehension reliance intelligent system 154 regard acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai table tabular summary xai evaluation dimension measure targeted user type design goal novice user data expert ai expert evaluation measure work algorithmic transparency user trust reliance bias mitigation privacy awareness model visualization inspection model tuning selection model interpretability model debugging mental model usefulness satisfaction user trust reliance task performance computational measure herlocker et al 2000 77 kulesza et al 2012 109 lim day 2009 124 stumpf et al 2018 195 bilgic mooney 2005 19 bunt et al 2012 25 gedikli et al 2014 62 kulesza et al 2013 111 lim et al 2009 125 lage et al 2019 112 schmid et al 2016 186 berkovsky et al 2017 17 glass et al 2008 65 haynes et al 2009 74 holliday et al 2016 89 nothdurft et al 2014 158 pu chen 2006 169 bussone et al 2015 26 groce et al 2014 70 myers et al 2006 156 binns et al 2018 20 lee et al 2019 115 rader et al 2018 170 datta et al 2015 44 kulesza et al 2015 108 kulesza et al 2010 110 krause et al 2016 107 krause et al 2017 105 liu et al 2014 131 ribeiro et al 2016 172 ribeiro et al 2018 173 ross 2017 177 adebayo et al 2018 3 samek et al 2017 183 zeiler fergus 2014 219 lakkaraju et al 2016 113 kahng et al 2018 95 liu et al 2018 129 liu et al 2017 130 ming et al 2017 143 pezzotti et al 2018 165 strobelt et al 2018 193 table includes 42 paper represent subset surveyed literature organized two dimension creating yet accurate representation complicated machine learning explanation novice challenging design tradeoff xai system note although overlap among goal ai novice ai expert build interpretable algorithm user group requires different set design method objective studied different research community acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al main design goal ai novice xai system itemized follows algorithmic transparency immediate goal xai comparison inexplicable intelligent help understand intelligent tem work machine learning explanation improve user mental model underlying intelligent algorithm providing comprehensible transparency complex intelligent rithms 208 transparency xai system improve user experience better user understanding model output 123 thus improving user interaction system 108 user trust reliance xai system improve trust intelligent algorithm providing explanation xai system let user ass system reliability calibrate perception system accuracy result user trust algorithm lead reliance system example application xai aim improve user reliance transparent design include recommendation system 17 autonomous system 209 critical system 26 bias mitigation unfair biased algorithmic critical side effect intelligent system bias machine learning ha many source including biased training data feature learning could result discrimination algorithmic 137 machine learning explanation help inspect intelligent system biased example case xai used bias mitigation fairness assessment criminal risk assessment 20 115 loan insurance rate prediction 32 worth mentioning overlap biased mitigation goal ai novice goal dataset bias ai expert section result shared implementation technique however two distinct user group require set xai design goal process privacy awareness another goal designing xai system provide mean ass data privacy machine learning explanation disclose user data used algorithmic example ai application example privacy awareness primarily important include personalized ments using user online advertisement 44 personalized news feed social medium 58 170 addition major xai goal interactive visualization tool also developed help ai novice learn machine learning concept model interacting fied data model representation example educative tool include tensorflow ground 191 teaching elementary neural network concept adversarial playground 157 learning concept adversarial example dnns minor goal cover xai system tives limited extent compared main goal data expert data expert include data scientist domain expert use machine learning analysis research visual analysis tool support interpretable machine learning many way visualizing network architecture trained model training ce machine learning model researcher implemented various visualization design interaction technique understand better improve machine learning model data expert analyze data specialized form domain cybersecurity 18 66 medicine 31 107 text 128 131 satellite image analysis 174 user might expert acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai certain domain area expert general area data science categorization consider user data expert category generally lack expertise technical specific machine learning algorithm instead group user often utilize intelligent data analysis tool visual analytics system obtain insight data notice overlap xai goal different discipline visual analytics tool designed data expert could used model designer data analyst however design need approach xai system may different across research community main design goal data expert user xai system follows model visualization inspection similar ai novice data expert also benefit machine learning interpretability inspect model uncertainty trustworthiness 181 instance machine learning explanation help data expert visualize model 86 inspect problem like bias 4 another important aspect model visualization inspection domain expert identify analyze failure case machine learning model system 144 therefore main challenge system improve model transparency via visualization interaction technique domain expert 216 model tuning selection visual analytics approach help data expert tune machine learning parameter specific data interactive visual fashion 131 terpretability element xai visual analytic system increase data expert ability compare multiple model 5 select right model targeted data example du et al 51 present eventaction event sequence recommendation approach allows user actively select record share desired attribute value case tuning dnn network visual analytics tool enhance designer ability modify network 165 improve training 129 compare different network 211 ai expert categorization ai expert machine learning scientist engineer design machine learning algorithm interpretability technique xai system machine learning interpretability technique either provide model interpretation instance explanation example model interpretation technique include inherently interpretable model 205 deep model simplification 213 visualization model internals 215 instance explanation technique however present feature importance individual instance saliency map image data attention textual data 43 ai engineer also benefit visualization visual analytics tool interactively inspect model internal variable 129 detect architecture training flaw monitor control training process 95 indicates possible overlap among design goal list main design goal ai expert two following item model interpretability model interpretability often primary xai goal ai expert model interpretability allows getting new insight deep model learn pattern data 162 regard various interpretability technique different domain proposed satisfy need explanation 99 188 example yosinski et al 215 created interactive toolbox explore cnn activation layer give intuition cnn work user model debugging ai researcher use interpretability technique different way improve model architecture training process example zeiler fergus 219 present use case visualization filter feature map cnn lead revising training acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al table evaluation measure method used studying user mental model xai system mental model measure evaluation method user understanding model interview 40 20 47 164 questionnaire 99 111 113 124 133 171 model output prediction user prediction model output 96 172 173 model failure prediction user prediction model failure 15 161 therefore model performance improvement another work ribeiro et al 172 used model instance explanation human review explanation improve model performance feature engineering main xai goal ai expert machine learning explanation used purpose including detecting dataset bias 220 adversarial attack detection 61 model failure prediction 146 also visual saliency map attention mechanism used weakly supervised object localization 190 multiple object recognition 12 knowledge transfer 122 technique 7 xai evaluation measure evaluation measure xai system another important factor design process xai system explanation designed answer different interpretability goal hence different measure needed verify explanation validity intended purpose example mental design study common approach perform evaluation ai novice various controlled online crowdsourced study used xai evaluation also case study aim collect domain expert user feedback performing cognitive task analytics tool contrast computational measure designed evaluate accuracy completeness explanation interpretable algorithm section review categorize main evaluation measure xai system algorithm table 2 show list five evaluation measure associated design goal additionally provide summarized xai evaluation measure method extracted literature table mental model following cognitive psychology theory mental model representation user stand system researcher hci study user mental model determine understanding intelligent system various application example costanza et al 40 studied user understand smart grid system kay et al 96 studied user understand adapt uncertainty machine learning prediction bus arrival time context xai explanation help user create mental model ai work machine learning explanation way help user building accurate mental model studying user mental model xai system help verify explanation effectiveness ing algorithm process table 3 summarizes different evaluation method used measure user mental model machine learning model psychology research interaction ha also explored structure type function explanation find essential ingredient ideal explanation better user understanding accurate mental model 97 132 instance lombrozo 133 studied different type explanation help structure conceptual representation order find intelligent system explain behavior research machine learning explanation acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai ha studied user interpret intelligent agent 47 164 algorithm 171 find user expect machine explanation related lim dey 124 elicit type tions user might expect four application specifically study type explanation user demand different scenario system recommendation critical event unexpected system behavior measuring user mental model model failure tion bansal et al 15 designed game participant receive monetary incentive based final performance score although experiment done simple task result indicate decrease user ability predict model failure data model get complicated useful way studying user comprehension intelligent system directly ask intelligent system process analyzing user interview provides valuable information user thought process tal model 110 studying user comprehension kulesza et al 111 studied impact planation soundness completeness fidelity mental model music mendation interface result found explanation completeness broadness significant effect user understanding agent compared explanation soundness example binns et al 20 studied relation machine explanation user perception justice algorithmic different set explanation style user attention expectation may also considered interpretable interface design cycle intelligent system 195 interest developing evaluating explanation ha also led pretable model explainers measure mental model example ribeiro et al 172 evaluated user understanding machine learning algorithm visual explanation showed explanation mitigate human overestimation accuracy image classifier help user choose better classifier based explanation work compared global explanation classifier model instance explanation model found global explanation effective solution finding model ness 173 another paper kim et al 99 conducted crowdsourced study evaluate based explanation understandability addressing understanding model tations lakkaraju et al 113 presented interpretable decision set interpretable classification model measured user mental model different metric user accuracy dicting machine output length user explanation usefulness satisfaction satisfaction usefulness machine explanation also importance ating explanation intelligent system 19 researcher use different subjective objective measure understandability usefulness sufficiency detail ass explanatory value user 142 although implicit method measure user satisfaction 80 erable part literature follows qualitative evaluation satisfaction explanation questionnaire interview example gedikli et al 62 evaluated 10 different tion type user rating explanation satisfaction transparency result showed strong relationship user satisfaction perceived transparency similarly lim et al 125 explore explanation usefulness efficiency interpretable system presenting different type explanation not explanation type measuring user response time another line research study whether intelligible system always appreciated user ha conditional value early work lim dey 124 studied user understanding satisfaction different explanation type four application acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al table user satisfaction measure study method used measuring user satisfaction usefulness explanation xai study satisfaction measure evaluation method user satisfaction interview 25 62 124 125 questionnaire 39 62 112 124 125 expert case study 95 106 128 130 193 explanation usefulness engagement explanation 39 task duration cognitive load 62 112 125 finding show considering scenario involved criticality user want tion explaining process experience higher level satisfaction ceiving explanation similarly bunt et al 25 considered whether explanation always necessary user every intelligent system result show some case cost viewing explanation diary entry like amazon youtube recommendation could weigh benefit study impact explanation complexity user comprehension lage et al 112 studied explanation length complexity affect user response time accuracy subjective satisfaction also observed increasing explanation complexity resulted lowered subjective user satisfaction recent study copper et al 39 also show adding telligibility doe not necessarily improve user experience study expert translator experiment suggests intelligible system preferred expert additional planation not part translator readily available knowledge another work curran et al 41 measured user understanding preference explanation image recognition task ranking coding user transcript provide three type instance explanation participant show although explanation coming model pant different level trust explanation correctness according explanation clarity understandability table 4 summarizes study method used measure user satisfaction usefulness chine learning explanation note primary goal xai system evaluation domain ai expert direct evaluation user satisfaction explanation design design cycle example case study participatory design common approach directly cluding expert user part system design evaluation process user trust reliance user trust intelligent system affective cognitive factor influence positive negative perception system 82 136 initial user trust development trust time studied presented different term swift trust 141 default trust 139 suspicious trust 21 prior knowledge belief important shaping initial state trust however trust confidence change response exploring challenging system edge case 81 therefore user may different feeling trust mistrust different stage experience any given system researcher define measure trust different way user knowledge technical competence familiarity confidence belief faith emotion personal attachment common term used analyze investigate trust 94 136 outcome user trust reliance sured explicitly asking user opinion working system done interview questionnaire example yin et al 214 studied portance model accuracy user trust finding show user trust system wa acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai table evaluation measure method used measuring user trust xai study trust measure evaluation method subjective measure interview 26 28 questionnaire 17 26 28 160 objective measure user perceived system competence 160 169 214 user compliance system 55 user perceived understandability 158 214 affected system stated accuracy user perceived accuracy time similarly nourani et al 160 explored explanation inclusion level meaningfulness would affect user perception accuracy controlled experiment result show whether tions significantly affect perception system accuracy independent actual accuracy observed system usage additionally trust assessment scale could specific system application context xai design purpose instance multiple scale would ass user opinion system reliability predictability safety separately related detailed trust measurement setup presented paper cahour forzy 28 measure user trust multiple trust scale construct trust video recording interview evaluate three mode system presentation also better derstand factor influence trust adaptive agent glass et al 65 studied type question user would like able ask adaptive assistant others looked change user awareness time displaying system confidence uncertainty machine ing output application different degree criticality 10 96 multiple effort studied impact xai developing justified trust user different domain instance pu chen 169 proposed organizational framework generating explanation measured perceived competence user intention return measure user trust another example compared user trust explanation different goal like transparency justification explanation 158 considered perceived understandability measure user trust show transparent explanation help reduce negative effect trust loss unexpected situation studying user trust application berkovsky et al 17 evaluated trust various recommendation interface content selection strategy measured user reliance movie recommender system six distinct construct trust also recommender algorithm eiband et al 55 repeat langer et experiment 114 role placebic explanation explanation convey no information mindlessness user behavior studied whether providing placebic explanation would increase user reliance recommender system result suggest future work explanation intelligent system may consider using placebic explanation baseline comparison machine learning generated explanation also concerned expert user trust bussone et al 26 measured trust found explanation fact lead higher user trust reliance clinical system table 5 summarizes list subjective objective evaluation method measure user trust machine learning system explanation many study evaluate user trust static property however essential take user perience learning time account working complex ai system collecting repeated measure time help understanding analyzing trend user oping trust progression experience instance study holliday et al 89 acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al table evaluation measure method used measuring task performance xai study performance measure evaluation method user performance task performance 70 95 110 125 task throughput 110 113 125 model failure prediction 70 105 194 model performance model accuracy 108 130 165 172 194 model tuning selection 131 evaluated trust reliance multiple stage working explainable system showed level user trust system varied time user gained experience familiarity system note although literature review not find direct measurement trust commonly prioritized analysis tool data machine learning expert user reliance tool tendency continue using tool often considered part evaluation pipeline case study word summarization not meant claim data expert not consider trust rather not find core outcome explicitly measured literature user group task performance key goal xai help successful task involving machine learning system 90 thus task performance measure relevant three group user type example lim et al 125 measured user performance term success rate task completion time evaluate impact different type explanation use generic interface applied various type system weather prediction explanation assist user adjusting intelligent system need kulesza et al 109 study explanation music recommender agent found positive effect explanation user satisfaction agent output well user confidence system overall experience another use case machine learning explanation help user judge correctness tem output 70 105 194 explanation also assist user debugging interactive machine learning program need 108 110 study interacting email classifier tem kulesza et al 108 measured classifier performance show explanatory debugging benefit user machine performance similarly ribeiro et al 172 found user could detect remove wrong explanation text classification resulting training better classifier higher performance explanation quality support goal myers et al 156 designed framework user ask not question expect explanation telligent interface table 6 summarizes list evaluation method measure task performance collaboration model tuning scenario visual analytics tool also help domain expert better perform task providing model interpretation visualizing model structure detail uncertainty machine output allow domain expert diagnose model adjust specific data better analysis visual analytics research ha explored need model interpretation text 92 128 210 multimedia 24 33 analysis task body work demonstrates portance integrating user feedback improve model result example visual analytics tool text analysis topicpanaroma 131 model textual corpus topic graph corporates machine learning feature selection allow user modify graph interactively acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai evaluation procedure ran case study two domain expert public relation manager used tool find set pattern news medium professor lyzed impact news medium public health crisis analysis streaming data automated approach require expert user review model detail tainty better decision making 18 179 example goodall et al 66 presented situ visual analytics system discovering suspicious behavior cyber network data goal wa make anomaly detection result understandable analyst performed multiple case study cybersecurity expert evaluate system could help user improve task formance ahn lin 4 present framework visual analytic design aid fair decision making proposed fairsight visual analytic system achieve different notion fairness ranking decision visualizing measuring diagnosing mitigating bias domain expert using visual analytics tool machine learning expert also use visual analytics find shortcoming model architecture training flaw dnns improve classification prediction performance 130 165 instance kahng et al 95 designed tem visualize neuron activation investigation development machine learning engineer case study interviewed three machine learning engineer data scientist used tool reported key tions similarly hohman et al 86 present interactive system scalably summarizes visualizes feature dnn model ha learned feature interact instance prediction visual analytic system present activation aggregation discover important neuron aggregation identify interaction important neuron case recurrent neural network rnns lstmvis 193 rnnvis 143 tool interpret rnn model natural language processing task another recent paper wang et al 206 presented dnn genealogy interactive visualization tool offer visual mary dnn representation another critical role visual analytics machine learning expert visualize model ing process 224 example visual analytics tool diagnosing training process deep generative model dgmtracker 129 help expert understand training ce visually representing training dynamic evaluation dgmtracker wa conducted two case study expert validate efficiency tool supporting understanding training process diagnosing failed training process computational measure computational measure common field machine learning evaluate ity technique correctness completeness term explaining model ha learned herman 78 note reliance human evaluation explanation may lead persuasive explanation rather transparent system due user preference simplified explanation therefore problem lead argument explanation fidelity model evaluated computational method instead study fidelity hoc explainer refers correctness technique generating true explanation correctness saliency map model prediction lead series computational method evaluate correctness generated explanation consistency explanation result fidelity interpretability technique original model 175 many case machine learning researcher often consider consistency explanation result computational interpretability qualitative result evidence nation correctness 162 215 217 226 example zeiler fergus 219 discus fidelity visualization cnn network validity finding model weakness resulted proved prediction result case comparing new explanation technique existing acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al table evaluation measure method used evaluating fidelity interpretability technique reliability trained model computational measure evaluation method explainer fidelity simulated experiment 172 173 sanity check 101 162 177 215 217 226 comparative evaluation 178 183 model trustworthiness debugging model training 219 evaluation 43 134 149 187 set evaluation method used machine learning data expert either evaluate correctness interpretability method evaluate training quality trained model beyond standard performance metric explanation technique used verify explanation quality 37 134 189 instance ross et al 178 designed set empirical evaluation compared explanation consistency computational cost lime technique 172 comprehensive setup samek et al 183 proposed framework evaluating saliency explanation image data quantify feature importance respect classifier prediction compared three ferent saliency explanation technique image data 190 deconvolution 219 layerwise relevance propagation 13 investigated correlation saliency map quality network performance different image datasets input perturbation contrary kindermans et al 101 show interpretability technique inconsistency ple image transformation hence saliency map misleading define input invariance property reliability explanation saliency method extend similar idea adebayo et al 3 propose three test measure adequacy interpretability technique task sensitive either data model evaluation method include assessing explanation fidelity comparison inherently interpretable model linear regression decision tree example ribeiro et al 172 compared explanation generated lime explainer explanation pretable model created gold standard explanation directly interpretable model sparse logistic regression decision tree used comparison study downside approach evaluation limited generating gold standard interpretable model user simulated evaluation another method perform computational uation model explanation ribeiro et al 172 simulated user trust explanation model defining untrustworthy explanation model tested hypothesis real user would prefer reliable explanation choose better model author later repeated lar user simulated evaluation anchor explanation approach 173 report simulated user precision coverage finding better classifier only looking explanation different approach quantifying explanation quality human intuition ha taken schmidt biessmann 187 defining explanation quality metric based user task completion time agreement prediction another example work lundberg lee 134 compared shap explainer model lime deeplift 189 assumption good model explanation consistent explanation human understand model lertvittayakumjorn toni 118 also present three user task evaluate local explanation technique text classification revealing model havior human user justifying prediction helping human investigate uncertain tions similar idea ha implemented 149 featurewise comparison acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai model explanation provide user annotated benchmark evaluate machine learning instance explanation later poerner et al 166 use benchmark human annotated ground truth comparison word level sentence level explanation evaluation user annotated benchmark valuable considering human meaningfulness explanation though discussion da et al 43 implies machine learning model sual question answering attention model case not seem look region human introduce dataset 42 collection data evaluate attention map generated model human interpretability technique also enable quantitative measure evaluating model ness model fairness reliability safety explanation trustworthiness model represents set goal fairness fair feature learning ity safety robust feature learning example zhang et al 220 present case using machine learning explanation find representation learning flaw caused potential bias training dataset technique mine relationship pair attribute ing inference pattern kim et al 99 presented quantitative testing machine learning model explanation concept activation vector technique model tested specific concept image pattern vector score show model biased toward concept later extended global explanation model sentation learning systematic discovery concept human meaningful important model prediction 63 used experiment evaluate learned concept table 7 summarizes list evaluation method measure fidelity interpretability technique model trustworthiness computational technique 8 xai design evaluation framework variety different xai design goal section 6 evaluation method section 7 review suggests need diverse set technique build xai system however generally insufficient take design practice evaluation method separately holistic actionable vantage require consideration dependency design goal evaluation method inform choose design cycle viously various model guideline design evaluation interactive user interface 7 54 visual analytics system 155 proposed help designer design process however challenge generating useful machine learning explanation presenting appropriate interface aligns target outcome call tidisciplinary workflow framework thus based analysis prior work propose design evaluation framework xai system impetus framework desire organize relate diverse set existing design goal evaluation method unified model framework intended give guidance evaluation measure appropriate use design stage xai system figure 4 summarizes framework nested model xai system design evaluation formulation model layer relates core design goal evaluation interest different research community identified literature review help promote interdisciplinary progress xai research model structured support system design step starting outer layer xai system goal addressing need middle layer explainable interface finally focusing underlying interpretable algorithm innermost layer interpretable algorithm nested model organized design pole focusing design goal choice evaluation pole presenting appropriate evaluation method measure layer framework suggests iterative cycle design evaluation cover algorithmic aspect acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al fig xai design evaluation framework nested model design evaluation explainable machine learning system outer layer demonstrates design goal paired evaluation xai outcome middle layer show explainable user interface visualization design step paired appropriate user understandability satisfaction evaluation measure layer present design evaluation trustworthy interpretable machine learning algorithm xai system section elaborate detail nested framework provide guideline using multidisciplinary xai system design case study example showcase practical example using framework also include case study collaborative design development effort xai system scenario case study multidisciplinary team researcher designed xai system fake news detection not ai expert news analyst daily newsreader design team planned add xai assistant feature news reading sharing website perform fake news detection system design consisted news reading interface equipped xai news assistant news assistant help user identify fake news reviewing news story article presented case result ongoing research done period team eight university researcher hci visualization ai background following subsection framework guideline followed example application case study xai system goal layer designer multidisciplinary team different role priority building xai system suggest beginning system design cycle xai goal layer outer layer figure 4 characterize design goal system expectation specifically step involves acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai identifying purpose explanation choosing explain targeted dedicated application iterative refinement xai goal top pole system tion bottom pole present paired evaluation measure help improve system design organize following guideline xai goal layer beginning system design process team need specify explainability requirement framework layer based evaluation metric explainability requirement intended satisfy overall system goal defined user customer need sometimes regulation law safety standard later evaluation step design cycle team revisit initial xai system requirement sufficiency evaluation result comparison initial design requirement serf key indicator whether stop continue design iteration however since many subjective measure used process important choose appropriate evaluation baseline see section track progress design cycle guideline 1 determine xai system goal identifying establishing clear goal pectations xai system first step design process xai design goal could driven many motivation like improving user experience existing system advancing entific finding 107 120 adhering new regulation 198 section 6 reviewed eight main goal xai system also ordering priority goal case multiple design goal beneficial next step process see guideline 2 given fact different xai user type application interested various design goal important establish goal early design process identify align appropriate design principle pitfall stage pick xai goal without considering group gorithmic limitation user preference context application overshooting xai goal could hurt evaluation result moving forward design process application case study first step case study news curation plication team started identifying main goal expectation xai news assistant design focused novice without any particular expertise xai design goal wa improve user reliance mental model news prediction explainable design team hypothesized would trust rely fake news detection assistant given new xai capable providing explanation news story also team hoped user would able use explanation learn model weakness strength provide feedback developer team guideline 2 decide explain second step xai system design identify explain user order achieve initial xai goal see guideline 1 tem reviewed multiple machine learning interpretability technique explanation type section provide different type information user although design framework discus explanation mechanism driven human reasoning semantics 126 method identify useful explanation online survey terviews user observation 26 138 understand need explained user understand better trust intelligent system preliminary experiment able early step design cycle identify narrow explanation option user order satisfy design goal typical approach evaluating effectiveness ness explanation choice experiment compare user mental model acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al system without explanation component subject lim dey 124 conducted experiment discover type information user interested different application scenario stumpf et al 195 also performed interview identify user perception expectation interpretable interface well finding main decision point user may need explanation another work haynes et al 74 provide review study incorporating different explanation operational ontological mechanistic design rationale explanation intelligent system similarly visual analytics design involves pert interview focus group design path identify design goal 155 design process step involves algorithmic implementation constraint like explained user example global explanation dnn may not feasible comprehensible due large number variable graph additionally research show instance explanation dnn lack completeness may fail present salient feature case 3 constraint decision point could solved focused group storming interview model designer interface designer team therefore design pitfall explanation choice not take limitation interpretability technique account application case study scenario efficient news curation required fake news detection help xai assistant analysis system explain design team decided identify candidate useful impactful explanation option started reviewing machine learning research false information rumor hoax fake news clickbait detection well hci research news feed news search system identify key attribute news veracity checking 148 given target explanatory information needed limit technical detail next user interface designer machine learning designer team discussed candidate explanation choice algorithmic constraint interpretability technique some option explain may not entirely possible given interpretability existing model team needed consider whether tive learning technique could provide better explanation design team would need figure meaningful way explain information wa available model guideline 3 evaluate system outcome evaluation xai system outcome final step evaluation process figure 4 show final system outcome evaluation paired initial design goal outer layer framework main goal stage quantitatively qualitatively ass effectiveness xai system initially lished xai goal clearly evaluation final system outcome could influenced design explainable user interface intermediate layer design interpretable algorithm innermost layer example evaluating newborn interpretable machine learning algorithm output using human subject weak crowdsourced user study may not meaningful productive xai system outcome core computational change still progress could ultimately change entire model interpretability explanation format later also change targeted user could affect evaluation result stage example system designed novice may not satisfy need expert user hence would not improve performance expected evaluation measure layer depend design goal application domain targeted user example evaluation measure final system outcome include user trust 169 reliance system 17 task performance 15 acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai user awareness 96 user understanding personal data 170 effective process evaluation xai outcome break evaluation goal multiple defined measure metric way team perform evaluation study different step using valid method controlled setup example evaluation xai system trustworthiness several factor human trust could measured period user experience xai system addition computational measure section used examine fidelity interpretability method trustworthiness model objective metric possible pitfall evaluation xai system outcome performing evaluation without considering model trustworthiness explanation correctness interpretable model layer see guideline 7 explanation understandability usefulness user interface layer see guideline 5 application case study case study news review curation needed evaluate xai news assistant user would gather news story flagging fake news article evaluation step team ran multiple study novice participant recruited amazon mechanical turk work news reading system note explainable interface interpretable algorithm passed multiple design testing iteration ation step major decision evaluation wa structure duration complexity user task appropriately testing system full range tionality task wa designed question built help collect subjective data addition objective user performance data multiple evaluation measure chosen system outcome including subjective user trust news assistant user agreement rate news assistant veracity news story user accuracy guessing news assistant output qualitative quantitative si user feedback interaction data valuable evaluation system come result analysis evaluation helped team understand effectiveness xai element algorithm interface initial system goal user interface design layer middle layer framework concerned designing evaluating explainable interface visualization user interact xai system interface design nation consists presenting model explanation interpretable algorithm term explanation format interaction design importance layer satisfy design requirement need determined xai system design layer see guideline 2 elegant translation explanation verbal numeric visual explanation need carefully designed satisfying explanation user interface section reviewed multiple type explanation format integrating xai element user interface iterative movement design pole evaluation pole layer present design refinement pursuit desired goal state guideline 4 decide explain identifying candidate explanation format targeted system user group first step deliver machine learning explanation design process account different level complexity length presentation state manent interactivity option depending application user type explanation format interface particularly important improve user understanding acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al underlying algorithm study show detailed complex interactive representation may aim communicate explanation expert user user xai system prefer simplified explanation representation interface 112 user satisfaction face design also another critical factor user engagement interface component 154 additionally interaction design explainable interface allow user communicate system adjust explanation could better support user inspection system 110 search intelligent interface design present multiple design method wireframing prototyping 26 138 could also adapted explainable interface sign also existing design guideline knowledge interface 7 visualization 140 could used stage leverage similar system plainable interface design aside model explanation providing prediction uncertainty also ha identified important factor general data expert user 181 example kay et al 96 presented full design cycle uncertainty visualization face bus arrival time application design process included surveying identify usage requirement developing alternative layout running user testing final evaluation user understanding machine learning output application case study determine explain news classification result end user user interface design team started process reviewing initial system goal explanation type team continued multiple terface sketch matched intended application user task initial design step team tried keep balance interface complexity tion usefulness choosing among available explanation type interpretable machine learning algorithm next top three design mented testing small number participant different arrangement data user task flow explanation format news assistant face experiment stage based user observation interview collect qualitative feedback regarding participant ing subjective satisfaction explanation component interface arrangement interview resulted selection comprehensible conclusive design among available option continue see guideline 5 guideline 5 evaluate explanation usefulness evaluation step used along various measure help ass user understanding xai underlying intelligent algorithm series evaluation explainable interface multiple goal granularity level could performed measure following 1 user understanding explanation 2 user satisfaction explanation 3 user mental model intelligent system evaluation middle layer particularly important due impact xai system come outer layer affected interpretable model output innermost layer cally evaluation measure stage inform well user understand interpretable system however design validity step also may reflected xai outcome evaluation user trust task performance note user understanding xai system could limited part system rather entire system similarly understanding may limited subspace scenario rather possible scenario acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai three evaluation measure introduced step could used multiple iterative cycle improve overall explainable interface design example saket et al 182 study user standing visualization encoding effectiveness interactive graphical encoding hand user satisfaction explanation type format depends factor geted application criticality cognitive load 48 evaluating user mental model also effective way measure usefulness explainable interface table 3 4 present list measure evaluating explainable interface step choice baseline another important factor evaluating explainable interface typically combination qualitative quantitative analysis used measure effect explanation component comparison system compare multiple explanation type however choice bic explanation ha proposed evaluation baseline accurate measurement explanation content 55 case expert review evaluation domain expert mental model commonly involves comparison ai expert mental model description model section review common choice baseline xai ation study approach update explanation component interface require assessment impact user experience understandability however metric depth evaluation vary evaluation cycle team narrow specific need finally possible evaluation pitfall explainable interface going broad measure xai outcome see guideline 3 rather focusing narrower scope explanation component interaction application case study case study interface designer started evaluation candidate explanation component series small study design study participant could experience different explanation design one session next analyzed quantitative qualitative data collected choose candidate design route improve interface plainable component discussion machine learning team also helped find source limitation interpretability technique could possibly affect user isfaction initial cycle revision collected round external internal expert review update study methodology data collection detail according project progress interpretable algorithm design layer innermost layer framework involves designing interpretable algorithm able generate explanation user last design step xai system framework choice interpretability technique design pole generate outlined explanation type however evaluating generated explanation evaluation pole first evaluation step evaluation explainable interface ideally interpretability technique generate explanation accordance requirement explainable interface design step see guideline 4 however choice interpretability technique depends domain carry implementation limitation example shallow model desired high interpretability model typically not perform well case complex data like image text hand highly accurate prediction model dnns random forest model require algorithm generate explanation approach also ha limitation choice explanation type need completeness 3 fidelity 172 validation compared original model show machine learning designer not only consider acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al tradeoff model interpretability performance also consider fidelity explainer model suggest two following design evaluation guideline layer guideline 6 design interpretability technique designing interpretable algorithm start choice machine learning model shallow machine learning model linear model decision tree intrinsic interpretability due low number variable model simplicity complex model random forest dnn explainer technique see section needed generate explanation however choice machine learning model shallow deep bounded model performance data domain secondly explainer technique certain limitation explanation type portance choosing right combination model explainer impact providing useful see guideline 4 trustworthy explanation machine learning research ha proposed various explainers generate tions feature attribution 99 134 explanation rule list 119 205 else explanation similar training instance 98 135 sensitivity analysis 219 explanation type however despite substantial research interpretable machine learning niques core issue model explanation difference machine learning model logic human receiver 100 223 application case study fake news detection case study explainable terface design team previously discussed candidate explanation choice machine learning design team see guideline 2 4 therefore final review explanation assessment implementation limitation performed step example removing feature saliency map normalizing attribution score resolving contradicting explanation ensemble model primary implementation bottleneck resolved step specifically decision point tradeoff clarity faithfulness planation team decided using heuristic filter eliminate feature low attribution score sake presentation simplicity guideline 7 evaluate model trustworthiness evaluating interpretable machine ing first evaluation step framework due impact outer layer evaluation measure high significance evaluation step stem possibility any unreliability interpretability inner layer propagate outer layer unintended error propagation may lead problematic design decision well misleading evaluation result discus two main evaluation goal innermost layer 1 evaluating model trustworthiness 2 evaluating explainer fidelity first evaluation goal aim utilize interpretability technique debugging tool lyze model trustworthiness learning concept beyond general performance measure 99 example model trustworthiness validation include evaluating model reliability financial risk assessment 60 model fairness social influencing application 220 model safety intended functionality 22 researcher also proposed various regularization technique enhancing trustworthy feature learning machine learning model 76 178 next second evaluation goal target fidelity explainer technique model research acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai show different interpretability technique inconsistency ing 3 evaluating explanation trustworthiness verify explainer fidelity term well represents model see section application case study case study paid careful attention qualitative viewing model explanation design iteration initial qualitative review model explanation led dataset cleaning heuristic search aimed removal mislabeled example unrelated news article improvement model performance wa achieved dataset cleaning first round evaluation explainable interface see guideline 5 team fied negative effect keyword explanation low attention score team decided using lower threshold visualizing attention map reduce clutter noisy explanation finally one round xai outcome evaluation see guideline 3 analysis user mental model revealed dataset balance fake news true news wa causing bias model model wa usually confident predicting fake news true news 9 discussion review discussed multiple xai design goal evaluation measure appropriate various targeted user type table 2 present categorization selected existing design uation method organizes literature along three perspective design goal evaluation method targeted user xai system categorization revealed necessity ciplinary effort designing evaluating xai system address issue proposed design evaluation framework connects design goal evaluation method end xai system design presented model figure 4 guideline section discus consideration xai designer benefit body knowledge xai tem design evaluation following recommendation support promote different layer proposed design evaluation framework well pairing design goal evaluation method essential use appropriate measure evaluating effectiveness design element common pitfall choosing evaluation measure xai system evaluation sure sometimes used multiple design goal simple solution address issue distinguish measurement using multiple scale capture different attribute evaluation target example concept user trust consists multiple construct 28 could measured separate scale questionnaire interview see section user satisfaction measurement could also designed various attribute ity explanation usefulness explanation sufficiency detail 85 target specific explanation quality see section efficient way pair design goal appropriate evaluation measure balance ferent design method evaluation type iterative cycle design managing tradeoff qualitative quantitative method design process allow designer take advantage different approach needed example focus group interview vide detailed feedback user mental model 109 remote measurement highly valuable due scalability collected data even though provide le tail drawing conclusion 112 thus one successful approach could start multiple acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al prototyping formative study collecting qualitative measure earlier stage design xai system goal layer framework continue study quantitative measure later stage interpretable model interface evaluation framework overlap among design goal categorization xai system chose two main dimension organize xai system design goal evaluation measure section xai design goal based goal extracted surveyed paper since xai design goal marily derived targeted user group note overlap among goal exist across discipline instance overlap goal algorithmic transparency novice user hci research model visualization data expert visual analytics pretability technique ai expert machine learning research overlapping similar goal studied different objective across three research discipline leading diverse set design requirement implementation path example designing xai system ai novice requires process step build explainable interface cate model explanation whereas designing new interpretability technique ai expert ha different set computational requirement another example overlap xai goal goal model visualization inspection data expert model debugging ai expert different set tool requirement used address different research objective address overlap xai goal among research discipline used xai user group auxiliary dimension organize xai goal topic section 6 emphasize diversity diverse research objective three user group chosen organize research objective effort hci ai novice visual analytics data expert machine learning ai expert research field additionally described framework three user group prioritize design objective design process xai system rather absolute separation design goal example objective priority xai system design algorithmic bias mitigation domain expert law firm certainly different model training tuning tool ai expert however following multidisciplinary design framework design team translate xai system goal design objective explainable interface machine learning technique improve design process different layer therefore example design team focus diverse interface design interpretability technique objective achieve primary xai goal bias mitigation domain expert note specific any particular system determine priority different objective system evaluation time important aspect evaluating complex ai xai system take user learning account learnability even critical measuring mental model user trust system user learns get familiar system time continued tion system brings importance repeated temporal data capture contrast static measurement xai evaluation holliday et al 89 present example multiple trust assessment user study measured user trust regular interval study capture change user trust user interacts system result indicate xai system outperformed counterpart maintaining user trust time measurement also referred dynamic measurement allow designer monitor explanation usability effectiveness various context situation 50 185 instance acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai zhang et al 222 explore effect intelligent system explanation user trust calibration experiment observe significant effect calibration trust model tion confidence score wa shown participant another example study nourani et al 159 controlled whether user early experience explainable activity recognition system better worse model output first impression significantly affected task mance user confidence understanding system work study news review task mohseni et al 150 identified different user profile change trust time trust namics working assistance explainable fake news detector analysis result revealed significant effect machine learning explanation user trust dynamic evaluation xai system also allow designer estimate valuable user perience factor system system racy 110 transparency 171 example measure explanation usability depend building user trust system interpretability information provided explanation time reasoning mental strategy may change user create new potheses system functionality therefore essential also consider user mental model trust extended study evaluate aspect xai system another use case measurement evaluate effect intelligent system behavior scenario mean although controlled study setup balanced set input example present system user scenario user may face alteration system performance interaction system measurement identify user unjust trust system due limited biased set teractions system example context autonomous vehicle kraus et al 104 presented model trust calibration presented study trust dynamic early phase user interaction system result indicate effect automation steady increase user trust well effect user priori information eliminating decrease trust case system malfunction evaluation ground truth research xai system study various goal different measure across multiple domain breadth xai research make challenging interpret transfer finding one task domain another knowing key factor interpreting implication evaluation result essential aggregate finding across domain discipline important factor ing xai evaluation result comparing result among multiple study choice ground truth following review common choice ground truth computational evaluation method experiment often take form controlled study examine effect machine learning explanation control group comparison baseline group setup choice baseline could affect result implication significance review paper space xai evaluation show majority study design use no tion condition baseline condition measure effectiveness model explanation explanation group example baseline include approach remove model tions related component feature form interface baseline condition 103 160 work et al 168 also included no ai baseline measure participant performance without help model prediction another way compare effect explanation type complexity study condition without no explanation baseline instance lage et al 112 present study evaluate effect explanation ity participant comprehension performance used linear logistic regression acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al estimate effect explanation complexity participant normalized response time response accuracy subjective task difficulty rating though mentioned study controlled experiment may still unaccounted human behavioral implication due difference complex process explaining worthy consideration langer et al 114 present experiment placebic explanation show ple mindless behavior facing explanation action simple setup study showed asking request inclusion explanation justification increased user ness comply even explanation convey no meaningful information recently eiband et al 55 proposed using placebic explanation instead no explanation condition baseline xai human subject study therefore using even randomly generated nation baseline condition could potentially counteract participant positive tendency toward explanation improve study result considering approach commonly accepted computational technique tively evaluating instance explanation create ground truth based input feature semantically contribute target class example image segmentation map annotation object image used evaluate model generated saliency map weakly supervised object localization task 121 mohseni ragan 149 proposed baseline evaluation machine learning explanation baseline provides feature attribution map higher level granularity compared object segmentation map similarly annotation used explanation ground truth text classification domain 53 le accurate mean ture attribution like bounding box image datasets used quantitative evaluation saliency map instance du et al 52 evaluated saliency map generated cnn model calculating pixelwise iou intersection union bounding box bounding box role user interaction xai another important consideration designing xai system leverage user interaction better support system understandability benefit interactive system design previously explored topic interactive machine learning 6 7 novice ai data expert also benefit interactive visual tool improve model task performance 57 section discus multiple example interaction design support user understanding underlying model focusing interactive design system ai novice amershi et al 6 reviewed multiple case study demonstrate effectiveness interactivity tight coupling tween algorithm user emphasize interactive machine learning process allow user instantly examine impact action adapt next query improve outcome interaction allow user test various input learn model creating explanation 204 particularly cycle trial error help novice understand machine learning model work steer model improve result context xai cai et al 29 present study user draw image see whether image recognition algorithm correctly recognize intended sketch system study allows interactive explore algorithm work addition system provides explanation case algorithm fails correctly classify drawing another approach allow user control tune algorithmic parameter achieve better result example kocielnik et al 103 present study user able freely control detection sensitivity ai assistant result showed significant effect user perception control acceptance acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai visual analytics tool also support model understanding expert user interaction algorithm example include allowing data scientist model expert interactively plore model representation 86 analyze model training process 129 detect learning 27 also embedded interaction technique support exploration large deep learning network instance hohman et al 86 present multiple interactive feature select filter neuron zoom pan feature representation support ai expert ing reviewing trained model generalization extension framework framework extendable compatible existing interface interaction design guideline example amershi et al 7 propose 18 design guideline teraction design guideline based review large number design recommendation source systematically validated guideline multiple round evaluation 49 design practitioner 20 product design guideline vide detail within user interface design layer framework section guide development appropriate user interaction model output interaction work dudley kristensson 54 present review characterization user interface design principle interactive machine learning system propose structural breakdown active machine learning system present six principle support system design work also benefit framework contributing practice interactive machine learning design xai system goal layer section user interface design layer section standpoint evaluation method mueller klein 153 discus common usability test not address intelligent tool software replicates human intelligence suggest new solution needed allow user experience tool strength weakness likewise nested framework point potential error propagation inner er interpretable algorithm design outer layer system outcome xai system evaluation pole iterative layer nested model age expert review system outcome evaluation explainable interface computational evaluation machine learning algorithm limitation framework framework provides basis xai system design interdisciplinary teamwork described case study example validate improve framework presented case study serf practical example using framework multidisciplinary collaborative xai design development effort use case result ongoing research done team eight university researcher diverse background lesson learned pitfall implementation case study incorporated presented design guideline however no framework perfect entirely comprehensive acknowledge validity usefulness framework proven practice case study future work plan run multiple validation case study examine practicality usefulness framework moreover framework ha common limitation many multidisciplinary design work light specific detail step rather contributing detailed guideline framework layer framework intended pave path efficient collaboration among within different team essential xai system design given inherently terdisciplinary nature field higher level freedom allows extendability design guideline see discussion section integrate tailored approach specific domain additionally diversity design goal evaluation method layer acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al help maintain balance attention design team different aspect xai system 10 conclusion reviewed research organize multiple xai design goal evaluation measure table 2 present categorization selected existing design evaluation method nizes literature along three perspective design goal evaluation method targeted user xai system provide summarized table evaluation method mendations different goal xai research categorization revealed necessity interdisciplinary effort designing evaluating xai system want draw attention related resource social science facilitate extent social cognitive aspect explanation address issue proposed design evaluation framework connects design goal evaluation method xai system design presented model series guideline hope framework drive discussion interplay design evaluation explainable artificial intelligent system although presented framework organized provide guideline multidisciplinary fort build xai system not meant offer aspect interface interaction design development interpretable machine learning technique lastly briefly discussed additional consideration xai designer benefit body knowledge xai system design evaluation acknowledgment author would like thank anonymous reviewer helpful comment earlier sion manuscript view conclusion paper author not interpreted representing any funding agency reference 1 ashraf abdul jo vermeulen danding wang brian lim mohan kankanhalli trend trajectory explainable accountable intelligible system hci research agenda proceeding 2018 chi conference human factor computing system acm 582 2 amina adadi mohammed berrada peeking inside survey explainable artificial ligence xai ieee access 6 2018 3 julius adebayo justin gilmer michael muelly ian goodfellow moritz hardt kim sanity check saliency map advance neural information processing system 4 yongsu ahn lin fairsight visual analytics fairness decision making ieee transaction visualization computer graphic 26 1 2019 5 eric alexander michael gleicher comparison topic model ieee transaction tion computer graphic 22 1 2015 6 saleema amershi maya cakmak william bradley knox todd kulesza power people role human interactive machine learning ai magazine 35 4 2014 7 saleema amershi dan weld mihaela vorvoreanu adam fourney besmira nushi penny collisson jina suh shamsi iqbal paul bennett kori inkpen et al guideline interaction proceeding 2019 chi conference human factor computing system acm 3 8 dario amodei chris olah jacob steinhardt paul christiano john schulman dan man concrete lem ai safety 9 mike ananny kate crawford seeing without knowing limitation transparency ideal plication algorithmic accountability new medium society 20 3 2018 10 stavros antifakos nicky kern bernt schiele adrian schwaninger towards improving trust aware system displaying system confidence proceeding international conference human puter interaction mobile device service acm 11 alejandro barredo arrieta natalia javier del ser adrien bennetot siham tabik alberto barbado salvador garca sergio daniel molina richard benjamin et al explainable artificial intelligence acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai xai concept taxonomy opportunity challenge toward responsible ai information fusion 58 2020 12 jimmy ba volodymyr mnih koray kavukcuoglu multiple object recognition visual attention 13 sebastian bach alexander binder grgoire montavon frederick klauschen mller wojciech samek explanation classifier decision relevance propagation plo one 10 7 2015 14 david baehrens timon schroeter stefan harmeling motoaki kawanabe katja hansen mller explain individual classification decision journal machine learning research 11 june 2010 15 gagan bansal besmira nushi ece kamar walter lasecki daniel weld eric horvitz beyond racy role mental model team performance proceeding aaai conference human computation crowdsourcing vol 7 16 victoria bellotti keith edward intelligibility accountability human consideration system interaction 16 2001 17 shlomo berkovsky ronnie taib dan conway recommend user trust factor movie mender system proceeding international conference intelligent user interface iui 17 acm new york ny 18 daniel best alex endert daniel kidwell 2014 7 key challenge visualization cyber network defense proceeding workshop visualization cyber security acm 19 mustafa bilgic raymond mooney explaining recommendation satisfaction promotion beyond personalization workshop iui vol 5 153 20 reuben binns max van kleek michael veale ulrik lyngs jun zhao nigel shadbolt 2018 reducing human percentage perception justice algorithmic decision proceeding 2018 chi conference human factor computing system acm 377 21 philip bobko alex barelka leanne hirshfield construct suspicion model research agenda automated information technology context human factor 56 3 2014 22 mariusz bojarski anna choromanska krzysztof choromanski bernhard firner larry ackel ur muller phil yeres karol zieba visualbackprop efficient visualization cnns autonomous driving 2018 ieee international conference robotics automation icra 18 ieee 23 engin bozdag jeroen van den hoven breaking filter bubble democracy design ethic mation technology 17 4 2015 24 nicholas bryan gautham mysore efficient posterior regularized latent variable model interactive sound source separation international conference machine learning 25 andrea bunt matthew lount catherine lauzon explanation always important study deployed intelligent interactive system proceeding 2012 acm international conference intelligent user interface acm 26 adrian bussone simone stumpf dympna sullivan role explanation trust reliance clinical decision support system international conference healthcare informatics ichi 15 ieee 27 angel cabrera epperson fred hohman minsuk kahng jamie morgenstern duen horng chau fairvis visual analytics discovering intersectional bias machine learning ieee conference visual analytics science technology vast 19 28 batrice cahour forzy doe projection use improve trust exploration example cruise control system safety science 47 9 2009 29 carrie cai jonas jongejan jess holbrook effect explanation machine learning interface proceeding international conference intelligent user interface 30 carrie cai emily reif narayan hegde jason hipp kim daniel smilkov martin wattenberg fernanda viegas greg corrado martin stumpe et al tool coping imperfect algorithm medical proceeding 2019 chi conference human factor computing system 31 rich caruana yin lou johannes gehrke paul koch marc sturm noemie elhadad intelligible model healthcare predicting pneumonia risk hospital readmission proceeding acm sigkdd international conference knowledge discovery data mining acm 32 jiahao chen nathan kallus xiaojie mao geoffry svacha madeleine udell fairness unawareness assessing disparity protected class unobserved proceeding conference fairness accountability transparency acm acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al 33 jaegul choo hanseung lee jaeyeon kihm haesun park ivisclassifier interactive visual analytics system classification based supervised dimension reduction 2010 ieee symposium visual analytics science technology vast 10 ieee 34 jaegul choo shixia liu visual analytics explainable deep learning ieee computer graphic cation 38 4 2018 35 alexandra chouldechova fair prediction disparate impact study bias recidivism prediction struments big data 5 2 2017 36 michael chromik malin eiband sarah vlkel daniel buschek dark pattern explainability transparency user control intelligent system iui workshop 37 lingyang chu xia hu juhua hu lanjun wang jian pei exact consistent interpretation piecewise linear neural network closed form solution proceeding acm sigkdd international conference knowledge discovery data mining 38 clinciu helen hastie survey explainable ai terminology proceeding workshop interactive natural language technology explainable artificial intelligence 19 39 sven copper jan van den bergh kris luyten karin coninx iulianna van der tom vanallemeersch vincent vandeghinste intellingo intelligible translation environment proceeding 2018 chi conference human factor computing system acm 524 40 enrico costanza joel fischer james colley tom rodden sarvapali ramchurn nicholas jennings laundry agent field trial future smart energy system home proceeding sigchi conference human factor computing system acm 41 william curran travis moore todd kulesza wong sinisa todorovic simone stumpf rachel white margaret burnett towards recognizing cool end user help computer vision recognize subjective attribute object image proceeding 2012 acm international conference intelligent user interface acm 42 abhishek da harsh agrawal lawrence zitnick devi parikh dhruv batra human attention visual question answering human deep network look region conference empirical method natural language processing emnlp 16 43 abhishek da harsh agrawal larry zitnick devi parikh dhruv batra human attention visual question answering human deep network look region computer vision image understanding 163 2017 44 amit datta michael carl tschantz anupam datta automated experiment ad privacy setting ceedings privacy enhancing technology 2015 1 2015 45 nicholas diakopoulos investigation black box tow center digital journalism 2014 46 nicholas diakopoulos enabling accountability algorithmic medium transparency constructive ical lens transparent data mining big small data springer 47 jonathan dodge sean penney andrew anderson margaret burnett xai nation ift reveals iui workshop 48 finale kim towards rigorous science interpretable machine learning 49 finale mason kortz ryan budish christopher bavitz samuel gershman david brien stuart shieber jim waldo david weinberger alexandra wood accountability ai law role explanation berkman center research publication forthcoming 2017 50 james doyle michael radzicki scott tree measuring change mental model complex dynamic system complex decision making springer 51 fan du catherine plaisant neil spring kenyon crowley ben shneiderman eventaction visual ic approach explainable recommendation event sequence acm transaction interactive intelligent system tiis 9 4 2019 52 mengnan du ninghao liu qingquan song xia hu towards explanation prediction guided feature inversion proceeding acm sigkdd international conference knowledge discovery data mining 53 du liu yang hu learning credible deep neural network rationale regularization 2019 ieee international conference data mining icdm 19 54 john dudley per ola kristensson review user interface design interactive machine learning acm transaction interactive intelligent system tiis 8 2 2018 8 55 malin eiband daniel buschek alexander kremer heinrich hussmann impact placebic tions trust intelligent system extended abstract 2019 chi conference human factor computing system acm acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai 56 malin eiband hanna schneider mark bilandzic julian mareike haug heinrich hussmann bringing transparency design practice international conference intelligent user interface iui 18 acm new york ny 57 endert ribarsky turkay wong ian nabney daz blanco rossi state art integrating machine learning visual analytics computer graphic forum vol wiley online library 58 motahhare eslami aimee rickman kristen vaccaro amirhossein aleyasen andy vuong karrie karahalios kevin hamilton christian sandvig always assumed really close reasoning visible algorithm news feed proceeding annual acm conference human factor computing system acm 59 motahhare eslami kristen vaccaro karrie karahalios kevin hamilton 2017 careful thing worse appear understanding biased algorithm user behavior around rating platform international aaai conference web social medium 60 raquel juan manuel enhancing accuracy interpretability ensemble strategy credit risk assessment decision forest proposal expert system application 42 13 2015 61 ruth fong andrea vedaldi interpretable explanation black box meaningful perturbation proceeding ieee international conference computer vision 62 fatih gedikli dietmar jannach mouzhi ge explain comparison different explanation type recommender system international journal study 72 4 2014 63 amirata ghorbani james wexler james zou kim towards automatic explanation advance neural information processing system 64 leilani gilpin david bau ben yuan ayesha bajwa michael specter lalana kagal explaining planation overview interpretability machine learning 2018 ieee international conference data science advanced analytics dsaa 18 ieee 65 alyssa glass deborah mcguinness michael wolverton toward establishing trust adaptive agent proceeding international conference intelligent user interface acm 66 john goodall eric ragan chad steed joel reed david richardson kelly huffer robert bridge jason laska situ identifying explaining suspicious behavior network ieee transaction visualization computer graphic 25 1 2018 67 bryce goodman seth flaxman european union regulation algorithmic right ai magazine 38 3 2017 68 colin gray yubo kou bryan battle joseph hoggatt austin toombs dark pattern side ux design proceeding 2018 chi conference human factor computing system acm 534 69 shirley gregor izak benbasat explanation intelligent system theoretical foundation cation practice management information system quarterly 23 4 1999 2 70 alex groce todd kulesza chaoqiang zhang shalini shamasunder margaret burnett wong simone stumpf shubhomoy da amber shinsel forrest bice et al only possible oracle effective test selection end user interactive machine learning system ieee transaction software engineering 40 3 2014 71 riccardo guidotti anna monreale salvatore ruggieri franco turini fosca giannotti dino pedreschi survey method explaining black box model acm computing survey csur 51 5 2018 93 72 david gunning explainable artificial intelligence xai defense advanced research project agency darpa 2017 73 aniko hannak piotr sapiezynski arash molavi kakhki balachander krishnamurthy david lazer alan mislove christo wilson measuring personalization web search proceeding international conference world wide web acm 74 steven haynes mark cohen frank ritter design explaining intelligent agent international journal study 67 1 2009 75 jeffrey heer agency plus automation designing artificial intelligence interactive system proceeding national academy science u 116 6 2019 76 lisa anne hendricks kaylee burn kate saenko trevor darrell anna rohrbach woman also snowboard overcoming bias captioning model european conference computer vision springer 77 jonathan herlocker joseph konstan john riedl explaining collaborative filtering recommendation proceeding 2000 acm conference computer supported cooperative work acm 78 bernease herman promise peril human evaluation model interpretability acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al 79 robert hoffman tim miller shane mueller gary klein william clancey explaining explanation part 4 deep dive deep net ieee intelligent system 33 3 2018 80 robert hoffman theory concept measure policy metric macrocognition metric scenario crc press 81 robert hoffman john hawley jeffrey bradshaw myth automation part 2 some human consequence ieee intelligent system 29 2 2014 82 robert hoffman matthew johnson jeffrey bradshaw al underbrink trust automation ieee intelligent system 28 1 2013 83 robert hoffman gary klein explaining explanation part 1 theoretical foundation ieee intelligent system 32 3 2017 84 robert hoffman shane mueller gary klein explaining explanation part 2 empirical foundation ieee intelligent system 32 4 2017 85 robert hoffman shane mueller gary klein jordan litman metric explainable ai challenge prospect 86 fred hohman haekyu park caleb robinson duen horng polo chau summit scaling deep learning interpretability visualizing activation attribution summarization ieee transaction visualization computer graphic 26 1 2019 87 fred hohman arjun srinivasan steven drucker telegam combining visualization verbalization interpretable machine learning ieee visualization conference vi 19 88 fred matthew hohman minsuk kahng robert pienta duen horng chau visual analytics deep learning interrogative survey next frontier ieee transaction visualization computer graphic 25 8 2018 89 daniel holliday stephanie wilson simone stumpf user trust intelligent system journey time proceeding international conference intelligent user interface acm 90 kristina hk step take intelligent user interface become real interacting computer 12 4 2000 91 philip howard bence kollanyi bot strongerin brexit computational propaganda referendum 92 yuening hu jordan brianna satinoff alison smith interactive topic modeling machine learning 95 3 2014 93 shagun jhaver yoni karpfen judd antin algorithmic anxiety coping strategy airbnb host proceeding 2018 chi conference human factor computing system acm 421 94 jian ann bisantz colin drury foundation empirically determined scale trust automated system international journal cognitive ergonomics 4 1 2000 95 minsuk kahng pierre andrew aditya kalro duen horng polo chau activis visual exploration deep neural network model ieee transaction visualization computer graphic 24 1 2018 96 matthew kay tara kola jessica hullman sean munson ish bus visualization uncertainty everyday mobile predictive system proceeding 2016 chi conference human factor computing system acm 97 frank keil explanation understanding annual review psychology 57 2006 98 kim rajiv khanna oluwasanmi koyejo example not enough learn criticize criticism interpretability advance neural information processing system 99 kim martin wattenberg justin gilmer carrie cai james wexler fernanda viegas et al interpretability beyond feature attribution quantitative testing concept activation vector tcav international conference machine learning 100 jaedeok kim jingoo seo human understandable explanation extraction classification model based matrix factorization 101 kindermans sara hooker julius adebayo maximilian alber kristof schtt sven dhne dumitru erhan kim un reliability saliency method explainable ai interpreting explaining visualizing deep learning springer 102 gary klein explaining explanation part 3 causal landscape ieee intelligent system 33 2 2018 103 rafal kocielnik saleema amershi paul bennett accept imperfect ai exploring design adjusting expectation ai system proceeding 2019 chi conference human factor computing system 104 johannes kraus david scholz dina stiegemeier martin baumann know trust dynamic calibration highly automated driving effect system malfunction system parency human factor 2019 acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai 105 josua krause aritra dasgupta jordan swartz yindalon aphinyanaphongs enrico bertini workflow visual diagnostics binary classifier using explanation 2017 ieee conference visual analytics science technology vast 17 ieee 106 josua krause adam perer enrico bertini infuse interactive feature selection predictive modeling high dimensional data ieee transaction visualization computer graphic 20 12 2014 107 josua krause adam perer kenney ng interacting prediction visual inspection machine learning model proceeding 2016 chi conference human factor computing system acm 108 todd kulesza margaret burnett wong simone stumpf principle explanatory debugging personalize interactive machine learning proceeding international conference intelligent user interface acm 109 todd kulesza simone stumpf margaret burnett irwin kwan tell effect mental model soundness personalizing intelligent agent proceeding sigchi conference human factor puting system chi 12 acm new york ny 110 todd kulesza simone stumpf margaret burnett wong yann riche travis moore ian oberst amber shinsel kevin mcintosh explanatory debugging supporting debugging program ieee symposium visual language computing 10 ieee 111 todd kulesza simone stumpf margaret burnett sherry yang irwin kwan wong much little right way explanation impact end user mental model 2013 ieee symposium visual language computing 13 ieee 112 isaac lage emily chen jeffrey menaka narayanan kim samuel gershman finale human evaluation model built interpretability proceeding aaai conference human computation crowdsourcing vol 7 113 himabindu lakkaraju stephen bach jure leskovec interpretable decision set joint framework description prediction proceeding acm sigkdd international conference knowledge discovery data mining acm 114 ellen langer arthur blank benzion chanowitz mindlessness ostensibly thoughtful action role placebic information interpersonal interaction journal personality social psychology 36 6 1978 635 115 min kyung lee anuraag jain hea jin cha shashank ojha daniel kusbit procedural justice algorithmic fairness leveraging transparency outcome control fair algorithmic mediation proceeding acm interaction 3 cscw 2019 182 116 min kyung lee daniel kusbit evan metsky laura dabbish working machine impact algorithmic management human worker proceeding annual acm conference human factor computing system acm 117 bruno lepri nuria oliver emmanuel letouz alex pentland patrick vinck fair transparent countable algorithmic process philosophy technology 2017 118 piyawat lertvittayakumjorn francesca toni evaluation explanation method text classification proceeding 2019 conference empirical method natural language processing international joint conference natural language processing 19 119 benjamin letham cynthia rudin tyler mccormick david madigan et al interpretable classifier using rule bayesian analysis building better stroke prediction model annals applied statistic 9 3 2015 120 alexander lex marc streit schulz christian partl dieter schmalstieg peter park nil gehlenborg stratomex visual analysis heterogeneous genomics data cancer subtype characterization computer graphic forum vol wiley online library 121 kunpeng li ziyan wu peng jan ernst yun fu tell look guided attention inference network proceeding ieee conference computer vision pattern recognition 122 kunpeng li yulun zhang kai li yuanyuan li yun fu attention bridging network knowledge transfer proceeding ieee international conference computer vision 123 brian lim improving understanding trust control intelligibility application carnegie mellon university 124 brian lim anind dey assessing demand intelligibility application ings international conference ubiquitous computing acm 125 brian lim anind dey daniel avrahami not explanation improve intelligibility intelligent system proceeding sigchi conference human factor computing system acm acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al 126 brian lim qian yang ashraf abdul danding wang explanation selecting intelligibility type explanation goal iui workshop 127 zachary lipton mythos model interpretability 128 mengchen liu shixia liu xizhou zhu qinying liao furu wei shimei pan proach exploratory microblog retrieval ieee transaction visualization computer graphic 22 1 2016 129 mengchen liu jiaxin shi kelei cao jun zhu shixia liu analyzing training process deep ative model ieee transaction visualization computer graphic 24 1 2018 130 mengchen liu jiaxin shi zhen li chongxuan li jun zhu shixia liu towards better analysis deep convolutional neural network ieee transaction visualization computer graphic 23 1 2017 131 shixia liu xiting wang jianfei chen jim zhu baining guo topicpanorama full picture relevant topic 2014 ieee conference visual analytics science technology vast 14 ieee 132 tania lombrozo structure function explanation trend cognitive science 10 10 2006 133 tania lombrozo explanation categorization informs cognition 110 2 2009 253 134 scott lundberg lee unified approach interpreting model prediction advance neural information processing system 135 laurens van der maaten geoffrey hinton visualizing data using journal machine learning research 9 2008 136 maria madsen shirley gregor measuring trust australasian conference mation system vol citeseer 137 ninareh mehrabi fred morstatter nripsuta saxena kristina lerman aram galstyan survey bias fairness machine learning 138 sarah mennicken jo vermeulen elaine huang today augmented house tomorrow smart home new direction home automation research proceeding 2014 acm international joint conference pervasive ubiquitous computing acm 139 stephanie merritt heather heimbaugh jennifer lachapell deborah lee trust know effect implicit attitude toward automation trust automated system human factor 55 3 2013 140 miriah meyer michael sedlmair samuel quinan tamara munzner nested block guideline model information visualization 14 3 2015 141 debra meyerson karl weick roderick kramer swift trust temporary group trust zations frontier theory research 166 1996 195 142 tim miller explanation artificial intelligence insight social science artificial intelligence 267 2019 143 yao ming shaozu cao ruixiang zhang zhen li yuanzhe chen yangqiu song huamin qu understanding hidden memory recurrent neural network 2017 ieee conference visual analytics science technology vast 17 ieee 144 yao ming huamin qu enrico bertini rulematrix visualizing understanding classifier rule ieee transaction visualization computer graphic 25 1 2018 145 brent mittelstadt automation algorithm politics auditing transparency content personalization system international journal communication 10 2016 12 146 sina mohseni akshay jagadeesh zhangyang wang predicting model failure using saliency map tonomous driving system icml workshop uncertainty robustness deep learning 147 sina mohseni mandar pitale vasu singh zhangyang wang practical solution machine learning safety autonomous vehicle aaai workshop artificial intelligence safety safe ai 20 148 sina mohseni eric ragan xia hu open issue combating fake news interpretability opportunity 149 sina mohseni eric ragan evaluation benchmark local explanation machine learning 150 sina mohseni fan yang shiva pentyala mengnan du yi liu nic lupfer xia hu shuiwang ji eric ragan trust evolution time explainable ai fake news detection fair responsible ai workshop chi 2020 151 christoph molnar interpretable machine learning 152 grgoire montavon wojciech samek mller method interpreting understanding deep neural network digital signal processing 73 2018 153 shane mueller gary klein improving user mental model intelligent software tool ieee intelligent system 26 2 2011 acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai 154 bonnie muir trust human machine design decision aid international journal study 27 1987 155 tamara munzner nested process model visualization design validation ieee transaction ization computer graphic 6 2009 156 brad myers david weitzman andrew ko duen chau answering not question user interface proceeding sigchi conference human factor computing system acm 157 andrew norton yanjun qi visualization suite showing adversarial example fool deep learning 2017 ieee symposium visualization cyber security vizsec 17 ieee 158 florian nothdurft felix richter wolfgang minker probabilistic trust handling ceedings annual meeting special interest group discourse dialogue sigdial 14 159 mahsan nourani dondald honeycutt jeremy block chiradeep roy tahrima rahman eric ragan vibhav gogate investigating importance first impression explainable ai interactive video analysis extended abstract 2019 chi conference human factor computing system acm 160 mahsan nourani samia kabir sina mohseni eric ragan effect meaningful meaningless explanation trust perceived system accuracy intelligent system proceeding aaai conference human computation crowdsourcing vol 7 161 besmira nushi ece kamar eric horvitz towards accountable ai hybrid analysis characterizing system failure aaai conference human computation crowdsourcing 162 chris olah arvind satyanarayan ian johnson carter ludwig schubert katherine ye alexander intsev building block interpretability distill 2018 163 cathy neil weapon math destruction big data increase inequality threatens democracy way book 164 sean penney jonathan dodge claudia hilderbrand andrew anderson logan simpson margaret burnett toward foraging understanding starcraft agent empirical study international conference telligent user interface iui 18 acm new york ny 165 nicola pezzotti thomas hllt jan van gemert boudewijn lelieveldt elmar eisemann anna vilanova deepeyes progressive visual analytics designing deep neural network ieee transaction visualization computer graphic 24 1 2018 166 nina poerner hinrich schtze benjamin roth evaluating neural network explanation method using brid document morphological prediction annual meeting association computational linguistics acl 18 167 brett poulin roman eisner duane szafron paul lu russell greiner david wishart alona fyshe brandon pearcy cam macdonell john anvik visual explanation evidence additive classifier proceeding conference innovative application artificial intelligence vol aaai press 168 forough daniel goldstein jake hofman jennifer wortman vaughan hanna wallach manipulating measuring model interpretability 169 pearl pu li chen trust building explanation interface proceeding international ference intelligent user interface acm 170 emilee rader kelley cotter janghee cho explanation mechanism supporting algorithmic parency proceeding 2018 chi conference human factor computing system acm 103 171 emilee rader rebecca gray understanding user belief algorithmic curation facebook news feed proceeding annual acm conference human factor computing system acm 172 marco tulio ribeiro sameer singh carlos guestrin 2016 trust explaining prediction any classifier proceeding acm sigkdd international conference knowledge discovery data mining acm 173 marco tulio ribeiro sameer singh carlos guestrin anchor explanation aaai conference artificial intelligence 174 caleb robinson fred hohman bistra dilkina deep learning approach population estimation satellite imagery proceeding acm sigspatial workshop geospatial humanity acm 175 marko marko bohanec explanation prediction model human machine learning springer 176 stephanie rosenthal sai selvaraj manuela veloso verbalization narration autonomous robot rience proceeding international joint conference artificial intelligence 177 andrew slavin ross finale improving adversarial robustness interpretability deep neural network regularizing input gradient aaai conference artificial intelligence acm transaction interactive intelligent system vol 11 no article publication date august 2021 mohseni et al 178 andrew slavin ross michael hughes finale right right reason training tiable model constraining explanation proceeding international joint conference artificial intelligence ijcai 17 179 stephen rudolph anya savikhin david ebert finvis applied visual analytics personal financial planning ieee symposium visual analytics science technology citeseer 180 dominik sacha michael sedlmair leishi zhang john aldo lee daniel weiskopf stephen north daniel keim machine learning interactive visualization european symposium artificial neural network computational intelligence machine learning 181 dominik sacha hansi senaratne bum chul kwon geoffrey elli daniel keim role tainty awareness trust visual analytics ieee transaction visualization computer graphic 22 1 2016 182 bahador saket arjun srinivasan eric ragan alex endert evaluating interactive graphical encoding data visualization ieee transaction visualization computer graphic 24 3 2017 183 wojciech samek alexander binder grgoire montavon sebastian lapuschkin mller ating visualization deep neural network ha learned ieee transaction neural network learning system 28 11 2017 184 christian sandvig kevin hamilton karrie karahalios cedric langbort auditing algorithm research method detecting discrimination internet platform data discrimination converting critical concern productive inquiry 2014 185 martin schaffernicht stefan groesser comprehensive method comparing mental model namic system european journal operational research 210 1 2011 186 ute schmid christina zeller tarek besold alireza stephen muggleton doe predicate invention affect human comprehensibility international conference inductive logic programming springer 187 philipp schmidt felix biessmann quantifying interpretability trust machine learning system 188 ramprasaath selvaraju michael cogswell abhishek da ramakrishna vedantam devi parikh dhruv batra visual explanation deep network via localization proceeding ieee international conference computer vision 189 avanti shrikumar peyton greenside anshul kundaje learning important feature propagating activation difference proceeding international conference machine jmlr org 190 karen simonyan andrea vedaldi andrew zisserman deep inside convolutional network visualising image classification model saliency map 191 daniel smilkov carter sculley fernanda vigas martin wattenberg visualization deep network 192 thilo spinner udo schlegel hanna schfer mennatallah explainer visual analytics work interactive explainable machine learning ieee transaction visualization computer graphic 26 1 2020 193 hendrik strobelt sebastian gehrmann hanspeter pfister alexander rush lstmvis tool visual analysis hidden state dynamic recurrent neural network ieee transaction visualization computer graphic 24 1 2018 194 simone stumpf vidya rajaram lida li wong margaret burnett thomas dietterich erin sullivan jonathan herlocker interacting meaningfully machine learning system three experiment international journal study 67 8 2009 195 simone stumpf simonas skrebe graeme aymer julie hobson explaining smart heating system courage fiddling optimized behavior 196 latanya sweeney discrimination online ad delivery communication acm 56 5 2013 197 jiliang tang huiji gao huan liu atish da sarma etrust understanding trust evolution online world proceeding acm sigkdd international conference knowledge discovery data mining acm 198 christina anna rohunen jouni markkula eu general data protection regulation change implication personal data collecting company computer law security review 34 1 2018 199 nava tintarev judith masthoff designing evaluating explanation recommender system ommender system handbook springer 200 richard tomsett dave braines dan harborne alun preece supriyo chakraborty interpretable model analyzing interpretable machine learning system acm transaction interactive intelligent system vol 11 no article publication date august multidisciplinary survey framework explainable ai 201 matteo turilli luciano floridi ethic information transparency ethic information technology 11 2 2009 202 jo vermeulen geert vanderhulst kris luyten karin coninx pervasivecrystal asking answering not question pervasive computing application 2010 sixth international conference intelligent environment ie 10 ieee 203 sandra wachter brent mittelstadt chris russell counterfactual explanation without opening black box automated decision gdpr harvard journal law technology 31 2017 841 204 danding wang qian yang ashraf abdul brian lim designing explainable ai proceeding 2019 chi conference human factor computing system chi 19 acm new york ny article 601 15 page 205 fulton wang cynthia rudin falling rule list artificial intelligence statistic 206 qianwen wang jun yuan shuxin chen hang su huamin qu shixia liu visual genealogy deep neural network ieee transaction visualization computer graphic 26 11 2020 207 daniel weld gagan bansal challenge crafting intelligible intelligence communication acm 62 6 may 2019 208 adrian weller challenge transparency 209 gesa wiegand matthias schmidmaier thomas weber yuanting liu heinrich hussmann trust explaining driving behavior autonomous car extended abstract 2019 chi conference human factor computing system acm 210 james wise james thomas kelly pennock david lantrip marc pottier anne schur vern crow visualizing spatial analysis interaction information text document proceeding information visualization ieee 211 kanit wongsuphasawat daniel smilkov james wexler jimbo wilson dandelion mane doug fritz dilip krishnan fernanda vigas martin wattenberg visualizing dataflow graph deep learning model tensorflow ieee transaction visualization computer graphic 24 1 2017 212 samuel woolley automating power social bot interference global politics first monday 21 4 2016 213 mike wu michael hughes sonali parbhoo maurizio zazzi volker roth finale beyond sparsity tree regularization deep model interpretability aaai conference artificial intelligence 214 ming yin jennifer wortman vaughan hanna wallach understanding effect accuracy trust machine learning model proceeding 2019 chi conference human factor computing system 215 jason yosinski jeff clune anh nguyen thomas fuchs hod lipson understanding neural network deep visualization icml deep learning workshop 2015 216 rulei yu lei shi taxonomy deep learning visualization visual informatics 2 3 2018 217 tom zahavy nir shie mannor graying black box understanding dqns international conference machine learning 218 tal zarsky trouble algorithmic decision analytic road map examine efficiency fairness automated opaque decision making science technology human value 41 1 2016 219 matthew zeiler rob fergus visualizing understanding convolutional network european ference computer vision springer 220 quanshi zhang wenguan wang zhu examining cnn representation respect dataset bias aaai conference artificial intelligence 221 zhang zhu visual interpretability deep learning survey frontier tion technology electronic engineering 19 1 2018 222 yunfeng zhang vera liao rachel bellamy effect confidence explanation accuracy trust calibration decision making proceeding 2020 conference fairness accountability transparency fat 20 223 zijian zhang jaspreet singh ujwal gadiraju avishek anand dissonance human machine understanding proceeding acm interaction 3 cscw 2019 56 224 wen zhong cong xie yuan zhong yang wang wei xu shenghui cheng klaus mueller evolutionary visual analysis deep neural network icml workshop visualization deep learning 225 jichen zhu antonios liapis sebastian risi rafael bidarra michael youngblood explainable ai designer perspective 2018 ieee conference computational intelligence game cig 18 ieee 226 luisa zintgraf taco cohen tameem adel max welling visualizing deep neural network decision prediction difference analysis received november 2019 revised july 2020 accepted july 2020 acm transaction interactive intelligent system vol 11 no article publication date august 2021