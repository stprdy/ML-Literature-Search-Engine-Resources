194 explainable ai xai core idea technique solution rudresh dwivedi netaji subhas university technology formerly nsit devam dave het naik smiti singhal pandit deendayal petroleum university rana omer cardiff university pankesh patel university south carolina bin qian zhenyu wen tejal shah graham morgan rajiv ranjan newcastle university dependence intelligent machine continues grow doe demand transparent interpretable model addition ability explain model generally gold standard building trust deployment artificial intelligence system critical domain explainable artificial intelligence xai aim provide suite machine learning technique enable human user derstand appropriately trust produce explainable model selecting appropriate approach building application requires clear understanding core idea within xai sociated programming framework survey programming technique xai present different phase xai typical machine learning development process classify various xai approach using taxonomy discus key difference among existing xai technique concrete example used describe technique mapped programming framework software toolkits intention survey help stakeholder selecting appropriate approach programming framework software toolkits comparing lens presented taxonomy cc concept computing methodology representation reasoning additional key word phrase explainable artificial intelligence interpretable ai programming work software toolkits acm reference format rudresh dwivedi devam dave het naik smiti singhal rana omer pankesh patel bin qian zhenyu wen tejal shah graham morgan rajiv ranjan explainable ai xai core idea technique tions acm comput surv 55 9 article 194 january 2023 33 page author address dwivedi corresponding author netaji subhas university technology formerly nsit dwarka delhi india email dave naik singhal pandit deendayal petroleum university raysan gandhinagar india email omer school computer science informatics cardiff university cardiff uk email ranaof patel ai institute university south carolina sc email ppankesh qian school computing newcastle university uk email wen institute cyberspace security college information engineering zhejiang university technology hangzhou 310023 china email zhenyuwen shah morgan ranjan school computing newcastle university uk email permission make digital hard copy part work personal classroom use granted without fee provided copy not made distributed profit commercial advantage copy bear notice full citation first page copyright component work owned others acm must honored abstracting credit permitted copy otherwise republish post server redistribute list requires prior specific permission fee request permission permission 2023 association computing machinery acm computing survey vol 55 no 9 article publication date january 2023 dwivedi et al 1 introduction society dependence intelligent machine continuous upswing driverless car flexible email filter preemptive law maintenance model machine learning ml system increasingly deployed across several domain frequent utilization complex deep learning architecture requires urgent attention derstand inner working get insight outcome core motivation explainable artificial intelligence xai 14 prime reason rapid growth xai increased robustness artificial intelligence ai system business enterprise computing critical industry 13 14 company google false prediction lead application user shown wrong recommendation 62 however critical sector healthcare finance military inaccurate prediction serious consequence human life hence crucial understand system make decision ai meates critical area human limitation understand complex ai model major roadblock main reason data insight task solved machine main sight increasingly complex model user would need access ton number explain deep neural network moreover no concrete way understand model completely addition black box nature model bias creep dealing data 60 model performance metric model accuracy not always exhibit true prediction decision 9 highly accurate model not sufficient trust deploy model application xai ha receiving much attention across multiple application domain 21 consequently increasing number xai tool technique proposed industry academia current xai system exhibit diverse set dimension functionality simple exploratory data analysis understanding complex ai model therefore selecting correct method given requirement necessitates clear understanding method basic difference among different xai approach however analysis respect existing approach building application ha investigated limited extent work došilović et al 15 general interpretability explainability discussed limited supervised ml model however bhatt el al 7 discus planation scheme adapted ml engineer end user stakeholder provide recommendation organization achieve explanation improve performance arrieta et al 4 present concept taxonomy opportunity challenge toward responsible ai focus survey scheme fairness discrimination ml model vey focus model data explainability model performance metric addition case study used explain significance feature importance furthermore design ations xai framework along software system discussed support fairness accountability believe comparative analysis needed counsel various stakeholder involved application development focus survey approach develop xai application covering tool technology xai related concept aid implementation system outline rest structured follows different stage building typical ml model model compared section need xai application explanation phase ml process discussed section 3 followed taxonomy xai technique section subsequent content derived unpu blished technical report acm computing survey vol 55 no 9 article publication date january explainable ai xai core idea technique solution fig pipeline building ml model explanation section section 5 6 cover major pillar xai system data explainability model explainability respectively focus section 5 basic data visualization dimensionality technique along associated software library focus section 6 model explainability technique associated software library using example xai technique discussed section 7 8 respectively open source commercially available toolkits building xai model covered section 9 analysis requirement implementation xai presented section article concludes section 11 2 developing application although ml pipeline provide accurate prediction lack two important phase derstanding explaining understanding phase involves training quality assurance ai model whereas explaining phase important ml model deployed used application figure 1 illustrates revised ml life cycle additional step 58 understanding phase objective understanding phase improve model training phase prior deploying stakeholder described table 1 cross check make sure final model precise work intended real world activity involved phase interpreting important feature interact interpreting pattern learned trained model analyzing bias data ensuring not propagated trained model following present use case xai understanding phase ml process debugging enhancing ai model ai model go iteration fully developed 33 improve performance incrementally process source model error found removed meticulously checking testing explanation shorten process helping recognition model error source detecting bias process fully partially automated ai model 33 however model trained biased historical data bias permeate entire system negatively impacting decision outcome xai valuable tool help acm computing survey vol 55 no 9 article publication date january 2023 dwivedi et al table stakeholder understanding phase stakeholder description developer build ai application primary motive seeking quality aid system testing debugging evaluation improve robustness application theorist understand advance ai theory motive better understand fundamental property deep neural network ha led interdisciplinary research labeled artificial membership group overlap developer example case industry researcher carry theoretical work deep neural network technology theorist also applying technology build system developer data scientist data scientist aware understand every aspect ai system data used training model implemented prediction developed model data scientist also address error faced ai system identify bias ai model example feature importance discussed section common type explanation show comparative significance input feature any specific model prediction scientific understanding automated statistician project 56 explains prediction breaking complex datasets manageable interpretable section communicating result user enables researcher enhance understanding data feature 33 building robust model model le likely impacted small change input referred robust model explainable also tend robust 33 35 way somewhat intuitive coherent explanation prediction implies reasoning logical logical reasoning le likely affected noise automl automl ha made explainability important enables entire data pipeline black box opposed ml model 27 using automl user doe not ability engineer select feature ha visibility model process explaining phase phase ml model deployed used application purpose phase interpret prediction data made model would pose explanation end user describing prediction result especially important application table 2 describes stakeholder phase explainability important following present use case xai explaining phase ml process better decision making ability understand ai system make certain prediction generate outcome help organization make appropriate decision illustrate churn diction ai model accurately predict customer likely leave future business no solution presented xai shed light ce answer armed knowledge business formulate appropriate plan discrimination datasets often contain inherent discrimination example 2015 searching ceo google image returned woman personality only 11 time not match representation 27 unlike black box model xai model present reasoning behind result help identify source discrimination address accordingly justifiability legal question may not addressable black box model modern ai system no choice except add explainability example area individual right acm computing survey vol 55 no 9 article publication date january explainable ai xai core idea technique solution table stakeholder explaining phase stakeholder description user people use ai system member group need explanation help decide act given output system help justify action example insurance company us ai tool help decide whether cost sell policy client end user tool director company client member user community consumer consumer recipient product service explanation need simple clear enabling user limited understanding make use information without assistance provides trustworthiness well increase transparency ai system business business stakeholder someone want deploy ai system within product business stakeholder include policeman judge bank associate government official doctor among others business stakeholder understand model make decision ensures fairness also protects user false decision taken model regulator expert regulator consistently monitor audit ai system case false made model group follows decision trail another job regulator ensure model date training new data required fig use case xai different phase 4 regulator demand explainability ai model people adversely impacted ai system decision rejected loan would interested know system chose not approve loan 3 taxonomy xai technique section explores different approach interpreting ml model lay foundation interpretability technique table 3 present comparative analysis taxonomy ferent xai technique covered subsequent section section 4 7 figure 3 provides overview different xai technique white box versus black box model technique black box model nature whereas white box model transparent comparatively easy understand black box model also termed intrinsic achieved limiting complexity ai model whereas white box model also termed post hoc applied model training acm computing survey vol 55 no 9 article publication date january 2023 dwivedi et al fig taxonomy xai technique applies data model white box model technique some ai model simple example predicted outcome mathematically expressed weighted sum feature visualized straight line graph slope line b intercept linear model white box model mechanism transparent simple opposed black box whose mechanism not readily understood although simple le capable representing larger dataset featuring complex interaction therefore higher accuracy require complex expressive model black box model technique black box model neural network complex ensemble much lower complexity like gradient boosting model based decision tree architecture model hard decipher not clear important role any given feature play acm computing survey vol 55 no 9 article publication date january explainable ai xai core idea technique solution table xai technique versus taxonomy xai technique classification xai technique global local model specific model agnostic white box black box data explainability commonly used data visualization plot dimensionality reduction technique white box model linear model section decision tree section generalized additive model gam section tree ensemble section artificial neural network neural network section section evaluation metric model evaluation metric section xai technique feature importance section partial dependence plot section individual conditional expectation section accumulated local effect ale section global surrogate section local interpretable explanation lime section shapley value section xai technique counterfactuals section anchor section contrastive explanation method section prototype counterfactuals section integrated gradient section kernel shap section tree shap section not applicable prediction model interacts feature example fully connected neural network tracing output feature rendered model specific causative input feature remains challenge technique versus technique another dimension understanding interpretability model depends model examined technique deal inner working model interpret result involve examining structure algorithm including intermediate sentations technique deal analyzing feature relationship output data distribution following briefly present two technique 44 technique interpretation tool designed purely interpret model specific feature capability used only single algorithm class present various technique section technique interpretation technique classified model agnostic used any ml model widely used local interpretable explanation lime technique model agnostic used analyze interpret any set ml input corresponding prediction output global interpretation versus local interpretation classification based scope interpretation 44 global interpretation analyzes process broader level goal oriented local interpretation give detailed explanation every decision made acm computing survey vol 55 no 9 article publication date january 2023 dwivedi et al global interpretation global interpretation method involve overall analysis model general behavior process defining variable dependency interaction go alongside process assigning importance component two global pretation importance partial dependence plot pdps described section local interpretation local interpretation involves analysis individual prediction decision made model clarify model suggested particular course action data point analyzed focus subregion around data point enables u understand contextual importance data point output space lime shapley value two technique used understand individual prediction section 6 4 data explainability first step explainability data visualization provides idea insight dataset model validation explanation kick section present commonly used data explainability first step validating explaining trusting model section present programming framework used implement data explainability technique exploratory analysis visualization visual analysis crucial interpretable ml knowing content vital setting baseline expectation model behave create long time exploratory data analysis visualization ha major tool gleaning meaningful information data dimensionality reduction technique visualizing crucial interpretable ml datasets sometimes hard visualize due many variable size although multiple dimension plotted interpretation complex error prone dimensionality reduction technique principal component analysis pca ica isometric mapping isomap stochastic neighbor bedding lda umap uniform manifold approximation projection dimension reduction lds locally linear embedding lle autoencoders used improve visualization interpretation pca convert observation correlated feature set linearly uncorrelated feature various orthogonal transformation whereas ica extract independent component equivalent number dimension feature present original dataset isomap used preserve geodesic distance lower dimension whereas produce slightly different result time dataset preserving structure neighboring point lda provides highest possible discrimination multiple class however umap lle manifold learning method based riemannian geometry algebraic topology md multidimensional scaling represents measure larity among pair object computing distance point space autoencoders contain abstract representation data unlike dimension reduction algorithm lle md software library data explainability table 4 describes framework implement data explainability technique checkmark note affirmative unrealizability technique methodology provide exploration well explanation observation data give insight dataset acm computing survey vol 55 no 9 article publication date january explainable ai xai core idea technique solution table programming framework data explainability xai technique numpy panda matplotlib seaborn sklearn wordcloud networkx data visualization plot kernel density estimation kde box whisker plot correlation matrix wordcloud network diagram principal component analysis pca heatmaps stochastic neighbor embedding expressing feature trend anomaly identified although data visualization plot give good data explainability plot histogram much le ible kernel density estimation latter give estimate unknown density function wherein not only differ bandwidth also implement kernel various shape size however pca purely dimensionality reduction technique example representation software ibraries sklearn networkx used building ml model work model provide algorithm classification regression sklearn provides ease interpretation deep learning model along many supervised unsupervised learning algorithm sklearn also consists method ass model tion unseen data using networkx different kind network random weighted metric asymmetric network created wordcloud support visualization frequent word given important word text 5 model explainability section outline commonly used model explainability technique understand ai el laying foundation technique discussed section 6 section first describe technique used white box model whose internal mechanism lend interpreted direct manner section outline technique used black box model whereas section discus application knowledge representation technique ability section describes model performance evaluation metric section present ming framework used implement presented model explainability technique white box model section present white box model programming method used interpretation linear model explainability linear model involves linear combination feature value adjusted coefficient model example mx c coefficient feature c coefficient polynomial degree 1 linear polynomial similarly logistic regression one interpretable linear ml model certain class event seaborn matplotlib sklearn accumulated local effect ale library used unfold visualize logistic regression model decision tree decision tree predicts value target variable multiple input variable terminal node also called leaf node depicts value target variable based input variable key benefit decision tree lie establishing input target variable relationship logic similar boolean library includes method used interpretation tree acm computing survey vol 55 no 9 article publication date january 2023 dwivedi et al dtreeviz graphviz package sklearn also provides way evaluate feature total decrease entropy due split given feature generalized additive model generalized additive model gam extension generalized linear model smoothing function gam offer simple interpretable model logistic regression complex sophisticated model neural network usually offer better accuracy predictive power compared simple model overfitting unlikely gam due regularization prediction function black box model black box model not understood interpreted black box model include tree ensemble support vector machine variety neural network inclusion several layer neural network make difficult designer explain algorithm ha reached particular prediction outcome 1 tree ensemble tree ensemble method learning technique focus grating several decision tree create output determines feature choose split since single decision tree may not enough yield optimal performance several sion tree combined together get optimal performance model however model complexity increase due multiple decision tree result becomes lot difficult understand model behavior following method developed interpreting complex tree ensemble simplified tree ensemble learner stel intrees interpretable tree package covert complex tree ensemble learner known stel ensemble method average variance multiple model turn deprives interpretation individual model tree interpreter tree interpreter provides interpretation decision tree random forest decomposes every prediction result sum feature contribution bias enables better interpretation feature led particular prediction support vector machine support vector machine supervised ml algorithm used classification regression hyperplane calculated based data point plotted space hyperplane oriented way differentiates class maximizing distance data point different class work well space scenario number dimension greater number sample explainable neural network inclusion several layer large complex neural net make difficult explain specific prediction conclusion 1 wa reached black box explainability refers assessing prediction made model any given input without knowledge inherent working design explainable neural network provides insight model post hoc explanation scheme used single multilayer neural network convolutional neural network recurrent neural network approach subsymbolic approach based artificial neural network growing ity due robustness noisy data ability perform complex task not otherwise manually possible however black box nature major hindrance explainability yet bolic ai ha long known explainable natively 3 since represents knowledge using acm computing survey vol 55 no 9 article publication date january explainable ai xai core idea technique solution meaningful symbol language understandable human well interpretable machine however lack scalability symbolic system inability handle noisy data ha limited use recently combination symbolic subsymbolic approach based artificial neural network variously known forth n system ha shown significant promise exploiting complementary strength dividual approach 20 28 consequently system often perform example mao et al 41 present concept learner incorporates symbolic reasoning better interpretation visual concept trained le data 10 original dataset explainable symbolic element n system commonly consists knowledge representation reasoning technique ontology knowledge graph 28 47 represent knowledge concept relationship together provide semantically rich background context domain knowledge graph represent fact usually standard resource description framework 5 using graph database linked ontology represent background contextual knowledge commonly web ontology language 29 ontology thus perform deductive reasoning derive new deeper knowledge existing knowledge traceable manner doran et al 13 postulate system comprehensible not only interpretable expert also understandable must emit symbol enable user understand conclusion reached symbol closer human understand compared vector numerical encoding seen neural network however achieve ability author argue critical need reasoning understand behind particular outcome 13 ontology enable reasoning not only deduce new knowledge fact also causal inference understand cause effect behind outcome transitivity 23 example ontology snomed ct 32 following concept represented transitive relationship acute rheumatic arthritis due pyoдenes inf ection causative aдent pyoдenes based transitivity z r x z x z inferred acute rheumatic arthritis effect associated streptococcus pyogenes cause using knowledge ontology type relationship classification natural language explanation generated acute rheumatic arthritis autoimmune disorder caused due infectious disease streptococcus pyogenes infection disease ha causative agent bacterium streptococcus pyogenes hence streptococcus pyogenes causative agent acute rheumatic arthritis causal inference ha practical application personalized instance drug contraindication 38 therefore follows output subsymbolic approach linked background contextual knowledge generate understandable explanation 30 sarker et al 47 provide useful pipeline demonstrate background knowledge made available ontology used explain classification behavior artificial neural network use 8 system supervised ml based inductive learning cally create class expression knowledge base kb used toward explaining classification behavior furthermore researcher exploited advance natural language processing specifically natural language generation approach linguistic refinement explanation 16 49 figure 4 show overview interaction neural symbolic approach within n system 3 explain process help previous example consider neural acm computing survey vol 55 no 9 article publication date january 2023 dwivedi et al fig view system 3 network model trained clinical prediction streptococcus pyogenes infection patient symbolic knowledge consisting example formal used determine subtypes infection include prediction neural model trained patient data several source clinical note electronic record tory result output model used refine kb incorporating new association causal analysis encode rationale behind neural network decision encoded knowledge kb used construct explanation exemplified earlier moreover system questioned analysis condition patient risk developing condition caused pyogenes practice however only partial element n integration utilized 3 many n system still based flat approach prompting sarker et al 48 argue need focus logical aspect n system realize improved explainability infer contemporary work combination neural symbolic approach brings together two fundamental aspect intelligent cognitive behavior ability learn experience ability reason wa learned 59 thus neural system excel learning symbolic approach employed explain ha learned fact deep understanding cognitive science certainly benefit field particularly integration approach 6 develop mature robust integrated model consequently design explainable system closer human understand think communicate model performance evaluation metric any data science life cycle model performance evaluation important phase optimal model chosen evaluation lead adjusting model hyperparameters evaluation metric problem specific agnostic present model performance metric incorporated interpret model software library model explainability following present some software library used model ity table 5 describes framework used implement data explainability technique explains particular explainable ml algorithm left carried help framework specified top checkmark denotes affirmative case similarly stated framework not used implement technique acm computing survey vol 55 no 9 article publication date january explainable ai xai core idea technique solution table programming framework model explainability xai technique basic library tensorflow kera pytorch pygam linear model coefficient section decision tree section generalized additive model gam section neural network section tree ensemble section model performance evaluation metric section basic library numpy panda matplotlib seaborn sklearn mentioned algorithm generation model linear model decision tree gam neural network tree ensemble belong family model accounted able however some technique linear model coefficient not ensure explainability practice due reason implementing input may not plainable could only possible number input feature limited using regularization furthermore linear model coefficient could unstable case multicollinearlity tiple correlated feature decision tree unlike linear model coefficient also applied model decision tree visualization intuituve reveal decision model training thus supporting interpretability explainability gam much flexible regression although gam interpretable come cost not fitting every type data gam explainable sense distribution value any given feature function plotted domain expert easily interpret reliable neural network flexible model explainability tree ensemble understood bunch decision tree whose result combined thus providing support explainability decision tree benefit stringer learning training ktrain interface kera build train explainable model work text image classification considering example image focus area image classifier used prediction visualization supported library based gradcam technique text dataset classification carried lime prediction made using relative importance word pytorch us captum model interpretability using smoothgrad integrated gradient deeplift gradientshap pytorch model used understanding important neuron layer pytorch model also provides web interface insight data visualization arize ai arize ml observability framework model monitoring assessment used diagnose root cause model output detect check addition unveils explains model arrive specific outcome any set prediction artificial intelligence fairness 360 also referred python toolkit detection reduction bias ml model increase trust ai primary vision xai toolkit includes extensive set metric datasets model test bias along explanation metric metric integral part explanation bias ml model also ha algorithm mitigate bias datasets model ai xplainability 360 open source python toolkit developed ibm similar python toolkit includes several algorithm whose primary motive explanation terpretation ml model datasets due diligence performed choosing appropriate algorithm explanation particular ml model decision tree choosing appropriate algorithm specific condition given following acm computing survey vol 55 no 9 article publication date january 2023 dwivedi et al fig decision tree 63 interpretml interpretml python toolkit microsoft training intelligible model explaining black box system interpretml based human interpretation global local explanation model also help debugging model able understand prediction interpretml ha feature comparing different xai method using function class different xai method include decision tree decision rule list linear regression logistic regression shap kernel explainer shap tree explainer lime morris sensitivity analysis partial dependence includes explainable boosting machine algorithm explaining model higher accuracy amazon sagemaker similar microsoft azure amazon sagemaker service offered zon web service aws prepare build train deploy ml model subcomponent amazon sagemaker called amazon sagemaker clarify used interpret ml model plaining prediction made model approach also used detect bias various stage model data cleaning model training deployment fairlearn fairlearn open source project make ml model transparent various kind bias factor pertaining unfairness exist ml model fairlearn belief fairness not introduced technical toolkits alone objective fairlearn detect reduce unfairness often form bias distributed learning small ml model lack representation capability expressing complex data pattern train large model data volume limitation computing power storage single machine call distributing ml workload across multiple machine ing result coherent model describe various technical issue consider acm computing survey vol 55 no 9 article publication date january explainable ai xai core idea technique solution implementation distributed model report impact qos metric scalability latency convergence speed data parallel model parallel pipeline parallel data parallel partition place data onto worker node one applying model training process worker node central server communicate periodically ensure model synchronized across node model parallel approach 42 43 model split placed different worker node processing copy dataset result aggregated submodels model parallel depends heavily model structure not split easily example training sequential model only one gpu utilized time ha wait result gpu alleviate problem pipeline parallel 31 split data smaller batch pipeline better utilization available resource topology one important design consideration distributed learning organization chronization worker node within system several consideration actual plementation first system ha scalable large number worker node dispatching aggregating worker information efficient constant second system communication efficient easy set parameter server p 37 prominent centralized aggregation data parallel training p worker node periodically upload model parameter update central server decentralized setting no central server worker node communicate via allreduce approach exchanging model update overall communication topology ha significant impact training performance ringallreduce 10 usually achieves better scalability due efficient use network bandwidth p suffer network congestion server side due model aggregation approach synchronization asynchronization sgd single machine training sgd used dating parameter single model distributed learning global model updated aggregate worker gradient computed sgd updated model sent worker node parameter aggregation worker node impact several system metric loss model training quality convergence speed overall three key approach nous sgd 51 stale asynchronous sgd 61 asynchronous sgd 11 asynchronous sgd delayed gradient create noise global model delay convergence speed however provides better model generalization capability fast training synchronous training usually take time requires waiting idle node compared asynchronous approach often generates model produce better performance distributed learning system apart commonly used computation framework used distributed computation mapreduce apache spark many work dedicated deep learning framework include generic deep learning work tensorflow pytorch mxnet incorporate different distributed learning approach however some framework dedicated distributed learning summarized table example framework paddlepaddle include support sync async training p allreduce topology theano optimizing compiler instead development framework aim list trending framework could choose thus theano not included table 6 technique section present model explainability technique describes input feature contribute model output many method available permutation acm computing survey vol 55 no 9 article publication date january 2023 dwivedi et al table distributed learning framework synchronization architecture sync async stale parameter server ringallreduce tensorflow pytorch mxnet paddlepaddle baidu allreduce horovod cntk distbelief petuum dmtk fig example output feature importance fig example pdps feature importance pdps individual conditional expectation ice plot ale plot global surrogate model lime shapley additive explanation shap feature importance feature importance 17 19 refers class technique used assign score input feature show feature relative importance prediction made also defines basis dimensionality reduction feature selection improve efficiency predictive model permutation importance widely used feature importance technique measure tance looking reshuffling predictor randomly impact performance model considered computationally expensive technique table 6 present feature importance example cancer dataset example output feature importance matrix shown figure 6 cancer dataset listing weight feature datasets topmost value indicate feature important number bottom represent least important feature randomness permutation importance evaluation measured repetition several shuffle partial dependence plot pdps depict scenario feature affect prediction example following question answered pdp 18 25 would impact longitude latitude price house would house similar size priced different geographic area figure 7 illustrates simple pdp example plot value feature f 0 worst concave point plot predicted value solid line drawn shaded area represents variation acm computing survey vol 55 no 9 article publication date january explainable ai xai core idea technique solution fig example ice plot average prediction value f 0 change represented change prediction would predicted baseline point leftmost value blue shaded area denotes level confidence example worst concave point feature ha initially increasing negative influence thereafter remains neutral hence say le value initially predict le chance malignant condition benefit first ease implementation 44 second computation part quite intuitive come pdps partial dependence function point specific feature value would represent average prediction easy layman understand logic pdps limitation concern associated pdps assumption independence 44 feature partial dependence computed assumed not correlated feature second maximum number feature partial dependence function realistically two underlying reason lie representation paper screen lack ability visualize three dimension third heterogeneous effect uncovered analyzing ice outlined section curve ignoring aggregated line individual conditional expectation instead average plotting pdp ice 24 show one line per instance ice score outperform pdp intuitiveness line stand prediction one instance partial pendence ice explains happens model prediction specific feature varies figure 8 present ice plot feature worst concave point cancer dataset worst concave point plot decreasing nature lower value concave point factor higher value target variable malignant case graph decrease constant ha lower case benefit far intuitive understand compared pdp 44 single line plot prediction one instance change feature interest distinct pdp ice curve unveil heterogeneous relationship limitation first many ice curve could lead overcrowded plot without ability ass anything second ice curve pdps share concern interest feature correlate feature point line could invalid data point third difficult find average ice plot one solution group together ice curve pdp pdp versus ice versus feature importance table 7 present comparative analysis among pdp ice feature importance pdp demonstrates global effect concealing heterogeneous effect ice unravels heterogeneous effect make difficult find average addition three method consider feature independent entity hence feature correlated result creation unlikely data point acm computing survey vol 55 no 9 article publication date january 2023 dwivedi et al table pdp versus ice versus feature importance 53 approach advantage disadvantage pdp intuitive easy implement show global effect assumption independence heterogeneous effect may hidden ice intuitive easy implement uncover heterogeneous relationship only display one feature meaningfully assumption independence not easy see average feature importance provides highly compressed global insight comparable across problem automatically take account interaction not additive shuffling feature added randomness need access true data assumption independence fig ale plot iris classification datasets ale plot ale plot 2 44 handle inherent limitation pdp case pd plot produce roneous result feature dataset highly correlated ale plot come picture ale plot work algorithm provides global explanation classification regression model tabular data ale plot preferred pd plot produce optimal result despite correlation feature le computationally expensive ale plot visualize effect prediction model feature isolated feature example figure 9 show implemented ale iris classification dataset sklearn ha four feature sepal length sepal width petal length petal width three target value setosa versicolor virginica ale plot visualize feature effect linearly logit space looking plot petal length bottom right plot observed three line overlap mark concluded effect petal length class one cm also observed petal length chance flower virginica specie le petal length chance setosa specie global surrogate global surrogate interpretable model developed approximating black box model tions figure 10 explains simple process step 1 data ha fed black box model making prediction step 2 model type need determined whether trained surrogate instance linear regression decision tree step 3 surrogate model trained surrogate training performed use independent acm computing survey vol 55 no 9 article publication date january explainable ai xai core idea technique solution fig process creating surrogate model 53 variable input data dependent variable black box prediction thy surrogate model any interpretable model linear model decision tree rule end prediction error surrogate model evaluated compared black box prediction smaller error mean explanation black box outperformed surrogate model benefit benefit surrogate model 44 53 lie flexibility interchange pretable model underlying black box model two surrogate model linear model decision tree trained original black box model aid provide two type explanation limitation creating surrogate model 44 53 derive conclusion ml model not data possible interpretable model close one subset broadly disparate different subset dataset scenario simple model interpreted not replicated across data point moreover method would not work well understand single prediction wa made given observation local interpretable explanation lime 46 different global surrogate sense lime doe not try explain whole model perturbing input data sample comprehending tions change lime try understand model lime enables local model interpretability single data sample modified adjusting some feature value resultant output impact observed often linked human interest output model observed computer vision example figure 11 illustrates lime used image classification example classifier ha explained predicts likelihood image containing egyptian cat image figure 11 b acquired split easily interpretable component seen figure 11 c dataset perturbed instance generated turning turning gray some interpretable component every instance perturbation compute probability egyptian cat image per model try understand locally weighted simple linear model dataset emphasis erring perturbed instance best match original image ultimately provide showing highest positive weight explanation figure 11 distinguish turning everything else gray example explanation lime figure 12 illustrates explanation generated lime technique using price prediction model generates price recommendation explanation describing recommended price derived explanation instead predicting price output figure 12 list explanation considering tion feature predicted price left part show range maximum minimum value predicted price prediction module middle part show feature wt ppk contribute predicted price animal observed weight range wt acm computing survey vol 55 no 9 article publication date january 2023 dwivedi et al fig explaining prediction lime original image egyptian cat read lime b erated image using mark boundary c perturbed image area image produced prediction egyptian cat fig model explanation generated lime contributing negative direction prediction also assessed ppk range ppk contributing positive side total price right part show actual value particular feature weiдht ppk potential pitfall trust important efficient interaction human ml tems explanation individual prediction ideal way ass trust even though lime seems superior term ease implementation computation cost couple potential limitation following first foremost only linear model used approximate local behavior current implementation assumption correct some tent only small region around data sample considered however region expanded possibility linear model might impotent explain original model behavior would local region datasets requiring complicated model inability deploy lime setting major pitfall shapley value shap approach explain output any ml model shap value 40 great tool similar lime interpretation measure impact certain value given feature comparison prediction section discus shapley value shap value arises concept shapley also demonstrates shap value increase transparency model example figure 13 show output shap plot instance cancer output value prediction observation base value value would predicted feature unknown current output mean prediction feature push prediction higher right shown red pushing prediction lower blue explanation figure 13 display feature contribute push model output base value model output basevalue model output average passed training dataset feature push prediction higher color coded red size show extent effect feature visually feature push prediction lower depicted cancer dataset acm computing survey vol 55 no 9 article publication date january explainable ai xai core idea technique solution fig example output shap plot table programming framework advanced xai technique xai technique basic library kera tensorflow pytorch lime shap skater pdpbox xai feature importance section partial dependence plot section individual conditional expectation section global surrogate section lime section shapely value section accumulated local effect plot section includes elementary library numpy scipy panda etc blue color predicted whereas basevalue biggest impact come worst area although mean concave point value ha high effect increasing prediction subtract length blue bar length pink bar equal distance base value output benefit important advantage shap value lie transparency interpretability locally implies observation shap value set possible plain instance get prediction contribution predictor example scenario include following model denies loan individual bank legally bound explain loan wa rejected physician want determine factor responsible patient disease risk directly look risk factor directed health intervention traditional variable importance algorithm provide result entire ulation ignoring individual scenario local interpretability facilitates locate compare effect factor shapley value versus lime shapley value provides local global interpretation ing explanation theoretical inference however computationally expensive calculate shapley value comparison lime problem may resolved recently veloped kernel shap method applies fast kernel approximation however crunching large background data still incurs high computational cost lime optimal alternative model knn model term computation cost however fails work model example lime not handle requirement xgboost use input data contrary shap vides fast reliable evaluation incorporates tree explainer optimally estimate shapley value xgboost software library xai technique following present software library used implement based xai technique table 8 describes framework used implement following technique acm computing survey vol 55 no 9 article publication date january 2023 dwivedi et al abbreviated form explain like popular python package provides explanation visualization prediction ml model aid debugging ml classifier two primary way ass ml model work using first global provides method explain parameter acting respect entire model second local provides look specific instance prediction explanation model prediction instance shap popular library used model explainability based shapley value sure influence various feature contribution every feature dataset toward prediction capable visualizing local global interpretation local explanation consists prediction individual instance wa made global explanation provides summary feature importance entire dataset shap offer method visualize data pattern understand model global context shap ha number explainers deep based deeplift algorithm 40 52 gradient kernel estimate shap regression classification model linear compute shap value linear model dent feature tree calculate shap value decision tree model sampling computes shap value using random permutation feature shap robust method provides integration several method feature importance feature dependence interaction clustering summary plot included single library shap computationally sive certain model knn run fast tree tree xgboost xai explainability toolbox ml specifically designed data analysis model evaluation follows regard data analysis facilitates user balance class thereby splitting testing training dataset ha ability visualizing correlation matrix thereby explaining model behavior regard model evaluation interaction prediction input feature analyzed building deep learning model pdp toolbox abbreviated pdpbox similar icebox used compute visualize impact effect feature prediction target variable any algorithm thereby explaining prediction model library one step ahead random forest pdpbox provides direction feature influencing prediction skater python library used interpreting identifying relationship data feature act input model final prediction model make used reveal interpretation black box model globally well locally skater one perform global interpretation using pdps feature importance technique loan prediction model us customer credit history account status income approve deny loan skater also measure model performance alters time deployment production environment skater ha ability interpret allowing practitioner measure feature interaction vary across different version model library used kera api tensorflow workflow primary feature include tensorboard integration callback support visualization heatmaps documentation documentation explainability toolbox machine learning partial dependence plot toolbox library model method tensorflow acm computing survey vol 55 no 9 article publication date january explainable ai xai core idea technique solution fig example output anchor gradient hyperparameter tuning confusion matrix generation explanation ization prediction method include gradcam 50 smoothgrad 54 7 technique section describe commonly used xai technique section present programming framework implement technique comparative analysis anchor anchor capture condition feature model give prediction anchor support approach classification model text image tabular data anchor take account global dataset give anchor feature value using rule finding feature associated input instance responsible prediction anchor similar lime provide local explanation linearly however lime only cover local region may not generalizable perturbation space allotted lime anchor latter build valid region instance better describe model behavior example consider example heart disease dataset feature instance suggesting heart disease individual value thalach person 131 ca 3 fore anchortabular method reach following conclusion figure 14 ply anchortabular method ass feature contribute significantly type prediction person maximum heart rate 131 le 138 blood vessel colored fluoroscopy 3 greater 1 maximum heart rate person high blood vessel colored fluoroscopy low preceding feature act anchor patient deduce person ha heart disease counterfactual explanation determining change feature switch prediction model applied data along rationale behind outcome important factual explanation xai technique provides change made feature value change current output predefined output counterfactual explainer method work black box model best suited binary datasets also applied classification three target value performance degrades compared binary classification heart disease dataset example value condition field 0 1 signifies presence sence heart disease dataset considered specific instance patient ha heart disease figure 15 show instance using input figure 15 generate four ent counterfactuals shown figure exhibit minimum change feature value change condition patient following observation regarding output first sex age type chest pain field cp person suffering heart disease not changed therefore feature fixed counterfactuals second four different counterfactuals different scheme change target value 1 example second counterfactual acm computing survey vol 55 no 9 article publication date january 2023 dwivedi et al fig input specific instance patient ha heart disease fig different counterfactual output counterfactual explainer list indicates reduction cholesterol lead decrease intensity heart disease also show normal result no defect upon performing thallium test heart third recurring theme counterfactuals reduction ca 2 0 ca signifies number blocked vessel important feature contributing heart disease hence important factor changing condition reduce number blocked vessel using method like angioplasty counterfactuals guided prototype refers explanation described basis prototype using representative sample instance belonging class counterfactuals guided prototype advanced accurate work black box model method approach interpret result using prototype class target able much faster counterfactual prototype speed search process significantly directing counterfactual prototype particular class contrastive explanation method contrastive explanation method cem xai method generating local tions black box model cem defines explanation classification model providing insight preferable feature along unwanted feature pertinent positive pertinent atives cem first method provides explanation minimally present necessarily absent instance explained maintain original prediction class also identifies minimal set feature adequate ferentiate nearest different class using cem accuracy ml model hanced looking case instance using explanation provided cem two category explanation include pertinent positive pertinent negative 12 pertinent positive explanation find feature necessary model predict output class predicted class example includes important pixel image feature high feature weight among others pertinent positive work similarly anchor pertinent negative explanation find feature minimally sufficiently removed instance maintaining original output class pertinent negative work similarly counterfactuals example using heart disease dataset figure 17 generates counter explanation term pertinent negative original prediction wa 0 altered 1 applying cem pertinent negative pertinent negative explanation work similarly counterfactual tions describe previously cem value array different original one acm computing survey vol 55 no 9 article publication date january explainable ai xai core idea technique solution fig output cem using heart disease dataset fig example local explanation using heart disease dataset change prediction class some cp ca thal hence change feature necessarily eliminated retain original prediction 0 responsible flipping prediction class kernel tree shap goal shap calculate impact every feature prediction 22 40 compared shapley value kernel shap provides computational efficiency accurate approximation higher dimension kernel shap full model ha utilized already trained instead retraining model different feature permutation missing feature replaced sample mean absent feature value equated feature value replaced random feature value selected modified feature space fitted linear model coefficient model act shapley value local explanation example local explanation using heart disease dataset illustrated figure base value average output value model training data pink value prediction toward 1 push prediction higher toward heart disease blue toward 0 push prediction lower toward no disease magnitude influence determined length feature horizontal line value shown corresponding feature value feature particular index ca highest influence ca increasing prediction value sex decreasing value global explanation figure 19 plot impact feature prediction class feature arranged highest influence topmost feature thus ca feature influence prediction followed thal color shade depict direction feature impact prediction example higher shap value ca shown red color mean high feature value higher value ca higher shap value toward 1 high value ca indicates chance heart disease however opposite some feature high thalach indicate no heart disease tree shap tree shap algorithm compute exact shap value decision tree based model algorithm provides interpretable explanation suitable regression tion model tree structure applied tabular data 22 39 attribute change model output respect baseline average reference set inferred node data input feature similar kernel shap shapley value feature acm computing survey vol 55 no 9 article publication date january 2023 dwivedi et al fig example global explanation using heart disease dataset computed averaging difference model output observed feature part group present feature integrated gradient integrated gradient also known gradient xai technique assigns importance value feature input using gradient model output 57 local method help explain individual prediction method provides specific attribution feature input positive attribution negative attribution positive attribution attribution contribute influence model make decision whereas negative attribution attribution contribute influence model decision made example let u understand using example mnist dataset presented positive attribution negative attribution consider example fashion mnist dataset consists grayscale 28 28 image different clothing item label consists 10 class denote different kind clothing item shirt hoodies shoe jean first example top part figure 20 image shoe attribution section show melange positive negative attribution together observed bar right side green signifies positive attribution purple signifies negative attribution shoe unique compared clothing item hence ha lot positive attribute negative one lining collar back part shoe main pixel influence decision model however negative attribution negligible particular instance second example bottom part figure 20 image shirt equal number positive negative attribution pixel around collar sleeve biggest positive attribution however middle portion shirt mistaken part pair jean trouser therefore due ambiguity negative attribution prediction affirm positive attribution outweigh negative attribution model make correct prediction acm computing survey vol 55 no 9 article publication date january explainable ai xai core idea technique solution fig example integrated gradient using fashion mnist dataset table programming framework xai technique xai technique basic library kera tensorflow pytorch dice alibi anchor section counterfactuals section prototype counterfactuals section contrastive explanation method section kernel shap section tree shap section integrated gradient section includes elementary library numpy scipy panda etc software library xai technique following present some software library used implement xai technique table 9 describes framework used implement technique diverse counterfactual explanation diverse counterfactual explanation dice 45 package give explanation model prediction ml model perform prediction based present data however practical situation might not enough important know answer question modification feature prediction flip dice not only provides influential feature prediction also recommends feature modification needed result implement counterfactual explanation include perturbed feature turn lead required result addition some feature difficult modify financial tus difficult change working hour per week dice allows input relative difficulty specifying feature higher feature weight mean feature difficult modify others alibi alibi 34 open source python library aim model inspection interpretation library consists wide range algorithm focus black box model local interpretation interpretation prediction explained particular instance library comprises different type explainers depending data dealing ml acm computing survey vol 55 no 9 article publication date january 2023 dwivedi et al 8 software toolkits section present toolkits available building xai application unlike previous section section consider candidate toolkit include extensive set tool technique instead focusing one single aspect xai cover different aspect xai technique visualization tool tool debug evaluate ml model among others following present some toolkits available commercially open source tool tool wit 10 attribute google open source tensorboard web application tool ha ability debug evaluate ml model effectively tool aid understand model simple intuitive way visual interface work classification regression model prominent feature wit allows everyone ml researcher developer stakeholder extensive use free complex coding also provides answer different scenario aid visualize explore counterfactual example example classification model return instance similar feature different prediction thus enables simplifies task ass modification model real time one feature visualization dataset inculcates diverse data impact model result altering different feature value hyperparameters tuned observation addition beneficial comparing result two different model input data also capable evaluation wit ha three tab different feature datapoint editor performance fairness ture datapoint editor show prediction every datapoint passed using datapoint editor also able inspect individual input point create custom visualization changing feature value performance fairness provides overall performance using evaluation metric confusion matrix roc curve provides way slice data different feature applies different strategy accuracy fairness enhancement feature tab show balance dataset every feature assigning range every feature prior training tensorboard tensorboard visualization tool help inspecting inner working model training deep neural network complex difficult comprehend used ing evaluation metric like loss accuracy needed ml workflow projecting embeddings space tensorboard also capable debugging ing tensorflow program tensorflow program two basic component operation tensor tensor ues multidimensional array data stored tensor operation manipulate stored data data fed model consists set operation tensor flow tween operation get output tensor current implementation tensorboard low five visualization scalar image audio histogram graph tensorboard scalar board visualizes scalar valued tensor vary time similar loss learning rate image dashboard display saved image also used build image classifier bitrary image data able display input image network generated output image autoencoder gan audio enables saving audio embedded played audio dashboard using audio widget histogram dashboard visualizes distribution 10 acm computing survey vol 55 no 9 article publication date january explainable ai xai core idea technique solution tensor different period time used visualize weight bias matrix neural network graph explorer visualize tensorboard graph allowing ing tensorflow model operation tensorboard also provides embedding projector aid visualizing data examining embedding layer example sentiment analysis allows searching specific term highlight word nearby embedding space requires point read input data well metadata vocabulary file tional information layer visualize another component tensorboard summary special operation required visualize model parameter weight bias neural network evaluation metric image input image network feed regular tensor output summarized data disk interpretml interpretml open source python toolkit microsoft training intelligible model explaining black box system based human interpretation global local tions model also simplifies debugging model able understand prediction interpretml ha ability comparing different xai method using feature timizes datasets different xai method include decision tree decision rule list linear regression logistic regression shap kernel explainer shap tree explainer lime morris sensitivity analysis partial dependence includes explainable boosting machine algorithm explaining glass box model higher accuracy currently three feature available 1 tabular data interpretability based interactive visualization available toolkit user ass modification feature particular data point model prediction 2 interpretability text data visualization dashboard also available text classification sentimental analysis 3 counterfactual example analysis using dice demo analysis also available recommends required modification input feature model yield desired output feature dashboard include filtering data creating cohort model mance tab visualizes model performance metric well distribution rejection probability overall model explanation show technique like feature importance individual feature portance factor 9 design consideration implementing xai enhancing explainability ai system bring many positive however tation xai not easiest task following discus consideration implementing xai model xai versus model performance applying xai concept model may choose model interpretability model performance 26 example simple linear model easily explain model decision lead specific insight prediction conversely complex ensemble deep learning model often produce superlative performance considered black box model quite difficult ass model decision different user need different form explanation different situation decision recommendation may explained many way depending audience requirement factor different scenario 55 understanding system functioning user may question kind data wa used system origin data reason data wa chosen doe model work factor impact process specific result wa achieved figure kind acm computing survey vol 55 no 9 article publication date january 2023 dwivedi et al fig xai cost poor decision versus human participation 36 explanation required necessary engage stakeholder build robust system design system design often need balance competing requirement xai technology utilize iou scheme approach pro con method used different application interpretability accuracy privacy varies instance healthcare finance application ai system privy confidential data making decision fering recommendation explainability case differs business must keep mind extent need transparent put question mark suitability tems application knowledge process vital purpose general acceptance overall accountability quality data part xai modern ai method rely huge volume data making prediction decision 55 aware data quantity provenance ai tems ensures system explainability instance image data might biased minority social medium data might restricted particular demographic city sensor data might only represent particular neighborhood explainability may not always priority designing ai system explainability need must looked within overall objective system figure 21 show various ai application based human participation cost poor decision extent requisite plainability differs one experiment another example shopping recommendation customer may not want explanation item recommended however situation ml model utilized making crucial decision explainability paramount tom left quadrant represents successful ai use case potential cost human participation low decision listed top right quadrant credit risk profiling medical diagnosis represent exponentially increasing cost top left quadrant feature decision any error result disastrous consequence ai not explain domain risk making wrong decision may override accuracy efficacy 10 conclusion survey covered variety xai technique currently use ranging white box model linear model decision tree gam model technique intrinsically explainable interpretable compared black box counterpart technique lime shap global surrogate model addition plot pdp ice ale presented along role acm computing survey vol 55 no 9 article publication date january explainable ai xai core idea technique solution progress development ai essence survey seek facilitate structured acute information coupled researcher explored discussion beyond ha gone far xai domain toward idea distributed ai paradigm imposes storage data multiple node implementing ai model practice including privacy transparency fairness also investigated implication espousing xai technique context different application livestock mart healthcare unveiling potential xai compromise privacy user cogitation toward future xai articulated contrasting discussion carried throughout work capitulate cogent need apt understanding capability limitation emerged xai technique prevision model interpretability must considered conjunction constraint requirement associated data explainability model explainability fairness accountability progenitive implementation deployment ai scheme institution organization worldwide only assured upon disquisition ai axiom jointly reference 1 plamen angelov eduardo soares towards explainable deep neural network xdnn neural network 130 2020 2 daniel apley jingyu zhu visualizing effect predictor variable black box supervised learning model arxiv preprint 3 sebastian bader pascal hitzler dimension structured survey show essay honour dov gabbay volume one sergei artëmov howard barringer artur avila garcez luís lamb john wood college publication 4 alejandro barredo arrieta natalia javier del ser adrien bennetot siham tabik alberto barbado salvador garcia et al explainable artificial intelligence xai concept taxonomy opportunity lenges toward responsible ai information fusion 58 2020 5 david beckett tim eric prud hommeaux gavin carothers rdf turtle world wide web consortium 2014 2014 6 tarek besold artur avila garcez sebastian bader howard bowman pedro domingo pascal hitzler kühnberger et al learning reasoning survey interpretation corr 7 umang bhatt alice xiang shubham sharma adrian weller ankur taly yunhan jia joydeep ghosh ruchir puri josé moura peter eckersley explainable machine learning deployment proceeding 2020 ference fairness accountability transparency acm new york ny 3375624 8 lorenz bühmann jens lehmann patrick westphal framework inductive learning semantic web journal web semantics 39 2016 9 diogo carvalho eduardo pereira jaime cardoso machine learning interpretability survey method metric electronics 8 8 2019 832 10 nvidia corporation nvidia collective communication library nccl retrieved july 15 2021 http 11 jeffrey dean greg corrado rajat monga kai chen matthieu devin quoc le mark mao et al large scale distributed deep network proceeding international conference neural information processing system nip 12 12 amit dhurandhar chen ronny lu tu paishun ting karthikeyan shanmugam payel da explanation based missing towards contrastive explanation pertinent negative advance neural information processing system 13 derek doran sarah schulz tarek besold doe explainable ai really mean new conceptualization perspective proceeding first international workshop comprehensibility explanation ai ml 2017 international conference italian association artificial intelligence ai ia 2017 bari italy november 2017 ceur workshop proceeding vol 2071 tarek besold oliver kutz 14 finale kim towards rigorous science interpretable machine learning acm computing survey vol 55 no 9 article publication date january 2023 dwivedi et al 15 filip karlo došilović mario brčić nikica hlupić explainable artificial intelligence survey proceeding 2018 international convention information communication technology electronics ic mipro 18 16 basil ell andreas harth elena simperl sparql query verbalization explaining semantic search engine query proceeding european semantic web conference 17 aaron fisher cynthia rudin francesca dominici model class reliance variable importance measure any machine learning model class rashomon perspective arxiv preprint 18 jerome friedman greedy function approximation gradient boosting machine annals statistic 29 5 2001 19 jerome friedman bogdan popescu predictive learning via rule ensemble annals applied statistic 2 3 2008 20 giuseppe futia antonio vetrò integration knowledge graph deep learning model comprehensible challenge future research information 11 2 2020 122 21 krishna gade sahin cem geyik krishnaram kenthapadi varun mithal ankur taly explainable ai industry proceeding acm sigkdd international conference knowledge discovery data mining kdd 19 acm new york ny 22 maría vega garcía josé aznarte shapley additive explanation forecasting ecological informatics 56 2020 101039 23 christine golbreich evan wallace peter owl 2 web ontology language new tures rationale working draft 11 june retrieved september 9 2022 24 alex goldstein adam kapelner justin bleich emil pitkin peeking inside black box visualizing tical learning plot individual conditional expectation journal computational graphical statistic 24 1 2015 25 brandon greenwell pdp r package constructing partial dependence plot r journal 9 1 2017 26 david gunning david aha darpa explainable artificial intelligence program ai magazine 40 2 2019 27 xin kaiyong zhao xiaowen chu automl survey 28 pascal hitzler federico bianchi monireh ebrahimi kamruzzaman sarker integration semantic web semantic web 11 1 2020 29 pascal hitzler markus krötzsch bijan parsia peter sebastian rudolph owl 2 web ontology language primer recommendation 27 1 2009 123 30 andreas holzinger chris biemann constantinos pattichis douglas kell need build explainable ai system medical domain arxiv preprint 31 yanping huang youlong cheng ankur bapna orhan firat dehao chen mia chen hyoukjoong lee et al gpipe efficient training giant neural network using pipeline parallelism advance neural information processing system 32 2019 32 snomed international data analytics snomed retrieved june 29 2022 33 bahador khaleghi explanation explainable ai xai retrieved june 9 2022 34 janis klaise arnaud van looveren giovanni vacanti alexandru coca alibi algorithm monitoring explaining machine learning model retrieved september 9 2022 35 alexey kurakin ian goodfellow samy bengio adversarial machine learning scale corr 36 accenture lab understanding machine explainable ai technical report retrieved september 9 2022 37 mu li david andersen jun woo park alexander smola amr ahmed vanja josifovski james long eugene shekita su scaling distributed machine learning parameter server proceeding usenix symposium operating system design implementation osdi 14 38 joanne luciano bosse andersson colin batchelor olivier bodenreider tim clark christine denney pher domarew et al translational medicine ontology knowledge base driving personalized medicine bridging gap bench bedside journal biomedical semantics 2 2011 39 scott lundberg gabriel erion hugh chen alex degrave jordan prutkin bala nair ronit katz jonathan himmelfarb nisha bansal lee local explanation global understanding explainable ai tree nature machine intelligence 2 1 2020 acm computing survey vol 55 no 9 article publication date january explainable ai xai core idea technique solution 40 scott lundberg lee unified approach interpreting model prediction advance neural information processing system 41 jiayuan mao chuang gan pushmeet kohli joshua tenenbaum jiajun wu concept learner interpreting scene word sentence natural supervision arxiv preprint 42 azalia mirhoseini anna goldie hieu pham benoit steiner quoc le jeff dean hierarchical model device placement proceeding international conference learning representation 43 azalia mirhoseini hieu pham quoc le benoit steiner rasmus larsen yuefeng zhou naveen kumar mohammad norouzi samy bengio jeff dean device placement optimization reinforcement learning ings international conference machine learning 44 christoph molnar interpretable machine learning retrieved september 9 2022 45 ramaravind kommiya mothilal amit sharma chenhao tan explaining machine learning classifier diverse counterfactual explanation corr 46 marco tulio ribeiro sameer singh carlos guestrin anchor explanation proceeding aaai conference artificial intelligence 47 kamruzzaman sarker ning xie derek doran michael raymer pascal hitzler explaining trained neural network semantic web technology first step proceeding twelfth international workshop symbolic learning reasoning nesy 2017 london uk july 2017 ceur workshop proceeding vol 2003 tarek besold artur avila garcez isaac noble 48 kamruzzaman sarker lu zhou aaron eberhart pascal hitzler artificial intelligence rent trend arxiv preprint 49 arne seeliger matthias pfaff helmut krcmar semantic web technology explainable machine learning model literature review iswc 2465 2019 50 ramprasaath selvaraju michael cogswell abhishek da ramakrishna vedantam devi parikh dhruv batra visual explanation deep network via localization proceeding ieee international conference computer vision 51 alexander sergeev mike del balso horovod fast easy distributed deep learning tensorflow arxiv preprint 52 avanti shrikumar peyton greenside anshul kundaje learning important feature propagating activation difference arxiv preprint 53 two sigma interpretability method machine learning brief survey retrieved september 9 2022 54 daniel smilkov nikhil thorat kim fernanda viégas martin wattenberg smoothgrad removing noise adding noise arxiv preprint 55 royal society explainable ai basic policy briefing retrieved september 9 2022 http 56 christian steinruecken emma smith david janz james lloyd zoubin ghahramani automatic tician automated machine learning method system challenge frank hutter lars kotthoff joaquin schoren springer 57 mukund sundararajan ankur taly qiqi yan axiomatic attribution deep network arxiv preprint 58 ajay thampi interpretable ai building explainable machine learning system manning publication 59 leslie valiant three problem computer science journal acm 50 1 2003 60 danding wang qian yang ashraf abdul brian lim designing explainable ai proceeding 2019 chi conference human factor computing system 61 eric xing qirong ho wei dai jin kyu kim jinliang wei seunghak lee xun zheng pengtao xie abhimanu kumar yaoliang yu petuum new platform distributed machine learning big data ieee transaction big data 1 2 2015 62 yongfeng zhang xu chen explainable recommendation survey new perspective corr 63 vijay arya rachel bellamy chen amit dhurandhar michael hind samuel hoffman stephanie houde vera liao ronny lu aleksandra mojsilović sami mourad pablo pedemonte ramya raghavendra john richards prasanna sattigeri karthikeyan shanmugam moninder singh kush varshney dennis wei yunfeng zhang one explanation doe not fit toolkit taxonomy ai explainability technique http received 20 october 2021 revised 8 july 2022 accepted 21 august 2022 acm computing survey vol 55 no 9 article publication date january 2023