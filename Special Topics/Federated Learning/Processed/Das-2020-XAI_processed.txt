1 opportunity challenge explainable artiﬁcial intelligence xai survey arun da graduate student member ieee paul rad senior member ieee deep neural network widely used mission critical system healthcare vehicle military direct impact human life however nature deep neural network challenge use mission critical application raising ethical judicial concern inducing lack trust explainable artiﬁcial intelligence xai ﬁeld artiﬁcial intelligence ai promotes set tool technique algorithm generate interpretable intuitive explanation ai decision addition providing holistic view current xai landscape deep learning paper provides mathematical summary seminal work start proposing taxonomy categorizing xai technique based scope explanation methodology behind algorithm explanation level usage help build trustworthy interpretable deep learning model describe main principle used xai research present historical timeline landmark study xai 2007 explaining category algorithm approach detail evaluate explanation map generated eight xai algorithm image data discus limitation approach provide potential future direction improve xai evaluation index ai xai interpretable deep learning machine learning computer vision neural network introduction artiﬁcial intelligence ai based algorithm especially using deep neural network transforming way approach task done human recent year seen surge use machine learning ml algorithm automating various facet science business social workﬂow surge partly due uptick research ﬁeld ml called deep learning dl thousand even billion neuronal parameter trained generalize carrying particular task successful use dl algorithm healthcare 1 3 ophthalmology 4 6 developmental disorder 7 9 autonomous robot vehicle 10 12 image processing classiﬁcation detection 13 14 speech audio processing 15 16 17 18 many indicate reach dl algorithm daily life easier access compute node using cloud computing ecosystem ai accelerator enhance performance access work ha submitted ieee possible publication copyright may transferred without notice version may no longer accessible da department electrical computer engineering university texas san antonio san antonio tx 78249 usa rad department information system cyber security university texas san antonio san antonio tx 78249 usa scale datasets storage enables deep learning provider research test operate ml algorithm scale small edge device 19 smartphones 20 using application programming interface apis wider exposure any application large number parameter deep neural network dnns make complex understand undeniably harder interpret regardless accuracy evaluation parameter might indicate good learning performance deep learning dl model could ently learn fail learn representation data human might consider important explaining decision made dnns require knowledge internal operation dnns missing focused getting accurate solution hence often ability interpret ai decision deemed secondary race achieve result crossing accuracy recent interest xai even government especially european general data protection regulation gdpr 21 regulation show important realization ethic 22 26 trust 27 29 bias 30 33 ai well impact adversarial example 34 37 fooling classiﬁer decision 38 miller et al describes curiosity one primary reason people ask explanation speciﬁc decision another reason might facilitate better learning reiterate model design generate better result explanation consistent across similar data point generate stable similar explanation data point time 39 explanation make ai algorithm expressive improve human understanding conﬁdence decision making promote impartial decision thus order maintain transparency trust fairness ml process explanation interpretable solution required ml system explanation way verify output decision made ai agent algorithm cancer detection model using microscopic image explanation might mean map input pixel contribute model output speech recognition model explanation might power spectrum information speciﬁc time contributed towards current output decision explanation also based parameter activation trained model explained either using surrogate decision tree using gradient method context reinforcement learning algorithm explanation might given agent made certain decision another however deﬁnitions interpretable explainable ai often generic might misleading 40 23 jun 2020 2 xai scope xai method focusing local instance trying understand model whole methodology algorithmic approach focused input data instance model parameter usage xai method developed integrated model applied any model general local mainly focus explanation individual data instance generates one explanation map g per data x global try understand model whole generally take group data instance generate one explanation map backprob core algorithmic logic dependent gradient propagated output prediction layer back input layer perturbation core mic logic dependent random carefully chosen change feature input data instance intrinsic explainability baked neural work architecture generally not transferrable architecture xai algorithm not dependent model architecture applied already trained neural network fig general categorization survey term scope methodology usage integrate some form reasoning 41 collection ai model based model inherently interpretable however affected drawback compared deep learning model per discus different approach perspective researcher address problem explainability deep learning algorithm method used effectively model parameter architecture already known however modern ai service produce challenge relative 42 nature problem ha information only input provided deep learning model not model survey present comprehensive overview explainable interpretable algorithm timeline important event research publication three deﬁned taxonomy illustrated figure unlike many survey only categorize summarize published research provide additional mathematical overview algorithm seminal work ﬁeld xai algorithm presented survey clustered three category described detail following section various evaluation technique xai presented literature also discussed along discussion limitation future direction method contribution summarized following 1 order systematically analyze explainable pretable algorithm deep learning taxonomize xai three category improve clarity accessibility approach 2 examine summarize classify core cal model algorithm recent xai research proposed taxonomy discus timeline seminal work 3 generate compare explanation map eight different xai algorithm outline limitation approach discus potential future direction improve trust transparency bias fairness using deep neural network explanation survey based published research year 2007 2020 various search source including google scholar acm digital library ieeexplore sciencedirect spinger preprints arxiv keywords able artiﬁcial intelligence xai explainable machine learning explainable deep learning interpretable machine learning used search parameter ii taxonomy organization prior published survey general explainability classiﬁed xai technique based scope usage 43 key difference survey classiﬁcation based methodology behind xai algorithm deep learning focus mathematical summary seminal paper evaluation strategy xai algorithm also mention popular software implementation various rithms described survey summarize taxonomy discussed survey section based illustration provided figure 1 scope scope explanation either local global some method extended locally explainable method designed express general individual feature attribution single instance input data x data population example given text document model understand sentiment text locally explainable model might generate attribution score individual word text globally explainable model provide insight decision model whole leading understanding attribution array input data local global scope explanation described detail section iv methodology core algorithmic concept behind plainable model generally categorized based methodology implementation general local global explainable algorithm categorized either 3 method method explainable algorithm doe one forward pas neural network generates attribution backpropagation stage utilizing partial derivative activation ples include saliency map saliency relevance map class activation map explainable algorithm focus turbing feature set given input instance either using occlusion partially substituting feature using ﬁlling operation generative algorithm masking conditional sampling etc generally only forward pas enough generate attribution representation without need backpropagating gradient methodology difference described section usage well developed explainable method speciﬁc scope methodology either embedded neural network model applied external algorithm explanation any explainable algorithm dependent model architecture fall category algorithm any change architecture need signiﬁcant change method minor change hyperparameters explainable algorithm generally signiﬁcant research interest seen oping explanation prediction already existing neural network model explained using explainable method method also widely applied variety input modality image text tabular data etc difference usage explainability method described section vi section vii discus some evaluation strategy used qualitatively quantitatively evaluate performance xai algorithm discussed survey present list desirable constraint applicable xai algorithm improve performance well expressiveness term transparency trust bias understanding desirable quality used guide generate novel xai algorithm favorable well expressive study suggests evaluation method still immature enormous potential research also provide list popular software package github platform chose package considerable user support implemented algorithm software platform support explaining either tensorﬂow pytorch machine learning model describing evaluation method software package conclude survey section viii survey mathematical equation algorithm described based set notation described table mathematical equation described survey might different respective research publication used similar notation describe mathematical idea throughout survey done aid reader common repository notation also timeline seminal research ﬁeld illustrated figure table table notation notation description x single instance input data population x set input feature except ith feature class label input x population predicted label input x xi ith feature input instance x x single instance x location single instance location predicted label input x f neural network model θ parameter neural network g explanation function g explanation model f zi j θ x activation output node layer j feature zj activation output summary node layer j feature activation map input x x binary activation map input x sc class score function z coalition vector shap maximum coalition size φj feature attribution feature j r zj relevance activation zj input image timeline provides information name xai method name ﬁrst author year publication iii definition preliminary various prior publication debate nuance deﬁning explainability interpretability neural network 44 45 support general concept explainable ai suite technique algorithm designed improve trustworthiness transparency ai system explanation described extra metadata information ai model offer insight speciﬁc ai decision internal functionality ai model whole various explainability approach applied deep neural network presented literature survey figure 2 illustrates one deep learning model take one input generates one output prediction goal explainable algorithm applied deep neural network towards explaining prediction using various method summarized survey fig illustration deep learning model generally single input instance x generates output no metadata explanation generated output classiﬁcation model inference scenario involve method model f considered blob information take input x generates output generally input x deep learning model function f θ describes f rd c number output class θ parameter model 4 classiﬁcation problem model inference described f θ x output prediction deﬁne key concept explored survey namely explainability deep learning model section survey explain deﬁnitions ﬁner detail deﬁnition 1 interpretability desirable quality feature algorithm provides enough expressive data understand algorithm work interpretable domain could include image text comprehensible human cambridge dictionary deﬁnes something interpretable possible ﬁnd meaning possible ﬁnd particular meaning deﬁnition 2 interpretation simpliﬁed representation complex domain output generated machine learning model meaningful concept understandable reasonable output prediction simple model easily interpreted traversing similarly small decision tree easily understood deep convolution network cnn model identify part input image led decision deﬁnition 3 explanation additional meta information generated external algorithm machine learning model describe feature importance relevance input instance towards particular output classiﬁcation deep learning model f input x output prediction class c explanation g generated generally explanation map e e rd g object shape input describes feature importance relevance particular dimension class output image explanation map equally sized pixel map whereas text might inﬂuence score deﬁnition 4 deep learning model f model parameter θ model architecture information known model considered model improves promotes trust however knowing model architecture parameter alone make model explainable deﬁnition 5 deep learning model f considered model parameter network architecture hidden typically deep learning model served vice restricted business platform exposed using apis take input form user provides model result text visual auditory presentation respective expected model output research xai important use ai algorithm healthcare 46 credit scoring 47 loan acceptance 48 need explain ml model result important ethical judicial well safety reason even though different facet xai important study suggests important concern 1 trustability 2 transparency 3 bias fairness ai algorithm current business model include interpretation step serving fig illustration 35 showing adversarial attack image class panda deliberately attacked predict gibbon high conﬁdence note attacked image visually similar original image human unable understand any change fig illustration 49 showing text image fool classiﬁers believing text feature particular task ml model production system however often limited small model use highly linear deep learning algorithm million parameter ml pipeline xai technique must improve three concern mentioned 1 improves transparency xai improves transparency fairness creating justiﬁcation decision could ﬁnd deter adversarial example 35 used properly deﬁnition 6 deep learning model considered transparent expressive enough understandable transparency part algorithm using external mean model decomposition simulation transparency important ass quality output prediction ward adversary adversarial example could hinder accurate decision making tie classiﬁer fooling classiﬁer believing fake image infact real figure 3 illustrates example image panda predicted gibbon high conﬁdence original panda image wa tampered adding some adversarial noise figure 4 illustrates classiﬁer learning classify based text data source tag watermark advertisement image rely autonomous algorithm aid daily life quality ai algorithm mitigate attack 34 provide transparency term model understanding textual visual report prime importance 2 improves trust social animal social life decision judgement primarily based knowledge available explanation situation 5 fig signiﬁcant expected improvement using xai technique support decision making believe xai important due improvement trust transparency understanding bias fairness trust generate scientiﬁc explanation logical reasoning decision better highly conﬁdent decision without any explanation deﬁnition 7 trustability deep learning model measure conﬁdence human intended working given model dynamic environment thus particular decision wa made prime importance improve trust 77 cluding subject matter expert developer layperson alike 78 80 fundamental explanation classiﬁer prediction ever important holder government agency build trustability transition connected environment 3 improves model bias understanding fairness xai promotes fairness help mitigate bias duced ai decision either input datasets poor neural network architecture deﬁnition 8 bias deep learning algorithm indicate disproportionate weight prejudice favor tion learnt model towards subset data due inherent bias human data collection deﬁciencies learning algorithm learning model behavior using xai technique different input data distribution could improve understanding skewness bias input data could generate robust ai model 81 understanding input space could help u invest bias mitigation method promote fairer model deﬁnition 9 fairness deep learning quality learnt model providing impartial decision without favoring any population input data 2020 2007 neural additive model 2020 50 gl causal concept effect cace 2019 51 gl ph 2019 automatic tions 2019 52 gl ph global attribution mapping 2019 53 gl per ph spectral relevance analysis 2019 49 gl bp ph salient relevance sr map 2019 54 lo bp ph randomization feature ing 2019 55 lo per ph 2018 56 lo bp ph 2018 randomized input sampling nation 2018 57 lo per ph concept activation vector 2018 58 gl ph axiomatic attribution 2017 59 lo bp ph 2017 deep attribution map 2017 60 lo bp ph deep taylor expansion 2017 61 lo ph prediction difference analysis 2017 62 lo per ph 2017 63 lo bp ph shapley additive explanation shap 2017 64 per ph local interpretable tions lime 2016 65 per ph 2016 class activation mapping cam 2016 66 lo bp ph guided backprop 2015 67 lo bp ph 2015 bayes rule list 2015 68 gl relevance backpropagation lrp 2015 69 bp ph generalized additive model 2015 70 gl deconvolution net 2014 71 lo bp ph 2014 bayesian case model 2014 72 gl saliency map 2013 73 lo bp ph 2013 activation maximization 2010 74 lo bp ph 2010 sparse penalized discriminant ysis 2008 75 gl 2008 bayesian averaging 2007 76 gl fig timeline seminal work towards explainable ai algorithm illustrated grey highlight indicate scope gl global lo local gl lo methodology bp backprop per perturbation neither bp per usage level intrinsic ph algorithm distribution xai technique could used way improve expressiveness generate meaningful explanation 6 table ii table abbreviation abbreviation deﬁnition ace automatic explanation ai artiﬁcial intelligence api application programming interface bam benchmarking attribution method brl bayesian rule list cace causal concept effect cam class activation mapping cav concept activation vector cnn convolutional neural network deconvnet deconvolution neural network dl deep learning dnn deep neural network eg expected gradient fmri functional magnetic resonance imaging gam generalized additive model ig integrated gradient irt interpretability randomization test lime local interpretable explanation lrp relevance backpropagation ml machine learning nam neural additive model osft feature test relu rectiﬁed linear unit rise randomized input sampling explanation rnn recurrent neural network sc system causability scale shap shapley additive explanation spda sparse penalized discriminant analysis spray spectral relevance analysis sr salient relevance tcav testing concept activation vector neighbor embedding vae variational auto encoders xai explainable artiﬁcial intelligence feature correlation many subspace data distribution understand fairness ai tracing back output prediction discrimination back input using xai technique understand subset feature correlated particular decision 82 discussed previously use xai could provide design ai continuously evolving model based prior parameter explanation issue improvement overall design thereby reducing human bias however choosing right method explanation done care considering interpretability machine learning model 40 proceed detailed discussion per taxonomy iv scope explanation local explanation consider scenario doctor ha make decision based result classiﬁer output doctor need careful understanding model prediction concrete answer decision question requires explanation local data point scrutiny fig illustration locally explainable model generally single input instance used explanation level explaining individual decision made classiﬁer categorized locally explainable algorithm generally locally explainable method focus single input data instance generate explanation utilizing different data feature interested generating g explaining decision made f single input instance diagram illustrated figure founding research local explanation used heatmaps based method bayesian technique feature importance matrix understand feature correlation importance towards output prediction output explanation always positive matrix vector newer research local explainable model improves old method attribution map based model get score positive negative correlation towards output classiﬁcation positive attribution value mean particular feature improves output class probability negative value mean feature decreased output class probability mathematical equation described section rest survey follows notation tabulated table 1 activation maximization interpreting feature importance convolutional neural network cnn model simpler ﬁrst layer generally learns level texture edge however move deeper cnn importance speciﬁc layer towards particular prediction hard summarize visualize since parameter subsequent layer inﬂuenced previous layer hence preliminary research tried understand neuronal activation input instance well individual ﬁlters speciﬁc layer 2010 locally explainable method called activation maximization wa introduced erhan et al 74 focus input pattern maximize given hidden unit activation author set optimization problem maximizing activation unit θ parameter model zi j θ x activation particular unit layer assuming ﬁxed parameter θ activation map found arg max x zij θ x 1 optimization converges could either ﬁnd average local minimum ﬁnd explanation map g pick one maximizes activation 7 fig class saliency map using gradient based attribution method shown image courtesy 73 goal minimize activation maximization loss ﬁnding larger ﬁlter activation correlated speciﬁc input pattern thus could understand feature importance input instance wa one ﬁrst published research express feature importance deep learning model wa later improved many researcher 2 saliency map visualization saliency map generation deep neural network ﬁrst introduced simonyan et al 73 way computing gradient output class category respect input image visualizing gradient fair summary pixel importance achieved studying positive gradient inﬂuence output author introduced two technique visualization 1 class model visualization 2 speciﬁc class visualization illustrated figure discus class model visualization global explainable method section class saliency visualization technique try ﬁnd approximate class score function sc x input image label class c using taylor expansion sc x b 2 w derivative class score function sc respect input image x speciﬁc point image w 3 light image processing visualize saliency map respect location input pixel positive gradient 3 relevance backpropagation lrp lrp technique introduced 2015 bach et al 69 used ﬁnd relevance score individual feature input data decomposing output prediction dnn relevance score atomic input calculated backpropagating class score output class node towards input layer propagation follows strict conservation property whereby equal redistribution relevance received neuron must enforced cnns lrp backpropagates information regarding relevance output class back input layer recurrent neural network rnns relevance propagated hidden state memory cell zero relevance assigned gate rnn consider simple neural network input instance x linear output activation output z system described yj p wijxi bj zj f yj 4 consider r zj relevance activation output goal get distribute r zj corresponding input x r zj xiwij yj ϵ sign yj 5 final relevance score individual input x summation relevance zj input xi r x x j 6 lrp method recently extended learn global explainability using lrp explanation map input global attribution algorithm discus some model section newer research ha also shown importance using method lrp model speciﬁc operation network pruning 83 author prune least important weight ﬁlters model understanding feature attribution individual layer reduces computation storage cost ai model without signiﬁcant drop model accuracy show another aspect using ai understanding model behavior utilizing new knowledge improve model performance 4 local interpretable explanation lime 2016 ribeiro et al introduced local interpretable agnostic explanation lime 65 derive representation understandable human lime try ﬁnd importance contiguous superpixels patch pixel source image towards output class hence lime ﬁnds binary vector x 0 1 represent presence absence continuous path superpixel provides highest representation towards class output work level single data input hence method fall local explanation also global explanation model based lime called described global explainable model sub section focus local explanation algorithm 1 lime algorithm local explanation input classiﬁer f input sample x number superpixels n number feature pick output explainable coefﬁcients linear model 1 x 2 n 3 pi x pick superpixels 4 obsi p 5 disti 6 end 7 simscore dist 8 xpick p simscore 9 l p simscore 10 return 8 fig local explanation image classiﬁcation prediction described using lime 65 top three class electric guitar p acoustic guitar p labrador p selecting group superpixels input image classiﬁer provides visual explanation top predicted label consider g explanation model class potentially interpretable model g decision tree linear model model varying interpretability let explanation complexity measured ω g πx z proximity measure two instance x z around x l f g πx represents faithfulness g approximating f locality deﬁned πx explanation ξ input data sample x given lime equation ξ x arg min l f g πx ω g 7 equation 7 goal lime optimization minimize loss l f g πx model agnostic way example visualization lime algorithm single instance illustrated figure algorithm 1 show step explain model single input sample overall procedure lime input instance permute data ﬁnding superpixel information fake data calculate distance similarity score permutation original observation know different class score original input new fake data make prediction new fake data using complex model depends amount superpixels choose original data descriptive feature picked improved prediction permuted data ﬁt simple model often time locally weighted regression model permuted data feature similarity score weight use feature weight coefﬁcients simple model make explanation local behavior complex model recent year seen many research improving extending lime algorithm variety new task summarize 84 mishra et al extended lime algorithm music content analysis temporal segmentation frequency segmentation input approach wa called slime wa applied explain prediction deep vocal detector 85 tomi peltola described kullbackleibler gence based lime called explain bayesian predictive model similar lime explanation generated using interpretable model whose parameter found minimizing predictive model thus local interpretable explanation generated projecting information tive distribution simpler interpretable probabilistic explanation model 86 rehman et al used agglomerative hierarchical clustering hc neighbor knn rithms replace random perturbation lime algorithm author use hc method group training data together cluster knn used ﬁnd closest neighbor test instance knn pick cluster cluster passed input data perturbation instead random perturbation lime algorithm author report approach generates model explanation stable traditional lime algorithm 87 bramhall et al adjusted linear relation lime consider relationship using quadratic approximation framework called lime qlime achieve considering linear approximation tangentials step within plex function result global stafﬁng company dataset suggests mean square loss mse lime linear relationship local level improves using qlime 88 shi et al introduced replacement method pick superpixels information image data using modiﬁed perturbed sampling operation lime thor converted traditional superpixel picking operation clique set construction problem converting superpixels undirected graph clique operation improves runtime due considerable reduction number perturbed sample method author compared method lime using mean absolute error mae coefﬁcient determination reported better result term understandability ﬁdelity efﬁciency 5 shapley additive explanation shap game oretically optimal solution using shapley value model explainability wa proposed lundberg et al 64 shap explains prediction input x computing individual feature contribution towards output prediction 9 lating data feature player coalition game shapley value computed learn distribute payout fairly shap method data feature individual category tabular data superpixel group image similar lime shap deduce problem set linear function function explanation linear function feature 89 consider g explanation model ml model f 0 1 coalition vector maximum coalition size φj feature attribution feature j g sum bias individual feature contribution g x j 8 lundberg et al 64 describes several variation baseline shap method kernelshap reduces evaluation required large input any ml model linearshap estimate shap value linear model weight coefﬁcients given independent input feature shap efﬁcient small maximum coalition size deepshap adapts deeplift method 59 leverage compositional nature deep neural network improve attribution since kernelshap applicable machine learning algorithm describe algorithm general idea kernelshap carry additive feature attribution method randomly sampling coalition removing feature input data linearizing model inﬂuence using shap kernel shap wa also explored widely research community wa applied directly improved many aspect use shap medical domain explain clinical making some recent work signiﬁcant merit summarized 90 antwarg et al extended shap method explain autoencoders used detect anomaly author classify anomaly using autoencoder comparing actual data instance reconstructed output since ﬁnal output reconstruction author suggests explanation based reconstruction error shap value found top performing feature divided contributing offsetting anomaly 91 sundararajan et al express various disadvantage shap method generating counterintuitive nation case certain feature not important uniqueness property attribution method proved using baseline shapley bshap method author algorithm 2 kernelshap algorithm local explanation input classiﬁer f input sample x output explainable coefﬁcients linear model 1 zk x 2 zk zk feature transformation reshape x 3 yk zk 4 wx f zk yk 5 linearmodel wx 6 return extend method using integrated gradient continuous domain 92 aa et al explored dependence shap value extending kernelshap method handle dependent feature author also presented method cluster shapley value corresponding dependent feature thorough comparison kernelshap method wa carried four proposed method replace conditional distribution kernelshap method using empirical approach either gaussian gaussian copula approach 93 lundberg et al described extension shap method tree framework called treeexplainer understand global model structure using local explanation author described algorithm compute local explanation tree polynomial time based exact shapley value 94 vegagarcia et al describe method explain prediction signal involving long memory lstm network author used deepshap algorithm explain individual instance test set based important feature training set however no change shap method wa done explanation generated time step input instance global explanation ai model behavior suite input data point could provide insight input feature pattern output correlation thereby promoting transparency model behavior various globally explainable method deduce complex deep model linear counterpart easier interpret model decision tree inherently globally interpretable output decision individual branch tree traced back source similarly linear model often fully explainable given model parameter generally globally explainable method work array input summarize overall behavior blackbox model illustrated figure explanation gf describes feature attribution model whole not individual input thus global explainability important understand general behavior model f large distribution input previously unseen data fig illustration globally explainable algorithm design 10 fig numerically computed image 73 us visualization method generate image representing target class mentioned illustration 1 global surrogate model global surrogate model could used way approximate prediction highly ai model interpretable linear model decision tree global explanation answer xai speciﬁcally generalized ai model variation ai model perform general use case surrogate model deep learning would extraction layer embeddings test input training linear classiﬁer embeddings coefﬁcients linear model could give insight model behaves shap lime considered surrogate model different methodology understand local correlation linear model spray technique see section also extract local feature group data understand model behavior 2 class model visualization activation maximization 74 introduced section also expanded global method using class model visualization described simonyan et al 73 given trained convnet f class interest c goal generate image visualization representative based scoring method used train f maximizes class probability score sc c arg max sc 2 9 thus generated image provides insight blackbox model learnt particular class dataset image generated using technique often called deep dream due colorful artefact generated visualization corresponding output class eration figure 11 illustrates three numerically computed class appearance model learnt cnn model goose ostrich limousine class respectively 3 lime algorithm global explanation lime model 65 wa extended submodular pick algorithm lime understand global correlation model study way lime provides global understanding model individual data instance providing non redundant global decision boundary machine learning model generating global importance individual feature done using submodular pick algorithm hence called lime algorithm 3 describes step generate global explanation blackbox model f learning individual feature importance input sample algorithm 3 lime algorithm global explanation input classiﬁer f input sample output explanation matrix submodular pick 1 define instance x budget b 2 x 3 flime f x 4 end 5 select b feature flime submodular pick 6 x b 7 xmin b number explanation inspect called budget w explanation matrix start explaining instance x using lime algorithm explained section domain image x represents individual input image b represent number superpixels selected lime algorithm select b feature f represents image better submodular pick algorithm start generating matrix size x b applying greedy optimization matrix chooses minimum number input min x cover number feature max f work similar gate model ﬁrst extracting independent explainability vector using lime operation hence computational overhead accuracy complexity depends partly amount data used understand model globally 4 concept activation vector cavs 58 kim et al introduced concept activation vector cavs global explainability method interpret internal state neural network concept domain consider machine learning model f vector space em spanned basis vector em see human understanding modelled vector space eh implicit vector eh correspond concept hence explanation function model global sense g becomes g em human understandable concept generated either input feature training data data simplify feature input domain example zebra deduced positive concept pc stripe illustrated figure negative set concept n gathered example set random photo contrast concept zebra layer activation layer j f zj calculated positive negative concept set activation trained using binary classiﬁer distinguish fj x x fj x x author proposed new method testing cavs tcav us directional derivative similar gradient based method evaluate sensitivity class prediction f change given input towards direction concept c speciﬁc layer h j k logit layer j class k particular input conceptual sensitivity class k c computed directional derivative sc k j x concept vector vj c 11 fig figure illustrates tcav process 58 describe random concept example b labelled example training data c trained neural network linear model segregating activation extracted speciﬁc layer neural network concept random example e ﬁnding conceptual sensitivity using directional derivative sc k j x lim hj k zj x ϵvj c k zj x ϵ k zj x vj c 10 tcav score calculated ﬁnd inﬂuence input towards xk denotes input label k tcav score given cavc k j x sc k j x 0 11 tcav unfortunately could generate meaningless cavs input concept not picked properly example input concept generated randomly would inherently generate bad linear model binary classiﬁcation thus tcav score good identiﬁer global explainability also concept high correlation shared object data car road could decrease efﬁciency tcav method human bias picking concept also considerable disadvantage using concept explainability cav method wa improved numerous research paper involved primary author cav 58 52 ghorbani et al described method called automatic explanation ace globally explain trained classiﬁer without human supervision unlike tcav method author carry resolution segmentation instance explained generates multiple resolution segment class segment reshaped similar input size activation segment found respect speciﬁc chosen bottleneck layer clustering activation removing outlier reveals similarity within activation tcav score individual concept provide importance score particular classiﬁcation author carried human subject experiment evaluate method found inspiring result one research question arise importance cluster author showed stitching clustered concept together image trained deep neural network wa capable classifying stitched image correct class category tends show extracted concept suitable within deep learning model work done goyal et al 51 improved tcav method proposing causal concept effect cace model look causal effect presence absence level concept towards deep learning model prediction method tcavs suffer confounding concept could happen training data instance multiple class even low correlation class also bias dataset could inﬂuence concept well color input data cace computed exactly change concept interest intervening counterfactual data tion author call ground truth cace also elaborate way estimate cace using variational auto encoders vaes called experimental result four datasets suggest improved clustering performance cace method even bias correlation dataset 95 yeh et al introduced conceptshap deﬁne importance completeness score discovered concept similar ace method mentioned earlier one aim conceptshap concept consistently clustered certain coherent spatial region however conceptshap ﬁnds importance individual concept high completeness score set concept vector c cm utilizing shapley value importance attribution 5 spectral relevance analysis spray spray technique lapuschkin et al 49 build top local instance based lrp explanation speciﬁc author described spectral clustering algorithm local explanation provided lrp understand process model globally analyzing spatial structure frequently occurring attribution lrp instance spray identiﬁes 12 normal abnormal behavior machine learning model algorithm 4 explains spray technique detail start ﬁnding local relevance map explanation every individual data instance x using lrp method relevance map downsized uniform shape size improve computation overhead generate tractable solution spectral cluster analysis sc carried lrp attribution relevance map cluster local explanation dimensional space eigenmap analysis carried ﬁnd relevant cluster ﬁnding eigengap difference two eigenvalue successive cluster completion important cluster returned user cluster optionally visualized using neighbor embedding visualization algorithm 4 spray analysis algorithm lrp attribution input classiﬁer f input sample x 1 x n output clustered input sample 1 x 2 fspray f x 3 end 4 reshape fspray 5 cluster fspray 6 cluster 7 return 8 optional visualize 6 global attribution mapping feature well deﬁned semantics treat attribution weighted conjoined ranking 53 feature rank vector ﬁnding local attribution global attribution mapping ﬁnds rank distance matrix cluster attribution minimizing cost function cluster distance way global attribution mapping identify difference explanation among subpopulation within cluster trace explanation individual sample tunable subpopulation granularity 7 neural additive model nams 50 agarwal et al introduced novel method train multiple deep neural network additive fashion neural network attend single input feature built extension generalized additive model gam nam instead use deep learning based neural network learn pattern feature jumping traditional gam not learn nams improved accurate gam introduced 70 scalable training several gpus consider general gam form g e β xk 12 fi univariate shape function e fi 0 x xk input k feature target variable g link function nams generalized parameterizing function fi neural network several hidden layer neuron layer see individual neural network applied feature xi output fi combined together using summing operation applying activation fig diagram interpretable nam architecture binary classiﬁcation illustrated 50 function fi used learn corresponding individual feature xi diagram nam provided 13 taken source paper author proposed exu hidden unit overcome failure relu activated neural network standard initialization ﬁt jagged function nams able learn jagged function due sharp change feature datasets often encountered gam exu hidden unit unit function calculated h x f ew x x w b input weight bias parameter author used weight initialization training normal distribution n x x 3 4 globally explainable model provides average score shape function individual neural network provide interpretable contribution feature positive negative value negative value reduce class probability positive value improve nam interesting architecture generate exact explanation feature space respect output prediction newer research could open venue expand idea cnns domain text difference methodology based core algorithmic approach followed xai method categorize xai method one focus change modiﬁcations input data one focus model architecture parameter fundamental change categorized survey respectively explanation generated iteratively probing trained machine learning model different variation input generally fall perturbation based xai technique 13 perturbation feature level replacing certain feature zero random counterfactual instance picking one group pixel superpixels explanation blurring shifting masking operation etc discussed prior section lime algorithm work superpixels information feature illustrated figure iteratively providing input patch visual explanation individual superpixels generated shap ha similar method probing feature correlation removing feature game theoretic framework intuitively see method trying understand neuronal activity impact individual feature corresponding class output any input perturbation mentioned categorized group method call xai method method described section summarized table iii 1 deconvolution net convolution visualization zeiler et al 71 visualized neural activation individual layer deep convolutional network occluding different segment input image generating visualization using deconvolution network deconvnet deconvnets cnns designed ﬁlters unpooling operation render opposite result traditional cnn hence instead reducing feature dimension deconvnet illustrated figure 14 used create activation map map back input pixel space thereby creating visualization neural feature activity individual activation map could help understand internal layer deep model interest learning allowing granular study dnns fig deconvolution operation applied using deconv layer attached end convnet deconvnet generates approximate version convolution feature thereby providing visual explanation figure 71 2 prediction difference analysis conditional sampling based approached wa used zintgraf et al 62 generate targeted explanation image classiﬁcation cnns assigning relevance value input feature respect predicted class c author summarize positive negative correlation individual data feature particular model decision given input feature x feature relevance estimated studying change model output prediction input different hidden feature hence denotes set input feature except x task ﬁnd difference p p 3 randomized input sampling explanation rise rise method introduced petsiuk et al 57 perturb input image multiplying randomized mask masked image given input saliency map corresponding individual image captured weighted average mask according conﬁdent score used ﬁnd ﬁnal saliency map positive valued heatmap individual prediction importance map blackbox prediction estimated using monte carlo sampling level architecture illustrated figure 15 4 randomization feature testing interpretability randomization test irt feature test osft introduced burn et al 55 focus discovering important feature replacing feature uninformative counterfactuals modeling feature replacement pothesis testing framework author illustrate interesting way examine contextual importance unfortunately deep learning algorithm removing one feature input possible due strict input dimension deep model value ﬁlling counterfactual value might lead unsatisfactory performance due correlation feature fig input image given deep learning model perturbed using various randomized mask conﬁdence score found individual masked input ﬁnal saliency map generated using weighting function 57 method saw previous section focus variation input feature space explain individual feature attribution f towards output class explainability method contrast utilize backward pas information ﬂow neural network understand neuronal inﬂuence relevance 14 table iii summary published research method method name interpretation perspective applied network comment discussion deconv net zeiler et al 71 neural activation individual layer ing input instance visualizing using deconv net alexnet author trained alexnet model genet dataset ﬁlter tions carried studied feature alization brought important insight dataset bias issue small training sample lime ribeiro et al 65 iterative perturbation input data instance ﬁnding superpixels author generated locally faithful tions using input perturbation around point interest study wa carried ass impact using lime explanation found explanation improve untrustworthy classiﬁer shap lundberg et al 64 probing feature correlation removing feature game theoretic framework shap produced consistently better result lime user study indicated shap explanation consistent man explanation however see evaluation section some recent study argue shap value albeit good generating explanation doe not improve ﬁnal decision making prediction difference analysis zintgraf et al 62 studying f removing individual feature x ﬁnd positive negative correlation individual feature towards output alexnet googlenet vgg one ﬁrst work look positive negative correlation individual feature towards output ﬁnding relevance value input feature trained various model imagenet dataset understand support output class various layer deep net randomized input pling explanation petsiuk et al 57 study saliency map randomized masking input randomization ture testing burn et al 55 counterfactual replacement feature study feature importance inception bert input x towards output see following subsection majority method focus either visualization activation individual neuron high inﬂuence overall feature attribution reshaped input dimension natural advantage xai method generation human understandable visual explanation 1 saliency map mentioned simonyan et al 73 introduced gradient based method generate saliency map convolutional net deconvnet work zeiler et al 71 mentioned previously perturbation method us backpropagation activation visualization deconvnet work wa impressive due relative importance given gradient value backprop rectiﬁed linear unit relu activation backprop traditional cnns would result zero value negative gradient however deconvnets gradient value not clipped zero allowed accurate visualization guided backpropagation method 67 96 also another class gradient based explanation improved upon 73 2 gradient class activation mapping cam saliency method use global average pooling layer pooling operation instead maxpooling zhou et al 66 modiﬁed global average pooling function class activation mapping cam localize image region input image single 63 56 improved cam operation deeper cnns better visualization gradcam attribution technique localizing neuronal activity cnn network allows query input image also counterfactual explanation highlight region image negatively contribute particular model output gradcam successfully applied explain classiﬁers image classiﬁcation image segmentation visual question answering vqa etc figure 16 illustrates segmentation method utilizing gradcam improve segmentation algorithm see another example using xai explanation improve performance deep neural network 3 salient relevance sr map li et al 54 proposed salient relevance sr map context aware salience map based lrp input image hence ﬁrst step ﬁnd lrp relevance map input image interest input dimension context aware salience relevance map algorithm take lrp relevance map ﬁnds saliency value individual pixel pixel salient group neighboring pixel distinct different pixel patch multiple scale done differentiate background foreground layer image aid visualization based detector posed sr map provide context explanation place sr gradient based method due use lrp relevance propagation method based taylor decomposition 61 also explored literature 15 fig illustration 63 showing segmentation result using cam output seed slightly different methodology global idea algorithm 5 describes sr map generation detail similar spray technique start lrp input instance contrast only ﬁnd lrp attribution relevance score single input interest context aware saliency relevance sr map generated ﬁnding dissimilarity measure based euclidean distance color space position saliency scale r r 2 r 4 found immediate context image x based attention function added generate sr map algorithm 5 salient relevance sr algorithm input classiﬁer f input sample x scale factor r output relevance map 1 flrp x 2 generatesrmap x flrp 3 r r 2 r 4 4 srmap x 5 return srmap 4 attribution map 60 ancona et al show gradient method gradient output corresponding input multiplied input useful generating interpretable explanation model outcome however 59 author proposed integrated gradient ig argue gradient based lack certain axiom desirable characteristic any gradient based technique author argue method deeplift 98 relevance propagation lrp 69 deconvolutional network deconvnets 71 guided 67 speciﬁc logic violates some axiom input data instance x consider baseline instance x attribution x model f summarized computing integral gradient point path baseline x method called integrated gradient igj x xj j z 1 dα 13 j describes dimension along gradient calculated calculation computer integral equation 13 efﬁciently approximated using summation instead many case baseline instance x chosen zero matrix vector example image domain baseline image chosen black image default text classiﬁcation baseline zero valued vector however choosing baseline arbitrarily could cause issue downstream example black baseline image could cause attribution method diminish importance black pixel source image attribution prior 99 concept try regularize feature attribution model training encode domain knowledge new method expected gradient eg wa also introduced paper substitute feature attribution method instead integrated gradient together attribution prior eg method encodes prior knowledge domain aid training process leading better model interpretability equation 14 show author remove inﬂuence baseline image integrated gradient still following axiom integrated gradient method distribution underlying data domain eg x z j z 1 δf δxj δα 14 since integration whole training distribution intractable author proposed reformulate integral expectation eg x e j δf δxj 15 5 desideratum method method saw mainly use saliency map class activation map gradient map visualization important feature recent research found numerous limitation method improve xai technique sundararajan et al 59 describes four desirable quality axiom gradient based method need follow 1 sensitivity every input baseline differ one feature different prediction differing feature given attribution 59 simple function f x 1 1 function value saturates x value greater equal one hence take simple gradient attribution method sensitivity hold 2 implementation invariance two network ally equivalent output equal input despite different implementation attribution method satisfy implementation invariance attribution always identical two functionally equivalent network 59 method deeplift lrp break implementation invariance use discrete gradient chain rule old discrete gradient general generally model fails provide implementation invariance attribution 16 table iv summary published research method method name interpretation perspective applied network comment discussion saliency map 67 71 73 96 visualizing gradient neural activation vidual layer using deconv net guided propagation etc image alexnet googlenet group technique xai research see evaluation section method serious disadvantage need improved selvaraju et al 63 localize neuronal activity ﬂowing last lutional layer cnn allow query counterfactual explanation describing negative inﬂuence input feature well alexnet resnet salient relevance li et al 54 take lrp relevance map ﬁnds saliency value individual pixel axiomatic attribution map sundararajan et al 59 feature importance based distance baseline instance googlenet lstm based nmt introduced axiom desirable quality method improved saliency map gradient time input map patternnet tribution kindermans et al 97 method mated signal input space cleaner attribution based using root point selection algorithm potentially sensitive unimportant feature aspect model deﬁnition 3 completeness attribution add difference output model function f input image x another baseline image x σn x f x x 4 linearity linearly composed neural network model linear combination two neural network model b attribution expected weighted sum attribution weight b respectively despite human understandable explanation explanation map practical disadvantage raise various concern application explain some concern later section vi model usage implementation level model intrinsic usage implementation level model intrinsic plainable method interpretable element baked model inherently interpretable either following strict axiom ﬁnal decision granular explanation decision etc deﬁnition intrinsic method explanation inherently mean fig illustration model intrinsic explainability algorithm explainability baked f f naturally explainable explainer depends model architecture not classiﬁer architecture without designing explanation algorithm speciﬁcally new architecture illustrated figure 17 1 tree model shallow el decision tree decision list inherently interpretable many explainable algorithm including lime shap us linear tree based model globally explainable extension core algorithm letham et al 68 introduced bayesian rule list brl generative model yield posterior distribution possible decision list improve interpretability keeping accuracy rule list ha else elseif rule generalized rule antecedent prediction add rule decision list model becomes accurate interpretable however support explanation deteriorate large number condition one way simplify problem ﬁnd frequent rule pattern learn decision list distribution using bayesian technique picking sample rule list priori distribution iteratively adding editing rule brl try optimize rule new rule distribution follows posteriori distribution optimized new rule sampled posteriori distribution recent research improved scalability brl 100 improving theoretical bound computational reuse highly tuned language library 2 generalized additive model gam caruana et al 70 introduced generalized additive model gam pairwise interaction improve accuracy ing interpretability gam however certain model gam require often million decision tree provide accurate result using additive algorithm also depending model architecture reduces accuracy gam model ﬁt using spline numerous method improved gam perhaps important work recent neural additive model discussed subsection 17 3 sparse lda discriminant analysis bayesian model lda wa introduced 101 ﬁnd interpretable predictive topic summary textual category datasets hierarchical labeling grosenick et al 75 introduced method called sparse penalized discriminant analysis spda improve interpretability classiﬁcation accuracy learning algorithm functional magnetic resonance imaging fmri data see published research several restriction use model intrinsic architecture requires careful rithm development problem setting difﬁculty using concept model intrinsic architecture apply existing model improve interpretability disadvantage method however long reasonable performance limit set model intrinsic architecture xai could help accelerate inherently interpretable model future ai research explaining classiﬁer decision require algorithm look ai model black white box black box mean xai algorithm know internal operation model architecture white box xai algorithm access model architecture layer structure hoc explanation methodology extremely useful existing accurate model beneﬁt added interpretability xai algorithm hence xai algorithm work any network architecture illustrated figure one main advantage explainable algorithm example already trained well established neural network decision explained without sacriﬁcing accuracy trained model deconvolution network 71 could used generate hoc explanation activation saliency map 73 attribution based method 59 applied considering network white black box lrp technique 69 discussed done training model completely shapley sampling method 64 also model agnostic activation maximization technique 74 applicable any network ﬁnd gradient value optimize activation fig illustration model explainability algorithm explainability algorithm applied f f made explainable externally vii evaluation methodology issue future direction far focused xai algorithm method rized scope methodology usage seminar work discussed survey tabulated table fundamental challenge xai research evaluate several proposed algorithm setting survey evaluation technique suggested ﬁeld still immature primary focus evaluation quantitative general evaluation scheme yet explored however summarize some method improve human understandability explainability method result based 38 102 103 general explanation follow constraint usable human setting 1 identity invariance identical data instance must produce identical attribution explanation 2 stability data instance belonging class c must generate comparable explanation 3 consistency data instance change one feature must generate explanation magniﬁes change 4 separability data instance different population must dissimilar explanation 5 similarity data instance regardless class difference closer generate similar explanation 6 implementation constraint time compute ment explainable algorithm minimal 7 bias detection inherent bias data instance detectable testing set similarity separability measure help achieve evaluation scheme several evaluation scheme suggested research community recent year present some evaluation technique actively gaining traction research community system causability scale sc explainability method applied ai system doe automated analysis data evaluation ai interface whole also important system causability scale sc wa introduced 104 understand requirement explanation facing often domain speciﬁc author described medical scenario sc tool wa applied framingham risk tool frt understand inﬂuence importance speciﬁc characteristic interface benchmarking attribution method bam preprint publication 105 introduced framework called marking attribution method bam evaluate correctness feature attribution relative tance bam dataset several model introduced bam dataset generated copying pixel group called common feature cf representing object category mscoco dataset 106 pasting miniplaces dataset 107 hypothesis 18 table v summary published research explainability interpretability deep learning algorithm indicates preprint version wa published year prior conference journal version method name publication year scope methodology usage agnostic speciﬁc domain bayesian averaging decision tree schetinin et al 76 2007 gl ot tab spda grosenick et al 75 2008 gl ot txt activation maximization erhan et al 74 2010 lo bp ph img saliency map simonyan et al 73 2013 lo bp ph img bayesian case model bcm kim et al 72 2014 gl ot any deconvolutional net zeiler et al 71 2013 lo bp ph img gam caruana et al 70 2015 gl ot tab lrp back et al 69 2015 bp ph img guided backprop springenberg et al 67 2015 lo bp ph img bayes rule list letham et al 68 2015 gl ot tab cam zhou et al 66 2016 lo bp ph img lime ribeiro et al 65 2016 per ph any shapley sampling lundberg et al 64 2017 per ph any selvaraju et al 63 2017 lo bp ph img prediction difference analysis pda zintgraf et al 62 2017 lo per ph img deep taylor expansion montavon et al 61 2017 lo ot ph img deep attribution map ancona et al 60 2017 lo bp ph img axiomatic attribution sundararajan et al 59 2017 lo bp ph img patternnet patternattribution kindermans et al 97 2017 lo bp ph img concept activation vector kim et al 58 2018 gl ot ph img rise petsiuk et al 57 2018 lo per ph img chattopadhay et al 56 2018 lo bp ph img randomization feature testing burn et al 55 2019 lo per ph img salient relevance sr map li et al 54 2019 lo bp ph img spectral relevance analysis lapuschkin et al 49 2019 gl bp ph img global attribution mapping ibrahim et al 53 2019 gl per ph img automatic explanation ghorbani et al 52 2019 gl ot ph img cace goyal et al 51 2019 gl ot ph img neural additive model agarwal et al 50 2020 gl ot img global gl local lo others ot backprop bp perturbation per tabular tab image img test txt any image text tabular pixel group information spatial location x model ignore feature relative importance hence attribution method focusing pasted object simply not good job enhancing feature attribution important feature author provided model contrast score mc compare relative feature importance difference model input dependence rate idr learn dependence cf single instance input independence rate iir percentage score image whose average feature attribution gr region r without cf le set threshold faithfulness monotonicity 108 author described metric named faithfullness evaluate correlation importance score feature performance effect feature towards correct prediction incrementally removing important feature predicting edited data instance measure effect feature importance later compare interpreter prediction relevance 109 author introduce monotonic attribute function thus monotonicity metric measure importance effect individual data feature performance model incrementally adding feature increasing order importance ﬁnd model performance model performance expected increase important feature added evaluation benchmark 110 seni et al introduced evaluation benchmark evaluate local explanation generated xai algorithm author created subset imagenet dataset 111 asked human annotator manually annotate image particular class weighted explanation map wa generated summarized average human representation explanation paring explanation generated locally explainable algorithm author presented method understand precision xai explanation compared human generated explanation one fundamental ﬂaw method could added human bias explanation however human label individual data point large population could nullify effect inherent bias software package opensource package greatly improved reproducible research ha real boon recent research deep learning xai alike mention some xai software package available github interpret interpretml used explain box model currently support explainable boosting decision tree decision rule list linearlogistic regression shap kernel explainer shap tree explainer lime ri sensitivity analysis partial dependence available 19 fig evaluate different technique ﬁgure lime shap us segmented superpixels understand feature importance gradient based saliency map integrated gradient lrp deeplift use backpropagation based feature importance pixel level original prediction accuracy model image row provided follows koala b sandbar c arabian camel leaf beetle column represents attribution map generated individual xai method scale ass shap value provided lower right section image gradient visualization ﬁgure created using deepexplain package visualization lime shap created individual implementation experiment carried jetstream cloud 112 image better viewed color iml package 113 maintained christoph molnar author 89 package cover feature importance tial dependence plot individual conditional expectation plot accumulated local effect tree surrogate lime shap available deepexplain package maintained marco ancona author 60 package support various based technique saliency map gradientinput integrated gradient deeplift lrp etc based method occlusion shap etc available drwhy modeloriented package several model agnostic model speciﬁc xai technique cluding feature importance ceteris paribus partial pendency plot conditional dependency etc available understanding explanation map figure 19 illustrate explanation map generated using various xai technique four image imagenet 111 dataset explain decision model imagenet row start original image imagenet followed explanation map generated gradient algorithm 1 saliency map 2 gradient time input 3 integrated gradient 4 lrp 5 deeplift 6 gradcam technique 1 lime 2 shap gradcam generates heatmap value ranging 0 1 0 mean no inﬂuence 1 mean highest inﬂuence individual pixel towards model output decision similarly shap method follows scale shap value however shap scale range indicating negative value decrease output class probability positive value increase output class probability corresponding input largest shap value generated set four image considered gradient visualization ﬁgure created using deepexplain package visualization lime shap created individual implementation original image column row figure 19 indicates correct prediction image koala prediction accuracy row b indicates correct prediction sandbar image accuracy row c indicates incorrect prediction horse arabian camel accuracy row indicates correct prediction leaf beetle percentage accuracy compare explanation map different column generated various xai technique discussed focusing saliency map gradient time input integrated gradient figure 19 visually verify improvement achieved integrated gradient prior method apparent image lower class probability example row b verify integrated gradient generated high attribution around sandy beach plastic chair little bit blue 20 fig illustration 114 showing adversarial attack involving small perturbation input layer neural network see small perturbation affect accuracy prediction however feature importance map highly affected small change illustrates ﬂaws current technique sky human evaluator make sense output human experience suggests sandbar involve beach hopefully sunny day bright blue cloud stark difference apparent visualization class output generated heatmap focused primarily plastic chair sandy beach without much emphasis cloud method lime shap generated superpixels maximized class probability see lime focusing primarily chair sky whereas shap focusing beach sky also note shap value generated low indicating lesser inﬂuence conﬁdence score limitation xai visualization future direction discussion brings some important ﬂaws xai visualization interpretability technique 1 inability deduce xai explanation map making 2 unavailability quantitative measure completeness correctness explanation map suggests use visualization technique application must reconsidered moving forward also better way representing presenting explanation considered example 115 weerts et al studied impact shap explanation improving human performance alert processing task author presented study evaluate whether certain scenario improved providing explanation decision result showed additional shap explanation class output probability not improve individual author saw interest ﬁnal class score making decision could catastrophic scenario similarly 110 mohseni et al presented grounded evaluation benchmark evaluated performance lime algorithm comparing explanation map erated lime weighted explanation map 10 human annotation result suggested lime creates some attribution irrelevant human explanation cause low explanation precision compared weighted explanation map generated human annotator shed light importance understanding mode explanation grounded explanation 102 improve explanation map meta information generated human adding constraint explanation introducing formal deﬁnitions explanation optimization problem several ﬂaws explanation map visualization explained researcher recent publication 114 ghorbani et al showed small perturbation input instance generate large change output interpretation popular xai method generate adversarial example thus threw interpretable saliency map generated popular method deeplift integrated gradient illustrated figure additionally 116 wang et al showed bias term often ignored could high correlation towards attribution 117 kindermans et al explained explanation network easily manipulable simple transformation author note expressiveness integrated gradient 59 deep taylor decomposition 61 highly depend reference point example baseline image x suggest reference point hyperparameter instead determined priori author mentioned method attribute incorrectly constant vector transformation input invariance prerequisite reliable attribution 118 adebayo et al suggested method inherently dependent model data erating process author proposed two randomization test gradient method namely model parameter randomization test data randomization test model parameter randomization test compared output saliency method trained model versus model random weight data randomization test applied saliency method input instance instance set invariance author found gradient gradcam passed sanity 21 check guided backprop guided gradcam method failed test suggesting method generate some explanation even without proper training newer method proposed literature explaining concept 51 52 58 discussed subsection could viewed new class improve xai method exploring explanation concept one could additional meta information factor contributed individual class prediction along traditional explanation locally explainable algorithm 119 zhou et al introduced interpretable basis composition way decomposing individual explanation based different object scene input instance decomposing decision several individual concept explanation ibd could help evaluate importance concept towards particular decision 97 kindermans et al suggested improvement method proposed patternnet nattribution estimate component data caused network activation patternnet similar ﬁnding gradient instead done using backprojection estimated signal data feature input space patternattribution improves upon lrp provide attribution input signal corresponding output class viii conclusion blindly trusting result highly predictive classiﬁer today standard inadvisable due strong inﬂuence data bias trustability adversarial example machine learning survey explored xai important several facet xai categorized respect scope methodology usage nature towards explaining deep neural network algorithm summary seminal algorithm explained survey tabulated table ﬁndings showed considerable research xai focused explainability algorithm due easier integration wider reach additionally large interest additive local surrogate model using superpixels information evaluate input feature attribution researcher uncovering limitation explanation map visualization see shift local model due shortcoming adversarial attack input invariance new trend using concept explanation gaining traction however evaluating method still challenge pose open question xai research current research landscape xai evaluation illustrates ﬁeld xai still evolving xai method developed chosen care user study shown typical explanation map alone might not aid decision making human bias interpreting visual explanation could hinder proper use xai application recent development evaluation show promising improvement xai evaluation landscape reference 1 torres yan aboutalebi da duan rad patient facial emotion recognition sentiment analysis using secure cloud hardware acceleration comput intell multimed big data cloud eng pp 2018 2 lee seo yun cho schiebler gefter van beek goo lee et deep learning application chest radiography computed tomography pp 2019 3 chen yang goodison sun approach identifying cancer subtypes using genomic data bioinformatics vol 36 no 5 pp 2020 4 sayres taly rahimy blumer coz hammel krause narayanaswamy rastegar wu et using deep learning algorithm integrated gradient explanation assist grading diabetic retinopathy ophthalmology vol 126 no 4 pp 2019 5 da rad choo nouhi lish martel distributed machine learning cloud teleophthalmology iot ing amd disease progression future generation computer system vol 93 pp 2019 6 son shin kim jung park park development validation deep learning model screening multiple abnormal ﬁndings retinal fundus image ophthalmology vol 127 no 1 pp 2020 7 mohammadian rad kia zarbo van laarhoven jurman venuti marchiori furlanello deep learning automatic stereotypical motor movement detection using wearable sensor autism spectrum disorder signal processing vol 144 pp mar 2018 8 heinsfeld franco craddock buchweitz meneguzzi identiﬁcation autism spectrum disorder using deep learning abide dataset neuroimage clinical vol 17 pp 2018 9 silva alaeddini najaﬁrad temporal graph traversal using reinforcement learning proximal policy optimization ieee access vol 8 pp 63 922 2020 10 lu filev tsiotras advanced planning autonomous vehicle using reinforcement learning deep inverse reinforcement learning robotics autonomous system vol 114 pp apr 2019 11 grigorescu trasnea cocias macesanu survey deep learning technique autonomous driving journal field robotics vol 37 no 3 pp 2020 12 feng rosenbaum hertlein glaser timm wiesbeck dietmayer deep object detection semantic segmentation autonomous driving datasets method challenge ieee transaction intelligent transportation system pp 2020 13 sahba da rad jamshidi image graph production dense captioning 2018 world autom vol ieee jun 2018 pp 14 bendre ebadi prevost najaﬁrad human action performance using deep recurrent attention model ieee access vol 8 pp 57 761 2020 15 bole rad voice biometrics deep voiceprint authentication system 2017 system system engineering conference sose ieee 2017 pp 16 panwar da roopaei rad deep learning approach mapping music genre 2017 system system engineering conference sose ieee 2017 pp 17 parra rad choo beebe detecting internet thing attack using distributed deep learning journal network computer application 102662 2020 18 chacon silva rad deep learning poison data attack detection 2019 ieee international conference tool artiﬁcial intelligence ictai ieee 2019 pp 19 kwasniewska szankin ozga wolfe da zajac ruminski rad deep learning optimization edge device analysis training quantization parameter iecon 2019 annu conf ieee ind electron pp oct 2019 20 zhang patras haddadi deep learning mobile wireless networking survey ieee communication survey tutorial vol 21 no 3 pp 2019 21 high level independent group artiﬁcial intelligence ai hleg ethic guideline trustworthy ai euorpean 2019 22 22 cath wachter mittelstadt taddeo floridi artiﬁcial intelligence good society u eu uk approach science engineering ethic vol 24 no 2 pp 2018 23 keskinbora medical ethic consideration artiﬁcial gence journal clinical neuroscience vol 64 pp jun 2019 24 etzioni etzioni incorporating ethic artiﬁcial telligence journal ethic vol 21 no 4 pp dec 2017 25 bostrom yudkowsky ethic artiﬁcial intelligence cambridge handbook artiﬁcial intelligence frankish ramsey ed cambridge cambridge university press 2014 pp 26 stahl wright ethic privacy ai big data implementing responsible research innovation ieee security privacy vol 16 no 3 pp 2018 27 weld bansal challenge crafting intelligible intelligence communication acm vol 62 no 6 pp may 2019 28 lui lamb artiﬁcial intelligence augmented intelligence collaboration regaining trust conﬁdence ﬁnancial sector information communication technology law vol 27 no 3 pp sep 2018 29 hengstler enkel duelli applied artiﬁcial intelligence trustthe case autonomous vehicle medical assistance device technological forecasting social change vol 105 pp apr 2016 30 chen cruz ramsey dickson duca hornak koes kurtzman hidden bias dataset lead misleading performance deep learning virtual screening plo one vol 14 no 8 2019 31 challen denny pitt gompels edward atanasova artiﬁcial intelligence bias clinical safety bmj quality safety vol 28 no 3 pp mar 2019 32 sinz pitkow reimer bethge tolias engineering le artiﬁcial intelligence neuron vol 103 no 6 pp sep 2019 33 osoba welser intelligence image risk bias error artiﬁcial intelligence rand corporation 2017 34 kurakin goodfellow bengio adversarial machine ing scale international conference learning representation iclr 2017 conference track proceeding nov 2016 35 goodfellow shlens szegedy explaining harnessing adversarial example international conference learning representation iclr 2015 conference track proceeding 2015 36 su vargas sakurai one pixel attack fooling deep neural network ieee transaction evolutionary computation vol 23 no 5 pp oct 2019 37 huang papernot goodfellow duan abbeel adversarial attack neural network policy international conference learning representation iclr 2017 workshop track proceeding feb 2017 38 miller explanation artiﬁcial intelligence insight social science pp 2019 39 sokol flach explainability fact sheet framework systematic assessment explainable approach fat 2020 proceeding 2020 conference fairness accountability transparency 2020 pp 40 rudin stop explaining black box machine learning model high stake decision use interpretable model instead nature machine intelligence vol 1 no 5 pp may 2019 41 doran schulz besold doe explainable ai really mean new conceptualization perspective ceur workshop proceeding 2018 42 castelvecchi open black box ai nature news vol 538 no 7623 20 2016 43 adadi berrada peeking inside survey explainable artiﬁcial intelligence xai ieee access vol 6 pp 52 160 2018 44 dosilovic brcic hlupic explainable artiﬁcial gence survey 2018 international convention information communication technology electronics microelectronics mipro 2018 proceeding 2018 45 chakraborty tomsett raghavendra harborne alzantot cerutti srivastava preece julier rao kelley braines sensoy willis gurram pretability deep learning model survey result 2017 ieee smartworld ubiquitous intelligence computing advanced trusted computed scalable computing communication cloud big data computing internet people smart city innovation ieee aug 2017 pp 46 zhu albadawy saha zhang harowicz mazurowski deep learning identifying radiogenomic association breast cancer computer biology medicine vol 109 pp 2019 47 van thiel van raaij artiﬁcial intelligent credit risk prediction empirical study analytical artiﬁcial intelligence tool credit risk prediction digital era journal accounting finance vol 19 no 8 dec 2019 48 turiel aste loan acceptance default prediction artiﬁcial intelligence royal society open science vol 7 no 6 191649 2020 49 lapuschkin aldchen binder montavon samek uller unmasking clever han predictor assessing machine really learn nature communication vol 10 no 1 1096 dec 2019 50 agarwal frosst zhang caruana hinton neural additive model interpretable machine learning neural net arxiv preprint 2020 51 goyal shalit kim explaining classiﬁers causal concept effect cace arxiv preprint 2019 52 ghorbani wexler zou kim towards automatic explanation advance neural information cessing system 2019 pp 53 ibrahim louie modarres paisley global tions neural network mapping landscape prediction proceeding 2019 conference ai ethic society 2019 pp 54 li tian mueller chen beyond saliency standing convolutional neural network saliency prediction relevance propagation image vision computing vol pp mar 2019 55 burn thomason tansey interpreting black box model via hypothesis testing arxiv preprint mar 2019 56 chattopadhay sarkar howlader balasubramanian generalized visual explanation deep convolutional network 2018 ieee winter conference application computer vision wacv ieee mar 2018 pp 847 57 petsiuk da saenko rise randomized input pling explanation model british machine vision conference 2018 bmvc 2018 jun 2018 58 kim wattenberg gilmer cai wexler viegas sayres interpretability beyond feature attribution quantitative testing concept activation vector tcav international conference machine learning icml 2018 2018 59 sundararajan taly yan axiomatic attribution deep network international conference machine learning icml 2017 2017 60 ancona ceolini oztireli gross towards better understanding attribution method deep neural network international conference learning representation iclr 2018 conference track proceeding 2018 61 montavon lapuschkin binder samek uller explaining nonlinear classiﬁcation decision deep taylor decomposition pattern recognition vol 65 pp 2017 62 zintgraf cohen adel welling visualizing deep neural network decision prediction difference analysis international conference learning representation iclr 2017 conference track proceeding feb 2017 63 selvaraju cogswell da vedantam parikh batra visual explanation deep network via localization proceeding ieee international conference computer vision 2017 64 lundberg lee uniﬁed approach interpreting model prediction advance neural information processing system 2017 pp 65 ribeiro singh guestrin trust proceeding acm sigkdd international conference knowledge discovery data mining kdd 16 new york new york usa acm press 2016 pp 23 66 zhou khosla lapedriza oliva torralba learning deep feature discriminative localization 2016 ieee ence computer vision pattern recognition cvpr ieee jun 2016 pp 67 springenberg dosovitskiy brox riedmiller striving simplicity convolutional net international conference learning representation iclr 2015 workshop track proceeding 2015 68 letham rudin mccormick madigan interpretable classiﬁers using rule bayesian analysis building better stroke prediction model annals applied statistic vol 9 no 3 pp sep 2015 69 bach binder montavon klauschen uller samek explanation classiﬁer decision relevance propagation plo one vol 10 no 7 jul 2015 70 caruana lou gehrke koch sturm elhadad intelligible model healthcare predicting pneumonia risk hospital readmission proceeding acm sigkdd international conference knowledge discovery data mining 2015 pp 71 zeiler fergus visualizing understanding tional network computer vision eccv 2014 fleet pajdla schiele tuytelaars ed cham springer international publishing 2014 pp 72 kim rudin shah bayesian case model generative approach reasoning prototype classiﬁcation advance neural information processing system 2014 73 simonyan vedaldi zisserman deep inside convolutional network visualising image classiﬁcation model saliency map international conference learning representation iclr 2014 workshop track proceeding dec 2013 74 erhan courville bengio understanding representation learned deep architecture department dinformatique et recherche operationnelle university montreal qc canada tech rep vol 1355 1 2010 75 grosenick greer knutson interpretable classiﬁers fmri improve prediction purchase ieee transaction neural system rehabilitation engineering vol 16 no 6 pp dec 2008 76 schetinin fieldsend partridge coat krzanowski everson bailey hernandez conﬁdent interpretation bayesian decision tree ensemble clinical application ieee transaction information technology biomedicine 2007 77 rossi building trust artiﬁcial intelligence journal tional affair vol 72 no 1 pp 2018 78 king aggarwal taddeo floridi artiﬁcial intelligence crime interdisciplinary analysis foreseeable threat solution science engineering ethic vol 26 no 1 pp 120 feb 2020 79 ˇ cerka e e liability damage caused artiﬁcial intelligence computer law security review vol 31 no 3 pp 2015 80 arrieta ıguez del ser bennetot tabik barbado ıa opez molina benjamin et explainable artiﬁcial intelligence xai concept taxonomy opportunity challenge toward responsible ai information fusion vol 58 pp 2020 81 zou schiebinger ai sexist racist time make fair nature vol 559 no 7714 pp jul 2018 82 du yang zou hu fairness deep learning computational perspective arxiv preprint 2019 83 yeom seegerer lapuschkin wiedemann uller samek pruning explaining novel criterion deep neural network pruning arxiv preprint 2019 84 mishra sturm dixon local interpretable agnostic explanation music content analysis proc int soc music inf retr conf ismir 2017 2017 pp 85 peltola local interpretable explanation bayesian predictive model via projection arxiv preprint 2018 86 rehman zafar mefraz khan dlime deterministic local interpretable explanation approach diagnosis system arxiv preprint 2019 87 bramhall horn tieu lohia quadratic local interpretable explanation approach smu data science review vol 3 no 1 4 2020 88 shi zhang fan modiﬁed perturbed sampling method local interpretable explanation arxiv preprint 2020 89 molnar interpretable machine learning lulu com 2020 90 antwarg shapira rokach explaining anomaly detected autoencoders using shap arxiv preprint 2019 91 sundararajan najmi many shapley value model explanation arxiv preprint 2019 92 aa jullum løland explaining individual prediction feature dependent accurate approximation shapley value arxiv preprint 2019 93 lundberg erion chen degrave prutkin nair katz himmelfarb bansal lee local explanation global understanding explainable ai tree nature machine intelligence vol 2 no 1 pp 2020 94 vega ıa aznarte shapley additive explanation forecasting ecol vol 56 101039 mar 2020 95 yeh kim arik li ravikumar pﬁster explanation deep neural network arxiv preprint 2019 96 mahendran vedaldi salient deconvolutional network lecture note computer science including subseries lecture note artiﬁcial intelligence lecture note bioinformatics springer 2016 pp 97 kindermans utt alber uller erhan kim ahne learning explain neural network patternnet patternattribution international conference learning representation iclr 2018 conference track proceeding jan 2018 98 shrikumar greenside kundaje learning important feature propagating activation difference proceeding international conference machine learning volume 70 ser 2017 31453153 99 erion janizek sturmfels lundberg lee learning explainable model using attribution prior arxiv preprint jun 2019 100 yang rudin seltzer scalable bayesian rule list international conference machine learning icml 2017 2017 101 wallace adam lda topic model structured sparsity proceeding ninth aaai conference artiﬁcial intelligence ser aaai press 2015 25752581 102 kim towards rigorous science interpretable machine learning arxiv preprint 2017 103 elshawi sherif sakr interpretability healthcare comparative study local machine learning interpretability technique proc ieee symp comput med 2019 104 holzinger carrington uller measuring quality explanation system causability scale sc ki unstliche intelligenz pp 2020 105 yang kim benchmarking attribution method relative feature importance corr vol 2019 106 lin maire belongie hay perona ramanan ar zitnick microsoft coco common object context european conference computer vision springer 2014 pp 107 zhou lapedriza khosla oliva torralba place 10 million image database scene recognition ieee trans pattern anal mach vol 40 no 6 pp jun 2018 108 melis jaakkola towards robust interpretability neural network advance neural information processing system 2018 pp 109 lu chen dhurandhar sattigeri shanmugam tu generating contrastive explanation monotonic attribute function arxiv preprint 2019 110 mohseni ragan evaluation mark local explanation machine learning arxiv preprint 2018 111 deng dong socher li kai li li imagenet hierarchical image database 2009 ieee conference computer vision pattern recognition 2009 pp 112 stewart cockerill foster hancock merchant skidmore stanzione taylor tuecke turner et jetstream scalable science engineering cloud environment proceeding 2015 xsede conference scientiﬁc advancement enabled enhanced cyberinfrastructure 2015 pp 24 113 molnar casalicchio bischl iml r package interpretable machine learning journal open source software vol 3 no 26 786 2018 114 ghorbani abid zou interpretation neural network fragile proceeding aaai conference artiﬁcial intelligence vol 33 2019 pp 115 weerts van ipenburg pechenizkiy grounded evaluation shap alert processing arxiv preprint 2019 116 wang zhou bilmes bias also matter bias attribution deep neural network explanation international conference machine learning 2019 pp 117 kindermans hooker adebayo alber utt ahne erhan kim un reliability saliency method explainable ai interpreting explaining visualizing deep learning springer 2019 pp 118 adebayo gilmer muelly goodfellow hardt kim sanity check saliency map advance neural information processing system 2018 pp 119 zhou sun bau torralba interpretable basis decomposition visual explanation proceeding european conference computer vision eccv 2018 pp arun da currently student research fellow secure ai autonomy lab open cloud institute university texas san antonio utsa san antonio tx usa arun received bachelor technology degree electrical electronics engineering cochin university science technology kerala india 2013 degree computer engineering university texas san antonio san antonio tx usa wa recipient utsa brain health consortium graduate student seed grant 2020 work behavior analytics child neurotypical disability member ieee ieee eta kappa nu honor society arun research interest area artiﬁcial intelligence computer vision distributed parallel computing cloud computing computer architecture peyman najaﬁrad paul rad received phd degree electrical computer engineering cyber analytics university texas san antonio san antonio tx usa founder director secure ai autonomy lab associate professor information system cyber security iscs university texas san antonio ha received ﬁrst degree sharif university technology computer engineering 1994 master artiﬁcial intelligence tehran polytechnic master computer science university texas san antonio magna cum laude december 1999 electrical computer engineering university texas san antonio wa recipient outstanding graduate student college engineering 2016 achieving rackspace innovation mentor program award establishing rackspace patent community board structure mentoring employee 2012 achieving dell corporation company excellence ace award austin exceptional performance innovative product research development contribution 2007 dell inventor milestone award top 3 dell inventor year hold 15 patent cyber infrastructure cloud computing big data analytics 300 product citation top fortune 500 leading technology company amazon microsoft ibm cisco amazon technology hp vmware ha advised 200 company cloud computing data analytics 50 keynote presentation serf advisory board several startup high performance cloud group chair cloud advisory council cac openstack foundation member number 1 open source cloud software san antonio tech bloc founding member childrens hospital san antonio foundation board member