graph neural network review method applic jie zhou ganqu cui shengd hu zhengyan zhang cheng yang b zhiyuan liu lifeng wang c changcheng li c maosong sun depart comput scienc technolog tsinghua univers beij china b school comput scienc beij univers post telecommun china c tencent incorpor shenzhen china r c l e n f keyword deep learn graph neural network b r c lot learn task requir deal graph data contain rich relat inform among element model physic system learn molecular ﬁngerprint predict protein interfac classifi diseas demand model learn graph input domain learn data like text imag reason extract structur like depend tree sentenc scene graph imag import research topic also need graph reason model graph neural network gnn neural model captur depend graph via messag pass node graph recent year variant gnn graph convolut network gcn graph attent network gat graph recurr network grn demonstr perform mani deep learn task thi survey propos gener design pipelin gnn model discuss variant compon temat categor applic propos four open problem futur research introduct graph kind data structur model set object node relationship edg recent research analyz graph machin learn receiv attent becaus great express power graph graph use denot larg number system across variou area includ social scienc social network wu et natur scienc physic system sanchez et battaglia et interact network fout et knowledg graph hamaguchi et mani research area khalil et uniqu data structur machin learn graph analysi focus task node cation link predict cluster graph neural network gnn deep learn base method oper graph domain due convinc perform gnn ha becom wide appli graph analysi method recent follow paragraph illustr fundament motiv graph neural network ﬁrst motiv gnn root histori neural network graph nineti recurs neural network ﬁrst util direct acycl graph sperduti starita frasconi et afterward recurr neural network feedforward neural network introduc thi literatur respect scarselli et mich tackl cle although success univers idea behind method build state transit system graph iter genc constrain extend represent abil recent advanc deep neural network especi convolut neural network cnn lecun et result rediscoveri gnn cnn abil extract local spatial featur compos construct highli express tion led breakthrough almost machin learn area start new era deep learn lecun et key cnn local connect share weight use multipl layer lecun et also great import solv problem graph howev cnn onli oper regular euclidean data like imag grid text sequenc data structur regard instanc graph therefor correspond author address zhou cui hu mail zhang yang liuzi liu fandywang wang harrychli li sm sun indic equal contribut content list avail sciencedirect ai open journal homepag http receiv septemb receiv revis form decemb accept januari avail onlin april author publish elsevi behalf keai commun thi open access articl cc licens http ai open straightforward gener cnn graph shown fig hard deﬁn local convolut ﬁlter pool oper hinder transform cnn euclidean domain domain extend deep neural model domain gener refer geometr deep learn ha emerg research area bronstein et thi umbrella term deep learn graph receiv enorm attent motiv come graph represent learn cui et hamilton et zhang et cai et goyal ferrara learn repres graph node edg subgraph vector ﬁeld graph analysi tradit machin learn approach usual reli hand engin featur limit inﬂex high cost follow idea represent learn success word embed mikolov et deepwalk perozzi et regard ﬁrst graph embed method base represent learn appli skipgram model mikolov et gener random walk similar approach grover leskovec line tang et tadw yang et also achiev howev method suffer two sever drawback hamilton et first paramet share node encod lead comput inefﬁci sinc mean number paramet grow linearli number node second direct embed method lack abil aliz mean deal dynam graph gener new graph base cnn graph embed variant graph neural work gnn propos collect aggreg inform graph structur thu model input output consist element depend exist sever comprehens review graph neural work bronstein et al provid thorough review geometr deep learn present problem difﬁculti solut plicat futur direct zhang et al propos anoth comprehens overview graph convolut network howev mainli focu convolut oper deﬁn graph investig comput modul gnn skip connect pool oper paper zhang et al wu et al chami et al survey paper gnn mainli focu model gnn wu et al categor gnn four group recurr graph neural network convolut graph neural network graph autoencod graph neural network zhang et al give systemat overview differ graph deep learn method chami et al propos graph encod decod model unifi network embed graph neural network model paper provid differ taxonomi mainli focu classic gnn model besid summar variant gnn differ graph type also provid detail summari gnn applic differ domain also sever survey focus speciﬁc graph learn ﬁeld sun et al chen et al give detail overview adversari learn method graph includ graph data attack defens lee et al provid review graph attent model paper propos yang et al focus heterogen graph represent learn node edg multipl type huang et al review exist gnn model dynam graph peng et al summar graph embed method combinatori optim conclud gnn erogen graph dynam graph combinatori optim section section section respect thi paper provid thorough review differ graph neural network model well systemat taxonomi applic summar contribut provid detail review exist graph neural network model present gener design pipelin discuss variant modul also introduc research theoret empir analys gnn model systemat categor applic divid cation structur scenario scenario present sever major applic correspond method scenario propos four open problem futur research provid thorough analysi problem propos futur research direct rest thi survey organ follow section present gener gnn design pipelin follow pipelin discuss step detail review gnn model variant detail includ section section section revisit research work theoret empir analys gnn section introduc sever major applic graph neural network appli structur scenario scenario scenario section propos four open problem graph neural network well sever futur research direct ﬁnalli conclud survey section gener design pipelin gnn thi paper introduc model gnn design view ﬁrst present gener design pipelin design gnn model thi section give detail step select comput modul consid graph type scale design loss function section respect ﬁnalli use exampl trate design process gnn speciﬁc task section fig left imag euclidean space right graph space zhou et al ai open later section denot graph g ðv eþ jvj n number node graph jej ne number edg adjac matrix graph represent learn use hv ov hidden state output vector node detail descript notat could found tabl thi section present gener design pipelin gnn model speciﬁc task speciﬁc graph type gener pipelin tain four step ﬁnd graph structur specifi graph type scale design loss function build model use comput ule give gener design principl background knowledg thi section design detail step discuss later section find graph structur ﬁrst ﬁnd graph structur applic usual two scenario structur scenario scenario structur scenario graph structur explicit applic applic molecul physic system knowledg graph scenario graph implicit ﬁrst build graph task build word graph text build scene graph imag get graph later design process tempt ﬁnd optim gnn model thi speciﬁc graph specifi graph type scale get graph applic ﬁnd graph type scale graph complex type could provid inform node connect graph usual categor graph edg direct graph direct one node anoth provid inform undirect graph edg undirect graph also regard two direct edg graph node edg mogen graph type node edg differ type heterogen graph type node edg play import role heterogen graph consid graph input featur topolog graph vari time graph regard dynam graph time inform care consid dynam graph note categori orthogon mean type combin one deal dynam direct heterogen graph also sever graph type design differ task hypergraph sign graph enumer type import idea consid addit format provid graph onc specifi graph type addit inform provid graph type consid design process graph scale clear classiﬁc criterion small larg graph criterion still chang develop comput devic speed memori gpu thi paper adjac matrix graph laplacian graph space complex store process devic regard graph graph sampl method consid design loss function thi step design loss function base task type train set graph learn task usual three kind task task focu node includ node classiﬁc node regress node cluster etc node classiﬁc tri categor node sever class node regress predict continu valu node node cluster aim partit node sever disjoint group similar node group task edg classiﬁc link predict requir model classifi edg type predict whether edg exist two given node task includ graph classiﬁc graph regress graph match need model learn graph represent perspect supervis also categor graph learn task three differ train set supervis set provid label data train set give small amount label node larg amount unlabel node train test phase transduct set requir model predict label given unlabel node induct set provid new unlabel node distribut infer node edg classiﬁc task recent mix scheme undertaken wang leskovec rossi et al crave new path toward mix set unsupervis set onli offer unlabel data model ﬁnd pattern node cluster typic unsupervis learn task task type train set design speciﬁc loss function task exampl classiﬁc task loss use label node train set build model use comput modul final start build model use comput modul commonli use comput modul tabl notat use thi paper notat descript rm euclidean space scalar vector matrix matrix transpos ident matrix dimens n gw convolut gw x n nv number node graph ne number edg graph n v neighborhood set node v v vector node v time step hv hidden state node v ht v hidden state node v time step ot v output node v time step evw featur edg node v w ek featur edg label k wi ui wo uo matric comput bi bo vector comput ρ altern function σ logist sigmoid function tanh hyperbol tangent function leakyrelu leakyrelu function multipl oper k vector concaten zhou et al ai open propag modul propag modul use propag inform node aggreg inform could captur featur topolog inform propag modul convolut oper recurr oper usual use aggreg inform neighbor skip connect oper use gather inform histor represent node mitig problem sampl modul graph larg sampl modul usual need conduct propag graph sampl modul usual combin propag modul pool modul need represent subgraph graph pool modul need extract mation node comput modul typic gnn model usual built combin typic architectur gnn model illustr middl part fig convolut oper recurr oper sampl modul skip connect use propag inform layer pool modul ad extract inform layer usual stack obtain better represent note thi architectur gener gnn model also except exampl ndcn zang wang combin ordinari differenti equat system ode gnn regard gnn model integr gnn layer continu time without propag discret number layer illustr gener design pipelin shown fig later section ﬁrst give exist instanti comput modul section introduc exist variant consid differ graph type scale section survey variant design differ train set section section correspond detail step step step pipelin ﬁnalli give concret design exampl section instanti comput modul thi section introduc exist instanti three tation modul propag modul sampl modul pool modul introduc three propag modul convolut oper recurr oper skip connect section respect introduc sampl modul pool modul section overview comput modul shown fig propag modul convolut oper convolut oper introduc thi section mostli use propag oper gnn model main idea tion oper gener convolut domain graph domain advanc thi direct often categor tral approach spatial approach spectral approach spectral approach work spectral represent graph method theoret base graph signal process shuman et deﬁn convolut oper spectral domain spectral method graph signal x ﬁrstli transform spectral domain graph fourier transform f convolut oper conduct convolut result signal transform back use invers graph fourier transform f transform deﬁn f ðxþ utx f ux u matrix eigenvector normal graph laplacian l degre matrix adjac matrix graph normal graph laplacian real symmetr posit semideﬁnit factor l uλut λ diagon matrix eigenvalu base convolut theorem mallat convolut oper deﬁn g f ðgþ f ðxþþ uðutg utxþ utg ﬁlter spectral domain simplifi ﬁlter fig gener design pipelin gnn model zhou et al ai open use learnabl diagon matrix gw basic function spectral method gw ugwutx next introduc sever typic spectral method design differ ﬁlter gw spectral network spectral network bruna et use abl diagon matrix ﬁlter gw diagðwþ w rn paramet howev thi oper comput inefﬁci ﬁlter local henaff et al attempt make spectral ﬁlter spatial local introduc izat smooth coefﬁcient chebnet hammond et al suggest gw approxim truncat expans term chebyshev polynomi tkðxþ kth order defferrard et al propos chebnet base thi theori thu oper written gw x k wktk l x l λmax l λmax denot largest eigenvalu rang eigenvalu l w rk vector chebyshev coefﬁcient chebyshev polynomi deﬁn tkðxþ observ oper sinc polynomi laplacian defferrard et al use thi convolut deﬁn convolut neural network could remov need comput eigenvector laplacian gcn kipf well simplifi convolut oper eq k allevi problem overﬁt assum λmax simplifi equat gw þ inþx two free paramet paramet constraint w obtain follow express gw w þ ax gcn introduc renorm trick solv vanish gradient problem eq þ fig overview comput modul zhou et al ai open þ dii p j aij final compact form gcn deﬁn h x input matrix w paramet h convolv matrix f f dimens input output respect note gcn also regard spatial method discuss later agcn model use origin graph structur denot relat node howev may implicit relat differ node adapt graph convolut network agcn propos learn underli relat li et agcn learn residu graph laplacian add origin laplacian matrix result proven effect sever dataset dgcn dual graph convolut network dgcn zhuang propos jointli consid local consist global consist graph use two convolut network captur local global consist adopt unsupervis loss ensembl ﬁrst convolut network eq second network replac adjac matrix posit pointwis mutual inform ppmi matrix h ρ b p apd p hw c ap ppmi matrix dp diagon degre matrix ap gwnn graph wavelet neural network gwnn xu et use graph wavelet transform replac graph fourier transform ha sever advantag graph wavelet fastli obtain without matrix decomposit graph wavelet spars ize thu result better explain gwnn outperform sever spectral method node classiﬁc task agcn dgcn tri improv spectral method tive augment graph laplacian gwnn replac fourier transform conclus spectral approach well theoret base also sever theoret analys propos recent see section howev almost spectral approach mention abov learn ﬁlter depend graph structur say ﬁlter appli graph differ structur model onli appli transduct set graph task basic spatial approach spatial approach deﬁn convolut directli graph base graph topolog major challeng spatial approach deﬁn convolut oper differ size neighborhood maintain local invari cnn neural fp neural fp duvenaud et use differ weight matric node differ degre ht v þ x v ht u v σ jn vj jn vj weight matrix node degre jn vj layer þ main drawback method appli scale graph node degre dcnn diffus convolut neural network dcnn atwood towsley use transit matric deﬁn neighborhood node node classiﬁc diffus represent node graph express h f ðwc p xþ x matrix input featur f dimens p n k n tensor contain power seri p pk matrix p transit matrix graph adjac matrix entiti transform diffus convolut represent k f matrix deﬁn k hop graph diffus f featur deﬁn k f weight matrix activ function model niepert et tract normal neighborhood exactli k node node normal neighborhood serv recept ﬁeld tional convolut oper lgcn learnabl graph convolut network lgcn gao et also exploit cnn aggreg perform max pool neighborhood matric node get featur element appli cnn comput hidden represent graphsag graphsag hamilton et gener induct framework gener embed sampl aggreg featur node local neighborhood n v ht u n v v σ h ht v k n v instead use full neighbor set graphsag uniformli sampl set neighbor aggreg inform gregat function graphsag suggest three aggreg mean aggreg lstm aggreg pool aggreg graphsag mean aggreg regard induct version gcn lstm aggreg permut invari requir speciﬁ order node spatial approach attent mechan ha success use mani task machin translat bahdanau et gehr et vaswani et machin read cheng et also sever model tri gener attent oper graph velickov et zhang et compar oper mention befor oper assign differ weight neighbor could allevi nois achiev better result gat graph attent network gat velickov et incorpor attent mechan propag step comput hidden state node attend neighbor follow strategi hidden state node v tain v ρ x v αvuwht u αvu k x v k w weight matrix associ linear transform appli node weight vector mlp moreov gat util attent use vaswani et al stabil learn process appli k independ tion head matric comput hidden state concaten featur comput averag result follow two zhou et al ai open output represent v kk σ x v αk vuwkht u v σ k x k x v αk vuwkht u αk ij normal attent coefﬁcient comput attent head attent architectur ha sever properti comput pair paralleliz thu ation efﬁcient appli graph node differ gree specifi arbitrari weight neighbor appli induct learn problem easili gaan gate attent network gaan zhang et also use attent mechan howev use mechan gather inform differ head replac averag oper gat gener framework spatial approach apart differ variant spatial approach sever gener framework propos aim integr differ model one singl framework monti et al propos mixtur model network monet gener spatial framework sever method deﬁn graph manifold gilmer et al propos messag pass neural network mpnn use messag pass function unifi sever variant wang et al propos neural network nlnn uniﬁ sever self attent method hoshen vaswani et velickov et battaglia et al propos graph network gn deﬁn gener framework learn represent monet mixtur model network monet monti et spatial framework tri uniﬁ model main includ cnn manifold gnn geodes cnn gcnn masci et anisotrop cnn acnn boscaini et manifold gcn kipf well dcnn atwood towsley graph formul ular instanc monet monet point manifold vertex graph denot v regard origin system neighbor u n v associ uðv uþ given two function f g deﬁn vertic graph point manifold convolut oper monet deﬁn ðf x j gjdjðvþf djðvþf x v wjðuðv uþþf ðuþ wjðuþ function assign weight neighbor accord thu djðvþf aggreg valu neighbor function deﬁn differ u w monet instanti sever method gcn function f g map node featur ðv uþ uðv uþ ðjn vj jn ujþ j uþþ ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ jn vjjn uj p monet model paramet wj learnabl mpnn messag pass neural network mpnn gilmer et extract gener characterist among sever classic model model contain two phase messag pass phase readout phase messag pass phase model ﬁrst use messag function mt aggreg messag mt v neighbor use updat function ut updat hidden state ht v v x v mt ht v ht u evu v ut ht v v evu repres featur undirect edg ðv uþ readout phase comput featur vector whole graph use readout function r b r ht v v g denot total time step messag function mt vertex updat function ut readout function r may differ set henc mpnn framework could instanti sever differ model via differ function set speciﬁc set differ model could found gilmer et nlnn neural network nlnn gener extend classic mean oper buad et comput vision oper comput hidden state posit weight sum featur possibl posit potenti tion space time spacetim thu nlnn view uniﬁc differ method hoshen vaswani et velickov et follow mean oper buad et gener oper deﬁn v c ðhtþ x f ht v ht u g ht u u index possibl posit posit v fðht v ht uþ comput scalar v u repres relat gðht uþ denot transform input ht u c ðhtþ normal factor differ variant nlnn deﬁn differ f g set detail found origin paper buad et graph network graph network gn battaglia et gener framework compar learn graph level represent unifi mani variant like mpnn nlnn interact network battaglia et watter et neural physic engin chang et commnet sukhbaatar ferguset dai et khalil et ggnn li et relat network raposo et santoro et deep set zaheer et point net qi et core comput unit gn call gn block gn block deﬁn three updat function three aggreg function k et k ht rk ht sk v v v φh v ht v ut h h rk receiv node sk sender node edg matric stack edg vector node vector time step respect v collect edg vector receiv node u global attribut graph represent φ ρ function variou set ρ function must invari input der take variabl length argument propag modul recurr oper recurr method pioneer thi research line major ferenc recurr oper convolut oper layer convolut oper use differ weight layer zhou et al ai open recurr oper share weight earli method base sive neural network focu deal direct acycl graph sperduti starita frasconi et mich et hammer et later concept graph neural network gnn wa ﬁrst propos scarselli et gori et extend exist neural network process graph type name model gnn thi paper distinguish gener name ﬁrst introduc gnn later variant requir converg hidden state talk method base gate mechan method graph node natur deﬁn featur relat node target gnn learn state embed hv rs contain inform neighborhood node state embed hv vector node v use produc output ov distribut predict node label comput step hv ov deﬁn hv f xv hn v xn v ov gðhv xvþ xv hn v xn v featur v featur edg state featur node neighborhood v tive f parametr function call local transit function share among node updat node state accord input neighborhood g local output function describ output produc note f g interpret feedforward neural network let h x xn matric construct stack state output featur node featur respect compact form h fðh xþ gðh xnþ f global transit function g global output function stack version f g node graph respect valu h ﬁxed point eq uniqu deﬁn assumpt f contract map suggest banach ﬁxed point theorem khamsi kirk gnn use follow classic iter scheme comput state fðht xþ ht denot iter dynam system eq converg exponenti fast solut ani initi valu though experiment result shown gnn power architectur model structur data still sever limit gnn requir f contract map limit model abil inefﬁci updat hidden state node iter toward ﬁxed point unsuit use ﬁxed point focu tion node instead graph becaus distribut tation ﬁxed point much smoother valu less inform distinguish node graphesn graph echo state network graphesn gallicchio mich gener echo state network esn jaeger graph use ﬁxed contract encod function onli train readout function converg ensur contract reservoir dynam consequ graphesn efﬁcient gnn sse stochast embed sse dai et also propos improv efﬁcienc gnn sse propos learn framework contain two step embed node updat parameter oper updat step bed project steadi state constraint space meet condit lagrangian propag gnn tiezzi et formal learn task constraint optim problem lagrangian framework avoid iter comput ﬁxed point converg procedur implicitli express constraint satisfact mechan method sever work attempt use gate mechan like gru cho et lstm hochreit schmidhub propag step diminish comput limit gnn improv propag inform across graph structur run ﬁxed number train step without ante converg ggnn gate graph neural network ggnn li et propos releas limit gnn releas requir function f contract map use gate recurr unit gru propag step also use time bptt comput gradient comput step ggnn found tabl node v ﬁrst aggreg messag neighbor updat function incorpor inform node previou timestep updat node hidden state hn v gather neighborhood inform node v z r updat reset gate lstm also use similar way gru propag process base tree graph tree lstm tai et al propos two extens tree structur basic lstm architectur tabl differ variant recurr oper variant aggreg updat ggnn ht n v p v k þ b zt v σðwzht n v þ v þ rt v σðwrht n v þ v þ ht v tanhðwht n v þ uðrt v v þþ ht v zt vþ v þ zt v ht v tree lstm child sum hti n v p v k htf n vk uf k hto n v p v k htu n v p v k v σðwixt v þ hti n v þ biþ ft vk σðwf xt v þ htf n vk þ bf þ ot v σðwoxt v þ hto n v þ boþ ut v tanhðwuxt v þ htu n v þ buþ ct v v ut v þ p v ft vk k ht v ot v tanhðct vþ tree lstm hti n v p k ui vl htf n vk p k uf vl hto n v p k uo l vl htu n v p k uu l vl graph lstm peng et hti n v p v ui mðv k htf n vk uf mðv k hto n v p v uo mðv k htu n v p v uu mðv k zhou et al ai open also extens recurs neural network base model mention befor tree special case graph node aggreg inform children instead singl forget gate tradit lstm unit node v contain one forget gate fvk child comput step display tabl v ot v ct v input gate output gate memori cell respect xt v input vector time design special kind tree node ha k children children order equat comput hti n v htf n vk hto n v htu n v tabl introduc separ paramet child paramet allow model learn tation condit state unit children graph lstm two type easili adapt graph lstm zayat ostendorf exampl appli graph howev simpliﬁ version sinc node graph ha incom edg parent sibl predecessor peng et al pose anoth variant graph lstm base relat extract task edg graph peng et variou label peng et al util differ weight matric repres differ label tabl mðv kþ denot edg label node v liang et al propos graph lstm network address semant object pars task use scheme adapt select start node determin node updat sequenc follow idea gener exist lstm data ha speciﬁc updat sequenc method mention abov agnost order node zhang et al propos sentenc lstm improv text encod convert text graph util graph lstm learn represent show strong resent power mani nlp problem propag modul skip connect mani applic unrol stack graph neural network layer aim achiev better result layer k layer make node aggreg inform neighbor k hop away howev ha observ mani experi deeper model could improv perform deeper model could even perform wors thi mainli becaus layer could also propag noisi format exponenti increas number expand borhood member also caus smooth problem becaus node tend similar represent aggreg tion model go deeper mani method tri add skip connect make gnn model deeper thi subsect duce three kind instanti skip connect highway gcn rahimi et al propos highway gcn use gate similar highway network zilli et output layer sum input gate weight tðhtþ σðwtht þ btþ tðhtþ þ ht tðhtþ þ ad highway gate perform peak layer speciﬁc problem discuss rahimi et column network cln pham et also util highway network ha differ function comput gate weight jkn xu et al studi properti limit hood aggreg scheme propos jump knowledg network jkn could learn adapt represent jkn select intermedi represent jump last layer node last layer make model adapt effect neighborhood size node need xu et al use three approach concaten experi aggreg inform jkn perform well experi social bioinformat citat network also combin model like gcn graphsag gat improv perform deepgcn li et al borrow idea resnet et densenet huang et resgcn segcn propos incorpor residu connect dens connect solv problem vanish gradient smooth detail hidden state node resgcn segcn comput þ ht dens experi deepgcn conduct point cloud mantic segment task best result achiev layer model sampl modul gnn model aggreg messag node neighborhood previou layer intuit track back multipl gnn layer size support neighbor grow exponenti depth allevi thi neighbor explos issu efﬁcient efﬁcaci way sampl besid deal larg graph alway store process neighborhood inform node thu sampl modul need conduct propag thi section introduc three kind graph sampl modul node sampl layer sampl subgraph sampl node sampl straightforward way reduc size neighbor node would select subset node neighborhood graphsag hamilton et sampl ﬁxed small number neighbor ensur neighborhood size node reduc sampl varianc chen et al introduc base stochast approxim rithm gcn util histor activ node control variat thi method limit recept ﬁeld neighborhood use histor hidden state afford approxim pinsag ying et propos sampl method simul random walk start target node thi approach choos top node highest normal visit count layer sampl instead sampl neighbor node layer sampl retain small set node aggreg layer control expans factor fastgcn chen et directli sampl recept ﬁeld layer use import sampl import node like sampl contrast ﬁxed sampl method abov huang et al introduc parameter trainabl sampler perform sampl condit former layer furthermor thi adapt sampler could optim sampl import reduc varianc simultan ladi zou et intend allevi sparsiti issu sampl gener sampl union neighbor node subgraph sampl rather sampl node edg build upon full graph fundament differ way sampl multipl subgraph restrict neighborhood search within subgraph tergcn chiang et sampl subgraph graph cluster gorithm graphsaint zeng et directli sampl node edg gener subgraph zhou et al ai open pool modul area comput vision convolut layer usual follow pool layer get gener featur complic graph usual carri rich hierarch structur great tanc classiﬁc task similar poolinglay alotofworkfocusesondesigninghierarchicalpoolinglay weintroduc twokindsof pool modul direct pool modul hierarch pool modul direct pool modul direct pool modul learn represent directli node differ node select strategi modul also call readout function variant simpl node pool simpl node pool method use sever model model tion appli node featur get global graph represent mpnn use method vinyal et readout function get graph represent design deal unord set fðht v xvþg use method produc order invari represent prediﬁn number step sortpool sortpool zhang et ﬁrst sort node embed accord structur role node sort embed fed cnn get represent hierarch pool modul method mention befor directli learn graph represent node investig hierarch properti graph structur next talk method follow chical pool pattern learn graph represent layer graph coarsen earli method usual base graph coarsen algorithm spectral cluster algorithm ﬁrstli use inefﬁci becaus eigendecomposit step graclu dhillon et provid faster way cluster node appli pool modul exampl chebnet monet use graclu merg node pair add addit node make sure pool procedur form balanc binari tree ecc convolut ecc simonovski daki design pool modul recurs downsampl oper downsampl method base split graph two compon sign largest eigenvector laplacian diffpool diffpool ying et use learnabl hierarch cluster modul train assign matrix st layer st softmax gnnt poolðat htþ ðstþtatst ht node featur matrix coarsen adjac matrix layer st denot probabl node layer assign coarser node layer þ gpool gpool gao ji use project vector learn jection score node select node score compar diffpool use vector instead matrix layer thu reduc storag complex project procedur doe consid graph structur eigenpool eigenpool et design use node featur local structur jointli use local graph fourier transform extract subgraph inform suffer efﬁcienc graph eigendecomposit sagpool sagpool lee et also propos use featur topolog jointli learn graph represent use base method reason time space complex fig overview variant consid graph type scale zhou et al ai open variant consid graph type scale abov section assum graph simplest format howev mani graph real world complex thi subsect introduc approach attempt address leng complex graph type overview variant shown fig direct graph ﬁrst type direct graph direct edg usual contain inform undirect edg exampl knowledg graph head entiti parent class tail entiti edg direct offer inform partial order instead simpli adopt asymmetr adjac matrix convolut oper model forward revers direct edg differ dgp kampffmey et use two kind weight matric wp wc convolut forward revers direct heterogen graph second variant graph heterogen graph node edg speciﬁc heterogen graph fv e φ ψg node vi associ type φðviþ edg ej type ψðejþ method approach toward thi graph type util concept path path scheme determin type node posit path l length train process instanti node sequenc connect two end node stanc captur similar two node may directli connect consequ one heterogen graph reduc sever homogen graph graph learn algorithm appli earli work base similar search investig sun et recent gnn model util propos han wang et ﬁrst perform graph attent neighbor use semant attent output embed node scheme gener ﬁnal represent node magnn fu et propos take intermedi node consider ﬁrst aggreg inform along use neural modul perform attent differ instanc associ node ﬁnalli perform attent differ scheme gtn yun et propos novel graph transform layer identiﬁ new connect unconnect node learn represent node learn new connect connect node server hop away close relat function method also work util work typic use differ function term sampl aggreg etc differ kind neighbor edg hetgnn zhang et address challeng directli treat neighbor differ type differ sampl featur encod aggreg step hgt hu et deﬁn type two neighbor node link ψðeijþ assign differ attent weight matric differ empow model take type inform consider method relat graph theedgeofsomegraphsmaycontainmoreinformationthanthetyp quantiti type may larg exert difﬁculti appli relat graph schlichtkrul et handl relat graph beck et convert origin graph bipartit graph origin edg also becom node one origin edg split two new edg mean two new edg edg node node thi transform use gate graph neural network follow recurr neural network convert graph edg inform sentenc aggreg function ggnn take hidden represent node relat input anoth approach schlichtkrul et requir convert origin graph format assign differ weight matric propag differ kind edg howev number relationsisverylarg number paramet inth model explod therefor introduc two kind regular reduc number paramet model amount relat basi decomposit wr deﬁn follow wr x b arbvb wr linear combin basi transform vb coefﬁcient arb decomposit gcn deﬁn wr direct sum set dimension matric need paramet ﬁrst one method multiplex graph complex scenario pair node graph ate multipl edg differ type view differ type edg graph form multipl layer layer repres one type relat therefor multiplex graph also refer graph graph exampl youtub three differ relat two user share subscript comment edg type assum independ therefor simpli split graph subgraph one type edg might optim solut mgcn et introduc gener represent resent node layer gnn represent project gener represent use fig overview method unsupervis loss zhou et al ai open differ project matric aggreg form next layer gener represent dynam graph anoth variant graph dynam graph graph structur exist edg node keep chang time model graph structur data togeth time seri data dcrnn li et stgcn yu et ﬁrst collect spatial inform gnn feed output sequenc model like model rnn differ jain et yan et collect spatial tempor messag time extend static graph structur tempor connect appli tradit gnn extend graph similarli dgnn manessi et feed output embed node gcn separ lstm weight lstm share node hand evolvegcn pareja et argu directli model dynam node represent hamper model perform graph node set keep chang therefor instead treat node featur input rnn feed weight gcn rnn captur intrins dynam graph interact recent survey huang et classiﬁ dynam network sever categori base link durat group exist model categori accord special also establish gener framework model dynam graph ﬁt exist model gener framework graph type variant graph hypergraph sign graph also model propos address challeng hypergraph hypergraph denot g ðv e weþ edg e e connect two vertic assign weight w adjac matrix hypergraph repres jej matrix l lv e v e v e hgnn feng et propos hypergraph convolut cess high order interact node h v e v xw dv de x node degre matrix edg weight matrix edg degre matrix node featur matrix respect w learnabl paramet thi formula deriv approxim hypergraph laplacian use truncat chebyshev polynomi sign graph sign graph graph sign edg edg either posit neg instead simpli treat neg edg absent edg anoth type edg sgcn derr et util balanc theori captur interact posit edg neg edg intuit balanc theori suggest friend posit edg friend also friend enemi neg edg enemi friend therefor provid theoret dation sgcn model interact posit edg neg edg larg graph mention section sampl oper usual use process graph besid sampl techniqu also method scale problem leverag approxim person pagerank method propos klicpera et al bojchevski et al avoid calcul propag matric rossi et al propos method precomput graph convolut ﬁlter differ size efﬁcient train infer model squeez multipl gcn layer one singl propag layer mitig neighbor explos issu henc highli scalabl efﬁcient variant differ train set thi section introduc variant differ train set supervis set label provid loss function easi design label sampl pervis set label sampl loss function depend inform provid graph input featur graph topolog thi section mainli introduc variant unsupervis train usual base idea contrast learn overview method mention shown fig graph unsupervis graph represent learn ha trend extend ae graph domain graph gae kipf well ﬁrst use gcn encod node graph use simpl decod reconstruct adjac matrix comput loss similar origin adjac matrix reconstruct matrix h gcnðx aþ ρðhhtþ kipf well also train gae model variat manner model name variat graph vgae adversari regular graph arga pan et employ gener adversari network gan regular graph could learn robust node represent instead recov adjac matrix wang et al park et al tri reconstruct featur matrix mgae wang et util margin denois get robust node represent build symmetr graph gala park et propos laplacian sharpen invers oper laplacian smooth decod hidden state thi mechan allevi oversmooth issu gnn train differ abov age cui et state recov loss compat downstream task therefor appli adapt learn measur pairwis node similar achiev perform node cluster link predict contrast learn besid graph contrast learn pave anoth way unsupervis graph represent learn deep graph infomax dgi velickov et maxim mutual inform node represent graph represent infograph sun et aim learn graph represent mutual inform mizat represent represent differ scale includ node edg triangl hassani khasahmadi contrast represent adjac matrix graph diffus achiev perform multipl graph learn task zhou et al ai open design exampl gnn thi section give exist gnn model illustr design process take task heterogen graph pretrain exampl use hu et model illustr design process find graph structur paper focus applic demic knowledg graph recommend system adem knowledg graph graph structur explicit recommend system user item review regard node interact among regard edg graph structur also easi construct specifi graph type scale task focu heterogen graph type node edg consid rate ﬁnal model academ graph dation graph contain million node model consid efﬁcienc problem conclus model focu heterogen graph design loss function downstream task hu et task predict academ graph model learn node represent pretrain step pretrain step label data avail graph gener task design learn node bed ﬁnetun step model ﬁnetun base train data task supervis loss task appli build model use comput modul final model built comput modul propag modul thor use convolut oper hgt hu et mention befor hgt incorpor type node edg propag step model skip connect also ad architectur sampl modul special design sampl method hgsampl hu et appli heterogen version ladi zou et model focus learn node represent pool modul need hgt layer stack multipl layer learn better node embed analys gnn theoret aspect thi section summar paper theoret dation explan graph neural network variou perspect fig applic scenario icon made freepik flaticon zhou et al ai open graph signal process spectral perspect view gcn perform convolut oper input featur spectral domain follow graph signal process theori exist sever work analyz gnn graph signal cess li et al ﬁrst address graph convolut graph neural network actual laplacian smooth smooth featur matrix nearbi node similar hidden represent laplacian smooth reﬂect homophili assumpt nearbi node suppos similar laplacian matrix serv ﬁlter input featur sgc wu et remov weight matric nonlinearti layer show ﬁlter reason whi gnn work follow idea ﬁltere zhang et al cui et al nt maehara nt maehara chen et al analyz differ ﬁlter provid new insight achiev ﬁltere eigenvalu agc zhang et design graph ﬁlter l accord frequenc respons function age cui et demonstr ﬁlter λmax l could get better result λmax maximum eigenvalu laplacian matrix despitelinearﬁlt graphheat leveragesheatkernelsfor better properti nt maehara nt maehara state graph convolut mainli denois process input featur model perform heavili depend amount nois featur matrix allevi issu chen et al present two metric measur smooth node represent gnn model author conclud ratio key factor gener gener abil gnn also receiv attent recent scarselli et al prove limit class gnn garg et al give much tighter gener bound base rademach bound neural network verma zhang analyz stabil gener properti gnn differ convolut ﬁlter author conclud stabil gnn depend largest eigenvalu ﬁlter knyazev et al focu gener abil attent mechan gnn conclus show attent help gnn gener larger noisi graph express express gnn xu et al morri et al show gcn graphsag less discrimin tabl applic graph neural network area applic refer graph mine graph match riba et li et graph cluster zhang et ying et tsitsulin et physic physic system model battaglia et sukhbaatar ferguset watter et hoshen kipf et sanchez et chemistri molecular fingerprint duvenaud et kearn et chemic reaction predict et al biolog protein interfac predict fout et al side effect predict zitnik et al diseas classiﬁc rhee et al knowledg graph kb complet hamaguchi et schlichtkrul et shang et kg align wang et zhang et xu et gener graph gener shchur et nowak et et et de cao kipf li et shi et liu et grover et combinatori optim combinatori optim khalil et nowak et li et kool et bello et vinyal et sutton barto dai et gass et zheng et selsam et sato et trafﬁc network trafﬁc state predict cui et yu et zheng et guo et recommend system interact predict van den berg et ying et social recommend wu et fan et structur stock market matsunaga et yang et chen et li et kim et softwar deﬁn network rusek et al amr graph text song et beck et text text classiﬁc peng et yao et zhang et tai et sequenc label zhang et marcheggiani titov neural machin translat bast et marcheggiani et beck et relat extract miwa bansal peng et song et zhang et event extract nguyen grishman liu et fact veriﬁc zhou et liu et zhong et question answer song et de cao et qiu et tu et ding et relat reason santoro et palm et battaglia et imag social relationship understand wang et al imag classiﬁc garcia bruna wang et lee et kampffmey et marino et visual question answer teney et wang et narasimhan et object detect hu et gu et interact detect qi et jain et region classiﬁc chen et al semant segment liang et landrieu simonovski wang et qi et program veriﬁc allamani et li et zhou et al ai open wl test algorithm graph isomorph test xu et al also propos gin express gnn go beyond wl test et al discuss gnn express fragment ﬁrst order logic author ﬁnd exist gnn hardli ﬁt logic learn graph topolog structur garg et al prove local depend gnn variant capabl learn global graph properti includ diamet cycl motif louka dehmami et al argu exist work onli consid express gnn inﬁnit layer unit work investig represent power gnn ﬁnite depth width oono suzuki discuss asymptot havior gnn model deepen model dynam system invari node order graph output embed gnn suppos equivari input featur maron et al character equivari linear layer build invari gnn maron et al prove result univers invari gnn obtain tensor keriven e provid altern proof extend thi conclus equivari case chen et al build connect graph isomorph test prove equival chen et al leverag describ express gnn transfer determinist characterist gnn parameter unti graph suggest abil transfer across graph transfer perform guarante levi et al investig transfer spectral graph ﬁlter show ﬁlter abl transfer graph domain ruiz et al analyz gnn behaviour graphon graphon refer limit sequenc graph also seen gener dens graph author conclud gnn transfer across graph obtain determinist graphon differ size label efﬁcienc supervis learn gnn need consider amount label data achiev satisfi perform improv label efﬁcienc ha studi perspect activ learn inform node activ select label oracl train gnn cai et al gao et al hu et al demonstr select inform node node uncertain node label efﬁcienc dramat improv empir aspect besid theoret analysi empir studi gnn also requir better comparison evalu includ sever empir studi gnn evalu benchmark evalu evalu machin learn model essenti step research concern experiment reproduc replic rais year whether extent gnn model work part model contribut ﬁnal perform investig fundament question studi fair evalu strategi urgent need node classiﬁc task shchur et al explor gnn model perform train strategi hyperparamet tune work conclud differ dataset split lead dramat differ rank model also simpl model could outperform complic one proper set errica et al review sever graph classiﬁc model point compar inproperli base rigor evalu structur format turn fulli exploit graph classiﬁc et al discuss architectur design gnn model number layer aggreg function huge amount experi thi work provid comprehens guidelin gnn design variou task benchmark benchmark dataset imagenet signiﬁc machin learn research howev graph learn benchmark problemat exampl node classiﬁc dataset contain onli node small compar graph furthermor experiment protocol across studi uniﬁ hazard atur mitig thi issu dwivedi et al hu et al provid scalabl reliabl benchmark graph learn dwivedi et al build benchmark dataset multipl main task ogb hu et offer dataset furthermor work evalu current gnn model provid leaderboard comparison applic graph neural network explor wide rang main across supervis unsupervis ment learn set thi section gener group applic two scenario structur scenario data ha explicit relat structur scenario one hand emerg scientiﬁc research graph mine model physic system chemic system hand rise dustrial applic knowledg graph trafﬁc network recommend system scenario tional structur implicit absent scenario gener includ imag comput vision text natur languag process two activ develop branch ai research simpl illustr applic fig note onli list sever repres applic instead provid exhaust list summari applic could found tabl structur scenario follow subsect introduc gnn applic structur scenario data natur perform graph structur graph mine ﬁrst applic solv basic task graph mine gener graph mine algorithm use identifi use structur downstream task tradit graph mine challeng includ frequent mine graph match graph classiﬁc graph cluster etc although deep learn downstream task directli solv without graph mine intermedi step basic challeng worth studi gnn perspect graph match ﬁrst challeng graph match tradit method graph match usual suffer high comput complex emerg gnn allow research captur structur graph use neural network thu offer anoth solut problem riba et al propos siames mpnn model learn graph edit distanc siames framework two parallel mpnn structur weight share train object emb pair graph small edit distanc close latent space li et al design similar method periment scenario similar search zhou et al ai open control ﬂow graph graph cluster graph cluster group vertic graph cluster base graph structur node attribut variou work zhang et node represent learn ope represent node pass tradit tere algorithm apart learn node embed graph pool ying et seen kind cluster recent tsitsulin et al directli target cluster task studi desir properti good graph cluster method propos optim spectral modular remark use graph cluster metric physic model physic system one fundament aspect understand human intellig physic system model object system interact object simul physic system requir model learn law system make predict next state tem model object node interact edg system simpliﬁ graph exampl particl system particl interact via multipl interact includ collis hoshen spring connect electromagnet forc kipf et particl seen node teract seen edg anoth exampl robot system form multipl bodi arm leg connect joint bodi joint seen node edg respect model need infer next state bodi base current state system principl physic befor advent graph neural network work process graph represent system use avail neural block tion network battaglia et util mlp encod denc matric graph commnet sukhbaatar ferguset perform node updat use node previou represent averag node previou represent vain hoshen introduc attent mechan vin watter et combin cnn rnn battaglia et emerg gnn let us perform reason object relat physic simpliﬁ effect way nri kipf et take trajectori object input infer explicit interact graph learn dynam model simultan interact graph learn former trajectori trajectori predict gener decod interact graph sanchez et al propos graph model encod graph form bodi joint robot system learn polici stabli control system combin gn reinforc learn chemistri biolog molecular fingerprint molecular ﬁngerprint serv way encod structur molecul simplest ﬁngerprint hot vector digit repres exist absenc particular substructur ﬁngerprint use molecul search core step drug design tional molecular ﬁngerprint ﬁxed vector howev molecul natur seen graph atom node edg therefor appli gnn molecular graph obtain better ﬁngerprint duvenaud et al propos neural graph ﬁngerprint neural fp calcul substructur featur vector via gcn sum get overal represent kearn et al explicitli model atom atom pair independ emphas atom interact troduc edg represent et uv instead aggreg function ht n v p ðvþ et uv chemic reaction predict chemic reaction product tion fundament issu organ chemistri graph transform polici network et encod input molecul erat intermedi graph node pair predict network polici network protein interfac predict protein interact use interfac form amino acid residu particip protein protein interfac predict task mine whether particular residu constitut part protein gener predict singl residu depend neighbor due let residu node protein repres graph leverag machin learn rithm fout et al propos method learn ligand receptor protein residu represent merg classiﬁc xu et introduc approach extract summar local global featur better predict biomed engin interact network rhee et al leverag graph convolut relat network breast cancer subtyp classiﬁc zitnik et al also suggest model polypharmaci side effect predict work model drug protein interact network separ deal edg differ type knowledg graph knowledg graph kg repres collect titi relat fact pair entiti ha wide applic question answer inform retriev knowledg guid gener task kg includ learn dimension embed contain rich semant entiti relat predict miss link entiti hop reason knowledg graph one line research treat graph collect tripl propos variou kind loss function distinguish correct tripl fals tripl bord et line leverag graph natur kg use method variou task treat graph kg seen heterogen graph howev unlik heterogen graph social network logic relat tanc pure graph structur schlichtkrul et ﬁrst work incorpor gnn knowledg graph embed deal variou relat propos transform messag pass step convolut network shang et combin gcn encod cnn decod togeth better knowledg represent challeng set knowledg base complet ookb entiti ookb entiti unseen train set directli connect observ entiti train set embed ookb entiti aggreg observ entiti hamaguchi et al use gnn solv lem achiev satisfi perform standard kbc set ookb set besid knowledg graph represent learn wang et al util gcn solv knowledg graph ment problem model emb entiti differ languag uniﬁ embed space align base embed ilar align heterogen knowledg graph oag zhang et use graph attent network model variou type entiti repres entiti surround graph xu et al transfer entiti align problem graph match problem solv graph match network gener model gener model graph drawn signiﬁc attent import applic includ model social teract discov new chemic structur construct knowledg graph deep learn method power abil learn implicit distribut graph surg neural graph zhou et al ai open gener model recent netgan shchur et one ﬁrst work build neural graph gener model gener graph via random walk transform problem graph gener problem walk gener take random walk speciﬁc graph input train walk gener model use gan architectur gener graph preserv import topolog properti inal graph number node unabl chang gener process origin graph graphrnn et manag gener adjac matrix graph ate adjac vector node step step output network differ number node li et al propos model gener edg node sequenti util graph neural network extract hidden state current graph use decid action next step dure sequenti gener process graphaf shi et also formul graph gener sequenti decis process combin gener autogress model toward molecul gener also conduct valid check gener molecul use exist chemic rule step gener instead gener graph sequenti work gener adjac matrix graph onc molgan de cao kipf util discrimin solv node variant problem adjac matrix besid appli reward network optim toward desir chemic properti et al propos constrain variat ensur semant valid gener graph gcpn et incorpor rule reinforc learn gnf liu et adapt normal ﬂow graph data normal ﬂow kind gener model use invert map transform observ data latent vector space transform latent vector back observ data use invers matrix serv gener process gnf combin normal ﬂow graph take graph structur data input gener new graph test time graphit grover et integr gnn variat encod graph structur featur latent variabl speciﬁc use isotrop gaussian latent iabl use iter reﬁnement strategi decod latent variabl combinatori optim combinatori optim problem graph set problem attract much attent scientist ﬁeld speciﬁc problem like travel salesman problem tsp minimum span tree mst got variou heurist solut recent use deep neural network solv problem ha spot solut leverag graph neural network becaus graph structur bello et al ﬁrst propos approach tackl tsp method consist two part pointer network vinyal et parameter reward polici gradient sutton barto modul train thi work ha prove compar tradit approach howev pointer network design sequenti data like text encod appropri work khalil et al kool et al improv abov method includ graph neural network former work ﬁrst obtain node embed dai et feed modul make decis latter one build system replac reinforc learn modul decod efﬁcient train work achiev better perform previou rithm prove represent power graph neural network gener gass et al repres state combinatori problem bipartit graph util gcn encod speciﬁc combinatori optim problem nowak et al focu quadrat assign problem measur ilar two graph gnn base model learn node embed graph independ match use attent mechan thi method offer intriguingli good perform even regim standard techniqu appear suffer zheng et al use gener graph neural network model learn problem also combinatori zation problem neurosat selsam et learn messag pass neural network classifi satisﬁ sat problem prove learn model gener novel tribut sat problem convert sat unlik previou work tri design speciﬁc gnn solv combinatori problem sato et al provid theoret analysi gnn model problem establish connect gnn distribut local algorithm group classic algorithm graph solv problem moreov strate optim approxim ratio optim solut power gnn reach also prove exist gnn model exceed thi upper bound furthermor add color node featur improv approxim ratio trafﬁc network predict trafﬁc state challeng task sinc trafﬁc network dynam complex depend cui et al combin gnn lstm captur spatial tempor depend stgcn yu et construct block spatial tempor convolut layer appli residu connect bottleneck strategi zheng et al guo et al incorpor attent mechan better model spatial tempor correl recommend system interact predict one classic problem recommend model interact graph gnn util thi area van den berg et ﬁrstli appli gcn rate graph learn user item embed efﬁcient adopt gnn scenario pinsag ying et build comput graph weight sampl strategi bipartit graph reduc repeat comput social recommend tri incorpor user social network enhanc recommend perform graphrec fan et learn user embed item side user side wu et al go beyond static social effect attempt model homophili inﬂuenc effect dual attent applic structur scenario becaus ubiqu data gnn appli larger varieti task introduc abov list scenario veri brieﬂi ﬁnancial market gnn use model interact differ stock predict futur trend stock matsunaga et yang et chen et li et kim et al also predict market index movement formul graph classiﬁc problem network sdn gnn use optim ing perform rusek et abstract mean tion amr graph text gener task song et al beck et al use gnn encod graph represent abstract mean scenario thi section talk applic nario gener two way appli gnn zhou et al ai open scenario incorpor structur inform domain improv perform exampl use inform edg graph allevi problem imag task infer assum relat structur task appli model solv problem deﬁn graph method zhang et model text graph common scenario includ imag text program sourc code ni et li et howev onli give detail duction ﬁrst two scenario imag zero imag classiﬁc imag classiﬁc veri basic import task ﬁeld comput vision attract much attent ha mani famou dataset like imagenet kovski et recent learn becom popular ﬁeld imag classiﬁc learn make predict test data sampl class onli n train sampl class provid train set therebi learn restrict n small quir n model must learn gener limit train data make new predict test data graph neural network hand assist imag classiﬁc system challeng scenario first knowledg graph use extra inform guid recognit classiﬁc wang et kampffmey et wang et al make visual classiﬁ learn onli visual input also word embed gori name relationship categori knowledg graph develop help connect relat categori use gcn encod knowledg graph fect happen graph convolut architectur becom deep gcn use wang et wash much use inform represent solv smooth problem kampffmey et al use singl layer gcn larger borhood includ node graph proven effect build classiﬁ exist one knowledg graph larg reason marino et al select relat entiti build base result object detect appli ggnn extract graph predict besid lee et al also leverag knowledg graph categori deﬁn three type relat categori posit correl neg lation propag conﬁdenc relat label graph directli except knowledg graph similar imag dataset also help learn garcia bruna garcia bruna build weight imag network base similar messag pass graph recognit visual reason system usual need perform reason incorpor spatial semant inform natur gener graph reason task typic visual reason task visual question answer vqa thi task model need answer question imag given text descript question usual answer lie spatial relat among object imag teney et al construct imag scene graph question syntact graph appli ggnn train embed predict ﬁnal answer despit spatial connect among object norcliffebrown et al build relat graph condit question knowledg graph wang et al narasimhan et al perform ﬁner relat explor interpret reason process applic visual reason includ object detect interact detect region classiﬁc object detect hu et gu et gnn use calcul roi featur interact detect qi et jain et gnn tool human object region cation chen et gnn perform reason graph connect region class semant segment semant segment crucial step toward imag understand task assign uniqu label categori everi singl pixel imag consid dens classiﬁc problem howev region imag often need inform lead failur tradit cnn sever work util data handl liang et al use model depend togeth spatial connect build graph form superpixel map appli lstm propag borhood inform global subsequ work improv perspect encod hierarch inform liang et furthermor semant segment rgbd semant tion point cloud classiﬁc util geometr inform therefor hard model cnn qi et al construct neighbor knn graph use gnn propag model unrol sever step predict model take hidden state node input predict semant label alway mani point point cloud classiﬁc task landrieu simonovski solv point cloud mentat build superpoint graph gener embed classifi supernod landrieu simonovski leverag ggnn graph convolut wang et al propos model point interact edg calcul edg represent vector feed coordin termin node node embed updat edg aggreg text graph neural network could appli sever task base text could appli task text cation well task sequenc label list sever major applic text follow text classiﬁc text classiﬁc import classic problem natur languag process tradit text classiﬁc use featur howev repres text graph word captur semant long distanc word peng et peng et al use base deep learn model ﬁrst convert text use graph convolut oper niepert et convolv word graph zhang et al propos sentenc lstm encod text view whole sentenc singl state consist individu word overal state use global represent classiﬁc task method either view document sentenc graph word node yao et al regard document word node construct corpu graph use text gcn learn embed word document sentiment classiﬁc could also regard text classiﬁc problem approach propos tai et sequenc label given sequenc observ variabl word sequenc label assign categor label abl typic task includ label word sentenc name entiti recognit ner predict whether word sentenc belong part name entiti consid variabl sequenc node depend edg util hidden state gnn address task zhang et al util sentenc lstm label sequenc conduct experi ner task achiev promis perform semant role label anoth task sequenc label ggiani titov present syntact gcn solv problem syntact gcn oper direct graph label edg special variant gcn kipf well integr gate let model regul contribut individu zhou et al ai open depend edg syntact gcn syntact depend tree use sentenc encod learn latent featur represent word sentenc neural machin translat neural machin translat nmt task translat text sourc languag target languag automat use neural network usual consid task transform vaswani et troduc attent mechan replac commonli use recurr convolut layer fact transform assum fulli connect graph structur word graph structur explor gnn one popular applic gnn incorpor syntact semant inform nmt task bast et al util syntact gcn nmt task marcheggiani et al incorpor inform structur sourc sentenc name represent use syntact gcn compar result incorpor onli syntact onli semant inform inform beck et al util ggnn nmt convert syntact pendenc graph new structur call levi graph levi turn edg addit node thu edg label repres embed relat extract extract semant relat entiti text help expand exist knowledg base tradit method use cnn rnn learn entiti featur predict relat type pair entiti sophist way util pendenc structur sentenc document graph built node repres word edg repres variou depend adjac syntact depend discours relat zhang et al propos extens graph convolut network tailor relat extract appli prune strategi input tree relat extract detect relat among n entiti across multipl sentenc peng et al explor gener framework relat extract appli graph lstm document graph song et al also use lstm model speed comput allow parallel event extract event extract import inform extract task recogn instanc speciﬁ type event text thi alway conduct recogn event trigger predict argument trigger nguyen grishman investig convolut neural network syntact gcn exactli base depend tree perform event detect liu et al propos jointli multipl event extract jmee framework jointli extract multipl event trigger argument introduc syntact shortcut arc enhanc inform ﬂow graph convolut network model graph inform fact veriﬁc fact veriﬁc task requir model extract evid verifi given claim howev claim requir reason multipl piec evid method like gear zhou et kgat liu et propos conduct evid aggreg reason base fulli connect evid graph zhong et al build graph format semant role label achiev promis result applic text gnn also appli mani task text exampl gnn also use question answer read comprehens song et de cao et qiu et tu et ding et anoth import direct relat reason relat network santoro et tion network battaglia et recurr relat network palm et propos solv relat reason task base text open problem although gnn achiev great success differ ﬁeld remark gnn model good enough offer satisfi solut ani graph ani condit thi section list open problem research robust famili model base neural network gnn also vulner adversari attack compar adversari tack imag text onli focus featur attack graph consid structur inform sever work propos attack exist graph model zügner et dai et robust model propos defend zhu et refer sun et comprehens review interpret interpret also import research rection neural model gnn also lack explan onli method ying et baldassarr azizpour propos gener explan gnn model import appli gnn model cation trust explan similar ﬁeld cv nlp interpret graph also import direct investig graph pretrain neural model requir abund label data costli obtain enorm data supervis method propos guid model learn bele data easi obtain websit knowledg base method achiev great success area cv nlp idea pretrain krizhevski et devlin et recent work focus pretrain graph qiu et hu et zhang et differ problem set focu differ aspect thi ﬁeld still ha mani open problem requir research effort design pretrain task effect exist gnn model learn structur featur inform etc complex graph structur graph structur ﬂexibl plex real life applic variou work propos deal complex graph structur dynam graph heterogen graph discuss befor rapid develop social network internet certainli problem challeng applic scenario emerg requir power model conclus past year graph neural network becom power practic tool machin learn task graph domain thi progress owe advanc express power model ﬂexibl train algorithm thi survey conduct comprehens review graph neural network gnn model introduc variant categor comput modul graph type train type moreov also summar sever gener framework introduc sever theoret analys term applic taxonomi divid gnn applic structur scenario scenario scenario give detail review tion scenario final suggest four open problem indic major challeng futur research direct graph neural network includ robust interpret pretrain plex structur model declar compet interest author declar known compet ﬁnancial interest person relationship could appear inﬂuenc work report thi paper acknowledg thi work support nation key research ment program china nation natur scienc foundat china nsfc beij academi artiﬁci intellig baai thi work also support tencent market solut focus research program zhou et al ai open appendix dataset mani task relat graph releas test perform variou graph neural network task base follow commonli use dataset list dataset tabl tabl dataset commonli use task relat graph field dataset citat network pubm yang et cora yang et cites yang et dblp tang et graph mutag debnath et wale et ppi zitnik leskovec dobson doig protein borgwardt et ptc toivonen et social network reddit hamilton et blogcatalog zafarani liu knowledg graph socher et bord et toutanova et socher et bord et dettmer et also broader rang open sourc dataset repositori contain graph dataset list tabl tabl popular graph learn dataset collect repositori introduct link network repositori scientiﬁc network data repositori interact visual mine tool http graph kernel dataset benchmark dataset graph kernel http relat dataset repositori support growth relat machin learn http stanford larg network dataset collect snap librari develop studi larg social inform network http open graph benchmark open graph benchmark ogb collect benchmark dataset evalu graph machin learn pytorch http appendix implement ﬁrst list sever platform provid code graph comput tabl tabl popular platform graph comput platform link refer pytorch geometr http fey lenssen deep graph librari http wang et al aligraph http zhu et al graphvit http zhu et al paddl graph learn http euler http plato http cogdl http openn http next list hyperlink current open sourc implement famou gnn model tabl tabl sourc code model mention survey model link ggnn http neural fp http chebnet http dngr http sdne http gae http drne http structur rnn http dcnn http gcn http cayleynet http graphsag http gat http cln http ecc http mpnn http monet http continu next column zhou et al ai open tabl continu model link http sse http lgcn http fastgcn http diffpool http graphrnn http molgan http netgan http dcrnn http http rgcn http http dgcn http gaan http dgi http graphwavenet http han http research ﬁled grow rapidli recommend reader paper list publish team gnnpaper http gnnpaper recent paper refer allamani brockschmidt khademi learn repres program graph proc iclr atwood towsley neural network proceed nip pp bahdanau cho bengio neural machin translat jointli learn align translat proceed iclr baldassarr azizpour explain techniqu graph convolut network icml workshop learn reason represent kostylev monet erez reutter silva logic express graph neural network proceed iclr bast titov aziz marcheggiani simaan graph convolut encod neural machin translat proceed emnlp pp battaglia pascanu lai rezend et interact network learn object relat physic proceed nip battaglia hamrick bapst zambaldi malinowski tacchetti raposo santoro faulkner et relat induct bias deep learn graph network arxiv preprint beck haffari cohn learn use gate graph neural network proceed acl bello pham le norouzi bengio neural combinatori optim reinforc learn arxiv preprint bojchevski klicpera perozzi kapoor blai ozemberczki lukasik günnemann scale graph neural network approxim pagerank proceed kdd acm pp bord usuni weston yakhnenko translat embed model data proceed nip borgwardt ong onauer vishwanathan smola kriegel protein function predict via graph kernel bioinformat boscaini masci bronstein learn shape correspond anisotrop convolut neural network proceed nip pp bronstein bruna lecun szlam vandergheynst geometr deep learn go beyond euclidean data ieee spm bruna zaremba szlam lecun spectral network local connect network graph proceed iclr buad coll morel algorithm imag denois proceed cvpr ieee pp cai zheng chang activ learn graph embed arxiv preprint cai zheng chang comprehens survey graph embed problem techniqu applic ieee tkde chami perozzi e murphi machin learn graph model comprehens taxonomi arxiv preprint chang ullman torralba tenenbaum composit approach learn physic dynam proceed iclr chen zhu song stochast train graph convolut network varianc reduct proceed icml pp chen xiao fastgcn fast learn graph convolut network via import sampl proceed iclr chen wei huang incorpor corpor relationship via graph convolut neural network stock price predict proceed cikm pp chen li gupta iter visual reason beyond convolut proceed cvpr pp chen villar chen bruna equival graph isomorph test function approxim gnn proceed neurip pp chen li peng xie cao xu zheng survey adversari learn graph arxiv preprint chen lin li li zhou sun measur reliev smooth problem graph neural network topolog view proceed aaai cheng dong lapata long machin read proceed emnlp pp chiang liu si li bengio hsieh efﬁcient algorithm train deep larg graph convolut network proceed kdd pp cho van merrienbo gulcehr bahdanau bougar schwenk bengio learn phrase represent use rnn statist machin translat proceed emnlp cui wang pei zhu survey network embed ieee tkde cui henrickson ke wang trafﬁc graph convolut recurr neural network deep learn framework trafﬁc learn forecast arxiv preprint cui zhou yang liu adapt graph encod attribut graph embed proceed kdd pp dai dai song discrimin embed latent variabl model structur data proceed icml dai kozareva dai smola song learn iter algorithm graph proceed icml dai li tian huang wang zhu song adversari attack graph structur data proceed icml pp de cao kipf molgan implicit gener model small molecular graph icml workshop theoret foundat applic deep gener model de cao aziz titov question answer reason across document graph convolut network proceed naacl debnath lopez de compadr debnath shusterman hansch relationship mutagen aromat heteroaromat nitro compound med chem defferrard bresson vandergheynst convolut neural network graph fast local spectral ﬁltere proceed nip pp dehmami asi yu understand represent power graph neural network learn graph topolog proceed neurip derr tang sign graph convolut network proceed icdm pp dettmer minervini stenetorp riedel convolut knowledg graph embed proceed aaai devlin chang lee toutanova bert deep bidirect transform languag understand proceed naacl dhillon guan kuli weight graph cut without eigenvector multilevel approach ieee tpami ding zhou chen yang tang cognit graph read comprehens scale proceed acl pp tran venkatesh graph transform polici network chemic reaction predict proceed sigkdd zhou et al ai open dobson doig distinguish enzym structur without align mol biol duvenaud maclaurin aguileraiparraguirr gomezbombarelli hirzel aspuruguzik adam convolut network graph learn molecular ﬁngerprint proceed nip pp dwivedi joshi laurent bengio bresson benchmark graph neural network arxiv preprint errica podda bacciu mich fair comparison graph neural network graph classiﬁc proceed iclr fan li zhao tang yin graph neural network social recommend proceed www pp feng zhang ji gao hypergraph neural network proceed aaai vol pp fey lenssen fast graph represent learn pytorch geometr iclr workshop represent learn graph manifold fout byrd shariat protein interfac predict use graph convolut network proceed nip pp frasconi gori sperduti gener framework adapt process data structur ieee tnn fu zhang meng king magnn metapath aggreg graph neural network heterogen graph embed proceed www gallicchio mich graph echo state network proceed ijcnn ieee pp gao ji graph proceed icml pp gao wang ji learnabl graph convolut network proceed kdd acm pp gao yang zhou wu pan hu activ discrimin network represent learn proceed ijcai garcia bruna learn graph neural network proceed iclr garg jegelka jaakkola gener represent limit graph neural network proceed icml gass etelat ferroni charlin lodi exact combinatori optim graph convolut neural network proceed neurip pp gehr auli grangier dauphin convolut encod model neural machin translat proceed acl pp gilmer schoenholz riley vinyal dahl neural messag pass quantum chemistri proceed icml gori monfardini scarselli new model learn graph domain proceed ijcnn ieee pp goyal ferrara graph embed techniqu applic perform survey knowl base sy grover leskovec scalabl featur learn network proceed kdd acm pp grover zweig ermon graphit iter gener model graph proceed icml pp gu hu wang wei dai learn region featur object detect proceed eccv pp guo lin feng song wan attent base graph convolut network trafﬁc ﬂow forecast proceed aaai hamaguchi oiwa shimbo matsumoto knowledg transfer entiti graph neural network approach proceed ijcai pp http hamilton ying leskovec induct represent learn larg graph proceed nip pp hamilton ying leskovec represent learn graph method applic ieee data base engin bulletin hamilton zhang jurafski leskovec loyalti onlin commun proceed icwsm pp hammer mich sperduti strickert recurs network model neural network hammond vandergheynst gribonv wavelet graph via spectral graph theori appl comput harmon anal hassani khasahmadi contrast represent learn graph proceed icml pp zhang ren sun deep residu learn imag recognit proceed cvpr pp zhang ren sun ident map deep residu network proceed eccv springer pp henaff bruna lecun deep convolut network data arxiv preprint hochreit schmidhub long memori neural comput hoshen vain attent predict model proceed nip pp hu gu zhang dai wei relat network object detect proceed cvpr pp hu dong wang sun heterogen graph transform proceed www pp hu dong wang chang sun gener graph neural network proceed kdd pp hu xiong qu yuan e liu tang graph polici network transfer activ learn graph proceed neurip hu fey zitnik dong ren liu catasta leskovec open graph benchmark dataset machin learn graph proceed neurip hu liu gome zitnik liang pand leskovec strategi graph neural network proceed iclr huang liu van der maaten weinberg dens connect convolut network proceed cvpr pp huang zhang rong huang adapt sampl toward fast graph represent learn proceed neurip huang xu duan ren feng wang model complex spatial pattern tempor featur via heterogen graph embed network arxiv preprint jaeger echo state approach analys train recurr neural erratum note vol german nation research center inform technolog gmd technic report jain zamir savares saxena deep learn graph proceed cvpr pp kampffmey chen liang wang zhang xing rethink knowledg graph propag learn proceed cvpr pp kearn mccloskey berndl pand riley molecular graph convolut move beyond ﬁngerprint comput aid mol de keriven e univers invari equivari graph neural network proceed neurip pp khalil dai zhang dilkina song learn combinatori optim algorithm graph proceed nip khamsi kirk introduct metric space fix point theori vol john wiley son kim jeong lee kim kang hat hierarch graph attent network stock movement predict arxiv preprint kipf well variat graph nip bayesian deep learn workshop kipf well classiﬁc graph convolut network proceed iclr kipf fetaya wang well zemel neural relat infer interact system proceed icml pmlr pp klicpera bojchevski günnemann predict propag graph neural network meet person pagerank proceed iclr knyazev taylor amer understand attent gener graph neural network proceed neurip pp kool van hoof well attent learn solv rout problem proceed iclr krizhevski sutskev hinton imagenet classiﬁc deep convolut neural network proceed nip landrieu simonovski point cloud semant segment superpoint graph proceed cvpr lecun bottou bengio haffner learn appli document recognit proceed ieee pp lecun bengio hinton deep learn natur lee rossi kim ahm koh attent model graph survey tkdd lee fang yeh wang learn structur knowledg graph proceed cvpr pp lee lee kang graph pool proceed icml pp levi finit geometr system six public lectu deliv februari univers calcutta univers calcutta levi huang bucci bronstein kutyniok transfer spectral graph convolut neural network arxiv preprint li tarlow brockschmidt zemel gate graph sequenc neural network proceed iclr li wang zhu huang adapt graph convolut neural network proceed aaai li yu shahabi liu diffus convolut recurr neural network trafﬁc forecast proceed iclr li han wu deeper insight graph convolut network learn proceed aaai li vinyal dyer pascanu battaglia learn deep gener model graph arxiv preprint li chen koltun combinatori optim graph convolut network guid tree search proceed neurip li muller thabet ghanem deepgcn gcn go deep cnn proceed iccv pp li gu dullien vinyal kohli graph match network learn similar graph structur object proceed icml pp li bao harimoto chen xu su model stock relat graph network overnight stock movement predict proceed ijcai pp liang shen feng lin yan semant object pars graph lstm proceed eccv pp liang lin shen feng yan xing interpret evolv lstm proceed cvpr pp liu luo huang jointli multipl event extract via graph inform aggreg proceed emnlp zhou et al ai open liu kumar ba kiro swerski graph normal ﬂow proceed neurip pp liu xiong sun liu fact veriﬁc kernel graph attent network proceed acl pp louka graph neural network learn depth vs width proceed iclr chen xiao constrain gener semant valid graph via regular variat autoencod proceed neurip wang aggarw tang graph convolut network eigenpool proceed kdd pp wang aggarw yin tang graph convolut network proceed sdm pp mallat wavelet tour signal process elsevi manessi rozza manzo dynam graph convolut network pattern recogn marcheggiani titov encod sentenc graph convolut network semant role label proceed emnlp pp marcheggiani bast titov exploit semant neural machin translat graph convolut network proceed naacl marino salakhutdinov gupta know use knowledg graph imag classiﬁc proceed cvpr maron shamir lipman invari equivari graph network proceed iclr maron fetaya segol lipman univers invari network proceed icml pmlr pp masci boscaini bronstein vandergheynst geodes convolut neural network riemannian manifold iccv workshop matsunaga suzumura takahashi explor graph neural network stock market predict roll window analysi arxiv preprint arxiv mich neural network graph contextu construct approach ieee tnn mich sona sperduti contextu process structur data recurs cascad correl ieee tnn mikolov chen corrado dean efﬁcient estim word represent vector space proceed iclr miwa bansal relat extract use lstm sequenc tree structur proceed acl monti boscaini masci rodola svoboda bronstein geometr deep learn graph manifold use mixtur model cnn proceed cvpr pp morri ritzert fey hamilton lenssen rattan grohe weisfeil leman go neural graph neural network proceed aaai narasimhan lazebnik schwing box reason graph convolut net factual visual question answer proceed neurip pp nguyen grishman graph convolut network pool event detect proceed aaai niepert ahm kutzkov learn convolut neural network graph proceed icml norcliffebrown vafeia parisot learn condit graph structur interpret visual question answer proceed neurip pp nowak villar bandeira bruna revis note learn quadrat assign graph neural network ieee dsw ieee pp nt maehara revisit graph neural network filter arxiv preprint oono suzuki graph neural network exponenti lose express power node classiﬁc proceed iclr palm paquet winther recurr relat network proceed neurip pan hu long jiang yao zhang adversari regular graph autoencod graph embed proceed ijcai pareja domeniconi chen suzumura kanezashi kaler schardl leiserson evolvegcn evolv graph convolut network dynam graph proceed aaai park lee chang lee choi symmetr graph convolut autoencod unsupervis graph represent learn proceed iccv pp peng poon quirk toutanova yih relat extract graph lstm tacl peng li liu bao wang song yang hierarch text classiﬁc recurs regular deep proceed www pp peng choi xu graph embed combinatori optim survey arxiv preprint perozzi skiena deepwalk onlin learn social represent proceed kdd acm pp pham tran phung venkatesh column network collect classiﬁc proceed aaai pp qi su mo guiba pointnet deep learn point set classiﬁc segment proceed cvpr qi liao jia fidler urtasun graph neural network rgbd semant segment proceed cvpr qi wang jia shen zhu learn interact graph pars neural network proceed eccv pp qiu xiao qu zhou li zhang yu dynam fuse graph network reason proceed acl pp qiu chen dong zhang yang ding wang tang gcc graph contrast code graph neural network proceed kdd rahimi cohn baldwin user geoloc via graph convolut network proceed acl vol pp raposo santoro barrett pascanu lillicrap battaglia discov object relat entangl scene represent proceed iclr rhee seo kim hybrid approach relat network local graph convolut filter breast cancer subtyp classiﬁc proceed ijcai pp riba fischer os es learn graph distanc messag pass neural network proceed icpr ieee pp rossi tiezzi dimitri bianchini maggini scarselli learn graph neural network iapr workshop artiﬁci neural network pattern recognit springer pp ruiz chamon ribeiro graphon neural network transfer graph neural network proceed neurip rusek mestr unveil potenti graph neural network network model optim sdn proceed sosr pp russakovski deng su kraus satheesh huang karpathi khosla bernstein et imagenet larg scale visual recognit challeng proceed ijcv pp sanchez heess springenberg merel hadsel riedmil battaglia graph network learnabl physic engin infer control proceed icml pp santoro raposo barrett malinowski pascanu battaglia lillicrap simpl neural network modul relat reason proceed nip sato yamada kashima approxim ratio graph neural network combinatori problem proceed neurip pp scarselli gori tsoi hagenbuchn monfardini graph neural network model ieee tnn scarselli tsoi hagenbuchn dimens graph recurs neural network neural network schlichtkrul kipf bloem van den berg titov well model relat data graph convolut network proceed eswc springer pp selsam lamm bünz liang de moura dill learn sat solver supervis proceed iclr shang tang huang bi zhou convolut network knowledg base complet proceed aaai shchur mumm bojchevski günnemann pitfal graph neural network evalu arxiv preprint shchur zugner bojchevski gunnemann netgan gener graph via random walk proceed icml pp shi xu zhu zhang zhang tang graphaf autoregress model molecular graph gener proceed iclr shuman narang frossard ortega vandergheynst emerg ﬁeld signal process graph extend data analysi network irregular domain ieee spm simonovski komodaki dynam ﬁlter convolut neural network graph proceed cvpr pp socher chen man ng reason neural tensor network knowledg base complet proceed nip pp song zhang wang gildea model text gener proceed acl pp song zhang wang gildea relat extract use graph state lstm proceed emnlp pp song wang yu zhang florian gildea explor structur passag represent read comprehens graph neural network arxiv preprint sperduti starita supervis neural network classiﬁc structur ieee tnn sukhbaatar fergu et learn multiag commun backpropag proceed nip pp sun han yan yu wu pathsim meta similar search heterogen inform network proceed vldb endow vol pp sun wang yu li adversari attack defens graph data survey arxiv preprint sun hoffmann verma tang infograph unsupervis supervis represent learn via mutual inform maxim proceed iclr sutton barto reinforc learn introduct mit press tai socher man improv semant represent structur long memori network proceed ijcnlp pp tang zhang yao li zhang su arnetmin extract mine academ social network proceed kdd acm pp zhou et al ai open tang qu wang zhang yan mei line inform network embed proceed www pp teney liu den hengel represent visual question answer proceed cvpr pp tiezzi marra melacci maggini deep lagrangian propag graph neural network arxiv preprint toivonen srinivasan king kramer helma statist evalu predict toxicolog challeng bioinformat toutanova chen pantel poon choudhuri gamon repres text joint embed text knowledg base proceed emnlp pp tsitsulin palowitch perozzi müller graph cluster graph neural network arxiv preprint tu wang huang tang zhou read comprehens across multipl document reason heterogen graph proceed acl pp van den berg kipf well graph convolut matrix complet arxiv preprint vaswani shazeer parmar jone uszkoreit gomez kaiser attent need proceed nip pp velickov cucurul casanova romero lio bengio graph attent network proceed iclr velickov fedu hamilton bengio hjelm deep graph infomax proceed iclr verma zhang stabil gener graph convolut neural network proceed kdd pp vinyal bengio kudlur order matter sequenc sequenc set arxiv preprint vinyal fortunato jaitli pointer network proceed nip pp wale watson karypi comparison descriptor space chemic compound retriev classiﬁc knowl inf syst wang leskovec unifi graph convolut neural network label propag arxiv preprint wang pan long zhu jiang mgae margin graph autoencod graph cluster proceed cikm pp wang girshick gupta neural network proceed cvpr pp wang lv lan zhang knowledg graph align via graph convolut network proceed emnlp wang chen ren yu cheng lin deep reason knowledg graph social relationship understand proceed ijcai wang ye gupta recognit via semant embed knowledg graph proceed cvpr wang sun liu sarma bronstein solomon dynam graph cnn learn point cloud acm transact graphic wang ji shi wang ye cui yu heterogen graph attent network proceed www pp wang yu zheng gan gai ye li zhou huang huang guo zhang lin zhao li smola zhang deep graph librari toward efﬁcient scalabl deep learn graph iclr workshop represent learn graph manifold watter zoran weber battaglia pascanu tacchetti visual interact network learn physic simul video proceed nip pp wu lian xu wu chen graph convolut network markov random ﬁeld reason social spammer detect proceed aaai pp wu pan chen long zhang yu comprehens survey graph neural network arxiv preprint wu souza zhang fifti yu weinberg simplifi graph convolut network volum proceed machin learn research pmlr pp wu zhang gao weng gao chen dual graph attent network deep latent represent multifacet social effect recommend system proceed www pp xu li tian sonob kawarabayashi jegelka represent learn graph jump knowledg network proceed icml pp xu shen cao qiu cheng graph wavelet neural network proceed iclr xu hu leskovec jegelka power graph neural network proceed iclr xu wang yu feng song wang yu knowledg graph align via graph match neural network proceed acl associ comput linguist pp xu wang chen tao zhao gnn dual graph neural network predict structur entiti interact proceed ijcai pp yan xiong lin spatial tempor graph convolut network action recognit proceed aaai yang liu zhao sun chang network represent learn rich text inform proceed ijcai pp yang cohen salakhudinov revisit learn graph embed proceed icml pmlr pp yang wei chen wu use extern knowledg ﬁnancial event predict base graph neural network proceed cikm pp yang xiao zhang sun han heterogen network represent learn survey benchmark evalu beyond arxiv preprint yao mao luo graph convolut network text classiﬁc proceed aaai ying chen eksombatchai hamilton leskovec graph convolut neural network recommend system proceed kdd updat ying morri ren hamilton leskovec hierarch graph represent learn differenti pool proceed neurip ying bourgeoi zitnik leskovec gnnexplain gener explan graph neural network proceed neurip liu ying pand leskovec graph convolut polici network molecular graph gener proceed neurip ying ren hamilton leskovec graphrnn gener realist graph deep model proceed icml ying leskovec design space graph neural network proceed neurip yu yin zhu graph convolut network deep learn framework trafﬁc forecast proceed ijcai pp yun jeong kim kang kim graph transform network proceed neurip pp zafarani liu social comput data repositori asu http zaheer kottur ravanbakhsh poczo salakhutdinov smola deep set proceed nip pp zang wang neural dynam complex network proceed kdd pp zayat ostendorf convers model reddit use structur lstm tacl zeng zhou srivastava kannan prasanna graphsaint graph sampl base induct learn method proceed iclr zhang yin zhu zhang network represent learn survey ieee tbd zhang cui zhu deep learn graph survey ieee tkde zhang shi xie king yeung gaan gate attent network learn larg spatiotempor graph proceed uai zhang liu song lstm text represent proceed acl pp zhang cui neumann chen deep learn architectur graph classiﬁc proceed aaai zhang qi man graph convolut prune depend tree improv relat extract proceed emnlp pp zhang tong xu maciejewski graph convolut network comprehens review comput social network zhang song huang swami chawla heterogen graph neural network proceed kdd pp zhang liu li wu attribut graph cluster via adapt graph convolut proceed ijcai pp zhang liu tang dong yao zhang gu wang shao li et oag toward link heterogen entiti graph proceed kdd zhang zhang sun xia onli attent need learn graph represent arxiv preprint zheng dan aragam ravikumar xing learn spars nonparametr dag proceed aistat pmlr pp zheng fan wang qi gman graph network trafﬁc predict proceed aaai zhong xu tang xu duan zhou wang yin reason graph fact check proceed acl zhou han yang liu wang li sun gear evid aggreg reason fact veriﬁc proceed acl pp zhu zhang cui zhu robust graph convolut network adversari attack proceed kdd zhu zhao yang lin zhou ai li zhou aligraph proceed vldb endow zhu xu qu tang graphvit hybrid system node embed proced www acm pp zhuang dual graph convolut network supervis classiﬁc proceed www pp zhou et al ai open zilli srivastava koutnik schmidhub recurr highway network proceed icml pp zitnik leskovec predict multicellular function tissu network bioinformat zitnik agraw leskovec model polypharmaci side effect graph convolut network bioinformat zou hu wang jiang sun gu import sampl train deep larg graph convolut network proceed neurip pp zügner akbarnejad günnemann adversari attack neural network graph data proceed kdd pp rossi frasca chamberlain eynard bronstein monti sign scalabl incept graph neural network arxiv preprint zhou et al ai open