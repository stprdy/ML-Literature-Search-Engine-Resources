e n equivari graph neural network victor garcia satorra emiel hoogeboom max well abstract thi paper introduc new model learn graph neural network equivari rotat tion reﬂect permut call e n equivari graph neural network egnn contrast exist method work doe requir comput expens represent intermedi layer still achiev competit better perform addit wherea exist method ite equivari dimension space model easili scale space demonstr effect method dynam system model sentat learn graph autoencod dict molecular properti introduct although deep learn ha larg replac featur mani advanc critic depend tive bias deep neural network effect method restrict neural network relev function exploit symmetri problem enforc equivari respect transform certain symmetri group notabl exampl translat equivari lution neural network permut equivari graph neural network bruna et defferrard et kipf well mani problem exhibit translat rotat tri exampl point cloud uy et molecular structur ramakrishnan et particl simul kipf et group spond symmetri name euclidean group se reﬂect includ e often sire predict task either equivari invari respect e transform delta lab univers amsterdam netherland correspond victor garcia ra emiel hoogeboom max well proceed th intern confer machin learn pmlr copyright author figur exampl rotat equivari graph graph neural network φ recent variou form method achiev e se equivari propos thoma et fuch et finzi et ohler et mani work achiev innov ing type represent intermedi network layer howev transform represent requir coefﬁcient imat expens comput addit practic mani type data input output restrict scalar valu instanc temperatur energi refer literatur vector instanc veloc momentum refer literatur thi work present new architectur translat rotat reﬂect equivari e n permut equivari respect input set point model simpler previou method doe requir spheric harmon thoma et fuch et still achiev competit ter result addit equivari model limit space scale larger dimension space without signiﬁc increas comput e n equivari graph neural network evalu method model dynam system represent learn graph autoencod ing molecular properti dataset method report best veri competit perform three experi background thi section introduc relev materi arianc graph neural network later ment deﬁnit method equivari let tg x x set transform x abstract group g say function φ x equivari g exist equival transform output space sg φ tg x sg φ x practic exampl let φ function x xm input set point cloud embed space φ x transform set point cloud tg translat input set tg x x g sg equival translat output set sg transform φ x translat equivari translat input set tg x appli function φ tx x deliv result ﬁrst run function φ x appli equival translat output tg equat fulﬁll φ φ x thi work explor follow three type equivari set particl x translat equivari translat input g rn result equival translat output let shorthand xm g φ x g rotat reﬂect equivari ani thogon matrix q let qx shorthand qxm rotat input result equival rotat output qy φ qx permut equivari permut input sult permut output p φ p x p permut row index note veloc v unaffect tion transform equival rotat permut method introduc section satisfi three abov mention equivari constraint graph neural network graph neural network permut equivari work oper graph structur data bruna et defferrard et kipf well given graph g v e node vi v edg eij e deﬁn graph convolut layer follow notat gilmer et mij φe hl hl j aij mi x mij φh hl mi hl rnf embed node vi layer aij edg attribut n repres set neighbor node vi final φe φh edg node oper respect commonli approxim multilay perceptron mlp equivari graph neural network thi section present equivari graph neural work egnn follow notat background section consid graph g v e node vi v edg eij addit featur node embed hi rnf also consid coordin xi rn associ graph node model preserv equivari rotat lation set coordin xi also preserv equivari permut set node v fashion gnn equivari graph convolut layer egcl take input set node embed hl hl hl coordin embed xl xl xl edg inform e eij output transform concis egcl hl xl e equat deﬁn thi layer follow mij φe hl hl j xl j aij xl c x xl j φx mij mi x mij φh hl mi notic main differ abov propos method origin graph neural network tion found equat equat input rel squar distanc two coordin kxl edg oper φe embed hl hl j edg attribut aij also provid input edg oper gnn case case edg attribut incorpor edg valu aij eij could also includ addit edg inform e n equivari graph neural network equat updat posit particl xi vector ﬁeld radial direct word posit particl xi updat weight sum rel differ weight thi sum provid output function φx rnf take input edg embed mij previou edg oper output scalar valu c chosen divid sum number element thi equat main differ model compar standard gnn reason whi equivari preserv proof appendix despit simplic thi equivari oper veri ﬂexibl sinc embed mij carri inform whole graph onli given edg eij final equat follow updat standard gnn equat aggreg incom messag neighbor node n node vi tion perform node oper φv take input aggreg messag mi node emed hl output updat node embed analysi e n equivari thi section analyz equivari properti model e symmetri properti state section word model translat equivari x ani translat vector g rn also rotat reﬂect equivari x ani orthogon matrix q formal model satisﬁ g egcl qxl g hl provid formal proof thi appendix intuit let consid hl featur alreadi e n invari see result edg embed mij equat also e n invari becaus addit hl onli depend squar distanc kxl e n invari next equat comput weight sum differ xi ad xi thi transform vector preserv arianc see appendix final last two equat gener next layer main e n invari sinc onli depend hl mij saw abov e n invari therefor output e n invari e n equivari xl induct composit egcl also equivari extend egnn vector type represent thi section propos slight modiﬁc sent method explicitli keep track particl momentum scenario thi use onli obtain estim particl veloc everi layer also provid initi veloc valu case includ momentum propos method replac equat model follow equat φv hl vl c x xl j φx mij xl note thi extend egcl layer egcl hl xl vl e onli differ broke coordin updat eq two step ﬁrst comput veloc use thi veloc updat posit xl veloc vl scale new function φv rn map node embed hl scalar valu notic initi veloc set zero v equat becom exactli ﬁrst layer l becom equival next layer sinc φv output φx previou layer scalar valu proof equivari thi variant model appendix thi variant use experi provid initi veloc system predict rel posit chang infer edg given point cloud set node may alway provid adjac matrix case assum fulli connect graph node exchang messag word neighborhood oper n equat would includ node graph except thi fulli connect proach may scale well larg graph may want local limit exchang messag avoid overﬂow inform similarli servianski et kipf et present simpl solut infer graph model aggreg oper model eq follow way mi x mij x eijmij eij take valu edg node j otherwis notic thi modifi yet origin equat use model chang notat choos approxim relat eij follow function eij φinf mij φinf rnf resembl linear layer follow sigmoid function take input current edg embed output soft estim edg valu thi modiﬁc chang e n properti model sinc onli oper messag mij alreadi e n invari e n equivari graph neural network gnn radial field tfn schnet egnn edg mij φe hl hl j aij mij φrf krl ijk rl ij mij p k wlkrl jihlk mij φcf krl ijk φs hl j mij φe hl hl j krl aij ˆ mij rl ijφx mij agg mi p mij mi p mij mi p mij mi p mij mi p mij ˆ mi c p ˆ mij node φh hl mi xl mi wllhl mi φh hl mi φh hl mi xl ˆ mi e n se e n e n tabl comparison differ work literatur messag pass framework notat creat thi tabl aim provid clear simpl way compar differ method name left right graph neural network gilmer et radial field equivari flow ohler et tensor field network thoma et schnet utt et equivari graph neural network differ two point written rij xi relat work group equivari neural network demonstr effect wide varieti task cohen well weiler cesa rezend et romero cordonni recent variou form method achiev e se equivari propos thoma et al fuch et al util spheric harmon comput basi format allow transform order represent downsid thi method spheric harmon need recomput expens current extens thi method trari dimens unknown finzi et al parametr transform map kernel lie algebra thi method neural network output certain ation stochast may undesir hori et al propos set isometr invari equivair transform graph neural network ohler et al ohler et al propos e n equivari network model point cloud method onli deﬁn posit data node without ani featur dimens anoth relat line research concern messag pass algorithm molecular data gilmer et sent messag pass set graph neural network quantum chemistri thi method permut ant translat rotat equivari kondor et extend equivari gnn neuron transform speciﬁc way permut thi extens onli affect permut group translat rotat geometr space work utt et build e n invari sage pass network input rel distanc point klicpera et al addit rel distanc includ modiﬁ messag pass scheme analog belief propag consid gle direct inform equivari rotat also use bessel function spheric harmon struct orthogon basi anderson et al miller et al includ equivari intermedi layer model behavior properti lar data method also frame messag pass framework contrast method achiev e n equivari relationship exist method tabl egnn equat detail togeth closest method literatur messag pass notat gilmer et thi ble aim provid simpl way compar differ algorithm structur three main row describ edg ii aggreg iii node updat oper gnn algorithm previous introduc section egnn algorithm also equival descript section notat ha modiﬁ match edg aggreg node format equat rl ij xi notic except egnn rithm aggreg oper main differ aris edg oper algorithm call radial field e n equivari updat ohler et thi method e n equivari howev main limit onli oper x propag node featur h among node method φrf model mlp tensor field network tfn thoma et instead propag node embed h use spheric harmon comput learnabl weight kernel w k r preserv se equivari expens comput limit dimension space se transform fuch et includ thi tabl interpret extens tfn attent schnet utt et interpret e n invari graph neural network φcf receiv input rel distanc output continu ﬁlter tion multipli neighbor embed egnn differ method term perform two differ updat tabl row one relat embed h anoth relat coordin e n equivari graph neural network x two variabl exchang inform edg oper summari egnn retain ﬂexibl gnn remain e n equivari radial field algorithm without need comput expens oper spheric harmon experi model dynam system system dynam system function deﬁn time denc point set point geometr space ell complex dynam crucial varieti plicat control system chua et model base dynam reinforc learn nagabandi et physic system simul grzeszczuk et watter et thi experi forecast posit set particl model simpl interact rule yet exhibit complex dynam similarli fuch et extend charg particl experi kipf et dimension space system consist particl carri posit neg charg posit veloc associ space system control physic rule particl attract repel depend charg thi equivari task sinc rotat translat input set particl result transform throughout entir trajectori dataset sampl trajectori train valid test trajectori ha durat timestep trajectori provid initi particl posit p p p initi veloc v v v respect charg c task estim tion ﬁve particl timestep optim averag mean squar error estim posit ground truth one implement detail thi experi use extens model includ veloc section input posit p ﬁrst layer coordin model veloc v initi veloc equat norm kv k also provid featur linear map charg input edg attribut aij cicj model output last layer coordin xl estim posit compar method non equivari graph neural network gnn cousin equivari method radial field ohler et tensor field network se transform algorithm compos layer train condit batch size epoch adam optim learn rate wa tune independ model use featur hidden layer radial field gnn egnn use swish activ function ramachandran et tfn se transform swept differ number vector type featur chose provid best perform implement detail provid appendix linear model simpli consid motion equat p p v also includ baselin also provid averag forward pass time second model batch sampl gtx ti gpu method mse forward time linear se transform tensor field network graph neural network radial field egnn tabl mean squar error futur posit estim system experi forward time second batch size sampl run gtx gpu result shown tabl model signiﬁcantli form equivari altern still efﬁcient term run time reduc error respect second best perform method addit requir comput spheric harmon make time efﬁcient tensor field network se transform figur mean squar error experi dial field gnn egnn method sweep differ amount train data analysi differ number train sampl want analyz perform egnn small larg data regim follow report similar experi abov instead use train sampl gener new train partit e n equivari graph neural network sampl sweep differ amount data sampl compar perform egnn vs gnn counterpart radial field algorithm result present figur method outperform radial field gnn small larg data regim thi show egnn data efﬁcient gnn sinc requir gener rotat translat data ensembl ﬂexibl gnn larger data regim due high model bia radial field algorithm perform well data scarc unabl learn subtleti dataset increas train size summari egnn beneﬁt high bia e n method ﬂexibl gnn graph autoencod graph autoencod learn unsupervis tion graph continu latent space kipf well simonovski komodaki thi iment section use egnn build equivari graph autoencod explain graph coder beneﬁt equivari show method outperform standard gnn autoencod provid dataset thi problem particularli interest sinc embed space scale larger sion limit dimension euclidean space similarli work kipf well extend section liu et graph encod z q g emb graph g set latent node z zm ber node n embed size per node notic thi may reduc memori complex store graph mn n may depend certain approxim error toler thi differ variat autoencod propos simonovski modaki emb graph singl vector z rk caus reconstruct alli veri expens sinc node decod graph match ground truth addit introduc graph gener represent learn method worth mention context graph compress method cand es recht use speciﬁc compar equivari graph task present liu et graph g v e node featur h adjac matrix embed latent space z q h follow kipf well liu et onli interest reconstruct adjac matrix sinc dataset work contain node featur decod g propos liu et take input ding space z output reconstruct adjac matrix ˆ g z thi decod function deﬁn follow ˆ aij ge zi zj exp w kzi b w b onli learnabl paramet ge decod edg function appli everi pair node bed reﬂect edg probabl depend rel distanc among node embed train loss deﬁn binari cross entropi mate ground truth edg l p ij bce ˆ aij aij symmetri problem abov state autoencod may seem straightforward implement ﬁrst sight case strong limit regard tri graph graph neural network convolut edg node graph function appli edg node graph deﬁn onli adjac matrix may input featur node reason differ among node reli onli edg neighborhood topolog therefor neighborhood two node exactli encod embed clear exampl thi cycl graph exampl node cycl graph provid figur run graph neural network encod node featureless cycl graph obtain exact embed node make possibl reconstruct edg origin graph node embed cycl graph sever exampl node exact neighborhood topolog symmetri present differ way graph differ edg distribut even includ node featur uniqu figur visual represent graph autoencod node cycl graph bottom row illustr ad nois input graph break symmetri embed allow reconstruct adjac matrix method break symmetri graph introduc liu et thi method introduc nois sampl gaussian distribut input node featur graph σi thi nois low differ represent node embed result graph decod come e n equivari graph neural network commun small erdo renyi encod bce error bce error baselin gnn radial field egnn figur tabl left report binari cross entropi error score test partit graph autoencod experi commun small erdo renyi dataset figur right report score overﬁt train partit sampl erdo renyi dataset differ valu sparsiti pe gnn abl success spars graph small pe valu erdo renyi dataset even train test small subset drawback network ha gener new introduc nois distribut equivari graph toencod remain translat rotat equivari thi sampl nois ﬁnd make gener much easier anoth way look thi ing sampl nois make node represent go structur posit srinivasan ribeiro e n equivari may beneﬁci case simpli input thi nois input coordin σi egnn output equivari transform xl thi output use embed graph z xl input decod equat dataset gener graph et liu et run origin code et graph contain node also gener second dataset use renyi gener model ela pling random graph initi number node edg probabl pe sampl graph train valid test dataset graph deﬁn adjac matrix implement detail equivari graph encod compos egnn encod follow decod equat graph edg aij input edg attribut aij equat nois use break symmetri input coordin σi ﬁrst layer ize one sinc work featureless graph mention befor encod output equivari transform coordin graph bed input decod z xl use n dimens embed space pare egnn gnn cousin also compar adapt gnn match set liu et also includ radial field algorithm addit baselin four model layer featur hidden layer swish activ function train epoch use adam optim learn rate detail provid appendix sinc number node larger number layer recept ﬁeld gnn may compris whole graph make comparison unfair egnn avoid thi limit model exchang sage among node edg inform provid edg attribut aij aij result tabl figur report binari cross entropi loss estim ground truth edg error deﬁn percentag wrong predict edg respect total amount potenti edg score edg classiﬁc number refer test partit also includ baselin predict edg miss ˆ aij standard gnn seem suffer symmetri problem provid worst perform introduc nois loss error decreas show actual use add nois input node final egnn remain e n equivari thi nois distribut provid best reconstruct error erdo renyi dataset close optim commun small dataset analysi reconstruct error differ n embed size report appendix overﬁt train set explain tri problem show egnn outperform method given dataset although observ ad nois gnn improv result difﬁcult exactli measur impact symmetri limit result independ factor aliz train test set thi section conduct experi train differ model subset erdo renyi graph embed size n aim overﬁt data evalu method train data thi experi gnn e n equivari graph neural network task homo lumo µ g h u zpve unit mev mev mev k mev mev mev mev mev nmp schnet cormor lieconv tfn se egnn tabl mean absolut error molecular properti predict benchmark dataset use slightli differ partit paper list unabl ﬁt train data properli egnn achiev perfect reconstruct close perfect sweep differ pe sparsiti valu sinc symmetri limit present veri spars veri dens graph report score thi experi right plot figur thi experi show e n equivari prove perform embed graph continu space set node dimens even though thi simpl reconstruct task think thi use step toward gener graph molecul often graph edg decod pairwis distanc similar node kipf well liu et grover et metric eq e n invari addit thi experi also show method success perform e n equivari task higher dimension space n molecular data dataset ramakrishnan et ha becom standard machin learn chemic properti diction task dataset consist small molecul repres set atom atom per molecul atom posit associ ﬁve mension node embed describ atom type h c n f dataset label varieti chemic properti molecul mate regress properti invari translat rotat reﬂect atom posit therefor model e invari highli suitabl thi task import dataset partit anderson et molecul train valid test varieti chemic properti estim per molecul optim report mean absolut error predict ground truth implement detail egnn receiv input coordin locat atom provid equat embed atom properti provid input node featur sinc thi invari task also posit static need updat particl posit x run equat previou experi consequ tri manner notic ani improv updat updat particl posit skip equat model becom e n variant analog standard gnn rel squar norm pair point kxi input edg oper eq addit sinc provid adjac matrix molecul scale node use extens model section infer soft estim edg egnn network consist layer featur per hidden layer swish activ function oper preced follow two layer mlp map node ding hl output egnn estim properti valu implement detail report appendix compar nmp gilmer et schnet utt et cormor anderson et miller et lieconv finzi et klicpera et tfn thoma et se fuch et result present tabl method report veri competit result properti predict task remain rel simpl introduc higher order represent angl spheric harmon perhap surprisingli outperform equivari network consid higher order represent thi task onli use represent rel distanc deﬁn geometri molecul appendix e prove onli posit inform given veloc higher order type featur geometri e n equivari graph neural network complet deﬁn norm point e n word two collect point separ e n transform consid ident rel norm point uniqu identiﬁ collect conclus equivari graph neural network receiv increas interest natur medic scienc sent new tool analyz molecul properti thi work present new e n equivari deep chitectur graph comput efﬁcient easi implement signiﬁcantli improv current wide rang task believ properti make ideal suit make direct impact topic drug discoveri protein fold design new materi well applic comput vision acknowledg would like thank patrick e hi support formal invari featur identiﬁc proof refer anderson hy kondor cormor covari molecular neural network arxiv preprint ela random graph number cambridg univers press bruna zaremba szlam lecun tral network local connect network graph arxiv preprint cand es recht exact matrix complet via vex optim foundat comput matic chua calandra mcallist levin deep reinforc learn hand trial ing probabilist dynam model arxiv preprint cohen well group equivari convolut network balcan weinberg q ed proceed intern confer machin learn icml cohen well steerabl cnn nation confer learn represent iclr defferrard bresson vandergheynst volut neural network graph fast local spectral ﬁltere advanc neural inform cess system pp finzi stanton izmailov wilson eral convolut neural network equivari lie group arbitrari continu data arxiv preprint fuch worral fischer well se equivari attent network advanc neural inform process system gilmer schoenholz riley vinyal dahl neural messag pass quantum istri arxiv preprint grover zweig ermon graphit iter gener model graph intern enc machin learn pp pmlr grzeszczuk terzopoulo hinton imat fast neural network emul control model proceed nual confer comput graphic interact techniqu pp hori morita hishinuma ihara sume isometr transform invari ariant graph convolut network arxiv preprint kipf fetaya wang well zemel neural relat infer interact system arxiv preprint kipf well tion graph convolut network arxiv preprint kipf well variat graph arxiv preprint klicpera giri margraf unnemann fast direct messag pass molecul arxiv preprint klicpera groß unnemann direct messag pass molecular graph arxiv preprint ohler klein e equivari ﬂow pling conﬁgur system metric energi corr e n equivari graph neural network ohler klein e equivari ﬂow exact likelihood gener learn symmetr densiti arxiv preprint kondor son pan anderson trivedi covari composit network learn graph arxiv preprint liu kumar ba kiro swerski graph normal ﬂow advanc neural inform process system pp miller geiger smidt e relev rotat equivari convolut predict molecular properti arxiv preprint nagabandi kahn fear levin neural network dynam deep ment learn ieee intern confer robot autom icra pp ieee ramachandran zoph le search activ function arxiv preprint ramakrishnan dral rupp von lilienfeld quantum chemistri structur properti kilo molecul scientiﬁc data rezend racani ere higgin toth ariant hamiltonian ﬂow corr romero cordonni group ariant vision ternat confer learn represent url http utt arbabzadah chmiela uller tkatchenko insight deep tensor neural network natur commun utt kinderman sauceda chmiela tkatchenko uller schnet ﬁlter convolut neural network model quantum interact arxiv preprint servianski segol shlomi cranmer gross maron lipman learn graph set advanc neural inform process system simonovski komodaki graphva toward gener small graph use variat autoencod intern confer artiﬁci neural work pp springer srinivasan ribeiro equival posit node embed structur graph sentat arxiv preprint thoma smidt kearn yang li kohlhoff riley tensor ﬁeld network neural network point cloud arxiv preprint uy pham hua nguyen yeung revisit point cloud classiﬁc new mark dataset classiﬁc model data proceed ieee intern confer comput vision pp watter zoran weber battaglia pascanu tacchetti visual interact network ing physic simul video advanc neural inform process system weiler cesa gener e steerabl cnn wallach larochel beygelzim fox garnett r ed advanc neural inform process system annual confer neural inform process system ying ren hamilton leskovec graphrnn gener realist graph deep regress model arxiv preprint