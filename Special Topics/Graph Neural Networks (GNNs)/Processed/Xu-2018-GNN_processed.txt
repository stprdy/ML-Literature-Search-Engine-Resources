published conference paper iclr 2019 powerful graph neural network keyulu xu mit keyulu weihua hu stanford university weihuahu jure leskovec stanford university jure stefanie jegelka mit stefje abstract graph neural network gnns effective framework representation learning graph gnns follow neighborhood aggregation scheme representation vector node computed recursively aggregating forming representation vector neighboring node many gnn variant proposed achieved result node graph classiﬁcation task however despite gnns revolutionizing graph representation learning limited understanding representational property limitation present theoretical framework analyzing expressive power gnns capture different graph structure result characterize discriminative power popular gnn variant graph convolutional network graphsage show not learn distinguish certain simple graph structure develop simple architecture provably expressive among class gnns powerful lehman graph isomorphism test empirically validate theoretical ﬁndings number graph classiﬁcation benchmark demonstrate model achieves performance 1 introduction learning graph structured data molecule social biological ﬁnancial network requires effective representation graph structure hamilton et recently ha surge interest graph neural network gnn approach representation learning graph li et 2016 hamilton et kipf welling 2017 velickovic et 2018 xu et 2018 gnns broadly follow recursive neighborhood aggregation message passing scheme node aggregate feature vector neighbor compute new feature vector xu et 2018 gilmer et 2017 k iteration aggregation node represented transformed feature vector capture structural information within node neighborhood representation entire graph obtained pooling ying et 2018 example summing representation vector node graph many gnn variant different neighborhood aggregation pooling scheme proposed scarselli et battaglia et 2016 defferrard et 2016 duvenaud et 2015 hamilton et kearnes et 2016 kipf welling 2017 li et 2016 velickovic et 2018 santoro et 2017 xu et 2018 santoro et 2018 verma zhang 2018 ying et 2018 zhang et 2018 empirically gnns achieved performance many task node classiﬁcation link prediction graph classiﬁcation however design new gnns mostly based empirical intuition heuristic experimental error little theoretical understanding property limitation gnns formal analysis gnns representational capacity limited contribution partially performed tokyo visiting kawarabayashi partially performed riken aip university tokyo 1 22 feb 2019 published conference paper iclr 2019 present theoretical framework analyzing representational power gnns formally characterize expressive different gnn variant learning represent guish different graph structure framework inspired close connection gnns wl graph isomorphism test weisfeiler lehman 1968 powerful test known distinguish broad class graph babai kucera 1979 similar gnns wl test iteratively update given node feature vector aggregating feature vector network neighbor make wl test powerful injective aggregation update map different node neighborhood different feature vector key insight gnn large discriminative power wl test gnn aggregation scheme highly expressive model injective function mathematically formalize insight framework ﬁrst represents set feature vector given node neighbor multiset set possibly repeating element neighbor aggregation gnns thought aggregation function multiset hence strong representational power gnn must able aggregate different multisets different representation rigorously study several variant multiset function theoretically characterize discriminative power well different aggregation function distinguish different multisets discriminative multiset function powerful representational power underlying gnn main result summarized follows 1 show gnns powerful wl test distinguishing graph structure 2 establish condition neighbor aggregation graph readout function resulting gnn powerful wl test 3 identify graph structure not distinguished popular gnn variant gcn kipf welling 2017 graphsage hamilton et precisely characterize kind graph structure model capture 4 develop simple neural architecture graph isomorphism network gin show power equal power wl test validate theory via experiment graph classiﬁcation datasets expressive power gnns crucial capture graph structure particular compare performance gnns various aggregation function result conﬁrm powerful gnn theory graph isomorphism network gin also empirically ha high representational power almost perfectly ﬁts training data whereas le powerful gnn variant often severely underﬁt training data addition representationally powerful gnns outperform others test set accuracy achieve performance many graph classiﬁcation benchmark 2 preliminary begin summarizing some common gnn model along way introduce notation let g v e denote graph node feature vector xv v two task interest 1 node classiﬁcation node v ha associated label yv goal learn representation vector hv v v label predicted yv f hv 2 graph classiﬁcation given set graph gn label yn aim learn representation vector hg help predict label entire graph yg g hg graph neural network gnns use graph structure node feature xv learn tion vector node hv entire graph hg modern gnns follow neighborhood aggregation strategy iteratively update representation node aggregating representation neighbor k iteration aggregation node representation capture structural information within network neighborhood formally layer gnn k v aggregate k h u u v h k v combine k h v k v h k v feature vector node v initialize h 0 v xv n v set node adjacent choice aggregate k combine k 2 published conference paper iclr 2019 graph rooted subtree 2 wl test iteration multiset gnn aggregation capture structure figure 1 overview theoretical framework middle panel rooted subtree structure blue node wl test us distinguish different graph right panel gnn aggregation function capture full multiset node neighbor gnn capture rooted subtrees recursive manner powerful wl test gnns crucial number architecture aggregate proposed pooling variant graphsage hamilton et aggregate ha formulated k v max relu w h u v w learnable matrix max represents combine step could concatenation followed linear mapping w h h v k v graphsage graph convolutional network gcn kipf welling 2017 mean pooling used instead aggregate combine step integrated follows h k v relu w mean n h u v v many gnns represented similarly eq xu et 2018 gilmer et 2017 node classiﬁcation node representation h k v ﬁnal iteration used prediction graph classiﬁcation readout function aggregate node feature ﬁnal iteration obtain entire graph representation hg hg readout h k v v readout simple permutation invariant function summation sophisticated pooling function ying et 2018 zhang et 2018 test graph isomorphism problem asks whether two graph topologically identical challenging problem no algorithm known yet garey 1979 garey johnson 2002 babai 2016 apart some corner case cai et 1992 wl test graph isomorphism weisfeiler lehman 1968 effective computationally efﬁcient test distinguishes broad class graph babai kucera 1979 form naïve vertex reﬁnement analogous neighbor aggregation gnns wl test iteratively 1 aggregate label node neighborhood 2 hash aggregated label unique new label algorithm decides two graph some iteration label node two graph differ based wl test shervashidze et al 2011 proposed wl subtree kernel measure similarity graph kernel us count node label different iteration wl test feature vector graph intuitively node label iteration wl test represents subtree structure height k rooted node figure 1 thus graph feature considered wl subtree kernel essentially count different rooted subtrees graph 3 theoretical framework overview start overview framework analyzing expressive power gnns figure 1 illustrates idea gnn recursively update node feature vector capture network structure feature node around rooted subtree structure figure 1 throughout paper assume node input feature countable universe ﬁnite graph node feature vector deeper layer any ﬁxed model also countable universe notational simplicity assign feature vector unique label b c feature vector set neighboring node form multiset figure 1 element appear multiple time since different node identical feature vector 3 published conference paper iclr 2019 deﬁnition 1 multiset multiset generalized concept set allows multiple instance element formally multiset x underlying set x formed distinct element give multiplicity element study representational power gnn analyze gnn map two node location embedding space intuitively maximally powerful gnn map two node location only identical subtree structure identical feature corresponding node since subtree structure deﬁned recursively via node neighborhood figure 1 reduce analysis question whether gnn map two neighborhood two multisets embedding representation maximally powerful gnn would never map two different neighborhood multisets feature vector representation mean aggregation scheme must injective thus abstract gnn aggregation scheme class function multisets neural network represent analyze whether able represent injective multiset function next use reasoning develop maximally powerful gnn section 5 study popular gnn variant see aggregation scheme inherently not injective thus le powerful capture interesting property graph 4 building powerful graph neural network first characterize maximum representational capacity general class model ideally maximally powerful gnn could distinguish different graph structure mapping different representation embedding space ability map any two different graph different embeddings however implies solving challenging graph isomorphism problem want isomorphic graph mapped representation one different representation analysis characterize representational capacity gnns via slightly weaker criterion powerful heuristic called wl graph isomorphism test known work well general exception regular graph cai et 1992 douglas 2011 evdokimov ponomarenko 1999 lemma let any two graph graph neural network g map different embeddings graph isomorphism test also decides not isomorphic proof lemma theorem found appendix hence any gnn powerful wl test distinguishing different graph natural question whether exist gnns principle powerful wl test answer theorem 3 yes neighbor aggregation readout function injective resulting gnn powerful wl test theorem let g gnn sufﬁcient number gnn layer map any graph test isomorphism decides different embeddings following condition hold aggregate update node feature iteratively h k v φ h v f h u u v function f operates multisets φ injective b readout operates multiset node feature n h k v injective prove theorem 3 appendix countable set injectiveness well characterizes whether function preserve distinctness input uncountable set node feature continuous need some consideration addition would interesting characterize close together learned feature lie function image leave question future work focus case input node feature countable set subset uncountable set rn lemma assume input feature space x countable let g k function parameterized gnn layer k 1 l g 1 deﬁned multisets x bounded size range g k space node hidden feature h k v also countable k 1 4 published conference paper iclr 2019 also worth discussing important beneﬁt gnns beyond distinguishing different graph capturing similarity graph structure note node feature vector wl test essentially encoding thus not capture similarity subtrees contrast gnn satisfying criterion theorem 3 generalizes wl test learning embed subtrees space enables gnns not only discriminate different structure also learn map similar graph structure similar embeddings capture dependency graph structure capturing structural similarity node label shown helpful generalization particularly subtrees sparse across different graph noisy edge node feature yanardag vishwanathan 2015 graph isomorphism network gin developed condition maximally powerful gnn next develop simple architecture graph isomorphism network gin provably satisﬁes condition theorem model generalizes wl test hence achieves maximum discriminative power among gnns model injective multiset function neighbor aggregation develop theory deep multisets parameterizing universal multiset function neural network next lemma state sum aggregator represent injective fact universal function multisets lemma assume x countable exists function f x h x p f x unique multiset x bounded size moreover any multiset function g decomposed g x φ f x some function prove lemma 5 appendix proof extends setting zaheer et 2017 set multisets important distinction deep multisets set certain popular injective set function mean aggregator not injective multiset function mechanism modeling universal multiset function lemma 5 building block conceive aggregation scheme represent universal function node multiset neighbor thus satisfy injectiveness condition theorem next corollary provides simple concrete formulation among many aggregation scheme corollary assume x countable exists function f x inﬁnitely many choice ϵ including irrational number h c x 1 ϵ f c p f x unique pair c x c x multiset bounded size moreover any function g pair decomposed g c x ϕ 1 ϵ f c p f x some function use perceptrons mlps model learn f ϕ corollary 6 thanks universal approximation theorem hornik et 1989 hornik 1991 practice model f k one mlp mlps represent composition function ﬁrst iteration not need mlps summation input feature encoding summation alone injective make ϵ learnable parameter ﬁxed scalar gin update node representation h k v mlp k 1 ϵ k h v x v h u generally may exist many powerful gnns gin one example among many maximally powerful gnns simple readout gin node embeddings learned gin directly used task like node classiﬁcation link prediction graph classiﬁcation task propose following readout function given embeddings individual node produce embedding entire graph important aspect readout node representation corresponding subtree structure get reﬁned global number iteration increase sufﬁcient number iteration key achieving good discriminative power yet feature earlier iteration may sometimes generalize better consider structural information use information model achieve architecture similar jumping knowledge 5 published conference paper iclr 2019 sum multiset mean distribution max set input figure 2 ranking expressive power sum mean max aggregator multiset left panel show input multiset network neighborhood aggregated next three panel illustrate aspect multiset given aggregator able capture sum capture full multiset mean capture element given type max aggregator ignores multiplicity reduces multiset simple set v mean max fail v b max fails v c mean max fail figure 3 example graph structure mean max aggregator fail distinguish two graph node v get embedding even though corresponding graph structure differ figure 2 give reasoning different aggregator compress different multisets thus fail distinguish network xu et 2018 replace eq graph representation concatenated across gin hg concat readout n h k v k 0 1 k theorem 3 corollary 6 gin replaces readout eq summing node feature iteration not need extra mlp summation reason eq provably generalizes wl test wl subtree kernel 5 le powerful still interesting gnns next study gnns not satisfy condition theorem 3 including gcn kipf welling 2017 graphsage hamilton et conduct ablation study two aspect aggregator eq 1 perceptrons instead mlps 2 mean instead sum see gnn variant get confused surprisingly simple graph le powerful wl test nonetheless model mean aggregator like gcn perform well node classiﬁcation task better understand precisely characterize different gnn variant not capture graph discus implication learning graph perceptrons not sufficient function f lemma 5 help map distinct multisets unique embeddings eterized mlp universal approximation theorem hornik 1991 nonetheless many existing gnns instead use perceptron σ duvenaud et 2015 kipf welling 2017 zhang et 2018 linear mapping followed activation function relu mapping example generalized linear model nelder wedderburn 1972 therefore interested understanding whether perceptrons enough graph learning lemma 7 suggests indeed network neighborhood multisets model perceptrons never distinguish lemma exist ﬁnite multisets any linear mapping w p relu wx p relu wx 6 published conference paper iclr 2019 main idea proof lemma 7 perceptrons behave much like linear mapping gnn layer degenerate simply summing neighborhood feature proof build fact bias term lacking linear mapping bias term sufﬁciently large output dimensionality perceptrons might able distinguish different multisets nonetheless unlike model using mlps perceptron even bias term not universal approximator multiset function consequently even gnns perceptrons embed different graph different location some degree embeddings may not adequately capture structural similarity difﬁcult simple classiﬁers linear classiﬁers ﬁt section 7 empirically see gnns perceptrons applied graph classiﬁcation sometimes severely underﬁt training data often perform worse gnns mlps term test accuracy structure confuse mean happens replace sum h x p f x mean gcn graphsage mean aggregator still multiset function permutation invariant not injective figure 2 rank three aggregator representational power figure 3 illustrates pair structure mean aggregator fail distinguish node color denote different node feature assume gnns aggregate neighbor ﬁrst combining central node labeled v figure every node ha feature f across node any function f performing neighborhood aggregation mean maximum f remains f induction always obtain node representation everywhere thus case mean aggregator fail capture any structural information contrast sum aggregator distinguishes structure 2 f 3 f give different value argument applied any unlabeled graph node degree instead constant value used node input feature principle mean recover sum not fig suggests mean max trouble distinguishing graph node repeating feature let hcolor r red g green denote node feature transformed fig show maximum neighborhood blue node v yield max hg hr max hg hr hr collapse representation even though corresponding graph structure different thus fails distinguish contrast sum aggregator still work 1 2 hg hr 1 3 hg hr hr general not equivalent similarly fig mean max fail 1 2 hg hr 1 4 hg hg hr hr mean learns distribution characterize class multisets mean aggregator distinguish consider example k set distinct element contains k copy element any mean aggregator map embedding simply take average individual element feature thus mean capture distribution proportion element multiset not exact multiset corollary assume x countable exists function f x h x 1 p f x h h only multisets distribution assuming k some k mean aggregator may perform well task statistical distributional information graph important exact structure moreover node feature diverse rarely repeat mean aggregator powerful sum aggregator may explain despite limitation identiﬁed section gnns mean aggregator effective node classiﬁcation task classifying article subject community detection node feature rich distribution neighborhood feature provides strong signal task learns set distinct element example figure 3 illustrate considers multiple node feature only one node treat multiset set capture neither exact structure 7 published conference paper iclr 2019 distribution however may suitable task important identify representative element skeleton rather distinguish exact structure distribution qi et al 2017 empirically show aggregator learns identify skeleton point cloud robust noise outlier completeness next corollary show aggregator capture underlying set multiset corollary assume x countable exists function f x h x f x h h only underlying set remark aggregator neighbor aggregation scheme not cover weighted average via attention velickovic et 2018 lstm pooling hamilton et murphy et 2018 emphasize theoretical framework general enough characterize representaional power any gnns future would interesting apply framework analyze understand aggregation scheme 6 related work despite empirical success gnns ha relatively little work mathematically study property exception work scarselli et al show perhaps earliest gnn model scarselli et approximate measurable function probability lei et al 2017 show proposed architecture lie rkhs graph kernel not study explicitly graph distinguish work focus speciﬁc architecture not easily generalize multple architecture contrast result provide general framework analyzing characterizing expressive power broad class gnns recently many architecture proposed including sum aggregation mlp encoding battaglia et 2016 scarselli et duvenaud et 2015 without theoretical derivation contrast many prior gnn architecture graph isomorphism network gin theoretically motivated simple yet powerful 7 experiment evaluate compare training test performance gin le powerful gnn training set performance allows u compare different gnn model based representational power test set performance quantiﬁes generalization ability datasets use 9 graph classiﬁcation benchmark 4 bioinformatics datasets mutag ptc protein 5 social network datasets collab binary yanardag vishwanathan 2015 importantly goal not allow model rely input node feature mainly learn network structure thus bioinformatic graph node categorical input feature social network no feature social network create node feature follows reddit datasets set node feature vector thus feature uninformative social graph use encoding node degree dataset statistic summarized table 1 detail data found appendix model conﬁgurations evaluate gin eq le powerful gnn variant gin framework consider two variant 1 gin learns ϵ eq gradient descent call 2 simpler slightly le powerful 2 gin ϵ eq ﬁxed 0 call see show strong empirical performance not only doe ﬁt training data equally well also demonstrates good generalization slightly consistently outperforming term test accuracy le powerful gnn variant consider architecture replace sum aggregation mean replace mlps perceptrons linear mapping followed code available exist certain somewhat contrived graph distinguish not collab not run experiment due gpu memory constraint 8 published conference paper iclr 2019 epoch 0 epoch 0 wl kernel gnn variant epoch 0 epoch 0 epoch 0 figure 4 training set performance gin le powerful gnn variant wl subtree kernel relu figure 4 table 1 model named us correspond gcn graphsage respectively minor architecture modiﬁcations apply readout readout eq gin gnn variant speciﬁcally sum readout bioinformatics datasets mean readout social datasets due better test performance following yanardag vishwanathan 2015 niepert et 2016 perform validation chang lin 2011 report average standard deviation validation accuracy across 10 fold within conﬁgurations 5 gnn layer including input layer applied mlps 2 layer batch normalization ioffe szegedy 2015 applied every hidden layer use adam optimizer kingma ba 2015 initial learning rate decay learning rate every 50 epoch tune dataset 1 number hidden unit 16 32 bioinformatics graph 64 social graph 2 batch size 32 128 3 dropout ratio 0 dense layer srivastava et 2014 4 number epoch single epoch best accuracy averaged 10 fold wa selected note due small dataset size alternative setting selection done using validation set extremely unstable mutag validation set only contains 18 data point also report training accuracy different gnns ﬁxed across datasets 5 gnn layer including input layer hidden unit size 64 minibatch size 128 dropout ratio comparison training accuracy wl subtree kernel reported set number iteration 4 comparable 5 gnn layer baseline compare gnns number baseline graph classiﬁcation 1 wl subtree kernel shervashidze et 2011 chang lin 2011 wa used classiﬁer tune c svm number wl iteration 1 2 6 2 deep learning architecture convolutional neural network dcnn atwood towsley 2016 niepert et 2016 deep graph cnn dgcnn zhang et 2018 3 anonymous walk embeddings awl ivanov burnaev 2018 deep learning method awl report accuracy reported original paper result training set performance validate theoretical analysis representational power gnns comparing training accuracy model higher representational power higher training set accuracy figure 4 show training curve gin le powerful gnn variant setting first theoretically powerful gnn able almost perfectly ﬁt training set experiment explicit learning ϵ yield no gain ﬁtting training data compared ﬁxing ϵ 0 comparison gnn variant using pooling perceptrons severely underﬁt many datasets particular training accuracy pattern align ranking model 9 published conference paper iclr 2019 datasets collab mutag protein ptc datasets graph 1000 1500 2000 5000 5000 188 1113 344 4110 class 2 3 2 5 3 2 2 2 2 avg node baseline wl subtree dcnn patchysan dgcnn awl gnn variant gcn graphsage table 1 test set classiﬁcation accuracy gnns highlighted boldface datasets gin accuracy not strictly highest among gnn variant see gin still comparable best gnn paired signiﬁcance level 10 doe not distinguish gin best thus gin also highlighted boldface baseline performs signiﬁcantly better gnns highlight boldface asterisk representational power gnn variant mlps tend higher training accuracy perceptrons gnns sum aggregator tend ﬁt training set better mean aggregator datasets training accuracy gnns never exceed wl subtree kernel expected gnns generally lower discriminative power wl test example imdbbinary none model perfectly ﬁt training set gnns achieve training accuracy wl kernel pattern aligns result wl test provides upper bound representational capacity gnns however wl kernel not able learn combine node feature might quite informative given prediction task see next test set performance next compare test accuracy although theoretical result not directly speak generalization ability gnns reasonable expect gnns strong expressive power accurately capture graph structure interest thus generalize well table 1 compare test accuracy gin gnn variant well baseline first gin especially outperform achieve comparable performance le powerful gnn variant 9 datasets achieving performance gin shine social network datasets contain relatively large number training graph reddit datasets node share scalar node feature gin gnns accurately capture graph structure signiﬁcantly outperform model gnns however fail capture any structure unlabeled graph predicted section not perform better random guessing even node degree provided input feature gnns perform much worse gnns accuracy gnn mlp aggregation comparing gin observe slightly consistently outperforms since model ﬁt training data equally well better generalization may explained simplicity compared 8 conclusion paper developed theoretical foundation reasoning expressive power gnns proved tight bound representational capacity popular gnn variant also designed provably maximally powerful gnn neighborhood aggregation framework interesting direction future work go beyond neighborhood aggregation message passing order pursue possibly even powerful architecture learning graph complete picture would also interesting understand improve generalization property gnns well better understand optimization landscape 10 published conference paper iclr 2019 acknowledgment research wa supported nsf career award 1553284 darpa award darpa dso lagrange program grant research wa also supported part nsf aro muri boeing huawei stanford data science initiative chan zuckerberg biohub weihua hu wa supported funai overseas scholarship thank kawarabayashi masashi sugiyama supporting research computing resource providing great advice thank tomohiro sonobe kento nozawa managing server thank rex ying william hamilton helpful feedback thank simon du yasuo tabei chengtao li jingling li helpful discussion positive comment reference james atwood towsley neural network advance neural information processing system nip pp lászló babai graph isomorphism quasipolynomial time proceeding annual acm symposium theory computing pp acm lászló babai ludik kucera canonical labelling graph linear average time foundation computer science annual symposium pp ieee peter battaglia razvan pascanu matthew lai danilo jimenez rezende et al interaction network learning object relation physic advance neural information processing system nip pp cai martin fürer neil immerman optimal lower bound number variable graph identiﬁcation combinatorica 12 4 chang lin libsvm library support vector machine acm transaction intelligent system technology tist 2 3 michaël defferrard xavier bresson pierre vandergheynst convolutional neural network graph fast localized spectral ﬁltering advance neural information processing system nip pp brendan l douglas method graph isomorphism testing arxiv preprint david k duvenaud dougal maclaurin jorge iparraguirre rafael bombarell timothy hirzel alán ryan p adam convolutional network graph learning molecular ﬁngerprints pp sergei evdokimov ilium ponomarenko isomorphism coloured graph slowly increasing multiplicity jordan block combinatorica 19 3 michael r garey guide theory computer intractability michael r garey david johnson computer intractability volume wh freeman new york justin gilmer samuel schoenholz patrick f riley oriol vinyals george e dahl neural message passing quantum chemistry international conference machine learning icml pp william l hamilton rex ying jure leskovec inductive representation learning large graph advance neural information processing system nip pp william l hamilton rex ying jure leskovec representation learning graph method application ieee data engineering bulletin 40 3 kurt hornik approximation capability multilayer feedforward network neural network 4 2 1991 11 published conference paper iclr 2019 kurt hornik maxwell stinchcombe halbert white multilayer feedforward network universal approximators neural network 2 5 sergey ioffe christian szegedy batch normalization accelerating deep network training reducing internal covariate shift international conference machine learning icml pp sergey ivanov evgeny burnaev anonymous walk embeddings international conference machine learning icml pp steven kearnes kevin mccloskey marc berndl vijay pande patrick riley molecular graph convolution moving beyond ﬁngerprints journal molecular design 30 8 diederik p kingma jimmy ba adam method stochastic optimization international conference learning representation iclr thomas n kipf max welling classiﬁcation graph convolutional network international conference learning representation iclr tao lei wengong jin regina barzilay tommi jaakkola deriving neural architecture sequence graph kernel pp yujia li daniel tarlow marc brockschmidt richard zemel gated graph sequence neural network international conference learning representation iclr ryan l murphy balasubramaniam srinivasan vinayak rao bruno ribeiro janossy ing learning deep function input arxiv preprint nelder wedderburn generalized linear model journal royal statistical society series general mathias niepert mohamed ahmed konstantin kutzkov learning convolutional neural network graph international conference machine learning icml pp charles r qi hao su kaichun mo leonidas j guibas pointnet deep learning point set classiﬁcation segmentation proc computer vision pattern recognition cvpr ieee 1 2 adam santoro david raposo david g barrett mateusz malinowski razvan pascanu peter battaglia timothy lillicrap simple neural network module relational reasoning advance neural information processing system pp adam santoro felix hill david barrett ari morcos timothy lillicrap measuring abstract reasoning neural network international conference machine learning pp franco scarselli marco gori ah chung tsoi markus hagenbuchner gabriele monfardini computational capability graph neural network ieee transaction neural network 20 1 franco scarselli marco gori ah chung tsoi markus hagenbuchner gabriele monfardini graph neural network model ieee transaction neural network 20 1 nino shervashidze pascal schweitzer erik jan van leeuwen kurt mehlhorn karsten borgwardt graph kernel journal machine learning research 12 sep nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov dropout simple way prevent neural network overﬁtting journal machine learning research 15 1 2014 12 published conference paper iclr 2019 petar velickovic guillem cucurull arantxa casanova adriana romero pietro lio yoshua bengio graph attention network international conference learning representation iclr saurabh verma zhang graph capsule convolutional neural network arxiv preprint boris weisfeiler aa lehman reduction graph canonical form algebra arising reduction informatsia 2 9 keyulu xu chengtao li yonglong tian tomohiro sonobe kawarabayashi stefanie jegelka representation learning graph jumping knowledge network international conference machine learning icml pp pinar yanardag svn vishwanathan deep graph kernel proceeding acm sigkdd international conference knowledge discovery data mining pp acm rex ying jiaxuan christopher morris xiang ren william l hamilton jure leskovec hierarchical graph representation learning differentiable pooling advance neural information processing system nip manzil zaheer satwik kottur siamak ravanbakhsh barnabas poczos ruslan r salakhutdinov alexander j smola deep set advance neural information processing system pp muhan zhang zhicheng cui marion neumann yixin chen deep learning architecture graph classiﬁcation aaai conference artiﬁcial intelligence pp 2018 13 published conference paper iclr 2019 proof lemma 2 proof suppose k iteration graph neural network ha wl test not decide follows iteration 0 k wl test always collection node label particular wl node label iteration 1 any 0 k collection multiset wl node label n l v well collection node neighborhood l v n l u u v otherwise wl test would obtained different collection node label iteration 1 different multisets get unique new label wl test always relabels different multisets neighboring node different new label show graph g wl node label l v l u always gnn node feature h v h u any iteration apparently hold 0 wl gnn start node feature suppose hold iteration j any u v l v l u must case l j v n l j w w v l j u n l j w w u assumption iteration j must h j v n h j w w v h j u n h j w w u aggregation process gnn aggregate combine applied input neighborhood feature generates output thus h v h u induction wl node label l v l u always gnn node feature h v h u any iteration creates valid mapping φ h v φ l v any v follows multiset wl neighborhood label also collection gnn neighborhood feature h v n h u u v φ l v n φ l u u v thus n h v particular collection gnn node feature n h k v graph level readout function permutation invariant respect collection node feature hence reached contradiction b proof theorem 3 proof let graph neural network condition hold let any graph wl test decides iteration readout function injective map distinct multiset node feature unique embeddings sufﬁcies show neighborhood aggregation process sufﬁcient iteration embeds different multisets node feature let u assume update node representation h k v φ h v f h u u v injective funtions f wl test applies predetermined injective hash function g update wl node label l k v l k v g l v n l u u v show induction any iteration k always exists injective function ϕ h k v ϕ l k v apparently hold k 0 initial node feature 14 published conference paper iclr 2019 wl gnn l 0 v h 0 v v ϕ could identity function k suppose hold iteration k show also hold substituting h v ϕ l v give u h k v φ ϕ l v f ϕ l u u v since composition injective function injective exists some injective function ψ h k v ψ l v n l u u v h k v ψ l v n l u u v ψ l k v ϕ ψ injective composition injective function injective hence any iteration k always exists injective function ϕ h k v ϕ l k v iteration wl test decides multisets n l k v different graph neural network node embeddings n h k v n ϕ l k v must also different injectivity c proof lemma 4 proof proving lemma ﬁrst show result later reduce problem nk countable every k ﬁnite cartesian product countable set countable observe sufﬁces show n n countable proof follows clearly induction show n n countable construct bijection φ n n n φ n go back proving lemma show range any function g deﬁned multisets bounded size countable set also countable lemma hold any g k induction thus goal show range g countable first clear mapping g x x injective g function follows sufﬁces show set multisets x countable since union two countable set countable following set x also countable x x e e dummy element not follows result showed nk countable every k x countable every k remains show exists injective mapping set multisets x x some k construct injective mapping h set multisets x x some k follows x countable exists mapping z x x natural number sort element x z x xn n multisets x bounded size exists k k deﬁne h h x xn e e e k coordinate ﬁlled dummy element clear h injective any multisets x bounded size h x h only x equivalent hence follows range g countable desired 15 published conference paper iclr 2019 proof lemma 5 proof ﬁrst prove exists mapping f p f x unique multiset x bounded size x countable exists mapping z x x natural number cardinality multisets x bounded exists number n n example f f x n x f viewed compressed form vector presentation thus h x p f x injective function multisets φ f x permutation invariant multiset function any multiset function g construct φ letting φ f x g x note φ h x p f x injective e proof corollary 6 proof following proof lemma 5 consider f x n x n z x deﬁned appendix let h c x 1 ϵ f c p f x goal show any c x c x h c x h hold ϵ irrational number prove contradiction any c x suppose exists c x h c x h hold let u consider following two case 1 c x 2 ﬁrst case h c x h c implies p f x p f x follows lemma 5 equality not hold f x n x x implies p f x p f x thus reach contradiction second case similarly rewrite h c x h ϵ f c f x f x f c x f x ϵ irrational number f c rational number eq irrational hand eq sum ﬁnite number rational number rational hence equality eq not hold reached contradiction any function g pair c x construct ϕ desired decomposition letting ϕ 1 ϵ f c p f x g c x note ϕ h c x 1 ϵ f c p f x injective f proof lemma 7 proof let u consider example 1 1 1 1 1 2 3 two different multisets positive number sum value using homogeneity relu let w arbitrary linear transform map x rn clear coordinate wx either positive negative x x positive follows relu wx either positive 0 coordinate x coordinate relu wx 0 p relu wx p relu wx coordinate wx positive linearity still hold follows linearity x relu wx relu w x x x could p x p x following desired x relu wx x relu wx 16 published conference paper iclr 2019 g proof corollary 8 proof suppose multisets distribution without loss generality let u assume k some k underlying set multiplicity element k time p f x k p f x thus 1 x f x 1 k k x f x 1 x f x show exists function f 1 p f x unique distributionally equivalent x countable exists mapping z x x natural number cardinality multisets x bounded exists number n n example f f x n x h proof corollary 9 proof suppose multisets underlying set max f x max f x max f x show exists mapping f f x unique x underlying set x countable exists mapping z x x natural number example f x deﬁned fi x 1 z x fi x 0 otherwise fi x coordinate f x f essentially map multiset embedding detail datasets give detailed description datasets used experiment detail found yanardag vishwanathan 2015 social network datasets movie collaboration datasets graph corresponds node correspond edge drawn betwen two appear movie graph derived genre movie task classify genre graph derived balanced datasets graph corresponds online discussion thread node correspond user edge wa drawn two node least one responded another comment task classify graph community subreddit belongs collab scientiﬁc collaboration dataset derived 3 public collaboration datasets namely high energy physic condensed matter physic astro physic graph corresponds different researcher ﬁeld task classify graph ﬁeld corresponding researcher belongs bioinformatics datasets mutag dataset 188 mutagenic aromatic heteroaromatic nitro compound 7 discrete label protein dataset node secondary structure element ss edge two node neighbor sequence space ha 3 discrete label representing helix sheet turn ptc dataset 344 chemical compound report carcinogenicity male female rat ha 19 discrete label dataset made publicly available national cancer institute nci subset balanced datasets chemical compound screened ability suppress inhibit growth panel human tumor cell line 37 discrete label 17