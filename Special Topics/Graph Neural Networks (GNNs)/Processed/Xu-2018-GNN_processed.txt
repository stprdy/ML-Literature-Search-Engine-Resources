publish confer paper iclr power graph neural network keyulu xu mit keyulu weihua hu stanford univers weihuahu jure leskovec stanford univers jure stefani jegelka mit stefj abstract graph neural network gnn effect framework represent learn graph gnn follow neighborhood aggreg scheme represent vector node comput recurs aggreg form represent vector neighbor node mani gnn variant propos achiev result node graph classiﬁc task howev despit gnn revolution graph represent learn limit understand represent properti limit present theoret framework analyz express power gnn captur differ graph structur result character discrimin power popular gnn variant graph convolut network graphsag show learn distinguish certain simpl graph structur develop simpl architectur provabl express among class gnn power lehman graph isomorph test empir valid theoret ﬁnding number graph classiﬁc benchmark demonstr model achiev perform introduct learn graph structur data molecul social biolog ﬁnancial network requir effect represent graph structur hamilton et recent ha surg interest graph neural network gnn approach represent learn graph li et hamilton et kipf well velickov et xu et gnn broadli follow recurs neighborhood aggreg messag pass scheme node aggreg featur vector neighbor comput new featur vector xu et gilmer et k iter aggreg node repres transform featur vector captur structur inform within node neighborhood represent entir graph obtain pool ying et exampl sum represent vector node graph mani gnn variant differ neighborhood aggreg pool scheme propos scarselli et battaglia et defferrard et duvenaud et hamilton et kearn et kipf well li et velickov et santoro et xu et santoro et verma zhang ying et zhang et empir gnn achiev perform mani task node classiﬁc link predict graph classiﬁc howev design new gnn mostli base empir intuit heurist experiment error littl theoret understand properti limit gnn formal analysi gnn represent capac limit contribut partial perform tokyo visit kawarabayashi partial perform riken aip univers tokyo feb publish confer paper iclr present theoret framework analyz represent power gnn formal character express differ gnn variant learn repres guish differ graph structur framework inspir close connect gnn wl graph isomorph test weisfeil lehman power test known distinguish broad class graph babai kucera similar gnn wl test iter updat given node featur vector aggreg featur vector network neighbor make wl test power inject aggreg updat map differ node neighborhood differ featur vector key insight gnn larg discrimin power wl test gnn aggreg scheme highli express model inject function mathemat formal abov insight framework ﬁrst repres set featur vector given node neighbor multiset set possibl repeat element neighbor aggreg gnn thought aggreg function multiset henc strong represent power gnn must abl aggreg differ multiset differ represent rigor studi sever variant multiset function theoret character discrimin power well differ aggreg function distinguish differ multiset discrimin multiset function power represent power underli gnn main result summar follow show gnn power wl test distinguish graph structur establish condit neighbor aggreg graph readout function result gnn power wl test identifi graph structur distinguish popular gnn variant gcn kipf well graphsag hamilton et precis character kind graph structur model captur develop simpl neural architectur graph isomorph network gin show power equal power wl test valid theori via experi graph classiﬁc dataset express power gnn crucial captur graph structur particular compar perform gnn variou aggreg function result conﬁrm power gnn theori graph isomorph network gin also empir ha high represent power almost perfectli ﬁt train data wherea less power gnn variant often sever underﬁt train data addit represent power gnn outperform test set accuraci achiev perform mani graph classiﬁc benchmark preliminari begin summar common gnn model along way introduc notat let g v e denot graph node featur vector xv v two task interest node classiﬁc node v ha associ label yv goal learn represent vector hv v v label predict yv f hv graph classiﬁc given set graph gn label yn aim learn represent vector hg help predict label entir graph yg g hg graph neural network gnn use graph structur node featur xv learn tion vector node hv entir graph hg modern gnn follow neighborhood aggreg strategi iter updat represent node aggreg represent neighbor k iter aggreg node represent captur structur inform within network neighborhood formal layer gnn k v aggreg k h u u v h k v combin k h v k v h k v featur vector node v initi h v xv n v set node adjac choic aggreg k combin k publish confer paper iclr graph root subtre wl test iter multiset gnn aggreg captur structur figur overview theoret framework middl panel root subtre structur blue node wl test use distinguish differ graph right panel gnn aggreg function captur full multiset node neighbor gnn captur root subtre recurs manner power wl test gnn crucial number architectur aggreg propos pool variant graphsag hamilton et aggreg ha formul k v max relu w h u v w learnabl matrix max repres combin step could concaten follow linear map w h h v k v graphsag graph convolut network gcn kipf well mean pool use instead aggreg combin step integr follow h k v relu w mean n h u v v mani gnn repres similarli eq xu et gilmer et node classiﬁc node represent h k v ﬁnal iter use predict graph classiﬁc readout function aggreg node featur ﬁnal iter obtain entir graph represent hg hg readout h k v v readout simpl permut invari function summat sophist pool function ying et zhang et test graph isomorph problem ask whether two graph topolog ident thi challeng problem algorithm known yet garey garey johnson babai apart corner case cai et wl test graph isomorph weisfeil lehman effect comput efﬁcient test distinguish broad class graph babai kucera form naïv vertex reﬁnement analog neighbor aggreg gnn wl test iter aggreg label node neighborhood hash aggreg label uniqu new label algorithm decid two graph iter label node two graph differ base wl test shervashidz et al propos wl subtre kernel measur similar graph kernel use count node label differ iter wl test featur vector graph intuit node label iter wl test repres subtre structur height k root node figur thu graph featur consid wl subtre kernel essenti count differ root subtre graph theoret framework overview start overview framework analyz express power gnn figur illustr idea gnn recurs updat node featur vector captur network structur featur node around root subtre structur figur throughout paper assum node input featur countabl univers ﬁnite graph node featur vector deeper layer ani ﬁxed model also countabl univers notat simplic assign featur vector uniqu label b c featur vector set neighbor node form multiset figur element appear multipl time sinc differ node ident featur vector publish confer paper iclr deﬁnit multiset multiset gener concept set allow multipl instanc element formal multiset x underli set x form distinct element give multipl element studi represent power gnn analyz gnn map two node locat embed space intuit maxim power gnn map two node locat onli ident subtre structur ident featur correspond node sinc subtre structur deﬁn recurs via node neighborhood figur reduc analysi question whether gnn map two neighborhood two multiset embed represent maxim power gnn would never map two differ neighborhood multiset featur vector represent thi mean aggreg scheme must inject thu abstract gnn aggreg scheme class function multiset neural network repres analyz whether abl repres inject multiset function next use thi reason develop maxim power gnn section studi popular gnn variant see aggreg scheme inher inject thu less power captur interest properti graph build power graph neural network first character maximum represent capac gener class model ideal maxim power gnn could distinguish differ graph structur map differ represent embed space thi abil map ani two differ graph differ embed howev impli solv challeng graph isomorph problem want isomorph graph map represent one differ represent analysi character represent capac gnn via slightli weaker criterion power heurist call wl graph isomorph test known work well gener except regular graph cai et dougla evdokimov ponomarenko lemma let ani two graph graph neural network g map differ embed graph isomorph test also decid isomorph proof lemma theorem found appendix henc ani gnn power wl test distinguish differ graph natur question whether exist gnn principl power wl test answer theorem ye neighbor aggreg readout function inject result gnn power wl test theorem let g gnn sufﬁcient number gnn layer map ani graph test isomorph decid differ embed follow condit hold aggreg updat node featur iter h k v φ h v f h u u v function f oper multiset φ inject b readout oper multiset node featur n h k v inject prove theorem appendix countabl set inject well character whether function preserv distinct input uncount set node featur continu need consider addit would interest character close togeth learn featur lie function imag leav question futur work focu case input node featur countabl set subset uncount set rn lemma assum input featur space x countabl let g k function parameter gnn layer k l g deﬁn multiset x bound size rang g k space node hidden featur h k v also countabl k publish confer paper iclr also worth discuss import beneﬁt gnn beyond distinguish differ graph captur similar graph structur note node featur vector wl test essenti encod thu captur similar subtre contrast gnn satisfi criteria theorem gener wl test learn emb subtre space thi enabl gnn onli discrimin differ structur also learn map similar graph structur similar embed captur depend graph structur captur structur similar node label shown help gener particularli subtre spars across differ graph noisi edg node featur yanardag vishwanathan graph isomorph network gin develop condit maxim power gnn next develop simpl architectur graph isomorph network gin provabl satisﬁ condit theorem thi model gener wl test henc achiev maximum discrimin power among gnn model inject multiset function neighbor aggreg develop theori deep multiset parameter univers multiset function neural network next lemma state sum aggreg repres inject fact univers function multiset lemma assum x countabl exist function f x h x p f x uniqu multiset x bound size moreov ani multiset function g decompos g x φ f x function prove lemma appendix proof extend set zaheer et set multiset import distinct deep multiset set certain popular inject set function mean aggreg inject multiset function mechan model univers multiset function lemma build block conceiv aggreg scheme repres univers function node multiset neighbor thu satisfi inject condit theorem next corollari provid simpl concret formul among mani aggreg scheme corollari assum x countabl exist function f x inﬁnit mani choic ϵ includ irrat number h c x ϵ f c p f x uniqu pair c x c x multiset bound size moreov ani function g pair decompos g c x ϕ ϵ f c p f x function use perceptron mlp model learn f ϕ corollari thank univers approxim theorem hornik et hornik practic model f k one mlp becaus mlp repres composit function ﬁrst iter need mlp befor summat input featur encod summat alon inject make ϵ learnabl paramet ﬁxed scalar gin updat node represent h k v mlp k ϵ k h v x v h u gener may exist mani power gnn gin one exampl among mani maxim power gnn simpl readout gin node embed learn gin directli use task like node classiﬁc link predict graph classiﬁc task propos follow readout function given embed individu node produc embed entir graph import aspect readout node represent correspond subtre structur get reﬁn global number iter increas sufﬁcient number iter key achiev good discrimin power yet featur earlier iter may sometim gener better consid structur inform use inform model achiev thi architectur similar jump knowledg publish confer paper iclr sum multiset mean distribut max set input figur rank express power sum mean max aggreg multiset left panel show input multiset network neighborhood aggreg next three panel illustr aspect multiset given aggreg abl captur sum captur full multiset mean captur element given type max aggreg ignor multipl reduc multiset simpl set v mean max fail v b max fail v c mean max fail figur exampl graph structur mean max aggreg fail distinguish two graph node v get embed even though correspond graph structur differ figur give reason differ aggreg compress differ multiset thu fail distinguish network xu et replac eq graph represent concaten across gin hg concat readout n h k v k k theorem corollari gin replac readout eq sum node featur iter need extra mlp befor summat reason eq provabl gener wl test wl subtre kernel less power still interest gnn next studi gnn satisfi condit theorem includ gcn kipf well graphsag hamilton et conduct ablat studi two aspect aggreg eq perceptron instead mlp mean instead sum see gnn variant get confus surprisingli simpl graph less power wl test nonetheless model mean aggreg like gcn perform well node classiﬁc task better understand thi precis character differ gnn variant captur graph discuss implic learn graph perceptron suffici function f lemma help map distinct multiset uniqu embed eter mlp univers approxim theorem hornik nonetheless mani exist gnn instead use perceptron σ duvenaud et kipf well zhang et linear map follow activ function relu map exampl gener linear model nelder wedderburn therefor interest understand whether perceptron enough graph learn lemma suggest inde network neighborhood multiset model perceptron never distinguish lemma exist ﬁnite multiset ani linear map w p relu wx p relu wx publish confer paper iclr main idea proof lemma perceptron behav much like linear map gnn layer degener simpli sum neighborhood featur proof build fact bia term lack linear map bia term sufﬁcient larg output dimension perceptron might abl distinguish differ multiset nonetheless unlik model use mlp perceptron even bia term univers approxim multiset function consequ even gnn perceptron emb differ graph differ locat degre embed may adequ captur structur similar difﬁcult simpl classiﬁ linear classiﬁ ﬁt section empir see gnn perceptron appli graph classiﬁc sometim sever underﬁt train data often perform wors gnn mlp term test accuraci structur confus mean happen replac sum h x p f x mean gcn graphsag mean aggreg still multiset function becaus permut invari inject figur rank three aggreg represent power figur illustr pair structur mean aggreg fail distinguish node color denot differ node featur assum gnn aggreg neighbor ﬁrst befor combin central node label v figur everi node ha featur f across node ani function f perform neighborhood aggreg mean maximum f remain f induct alway obtain node represent everywher thu thi case mean aggreg fail captur ani structur inform contrast sum aggreg distinguish structur becaus f f give differ valu argument appli ani unlabel graph node degre instead constant valu use node input featur principl mean recov sum fig suggest mean max troubl distinguish graph node repeat featur let hcolor r red g green denot node featur transform fig show maximum neighborhood blue node v yield max hg hr max hg hr hr collaps represent even though correspond graph structur differ thu fail distinguish contrast sum aggreg still work becaus hg hr hg hr hr gener equival similarli fig mean max fail hg hr hg hg hr hr mean learn distribut character class multiset mean aggreg distinguish consid exampl k set distinct element contain k copi element ani mean aggreg map embed becaus simpli take averag individu element featur thu mean captur distribut proport element multiset exact multiset corollari assum x countabl exist function f x h x p f x h h onli multiset distribut assum k k mean aggreg may perform well task statist distribut inform graph import exact structur moreov node featur divers rare repeat mean aggreg power sum aggreg thi may explain whi despit limit identiﬁ section gnn mean aggreg effect node classiﬁc task classifi articl subject commun detect node featur rich distribut neighborhood featur provid strong signal task learn set distinct element exampl figur illustr consid multipl node featur onli one node treat multiset set captur neither exact structur publish confer paper iclr distribut howev may suitabl task import identifi repres element skeleton rather distinguish exact structur distribut qi et al empir show aggreg learn identifi skeleton point cloud robust nois outlier complet next corollari show aggreg captur underli set multiset corollari assum x countabl exist function f x h x f x h h onli underli set remark aggreg neighbor aggreg scheme cover weight averag via attent velickov et lstm pool hamilton et murphi et emphas theoret framework gener enough character representaion power ani gnn futur would interest appli framework analyz understand aggreg scheme relat work despit empir success gnn ha rel littl work mathemat studi properti except thi work scarselli et al show perhap earliest gnn model scarselli et approxim measur function probabl lei et al show propos architectur lie rkh graph kernel studi explicitli graph distinguish work focus speciﬁc architectur easili gener multpl architectur contrast result abov provid gener framework analyz character express power broad class gnn recent mani architectur propos includ sum aggreg mlp encod battaglia et scarselli et duvenaud et without theoret deriv contrast mani prior gnn architectur graph isomorph network gin theoret motiv simpl yet power experi evalu compar train test perform gin less power gnn train set perform allow us compar differ gnn model base represent power test set perform quantiﬁ gener abil dataset use graph classiﬁc benchmark bioinformat dataset mutag ptc protein social network dataset collab binari yanardag vishwanathan importantli goal allow model reli input node featur mainli learn network structur thu bioinformat graph node categor input featur social network featur social network creat node featur follow reddit dataset set node featur vector thu featur uninform social graph use encod node degre dataset statist summar tabl detail data found appendix model conﬁgur evalu gin eq less power gnn variant gin framework consid two variant gin learn ϵ eq gradient descent call simpler slightli less power gin ϵ eq ﬁxed call see show strong empir perform onli doe ﬁt train data equal well also demonstr good gener slightli consist outperform term test accuraci less power gnn variant consid architectur replac sum aggreg mean replac mlp perceptron linear map follow code avail http exist certain somewhat contriv graph distinguish collab run experi due gpu memori constraint publish confer paper iclr epoch epoch wl kernel gnn variant epoch epoch epoch figur train set perform gin less power gnn variant wl subtre kernel relu figur tabl model name use correspond gcn graphsag respect minor architectur modiﬁc appli readout readout eq gin gnn variant speciﬁc sum readout bioinformat dataset mean readout social dataset due better test perform follow yanardag vishwanathan niepert et perform valid chang lin report averag standard deviat valid accuraci across fold within conﬁgur gnn layer includ input layer appli mlp layer batch normal ioff szegedi appli everi hidden layer use adam optim kingma ba initi learn rate decay learn rate everi epoch tune dataset number hidden unit bioinformat graph social graph batch size dropout ratio dens layer srivastava et number epoch singl epoch best accuraci averag fold wa select note due small dataset size altern set select done use valid set extrem unstabl mutag valid set onli contain data point also report train accuraci differ gnn ﬁxed across dataset gnn layer includ input layer hidden unit size minibatch size dropout ratio comparison train accuraci wl subtre kernel report set number iter compar gnn layer baselin compar gnn abov number baselin graph classiﬁc wl subtre kernel shervashidz et chang lin wa use classiﬁ tune c svm number wl iter deep learn architectur convolut neural network dcnn atwood towsley niepert et deep graph cnn dgcnn zhang et anonym walk embed awl ivanov burnaev deep learn method awl report accuraci report origin paper result train set perform valid theoret analysi represent power gnn compar train accuraci model higher represent power higher train set accuraci figur show train curv gin less power gnn variant set first theoret power gnn abl almost perfectli ﬁt train set experi explicit learn ϵ yield gain ﬁtting train data compar ﬁxing ϵ comparison gnn variant use pool perceptron sever underﬁt mani dataset particular train accuraci pattern align rank model publish confer paper iclr dataset collab mutag protein ptc dataset graph class avg node baselin wl subtre dcnn patchysan dgcnn awl gnn variant gcn graphsag tabl test set classiﬁc accuraci gnn highlight boldfac dataset gin accuraci strictli highest among gnn variant see gin still compar best gnn becaus pair signiﬁc level doe distinguish gin best thu gin also highlight boldfac baselin perform signiﬁcantli better gnn highlight boldfac asterisk represent power gnn variant mlp tend higher train accuraci perceptron gnn sum aggreg tend ﬁt train set better mean aggreg dataset train accuraci gnn never exceed wl subtre kernel thi expect becaus gnn gener lower discrimin power wl test exampl imdbbinari none model perfectli ﬁt train set gnn achiev train accuraci wl kernel thi pattern align result wl test provid upper bound represent capac gnn howev wl kernel abl learn combin node featur might quit inform given predict task see next test set perform next compar test accuraci although theoret result directli speak gener abil gnn reason expect gnn strong express power accur captur graph structur interest thu gener well tabl compar test accuraci gin gnn variant well baselin first gin especi outperform achiev compar perform less power gnn variant dataset achiev perform gin shine social network dataset contain rel larg number train graph reddit dataset node share scalar node featur gin gnn accur captur graph structur signiﬁcantli outperform model gnn howev fail captur ani structur unlabel graph predict section perform better random guess even node degre provid input featur gnn perform much wors gnn accuraci gnn mlp aggreg compar gin observ slightli consist outperform sinc model ﬁt train data equal well better gener may explain simplic compar conclus thi paper develop theoret foundat reason express power gnn prove tight bound represent capac popular gnn variant also design provabl maxim power gnn neighborhood aggreg framework interest direct futur work go beyond neighborhood aggreg messag pass order pursu possibl even power architectur learn graph complet pictur would also interest understand improv gener properti gnn well better understand optim landscap publish confer paper iclr acknowledg thi research wa support nsf career award darpa award darpa dso lagrang program grant thi research wa also support part nsf aro muri boe huawei stanford data scienc initi chan zuckerberg biohub weihua hu wa support funai oversea scholarship thank kawarabayashi masashi sugiyama support thi research comput resourc provid great advic thank tomohiro sonob kento nozawa manag server thank rex ying william hamilton help feedback thank simon du yasuo tabei chengtao li jingl li help discuss posit comment refer jame atwood towsley neural network advanc neural inform process system nip pp lászló babai graph isomorph quasipolynomi time proceed annual acm symposium theori comput pp acm lászló babai ludik kucera canon label graph linear averag time foundat comput scienc annual symposium pp ieee peter battaglia razvan pascanu matthew lai danilo jimenez rezend et al interact network learn object relat physic advanc neural inform process system nip pp cai martin fürer neil immerman optim lower bound number variabl graph identiﬁc combinatorica chang lin libsvm librari support vector machin acm transact intellig system technolog tist michaël defferrard xavier bresson pierr vandergheynst convolut neural network graph fast local spectral ﬁltere advanc neural inform process system nip pp brendan l dougla method graph isomorph test arxiv preprint david k duvenaud dougal maclaurin jorg iparraguirr rafael bombarel timothi hirzel alán ryan p adam convolut network graph learn molecular ﬁngerprint pp sergei evdokimov ilia ponomarenko isomorph colour graph slowli increas multipl jordan block combinatorica michael r garey guid theori comput intract michael r garey david johnson comput intract volum wh freeman new york justin gilmer samuel schoenholz patrick f riley oriol vinyal georg e dahl neural messag pass quantum chemistri intern confer machin learn icml pp william l hamilton rex ying jure leskovec induct represent learn larg graph advanc neural inform process system nip pp william l hamilton rex ying jure leskovec represent learn graph method applic ieee data engin bulletin kurt hornik approxim capabl multilay feedforward network neural network publish confer paper iclr kurt hornik maxwel stinchcomb halbert white multilay feedforward network univers approxim neural network sergey ioff christian szegedi batch normal acceler deep network train reduc intern covari shift intern confer machin learn icml pp sergey ivanov evgeni burnaev anonym walk embed intern confer machin learn icml pp steven kearn kevin mccloskey marc berndl vijay pand patrick riley molecular graph convolut move beyond ﬁngerprint journal molecular design diederik p kingma jimmi ba adam method stochast optim intern confer learn represent iclr thoma n kipf max well classiﬁc graph convolut network intern confer learn represent iclr tao lei wengong jin regina barzilay tommi jaakkola deriv neural architectur sequenc graph kernel pp yujia li daniel tarlow marc brockschmidt richard zemel gate graph sequenc neural network intern confer learn represent iclr ryan l murphi balasubramaniam srinivasan vinayak rao bruno ribeiro janossi ing learn deep function input arxiv preprint nelder wedderburn gener linear model journal royal statist societi seri gener mathia niepert moham ahm konstantin kutzkov learn convolut neural network graph intern confer machin learn icml pp charl r qi hao su kaichun mo leonida j guiba pointnet deep learn point set classiﬁc segment proc comput vision pattern recognit cvpr ieee adam santoro david raposo david g barrett mateusz malinowski razvan pascanu peter battaglia timothi lillicrap simpl neural network modul relat reason advanc neural inform process system pp adam santoro felix hill david barrett ari morco timothi lillicrap measur abstract reason neural network intern confer machin learn pp franco scarselli marco gori ah chung tsoi marku hagenbuchn gabriel monfardini comput capabl graph neural network ieee transact neural network franco scarselli marco gori ah chung tsoi marku hagenbuchn gabriel monfardini graph neural network model ieee transact neural network nino shervashidz pascal schweitzer erik jan van leeuwen kurt mehlhorn karsten borgwardt graph kernel journal machin learn research sep nitish srivastava geoffrey hinton alex krizhevski ilya sutskev ruslan salakhutdinov dropout simpl way prevent neural network overﬁt journal machin learn research publish confer paper iclr petar velickov guillem cucurul arantxa casanova adriana romero pietro lio yoshua bengio graph attent network intern confer learn represent iclr saurabh verma zhang graph capsul convolut neural network arxiv preprint bori weisfeil aa lehman reduct graph canon form algebra aris dure thi reduct informatsia keyulu xu chengtao li yonglong tian tomohiro sonob kawarabayashi stefani jegelka represent learn graph jump knowledg network intern confer machin learn icml pp pinar yanardag svn vishwanathan deep graph kernel proceed acm sigkdd intern confer knowledg discoveri data mine pp acm rex ying jiaxuan christoph morri xiang ren william l hamilton jure leskovec hierarch graph represent learn differenti pool advanc neural inform process system nip manzil zaheer satwik kottur siamak ravanbakhsh barnaba poczo ruslan r salakhutdinov alexand j smola deep set advanc neural inform process system pp muhan zhang zhicheng cui marion neumann yixin chen deep learn architectur graph classiﬁc aaai confer artiﬁci intellig pp publish confer paper iclr proof lemma proof suppos k iter graph neural network ha wl test decid follow iter k wl test alway collect node label particular becaus wl node label iter ani k collect multiset wl node label n l v well collect node neighborhood l v n l u u v otherwis wl test would obtain differ collect node label iter differ multiset get uniqu new label wl test alway relabel differ multiset neighbor node differ new label show graph g wl node label l v l u alway gnn node featur h v h u ani iter thi appar hold becaus wl gnn start node featur suppos thi hold iter j ani u v l v l u must case l j v n l j w w v l j u n l j w w u assumpt iter j must h j v n h j w w v h j u n h j w w u aggreg process gnn aggreg combin appli input neighborhood featur gener output thu h v h u induct wl node label l v l u alway gnn node featur h v h u ani iter thi creat valid map φ h v φ l v ani v follow multiset wl neighborhood label also collect gnn neighborhood featur h v n h u u v φ l v n φ l u u v thu n h v particular collect gnn node featur n h k v becaus graph level readout function permut invari respect collect node featur henc reach contradict b proof theorem proof let graph neural network condit hold let ani graph wl test decid iter becaus readout function inject map distinct multiset node featur uniqu embed sufﬁci show neighborhood aggreg process sufﬁcient iter emb differ multiset node featur let us assum updat node represent h k v φ h v f h u u v inject funtion f wl test appli predetermin inject hash function g updat wl node label l k v l k v g l v n l u u v show induct ani iter k alway exist inject function ϕ h k v ϕ l k v thi appar hold k becaus initi node featur publish confer paper iclr wl gnn l v h v v ϕ could ident function k suppos thi hold iter k show also hold substitut h v ϕ l v give us h k v φ ϕ l v f ϕ l u u v sinc composit inject function inject exist inject function ψ h k v ψ l v n l u u v h k v ψ l v n l u u v ψ l k v ϕ ψ inject becaus composit inject function inject henc ani iter k alway exist inject function ϕ h k v ϕ l k v iter wl test decid multiset n l k v differ graph neural network node embed n h k v n ϕ l k v must also differ becaus inject c proof lemma proof befor prove lemma ﬁrst show result later reduc problem nk countabl everi k ﬁnite cartesian product countabl set countabl observ sufﬁc show n n countabl becaus proof follow clearli induct show n n countabl construct biject φ n n n φ n go back prove lemma show rang ani function g deﬁn multiset bound size countabl set also countabl lemma hold ani g k induct thu goal show rang g countabl first clear map g x x inject becaus g function follow sufﬁc show set multiset x countabl sinc union two countabl set countabl follow set x also countabl x x e e dummi element follow result show abov nk countabl everi k x countabl everi k remain show exist inject map set multiset x x k construct inject map h set multiset x x k follow becaus x countabl exist map z x x natur number sort element x z x xn n becaus multiset x bound size exist k k deﬁn h h x xn e e e k coordin ﬁlled dummi element clear h inject becaus ani multiset x bound size h x h onli x equival henc follow rang g countabl desir publish confer paper iclr proof lemma proof ﬁrst prove exist map f p f x uniqu multiset x bound size becaus x countabl exist map z x x natur number becaus cardin multiset x bound exist number n n exampl f f x n x thi f view compress form vector present thu h x p f x inject function multiset φ f x permut invari multiset function ani multiset function g construct φ let φ f x g x note φ becaus h x p f x inject e proof corollari proof follow proof lemma consid f x n x n z x deﬁn appendix let h c x ϵ f c p f x goal show ani c x c x h c x h hold ϵ irrat number prove contradict ani c x suppos exist c x h c x h hold let us consid follow two case c x ﬁrst case h c x h c impli p f x p f x follow lemma equal hold becaus f x n x x impli p f x p f x thu reach contradict second case similarli rewrit h c x h ϵ f c f x f x f c x f x becaus ϵ irrat number f c ration number eq irrat hand eq sum ﬁnite number ration number ration henc equal eq hold reach contradict ani function g pair c x construct ϕ desir decomposit let ϕ ϵ f c p f x g c x note ϕ becaus h c x ϵ f c p f x inject f proof lemma proof let us consid exampl two differ multiset posit number sum valu use homogen relu let w arbitrari linear transform map x rn clear coordin wx either posit neg x becaus x posit follow relu wx either posit coordin x coordin relu wx p relu wx p relu wx coordin wx posit linear still hold follow linear x relu wx relu w x x x could becaus p x p x follow desir x relu wx x relu wx publish confer paper iclr g proof corollari proof suppos multiset distribut without loss gener let us assum k k underli set multipl element k time p f x k p f x thu x f x k k x f x x f x show exist function f p f x uniqu distribut equival becaus x countabl exist map z x x natur number becaus cardin multiset x bound exist number n n exampl f f x n x h proof corollari proof suppos multiset underli set max f x max f x max f x show exist map f f x uniqu xs underli set becaus x countabl exist map z x x natur number exampl f x deﬁn fi x z x fi x otherwis fi x coordin f x f essenti map multiset embed detail dataset give detail descript dataset use experi detail found yanardag vishwanathan social network dataset movi collabor dataset graph correspond node correspond edg drawn betwen two appear movi graph deriv genr movi task classifi genr graph deriv balanc dataset graph correspond onlin discuss thread node correspond user edg wa drawn two node least one respond anoth comment task classifi graph commun subreddit belong collab scientiﬁc collabor dataset deriv public collabor dataset name high energi physic condens matter physic astro physic graph correspond differ research ﬁeld task classifi graph ﬁeld correspond research belong bioinformat dataset mutag dataset mutagen aromat heteroaromat nitro compound discret label protein dataset node secondari structur element ss edg two node neighbor sequenc space ha discret label repres helix sheet turn ptc dataset chemic compound report carcinogen male femal rat ha discret label dataset made publicli avail nation cancer institut nci subset balanc dataset chemic compound screen abil suppress inhibit growth panel human tumor cell line discret label