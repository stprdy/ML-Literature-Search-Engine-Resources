ieee transact neural network learn system vol januari comprehens survey graph neural network zonghan wu shirui pan member ieee fengwen chen guodong long chengqi zhang senior member ieee philip yu life fellow ieee learn ha revolution mani machin learn task recent year rang imag classiﬁc video process speech recognit natur languag understand data task typic repres euclidean space howev increas number applic data gener domain repres graph complex ship interdepend object complex graph data ha impos signiﬁc challeng exist machin learn algorithm recent mani studi ing deep learn approach graph data emerg thi articl provid comprehens overview graph neural network gnn data mine machin learn ﬁeld propos new taxonomi divid gnn four categori name recurr gnn tional gnn graph autoencod gnn discuss applic gnn across variou domain summar code benchmark data set model evalu gnn final propos potenti research direct thi rapidli grow ﬁeld index learn graph autoencod gae graph convolut network gcn graph neural network gnn graph represent learn network embed introduct recent success neural network ha boost research pattern recognit data mine mani machin learn task object detect machin translat speech recognit onc heavili reli handcraft featur engin extract inform featur set recent tioniz variou deep learn paradigm convolut neural network cnn recurr neural manuscript receiv januari revis august decemb accept march date public march date current version januari thi work wa support part australian govern australian research council arc grant grant part nsf grant grant grant grant correspond author shirui pan zonghan wu fengwen chen guodong long chengqi zhang centr artiﬁci intellig faculti engin inform technolog univers technolog sydney ultimo nsw australia shirui pan faculti inform technolog monash siti clayton vic australia philip yu depart comput scienc univers illinoi chicago chicago il usa psyu thi articl ha supplementari download materi avail http provid author color version one ﬁgure thi articl avail onlin http digit object identiﬁ network rnn autoencod success deep learn mani domain partial attribut rapidli develop comput resourc gpu avail big train data effect deep learn extract latent represent euclidean data imag text video take imag data exampl repres imag regular grid euclidean space cnn abl exploit invari local connect composition imag data result cnn extract local meaning featur share entir data set variou imag analys deep learn effect captur hidden pattern euclidean data increas number tion data repres form graph exampl learn system exploit interact user product make highli accur recommend chemistri molecul model graph bioactiv need identiﬁ drug discoveri citat network articl link via citationship need categor differ group complex graph data ha impos signiﬁc challeng exist machin learn algorithm graph irregular graph may variabl size unord node node graph may differ number neighbor result import oper convolut easi comput imag domain difﬁcult appli graph domain furthermor core assumpt exist machin learn algorithm instanc independ thi assumpt longer hold graph data becaus instanc node relat link variou type citat friendship interact recent increas interest extend deep learn approach graph data motiv cnn rnn autoencod deep learn new tion deﬁnit import oper rapidli develop past year handl complex graph data exampl graph convolut gener convolut illustr fig imag consid special case graph pixel connect adjac pixel similar convolut one may perform graph convolut take weight averag node neighborhood inform limit number exist review topic graph neural network gnn use term geometr deep learn bronstein et al give overview ieee person use permit requir ieee permiss see http inform author licens use limit queen mari univers london download march utc ieee xplore restrict appli wu et al comprehens survey gnn fig convolut versu graph convolut convolut analog graph pixel imag taken node neighbor determin ﬁlter size convolut take weight averag pixel valu red node along neighbor neighbor node order ﬁxed size b graph convolut get hidden represent red node one simpl solut graph convolut oper take averag valu node featur red node along neighbor differ imag data neighbor node unord variabl size deep learn method domain ing graph manifold although ﬁrst review gnn thi articl mainli review convolut gnn hamilton et al cover limit number gnn focu address problem network embed battaglia et al posit graph network build block learn relat data review part gnn uniﬁ framework lee et al conduct partial survey gnn appli differ attent anism summari exist survey onli includ gnn examin limit number work therebi miss recent develop gnn thi articl provid comprehens overview gnn est research want enter thi rapidli develop ﬁeld expert would like compar gnn model cover broader rang method thi articl consid gnn deep learn approach graph data contribut thi articl make notabl contribut summar follow new taxonomi propos new taxonomi gnn gnn categor four group recurr gnn recgnn convolut gnn convgnn graph autoencod gae gnn stgnn comprehens review provid hensiv overview modern deep learn techniqu graph data type gnn provid detail descript repres model make necessari comparison summar correspond algorithm abund resourc collect abund resourc gnn includ model benchmark data set code practic applic thi articl use guid stand use develop differ deep learn approach variou applic futur direct discuss theoret aspect gnn analyz limit exist method suggest four possibl futur research direct term model depth scalabl tradeoff heterogen dynam organ thi articl rest thi articl organ follow section ii outlin background gnn list commonli use tion deﬁn concept section iii clariﬁ categor gnn section provid overview gnn model section viii present collect applic across variou domain section ix discuss current challeng suggest futur direct section x summar thi articl ii background definit thi section outlin background gnn list commonli use notat deﬁn concept background brief histori graph neural network sperduti starita ﬁrst appli neural network direct acycl graph motiv earli studi gnn notion gnn wa initi outlin elabor earli studi fall categori recgnn learn target node represent propag neighbor inform iter manner stabl ﬁxed point reach thi process comput expens recent increas effort overcom challeng encourag success cnn comput vision domain larg number method redeﬁn notion convolut graph data develop parallel approach umbrella vgnn convgnn divid two main stream approach approach ﬁrst promin research convgnn wa present bruna et al develop graph convolut base spectral graph theori sinc increas improv extens approxim convgnn research convgnn start much earlier convgnn mich ﬁrst address graph mutual depend architectur posit nonrecurs layer inherit idea messag pass recgnn howev import thi articl wa overlook recent mani convgnn emerg apart recgnn convgnn mani altern gnn ope past year includ gae stgnn learn framework built recgnn vgnn neural architectur graph model detail categor method given section iii graph neural network versu network embed research gnn close relat graph embed network embed anoth topic attract increas attent data mine machin learn commun network embed aim resent network node vector tation preserv network topolog structur node author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact neural network learn system vol januari content inform ani subsequ graph analyt task classiﬁc cluster recommend easili perform use simpl machin learn algorithm support vector machin cation meanwhil gnn deep learn model aim address task manner mani gnn explicitli extract represent main distinct gnn network embed gnn group neural network model design variou task network embed cover variou kind method target task therefor gnn address network embed problem gae framework hand network embed contain nondeep learn method matrix factor random walk graph neural network versu graph kernel method graph kernel histor domin techniqu solv problem graph classiﬁc method employ kernel function measur similar pair graph algorithm support vector machin use supervis learn graph similar gnn graph kernel emb graph node vector space map function differ thi map function determinist rather abl due pairwis similar calcul graph kernel method suffer signiﬁcantli comput bottleneck gnn one hand directli perform graph classiﬁc base extract graph represent therefor much efﬁcient graph kernel method review graph kernel method refer reader deﬁnit throughout thi articl use bold uppercas charact denot matric bold lowercas charact denot vector unless particularli speciﬁ notat use thi articl illustr tabl deﬁn minim set deﬁnit requir understand thi articl deﬁnit graph graph repres g v e v set vertic node use node throughout thi articl e set edg let vi denot node ei j vi v j denot edg point v j vi neighborhood node v deﬁn n v u v u adjac matrix n n matrix ai j ei j ai j ei j graph may node attribut x node featur matrix xv repres featur vector node meanwhil graph may edg attribut xe xe edg featur matrix xe v u repres featur vector edg v u deﬁnit direct graph direct graph graph edg direct one node anoth rect graph consid special case direct graph pair edg invers direct two node connect graph undirect onli adjac matrix symmetr graph refer attribut graph literatur tabl commonli use notat deﬁnit graph oral graph attribut graph node attribut chang dynam time graph deﬁn g v e x x iii categor framework thi section present taxonomi gnn shown tabl ii categor gnn recgnn convgnn gae stgnn fig show exampl variou model architectur follow give brief duction categori taxonomi graph neural network recurr graph neural network mostli pioneer work gnn recgnn aim learn node tation recurr neural architectur assum node graph constantli exchang neighbor stabl equilibrium reach recgnn conceptu import inspir later research convgnn particular idea messag pass inherit convgnn convolut graph neural network gener oper convolut grid data graph data main idea gener node v represent aggreg featur xv neighbor featur xu u v differ recgnn convgnn stack multipl graph convolut layer extract node represent convgnn play central role build mani complex gnn model fig show author licens use limit queen mari univers london download march utc ieee xplore restrict appli wu et al comprehens survey gnn tabl ii taxonomi repres public gnn convgnn node classiﬁc fig b demonstr convgnn graph classiﬁc graph autoencod unsupervis learn framework encod latent vector space reconstruct graph data encod inform gae use learn network embed graph ativ distribut network embed gae learn latent node represent reconstruct graph structur inform graph adjac matrix graph gener method gener node edg graph step step method output graph onc fig c present gae network embed graph neural network aim learn hidden pattern graph becom increasingli import varieti applic trafﬁc speed forecast driver maneuv pation human action recognit key idea stgnn consid spatial depend tempor depend time mani current approach grate graph convolut captur spatial depend rnn cnn model tempor depend fig illustr stgnn graph forecast framework graph structur node content inform input output gnn focu differ graph analyt task one follow mechan node level output relat node regress node classiﬁc task recgnn convgnn extract node represent inform convolut multiperceptron softmax layer output layer gnn abl perform task manner edg level output relat edg classiﬁc link predict task two node hidden sentat gnn input similar function neural network util predict strength edg graph level output relat graph classiﬁc task obtain compact represent graph level gnn often combin pool oper detail inform pool readout review section train framework mani gnn convgnn train semi supervis pure unsupervis way within learn framework depend learn task label inform avail hand semisupervis learn classiﬁc given singl network partial node label remain unlabel convgnn learn robust model effect identiﬁ class label unlabel node thi end end framework built stack coupl graph convolut layer follow softmax layer multiclass classiﬁc supervis learn classiﬁc classiﬁc aim predict class label entir graph learn thi task realiz combin graph convolut layer graph pool layer readout layer graph volut layer respons exact node represent graph pool layer play role downsampl coarsen graph substructur time readout layer collaps node represent graph graph tion appli multilay perceptron softmax layer graph represent build end framework graph classiﬁc exampl given fig b unsupervis learn graph embed class label avail graph learn graph embed pure unsupervis way framework algorithm exploit level inform two way one simpl way adopt autoencod framework encod employ graph convolut layer emb graph latent represent upon decod use reconstruct graph structur anoth popular way util neg sampl approach sampl portion node pair neg pair exist node pair link graph posit pair logist regress layer appli distinguish posit neg pair tabl iii summar main characterist repres recgnn convgnn input sourc ing layer readout layer time complex compar among variou model detail onli compar time complex convolut author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact neural network learn system vol januari fig differ gnn model built graph convolut layer term gconv denot graph convolut layer term mlp denot multilay perceptron term cnn denot standard convolut layer convgnn multipl graph convolut layer graph tional layer encapsul node hidden represent aggreg featur inform neighbor featur aggreg nonlinear transform appli result output stack multipl layer ﬁnal hidden represent node receiv messag neighborhood b convgnn pool readout layer graph classiﬁc graph convolut layer follow pool layer coarsen graph subgraph node represent coarsen graph repres higher represent readout layer summar ﬁnal graph represent take hidden represent subgraph c gae network embed encod use graph convolut layer get network embed node decod comput pairwis distanc given network embed appli nonlinear activ function decod reconstruct graph adjac matrix network train minim discrep real adjac matrix reconstruct adjac matrix stgnn graph forecast graph convolut layer follow layer graph convolut layer oper x captur spatial depend layer slide x along time axi captur tempor depend output layer linear transform gener predict node futur valu next time step oper model method requir eigenvalu decomposit time complex time complex also due node pairwis comput method incur equival time complex graph adjac matrix spars otherwis thi becaus method comput node vi represent involv di neighbor sum di node exactli equal number edg time complex sever method miss tabl iii method either lack time complex analysi articl report time complex overal model algorithm iv recurr graph neural network recgnn mostli pioneer work gnn appli set paramet recurr node graph extract node represent constrain comput power earlier research mainli focus direct acycl graph gnn propos scarselli et al extend prior recurr model handl gener type graph acycl cyclic direct undirect graph base tion diffus mechan gnn updat node state exchang neighborhood inform recurr ble equilibrium reach node hidden state recurr updat h v v f xv xe v u xu h u f parametr function h v initi randomli sum oper enabl gnn applic node even number neighbor differ neighborhood order known ensur converg recurr function f must contract map shrink distanc two point project latent space case f neural work penalti term ha impos jacobian matrix paramet converg criterion satisﬁ last step node hidden state forward readout layer gnn altern stage node state propag stage paramet gradient comput minim ing object thi strategi enabl gnn handl cyclic graph work graph echo state network graphesn extend echo state network improv train efﬁcienc gnn graphesn consist encod output layer encod randomli initi requir train implement contract state transit function recurr updat node state global graph state reach converg afterward output layer train take ﬁxed node state input gate gnn ggnn employ gate recurr unit gru recurr function reduc recurr ﬁxed number step advantag longer need constrain paramet ensur converg node hidden state updat previou hidden state neighbor hidden state deﬁn h v gru v v wh u h v xv differ gnn graphesn ggnn use backpropag time bptt rithm learn model paramet thi problemat gnn use repres broad graph neural network thi articl name thi particular method gnn avoid ambigu author licens use limit queen mari univers london download march utc ieee xplore restrict appli wu et al comprehens survey gnn tabl iii summari recgnn convgnn miss valu pool readout layer indic method onli experi task larg graph ggnn need run recurr function multipl time node requir intermedi state node store memori stochast embed sse propos ing algorithm scalabl larg graph sse updat node hidden state recurr stochast asynchron fashion altern sampl batch node state updat batch node gradient comput maintain stabil recurr function sse deﬁn weight averag histor state new state take form h v h v v h u xu α hyperparamet h v initi randomli conceptu import sse doe theoret prove node state gradual converg ﬁxed point appli repeatedli convolut graph neural network convgnn close relat recurr graph neural network instead iter node state contract straint convgnn address cyclic mutual depend architectur use ﬁxed number layer ent weight layer thi key distinct illustr fig graph convolut efﬁcient nient composit neural network popular convgnn ha rapidli grow recent year convgnn fall two categori base approach deﬁn graph convolut author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact neural network learn system vol januari fig recgnn versu convgnn recgnn use graph recurr layer grec updat node represent b convgnn use differ graph convolut layer gconv updat node represent introduc ﬁlter perspect graph signal process graph convolut oper interpret remov nois graph signal base approach inherit idea recgnn deﬁn graph convolut inform propag sinc gcn bridg gap approach approach method ope rapidli recent due attract efﬁcienc ﬂexibl gener convgnn background method solid matic foundat graph signal process assum graph undirect normal graph cian matrix mathemat represent undirect graph deﬁn l diagon matrix node degre dii j ai j normal graph laplacian matrix possess properti real symmetr posit semideﬁnit thi properti normal laplacian matrix factor l u matrix eigenvector order eigenvalu diagon matrix eigenvalu spectrum λi eigenvector normal laplacian matrix form orthonorm space mathemat word ut u graph signal process graph signal x featur vector node graph xi valu ith node graph fourier transform signal x deﬁn f x ut x invers graph fourier transform deﬁn f ˆ x uˆ x ˆ x repres result signal graph fourier transform graph fourier transform project input graph signal orthonorm space basi form eigenvector normal graph laplacian element transform signal ˆ x coordin graph signal new space input signal repres x ˆ xiui exactli invers graph fourier transform graph convolut input signal x ﬁlter g deﬁn x g f f x g u ut x g elementwis product denot ﬁlter gθ diag ut g spectral graph convolut simpliﬁ x gθ ugθut x convgnn follow thi deﬁnit key differ lie choic ﬁlter gθ spectral cnn assum ﬁlter gθ k j set learnabl paramet consid graph signal multipl channel graph convolut layer spectral cnn deﬁn h k j σ k j ut h j fk k layer index h input graph signal h x number input channel fk number output channel k j diagon matrix ﬁlled learnabl paramet due decomposit laplacian matrix spectral cnn face three limit first ani perturb graph result chang eigenbasi second learn ﬁlter domain depend mean appli graph differ structur third eigendecomposit requir comput complex work chebnet gcn reduc comput complex make sever approxim simpliﬁc chebyshev spectral cnn chebnet approxim ﬁlter gθ chebyshev polynomi diagon matrix eigenvalu gθ k θiti valu lie chebyshev polynomi deﬁn recurs ti x x x x x result convolut graph signal x deﬁn ﬁlter gθ x gθ u k θiti ut x l ti l uti ut proven induct chebnet take form x gθ k θiti l x improv spectral cnn ﬁlter deﬁn chebnet local space mean ﬁlter extract local featur independ graph size spectrum chebnet map linearli cayleynet appli cayley polynomi parametr ration complex function captur narrow frequenc band spectral graph convolut cayleynet deﬁn x gθ r c j hl j hl ii return real part complex number real coefﬁcent c j complex coefﬁcent imaginari number h paramet control spectrum cayley ﬁlter preserv spatial local cayleynet author licens use limit queen mari univers london download march utc ieee xplore restrict appli wu et al comprehens survey gnn show chebnet consid special case cayleynet graph convolut network gcn introduc order approxim chebnet assum k λmax simpliﬁ x gθ x restrain number paramet avoid overﬁt gcn assum θ lead follow deﬁnit graph convolut x gθ θ x allow multichannel input output gcn modiﬁ composit layer deﬁn h x f f activ function use empir caus numer instabl gcn address thi problem gcn appli normal trick replac dii j ai method gcn also interpret method perspect gcn consid aggreg featur inform node neighborhood equat express hv f n v av uxu sever recent work made increment improv gcn explor altern symmetr matric tive gcn agcn learn hidden structur relat unspeciﬁ graph adjac matrix construct residu graph adjac matrix learnabl distanc function take two node featur input dual gcn dgcn introduc convolut architectur two graph convolut layer parallel two layer share paramet use ize adjac matrix posit pointwis mutual inform ppmi matrix captur node inform random walk sampl graph ppmi matrix deﬁn max log count count count count count function return frequenc node v node u sampl random walk ensembl output convolut layer dgcn encod local global structur inform without need stack multipl graph convolut layer convgnn analog convolut oper convent cnn imag method deﬁn graph convolut base node spatial relat imag consid special form graph pixel repres node pixel directli connect nearbi pixel shown fig ﬁlter appli patch take weight averag pixel valu central node neighbor across channel similarli graph convolut convolv central node represent neighbor represent deriv updat represent central node shown fig b anoth perspect convgnn share idea inform pass recgnn spatial graph convolut oper essenti propag node inform along edg neural network graph propos parallel gnn ﬁrst work toward convgnn distinct differ recgnn learn graph mutual depend composit neural architectur independ paramet layer neighborhood node extend increment construct architectur perform graph convolut sum node neighborhood inform directli also appli residu connect skip connect memor inform layer result deriv node state h k v f k xv v k h u f activ function h v tion also written matrix form h k f xw k ah k resembl form gcn one differ use unnorm adjac matrix may potenti caus hidden node state extrem differ scale contextu graph markov model cgmm propos probabilist model inspir maintain spatial local cgmm ha beneﬁt bilist interpret diffus cnn dcnn regard graph convolut diffus process assum inform transfer one node one neighbor node certain transit probabl inform distribut reach equilibrium sever round dcnn deﬁn diffus graph convolut dgc h k f w k f activ function probabl sition matrix p comput p note dcnn hidden represent matrix h k remain dimens input featur matrix x function previou hidden represent matrix h dcnn concaten h h h k togeth ﬁnal model output stationari distribut diffus process summat power seri probabl sition matric dgc sum output diffus author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact neural network learn system vol januari step instead concaten deﬁn dgc h k f pkxw k w k f activ function use power transit probabl matrix impli distant neighbor contribut veri littl inform central node increas contribut distant neighbor base shortest path deﬁn adjac matrix j shortest path node v node u length j j v u otherwis hyperparamet r control recept ﬁeld size introduc graph convolut oper follow h k f j j h w j k j ii l j l h x concaten vector calcul adjac matrix expens maximum partit graph convolut pgc partit node neighbor q group base certain criteria ite shortest path pgc construct q adjac matric accord deﬁn neighborhood group pgc appli gcn differ paramet matrix neighbor group sum result h k q j h w j k h x j j j j j j neural network mpnn outlin gener framework convgnn treat graph convolut process inform pass one node anoth along edg directli mpnn run iter let inform propag function name spatial graph convolut deﬁn h k v uk v v mk h v h u xe vu h v xv uk mk function learnabl paramet deriv hidden represent node h k v pass output layer perform predict task readout function perform predict task readout function gener represent entir graph base node hidden represent gener deﬁn hg k v r repres readout function learnabl paramet mpnn cover mani exist gnn ing differ form uk mk r howev graph isomorph network gin ﬁnd previou method incap distinguish differ graph structur base graph fig differ gcn gat gcn explicitli assign nonparametr weight aij deg vi deg v j neighbor v j vi dure aggreg process b gat implicitli captur weight aij via neural network architectur import node receiv larger weight embed produc amend thi drawback gin adjust weight central node learnabl paramet ϵ k perform graph convolut h k v mlp ϵ k h v v h u mlp repres multilay perceptron number neighbor node vari one thousand even inefﬁci take full size node neighborhood graphsag adopt sampl obtain ﬁxed number neighbor node perform graph convolut h k v σ w k fk h v h u v h v xv fk aggreg function sn v random sampl node v neighbor aggreg function invari permut node order mean sum max function graph attent network gat assum bution neighbor node central node neither ident like graphsag predetermin like gcn thi differ illustr fig gat adopt attent mechan learn rel weight two connect node graph convolut oper accord gat deﬁn h k v σ v α k vu w k h u h v xv attent weight α k vu measur connect strength node v neighbor u α k vu k h v k h u g leakyrelu activ function vector learnabl paramet softmax function ensur attent weight sum one bor node gat perform multihead attent increas model express capabl thi show impress improv graphsag node author licens use limit queen mari univers london download march utc ieee xplore restrict appli wu et al comprehens survey gnn classiﬁc task gat assum contribut attent head equal gate attent network gaan introduc mechan comput addit attent score attent head apart appli graph attent spatial geniepath pose gate mechan control inform ﬂow across graph convolut layer graph attent model might interest howev belong convgnn framework mixtur model network monet adopt differ approach assign differ weight node neighbor introduc node pseudocoordin determin rel posit node neighbor onc rel posit two node known weight function map rel posit rel weight two node way paramet graph ﬁlter share across differ locat monet framework sever exist approach manifold geodes cnn gcnn anisotrop cnn acnn spline cnn graph gcn dcnn gener special instanc monet construct nonparametr weight function monet addit propos gaussian kernel learnabl paramet learn weight function adapt anoth distinct line work achiev weight share across differ locat rank node neighbor base certain criteria associ rank learnabl weight order neighbor node accord graph label select top q neighbor graph label essenti node score deriv node degre central lehman wl color node ha ﬁxed number order neighbor data convert data appli standard convolut ﬁlter aggreg neighborhood featur inform order ﬁlter weight correspond order node neighbor rank criterion onli consid graph structur requir heavi comput data process scale gcn lgcn rank node neighbor base node featur inform node lgcn assembl featur matrix consist neighborhood sort thi featur matrix along column ﬁrst q row sort featur matrix taken input data central node improv term train efﬁcienc ing convgnn gcn usual requir save whole graph data intermedi state node memori train algorithm convgnn suffer signiﬁcantli memori overﬂow problem especi graph contain million node save memori graphsag propos algorithm convgnn sampl tree root node recurs expand root node neighborhood k step ﬁxed sampl size sampl tree graphsag comput root node hidden represent hierarch aggreg hidden node represent bottom top fast learn gcn fastgcn sampl ﬁxed number node graph convolut layer instead sampl ﬁxed number neighbor node like graphsag interpret graph convolut integr transform embed function node probabl measur mont carlo approxim varianc reduct techniqu employ facilit train process fastgcn sampl node independ layer connect potenti spars huang et al propos adapt layerwis sampl approach node sampl lower layer condit top one thi method achiev higher accuraci compar fastgcn cost employ much complic sampl scheme anoth work stochast train gcn stogcn reduc recept ﬁeld size graph convolut arbitrarili small scale use histor node represent control variat stogcn achiev compar perform even two neighbor per node howev stogcn still ha save intermedi state node memori consum larg graph sampl subgraph use graph ing algorithm perform graph convolut node within sampl subgraph neighborhood search also restrict within sampl subgraph ble handl larger graph use deeper architectur time less time less memori gcn notabl provid straightforward comparison time complex memori complex exist convgnn train algorithm analyz result base tabl iv tabl iv gcn baselin method conduct train graphsag save memori cost sacriﬁc time efﬁcienc meanwhil time ori complex graphsag grow exponenti increas k time complex highest bottleneck memori remain unsolv howev achiev satisfactori perform veri small time complex remain baselin method sinc doe introduc redund comput method realiz lowest memori complex comparison spectral spatial model ctral model theoret foundat graph signal process design new graph signal ﬁlter leynet one build new convgnn howev spatial model prefer spectral model due efﬁcienc gener ﬂexibl issu first spectral model less efﬁcient spatial model spectral model either need perform eigenvector comput handl whole graph time spatial model scalabl larg graph directli perform convolut graph domain via inform propag comput perform batch node instead whole graph second spectral model reli graph fourier basi gener poorli new graph assum ﬁxed graph ani perturb graph would result chang eigenbasi model hand perform graph convolut local node weight easili share across author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact neural network learn system vol januari tabl iv time memori complex comparison convgnn train algorithm summar n total number node total number edg k number layer batch size r number neighbor sampl node simplic dimens node hidden featur remain constant denot differ locat structur third model limit oper undirect graph model ﬂexibl handl multisourc graph input edg input direct graph sign graph heterogen graph becaus graph input incorpor aggreg function easili graph pool modul gnn gener node featur use ﬁnal task howev use featur directli comput challeng thu downsampl strategi need depend object role play network differ name given thi strategi pool oper aim reduc size meter downsampl node gener smaller represent thu avoid overﬁt permut invari comput complex issu readout oper mainli use gener level represent base node represent mechan veri similar thi section use pool refer kind downsampl strategi appli gnn earlier work graph coarsen algorithm use eigendecomposit coarsen graph base ical structur howev method suffer time complex issu graclu algorithm altern eigendecomposit calcul cluster version origin graph recent work employ pool oper coarsen graph nowaday pool primit effect way implement downsampl sinc calcul valu pool window fast hg max h k h k h k n k index last graph convolut layer henaff et al show perform simpl pool begin network especi import reduc dimension graph domain mitig cost expens graph fourier transform oper furthermor work also use attent mechan enhanc pool even attent mechan reduct oper sum pool satisfactori sinc make embed inefﬁci embed gener regardless graph size vinyal et al propos method gener memori increas size input implement lstm intend integr inform memori embed befor reduct appli would otherwis destroy inform defferrard et al address thi issu anoth way rearrang node graph meaning way devis efﬁcient pool strategi approach chebnet input graph ﬁrst coarsen multipl level graclu algorithm coarsen node input graph coarsen version rearrang balanc binari tree arbitrarili aggreg balanc binari tree bottom top arrang similar node togeth pool rearrang signal much efﬁcient pool origin zhang et al propos dgcnn similar pool strategi name sortpool perform pool rearrang node meaning order differ net dgcnn sort node accord structur role within graph graph unord node featur spatial graph convolut treat continu wl color use sort node addit sort node featur uniﬁ graph size q node featur matrix last n row delet n q otherwis q zero row ad aforement pool method mainli consid graph featur ignor structur inform graph recent differenti pool diffpool propos gener hierarch represent graph compar previou coarsen method diffpool doe simpli cluster node graph learn cluster assign matrix layer k refer k nk number node kth layer probabl valu matrix k gener base node featur topolog structur use k softmax convgnnk k h k core idea thi learn comprehens node ment consid topolog featur inform graph implement ani standard vgnn howev drawback diffpool gener dens graph pool thereaft comput complex becom recent sagpool approach propos consid node featur graph topolog learn pool manner overal pool essenti oper reduc graph size improv effect putat complex pool open question investig author licens use limit queen mari univers london download march utc ieee xplore restrict appli wu et al comprehens survey gnn discuss theoret aspect discuss theoret foundat gnn ent perspect shape recept field recept ﬁeld node set node contribut determin ﬁnal node represent composit multipl spatial graph convolut layer recept ﬁeld node grow one step ahead toward distant neighbor time mich prove ﬁnite number spatial graph convolut layer exist node v recept ﬁeld node v cover node graph result convgnn abl extract global inform stack local graph convolut layer vc dimens vc dimens measur model complex deﬁn largest number point shatter model work analyz vc dimens gnn given number model paramet p number node n scarselli et al deriv vc dimens gnn use sigmoid tangent hyperbol activ use piecewis polynomi activ function thi result suggest model complex gnn increas rapidli p n sigmoid tangent hyperbol activ use graph isomorph two graph isomorph topolog ident given two nonisomorph graph xu et al prove gnn map differ embed two graph identiﬁ nonisomorph wl test isomorph show common gnn gcn graphsag incap distinguish differ graph structur xu et al prove aggreg function readout function gnn inject gnn power wl test distinguish differ graph equivari invari gnn must equivari function perform task must invari function perform task task let f x gnn q ani permut matrix chang order node gnn equivari satisﬁ f qaqt qx q f x task let f x gnn invari satisﬁ f qaqt qx f x order achiev equivari invari compon gnn must invari node order maron et al theoret studi characterist permut invari equivari linear layer graph data univers approxim well known tiperceptron feedforward neural network one hidden layer approxim ani borel measur function univers approxim capabl gnn ha seldom studi hammer et al prove cascad relat approxim function structur output scarselli et al prove recgnn mate ani function preserv unfold equival ani degre precis two node unfold equival unfold tree ident unfold tree node construct iter extend node neighborhood certain depth xu et al show convgnn framework messag pass univers approxim continu function deﬁn multiset maron et al prove invari graph network approxim arbitrari invari function deﬁn graph vi graph autoencod gae deep neural architectur map node latent featur space decod graph inform latent represent gae use learn network embed gener new graph main characterist select gae summar tabl follow provid brief review gae two perspect network embed graph gener network embed network embed vector sentat node preserv node topolog mation gae learn network embed use encod extract network embed use decod enforc network embed preserv graph topolog mation ppmi matrix adjac matrix earlier approach mainli employ multilay perceptron build gae network embed learn deep neural work graph represent dngr use stack denois autoencod encod decod ppmi matrix via multilay perceptron concurr structur deep network embed sdne use stack coder preserv node proxim order proxim jointli sdne propos two loss function output encod output decod separ ﬁrst loss function enabl learn network embed preserv node proxim minim distanc node network embed neighbor network embed ﬁrst loss function deﬁn v u av xv xu xv av enc encod consist multilay perceptron second loss function enabl learn network embed preserv node order proxim minim distanc node input reconstruct input concret second loss function deﬁn dec enc xv bv u av u bv u β av u dec decod consist multilay perceptron dngr sdne onli consid node structur inform connect pair node ignor node may contain featur mation depict attribut node themselv gae leverag gcn encod node structur inform name gae avoid ambigu thi articl author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact neural network learn system vol januari tabl v main characterist select gae node featur inform time encod gae consist two graph convolut layer take form z enc x gconv f gconv x z denot network embed matrix graph f relu activ function gconv function graph convolut layer deﬁn decod gae aim decod node relat inform embed reconstruct graph adjac matrix deﬁn ˆ av u dec zv zu σ zt v zu zv embed node gae train minim neg cross entropi given real adjac matrix reconstruct adjac matrix ˆ simpli reconstruct graph adjac matrix may lead overﬁt due capac autoencod variat gae vgae variat version gae learn distribut data vgae optim variat lower bound l l eq log p l q z k l diverg function measur distanc two distribut p z gaussian prior p z n p zi n n p ai j z j dec zi z j σ zt z j q n q q n diag σ mean vector μi ith row encod output deﬁn log σi deriv similarli μi anoth encod accord vgae assum empir distribut q close possibl prior distribut p z enforc empir distribut q approxim prior distribut p z adversari regular vgae arvga employ train scheme gener adversari network gan gan play tion game gener discrimin train gener model gener tri gener fake ple real possibl discrimin attempt distinguish fake sampl real one inspir gan arvga endeavor learn encod produc empir distribut q indistinguish prior distribut p z similar gae graphsag encod node featur two graph convolut layer instead optim reconstruct error graphsag show relat inform two node preserv neg sampl loss l zv dec zv zu v log zv zvn node u neighbor node v node vn distant node node v sampl neg sampl distribut pn v q number neg sampl thi loss function essenti enforc close node similar sentat distant node dissimilar represent dgi altern drive local network embed captur global structur inform maxim local mutual inform show distinct improv graphsag experiment aforement method essenti learn network embed solv link predict problem howev sparsiti graph caus number posit node pair far less number neg node pair allevi data sparsiti problem learn network embed anoth line work convert graph sequenc random permut random walk way deep learn approach applic sequenc directli use process graph deep sive network embed drne assum node network embed approxim aggreg neighborhood network embed adopt long author licens use limit queen mari univers london download march utc ieee xplore restrict appli wu et al comprehens survey gnn memori lstm network aggreg node neighbor reconstruct error drne deﬁn l v zv network embed node v obtain dictionari lookup lstm network take random sequenc node v neighbor order node degre input suggest drne implicitli learn network embed via lstm network rather use lstm network gener network embed avoid problem lstm network invari permut node sequenc network represent adversari regular autoencod netra propos graph framework gener loss tion deﬁn l z dist z dec enc z dist distanc measur node ding z reconstruct encod decod netra lstm network random walk root node v input similar arvga netra regular learn network embed within prior distribut via adversari train although netra ignor node permut variant problem lstm network experiment result valid effect netra graph gener multipl graph gae abl learn ativ distribut graph encod graph hidden represent decod graph structur given hidden represent major gae graph gener design solv molecular graph gener problem ha high practic valu drug discoveri method either propos new graph sequenti manner global manner sequenti approach gener graph propos node edg step step et al kusner et al dai et al model gener process string represent molecular graph name smile deep cnn rnn encod decod respect method speciﬁc altern solut applic gener graph mean iter ad node edg grow graph certain criterion satisﬁ deep gener model graph deepgmg assum probabl graph sum possibl node permut p g π p g π π denot node order captur complex joint probabl node edg graph deepgmg gener graph make sequenc decis name whether add node node add whether add edg node connect new node decis process gener node edg condit node state graph state grow graph updat recgnn anoth work graphrnn propos rnn rnn model gener process node edg rnn add new node node sequenc time rnn produc binari sequenc indic connect new node node previous gener sequenc global approach output graph onc graph tional autoencod graphva model exist node edg independ random variabl ing posterior distribut qφ deﬁn encod gener distribut pθ deﬁn decod graphva optim variat lower bound l φ θ g eqφ pθ k l qφ z p z follow gaussian prior φ θ learnabl paramet convgnn encod simpl multilay percept decod graphva output gener graph adjac matrix node attribut edg attribut challeng control global properti gener graph graph connect valid node compat regular graphva rgvae impos valid constraint graphva ular output distribut decod molecular gan molgan integr convgnn gan reinforc learn object gener graph desir properti molgan consist gener discrimin compet improv authent gener molgan gener tri propos fake graph along featur matrix discrimin aim distinguish fake sampl empir data addit reward network introduc parallel discrimin encourag gener graph possess certain properti accord extern evalu netgan combin lstm wasserstein gan gener graph approach netgan train gener produc plausibl random walk lstm network enforc discrimin identifi fake random walk real one train new graph deriv normal matrix node comput base random walk produc gener brief sequenti approach linear graph sequenc lose structur inform due presenc cycl global approach produc graph onc scalabl larg graph output space gae vii graph neural network graph mani applic dynam term graph structur graph input stgnn occupi import posit captur dynam graph method thi categori aim model dynam node input assum interdepend connect node exampl trafﬁc network consist speed sensor place road edg weight determin distanc pair sensor trafﬁc condit author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact neural network learn system vol januari one road may depend adjac road condit necessari consid spatial depend perform trafﬁc speed forecast solut stgnn captur spatial tempor depend graph simultan task stgnn forecast futur node valu label predict graph label stgnn follow two direct method method approach captur depend ﬁltere input hidden state pass recurr unit use graph convolut illustr thi suppos simpl rnn take form h σ wx uh b x node featur matrix time step insert graph convolut becom h σ gconv x w gconv h u b gconv graph convolut layer graph lution recurr network gcrn combin lstm network chebnet diffus convolut rnn dcrnn incorpor propos diffus graph volut layer see gru network addit dcrnn adopt framework predict futur k step node valu anoth parallel work use rnn rnn handl differ aspect tempor inform propos recurr framework predict node label time step compris two kind rnn name tempor inform node edg pass respect incorpor spatial inform take output rnn input sinc assum differ rnn differ node edg signiﬁcantli increas model complex instead split node edg semant group node edg semant group share rnn model save comput cost approach suffer ativ propag gradient issu altern solut approach tackl graph nonrecurs manner advantag parallel comput stabl gradient memori requir illustr fig approach interleav layer graph lution layer learn tempor spatial depend respect assum input stgnn tensor x layer slide x along time axi aggreg tempor inform node graph convolut layer oper x aggreg spatial inform time step cgcn integr convolut layer chebnet gcn layer construct block stack gate convolut layer graph convolut layer anoth gate convolut layer sequenti order compos block use convolut layer pgc layer see previou method use predeﬁn graph structur assum predeﬁn graph structur reﬂect genuin depend relationship among node howev mani snapshot graph data set possibl learn latent static graph structur automat data realiz thi graph wavenet propos adjac matrix perform graph convolut adjac matrix deﬁn aadp softmax relu softmax function comput along row dimens denot sourc node embed denot target node embed learnabl paramet multipli one get depend weight sourc node target node complex neural network graph wavenet perform well without given adjac matrix learn latent static spatial depend help research discov interpret stabl correl among differ entiti network howev circumst learn latent dynam spatial depend may improv model precis exampl trafﬁc network travel time two road may depend current trafﬁc condit gaan employ attent mechan learn dynam spatial depend approach attent function use updat edg weight two connect node given current node input astgcn includ spatial attent function tempor attent function learn latent dynam spatial depend tempor depend approach common drawback learn latent spatial depend need calcul spatial depend weight pair node cost viii applic data ubiquit gnn wide varieti applic thi section summar benchmark graph data set evalu method sourc implement respect detail practic cation gnn variou domain data set mainli sort data set four group name citat network biochem graph social network tabl vi summar select benchmark data set detail given supplementari materi evalu implement node classiﬁc graph classiﬁc common task assess perform recgnn convgnn node classiﬁc node classiﬁc od follow standard split benchmark data set includ cora cites pubm ppi reddit report averag accuraci score test data set multipl run summar tal result method found supplementari author licens use limit queen mari univers london download march utc ieee xplore restrict appli wu et al comprehens survey gnn tabl vi summari select benchmark data set materi note result essarili repres rigor comparison shchur et al identiﬁ two pitfal evalu perform gnn node classiﬁc first use split throughout experi underestim gener error second differ method employ differ train techniqu hyperparamet tune paramet izat learn rate decay earli stop rel fair comparison refer reader graph classiﬁc graph classiﬁc research often adopt tenfold cross valid cv model evalu howev point experiment set ambigu uniﬁ across differ work ticular rais concern correct usag data split model select versu model assess often encount problem extern test set fold use model select risk assess refer compar gnn standard uniform tion framework appli extern tenfold cv get estim gener perform model inner holdout techniqu split model select altern procedur would method use extern cv model assess inner cv model select refer reader detail rigor comparison gnn method graph classiﬁc implement facilit work baselin experi deep learn research supplementari materi c provid hyperlink implement gnn model review thi articl notic fey et al publish ric learn librari pytorch name pytorch implement mani gnn recent deep graph librari dgl releas provid fast mentat mani gnn top popular deep learn platform pytorch mxnet practic applic gnn mani applic across differ task domain despit gener task handl categori gnn directli includ node classiﬁc graph classiﬁc network embed graph gener graph forecast gener relat task node cluster link predict graph partit also address gnn detail applic base follow research domain comput vision applic gnn comput vision includ scene graph gener point cloud ﬁcation action recognit recogn semant relationship object tate understand mean behind visual scene scene graph gener model aim pars imag semant graph consist object semant relationship anoth applic invers process gener realist imag given scene graph natur languag pars semant graph word repres object promis solut synthes imag given textual descript classifi segment point cloud enabl lidar devic see surround environ point cloud set point record lidar scan enc convert point cloud neighbor graph superpoint graph use convgnn explor topolog structur identifi human action contain video facilit better understand video content machin aspect author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact neural network learn system vol januari solut detect locat human joint video clip human joint link skeleton natur form graph given time seri human joint locat appli stgnn learn human action pattern moreov number applic direct gnn comput vision still grow includ interact imag classiﬁc semant segment visual reason question answer natur languag process common applic gnn natur languag process text classiﬁc gnn util interrel document word infer document label despit fact natur languag data exhibit tial order may also contain intern graph structur syntact depend tree syntact depend tree deﬁn syntact relat among word sentenc marcheggiani titov propos syntact gcn run top sentenc encod syntact gcn aggreg hidden word represent base syntact depend tree sentenc bast et al appli syntact gcn task neural machin translat marcheggiani et al adopt model handl semant depend graph sentenc learn learn gener sentenc mean given semant graph abstract word known abstract mean represent song et al propos encod level semant inform beck et al appli ggnn learn neural machin lation invers task learn ate semant knowledg graph given sentenc veri use knowledg discoveri trafﬁc accur forecast trafﬁc speed volum densiti road trafﬁc network fundament import smart transport system zhang et al li et al yu et al address trafﬁc predict problem use stgnn consid trafﬁc network graph node sensor instal road edg measur distanc pair node node ha averag trafﬁc speed within window dynam input featur anoth applic predict given histor taxi demand locat inform weather data event featur yao et al incorpor lstm cnn network embed train line form joint represent locat predict number taxi demand locat within time interv recommend system recommend tem take item user node leverag relat item item user user user item well content inform recommend system abl produc recommend key recommend system score import item user result cast link predict problem predict miss link user item berg et al ying et al propos gae use convgnn encod monti et al combin rnn graph convolut learn underli process gener known rate chemistri ﬁeld chemistri research appli gnn studi graph structur graph atom consid node chemic bond treat edg node classiﬁc graph classiﬁc graph gener three main task target graph order learn molecular ﬁngerprint predict molecular properti infer protein interfac synthes chemic compound applic gnn limit aforement domain task explor appli gnn varieti problem program veriﬁc program reason social inﬂuenc predict adversari attack tion electr health record model brain network event detect combinatori optim ix futur direct though gnn proven power learn graph data challeng still exist due complex graph thi section suggest four futur direct gnn model depth success deep learn lie deep neural ture howev li et al show perform convgnn drop dramat increas number graph convolut layer graph convolut push represent adjac node closer theori inﬁnit number graph convolut layer node represent converg singl point thi rais question whether go deep still good strategi learn graph data scalabl tradeoff scalabl gnn gain price corrupt graph complet whether use sampl cluster model lose part graph inform pling node may miss inﬂuenti neighbor ing graph may depriv distinct structur pattern tradeoff algorithm scalabl graph integr could futur research direct heterogen major current gnn assum homogen graph difﬁcult directli appli current gnn heterogen graph may contain differ type node edg differ form node edg input imag text therefor new method develop handl heterogen graph dynam graph natur dynam way node edg may appear disappear input may chang author licens use limit queen mari univers london download march utc ieee xplore restrict appli wu et al comprehens survey gnn time time new graph convolut need adapt dynam graph although dynam graph partli address stgnn consid perform graph convolut case dynam spatial relat conclus thi articl conduct comprehens overview gnn provid taxonomi group gnn four categori recgnn convgnn gae stgnn provid thorough review comparison tion method within categori introduc wide rang applic gnn data set code model assess gnn mariz final suggest four futur direct gnn refer redmon divvala girshick farhadi onli look onc uniﬁ object detect proc ieee conf comput vi pattern recognit cvpr jun pp ren girshick sun faster toward time object detect region propos network proc nip pp luong pham man effect approach neural machin translat proc conf empir method natur lang pp wu et googl neural machin translat system bridg gap human machin translat onlin avail http hinton et deep neural network acoust model speech recognit share view four research group ieee signal process vol pp lecun bengio convolut network imag speech time seri handbook brain theori neural network vol cambridg usa mit press hochreit schmidhub long memori neural vol pp vincent larochel lajoi bengio manzagol stack denois autoencod learn use represent deep network local denois criterion mach learn vol pp bronstein bruna lecun szlam van der gheynst geometr deep learn go beyond euclidean data ieee signal process vol pp jul hamilton ying leskovec represent ing graph method applic proc nip pp battaglia et relat induct bias deep learn graph network onlin avail http boaz lee rossi kim ahm koh attent model graph survey onlin avail http sperduti starita supervis neural network classiﬁc structur ieee tran neural vol pp may gori monfardini scarselli new model learn graph domain proc ieee int joint conf neural vol pp scarselli gori tsoi hagenbuchn monfardini graph neural network model ieee tran neural vol pp gallicchio mich graph echo state network proc int joint conf neural netw ijcnn jul pp li tarlow brockschmidt zemel gate graph sequenc neural network proc iclr pp dai kozareva dai smola song learn iter algorithm graph proc icml pp bruna zaremba szlam lecun spectral network local connect network graph proc iclr pp henaff bruna lecun deep convolut network data onlin avail http defferrard bresson van der gheynst convolut neural network graph fast local spectral ﬁltere proc nip pp kipf well classiﬁc graph convolut network proc iclr pp levi monti bresson bronstein cayleynet graph convolut neural network complex ration spectral ﬁlter ieee tran signal vol pp mich neural network graph contextu construct approach ieee tran neural vol pp mar atwood towsley neural network proc nip pp niepert ahm kutzkov learn convolut neural network graph proc icml pp gilmer schoenholz riley vinyal dahl neural messag pass quantum chemistri proc icml pp cui wang pei zhu survey network embed ieee tran knowl data vol pp may zhang yin zhu zhang network represent learn survey ieee tran big data vol pp mar cai zheng chang comprehens survey graph embed problem techniqu applic ieee tran knowl data vol pp goyal ferrara graph embed techniqu applic perform survey vol pp jul pan wu zhu zhang wang deep network represent proc ijcai pp shen pan liu ong sun discret network embed proc int joint conf artif jul pp yang pan zhang chen lian zhang binar attribut network embed proc ieee int conf data mine icdm pp perozzi skiena deepwalk onlin learn social represent proc acm sigkdd int conf knowl discoveri data mine kdd pp vishwanathan schraudolph kondor borgwardt graph kernel mach learn vol pp mar shervashidz schweitzer van leeuwen mehlhorn borgwardt graph kernel mach learn vol pp navarin sperduti approxim neighbour minhash graph node kernel proc esann pp krieg johansson morri survey graph kernel onlin avail http li wang zhu huang adapt graph convolut neural network proc aaai pp zhuang dual graph convolut network base classiﬁc proc world wide web conf world wide web www pp hamilton ying leskovec induct represent learn larg graph proc nip pp velickov cucurul casanova romero lio bengio graph attent network proc iclr pp monti boscaini masci rodola svoboda bronstein geometr deep learn graph manifold use mixtur model cnn proc ieee conf comput vi pattern recognit cvpr jul pp gao wang ji learnabl graph convolut network proc acm sigkdd int conf knowl discoveri data mine pp tran navarin sperduti ﬁlter size graph convolut network proc ieee symp ser comput intel ssci pp author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact neural network learn system vol januari bacciu errica mich contextu graph markov model deep gener approach graph process proc icml pp zhang shi xie king yeung gaan gate attent network learn larg spatiotempor graph proc uai pp chen xiao fastgcn fast learn graph convolut network via import sampl proc iclr pp chen zhu song stochast train graph volut network varianc reduct proc icml pp huang zhang rong huang adapt sampl toward fast graph represent learn proc neurip pp zhang cui neumann chen deep learn architectur graph classiﬁc proc aaai pp li han wu deeper insight graph convolut network learn proc aaai pp ying morri ren hamilton leskovec hierarch graph represent learn differenti ing proc neurip pp liu et geniepath graph neural network adapt tive path proc aaai conf artif jul pp veliˇ c fedu hamilton liò bengio hjelm deep graph infomax proc iclr pp xu hu leskovec jegelka power graph neural network proc iclr pp chiang liu si li bengio hsieh efﬁcient algorithm train deep larg graph convolut network proc acm sigkdd int conf knowl discoveri data mine kdd pp cao lu xu deep neural network learn graph represent proc aaai pp wang cui zhu structur deep network embed proc acm sigkdd int conf knowl discoveri data mine kdd pp kipf well variat graph proc nip workshop bayesian deep pp pan hu long jiang yao zhang adversari regular graph autoencod graph embed proc ijcai jul pp tu cui wang yu zhu deep recurs network embed regular equival proc acm sigkdd int conf knowl discoveri data mine pp yu et learn deep network represent adversari regular autoencod proc acm sigkdd int conf knowl discoveri data mine pp li vinyal dyer pascanu battaglia learn deep gener model graph proc icml pp ying ren hamilton leskovec graphrnn deep gener model graph proc icml pp simonovski komodaki graphva toward gener small graph use variat autoencod proc icann cham switzerland springer pp chen xiao constrain gener calli valid graph via regular variat autoencod proc neurip pp de cao kipf molgan implicit gener model small molecular graph icml workshop theor found appl deep gener model pp bojchevski shchur zügner günnemann netgan gener graph via random walk proc icml pp seo defferrard vandergheynst bresson structur sequenc model graph convolut recurr network proc neurip springer pp li yu shahabi liu diffus convolut recurr neural network trafﬁc forecast proc iclr pp jain zamir savares saxena deep learn graph proc ieee conf put vi pattern recognit cvpr jun pp yu yin zhu graph convolut network deep learn framework trafﬁc forecast proc ijcai jul pp yan xiong lin spatial tempor graph convolut network action recognit proc aaai pp wu pan long jiang zhang graph wavenet deep graph model proc ijcai pp guo lin feng song wan attent base tempor graph convolut network trafﬁc ﬂow forecast proc aaai pp pan wu zhu zhang yu joint structur featur explor regular graph classiﬁc ieee tran knowl data vol pp mar pan wu zhu long zhang task sensit featur explor learn multitask graph classiﬁc ieee tran vol pp mar mich sona sperduti contextu process structur data recurs cascad correl ieee tran neural vol pp cho et learn phrase represent use rnn decod statist machin translat proc conf empir method natur lang process emnlp pp shuman narang frossard ortega van der gheynst emerg ﬁeld signal process graph extend data analysi network irregular domain ieee signal process vol pp may sandryhaila moura discret signal process graph ieee tran signal vol pp apr chen varma sandryhaila kovacev discret signal process graph sampl theori ieee tran signal vol pp duvenaud et convolut network graph learn molecular ﬁngerprint proc nip pp kearn mccloskey berndl pand riley lar graph convolut move beyond ﬁngerprint mol vol pp schütt arbabzadah chmiela müller tkatchenko insight deep tensor neural network natur vol lee rossi kong graph classiﬁc use structur attent proc acm sigkdd int conf knowl discoveri data mine pp perozzi alemi watch step learn node embed via graph attent proc neurip pp masci boscaini bronstein vandergheynst desic convolut neural network riemannian manifold proc ieee int conf comput vi workshop iccvw pp boscaini masci rodolà bronstein learn shape correspond anisotrop convolut neural network proc nip pp fey lenssen weichert müller splinecnn fast geometr deep learn continu kernel proc conf comput vi pattern jun pp weisfeil lehman reduct graph ical form algebra aris dure thi reduct technicheskaya informatsia vol pp dougla method graph morphism test onlin avail http pham tran phung venkatesh column network collect classiﬁc proc aaai pp simonovski komodaki dynam ﬁlter convolut neural network graph proc ieee conf comput vi pattern recognit cvpr jul pp derr tang sign graph convolut network proc ieee int conf data mine icdm pp et robust spatial ﬁltere graph convolut neural network ieee sel topic signal vol pp wang et heterogen graph attent network proc world wide web conf www pp author licens use limit queen mari univers london download march utc ieee xplore restrict appli wu et al comprehens survey gnn dhillon guan kuli weight graph cut without eigenvector multilevel approach ieee tran pattern anal mach vol pp vinyal bengio kudlur order matter sequenc sequenc set proc iclr pp lee lee kang graph pool proc icml pp scarselli tsoi hagenbuchn chervonenki dimens graph recurs neural network neural vol pp maron shamir lipman invari equivari graph network iclr pp hornik stinchcomb white multilay feedforward network univers approxim neural vol pp hammer mich sperduti univers approxim capabl cascad correl structur neural vol pp may scarselli gori chung tsoi hagenbuchn monfardini comput capabl graph neural network ieee tran neural vol pp vincent larochel bengio manzagol extract compos robust featur denois autoencod proc int conf mach learn icml pp pan hu fung long jiang zhang learn graph embed adversari train method ieee tran earli access doi goodfellow et gener adversari net proc nip pp et automat chemic design use continu represent molecul ac central vol pp kusner paig grammar variat autoencod proc icml pp dai tian dai skiena song variat autoencod structur data proc iclr pp schlichtkrul kipf bloem van den berg titov well model relat data graph convolut network eswc cham switzerland springer pp gulrajani ahm arjovski dumoulin courvil improv train wasserstein gan proc nip pp arjovski chintala bottou wasserstein gan onlin avail http sen namata bilgic getoor galligh collect classiﬁc network data ai vol tang zhang yao li zhang su arnetmin extract mine academ social network proc kdd pp zitnik leskovec predict multicellular function tissu network bioinformat vol pp jul wale watson karypi comparison descriptor space chemic compound retriev classiﬁc knowl inf vol pp debnath de compadr debnath shusterman hansch relationship mutagen matic heteroaromat nitro compound correl molecular orbit energi hydrophob medicin vol pp dobson doig distinguish enzym structur without align mol vol pp jul borgwardt ong schönauer vishwanathan smola kriegel protein function predict via graph kernel bioinformat vol pp jun toivonen srinivasan king kramer helma statist evalu predict toxicolog challeng bioinformat vol pp jul ramakrishnan dral rupp von lilienfeld quantum chemistri structur properti kilo molecul sci data vol art chen et alchemi quantum chemistri dataset mark ai model onlin avail http tang liu relat learn via latent social dimens proc acm sigkdd int conf knowl discoveri data mine kdd pp lecun bottou bengio haffner learn appli document recognit proc ieee vol pp jagadish et big data technic challeng commun acm vol pp jul carlson betteridg kisiel settl hruschka mitchel toward architectur languag learn proc aaai pp shchur mumm bojchevski günnemann pitfal graph neural network evalu proc neurip workshop pp errica podda bacciu mich fair parison graph neural network graph classiﬁc proc iclr pp onlin avail http wang et deep graph librari toward efﬁcient scalabl deep learn graph proc iclr workshop repres learn graph manifold pp wang pan long zhu jiang mgae ize graph autoencod graph cluster proc cikm pp zhang chen link predict base graph neural network proc neurip pp kawamoto tsubaki obuchi theori graph neural network graph partit proc neurip pp xu zhu choy scene graph gener iter messag pass proc ieee conf comput vi pattern recognit cvpr jul pp yang lu lee batra parikh graph scene graph gener proc eccv springer pp li ouyang zhou shi zhang wang toriz net efﬁcient framework scene graph gener proc eccv springer pp johnson gupta imag gener scene graph proc conf comput vi pattern jun pp wang sun liu sarma bronstein solomon dynam graph cnn learn point cloud acm tran vol pp landrieu simonovski point cloud semant segment superpoint graph proc conf put vi pattern jun pp te hu zheng guo rgcnn regular graph cnn point cloud segment proc acm multimedia conf multimedia conf mm pp qi wang jia shen zhu learn object interact graph pars neural network proc eccv springer pp satorra estrach learn graph neural network proc iclr pp guo chou huang song yeung neural graph match network fewshot action recognit proc eccv springer pp liu zhou long jiang yao zhang totyp propag network ppn learn categori graph proc int joint conf artif pp qi liao jia fidler urtasun graph neural work rgbd semant segment proc cvpr pp yi su guo guiba syncspeccnn synchron spectral cnn shape segment proc ieee conf comput vi pattern recognit cvpr jul pp chen li gupta iter visual reason beyond convolut proc conf comput vi pattern jun pp narasimhan lazebnik schwing box reason graph convolut net factual visual question answer proc neurip pp marcheggiani titov encod sentenc graph lution network semant role label proc conf empir method natur lang pp author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact neural network learn system vol januari bast titov aziz marcheggiani simaan graph convolut encod neural machin translat proc conf empir method natur lang pp marcheggiani bast titov exploit semant neural machin translat graph convolut network proc naacl pp song zhang wang gildea model gener proc acl pp beck haffari cohn learn use gate graph neural network proc acl pp johnson learn graphic state transit proc iclr mar pp chen sun han tic graph gener semant pars proc acl pp yao et deep network taxi demand predict proc aaai pp tang qu wang zhang yan mei line inform network embed proc www pp van den berg kipf well graph tional matrix complet onlin abl http ying chen eksombatchai hamilton leskovec graph convolut neural network recommend system proc acm sigkdd int conf knowl discoveri data mine pp monti bronstein bresson geometr matrix complet recurr neural network proc nip pp fout byrd shariat protein interfac predict use graph convolut network proc nip pp liu ying pand leskovec graph tional polici network molecular graph gener proc neurip pp allamani brockschmidt khademi learn repres program graph proc iclr pp qiu tang dong wang tang deepinf social inﬂuenc predict deep learn proc kdd pp zügner akbarnejad günnemann adversari attack neural network graph data proc acm sigkdd int conf knowl discoveri data mine kdd pp choi bahadori song stewart sun gram attent model healthcar represent learn proc acm sigkdd int conf knowl discoveri data mine kdd pp choi xiao stewart sun mime multilevel medic embed electron health record predict healthcar proc neurip pp kawahara et brainnetcnn convolut neural network brain network toward predict neurodevelop neuroimag vol pp nguyen grishman graph convolut network pool event detect proc aaai pp li chen koltun combinatori optim graph convolut network guid tree search proc neurip pp zhang ren sun deep residu learn imag recognit proc ieee conf comput vi pattern recognit cvpr jun pp zonghan wu receiv degre system scienc univers shanghai scienc technolog shanghai china degre statist linköp univers linköp sweden current ing degre comput scienc univers technolog sydney ut ultimo nsw australia hi research concentr data mine machin learn deep learn graph shirui pan member ieee receiv degre comput scienc siti technolog sydney ut ultimo nsw australia wa lectur school softwar ut current lectur ulti inform technolog monash univers clayton vic australia ha publish research articl journal ferenc includ ieee transact neural network learn system tnnl ieee transact knowledg data neer tkde ieee transact cybernet tcyb ieee intern confer data engin icd aaai confer artiﬁci intellig aaai intern joint enc artiﬁci intellig ijcai ieee intern enc data mine icdm hi research interest includ data mine machin learn fengwen chen receiv degre puter scienc softwar engin arizona state univers temp az usa current pursu degre comput scienc univers technolog sydney ut ultimo nsw australia hi research concentr data mine deep learn graph guodong long wa born china receiv degre comput scienc univers technolog sydney ultimo nsw australia current senior lectur core ber centr artiﬁci intellig cai faculti engin inform ogi univers technolog sydney hi research focus machin learn data mine cloud comput chengqi zhang senior member ieee receiv degre univers land brisban qld australia higher doctor degre deakin versiti geelong vic australia sinc decemb ha professor inform technolog univers technolog sydney ut ultimo nsw australia wa director ut prioriti invest research centr quantum tation intellig system hi research interest mainli focu data mine applic zhang fellow australian comput societi wa gener acm sigkdd confer knowledg discoveri data mine kdd sydney local arrang chair intern joint confer artiﬁci intellig ijcai melbourn philip yu life fellow ieee receiv degre electr engin stanford univers stanford ca usa current distinguish professor puter scienc univers illinoi chicago chicago il usa also wexler chair inform technolog ha publish articl refere journal confer hold ha appli patent hi research interest includ big data data mine data stream databas privaci yu fellow acm receiv acm sigkdd vation award research contribut award ieee intern confer data mine technic achiev award ieee comput societi author licens use limit queen mari univers london download march utc ieee xplore restrict appli