4 ieee transaction neural network learning system vol 32 no 1 january 2021 comprehensive survey graph neural network zonghan wu shirui pan member ieee fengwen chen guodong long chengqi zhang senior member ieee philip yu life fellow ieee learning ha revolutionized many machine learning task recent year ranging image classiﬁcation video processing speech recognition natural language understanding data task typically represented euclidean space however increasing number application data generated domain represented graph complex ship interdependency object complexity graph data ha imposed signiﬁcant challenge existing machine learning algorithm recently many study ing deep learning approach graph data emerged article provide comprehensive overview graph neural network gnns data mining machine learning ﬁelds propose new taxonomy divide gnns four category namely recurrent gnns tional gnns graph autoencoders gnns discus application gnns across various domain summarize code benchmark data set model evaluation gnns finally propose potential research direction rapidly growing ﬁeld index learning graph autoencoder gae graph convolutional network gcns graph neural network gnns graph representation learning network embedding introduction recent success neural network ha boosted research pattern recognition data mining many machine learning task object detection 1 2 machine translation 3 4 speech recognition 5 heavily relied handcrafted feature engineering extract informative feature set recently tionized various deep learning paradigm convolutional neural network cnns 6 recurrent neural manuscript received january 2 2019 revised august 7 2019 december 3 2019 accepted march 2 date publication march 24 2020 date current version january 5 work wa supported part australian government australian research council arc grant grant part nsf grant grant grant grant corresponding author shirui pan zonghan wu fengwen chen guodong long chengqi zhang centre artiﬁcial intelligence faculty engineering information technology university technology sydney ultimo nsw 2007 australia shirui pan faculty information technology monash sity clayton vic 3800 australia philip yu department computer science university illinois chicago chicago il usa psyu article ha supplementary downloadable material available provided author color version one ﬁgures article available online digital object identiﬁer network rnns 7 autoencoders 8 success deep learning many domain partially attributed rapidly developing computational resource gpu availability big training data effectiveness deep learning extract latent representation euclidean data image text video taking image data example represent image regular grid euclidean space cnn able exploit invariance local connectivity compositionality image data 9 result cnns extract local meaningful feature shared entire data set various image analysis deep learning effectively capture hidden pattern euclidean data increasing number tions data represented form graph example learning system exploit interaction user product make highly accurate recommendation chemistry molecule modeled graph bioactivity need identiﬁed drug discovery citation network article linked via citationships need categorized different group complexity graph data ha imposed signiﬁcant challenge existing machine learning algorithm graph irregular graph may variable size unordered node node graph may different number neighbor resulting some important operation convolution easy compute image domain difﬁcult apply graph domain furthermore core assumption existing machine learning algorithm instance independent assumption no longer hold graph data instance node related others link various type citation friendship interaction recently increasing interest extending deep learning approach graph data motivated cnns rnns autoencoders deep learning new tions deﬁnitions important operation rapidly developed past year handle complexity graph data example graph convolution generalized convolution illustrated fig 1 image considered special case graph pixel connected adjacent pixel similar convolution one may perform graph convolution taking weighted average node neighborhood information limited number existing review topic graph neural network gnns using term geometric deep learning bronstein et al 9 give overview 2020 ieee personal use permitted requires ieee permission see information authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply wu et al comprehensive survey gnns 5 fig 1 convolution versus graph convolution convolution analogous graph pixel image taken node neighbor determined ﬁlter size convolution take weighted average pixel value red node along neighbor neighbor node ordered ﬁxed size b graph convolution get hidden representation red node one simple solution graph convolutional operation take average value node feature red node along neighbor different image data neighbor node unordered variable size deep learning method domain ing graph manifold although ﬁrst review gnns article mainly review convolutional gnns hamilton et al 10 cover limited number gnns focus addressing problem network embedding battaglia et al 11 position graph network building block learning relational data reviewing part gnns uniﬁed framework lee et al 12 conduct partial survey gnns apply different attention anisms summary existing survey only include some gnns examine limited number work thereby missing recent development gnns article provides comprehensive overview gnns ested researcher want enter rapidly developing ﬁeld expert would like compare gnn model cover broader range method article considers gnns deep learning approach graph data contribution article make notable contribution summarized follows 1 new taxonomy propose new taxonomy gnns gnns categorized four group recurrent gnns recgnn convolutional gnns convgnns graph autoencoders gaes gnns stgnns 2 comprehensive review provide hensive overview modern deep learning technique graph data type gnns provide detailed description representative model make necessary comparison summarize corresponding algorithm 3 abundant resource collect abundant resource gnns including model benchmark data set code practical application article used guide standing using developing different deep learning approach various application 4 future direction discus theoretical aspect gnns analyze limitation existing method suggest four possible future research direction term model depth scalability tradeoff heterogeneity dynamicity organization article rest article organized follows section ii outline background gnns list commonly used tions deﬁnes concept section iii clariﬁes categorization gnns section provides overview gnn model section viii present collection application across various domain section ix discus current challenge suggests future direction section x summarizes article ii background definition section outline background gnns list commonly used notation deﬁne concept background 1 brief history graph neural network sperduti starita 13 ﬁrst applied neural network directed acyclic graph motivated early study gnns notion gnns wa initially outlined 14 elaborated 15 16 early study fall category recgnns learn target node representation propagating neighbor information iterative manner stable ﬁxed point reached process computationally expensive recently increasing effort overcome challenge 17 18 encouraged success cnns computer vision domain large number method redeﬁne notion convolution graph data developed parallel approach umbrella vgnns convgnns divided two main stream approach approach ﬁrst prominent research convgnns wa presented bruna et al 19 developed graph convolution based spectral graph theory since increasing improvement extension approximation convgnns 20 23 research convgnns started much earlier convgnns 2009 micheli 24 ﬁrst addressed graph mutual dependence architecturally posite nonrecursive layer inheriting idea message passing recgnns however importance article wa overlooked recently many convgnns 25 27 emerged apart recgnns convgnns many alternative gnns oped past year including gaes stgnns learning framework built recgnns vgnns neural architecture graph modeling detail categorization method given section iii 2 graph neural network versus network embedding research gnns closely related graph embedding network embedding another topic attracts increasing attention data mining machine learning community 10 28 32 network embedding aim resenting network node vector tations preserving network topology structure node authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 6 ieee transaction neural network learning system vol 32 no 1 january 2021 content information any subsequent graph analytics task classiﬁcation clustering recommendation easily performed using simple machine learning algorithm support vector machine cation meanwhile gnns deep learning model aiming addressing task manner many gnns explicitly extract representation main distinction gnns network embedding gnns group neural network model designed various task network embedding cover various kind method targeting task therefore gnns address network embedding problem gae framework hand network embedding contains nondeep learning method matrix factorization 33 34 random walk 35 3 graph neural network versus graph kernel method graph kernel historically dominant technique solve problem graph classiﬁcation 36 38 method employ kernel function measure similarity pair graph algorithm support vector machine used supervised learning graph similar gnns graph kernel embed graph node vector space mapping function difference mapping function deterministic rather able due pairwise similarity calculation graph kernel method suffer signiﬁcantly computational bottleneck gnns one hand directly perform graph classiﬁcation based extracted graph representation therefore much efﬁcient graph kernel method review graph kernel method refer reader 39 deﬁnition throughout article use bold uppercase character denote matrix bold lowercase character denote vector unless particularly speciﬁed notation used article illustrated table deﬁne minimal set deﬁnitions required understand article deﬁnition 1 graph graph represented g v e v set vertex node use node throughout article e set edge let vi denote node ei j vi v j denote edge pointing v j vi neighborhood node v deﬁned n v u v u adjacency matrix n n matrix ai j 1 ei j ai j 0 ei j graph may node attribute x node feature matrix xv representing feature vector node meanwhile graph may edge attribute xe xe edge feature matrix xe v u representing feature vector edge v u deﬁnition 2 directed graph directed graph graph edge directed one node another rected graph considered special case directed graph pair edge inverse direction two node connected graph undirected only adjacency matrix symmetric graph referred attributed graph literature table commonly used notation deﬁnition 3 graph oral graph attributed graph node attribute change dynamically time graph deﬁned g v e x x iii categorization framework section present taxonomy gnns shown table ii categorize gnns recgnns convgnns gaes stgnns fig 2 show example various model architecture following give brief duction category taxonomy graph neural network 1 recurrent graph neural network mostly pioneer work gnns recgnns aim learn node tations recurrent neural architecture assume node graph constantly exchange neighbor stable equilibrium reached recgnns conceptually important inspired later research convgnns particular idea message passing inherited convgnns 2 convolutional graph neural network generalize operation convolution grid data graph data main idea generate node v representation aggregating feature xv neighbor feature xu u v different recgnns convgnns stack multiple graph convolutional layer extract node representation convgnns play central role building many complex gnn model fig 2 show authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply wu et al comprehensive survey gnns 7 table ii taxonomy representative publication gnns convgnn node classiﬁcation fig 2 b demonstrates convgnn graph classiﬁcation 3 graph autoencoders unsupervised learning framework encode latent vector space reconstruct graph data encoded information gaes used learn network embeddings graph ative distribution network embedding gaes learn latent node representation reconstructing graph structural information graph adjacency matrix graph generation some method generate node edge graph step step method output graph fig 2 c present gae network embedding 4 graph neural network aim learn hidden pattern graph becomes increasingly important variety application trafﬁc speed forecasting 72 driver maneuver pation 73 human action recognition 75 key idea stgnns consider spatial dependence temporal dependence time many current approach grate graph convolution capture spatial dependence rnns cnns model temporal dependence fig 2 illustrates stgnn graph forecasting framework graph structure node content information input output gnns focus different graph analytics task one following mechanism 1 node level output relate node regression node classiﬁcation task recgnns convgnns extract node representation information convolution multiperceptron softmax layer output layer gnns able perform task manner 2 edge level output relate edge classiﬁcation link prediction task two node hidden sentations gnns input similarity function neural network utilized predict strength edge 3 graph level output relate graph classiﬁcation task obtain compact representation graph level gnns often combined pooling operation detailed information pooling readout reviewed section training framework many gnns convgnns trained semi supervised purely unsupervised way within learning framework depending learning task label information available hand 1 semisupervised learning classiﬁcation given single network partial node labeled others remaining unlabeled convgnns learn robust model effectively identiﬁes class label unlabeled node 22 end end framework built stacking couple graph convolutional layer followed softmax layer multiclass classiﬁcation 2 supervised learning classiﬁcation classiﬁcation aim predict class label entire graph 52 54 78 79 learning task realized combination graph convolutional layer graph pooling layer readout layer graph volutional layer responsible exacting node representation graph pooling layer play role downsampling coarsens graph substructure time readout layer collapse node representation graph graph tion applying multilayer perceptron softmax layer graph representation build end framework graph classiﬁcation example given fig 2 b 3 unsupervised learning graph embedding no class label available graph learn graph embedding purely unsupervised way framework algorithm exploit level information two way one simple way adopt autoencoder framework encoder employ graph convolutional layer embed graph latent representation upon decoder used reconstruct graph structure 61 62 another popular way utilize negative sampling approach sample portion node pair negative pair existing node pair link graph positive pair logistic regression layer applied distinguish positive negative pair 42 table iii summarize main characteristic representative recgnns convgnns input source ing layer readout layer time complexity compared among various model detail only compare time complexity convolutional authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 8 ieee transaction neural network learning system vol 32 no 1 january 2021 fig different gnn model built graph convolutional layer term gconv denotes graph convolutional layer term mlp denotes multilayer perceptron term cnn denotes standard convolutional layer convgnn multiple graph convolutional layer graph tional layer encapsulates node hidden representation aggregating feature information neighbor feature aggregation nonlinear transformation applied resulted output stacking multiple layer ﬁnal hidden representation node receives message neighborhood b convgnn pooling readout layer graph classiﬁcation 21 graph convolutional layer followed pooling layer coarsen graph subgraphs node representation coarsened graph represent higher representation readout layer summarizes ﬁnal graph representation taking hidden representation subgraphs c gae network embedding 61 encoder us graph convolutional layer get network embedding node decoder computes pairwise distance given network embeddings applying nonlinear activation function decoder reconstructs graph adjacency matrix network trained minimizing discrepancy real adjacency matrix reconstructed adjacency matrix stgnn graph forecasting 74 graph convolutional layer followed layer graph convolutional layer operates x capture spatial dependence layer slide x along time axis capture temporal dependence output layer linear transformation generating prediction node future value next time step operation model method 19 20 require eigenvalue decomposition time complexity time complexity 46 also due node pairwise computation method incur equivalent time complexity graph adjacency matrix sparse otherwise method computation node vi representation involves di neighbor sum di node exactly equal number edge time complexity several method missing table iii method either lack time complexity analysis article report time complexity overall model algorithm iv recurrent graph neural network recgnns mostly pioneer work gnns apply set parameter recurrently node graph extract node representation constrained computational power earlier research mainly focused directed acyclic graph 13 80 gnn 2 proposed scarselli et al extends prior recurrent model handle general type graph acyclic cyclic directed undirected graph 15 based tion diffusion mechanism gnn update node state exchanging neighborhood information recurrently ble equilibrium reached node hidden state recurrently updated h v v f xv xe v u xu h u 1 f parametric function h 0 v initialized randomly sum operation enables gnn applicable node even number neighbor differs no neighborhood ordering known ensure convergence recurrent function f must contraction mapping shrink distance two point projecting latent space case f neural work penalty term ha imposed jacobian matrix parameter convergence criterion satisﬁed last step node hidden state forwarded readout layer gnn alternate stage node state propagation stage parameter gradient computation minimize ing objective strategy enables gnn handle cyclic graph work graph echo state network graphesn 16 extends echo state network improve training efﬁciency gnn graphesn consists encoder output layer encoder randomly initialized requires no training implement contractive state transition function recurrently update node state global graph state reach convergence afterward output layer trained taking ﬁxed node state input gated gnn ggnn 17 employ gated recurrent unit gru 81 recurrent function reducing recurrence ﬁxed number step advantage no longer need constrain parameter ensure convergence node hidden state updated previous hidden state neighboring hidden state deﬁned h v gru v v wh u 2 h 0 v xv different gnn graphesn ggnn us backpropagation time bptt rithm learn model parameter problematic gnn used represent broad graph neural network article name particular method gnn avoid ambiguity authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply wu et al comprehensive survey gnns 9 table iii summary recgnns convgnns missing value pooling readout layer indicate method only experiment task large graph ggnn need run recurrent function multiple time node requiring intermediate state node stored memory stochastic embedding sse proposes ing algorithm scalable large graph 18 sse update node hidden state recurrently stochastic asynchronous fashion alternatively sample batch node state update batch node gradient computation maintain stability recurrent function sse deﬁned weighted average historical state new state take form h v 1 h v v h u xu 3 α hyperparameter h 0 v initialized randomly conceptually important sse doe not theoretically prove node state gradually converge ﬁxed point applying 3 repeatedly convolutional graph neural network convgnns closely related recurrent graph neural network instead iterating node state contractive straints convgnns address cyclic mutual dependency architecturally using ﬁxed number layer ent weight layer key distinction illustrated fig graph convolution efﬁcient nient composite neural network popularity convgnns ha rapidly growing recent year convgnns fall two category based approach deﬁne graph convolution authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 10 ieee transaction neural network learning system vol 32 no 1 january 2021 fig recgnns versus convgnns recgnns use graph recurrent layer grec updating node representation b convgnns use different graph convolutional layer gconv updating node representation introducing ﬁlters perspective graph signal processing 82 graph convolutional operation interpreted removing noise graph signal based approach inherit idea recgnns deﬁne graph convolution information propagation since gcn 22 bridged gap approach approach method oped rapidly recently due attractive efﬁciency ﬂexibility generality convgnns background method solid matical foundation graph signal processing 82 84 assume graph undirected normalized graph cian matrix mathematical representation undirected graph deﬁned l diagonal matrix node degree dii j ai j normalized graph laplacian matrix posse property real symmetric positive semideﬁnite property normalized laplacian matrix factored l u matrix eigenvectors ordered eigenvalue diagonal matrix eigenvalue spectrum λi eigenvectors normalized laplacian matrix form orthonormal space mathematical word ut u graph signal processing graph signal x feature vector node graph xi value ith node graph fourier transform signal x deﬁned f x ut x inverse graph fourier transform deﬁned f ˆ x uˆ x ˆ x represents resulted signal graph fourier transform graph fourier transform project input graph signal orthonormal space basis formed eigenvectors normalized graph laplacian element transformed signal ˆ x coordinate graph signal new space input signal represented x ˆ xiui exactly inverse graph fourier transform graph convolution input signal x ﬁlter g deﬁned x g f f x g u ut x g 4 elementwise product denote ﬁlter gθ diag ut g spectral graph convolution simpliﬁed x gθ ugθut x 5 convgnns follow deﬁnition key difference lie choice ﬁlter gθ spectral cnn 19 assumes ﬁlter gθ k j set learnable parameter considers graph signal multiple channel graph convolutional layer spectral cnn deﬁned h k j σ k j ut h j 1 2 fk 6 k layer index h input graph signal h 0 x number input channel fk number output channel k j diagonal matrix ﬁlled learnable parameter due decomposition laplacian matrix spectral cnn face three limitation first any perturbation graph result change eigenbasis second learned ﬁlters domain dependent mean not applied graph different structure third eigendecomposition requires computational complexity work chebnet 21 gcn 22 reduce computational complexity making several approximation simpliﬁcations chebyshev spectral cnn chebnet 21 approximates ﬁlter gθ chebyshev polynomial diagonal matrix eigenvalue gθ k θiti value lie 1 chebyshev polynomial deﬁned recursively ti x x x x 1 x result convolution graph signal x deﬁned ﬁlter gθ x gθ u k θiti ut x 7 l ti l uti ut proven induction chebnet take form x gθ k θiti l x 8 improvement spectral cnn ﬁlters deﬁned chebnet localized space mean ﬁlters extract local feature independently graph size spectrum chebnet mapped 1 linearly cayleynet 23 applies cayley polynomial parametric rational complex function capture narrow frequency band spectral graph convolution cayleynet deﬁned x gθ r c j hl j hl ii 9 return real part complex number real coefﬁcent c j complex coefﬁcent imaginary number h parameter control spectrum cayley ﬁlter preserving spatial locality cayleynet authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply wu et al comprehensive survey gnns 11 show chebnet considered special case cayleynet graph convolutional network gcn 22 introduces order approximation chebnet assuming k 1 λmax 2 8 simpliﬁed x gθ 2 2 x 10 restrain number parameter avoid overﬁtting gcn assume θ leading following deﬁnition graph convolution x gθ θ 2 2 x 11 allow multichannels input output gcn modiﬁes 11 compositional layer deﬁned h x f 12 f activation function using empirically cause numerical instability gcn address problem gcn applies normalization trick replace dii j ai method gcn also interpreted method perspective gcn considered aggregating feature information node neighborhood equation 12 expressed hv f n v av uxu 13 several recent work made incremental improvement gcn 22 exploring alternative symmetric matrix tive gcn agcn 40 learns hidden structural relation unspeciﬁed graph adjacency matrix construct residual graph adjacency matrix learnable distance function take two node feature input dual gcn dgcn 41 introduces convolutional architecture two graph convolutional layer parallel two layer share parameter use ized adjacency matrix positive pointwise mutual information ppmi matrix capture node information random walk sampled graph ppmi matrix deﬁned max log count count count 0 14 count count function return frequency node v node u sampled random walk ensembling output convolutional layer dgcn encodes local global structural information without need stack multiple graph convolutional layer convgnns analogous convolutional operation conventional cnn image method deﬁne graph convolution based node spatial relation image considered special form graph pixel representing node pixel directly connected nearby pixel shown fig 1 ﬁlter applied 3 3 patch taking weighted average pixel value central node neighbor across channel similarly graph convolution convolve central node representation neighbor representation derive updated representation central node shown fig 1 b another perspective convgnns share idea information passing recgnns spatial graph convolutional operation essentially propagates node information along edge neural network graph 24 proposed parallel gnn ﬁrst work toward convgnns distinctively different recgnns learns graph mutual dependence compositional neural architecture independent parameter layer neighborhood node extended incremental construction architecture performs graph convolution summing node neighborhood information directly also applies residual connection skip connection memorize information layer result derives node state h k v f k xv v k h u 15 f activation function h 0 v tion 15 also written matrix form h k f xw k ah k 16 resembles form gcn 22 one difference us unnormalized adjacency matrix may potentially cause hidden node state extremely different scale contextual graph markov model cgmm 47 proposes probabilistic model inspired maintaining spatial locality cgmm ha beneﬁt bilistic interpretability diffusion cnn dcnn 25 regard graph convolution diffusion process assumes information transferred one node one neighboring node certain transition probability information distribution reach equilibrium several round dcnn deﬁnes diffusion graph convolution dgc h k f w k 17 f activation function probability sition matrix p computed p note dcnn hidden representation matrix h k remains dimension input feature matrix x not function previous hidden representation matrix h dcnn concatenates h 1 h 2 h k together ﬁnal model output stationary distribution diffusion process summation power series probability sition matrix dgc 72 sum output diffusion authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 12 ieee transaction neural network learning system vol 32 no 1 january 2021 step instead concatenation deﬁnes dgc h k f pkxw k 18 w k f activation function using power transition probability matrix implies distant neighbor contribute little information central node 46 increase contribution distant neighbor based shortest path deﬁnes adjacency matrix j shortest path node v node u length j j v u 1 otherwise hyperparameter r control receptive ﬁeld size introduces graph convolutional operation follows h k f j j h w j k 19 j ii l j l h 0 x concatenation vector calculation adjacency matrix expensive maximum partition graph convolution pgc 75 partition node neighbor q group based certain criterion not ited shortest path pgc construct q adjacency matrix according deﬁned neighborhood group pgc applies gcn 22 different parameter matrix neighbor group sum result h k q j h w j k 20 h 0 x j j j j j j neural network mpnn 27 outline general framework convgnns treat graph convolution process information passed one node another along edge directly mpnn run iteration let information propagate function namely spatial graph convolution deﬁned h k v uk v v mk h v h u xe vu 21 h 0 v xv uk mk function learnable parameter deriving hidden representation node h k v passed output layer perform prediction task readout function perform prediction task readout function generates representation entire graph based node hidden representation generally deﬁned hg k v 22 r represents readout function learnable parameter mpnn cover many existing gnns ing different form uk mk r 22 85 87 however graph isomorphism network gin 57 ﬁnds previous method incapable distinguishing different graph structure based graph fig difference gcn 22 gat 43 gcn 22 explicitly assigns nonparametric weight aij deg vi deg v j neighbor v j vi aggregation process b gat 43 implicitly capture weight aij via neural network architecture important node receive larger weight embedding produced amend drawback gin adjusts weight central node learnable parameter ϵ k performs graph convolution h k v mlp 1 ϵ k h v v h u 23 mlp represents multilayer perceptron number neighbor node vary one thousand even inefﬁcient take full size node neighborhood graphsage 42 adopts sampling obtain ﬁxed number neighbor node performs graph convolution h k v σ w k fk h v h u v 24 h 0 v xv fk aggregation function sn v random sample node v neighbor aggregation function invariant permutation node ordering mean sum max function graph attention network gat 43 assumes butions neighboring node central node neither identical like graphsage 42 predetermined like gcn 22 difference illustrated fig 4 gat adopts attention mechanism learn relative weight two connected node graph convolutional operation according gat deﬁned h k v σ v α k vu w k h u 25 h 0 v xv attention weight α k vu measure connective strength node v neighbor u α k vu k h v k h u 26 g leakyrelu activation function vector learnable parameter softmax function ensures attention weight sum one bors node gat performs multihead attention increase model expressive capability show impressive improvement graphsage node authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply wu et al comprehensive survey gnns 13 classiﬁcation task gat assumes contribution attention head equal gated attention network gaan 48 introduces mechanism computes additional attention score attention head apart applying graph attention spatially geniepath 55 pose gating mechanism control information ﬂow across graph convolutional layer graph attention model might interest 88 89 however not belong convgnn framework mixture model network monet 44 adopts different approach assign different weight node neighbor introduces node pseudocoordinates determine relative position node neighbor relative position two node known weight function map relative position relative weight two node way parameter graph ﬁlter shared across different location monet framework several existing approach manifold geodesic cnn gcnn 90 anisotropic cnn acnn 91 spline cnn 92 graph gcn 22 dcnn 25 generalized special instance monet constructing nonparametric weight function monet additionally proposes gaussian kernel learnable parameter learn weight function adaptively another distinct line work achieves weight sharing across different location ranking node neighbor based certain criterion associating ranking learnable weight 26 order neighbor node according graph labelings selects top q neighbor graph labelings essentially node score derived node degree centrality lehman wl color 93 94 node ha ﬁxed number ordered neighbor data converted data applies standard convolutional ﬁlter aggregate neighborhood feature information order ﬁlter weight corresponds order node neighbor ranking criterion only considers graph structure requires heavy computation data processing scale gcn lgcn 45 rank node neighbor based node feature information node lgcn assembles feature matrix consists neighborhood sort feature matrix along column ﬁrst q row sorted feature matrix taken input data central node 1 improvement term training efﬁciency ing convgnns gcn 22 usually required save whole graph data intermediate state node memory training algorithm convgnns suffers signiﬁcantly memory overﬂow problem especially graph contains million node save memory graphsage 42 proposes algorithm convgnns sample tree rooted node recursively expanding root node neighborhood k step ﬁxed sample size sampled tree graphsage computes root node hidden representation hierarchically aggregating hidden node representation bottom top fast learning gcn fastgcn 49 sample ﬁxed number node graph convolutional layer instead sampling ﬁxed number neighbor node like graphsage 42 interprets graph convolution integral transforms embedding function node probability measure monte carlo approximation variance reduction technique employed facilitate training process fastgcn sample node independently layer connection potentially sparse huang et al 51 propose adaptive layerwise sampling approach node sampling lower layer conditioned top one method achieves higher accuracy compared fastgcn cost employing much complicated sampling scheme another work stochastic training gcns stogcn 50 reduces receptive ﬁeld size graph convolution arbitrarily small scale using historical node representation control variate stogcn achieves comparable performance even two neighbor per node however stogcn still ha save intermediate state node memory consuming large graph 58 sample subgraph using graph ing algorithm performs graph convolution node within sampled subgraph neighborhood search also restricted within sampled subgraph ble handling larger graph using deeper architecture time le time le memory gcn notably provides straightforward comparison time complexity memory complexity existing convgnn training algorithm analyze result based table iv table iv gcn 22 baseline method conduct training graphsage save memory cost sacriﬁcing time efﬁciency meanwhile time ory complexity graphsage grow exponentially increase k time complexity highest bottleneck memory remains unsolved however achieve satisfactory performance small time complexity remains baseline method since doe not introduce redundant computation method realizes lowest memory complexity 2 comparison spectral spatial model ctral model theoretical foundation graph signal processing designing new graph signal ﬁlters leynets 23 one build new convgnns however spatial model preferred spectral model due efﬁciency generality ﬂexibility issue first spectral model le efﬁcient spatial model spectral model either need perform eigenvector computation handle whole graph time spatial model scalable large graph directly perform convolution graph domain via information propagation computation performed batch node instead whole graph second spectral model rely graph fourier basis generalize poorly new graph assume ﬁxed graph any perturbation graph would result change eigenbasis model hand perform graph convolution locally node weight easily shared across authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 14 ieee transaction neural network learning system vol 32 no 1 january 2021 table iv time memory complexity comparison convgnn training algorithm summarized 58 n total number node total number edge k number layer batch size r number neighbor sampled node simplicity dimension node hidden feature remain constant denoted different location structure third model limited operate undirected graph model ﬂexible handle multisource graph input edge input 15 27 86 95 96 directed graph 25 72 signed graph 97 heterogeneous graph 98 99 graph input incorporated aggregation function easily graph pooling module gnn generates node feature use ﬁnal task however using feature directly computationally challenging thus downsampling strategy needed depending objective role play network different name given strategy 1 pooling operation aim reduce size meter downsampling node generate smaller representation thus avoid overﬁtting permutation invariance computational complexity issue 2 readout operation mainly used generate level representation based node representation mechanism similar section use pooling refer kind downsampling strategy applied gnns some earlier work graph coarsening algorithm use eigendecomposition coarsen graph based ical structure however method suffer time complexity issue graclus algorithm 100 alternative eigendecomposition calculate clustering version original graph some recent work 23 employed pooling operation coarsen graph nowadays pooling primitive effective way implement downsampling since calculating value pooling window fast hg max h k 1 h k 2 h k n 27 k index last graph convolutional layer henaff et al 20 show performing simple pooling beginning network especially important reduce dimensionality graph domain mitigate cost expensive graph fourier transform operation furthermore some work 17 27 46 also use attention mechanism enhance pooling even attention mechanism reduction operation sum pooling not satisfactory since make embedding inefﬁcient embedding generated regardless graph size vinyals et al 101 propose method generate memory increase size input implement lstm intends integrate information memory embedding reduction applied would otherwise destroy information defferrard et al 21 address issue another way rearranging node graph meaningful way devise efﬁcient pooling strategy approach chebnet input graph ﬁrst coarsened multiple level graclus algorithm 100 coarsening node input graph coarsened version rearranged balanced binary tree arbitrarily aggregating balanced binary tree bottom top arrange similar node together pooling rearranged signal much efﬁcient pooling original zhang et al 52 propose dgcnn similar pooling strategy named sortpooling performs pooling rearranging node meaningful order different net 21 dgcnn sort node according structural role within graph graph unordered node feature spatial graph convolution treated continuous wl color 93 used sort node addition sorting node feature uniﬁes graph size q node feature matrix last n row deleted n q otherwise q zero row added aforementioned pooling method mainly consider graph feature ignore structural information graph recently differentiable pooling diffpool 54 proposed generate hierarchical representation graph compared previous coarsening method diffpool doe not simply cluster node graph learns cluster assignment matrix layer k referred k nk number node kth layer probability value matrix k generated based node feature topological structure using k softmax convgnnk k h k 28 core idea learn comprehensive node ments consider topological feature information graph 28 implemented any standard vgnns however drawback diffpool generates dense graph pooling thereafter computational complexity becomes recently sagpool 102 approach proposed considers node feature graph topology learns pooling manner overall pooling essential operation reduce graph size improve effectiveness putational complexity pooling open question investigation authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply wu et al comprehensive survey gnns 15 discussion theoretical aspect discus theoretical foundation gnns ent perspective 1 shape receptive field receptive ﬁeld node set node contribute determination ﬁnal node representation compositing multiple spatial graph convolutional layer receptive ﬁeld node grows one step ahead toward distant neighbor time micheli 24 prove ﬁnite number spatial graph convolutional layer exists node v receptive ﬁeld node v cover node graph result convgnn able extract global information stacking local graph convolutional layer 2 vc dimension vc dimension measure model complexity deﬁned largest number point shattered model work analyzing vc dimension gnns given number model parameter p number node n scarselli et al 103 derive vc dimension gnn 15 us sigmoid tangent hyperbolic activation us piecewise polynomial activation function result suggests model complexity gnn 15 increase rapidly p n sigmoid tangent hyperbolic activation used 3 graph isomorphism two graph isomorphic topologically identical given two nonisomorphic graph xu et al 57 prove gnn map different embeddings two graph identiﬁed nonisomorphic wl test isomorphism 93 show common gnns gcn 22 graphsage 42 incapable distinguishing different graph structure xu et al 57 prove aggregation function readout function gnn injective gnn powerful wl test distinguishing different graph 4 equivariance invariance gnn must equivariant function performing task must invariant function performing task task let f x gnn q any permutation matrix change order node gnn equivariant satisﬁes f qaqt qx q f x task let f x gnn invariant satisﬁes f qaqt qx f x order achieve equivariance invariance component gnn must invariant node ordering maron et al 104 theoretically study characteristic permutation invariant equivariant linear layer graph data 5 universal approximation well known tiperceptron feedforward neural network one hidden layer approximate any borel measurable function 105 universal approximation capability gnns ha seldom studied hammer et al 106 prove cascade relation approximate function structured output scarselli et al 107 prove recgnn 15 mate any function preserve unfolding equivalence any degree precision two node unfolding equivalent unfolding tree identical unfolding tree node constructed iteratively extending node neighborhood certain depth xu et al 57 show convgnns framework message passing 27 not universal approximators continuous function deﬁned multisets maron et al 104 prove invariant graph network approximate arbitrary invariant function deﬁned graph vi graph autoencoders gaes deep neural architecture map node latent feature space decode graph information latent representation gaes used learn network embeddings generate new graph main characteristic selected gaes summarized table following provide brief review gaes two perspective network embedding graph generation network embedding network embedding vector sentation node preserve node topological mation gaes learn network embeddings using encoder extract network embeddings using decoder enforce network embeddings preserve graph topological mation ppmi matrix adjacency matrix earlier approach mainly employ multilayer perceptrons build gaes network embedding learning deep neural work graph representation dngrs 59 use stacked denoising autoencoder 108 encode decode ppmi matrix via multilayer perceptrons concurrently structural deep network embedding sdne 60 us stacked coder preserve node proximity order proximity jointly sdne proposes two loss function output encoder output decoder separately ﬁrst loss function enables learned network embeddings preserve node proximity minimizing distance node network embedding neighbor network embeddings ﬁrst loss function deﬁned v u av xv xu 29 xv av enc encoder consists multilayer perceptron second loss function enables learned network embeddings preserve node order proximity minimizing distance node input reconstructed input concretely second loss function deﬁned dec enc xv 30 bv u 1 av u 0 bv u β 1 av u 1 dec decoder consists multilayer perceptron dngr 59 sdne 60 only consider node structural information connectivity pair node ignore node may contain feature mation depicts attribute node gae 3 61 leverage gcn 22 encode node structural information name gae avoid ambiguity article authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 16 ieee transaction neural network learning system vol 32 no 1 january 2021 table v main characteristic selected gaes node feature information time encoder gae consists two graph convolutional layer take form z enc x gconv f gconv x 31 z denotes network embedding matrix graph f relu activation function gconv function graph convolutional layer deﬁned 12 decoder gae aim decode node relational information embeddings reconstructing graph adjacency matrix deﬁned ˆ av u dec zv zu σ zt v zu 32 zv embedding node gae trained minimizing negative cross entropy given real adjacency matrix reconstructed adjacency matrix ˆ simply reconstructing graph adjacency matrix may lead overﬁtting due capacity autoencoders variational gae vgae 61 variational version gae learn distribution data vgae optimizes variational lower bound l l eq log p l q z 33 k l divergence function measure distance two distribution p z gaussian prior p z n p zi n n p ai j z j dec zi z j σ zt z j q n q q n diag σ 2 mean vector μi ith row encoder output deﬁned 31 log σi derived similarly μi another encoder according 33 vgae assumes empirical distribution q close possible prior distribution p z enforce empirical distribution q approximates prior distribution p z adversarially regularized vgae arvga 62 109 employ training scheme generative adversarial network gans 110 gan play tion game generator discriminator training generative model generator try generate fake ples real possible discriminator attempt distinguish fake sample real one inspired gans arvga endeavor learn encoder produce empirical distribution q indistinguishable prior distribution p z similar gae graphsage 42 encodes node feature two graph convolutional layer instead optimizing reconstruction error graphsage show relational information two node preserved negative sampling loss l zv dec zv zu v log zv zvn 34 node u neighbor node v node vn distant node node v sampled negative sampling distribution pn v q number negative sample loss function essentially enforces close node similar sentations distant node dissimilar representation dgi 56 alternatively drive local network embeddings capture global structural information maximizing local mutual information show distinct improvement graphsage 42 experimentally aforementioned method essentially learn network embeddings solving link prediction problem however sparsity graph cause number positive node pair far le number negative node pair alleviate data sparsity problem learning network embedding another line work convert graph sequence random permutation random walk way deep learning approach applicable sequence directly used process graph deep sive network embedding drne 63 assumes node network embedding approximate aggregation neighborhood network embeddings adopts long authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply wu et al comprehensive survey gnns 17 memory lstm network 7 aggregate node neighbor reconstruction error drne deﬁned l v 35 zv network embedding node v obtained dictionary lookup lstm network take random sequence node v neighbor ordered node degree input suggested 35 drne implicitly learns network embeddings via lstm network rather using lstm network generate network embeddings avoids problem lstm network not invariant permutation node sequence network representation adversarially regularized autoencoders netras 64 propose graph framework general loss tion deﬁned l z dist z dec enc z 36 dist distance measure node ding z reconstructed encoder decoder netra lstm network random walk rooted node v input similar arvga 62 netra regularizes learned network embeddings within prior distribution via adversarial training although netra ignores node permutation variant problem lstm network experimental result validate effectiveness netra graph generation multiple graph gaes able learn ative distribution graph encoding graph hidden representation decoding graph structure given hidden representation majority gaes graph generation designed solve molecular graph generation problem ha high practical value drug discovery method either propose new graph sequential manner global manner sequential approach generate graph proposing node edge step step et al 111 kusner et al 112 dai et al 113 model generation process string representation molecular graph named smile deep cnns rnns encoder decoder respectively method speciﬁc alternative solution applicable general graph mean iteratively adding node edge growing graph certain criterion satisﬁed deep generative model graph deepgmg 65 assumes probability graph sum possible node permutation p g π p g π 37 π denotes node ordering capture complex joint probability node edge graph deepgmg generates graph making sequence decision namely whether add node node add whether add edge node connect new node decision process generating node edge conditioned node state graph state growing graph updated recgnn another work graphrnn 66 proposes rnn rnn model generation process node edge rnn add new node node sequence time rnn produce binary sequence indicating connection new node node previously generated sequence global approach output graph graph tional autoencoder graphvae 67 model existence node edge independent random variable ing posterior distribution qφ deﬁned encoder generative distribution pθ deﬁned decoder graphvae optimizes variational lower bound l φ θ g eqφ pθ k l qφ z 38 p z follows gaussian prior φ θ learnable parameter convgnn encoder simple multilayer perception decoder graphvae output generated graph adjacency matrix node attribute edge attribute challenging control global property generated graph graph connectivity validity node compatibility regularized graphvae rgvae 68 imposes validity constraint graphvae ularize output distribution decoder molecular gan molgan 69 integrates convgnns 114 gans 115 reinforcement learning objective generate graph desired property molgan consists generator discriminator competing improve authenticity generator molgan generator try propose fake graph along feature matrix discriminator aim distinguish fake sample empirical data addition reward network introduced parallel discriminator encourage generated graph posse certain property according external evaluator netgan 70 combine lstms 7 wasserstein gans 116 generate graph approach netgan train generator produce plausible random walk lstm network enforces discriminator identify fake random walk real one training new graph derived normalizing matrix node computed based random walk produced generator brief sequential approach linearize graph sequence lose structural information due presence cycle global approach produce graph not scalable large graph output space gae vii graph neural network graph many application dynamic term graph structure graph input stgnns occupy important position capturing dynamicity graph method category aim model dynamic node input assuming interdependency connected node example trafﬁc network consists speed sensor placed road edge weight determined distance pair sensor trafﬁc condition authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 18 ieee transaction neural network learning system vol 32 no 1 january 2021 one road may depend adjacent road condition necessary consider spatial dependence performing trafﬁc speed forecasting solution stgnns capture spatial temporal dependency graph simultaneously task stgnns forecasting future node value label predicting graph label stgnns follow two direction method method approach capture dependency ﬁltering input hidden state passed recurrent unit using graph convolution 48 71 72 illustrate suppose simple rnn take form h σ wx uh b 39 x node feature matrix time step inserting graph convolution 39 becomes h σ gconv x w gconv h u b 40 gconv graph convolutional layer graph lutional recurrent network gcrn 71 combine lstm network chebnet 21 diffusion convolutional rnn dcrnn 72 incorporates proposed diffusion graph volutional layer see 18 gru network addition dcrnn adopts framework predict future k step node value another parallel work us rnns rnns handle different aspect temporal information 73 proposes recurrent framework predict node label time step comprises two kind rnns namely temporal information node edge passed respectively incorporate spatial information take output rnns input since assuming different rnns different node edge signiﬁcantly increase model complexity instead split node edge semantic group node edge semantic group share rnn model save computational cost approach suffer ative propagation gradient issue alternative solution approach tackle graph nonrecursive manner advantage parallel computing stable gradient memory requirement illustrated fig 2 approach interleave layer graph lutional layer learn temporal spatial dependency respectively assume input stgnn tensor x layer slide x along time axis aggregate temporal information node graph convolutional layer operates x aggregate spatial information time step cgcn 74 integrates convolutional layer chebnet 21 gcn 22 layer construct block stacking gated convolutional layer graph convolutional layer another gated convolutional layer sequential order 75 composes block using convolutional layer pgc layer see 20 previous method use predeﬁned graph structure assume predeﬁned graph structure reﬂects genuine dependence relationship among node however many snapshot graph data setting possible learn latent static graph structure automatically data realize graph wavenet 76 proposes adjacency matrix perform graph convolution adjacency matrix deﬁned aadp softmax relu 2 41 softmax function computed along row dimension denotes source node embedding denotes target node embedding learnable parameter multiplying one get dependence weight source node target node complex neural network graph wavenet performs well without given adjacency matrix learning latent static spatial dependency help researcher discover interpretable stable correlation among different entity network however some circumstance learning latent dynamic spatial dependency may improve model precision example trafﬁc network travel time two road may depend current trafﬁc condition gaan 48 employ attention mechanism learn dynamic spatial dependency approach attention function used update edge weight two connected node given current node input astgcn 77 includes spatial attention function temporal attention function learn latent dynamic spatial dependency temporal dependency approach common drawback learning latent spatial dependency need calculate spatial dependence weight pair node cost viii application data ubiquitous gnns wide variety application section summarize benchmark graph data set evaluation method source implementation respectively detail practical cation gnns various domain data set mainly sort data set four group namely citation network biochemical graph social network others table vi summarize selected benchmark data set detail given supplementary material evaluation implementation node classiﬁcation graph classiﬁcation common task ass performance recgnns convgnns 1 node classiﬁcation node classiﬁcation od follow standard split benchmark data set including cora citeseer pubmed ppi reddit reported average accuracy score test data set multiple run summarization tal result method found supplementary authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply wu et al comprehensive survey gnns 19 table vi summary selected benchmark data set material noted result not essarily represent rigorous comparison shchur et al 131 identiﬁed two pitfall evaluating performance gnns node classiﬁcation first using split throughout experiment underestimate generalization error second different method employed different training technique hyperparameter tuning parameter ization learning rate decay early stopping relatively fair comparison refer reader 131 2 graph classiﬁcation graph classiﬁcation researcher often adopt tenfold cross validation cv model evaluation however pointed 132 experimental setting ambiguous not uniﬁed across different work ticular 132 raise concern correct usage data split model selection versus model assessment often encountered problem external test set fold used model selection risk assessment reference 132 compare gnns standardized uniform tion framework apply external tenfold cv get estimate generalization performance model inner holdout technique 90 split model selection alternative procedure would method us external cv model assessment inner cv model selection refer reader 132 detailed rigorous comparison gnn method graph classiﬁcation 3 implementation facilitate work baseline experiment deep learning research supplementary material c provide hyperlink implementation gnn model reviewed article noticeably fey et al 92 published ric learning library pytorch named pytorch implement many gnns recently deep graph 4 library dgl 5 133 released provides fast mentation many gnns top popular deep learning platform pytorch mxnet practical application gnns many application across different task domain despite general task handled category gnns directly including node classiﬁcation graph classiﬁcation network embedding graph generation graph forecasting general related task node clustering 134 link prediction 135 graph partitioning 136 also addressed gnns detail some application based following research domain 1 computer vision application gnns computer vision include scene graph generation point cloud ﬁcation action recognition recognizing semantic relationship object tate understanding meaning behind visual scene scene graph generation model aim parse image semantic graph consists object semantic relationship 137 139 another application inverse process generating realistic image given scene graph 140 natural language parsed semantic graph word represents object promising solution synthesize image given textual description classifying segmenting point cloud enable lidar device see surrounding environment point cloud set point recorded lidar scan ences 141 143 convert point cloud neighbor graph superpoint graph use convgnns explore topological structure identifying human action contained video facilitates better understanding video content machine aspect 5 authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 20 ieee transaction neural network learning system vol 32 no 1 january 2021 some solution detect location human joint video clip human joint linked skeleton naturally form graph given time series human joint location 73 75 apply stgnns learn human action pattern moreover number applicable direction gnns computer vision still growing includes interaction 144 image classiﬁcation 145 147 semantic segmentation 148 149 visual reasoning 150 question answering 151 2 natural language processing common application gnns natural language processing text classiﬁcation gnns utilize interrelation document word infer document label 22 42 43 despite fact natural language data exhibit tial order may also contain internal graph structure syntactic dependency tree syntactic dependency tree deﬁnes syntactic relation among word sentence marcheggiani titov 152 propose syntactic gcn run top sentence encoder syntactic gcn aggregate hidden word representation based syntactic dependency tree sentence basting et al 153 apply syntactic gcn task neural machine translation marcheggiani et al 154 adopt model 153 handle semantic dependency graph sentence learning learns generate sentence meaning given semantic graph abstract word known abstract meaning representation song et al 155 propose encode level semantic information beck et al 156 apply ggnn 17 learning neural machine lation inverse task learning ating semantic knowledge graph given sentence useful knowledge discovery 157 158 3 trafﬁc accurately forecasting trafﬁc speed volume density road trafﬁc network fundamentally important smart transportation system zhang et al 48 li et al 72 yu et al 74 address trafﬁc prediction problem using stgnns consider trafﬁc network graph node sensor installed road edge measured distance pair node node ha average trafﬁc speed within window dynamic input feature another application prediction given historical taxi demand location information weather data event feature yao et al 159 incorporate lstm cnn network embeddings trained line 160 form joint representation location predict number taxi demanded location within time interval 4 recommender system recommender tems take item user node leveraging relation item item user user user item well content information recommender system able produce recommendation key recommender system score importance item user result cast link prediction problem predict missing link user item berg et al 161 ying et al 162 propose gae us convgnns encoders monti et al 163 combine rnns graph convolution learn underlying process generates known rating 5 chemistry ﬁeld chemistry researcher apply gnns study graph structure graph atom considered node chemical bond treated edge node classiﬁcation graph classiﬁcation graph generation three main task targeting graph order learn molecular ﬁngerprints 85 86 predict molecular property 27 infer protein interface 164 synthesize chemical compound 65 69 165 6 others application gnns not limited aforementioned domain task exploration applying gnns variety problem program veriﬁcation 17 program reasoning 166 social inﬂuence prediction 167 adversarial attack tion 168 electrical health record modeling 169 170 brain network 171 event detection 172 combinatorial optimization 173 ix future direction though gnns proven power learning graph data challenge still exist due complexity graph section suggest four future direction gnns model depth success deep learning lie deep neural tures 174 however li et al 53 show performance convgnn drop dramatically increase number graph convolutional layer graph convolution push representation adjacent node closer theory inﬁnite number graph convolutional layer node representation converge single point 53 raise question whether going deep still good strategy learning graph data scalability tradeoff scalability gnns gained price corrupting graph completeness whether using sampling clustering model lose part graph information pling node may miss inﬂuential neighbor ing graph may deprived distinct structural pattern tradeoff algorithm scalability graph integrity could future research direction heterogenity majority current gnns assume homogeneous graph difﬁcult directly apply current gnns heterogeneous graph may contain different type node edge different form node edge input image text therefore new method developed handle heterogeneous graph dynamicity graph nature dynamic way node edge may appear disappear input may change authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply wu et al comprehensive survey gnns 21 time time new graph convolution needed adapt dynamicity graph although dynamicity graph partly addressed stgnns consider perform graph convolution case dynamic spatial relation conclusion article conduct comprehensive overview gnns provide taxonomy group gnns four category recgnns convgnns gaes stgnns provide thorough review comparison tions method within category introduce wide range application gnns data set code model assessment gnns marized finally suggest four future direction gnns reference 1 redmon divvala girshick farhadi only look uniﬁed object detection proc ieee conf comput vi pattern recognit cvpr jun 2016 pp 2 ren girshick sun faster towards time object detection region proposal network proc nip 2015 pp 3 luong pham manning effective approach neural machine translation proc conf empirical method natural lang 2015 pp 4 wu et google neural machine translation system bridging gap human machine translation 2016 online available 5 hinton et deep neural network acoustic modeling speech recognition shared view four research group ieee signal process vol 29 no 6 pp 2012 6 lecun bengio convolutional network image speech time series handbook brain theory neural network vol 3361 no cambridge usa mit press 1995 7 hochreiter schmidhuber long memory neural vol 9 no 8 pp 1997 8 vincent larochelle lajoie bengio manzagol stacked denoising autoencoders learning useful representation deep network local denoising criterion mach learn vol 11 no 12 pp 2010 9 bronstein bruna lecun szlam van der gheynst geometric deep learning going beyond euclidean data ieee signal process vol 34 no 4 pp jul 2017 10 hamilton ying leskovec representation ing graph method application proc nip 2017 pp 11 battaglia et relational inductive bias deep learning graph network 2018 online available 12 boaz lee rossi kim ahmed koh attention model graph survey 2018 online available 13 sperduti starita supervised neural network classiﬁcation structure ieee trans neural vol 8 no 3 pp may 1997 14 gori monfardini scarselli new model learning graph domain proc ieee int joint conf neural vol 2 2005 pp 15 scarselli gori tsoi hagenbuchner monfardini graph neural network model ieee trans neural vol 20 no 1 pp 2009 16 gallicchio micheli graph echo state network proc int joint conf neural netw ijcnn jul 2010 pp 17 li tarlow brockschmidt zemel gated graph sequence neural network proc iclr 2015 pp 18 dai kozareva dai smola song learning iterative algorithm graph proc icml 2018 pp 19 bruna zaremba szlam lecun spectral network locally connected network graph proc iclr 2014 pp 20 henaff bruna lecun deep convolutional network data 2015 online available 21 defferrard bresson van der gheynst convolutional neural network graph fast localized spectral ﬁltering proc nip 2016 pp 22 kipf welling classiﬁcation graph convolutional network proc iclr 2017 pp 23 levie monti bresson bronstein cayleynets graph convolutional neural network complex rational spectral ﬁlters ieee trans signal vol 67 no 1 pp 2019 24 micheli neural network graph contextual constructive approach ieee trans neural vol 20 no 3 pp mar 2009 25 atwood towsley neural network proc nip 2016 pp 26 niepert ahmed kutzkov learning convolutional neural network graph proc icml 2016 pp 27 gilmer schoenholz riley vinyals dahl neural message passing quantum chemistry proc icml 2017 pp 28 cui wang pei zhu survey network embedding ieee trans knowl data vol 31 no 5 pp may 2019 29 zhang yin zhu zhang network representation learning survey ieee trans big data vol 6 no 1 pp mar 2020 30 cai zheng chang comprehensive survey graph embedding problem technique application ieee trans knowl data vol 30 no 9 pp 2018 31 goyal ferrara graph embedding technique application performance survey vol 151 pp jul 2018 32 pan wu zhu zhang wang deep network representation proc ijcai 2016 pp 33 shen pan liu ong sun discrete network embedding proc int joint conf artif jul 2018 pp 34 yang pan zhang chen lian zhang binarized attributed network embedding proc ieee int conf data mining icdm 2018 pp 35 perozzi skiena deepwalk online learning social representation proc acm sigkdd int conf knowl discovery data mining kdd 2014 pp 36 vishwanathan schraudolph kondor borgwardt graph kernel mach learn vol 11 pp mar 2010 37 shervashidze schweitzer van leeuwen mehlhorn borgwardt graph kernel mach learn vol 12 pp 2011 38 navarin sperduti approximated neighbour minhash graph node kernel proc esann 2017 pp 39 kriege johansson morris survey graph kernel 2019 online available 40 li wang zhu huang adaptive graph convolutional neural network proc aaai 2018 pp 41 zhuang dual graph convolutional network based classiﬁcation proc world wide web conf world wide web www 2018 pp 42 hamilton ying leskovec inductive representation learning large graph proc nip 2017 pp 43 velickovic cucurull casanova romero lio bengio graph attention network proc iclr 2017 pp 44 monti boscaini masci rodola svoboda bronstein geometric deep learning graph manifold using mixture model cnns proc ieee conf comput vi pattern recognit cvpr jul 2017 pp 45 gao wang ji learnable graph convolutional network proc acm sigkdd int conf knowl discovery data mining 2018 pp 46 tran navarin sperduti ﬁlter size graph convolutional network proc ieee symp ser comput intell ssci 2018 pp authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 22 ieee transaction neural network learning system vol 32 no 1 january 2021 47 bacciu errica micheli contextual graph markov model deep generative approach graph processing proc icml 2018 pp 48 zhang shi xie king yeung gaan gated attention network learning large spatiotemporal graph proc uai 2018 pp 49 chen xiao fastgcn fast learning graph convolutional network via importance sampling proc iclr 2018 pp 50 chen zhu song stochastic training graph volutional network variance reduction proc icml 2018 pp 51 huang zhang rong huang adaptive sampling towards fast graph representation learning proc neurips 2018 pp 52 zhang cui neumann chen deep learning architecture graph classiﬁcation proc aaai 2018 pp 53 li han wu deeper insight graph convolutional network learning proc aaai 2018 pp 54 ying morris ren hamilton leskovec hierarchical graph representation learning differentiable ing proc neurips 2018 pp 55 liu et geniepath graph neural network adaptive tive path proc aaai conf artif jul 2019 pp 56 veliˇ c fedus hamilton liò bengio hjelm deep graph infomax proc iclr 2019 pp 57 xu hu leskovec jegelka powerful graph neural network proc iclr 2019 pp 58 chiang liu si li bengio hsieh efﬁcient algorithm training deep large graph convolutional network proc acm sigkdd int conf knowl discovery data mining kdd 2019 pp 59 cao lu xu deep neural network learning graph representation proc aaai 2016 pp 60 wang cui zhu structural deep network embedding proc acm sigkdd int conf knowl discovery data mining kdd 2016 pp 61 kipf welling variational graph proc nip workshop bayesian deep 2016 pp 62 pan hu long jiang yao zhang adversarially regularized graph autoencoder graph embedding proc ijcai jul 2018 pp 63 tu cui wang yu zhu deep recursive network embedding regular equivalence proc acm sigkdd int conf knowl discovery data mining 2018 pp 64 yu et learning deep network representation adversarially regularized autoencoders proc acm sigkdd int conf knowl discovery data mining 2018 pp 65 li vinyals dyer pascanu battaglia learning deep generative model graph proc icml 2018 pp 66 ying ren hamilton leskovec graphrnn deep generative model graph proc icml 2018 pp 67 simonovsky komodakis graphvae towards generation small graph using variational autoencoders proc icann cham switzerland springer 2018 pp 68 chen xiao constrained generation cally valid graph via regularizing variational autoencoders proc neurips 2018 pp 69 de cao kipf molgan implicit generative model small molecular graph icml workshop theor found appl deep generative model 2018 pp 70 bojchevski shchur zügner günnemann netgan generating graph via random walk proc icml 2018 pp 71 seo defferrard vandergheynst bresson structured sequence modeling graph convolutional recurrent network proc neurips springer 2018 pp 72 li yu shahabi liu diffusion convolutional recurrent neural network trafﬁc forecasting proc iclr 2018 pp 73 jain zamir savarese saxena deep learning graph proc ieee conf put vi pattern recognit cvpr jun 2016 pp 74 yu yin zhu graph convolutional network deep learning framework trafﬁc forecasting proc ijcai jul 2018 pp 75 yan xiong lin spatial temporal graph convolutional network action recognition proc aaai 2018 pp 76 wu pan long jiang zhang graph wavenet deep graph modeling proc ijcai 2019 pp 77 guo lin feng song wan attention based temporal graph convolutional network trafﬁc ﬂow forecasting proc aaai 2019 pp 78 pan wu zhu zhang yu joint structure feature exploration regularization graph classiﬁcation ieee trans knowl data vol 28 no 3 pp mar 2016 79 pan wu zhu long zhang task sensitive feature exploration learning multitask graph classiﬁcation ieee trans vol 47 no 3 pp mar 2017 80 micheli sona sperduti contextual processing structured data recursive cascade correlation ieee trans neural vol 15 no 6 pp 2004 81 cho et learning phrase representation using rnn decoder statistical machine translation proc conf empirical method natural lang process emnlp 2014 pp 82 shuman narang frossard ortega van der gheynst emerging ﬁeld signal processing graph extending data analysis network irregular domain ieee signal process vol 30 no 3 pp may 2013 83 sandryhaila moura discrete signal processing graph ieee trans signal vol 61 no 7 pp apr 2013 84 chen varma sandryhaila kovacevic discrete signal processing graph sampling theory ieee trans signal vol 63 no 24 pp 2015 85 duvenaud et convolutional network graph learning molecular ﬁngerprints proc nip 2015 pp 86 kearnes mccloskey berndl pande riley lar graph convolution moving beyond ﬁngerprints mol vol 30 no 8 pp 2016 87 schütt arbabzadah chmiela müller tkatchenko insight deep tensor neural network nature vol 8 no 1 13890 2017 88 lee rossi kong graph classiﬁcation using structural attention proc acm sigkdd int conf knowl discovery data mining 2018 pp 89 perozzi alemi watch step learning node embeddings via graph attention proc neurips 2018 pp 90 masci boscaini bronstein vandergheynst desic convolutional neural network riemannian manifold proc ieee int conf comput vi workshop iccvw 2015 pp 91 boscaini masci rodolà bronstein learning shape correspondence anisotropic convolutional neural network proc nip 2016 pp 92 fey lenssen weichert müller splinecnn fast geometric deep learning continuous kernel proc conf comput vi pattern jun 2018 pp 93 weisfeiler lehman reduction graph ical form algebra arising reduction technicheskaya informatsia vol 2 no 9 pp 1968 94 douglas method graph morphism testing 2011 online available 95 pham tran phung venkatesh column network collective classiﬁcation proc aaai 2017 pp 96 simonovsky komodakis dynamic ﬁlters convolutional neural network graph proc ieee conf comput vi pattern recognit cvpr jul 2017 pp 97 derr tang signed graph convolutional network proc ieee int conf data mining icdm 2018 pp 98 et robust spatial ﬁltering graph convolutional neural network ieee sel topic signal vol 11 no 6 pp 2017 99 wang et heterogeneous graph attention network proc world wide web conf www 2019 pp authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply wu et al comprehensive survey gnns 23 100 dhillon guan kulis weighted graph cut without eigenvectors multilevel approach ieee trans pattern anal mach vol 29 no 11 pp 2007 101 vinyals bengio kudlur order matter sequence sequence set proc iclr 2016 pp 102 lee lee kang graph pooling proc icml 2019 pp 103 scarselli tsoi hagenbuchner chervonenkis dimension graph recursive neural network neural vol 108 pp 2018 104 maron shamir lipman invariant equivariant graph network iclr 2019 pp 105 hornik stinchcombe white multilayer feedforward network universal approximators neural vol 2 no 5 pp 1989 106 hammer micheli sperduti universal approximation capability cascade correlation structure neural vol 17 no 5 pp may 2005 107 scarselli gori chung tsoi hagenbuchner monfardini computational capability graph neural network ieee trans neural vol 20 no 1 pp 2009 108 vincent larochelle bengio manzagol extracting composing robust feature denoising autoencoders proc int conf mach learn icml 2008 pp 109 pan hu fung long jiang zhang learning graph embedding adversarial training method ieee trans early access 2 2019 doi 110 goodfellow et generative adversarial net proc nip 2014 pp 111 et automatic chemical design using continuous representation molecule ac central vol 4 no 2 pp 2018 112 kusner paige grammar variational autoencoder proc icml 2017 pp 113 dai tian dai skiena song variational autoencoder structured data proc iclr 2018 pp 114 schlichtkrull kipf bloem van den berg titov welling modeling relational data graph convolutional network eswc cham switzerland springer 2018 pp 115 gulrajani ahmed arjovsky dumoulin courville improved training wasserstein gans proc nip 2017 pp 116 arjovsky chintala bottou wasserstein gan 2017 online available 117 sen namata bilgic getoor galligher collective classiﬁcation network data ai vol 29 no 3 93 2008 118 tang zhang yao li zhang su arnetminer extraction mining academic social network proc kdd 2008 pp 119 zitnik leskovec predicting multicellular function tissue network bioinformatics vol 33 no 14 pp jul 2017 120 wale watson karypis comparison descriptor space chemical compound retrieval classiﬁcation knowl inf vol 14 no 3 pp 2008 121 debnath de compadre debnath shusterman hansch relationship mutagenic matic heteroaromatic nitro compound correlation molecular orbital energy hydrophobicity medicinal vol 34 no 2 pp 1991 122 dobson doig distinguishing enzyme structure without alignment mol vol 330 no 4 pp jul 2003 123 borgwardt ong schönauer vishwanathan smola kriegel protein function prediction via graph kernel bioinformatics vol 21 no 1 pp jun 2005 124 toivonen srinivasan king kramer helma statistical evaluation predictive toxicology challenge 2001 bioinformatics vol 19 no 10 pp jul 2003 125 ramakrishnan dral rupp von lilienfeld quantum chemistry structure property 134 kilo molecule sci data vol 1 no 1 2014 art no 140022 126 chen et alchemy quantum chemistry dataset marking ai model 2019 online available 127 tang liu relational learning via latent social dimension proc acm sigkdd int conf knowl discovery data mining kdd 2009 pp 128 lecun bottou bengio haffner learning applied document recognition proc ieee vol 86 no 11 pp 1998 129 jagadish et big data technical challenge commun acm vol 57 no 7 pp jul 2014 130 carlson betteridge kisiel settle hruschka mitchell toward architecture language learning proc aaai 2010 pp 131 shchur mumme bojchevski günnemann pitfall graph neural network evaluation proc neurips workshop 2018 pp 132 errica podda bacciu micheli fair parison graph neural network graph classiﬁcation proc iclr 2020 pp online available 133 wang et deep graph library towards efﬁcient scalable deep learning graph proc iclr workshop represent learn graph manifold 2019 pp 134 wang pan long zhu jiang mgae ized graph autoencoder graph clustering proc cikm 2017 pp 135 zhang chen link prediction based graph neural network proc neurips 2018 pp 136 kawamoto tsubaki obuchi theory graph neural network graph partitioning proc neurips 2018 pp 137 xu zhu choy scene graph generation iterative message passing proc ieee conf comput vi pattern recognit cvpr jul 2017 pp 138 yang lu lee batra parikh graph scene graph generation proc eccv springer 2018 pp 139 li ouyang zhou shi zhang wang torizable net efﬁcient framework scene graph generation proc eccv springer 2018 pp 140 johnson gupta image generation scene graph proc conf comput vi pattern jun 2018 pp 141 wang sun liu sarma bronstein solomon dynamic graph cnn learning point cloud acm trans vol 38 no 5 pp 2019 142 landrieu simonovsky point cloud semantic segmentation superpoint graph proc conf put vi pattern jun 2018 pp 143 te hu zheng guo rgcnn regularized graph cnn point cloud segmentation proc acm multimedia conf multimedia conf mm 2018 pp 144 qi wang jia shen zhu learning object interaction graph parsing neural network proc eccv springer 2018 pp 145 satorras estrach learning graph neural network proc iclr 2018 pp 146 guo chou huang song yeung neural graph matching network fewshot action recognition proc eccv springer 2018 pp 147 liu zhou long jiang yao zhang totype propagation network ppn learning category graph proc int joint conf artif 2019 pp 148 qi liao jia fidler urtasun graph neural work rgbd semantic segmentation proc cvpr 2017 pp 149 yi su guo guibas syncspeccnn synchronized spectral cnn shape segmentation proc ieee conf comput vi pattern recognit cvpr jul 2017 pp 150 chen li gupta iterative visual reasoning beyond convolution proc conf comput vi pattern jun 2018 pp 151 narasimhan lazebnik schwing box reasoning graph convolution net factual visual question answering proc neurips 2018 pp 152 marcheggiani titov encoding sentence graph lutional network semantic role labeling proc conf empirical method natural lang 2017 pp authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 24 ieee transaction neural network learning system vol 32 no 1 january 2021 153 basting titov aziz marcheggiani simaan graph convolutional encoders neural machine translation proc conf empirical method natural lang 2017 pp 154 marcheggiani basting titov exploiting semantics neural machine translation graph convolutional network proc naacl 2018 pp 155 song zhang wang gildea model generation proc acl 2018 pp 156 beck haffari cohn learning using gated graph neural network proc acl 2018 pp 157 johnson learning graphical state transition proc iclr mar 2016 pp 158 chen sun han tic graph generation semantic parsing proc acl 2018 pp 159 yao et deep network taxi demand prediction proc aaai 2018 pp 160 tang qu wang zhang yan mei line information network embedding proc www 2015 pp 161 van den berg kipf welling graph tional matrix completion 2017 online able 162 ying chen eksombatchai hamilton leskovec graph convolutional neural network recommender system proc acm sigkdd int conf knowl discovery data mining 2018 pp 163 monti bronstein bresson geometric matrix completion recurrent neural network proc nip 2017 pp 164 fout byrd shariat protein interface prediction using graph convolutional network proc nip 2017 pp 165 liu ying pande leskovec graph tional policy network molecular graph generation proc neurips 2018 pp 166 allamanis brockschmidt khademi learning represent program graph proc iclr 2017 pp 167 qiu tang dong wang tang deepinf social inﬂuence prediction deep learning proc kdd 2018 pp 168 zügner akbarnejad günnemann adversarial attack neural network graph data proc acm sigkdd int conf knowl discovery data mining kdd 2019 pp 169 choi bahadori song stewart sun gram attention model healthcare representation learning proc acm sigkdd int conf knowl discovery data mining kdd 2017 pp 170 choi xiao stewart sun mime multilevel medical embedding electronic health record predictive healthcare proc neurips 2018 pp 171 kawahara et brainnetcnn convolutional neural network brain network towards predicting neurodevelopment neuroimage vol 146 pp 2017 172 nguyen grishman graph convolutional network pooling event detection proc aaai 2018 pp 173 li chen koltun combinatorial optimization graph convolutional network guided tree search proc neurips 2018 pp 174 zhang ren sun deep residual learning image recognition proc ieee conf comput vi pattern recognit cvpr jun 2016 pp zonghan wu received degree system science university shanghai science technology shanghai china 2014 degree statistic linköping university linköping sweden currently ing degree computer science university technology sydney ut ultimo nsw australia research concentrate data mining machine learning deep learning graph shirui pan member ieee received degree computer science sity technology sydney ut ultimo nsw australia wa lecturer school software ut currently lecturer ulty information technology monash university clayton vic australia ha published 60 research article journal ferences including ieee transaction neural network learning system tnnls ieee transaction knowledge data neering tkde ieee transaction cybernetics tcyb ieee international conference data engineering icde aaai conference artiﬁcial intelligence aaai international joint ences artiﬁcial intelligence ijcai ieee international ence data mining icdm research interest include data mining machine learning fengwen chen received degree puter science software engineering arizona state university tempe az usa currently pursuing degree computer science university technology sydney ut ultimo nsw australia research concentrate data mining deep learning graph guodong long wa born china received degree computer science university technology sydney ultimo nsw australia currently senior lecturer core ber centre artiﬁcial intelligence cai faculty engineering information ogy university technology sydney research focus machine learning data mining cloud computing chengqi zhang senior member ieee received degree university land brisbane qld australia 1991 higher doctorate degree deakin versity geelong vic australia since december 2001 ha professor information technology university technology sydney ut ultimo nsw australia wa director ut priority investment research centre quantum tation intelligent system 2008 research interest mainly focus data mining application zhang fellow australian computer society wa general acm sigkdd conference knowledge discovery data mining kdd 2015 sydney local arrangement chair international joint conference artiﬁcial intelligence ijcai melbourne philip yu life fellow ieee received degree electrical engineering stanford university stanford ca usa currently distinguished professor puter science university illinois chicago chicago il usa also wexler chair information technology ha published 830 article refereed journal conference hold ha applied 300 patent research interest include big data data mining data stream database privacy yu fellow acm received acm sigkdd 2016 vation award research contribution award ieee international conference data mining 2003 technical achievement award ieee computer society authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply