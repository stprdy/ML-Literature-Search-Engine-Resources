e n equivariant graph neural network victor garcia satorras 1 emiel hoogeboom 1 max welling 1 abstract paper introduces new model learn graph neural network equivariant rotation tions reﬂections permutation called e n equivariant graph neural network egnns contrast existing method work doe not require computationally expensive representation intermediate layer still achieves competitive better performance addition whereas existing method ited equivariance 3 dimensional space model easily scaled space demonstrate effectiveness method dynamical system modelling sentation learning graph autoencoders dicting molecular property introduction although deep learning ha largely replaced feature many advance critically dependent tive bias deep neural network effective method restrict neural network relevant function exploit symmetry problem enforcing equivariance respect transformation certain symmetry group notable example translation equivariance lutional neural network permutation equivariance graph neural network bruna et 2013 defferrard et 2016 kipf welling many problem exhibit translation rotation try some example point cloud uy et 2019 molecular structure ramakrishnan et 2014 particle simulation kipf et 2018 group sponding symmetry named euclidean group se 3 reﬂections included e 3 often sired prediction task either equivariant invariant respect e 3 transformation delta lab university amsterdam netherlands correspondence victor garcia ra emiel hoogeboom max welling proceeding 38 th international conference machine learning pmlr 139 copyright 2021 author figure example rotation equivariance graph graph neural network φ recently various form method achieve e 3 se 3 equivariance proposed thomas et 2018 fuchs et 2020 finzi et 2020 ohler et 2020 many work achieve innovation ing type representation intermediate network layer however transformation representation require coefﬁcients imations expensive compute additionally practice many type data input output restricted scalar value instance temperature energy referred literature vector instance velocity momentum referred literature work present new architecture translation rotation reﬂection equivariant e n permutation equivariant respect input set point model simpler previous method doe not require spherical harmonic thomas et 2018 fuchs et 2020 still achieve competitive ter result addition equivariance model not limited space scaled larger dimensional space without signiﬁcant increase computation e n equivariant graph neural network evaluate method modelling dynamical system representation learning graph autoencoders ing molecular property dataset method report best competitive performance three experiment background section introduce relevant material ariance graph neural network later ment deﬁnition method equivariance let tg x x set transformation x abstract group g 2 say function φ x equivariant g exists equivalent transformation output space sg φ tg x sg φ x 1 practical example let φ function x xm 2 input set point cloud embedded space φ x 2 transformed set point cloud tg translation input set tg x x g sg equivalent translation output set sg transformation φ x translation equivariant translating input set tg x applying function φ tx x deliver result ﬁrst running function φ x applying equivalent translation output tg equation 1 fulﬁlled φ φ x work explore following three type equivariance set particle x translation equivariance translating input g 2 rn result equivalent translation output let shorthand xm g φ x g rotation reﬂection equivariance any thogonal matrix q 2 let qx shorthand qxm rotating input result equivalent rotation output qy φ qx permutation equivariance permuting input sults permutation output p φ p x p permutation row index note velocity v 2 unaffected tions transform equivalently rotation 2 permutation 3 method introduced section 3 satisfy three mentioned equivariant constraint graph neural network graph neural network permutation equivariant work operate graph structured data bruna et 2013 defferrard et 2016 kipf welling given graph g v e node vi 2 v edge eij 2 e deﬁne graph convolutional layer following notation gilmer et 2017 mij φe hl hl j aij mi x mij φh hl mi 2 hl 2 rnf embedding node vi layer aij edge attribute n represents set neighbor node vi finally φe φh edge node operation respectively commonly approximated multilayer perceptrons mlps equivariant graph neural network section present equivariant graph neural work egnns following notation background section consider graph g v e node vi 2 v edge eij 2 addition feature node embeddings hi 2 rnf also consider coordinate xi 2 rn associated graph node model preserve equivariance rotation lations set coordinate xi also preserve equivariance permutation set node v fashion gnns equivariant graph convolutional layer egcl take input set node embeddings hl hl 0 hl coordinate embeddings xl xl 0 xl edge information e eij output transformation concisely egcl hl xl e equation deﬁne layer following mij φe hl hl j xl j 2 aij 3 xl c x xl j φx mij 4 mi x mij 5 φh hl mi 6 notice main difference proposed method original graph neural network tion 2 found equation 3 equation 3 input relative squared distance two coordinate kxl edge operation φe embeddings hl hl j edge attribute aij also provided input edge operation gnn case case edge attribute incorporate edge value aij eij could also include additional edge information e n equivariant graph neural network equation 4 update position particle xi vector ﬁeld radial direction word position particle xi updated weighted sum relative difference weight sum provided output function φx rnf take input edge embedding mij previous edge operation output scalar value c chosen divide sum number element equation main difference model compared standard gnns reason equivariances 1 2 preserved proof appendix despite simplicity equivariant operation ﬂexible since embedding mij carry information whole graph not only given edge eij finally equation 5 6 follow update standard gnns equation 5 aggregate incoming message neighbor node n node vi tion 6 performs node operation φv take input aggregated message mi node emedding hl output updated node embedding analysis e n equivariance section analyze equivariance property model e 3 symmetry property 1 2 stated section word model translation equivariant x any translation vector g 2 rn also rotation reﬂection equivariant x any orthogonal matrix q 2 formally model satisﬁes g egcl qxl g hl provide formal proof appendix intuitively let consider hl feature already e n invariant see resultant edge embedding mij equation 3 also e n invariant addition hl only depends squared distance kxl e n invariant next equation 4 computes weighted sum difference xi added xi transforms vector preserve ariance see appendix finally last two equation 5 6 generate next layer main e n invariant since only depend hl mij saw e n invariant therefore output e n invariant e n equivariant xl inductively composition egcls also equivariant extending egnns vector type representation section propose slight modiﬁcation sented method explicitly keep track particle momentum some scenario useful not only obtain estimate particle velocity every layer also provide initial velocity value case not include momentum proposed method replacing equation 4 model following equation φv hl vl c x xl j φx mij xl 7 note extends egcl layer egcl hl xl vl e only difference broke coordinate update eq 4 two step ﬁrst compute velocity use velocity update position xl velocity vl scaled new function φv rn map node embedding hl scalar value notice initial velocity set zero v 0 0 equation 4 7 become exactly ﬁrst layer l 0 become equivalent next layer since φv output φx previous layer scalar value proof equivariance variant model appendix variant used experiment provide initial velocity system predict relative position change inferring edge given point cloud set node may not always provided adjacency matrix case assume fully connected graph node exchange message word neighborhood operator n equation 5 would include node graph except fully connected proach may not scale well large graph may want locally limit exchange message avoid overﬂow information similarly serviansky et 2020 kipf et 2018 present simple solution infer graph model aggregation operation model eq 5 following way mi x mij x eijmij 8 eij take value 1 edge node j 0 otherwise notice modify yet original equation used model change notation choose approximate relation eij following function eij φinf mij φinf rnf 0 1 1 resembles linear layer followed sigmoid function take input current edge embedding output soft estimation edge value modiﬁcation change e n property model since only operating message mij already e n invariant e n equivariant graph neural network gnn radial field tfn schnet egnn edge mij φe hl hl j aij mij φrf krl ijk rl ij mij p k wlkrl jihlk mij φcf krl ijk φs hl j mij φe hl hl j krl aij ˆ mij rl ijφx mij agg mi p mij mi p mij mi p mij mi p mij mi p mij ˆ mi c p ˆ mij node φh hl mi xl mi wllhl mi φh hl mi φh hl mi xl ˆ mi e n se 3 e n e n table comparison different work literature message passing framework notation created table aim provide clear simple way compare different method name left right graph neural network gilmer et 2017 radial field equivariant flow ohler et 2019 tensor field network thomas et 2018 schnet utt et equivariant graph neural network difference two point written rij xi related work group equivariant neural network demonstrated effectiveness wide variety task cohen welling 2016 2017 weiler cesa 2019 rezende et 2019 romero cordonnier 2021 recently various form method achieve e 3 se 3 equivariance proposed thomas et al 2018 fuchs et al 2020 utilize spherical harmonic compute basis formation allows transformation order representation downside method spherical harmonic need recomputed expensive currently extension method trary dimension unknown finzi et al 2020 parametrize transformation mapping kernel lie algebra method neural network output certain ations stochastic may undesirable horie et al 2020 proposes set isometric invariant equivairant transformation graph neural network ohler et al 2019 ohler et al 2020 propose e n equivariant network model point cloud method only deﬁned positional data node without any feature dimension another related line research concern message passing algorithm molecular data gilmer et 2017 sented message passing setting graph neural network quantum chemistry method permutation ant not translation rotation equivariant kondor et 2018 extends equivariance gnns neuron transforms speciﬁc way permutation extension only affect permutation group not translation rotation geometric space work utt et build e n invariant sage passing network inputting relative distance point klicpera et al addition relative distance includes modiﬁed message passing scheme analogous belief propagation considers gles directional information equivariant rotation also us bessel function spherical harmonic struct orthogonal basis anderson et al 2019 miller et al 2020 include 3 equivariance intermediate layer modelling behavior property lar data method also framed message passing framework contrast method achieves e n equivariance relationship existing method table 1 egnn equation detailed together some closest method literature message passing notation gilmer et 2017 ble aim provide simple way compare different algorithm structured three main row describe edge ii aggregation iii node update operation gnn algorithm previously introduced section egnn algorithm also equivalent description section 3 notation ha modiﬁed match edge aggregation node format equation rl ij xi notice except egnn rithms aggregation operation main difference arise edge operation algorithm call radial field e n equivariant update ohler et 2019 method e n equivariant however main limitation only operates x propagate node feature h among node method φrf modelled mlp tensor field network tfn thomas et 2018 instead propagate node embeddings h us spherical harmonic compute learnable weight kernel w k r 2 preserve se 3 equivariance expensive compute limited 3 dimensional space se 3 transformer fuchs et 2020 not included table interpreted extension tfn attention schnet utt et interpreted e n invariant graph neural network φcf receives input relative distance output continuous ﬁlter tion multiplies neighbor embeddings egnn differs method term performs two different update table row one related embeddings h another related coordinate e n equivariant graph neural network x two variable exchange information edge operation summary egnn retain ﬂexibility gnns remaining e n equivariant radial field algorithm without need compute expensive operation spherical harmonic experiment modelling dynamical system system dynamical system function deﬁnes time dence point set point geometrical space elling complex dynamic crucial variety plication control system chua et 2018 model based dynamic reinforcement learning nagabandi et 2018 physical system simulation grzeszczuk et 1998 watters et 2017 experiment forecast position set particle modelled simple interaction rule yet exhibit complex dynamic similarly fuchs et 2020 extended charged particle experiment kipf et 2018 3 dimensional space system consists 5 particle carry positive negative charge position velocity associated space system controlled physic rule particle attracted repelled depending charge equivariant task since rotation translation input set particle result transformation throughout entire trajectory dataset sampled trajectory training validation testing trajectory ha duration timesteps trajectory provided initial particle position p 0 p 0 1 p 0 5 2 initial velocity v 0 v 0 1 v 0 5 2 respective charge c 2 1 task estimate tions ﬁve particle timesteps optimize averaged mean squared error estimated position ground truth one implementation detail experiment used extension model includes velocity section input position p 0 ﬁrst layer coordinate model velocity v 0 initial velocity equation 7 norm kv 0 k also provided feature linear mapping charge input edge attribute aij cicj model output last layer coordinate xl estimated position compare method non equivariant graph neural network gnn cousin equivariant method radial field ohler et 2019 tensor field network se 3 transformer algorithm composed 4 layer trained condition batch size 100 epoch adam optimizer learning rate wa tuned independently model used 64 feature hidden layer radial field gnn egnn used swish activation function ramachandran et 2017 tfn se 3 transformer swept different number vector type feature chose provided best performance implementation detail provided appendix linear model simply considers motion equation p p 0 v 0 also included baseline also provide average forward pas time second model batch 100 sample gtx 1080 ti gpu method mse forward time linear se 3 transformer tensor field network graph neural network radial field egnn table mean squared error future position estimation system experiment forward time second batch size 100 sample running gtx gpu result shown table 2 model signiﬁcantly form equivariant alternative still efﬁcient term running time reduces error respect second best performing method 32 addition require computation spherical harmonic make time efﬁcient tensor field network se 3 transformer figure mean squared error experiment dial field gnn egnn method sweeping different amount training data analysis different number training sample want analyze performance egnn small large data regime following report similar experiment instead using training sample generated new training partition e n equivariant graph neural network sample sweep different amount data 100 sample compare performance egnn v gnn counterpart radial field algorithm result presented figure method outperforms radial field gnns small large data regime show egnn data efﬁcient gnns since require generalize rotation translation data ensemble ﬂexibility gnns larger data regime due high model bias radial field algorithm performs well data scarce unable learn subtlety dataset increase training size summary egnn beneﬁts high bias e n method ﬂexibility gnns graph autoencoder graph autoencoder learn unsupervised tions graph continuous latent space kipf welling simonovsky komodakis 2018 iment section use egnn build equivariant graph autoencoder explain graph coder beneﬁt equivariance show method outperforms standard gnn autoencoders provided datasets problem particularly interesting since embedding space scaled larger sion not limited 3 dimensional euclidean space similarly work kipf welling extended section liu et 2019 graph encoder z q g embeds graph g set latent node z zm 2 ber node n embedding size per node notice may reduce memory complexity store graph 2 mn n may depend certain approximation error tolerance differs variational autoencoder proposed simonovsky modakis 2018 embeds graph single vector z 2 rk cause reconstruction ally expensive since node decoded graph matched ground truth addition introduced graph generation representation learning method worth mentioning context graph compression method cand e recht 2009 used speciﬁcally compare equivariant graph task presented liu et 2019 graph g v e node feature h 2 adjacency matrix 2 0 1 embedded latent space z q h 2 following kipf welling liu et 2019 only interested reconstructing adjacency matrix since datasets work not contain node feature decoder g proposed liu et 2019 take input ding space z output reconstructed adjacency matrix ˆ g z decoder function deﬁned follows ˆ aij ge zi zj 1 1 exp w kzi b 9 w b only learnable parameter ge decoder edge function applied every pair node bedding reﬂects edge probability depend relative distance among node embeddings training loss deﬁned binary cross entropy mated ground truth edge l p ij bce ˆ aij aij symmetry problem stated autoencoder may seem straightforward implement ﬁrst sight some case strong limitation regarding try graph graph neural network convolution edge node graph function applied edge node some graph deﬁned only adjacency matrix may not input feature node reason difference among node relies only edge neighborhood topology therefore neighborhood two node exactly encoded embeddings clear example cycle graph example 4 node cycle graph provided figure 3 running graph neural network encoder node featureless cycle graph obtain exact embedding node make possible reconstruct edge original graph node embeddings cycle graph severe example node exact neighborhood topology symmetry present different way graph different edge distribution even including node feature not unique figure visual representation graph autoencoder 4 node cycle graph bottom row illustrates adding noise input graph break symmetry embedding allowing reconstruction adjacency matrix method break symmetry graph introduced liu et 2019 method introduces noise sampled gaussian distribution input node feature graph 0 σi noise low different representation node embeddings result graph decoded come e n equivariant graph neural network community small erdos renyi encoder bce error bce error baseline gnn radial field egnn figure table left report binary cross entropy error score test partition graph autoencoding experiment community small erdos renyi datasets figure right report score overﬁtting training partition 100 sample erdos renyi dataset different value sparsity pe gnn not able successfully sparse graph small pe value erdos renyi dataset even training testing small subset drawback network ha generalize new introduced noise distribution equivariant graph toencoder remain translation rotation equivariant sampled noise ﬁnd make generalization much easier another way looking ing sampled noise make node representation go structural positional srinivasan ribeiro 2019 e n equivariance may beneﬁcial case simply input noise input coordinate 0 σi 2 egnn output equivariant transformation xl output used embedding graph z xl input decoder equation dataset generated graph et 2018 liu et 2019 running original code et 2018 graph contain 12 node also generated second dataset using renyi generative model ela 2001 pling random graph initial number 7 node edge probability pe sampled graph training 500 validation 500 test datasets graph deﬁned adjacency matrix 2 0 1 implementation detail equivariant graph encoder composed egnn encoder followed decoder equation graph edge aij input edge attribute aij equation noise used break symmetry input coordinate 0 σi 2 ﬁrst layer ized one since working featureless graph mentioned encoder output equivariant transformation coordinate graph bedding input decoder z xl 2 use n 8 dimension embedding space pare egnn gnn cousin also compare adaptation gnn match setting liu et 2019 also include radial field algorithm additional baseline four model 4 layer 64 feature hidden layer swish activation function trained 100 epoch using adam optimizer learning rate detail provided appendix since number node larger number layer receptive ﬁeld gnn may not comprise whole graph make comparison unfair egnn avoid limitation model exchange sage among node edge information provided edge attribute aij aij result table figure 5 report binary cross entropy loss estimated ground truth edge error deﬁned percentage wrong predicted edge respect total amount potential edge score edge classiﬁcation number refer test partition also include baseline predicts edge missing ˆ aij standard gnn seems suffer symmetry problem provides worst performance introducing noise loss error decrease showing actually useful add noise input node finally egnn remains e n equivariant noise distribution provides best reconstruction error erdos renyi dataset close optimal community small dataset analysis reconstruction error different n embedding size reported appendix overﬁtting training set explained try problem showed egnn outperforms method given datasets although observed adding noise gnn improves result difﬁcult exactly measure impact symmetry limitation result independent factor alization training test set section conduct experiment train different model subset 100 erdos renyi graph embedding size n 16 aim overﬁt data evaluate method training data experiment gnn e n equivariant graph neural network task homo lumo µ g h u zpve unit mev mev mev k mev mev mev mev mev nmp 69 43 38 19 17 20 20 schnet 63 41 34 14 14 19 14 cormorant 61 34 38 20 21 21 22 68 46 35 14 14 14 13 lieconv 49 30 25 22 24 19 19 33 25 20 8 7 6 6 tfn 58 40 38 se 3 53 35 33 egnn 48 29 25 12 12 12 11 table mean absolute error molecular property prediction benchmark dataset us slightly different partition paper listed unable ﬁt training data properly egnn achieve perfect reconstruction close perfect sweep different pe sparsity value since symmetry limitation present sparse dense graph report score experiment right plot figure experiment showed e n equivariance prof performance embedding graph continuous space set node dimension even though simple reconstruction task think useful step towards generating graph molecule often graph edge decoded pairwise distance similarity node kipf welling liu et 2019 grover et 2019 metric eq 9 e n invariant additionally experiment also showed method successfully perform e n equivariant task higher dimensional space n 3 molecular data dataset ramakrishnan et 2014 ha become standard machine learning chemical property diction task dataset consists small molecule represented set atom 29 atom per molecule atom position associated ﬁve mensional node embedding describe atom type h c n f dataset label variety chemical property molecule mated regression property invariant translation rotation reﬂections atom position therefore model e 3 invariant highly suitable task imported dataset partition anderson et 2019 molecule training validation testing variety 12 chemical property estimated per molecule optimized report mean absolute error prediction ground truth implementation detail egnn receives input coordinate location atom provided equation 3 embedding atom property provided input node feature since invariant task also position static no need update particle position x running equation 4 previous experiment consequently tried manner notice any improvement updating not updating particle position skipping equation 4 model becomes e n variant analogous standard gnn relative squared norm pair point kxi inputted edge operation eq 3 additionally since not provided adjacency matrix molecule scale 29 node use extension model section infers soft estimation edge egnn network consists 7 layer 128 feature per hidden layer swish activation function operation preceded followed two layer mlps map node ding hl output egnn estimated property value implementation detail reported appendix compare nmp gilmer et 2017 schnet utt et cormorant anderson et 2019 miller et 2020 lieconv finzi et 2020 klicpera et tfn thomas et 2018 se 3 fuchs et 2020 result presented table method report competitive result property prediction task remaining relatively simple not introducing higher order representation angle spherical harmonic perhaps surprisingly outperform equivariant network consider higher order representation task only using representation relative distance deﬁne geometry molecule appendix e prove only positional information given no velocity higher order type feature geometry e n equivariant graph neural network completely deﬁned norm point e n word two collection point separated e n transformation considered identical relative norm point unique identiﬁer collection conclusion equivariant graph neural network receiving increasing interest natural medical science sent new tool analyzing molecule property work presented new e n equivariant deep chitecture graph computationally efﬁcient easy implement signiﬁcantly improves current wide range task believe property make ideally suited make direct impact topic drug discovery protein folding design new material well application computer vision acknowledgement would like thank patrick e support formalize invariance feature identiﬁcation proof reference anderson hy kondor cormorant covariant molecular neural network arxiv preprint ela random graph number cambridge university press bruna zaremba szlam lecun tral network locally connected network graph arxiv preprint cand e recht exact matrix completion via vex optimization foundation computational matics 9 6 chua calandra mcallister levine deep reinforcement learning handful trial ing probabilistic dynamic model arxiv preprint cohen welling group equivariant convolutional network balcan weinberger q ed proceeding international conference machine learning icml cohen welling steerable cnns national conference learning representation iclr defferrard bresson vandergheynst volutional neural network graph fast localized spectral ﬁltering advance neural information cessing system pp finzi stanton izmailov wilson eralizing convolutional neural network equivariance lie group arbitrary continuous data arxiv preprint fuchs worrall fischer welling se 3 equivariant attention network advance neural information processing system 33 gilmer schoenholz riley vinyals dahl neural message passing quantum istry arxiv preprint grover zweig ermon graphite iterative generative modeling graph international ence machine learning pp pmlr grzeszczuk terzopoulos hinton imator fast neural network emulation control model proceeding nual conference computer graphic interactive technique pp horie morita hishinuma ihara sume isometric transformation invariant ariant graph convolutional network arxiv preprint kipf fetaya wang welling zemel neural relational inference interacting system arxiv preprint kipf welling tion graph convolutional network arxiv preprint kipf welling variational graph arxiv preprint klicpera giri margraf unnemann fast directional message passing molecule arxiv preprint klicpera groß unnemann directional message passing molecular graph arxiv preprint ohler klein e equivariant ﬂows pling conﬁgurations system metric energy corr e n equivariant graph neural network ohler klein e equivariant ﬂows exact likelihood generative learning symmetric density arxiv preprint kondor son pan anderson trivedi covariant compositional network learning graph arxiv preprint liu kumar ba kiros swersky graph normalizing ﬂows advance neural information processing system pp miller geiger smidt e relevance rotationally equivariant convolution predicting molecular property arxiv preprint nagabandi kahn fearing levine neural network dynamic deep ment learning 2018 ieee international conference robotics automation icra pp ieee ramachandran zoph le searching activation function arxiv preprint ramakrishnan dral rupp von lilienfeld quantum chemistry structure property 134 kilo molecule scientiﬁc data 1 1 rezende racani ere higgins toth ariant hamiltonian ﬂows corr romero cordonnier group ariant vision ternational conference learning representation url utt arbabzadah chmiela uller tkatchenko insight deep tensor neural network nature communication 8 1 utt kindermans sauceda chmiela tkatchenko uller schnet ﬁlter convolutional neural network modeling quantum interaction arxiv preprint serviansky segol shlomi cranmer gross maron lipman learning graph set advance neural information processing system 33 simonovsky komodakis graphvae towards generation small graph using variational autoencoders international conference artiﬁcial neural work pp springer srinivasan ribeiro equivalence positional node embeddings structural graph sentations arxiv preprint thomas smidt kearnes yang li kohlhoff riley tensor ﬁeld network neural network point cloud arxiv preprint uy pham hua nguyen yeung revisiting point cloud classiﬁcation new mark dataset classiﬁcation model data proceeding ieee international conference computer vision pp watters zoran weber battaglia pascanu tacchetti visual interaction network ing physic simulator video advance neural information processing system weiler cesa general e 2 steerable cnns wallach larochelle beygelzimer fox garnett r ed advance neural information processing system 32 annual conference neural information processing system ying ren hamilton leskovec graphrnn generating realistic graph deep regressive model arxiv preprint 2018