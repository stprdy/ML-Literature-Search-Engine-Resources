gnnexplainer generating explanation graph neural network rex dylan jiaxuan marinka jure computer science stanford university rexying dtsbourg jiaxuan marinka jure abstract graph neural network gnns powerful tool machine learning graph gnns combine node feature information graph structure recursively passing neural message along edge input graph however corporating graph structure feature information lead complex model explaining prediction made gnns remains unsolved propose gnnexplainer ﬁrst general approach providing pretable explanation prediction any model any machine learning task given instance gnnexplainer identiﬁes compact subgraph structure small subset node feature crucial role gnn prediction gnnexplainer generate consistent concise explanation entire class instance formulate gnnexplainer optimization task maximizes mutual information gnn tion distribution possible subgraph structure experiment synthetic graph show approach identify important graph structure well node feature outperforms alternative baseline approach explanation accuracy gnnexplainer provides variety beneﬁts ability visualize semantically relevant structure interpretability giving insight error faulty gnns 1 introduction many application including social information chemical biological domain data naturally modeled graph 9 41 49 graph powerful data representation challenging work require modeling rich relational information well node feature information 45 46 address challenge graph neural network gnns emerged machine learning graph due ability recursively incorporate information neighboring node graph naturally capturing graph structure node feature 16 21 40 44 despite strength gnns lack transparency not easily allow explanation prediction yet ability understand gnn prediction important useful several reason increase trust gnn model ii improves model transparency growing number application pertaining fairness privacy safety challenge 11 iii allows practitioner get understanding network characteristic identify correct systematic pattern mistake made model deploying real world currently no method explaining gnns recent approach explaining type neural network taken one two main route one line work locally approximates conference neural information processing system neurips 2019 vancouver canada gnn model training prediction explaning gnn prediction gnnexplainer figure 1 gnnexplainer provides interpretable explanation prediction made any gnn model any machine learning task shown hypothetical node classiﬁcation task gnn model φ trained social interaction graph predict future sport activity given trained gnn φ prediction ˆ yi basketball person vi gnnexplainer generates explanation identifying small subgraph input graph together small subset node feature shown right inﬂuential ˆ yi examining explanation ˆ yi see many friend one part vi social circle enjoy ball game gnn predicts vi like basketball similarly examining explanation ˆ yj see vj friend friend friend enjoy water beach sport gnn predicts ˆ yj model simpler surrogate model probed explanation 25 29 30 method carefully examine model relevant feature ﬁnd good qualitative interpretation high level feature 6 13 27 32 identify inﬂuential input instance 23 38 however approach fall short ability incorporate relational information essence graph since aspect crucial success machine learning graph any explanation gnn prediction leverage rich relational information provided graph well node feature propose gnnexplainer approach explaining prediction made gnns plainer take trained gnn prediction return explanation form small subgraph input graph together small subset node feature inﬂuential prediction figure 1 approach explain prediction any gnn any machine learning task graph including node classiﬁcation link prediction graph classiﬁcation handle well explanation case explanation gnnexplainer explains gnn prediction one particular instance node label new link label case explanation gnnexplainer provides explanation consistently explains set instance node given class gnnexplainer speciﬁes explanation rich subgraph entire graph gnn wa trained subgraph maximizes mutual information gnn prediction achieved formulating mean ﬁeld variational approximation learning graph mask selects important subgraph gnn computation graph simultaneously gnnexplainer also learns feature mask mask unimportant node feature figure 1 evaluate gnnexplainer synthetic well graph experiment show gnnexplainer provides consistent concise explanation gnn prediction synthetic graph planted network motif play role determining node label show gnnexplainer accurately identiﬁes well node feature determine node label outperforming alternative baseline approach explanation accuracy using two datasets show gnnexplainer provide important domain insight robustly identifying important graph structure node feature inﬂuence gnn prediction speciﬁcally using molecular graph social interaction network show gnnexplainer identify important graph structure chemical group ring structure molecule star structure reddit thread overall experiment demonstrate gnnexplainer provides consistent concise explanation model different machine learning task graph 2 related work although problem explaining gnns not related problem interpretability neural debugging received substantial attention machine learning high level group interpretability method neural network two main family 2 b figure 2 gnn computation graph gc green orange making prediction ˆ node some edge gc form important neural pathway green allow useful node information propagated across gc aggregated v prediction edge not orange however gnn need aggregate important well unimportant message form prediction node v dilute signal accumulated v neighborhood goal gnnexplainer identify small set important feature pathway green crucial prediction addition g green gnnexplainer identiﬁes feature dimension g node important prediction learning node feature mask method ﬁrst family formulate simple proxy model full neural network done way usually learning locally faithful approximation around prediction example linear model 29 set rule representing sufﬁcient condition prediction 3 25 47 method second family identify important aspect computation example feature gradient 13 43 backpropagation neuron contribution input feature 6 31 32 counterfactual reasoning 19 however saliency map 43 produced method shown misleading some instance 2 prone issue like gradient saturation 31 32 issue exacerbated discrete input graph adjacency matrix since gradient value large only small interval approach not suitable explaining prediction made neural network graph instead creating new inherently interpretable model interpretability method 1 14 15 17 23 38 consider model black box probe relevant information however no work ha done leverage relational structure like graph lack method explaining prediction data problematic many case prediction graph induced complex combination node path edge example some task edge important only another alternative path exists graph form cycle two feature only considered together accurately predict node label 10 12 joint contribution thus not modeled simple linear combination individual contribution finally recent gnn model augment interpretability via attention mechanism 28 33 34 however although learned edge attention value indicate important graph structure value prediction across node thus contradicts many application edge essential predicting label one node not label another node furthermore approach either limited speciﬁc gnn architecture not explain prediction jointly considering graph structure node feature information 3 formulating explanation graph neural network let g denote graph edge e node v associated node feature x xn xi without loss generality consider problem explaining node classiﬁcation task see section task let f denote label function node f v 1 c map every node v one c class gnn model φ optimized node training set used prediction approximate f new node background graph neural network layer l update gnn model φ involves three key computation 4 45 46 1 first model computes neural message every pair node message node pair vi vj function msg vi vj representation j previous layer relation rij node ml ij msg j rij 2 second node vi gnn aggregate 3 message vi neighborhood nvi calculates aggregated message mi via aggregation method agg 16 35 l agg ml nvi neighborhood node vi whose deﬁnition depends particular gnn variant 3 finally gnn take aggregated message l along vi representation previous layer transforms obtain vi representation hl layer l hl update l ﬁnal embedding node vi l layer computation zi hl gnnexplainer provides explanation any gnn formulated term msg agg update computation gnnexplainer problem formulation key insight observation computation graph node v deﬁned gnn aggregation figure 2 fully determines information gnn us generate prediction ˆ node particular v computation graph tell gnn generate v embedding let u denote computation graph gc v associated binary adjacency matrix ac v 0 1 associated feature set xc v v gnn model φ learns conditional distribution pφ xc random variable representing label 1 c indicating probability node belonging c class gnn prediction given ˆ φ gc v xc v meaning fully determined model φ graph structural information gc v node feature information xc v effect observation implies only need consider graph structure gc v node feature xc v explain ˆ figure formally gnnexplainer generates explanation prediction ˆ g xf g small subgraph computation graph x associated feature g xf small subset node feature masked mask f xf xf j g important explaining ˆ figure 4 gnnexplainer next describe approach gnnexplainer given trained gnn model φ prediction explanation section set prediction explanation section gnnexplainer generate explanation identifying subgraph computation graph subset node feature inﬂuential model φ prediction case explaining set prediction gnnexplainer aggregate individual explanation set automatically summarize prototype conclude section discussion gnnexplainer used any machine learning task graph including link prediction graph classiﬁcation section explanation given node v goal identify subgraph g associated feature x important gnn prediction ˆ assume x small subset node feature later discus automatically determine dimension node feature need included explanation section formalize notion importance using mutual information mi formulate gnnexplainer following optimization framework max g mi g x h g x x 1 node v mi quantiﬁes change probability prediction ˆ φ gc xc v computation graph limited explanation subgraph g node feature limited x example consider situation vj vi vj vi removing vj gc vi strongly decrease probability prediction ˆ yi node vj good counterfactual explanation prediction vi similarly consider situation vj vk vi vj vk vi removing edge vj vk strongly decrease probability prediction ˆ yi absence edge good counterfactual explanation prediction vi examining eq 1 see entropy term h constant φ ﬁxed trained gnn result maximizing mutual information predicted label distribution 4 explanation g x equivalent minimizing conditional entropy h g x x expressed follows h x x log pφ x 2 explanation prediction ˆ thus subgraph g minimizes uncertainty φ gnn computation limited g effect g maximizes probability ˆ figure 2 obtain compact explanation impose constraint g size g ha km node effect implies gnnexplainer aim denoise gc taking km edge give highest mutual information prediction gnnexplainer optimization framework direct optimization gnnexplainer objective not tractable gc ha exponentially many subgraphs g candidate explanation ˆ thus consider fractional adjacency subgraphs g 0 1 enforce subgraph constraint j k j k j continuous relaxation interpreted variational approximation distribution subgraphs gc particular treat g random graph variable objective eq 2 becomes min g g x x 3 convexity assumption jensen inequality give following upper bound min g h eg g x x 4 practice due complexity neural network convexity assumption doe not hold however experimentally found minimizing objective regularization often lead local minimum corresponding explanation tractably estimate eg use variational approximation decompose g multivariate bernoulli distribution pg g q j k j k allows u estimate expectation respect approximation thereby obtaining j k entry represents expectation whether edge vj vk exists observed empirically approximation together regularizer promoting discreteness 40 converges good local minimum despite gnns conditional entropy equation 4 optimized replacing eg g optimized masking computation graph adjacency matrix ac denotes mask need learn multiplication σ denotes sigmoid map mask 0 1 some application instead ﬁnding explanation term model conﬁdence user care doe trained model predict certain class label make trained model predict desired class label modify conditional entropy objective equation 4 cross entropy objective label class model answer query computationally efﬁcient version gnnexplainer objective optimize using gradient descent follows min c x 1 c log pφ ac x xc 5 masking approach also found neural relational inference 22 albeit different motivation objective lastly compute multiplication σ ac remove low value thresholding arrive explanation g gnn model prediction ˆ node joint learning graph structural node feature information identify node feature important prediction ˆ gnnexplainer learns feature selector f node explanation g instead deﬁning x consists node feature typed edge deﬁne g 0 1 ce number edge type label class predicted label class gnn model explained answering doe trained model predict certain class label make trained model predict desired class label answered using label class 5 x gnnexplainer considers xf subset feature node g deﬁned binary feature selector f 0 1 figure xf xf j xf j xj xj tk fti 1 6 xf j ha node feature not masked explanation g x jointly optimized maximizing mutual information objective max g f mi g f h g x xf 7 represents modiﬁed objective function eq 1 considers structural node feature information generate explanation prediction ˆ learning binary feature selector specify xf x f act feature mask need learn intuitively particular feature not important corresponding weight gnn weight matrix take value close zero effect implies masking feature doe not decrease predicted probability ˆ conversely feature important masking would decrease predicted probability however some case approach ignores feature important prediction take value close zero address issue marginalize feature subset use monte carlo estimate sample empirical marginal distribution node x training 48 use reparametrization trick 20 backpropagate gradient eq 7 feature mask particular backpropagate random variable x reparametrize x x z x p j fj z random variable sampled empirical distribution kf parameter representing maximum number feature kept explanation integrating additional constraint explanation impose property tion extend gnnexplainer objective function eq 7 regularization term example use entropy encourage structural node feature mask discrete gnnexplainer encode constraint technique like lagrange multiplier constraint additional regularization term include number regularization term produce explanation desired property penalize large size explanation adding sum element mask paramters regularization term finally important note explanation must valid computation graph particular explanation g x need allow gnn neural message ﬂow towards node v gnn make prediction ˆ importantly gnnexplainer automatically provides explanation represent valid computation graph optimizes structural mask across entire computation graph even disconnected edge important neural not selected explanation not inﬂuence gnn prediction effect implies explanation g tends small connected subgraph explanation graph prototype output explanation section small subgraph input graph small subset associated node feature inﬂuential single prediction answer question like gnn predict given set node label c need obtain global explanation class goal provide insight identiﬁed subgraph particular node relates graph structure explains entire class gnnexplainer provide explanation based graph alignment prototype approach ha two stage first given class c any set prediction want explain ﬁrst choose reference node vc example computing mean embedding node assigned take explanation g vc reference vc align explanation node assigned class finding optimal matching large graph challenging practice however instance gnnexplainer generates small graph section thus pairwise graph matchings efﬁciently computed second aggregate aligned adjacency matrix graph prototype aproto using example robust approach prototype aproto give insight graph pattern shared node belong class one study prediction particular node comparing explanation node prediction returned explanation approach prototype see appendix information 6 gnnexplainer model extension any machine learning task graph addition explaining node classiﬁcation plainer provides explanation link prediction graph classiﬁcation no change optimization algorithm predicting link vj vk gnnexplainer learns two mask x vj x vk endpoint link classifying graph adjacency matrix eq 5 union adjacency matrix node graph whose label want explain however note graph classiﬁcation unlike node classiﬁcation due aggregation node ding no longer true explanation g necessarily connected subgraph depending application some scenario chemistry explanation functional group connected one extract largest connected component explanation any gnn model modern gnns based message passing architecture input graph message passing computation graph composed many different way gnnexplainer account thus gnnexplainer applied graph convolutional network 21 gated graph sequence neural network 26 jumping knowledge network 36 attention network 33 graph network 4 gnns various node aggregation scheme 7 5 18 16 40 39 35 nns 8 gnn 42 many gnn architecture computational complexity number parameter gnnexplainer optimization depends size computation graph gc node v whose prediction aim explain particular gc v adjacency matrix ac v equal size mask need learned gnnexplainer however since computation graph typically relatively small compared size exhaustive neighborhood hop neighborhood 21 neighborhood 39 neighborhood attention 33 gnnexplainer effectively generate explanation even input graph large 5 experiment begin describing graph alternative baseline approach experimental setup present experiment explaining gnns node classiﬁcation graph classiﬁcation task qualitative quantitative analysis demonstrates gnnexplainer accurate effective identifying explanation term graph structure node feature synthetic datasets construct four kind node classiﬁcation datasets table 1 1 shape start base ba graph 300 node set 80 house network motif attached randomly selected node base graph resulting graph perturbed adding random edge node assigned 4 class based structural role motif 3 type role top middle bottom node house therefore 4 different class corresponding node top middle bottom house node not belong house 2 dataset union two graph node normally distributed feature vector assigned one 8 class based structural role community membership 3 start base balanced binary tree 80 cycle motif attached random node base graph 4 except grid motif attached base tree graph place cycle motif datasets consider two graph classiﬁcation datasets 1 mutag dataset molecule graph labeled according mutagenic effect bacterium typhimurium 10 2 dataset graph representing online discussion thread reddit graph node user participating thread edge indicate one user replied another user comment graph labeled according type user interaction thread contain interaction contain interaction 37 alternative baseline approach many explainability method not directly applied graph section 2 nevertheless consider following alternative approach provide insight prediction made gnns 1 grad method compute gradient gnn loss function respect adjacency matrix associated node feature similar saliency map approach 2 att graph attention gnn gat 33 learns attention weight edge computation graph use proxy measure edge importance att doe consider graph structure doe not explain using node feature only explain gat model furthermore att not obvious attention weight need used edge 7 base motif node feature none none community id graph structure graph structure node feature information explanation content explanation accuracy att grad gnnexplainer community 1 community 0 none graph structure graph structure table 1 illustration synthetic datasets refer synthetic datasets detail together performance evaluation gnnexplainer alternative baseline explainability approach b b computation graph grad ground truth att computation graph grad att ground truth gnnexplainer gnnexplainer figure 3 evaluation explanation shown exemplar explanation subgraphs node classiﬁcation task four synthetic datasets method provides explanation red node prediction importance since neighbor node also neighbor node due cycle edge importance thus computed average attention weight across layer setup implementation detail dataset ﬁrst train single gnn dataset use grad gnnexplainer explain prediction made gnn note att baseline requires using graph attention architecture like gat 33 thus train separate gat model dataset use learned edge attention weight explanation hyperparameters km kf control size subgraph feature explanation respectively informed prior knowledge dataset synthetic datasets set km size ground truth datasets set km set kf 5 datasets ﬁx weight regularization hyperparameters across node graph classiﬁcation experiment refer reader appendix training detail code datasets available result investigate question doe gnnexplainer provide sensible explanation explanation compare knowledge doe gnnexplainer perform various prediction task explain prediction made different gnns 1 quantitative analysis result node classiﬁcation datasets shown table explanation synthetic datasets use calculate explanation accuracy explanation method speciﬁcally formalize explanation problem binary classiﬁcation task edge explanation treated label importance weight given explainability method viewed prediction score better explainability method predicts mutag mutag computation graph grad ground truth att computation graph grad att ground truth gnnexplainer gnnexplainer b topic reaction ring structure no group 2 expert answering multiple question figure 4 evaluation explanation shown exemplar explanation subgraphs graph classiﬁcation task two datasets mutag 8 graph classification node classification b c cl h n f br p nak lica not applicable not applicable molecular graph node feature computation graph red node node feature input gnn node structural role molecule mutagenicity gnn prediction ground truth feature importance gnnexplainer grad att figure 5 visualization feature important gnn prediction shown representative molecular graph mutag dataset top tance associated graph feature visualized heatmap bottom contrast baseline gnnexplainer correctly identiﬁes feature important predicting molecule mutagenicity c h n atom shown computation graph red node dataset top gnnexplainer successfully identiﬁes node feature important predicting structural role node baseline method fail high score edge explanation thus achieves higher explanation accuracy result show gnnexplainer outperforms alternative approach average gnnexplainer achieves higher accuracy hardest dataset 2 qualitative analysis result shown figure prediction task no node feature gnnexplainer correctly identiﬁes network motif explain node label structural label figure 3 illustrated ﬁgures house cycle tree motif identiﬁed gnnexplainer not baseline method figure 4 investigate explanation graph classiﬁcation task mutag example color indicate node feature represent atom hydrogen h carbon c etc gnnexplainer correctly identiﬁes carbon ring well chemical group known mutagenic 10 example see graph row figure high degree node simultaneously connect many low degree node make sense qa thread reddit typically expert answer many different question 24 conversely observe discussion pattern commonly exhibit pattern row figure since thread reddit usually reaction single topic 24 hand grad att method give incorrect incomplete explanation example baseline method miss cycle motif mutag dataset complex grid motif dataset furthermore although edge attention weight att interpreted importance score message passing weight shared across node input graph att fails provide high quality explanation essential criterion explanation must interpretable provide qualitative understanding relationship input node prediction requirement implies explanation easy understand remaining exhaustive mean gnn explainer take account structure underlying graph well associated feature available figure 5 show result experiment gnnexplainer jointly considers structural information well information small number feature gnnexplainer indeed highlight compact feature representation figure 5 approach struggle cope added noise giving high importance score irrelevant feature dimension 6 conclusion present gnnexplainer novel method explaining prediction any gnn any based machine learning task without requiring modiﬁcation underlying gnn architecture show gnnexplainer leverage recursive scheme graph neural network identify important graph pathway well highlight relevant node feature information passed along edge pathway problem explainability prediction ha received substantial attention recent literature work unique sense present approach operates relational rich node provides straightforward interface making sense gnn prediction debugging gnn model identifying systematic pattern mistake explanation shown two datasets node feature mutag 9 acknowledgment jure leskovec chan zuckerberg biohub investigator gratefully acknowledge support darpa ased msc nih no mobilize aro no muri iarpa no hfc nsf no cines hdr stanford data science initiative chan zuckerberg biohub amazon boeing docomo huawei hitachi observe siemens ust global government authorized reproduce distribute reprint governmental purpose notwithstanding any copyright notation thereon any opinion ﬁndings conclusion ommendations expressed material author not necessarily reﬂect view policy endorsement either expressed implied darpa nih onr government reference 1 adadi berrada peeking inside survey explainable artiﬁcial intelligence xai ieee access 2018 2 adebayo gilmer muelly goodfellow hardt kim sanity check saliency map neurips 2018 3 gethsiyal augasta kathirvalavakumar reverse engineering neural network rule extraction classiﬁcation problem neural processing letter 35 2 april 2012 4 peter w battaglia jessica b hamrick victor bapst alvaro vinicius baldi mateusz malinowski andrea tacchetti david raposo adam santoro ryan faulkner et al relational inductive bias deep learning graph network 2018 5 chen zhu song stochastic training graph convolutional network variance reduction icml 2018 6 jianbo chen le song martin j wainwright michael jordan learning explain perspective model interpretation arxiv preprint 2018 7 jie chen tengfei cao xiao fastgcn fast learning graph convolutional network via importance sampling iclr 2018 8 chen li bruna supervised community detection line graph neural network iclr 2019 9 cho myers leskovec friendship mobility user movement social network kdd 2011 10 debnath et al relationship mutagenic aromatic heteroaromatic nitro compound correlation molecular orbital energy hydrophobicity journal medicinal chemistry 34 2 1991 11 kim towards rigorous science interpretable machine learning arxiv 12 duvenaud et al convolutional network graph learning molecular ﬁngerprints nip 2015 13 erhan bengio courville vincent visualizing feature deep network university montreal 1341 3 2009 14 fisher rudin dominici model wrong many useful variable importance proprietary misspeciﬁed prediction model using model class reliance january arxiv 15 guidotti et al survey method explaining black box model acm comput 51 5 2018 10 16 hamilton ying leskovec inductive representation learning large graph nip 2017 17 hooker discovering additive structure black box function kdd 2004 18 huang zhang rong huang adaptive sampling towards fast graph tation learning neurips 2018 19 bo kang jefrey lijfﬁjt tijl de bie explaine approach explaining network link prediction 2019 20 diederik p kingma max welling variational bayes neurips 2013 21 kipf welling classiﬁcation graph convolutional network iclr 2016 22 thomas kipf ethan fetaya wang max welling richard zemel neural relational inference interacting system icml 2018 23 koh liang understanding prediction via inﬂuence function icml 2017 24 srijan kumar william l hamilton jure leskovec dan jurafsky community interaction conﬂict web www page 2018 25 lakkaraju kamar caruana leskovec interpretable explorable tions black box model 2017 26 li tarlow brockschmidt zemel gated graph sequence neural network 2015 27 lundberg lee uniﬁed approach interpreting model prediction nip 2017 28 neil et al interpretable graph convolutional neural network inference noisy knowledge graph workshop neurips 2018 29 ribeiro singh guestrin trust explaining prediction any classiﬁer kdd 2016 30 schmitz aldrich gouws algorithm extraction decision tree artiﬁcial neural network ieee transaction neural network 1999 31 shrikumar greenside kundaje learning important feature propagating activation difference icml 2017 32 sundararajan taly yan axiomatic attribution deep network icml 2017 33 velickovic cucurull casanova romero li bengio graph attention network iclr 2018 34 xie grossman crystal graph convolutional neural network accurate interpretable prediction material property phys rev 2018 35 xu hu leskovec jegelka powerful graph neural network icrl 2019 36 xu li tian sonobe kawarabayashi jegelka representation learning graph jumping knowledge network icml 2018 37 pinar yanardag svn vishwanathan deep graph kernel kdd page acm 2015 38 yeh kim yen ravikumar representer point selection explaining deep neural network neurips 2018 11 39 ying chen eksombatchai hamilton leskovec graph convolutional neural network recommender system kdd 2018 40 ying morris ren hamilton leskovec hierarchical graph representation learning differentiable pooling neurips 2018 41 liu ying pande leskovec graph convolutional policy network molecular graph generation 2018 42 rex ying leskovec graph neural network icml 2019 43 zeiler fergus visualizing understanding convolutional network eccv 2014 44 zhang chen link prediction based graph neural network nip 2018 45 zhang peng zhu deep learning graph survey 2018 46 zhou cui zhang yang liu sun graph neural network review method application 2018 47 zilke loza mencia janssen deepred rule extraction deep neural network discovery science springer international publishing 2016 48 zintgraf cohen adel welling visualizing deep neural network decision prediction difference analysis iclr 2017 49 zitnik agrawal leskovec modeling polypharmacy side effect graph convolutional network bioinformatics 34 2018 12