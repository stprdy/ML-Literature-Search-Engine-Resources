graph neural network review method application jie zhou ganqu cui shengding hu zhengyan zhang cheng yang b zhiyuan liu lifeng wang c changcheng li c maosong sun department computer science technology tsinghua university beijing china b school computer science beijing university post telecommunication china c tencent incorporation shenzhen china r c l e n f keywords deep learning graph neural network b r c lot learning task require dealing graph data contains rich relation information among element modeling physic system learning molecular ﬁngerprints predicting protein interface classifying disease demand model learn graph input domain learning data like text image reasoning extracted structure like dependency tree sentence scene graph image important research topic also need graph reasoning model graph neural network gnns neural model capture dependence graph via message passing node graph recent year variant gnns graph convolutional network gcn graph attention network gat graph recurrent network grn demonstrated performance many deep learning task survey propose general design pipeline gnn model discus variant component tematically categorize application propose four open problem future research introduction graph kind data structure model set object node relationship edge recently research analyzing graph machine learning receiving attention great expressive power graph graph used denotation large number system across various area including social science social network wu et 2020 natural science physical system sanchez et 2018 battaglia et 2016 interaction network fout et 2017 knowledge graph hamaguchi et 2017 many research area khalil et 2017 unique data structure machine learning graph analysis focus task node cation link prediction clustering graph neural network gnns deep learning based method operate graph domain due convincing performance gnn ha become widely applied graph analysis method recently following paragraph illustrate fundamental motivation graph neural network ﬁrst motivation gnns root history neural network graph ninety recursive neural network ﬁrst utilized directed acyclic graph sperduti starita 1997 frasconi et 1998 afterwards recurrent neural network feedforward neural network introduced literature respectively scarselli et 2009 micheli 2009 tackle cles although successful universal idea behind method building state transition system graph iterate gence constrained extendability representation ability recent advancement deep neural network especially convolutional neural network cnns lecun et 1998 result rediscovery gnns cnns ability extract localized spatial feature compose construct highly expressive tions led breakthrough almost machine learning area started new era deep learning lecun et 2015 key cnns local connection shared weight use multiple layer lecun et 2015 also great importance solving problem graph however cnns only operate regular euclidean data like image grid text sequence data structure regarded instance graph therefore corresponding author address zhou cui hu mail zhang yang liuzy liu fandywang wang harrychli li sm sun 1 indicates equal contribution content list available sciencedirect ai open journal homepage received 16 september 2020 received revised form 15 december 2020 accepted 27 january 2021 available online 8 april 2021 2021 author published elsevier behalf keai communication open access article cc license ai open 1 2020 straightforward generalize cnns graph shown fig 1 hard deﬁne localized convolutional ﬁlters pooling operator hinders transformation cnn euclidean domain domain extending deep neural model domain generally referred geometric deep learning ha emerging research area bronstein et 2017 umbrella term deep learning graph receives enormous attention motivation come graph representation learning cui et hamilton et zhang et cai et 2018 goyal ferrara 2018 learns represent graph node edge subgraphs vector ﬁeld graph analysis traditional machine learning approach usually rely hand engineered feature limited inﬂexibility high cost following idea representation learning success word embedding mikolov et 2013 deepwalk perozzi et 2014 regarded ﬁrst graph embedding method based representation learning applies skipgram model mikolov et 2013 generated random walk similar approach grover leskovec 2016 line tang et 2015 tadw yang et 2015 also achieved throughs however method suffer two severe drawback hamilton et first no parameter shared node encoder lead computationally inefﬁciency since mean number parameter grows linearly number node second direct embedding method lack ability alization mean not deal dynamic graph generalize new graph based cnns graph embedding variant graph neural work gnns proposed collectively aggregate information graph structure thus model input output consisting element dependency exists several comprehensive review graph neural work bronstein et al 2017 provide thorough review geometric deep learning present problem difﬁculties solution plication future direction zhang et al propose another comprehensive overview graph convolutional network however mainly focus convolution operator deﬁned graph investigate computation module gnns skip connection pooling operator paper zhang et al wu et al chami et al 2020 survey paper gnns mainly focus model gnn wu et al categorize gnns four group recurrent graph neural network convolutional graph neural network graph autoencoders graph neural network zhang et al give systematic overview different graph deep learning method chami et al 2020 propose graph encoder decoder model unify network embedding graph neural network model paper provides different taxonomy mainly focus classic gnn model besides summarize variant gnns different graph type also provide detailed summary gnns application different domain also several survey focusing some speciﬁc graph learning ﬁelds sun et al 2018 chen et al give detailed overview adversarial learning method graph including graph data attack defense lee et al provide review graph attention model paper proposed yang et al 2020 focus heterogeneous graph representation learning node edge multiple type huang et al 2020 review existing gnn model dynamic graph peng et al 2020 summarize graph embeddings method combinatorial optimization conclude gnns erogeneous graph dynamic graph combinatorial optimization section section section respectively paper provide thorough review different graph neural network model well systematic taxonomy application summarize contribution provide detailed review existing graph neural network model present general design pipeline discus variant module also introduce research theoretical empirical analysis gnn model systematically categorize application divide cation structural scenario scenario present several major application corresponding method scenario propose four open problem future research provide thorough analysis problem propose future research direction rest survey organized follows section 2 present general gnn design pipeline following pipeline discus step detail review gnn model variant detail included section 3 section section 7 revisit research work theoretical empirical analysis gnns section 8 introduce several major application graph neural network applied structural scenario scenario scenario section 9 propose four open problem graph neural network well several future research direction ﬁnally conclude survey section 10 general design pipeline gnns paper introduce model gnns designer view ﬁrst present general design pipeline designing gnn model section give detail step selecting computational module considering graph type scale designing loss function section 3 4 5 respectively ﬁnally use example trate design process gnn speciﬁc task section fig left image euclidean space right graph space zhou et al ai open 1 2020 58 later section denote graph g ðv eþ jvj n number node graph jej ne number edge 2 adjacency matrix graph representation learning use hv ov hidden state output vector node detailed description notation could found table section present general design pipeline gnn model speciﬁc task speciﬁc graph type generally pipeline tains four step 1 ﬁnd graph structure 2 specify graph type scale 3 design loss function 4 build model using computational ules give general design principle some background knowledge section design detail step discussed later section find graph structure ﬁrst ﬁnd graph structure application usually two scenario structural scenario scenario structural scenario graph structure explicit application application molecule physical system knowledge graph scenario graph implicit ﬁrst build graph task building word graph text building scene graph image get graph later design process tempts ﬁnd optimal gnn model speciﬁc graph specify graph type scale get graph application ﬁnd graph type scale graph complex type could provide information node connection graph usually categorized graph edge directed graph directed one node another provide information undirected graph edge undirected graph also regarded two directed edge graph node edge mogeneous graph type node edge different type heterogeneous graph type node edge play important role heterogeneous graph considered graph input feature topology graph vary time graph regarded dynamic graph time information carefully considered dynamic graph note category orthogonal mean type combined one deal dynamic directed heterogeneous graph also several graph type designed different task hypergraphs signed graph not enumerate type important idea consider additional formation provided graph specify graph type additional information provided graph type considered design process graph scale no clear classiﬁcation criterion small large graph criterion still changing development computation device speed memory gpus paper adjacency matrix graph laplacian graph space complexity not stored processed device regard graph graph some sampling method considered design loss function step design loss function based task type training setting graph learning task usually three kind task task focus node include node classiﬁcation node regression node clustering etc node classiﬁcation try categorize node several class node regression predicts continuous value node node clustering aim partition node several disjoint group similar node group task edge classiﬁcation link prediction require model classify edge type predict whether edge existing two given node task include graph classiﬁcation graph regression graph matching need model learn graph representation perspective supervision also categorize graph learning task three different training setting supervised setting provides labeled data training setting give small amount labeled node large amount unlabeled node training test phase transductive setting requires model predict label given unlabeled node inductive setting provides new unlabeled node distribution infer node edge classiﬁcation task recently mixed scheme undertaken wang leskovec 2020 rossi et al 2018 craving new path towards mixed setting unsupervised setting only offer unlabeled data model ﬁnd pattern node clustering typical unsupervised learning task task type training setting design speciﬁc loss function task example classiﬁcation task loss used labeled node training set build model using computational module finally start building model using computational module some commonly used computational module table 1 notation used paper notation description rm euclidean space scalar vector matrix matrix transpose identity matrix dimension n gw convolution gw x n nv number node graph ne number edge graph n v neighborhood set node v v vector node v time step hv hidden state node v ht v hidden state node v time step ot v output node v time step evw feature edge node v w ek feature edge label k wi ui wo uo matrix computing bi bo vector computing ρ alternative function σ logistic sigmoid function tanh hyperbolic tangent function leakyrelu leakyrelu function multiplication operation k vector concatenation zhou et al ai open 1 2020 59 propagation module propagation module used propagate information node aggregated information could capture feature topological information propagation module convolution operator recurrent operator usually used aggregate information neighbor skip connection operation used gather information historical representation node mitigate problem sampling module graph large sampling module usually needed conduct propagation graph sampling module usually combined propagation module pooling module need representation subgraphs graph pooling module needed extract mation node computation module typical gnn model usually built combining typical architecture gnn model illustrated middle part fig 2 convolutional operator recurrent operator sampling module skip connection used propagate information layer pooling module added extract information layer usually stacked obtain better representation note architecture generalize gnn model also exception example ndcn zang wang 2020 combine ordinary differential equation system ode gnns regarded gnn model integrates gnn layer continuous time without propagating discrete number layer illustration general design pipeline shown fig later section ﬁrst give existing instantiation computational module section 3 introduce existing variant consider different graph type scale section survey variant designed different training setting section section correspond detail step 4 step 2 step 3 pipeline ﬁnally give concrete design example section 6 instantiation computational module section introduce existing instantiation three tational module propagation module sampling module pooling module introduce three propagation module convolution operator recurrent operator skip connection section respectively introduce sampling module pooling module section overview computational module shown fig 3 propagation module convolution operator convolution operator introduce section mostly used propagation operator gnn model main idea tion operator generalize convolution domain graph domain advance direction often categorized tral approach spatial approach spectral approach spectral approach work spectral representation graph method theoretically based graph signal processing shuman et 2013 deﬁne convolution operator spectral domain spectral method graph signal x ﬁrstly transformed spectral domain graph fourier transform f convolution operation conducted convolution resulted signal transformed back using inverse graph fourier transform f transforms deﬁned f ðxþ utx f ux 1 u matrix eigenvectors normalized graph laplacian l 2 degree matrix adjacency matrix graph normalized graph laplacian real symmetric positive semideﬁnite factorized l uλut λ diagonal matrix eigenvalue based convolution theorem mallat 1999 convolution operation deﬁned g f ðgþ f ðxþþ uðutg utxþ 2 utg ﬁlter spectral domain simplify ﬁlter fig general design pipeline gnn model zhou et al ai open 1 2020 60 using learnable diagonal matrix gw basic function spectral method gw ugwutx 3 next introduce several typical spectral method design different ﬁlters gw spectral network spectral network bruna et 2014 us able diagonal matrix ﬁlter gw diagðwþ w 2 rn parameter however operation computationally inefﬁcient ﬁlter localized henaff et al 2015 attempt make spectral ﬁlters spatially localized introducing ization smooth coefﬁcients chebnet hammond et al 2011 suggest gw approximated truncated expansion term chebyshev polynomial tkðxþ kth order defferrard et al 2016 propose chebnet based theory thus operation written gw x k wktk l x 4 l 2 λmax l λmax denotes largest eigenvalue range eigenvalue l 1 w 2 rk vector chebyshev coefﬁcients chebyshev polynomial deﬁned tkðxþ 1 observed operation since polynomial laplacian defferrard et al 2016 use convolution deﬁne convolutional neural network could remove need compute eigenvectors laplacian gcn kipf welling 2017 simplify convolution operation eq 4 k 1 alleviate problem overﬁtting assume λmax 2 simplify equation gw þ inþx 5 two free parameter parameter constraint w obtain following expression gw w 0 þ 2 1 ax 6 gcn introduces renormalization trick solve vanishing gradient problem eq 6 þ 2 2 2 fig overview computational module zhou et al ai open 1 2020 61 þ dii p j aij finally compact form gcn deﬁned h 2 7 x 2 input matrix w 2 0 parameter h 2 0 convolved matrix f f 0 dimension input output respectively note gcn also regarded spatial method discus later agcn model use original graph structure denote relation node however may implicit relation different node adaptive graph convolution network agcn proposed learn underlying relation li et agcn learns residual graph laplacian add original laplacian matrix result proven effective several datasets dgcn dual graph convolutional network dgcn zhuang 2018 proposed jointly consider local consistency global consistency graph us two convolutional network capture local global consistency adopts unsupervised loss ensemble ﬁrst convolutional network eq 7 second network replaces adjacency matrix positive pointwise mutual information ppmi matrix h ρ 0 b 2 p apd 2 p hw 1 c 8 ap ppmi matrix dp diagonal degree matrix ap gwnn graph wavelet neural network gwnn xu et us graph wavelet transform replace graph fourier transform ha several advantage 1 graph wavelet fastly obtained without matrix decomposition 2 graph wavelet sparse ized thus result better explainable gwnn outperforms several spectral method node classiﬁcation task agcn dgcn try improve spectral method tive augmenting graph laplacian gwnn replaces fourier transform conclusion spectral approach well theoretically based also several theoretical analysis proposed recently see section however almost spectral approach mentioned learned ﬁlters depend graph structure say ﬁlters not applied graph different structure model only applied transductive setting graph task basic spatial approach spatial approach deﬁne convolution directly graph based graph topology major challenge spatial approach deﬁning convolution operation differently sized neighborhood maintaining local invariance cnns neural fps neural fps duvenaud et 2015 us different weight matrix node different degree ht v þ x v ht u v σ jn vj 9 jn vj weight matrix node degree jn vj layer þ main drawback method not applied scale graph node degree dcnn diffusion convolutional neural network dcnn atwood towsley 2016 us transition matrix deﬁne neighborhood node node classiﬁcation diffusion representation node graph expressed h f ðwc p xþ 2 10 x 2 matrix input feature f dimension p n k n tensor contains power series p pk matrix p transition matrix graph adjacency matrix entity transformed diffusion convolutional representation k f matrix deﬁned k hop graph diffusion f feature deﬁned k f weight matrix activation function model niepert et 2016 tract normalizes neighborhood exactly k node node normalized neighborhood serf receptive ﬁeld tional convolutional operation lgcn learnable graph convolutional network lgcn gao et also exploit cnns aggregator performs max pooling neighborhood matrix node get feature element applies cnn compute hidden representation graphsage graphsage hamilton et general inductive framework generates embeddings sampling aggregating feature node local neighborhood n v ht u 2 n v v σ h ht v k n v 11 instead using full neighbor set graphsage uniformly sample set neighbor aggregate information gregation function graphsage suggests three aggregator mean aggregator lstm aggregator pooling aggregator graphsage mean aggregator regarded inductive version gcn lstm aggregator not permutation invariant requires speciﬁed order node spatial approach attention mechanism ha successfully used many task machine translation bahdanau et 2015 gehring et 2017 vaswani et 2017 machine reading cheng et 2016 also several model try generalize attention operator graph velickovic et 2018 zhang et compared operator mentioned operator assign different weight neighbor could alleviate noise achieve better result gat graph attention network gat velickovic et 2018 incorporates attention mechanism propagation step computes hidden state node attending neighbor following strategy hidden state node v tained v ρ x v αvuwht u αvu k x v k 12 w weight matrix associated linear transformation applied node weight vector mlp moreover gat utilizes attention used vaswani et al 2017 stabilize learning process applies k independent tion head matrix compute hidden state concatenates feature computes average resulting following two zhou et al ai open 1 2020 62 output representation v kk σ x v αk vuwkht u v σ 1 k x k x v αk vuwkht u 13 αk ij normalized attention coefﬁcient computed attention head attention architecture ha several property 1 computation pair parallelizable thus ation efﬁcient 2 applied graph node different grees specifying arbitrary weight neighbor 3 applied inductive learning problem easily gaan gated attention network gaan zhang et also us attention mechanism however us mechanism gather information different head replace average operation gat general framework spatial approach apart different variant spatial approach several general framework proposed aiming integrate different model one single framework monti et al 2017 propose mixture model network monet general spatial framework several method deﬁned graph manifold gilmer et al 2017 propose message passing neural network mpnn us message passing function unify several variant wang et al propose neural network nlnn uniﬁes several self attention method hoshen 2017 vaswani et 2017 velickovic et 2018 battaglia et al 2018 propose graph network gn deﬁnes general framework learning representation monet mixture model network monet monti et 2017 spatial framework try uniﬁes model main including cnns manifold gnns geodesic cnn gcnn masci et 2015 anisotropic cnn acnn boscaini et 2016 manifold gcn kipf welling 2017 dcnn atwood towsley 2016 graph formulated ular instance monet monet point manifold vertex graph denoted v regarded origin system neighbor u 2 n v associated uðv uþ given two function f g deﬁned vertex graph point manifold convolution operator monet deﬁned ðf x j gjdjðvþf djðvþf x v wjðuðv uþþf ðuþ 14 wjðuþ function assigning weight neighbor according thus djðvþf aggregated value neighbor function deﬁning different u w monet instantiate several method gcn function f g map node feature ðv uþ uðv uþ ðjn vj jn ujþ j 1 uþþ 1 ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ jn vjjn uj p monet model parameter wj learnable mpnn message passing neural network mpnn gilmer et 2017 extract general characteristic among several classic model model contains two phase message passing phase readout phase message passing phase model ﬁrst us message function mt aggregate message mt v neighbor us update function ut update hidden state ht v v x v mt ht v ht u evu v ut ht v v 15 evu represents feature undirected edge ðv uþ readout phase computes feature vector whole graph using readout function r b r ht v v 2 g 16 denotes total time step message function mt vertex update function ut readout function r may different setting hence mpnn framework could instantiate several different model via different function setting speciﬁc setting different model could found gilmer et 2017 nlnn neural network nlnn generalizes extends classic mean operation buades et 2005 computer vision operation computes hidden state position weighted sum feature possible position potential tions space time spacetime thus nlnn viewed uniﬁcation different method hoshen 2017 vaswani et 2017 velickovic et 2018 following mean operation buades et 2005 generic operation deﬁned v 1 c ðhtþ x f ht v ht u g ht u 17 u index possible position position v fðht v ht uþ computes scalar v u representing relation gðht uþ denotes transformation input ht u c ðhtþ normalization factor different variant nlnn deﬁned different f g setting detail found original paper buades et 2005 graph network graph network gn battaglia et 2018 general framework compared others learning graph level representation unify many variant like mpnn nlnn interaction network battaglia et 2016 watters et 2017 neural physic engine chang et 2017 commnet sukhbaatar ferguset 2016 dai et 2016 khalil et 2017 ggnn li et 2016 relation network raposo et 2017 santoro et 2017 deep set zaheer et 2017 point net qi et core computation unit gn called gn block gn block deﬁnes three update function three aggregation function k et k ht rk ht sk v v v φh v ht v ut h h 18 rk receiver node sk sender node edge matrix stacked edge vector node vector time step respectively v collect edge vector receiver node u global attribute graph representation φ ρ function various setting ρ function must invariant input ders take variable length argument propagation module recurrent operator recurrent method pioneer research line major ference recurrent operator convolution operator layer convolution operator use different weight layer zhou et al ai open 1 2020 63 recurrent operator share weight early method based sive neural network focus dealing directed acyclic graph sperduti starita 1997 frasconi et 1998 micheli et 2004 hammer et 2004 later concept graph neural network gnn wa ﬁrst proposed scarselli et 2009 gori et 2005 extended existing neural network process graph type name model gnn paper distinguish general name ﬁrst introduce gnn later variant require convergence hidden state talk method based gate mechanism method graph node naturally deﬁned feature related node target gnn learn state embedding hv 2 r contains information neighborhood node state embedding hv vector node v used produce output ov distribution predicted node label computation step hv ov deﬁned hv f xv hn v xn v ov gðhv xvþ 19 xv hn v xn v feature v feature edge state feature node neighborhood v tively f parametric function called local transition function shared among node update node state according input neighborhood g local output function describes output produced note f g interpreted feedforward neural network let h x xn matrix constructed stacking state output feature node feature respectively compact form h fðh xþ gðh xnþ 20 f global transition function g global output function stacked version f g node graph respectively value h ﬁxed point eq 20 uniquely deﬁned assumption f contraction map suggestion banach ﬁxed point theorem khamsi kirk 2011 gnn us following classic iterative scheme compute state fðht xþ 21 ht denotes iteration dynamical system eq 21 converges exponentially fast solution any initial value though experimental result shown gnn powerful architecture modeling structural data still several limitation gnn requires f contraction map limit model ability inefﬁcient update hidden state node iteratively towards ﬁxed point unsuitable use ﬁxed point focus tion node instead graph distribution tation ﬁxed point much smoother value le informative distinguishing node graphesn graph echo state network graphesn gallicchio micheli 2010 generalizes echo state network esn jaeger 2001 graph us ﬁxed contractive encoding function only train readout function convergence ensured contractivity reservoir dynamic consequence graphesn efﬁcient gnn sse stochastic embedding sse dai et also proposed improve efﬁciency gnn sse proposes learning framework contains two step embeddings node updated parameterized operator update step bedding projected steady state constraint space meet condition lagrangian propagation gnn tiezzi et 2020 formalizes learning task constraint optimization problem lagrangian framework avoids iterative computation ﬁxed point convergence procedure implicitly expressed constraint satisfaction mechanism method several work attempting use gate mechanism like gru cho et 2014 lstm hochreiter schmidhuber 1997 propagation step diminish computational limitation gnn improve propagation information across graph structure run ﬁxed number training step without antee convergence ggnn gated graph neural network ggnn li et 2016 proposed release limitation gnn release requirement function f contraction map us gate recurrent unit gru propagation step also us time bptt compute gradient computation step ggnn found table node v ﬁrst aggregate message neighbor update function incorporate information node previous timestep update node hidden state hn v gather neighborhood information node v z r update reset gate lstms also used similar way gru propagation process based tree graph tree lstm tai et al 2015 propose two extension tree structure basic lstm architecture table 2 different variant recurrent operator variant aggregator updater ggnn ht n v p v k þ b zt v σðwzht n v þ v þ rt v σðwrht n v þ v þ ht v tanhðwht n v þ uðrt v v þþ ht v zt vþ v þ zt v ht v tree lstm child sum hti n v p v k htf n vk uf k hto n v p v k htu n v p v k v σðwixt v þ hti n v þ biþ ft vk σðwf xt v þ htf n vk þ bf þ ot v σðwoxt v þ hto n v þ boþ ut v tanhðwuxt v þ htu n v þ buþ ct v v ut v þ p v ft vk k ht v ot v tanhðct vþ tree lstm hti n v p k ui vl htf n vk p k uf vl hto n v p k uo l vl htu n v p k uu l vl graph lstm peng et 2017 hti n v p v ui mðv k htf n vk uf mðv k hto n v p v uo mðv k htu n v p v uu mðv k zhou et al ai open 1 2020 64 also extension recursive neural network based model mentioned tree special case graph node aggregate information child instead single forget gate traditional lstm unit node v contains one forget gate fvk child computation step displayed table v ot v ct v input gate output gate memory cell respectively xt v input vector time designed special kind tree node ha k child child ordered equation computing hti n v htf n vk hto n v htu n v table 2 introduce separate parameter child parameter allow model learn tations conditioning state unit child graph lstm two type easily adapted graph lstm zayats ostendorf 2018 example applied graph however simpliﬁed version since node graph ha 2 incoming edge parent sibling predecessor peng et al 2017 pose another variant graph lstm based relation extraction task edge graph peng et 2017 various label peng et al 2017 utilize different weight matrix represent different label table 2 mðv kþ denotes edge label node v liang et al 2016 propose graph lstm network address semantic object parsing task us scheme adaptively select starting node determine node updating sequence follows idea generalizing existing lstms data ha speciﬁc updating sequence method mentioned agnostic order node zhang et al propose sentence lstm improving text encoding convert text graph utilizes graph lstm learn representation show strong resentation power many nlp problem propagation module skip connection many application unroll stack graph neural network layer aiming achieve better result layer k layer make node aggregate information neighbor k hop away however ha observed many experiment deeper model could not improve performance deeper model could even perform worse mainly layer could also propagate noisy formation exponentially increasing number expanded borhood member also cause smoothing problem node tend similar representation aggregation tion model go deeper many method try add skip connection make gnn model deeper subsection duce three kind instantiation skip connection highway gcn rahimi et al 2018 propose highway gcn us gate similar highway network zilly et 2016 output layer summed input gating weight tðhtþ σðwtht þ btþ tðhtþ þ ht tðhtþ þ 22 adding highway gate performance peak 4 layer speciﬁc problem discussed rahimi et 2018 column network cln pham et 2017 also utilizes highway network ha different function compute gating weight jkn xu et al 2018 study property limitation hood aggregation scheme propose jump knowledge network jkn could learn adaptive representation jkn selects intermediate representation jump last layer node last layer make model adapt effective neighborhood size node needed xu et al 2018 use three approach concatenation experiment aggregate information jkn performs well experiment social bioinformatics citation network also combined model like gcn graphsage gat improve performance deepgcns li et al borrow idea resnet et densenet huang et 2017 resgcn segcn proposed incorporating residual connection dense connection solve problem vanishing gradient smoothing detail hidden state node resgcn segcn computed þ ht dense 23 experiment deepgcns conducted point cloud mantic segmentation task best result achieved layer model sampling module gnn model aggregate message node neighborhood previous layer intuitively track back multiple gnn layer size supporting neighbor grow exponentially depth alleviate neighbor explosion issue efﬁcient efﬁcacious way sampling besides deal large graph not always store process neighborhood information node thus sampling module needed conduct propagation section introduce three kind graph sampling module node sampling layer sampling subgraph sampling node sampling straightforward way reduce size neighboring node would selecting subset node neighborhood graphsage hamilton et sample ﬁxed small number neighbor ensuring 2 50 neighborhood size node reduce sampling variance chen et al introduce based stochastic approximation rithm gcn utilizing historical activation node control variate method limit receptive ﬁeld neighborhood us historical hidden state affordable approximation pinsage ying et proposes sampling method simulating random walk starting target node approach chooses top node highest normalized visit count layer sampling instead sampling neighbor node layer sampling retains small set node aggregation layer control expansion factor fastgcn chen et directly sample receptive ﬁeld layer us importance sampling important node likely sampled contrast ﬁxed sampling method huang et al 2018 introduce parameterized trainable sampler perform sampling conditioned former layer furthermore adaptive sampler could optimize sampling importance reduce variance simultaneously lady zou et 2019 intends alleviate sparsity issue sampling generating sample union neighbor node subgraph sampling rather sampling node edge build upon full graph fundamentally different way sample multiple subgraphs restrict neighborhood search within subgraphs tergcn chiang et 2019 sample subgraphs graph clustering gorithms graphsaint zeng et 2020 directly sample node edge generate subgraph zhou et al ai open 1 2020 65 pooling module area computer vision convolutional layer usually followed pooling layer get general feature complicated graph usually carry rich hierarchical structure great tance classiﬁcation task similar poolinglayers alotofworkfocusesondesigninghierarchicalpoolinglayers weintroduce twokindsof pooling module direct pooling module hierarchical pooling module direct pooling module direct pooling module learn representation directly node different node selection strategy module also called readout function some variant simple node pooling simple node pooling method used several model model tions applied node feature get global graph representation mpnn us method vinyals et readout function get graph representation designed deal unordered set fðht v xvþg us method produce order invariant representation prediﬁned number step sortpooling sortpooling zhang et ﬁrst sort node embeddings according structural role node sorted embeddings fed cnns get representation hierarchical pooling module method mentioned directly learn graph representation node not investigate hierarchical property graph structure next talk method follow chical pooling pattern learn graph representation layer graph coarsening early method usually based graph coarsening algorithm spectral clustering algorithm ﬁrstly used inefﬁcient eigendecomposition step graclus dhillon et 2007 provides faster way cluster node applied pooling module example chebnet monet use graclus merge node pair add additional node make sure pooling procedure form balanced binary tree ecc convolution ecc simonovsky dakis 2017 design pooling module recursively downsampling operation downsampling method based splitting graph two component sign largest eigenvector laplacian diffpool diffpool ying et us learnable hierarchical clustering module training assignment matrix st layer st softmax gnnt poolðat htþ ðstþtatst 24 ht node feature matrix coarsened adjacency matrix layer st denotes probability node layer assigned coarser node layer þ gpool gpool gao ji 2019 us project vector learn jection score node select node score compared diffpool us vector instead matrix layer thus reduces storage complexity projection procedure doe not consider graph structure eigenpooling eigenpooling et designed use node feature local structure jointly us local graph fourier transform extract subgraph information suffers efﬁciency graph eigendecomposition sagpool sagpool lee et 2019 also proposed use feature topology jointly learn graph representation us based method reasonable time space complexity fig overview variant considering graph type scale zhou et al ai open 1 2020 66 variant considering graph type scale section assume graph simplest format however many graph real world complex subsection introduce approach attempt address lenges complex graph type overview variant shown fig 4 directed graph ﬁrst type directed graph directed edge usually contain information undirected edge example knowledge graph head entity parent class tail entity edge direction offer information partial order instead simply adopting asymmetric adjacency matrix convolution operator model forward reverse direction edge differently dgp kampffmeyer et 2019 us two kind weight matrix wp wc convolution forward reverse direction heterogeneous graph second variant graph heterogeneous graph node edge speciﬁcally heterogeneous graph fv e φ ψg node vi associated type φðviþ edge ej type ψðejþ method approach toward graph type utilize concept path path scheme determines type node position path l length training process instantiated node sequence connecting two end node stance capture similarity two node may not directly connected consequently one heterogeneous graph reduced several homogeneous graph graph learning algorithm applied early work based similarity search investigated sun et 2011 recently gnn model utilize proposed han wang et ﬁrst performs graph attention neighbor us semantic attention output embeddings node scheme generate ﬁnal representation node magnn fu et 2020 proposes take intermediate node consideration ﬁrst aggregate information along using neural module performs attention different instance associated node ﬁnally performs attention different scheme gtn yun et 2019 proposes novel graph transformer layer identiﬁes new connection unconnected node learning representation node learned new connection connect node serveral hop away closely related function method also work utilize work typically use different function term sampling aggregation etc different kind neighbor edge hetgnn zhang et address challenge directly treating neighbor different type differently sampling feature encoding aggregation step hgt hu et deﬁnes type two neighboring node link ψðeijþ assigns different attention weight matrix different empowering model take type information consideration method relational graph theedgeofsomegraphsmaycontainmoreinformationthanthetype quantity type may large exerting difﬁculties applying relational graph schlichtkrull et 2018 handle relational graph beck et 2018 convert original graph bipartite graph original edge also become node one original edge split two new edge mean two new edge edge node node transformation us gated graph neural network followed recurrent neural network convert graph edge information sentence aggregation function ggnn take hidden representation node relation input another approach schlichtkrull et 2018 require convert original graph format assigns different weight matrix propagation different kind edge however number relationsisverylarge number parameter inthe model explodes therefore introduces two kind regularization reduce number parameter modeling amount relation basis decomposition wr deﬁned follows wr x b arbvb 25 wr linear combination basis transformation vb 2 coefﬁcients arb decomposition gcn deﬁnes wr direct sum set dimensional matrix need parameter ﬁrst one method multiplex graph complex scenario pair node graph ated multiple edge different type viewing different type edge graph form multiple layer layer represents one type relation therefore multiplex graph also referred graph graph example youtube three different relation two user sharing subscription comment edge type not assumed independent therefore simply splitting graph subgraphs one type edge might not optimal solution mgcn et introduces general representation resentations node layer gnn representation projected general representation using fig overview method unsupervised loss zhou et al ai open 1 2020 67 different projection matrix aggregated form next layer general representation dynamic graph another variant graph dynamic graph graph structure existence edge node keep changing time model graph structured data together time series data dcrnn li et stgcn yu et 2018 ﬁrst collect spatial information gnns feed output sequence model like model rnns differently jain et 2016 yan et 2018 collect spatial temporal message time extend static graph structure temporal connection apply traditional gnns extended graph similarly dgnn manessi et 2020 feed output embeddings node gcn separate lstms weight lstms shared node hand evolvegcn pareja et 2020 argues directly modeling dynamic node representation hamper model performance graph node set keep changing therefore instead treating node feature input rnn feed weight gcn rnn capture intrinsic dynamic graph interaction recently survey huang et 2020 classiﬁes dynamic network several category based link duration group existing model category according specialization also establishes general framework model dynamic graph ﬁts existing model general framework graph type variant graph hypergraphs signed graph also some model proposed address challenge hypergraphs hypergraph denoted g ðv e weþ edge e 2 e connects two vertex assigned weight w 2 adjacency matrix hypergraph represented jej matrix l lv e 1 v 2 e 0 v 62 e 26 hgnn feng et 2019 proposes hypergraph convolution ce high order interaction node h 2 v e 2 v xw 27 dv de x node degree matrix edge weight matrix edge degree matrix node feature matrix respectively w learnable parameter formula derived approximating hypergraph laplacian using truncated chebyshev polynomial signed graph signed graph graph signed edge edge either positive negative instead simply treating negative edge absent edge another type edge sgcn derr et 2018 utilizes balance theory capture interaction positive edge negative edge intuitively balance theory suggests friend positive edge friend also friend enemy negative edge enemy friend therefore provides theoretical dation sgcn model interaction positive edge negative edge large graph mentioned section sampling operator usually used process graph besides sampling technique also method scaling problem leveraging approximate personalized pagerank method proposed klicpera et al 2019 bojchevski et al 2020 avoid calculating propagation matrix rossi et al 2020 propose method precompute graph convolutional ﬁlters different size efﬁcient training inference model squeeze multiple gcn layer one single propagation layer mitigate neighbor explosion issue hence highly scalable efﬁcient variant different training setting section introduce variant different training setting supervised setting label provided loss function easy design labeled sample pervised setting no labeled sample loss function depend information provided graph input feature graph topology section mainly introduce variant unsupervised training usually based idea contrastive learning overview method mention shown fig 5 graph unsupervised graph representation learning ha trend extend ae graph domain graph gae kipf welling 2016 ﬁrst us gcns encode node graph us simple decoder reconstruct adjacency matrix compute loss similarity original adjacency matrix reconstructed matrix h gcnðx aþ ρðhhtþ 28 kipf welling 2016 also train gae model variational manner model named variational graph vgae adversarially regularized graph arga pan et 2018 employ generative adversarial network gans regularize graph could learn robust node representation instead recovering adjacency matrix wang et al 2017 park et al 2019 try reconstruct feature matrix mgae wang et 2017 utilizes marginalized denoising get robust node representation build symmetric graph gala park et 2019 proposes laplacian sharpening inverse operation laplacian smoothing decode hidden state mechanism alleviates oversmoothing issue gnn training different age cui et 2020 state recovering loss not compatible downstream task therefore apply adaptive learning measurement pairwise node similarity achieve performance node clustering link prediction contrastive learning besides graph contrastive learning pave another way unsupervised graph representation learning deep graph infomax dgi velickovic et 2019 maximizes mutual information node representation graph representation infograph sun et 2020 aim learn graph representation mutual information mization representation representation different scale including node edge triangle hassani khasahmadi 2020 contrast representation adjacency matrix graph diffusion achieves performance multiple graph learning task zhou et al ai open 1 2020 68 design example gnn section give existing gnn model illustrated design process taking task heterogeneous graph pretraining example use hu et model illustrate design process find graph structure paper focus application demic knowledge graph recommendation system ademic knowledge graph graph structure explicit recommendation system user item review regarded node interaction among regarded edge graph structure also easy construct specify graph type scale task focus heterogeneous graph type node edge considered rated ﬁnal model academic graph dation graph contain million node model consider efﬁciency problem conclusion model focus heterogeneous graph design loss function downstream task hu et task prediction academic graph model learn node representation pretraining step pretraining step no labeled data available graph generation task designed learn node bedding ﬁnetuning step model ﬁnetuned based training data task supervised loss task applied build model using computational module finally model built computational module propagation module thor use convolution operator hgt hu et mentioned hgt incorporates type node edge propagation step model skip connection also added architecture sampling module specially designed sampling method hgsampling hu et applied heterogeneous version lady zou et 2019 model focus learning node representation pooling module not needed hgt layer stacked multiple layer learn better node embeddings analysis gnns theoretical aspect section summarize paper theoretic dations explanation graph neural network various perspective fig application scenario icon made freepik flaticon zhou et al ai open 1 2020 69 graph signal processing spectral perspective view gcns perform convolution operation input feature spectral domain follows graph signal processing theory exists several work analyzing gnns graph signal cessing li et al ﬁrst address graph convolution graph neural network actually laplacian smoothing smooth feature matrix nearby node similar hidden representation laplacian smoothing reﬂects homophily assumption nearby node supposed similar laplacian matrix serf ﬁlter input feature sgc wu et remove weight matrix nonlinearties layer showing ﬁlter reason gnns work following idea ﬁltering zhang et al cui et al 2020 nt maehara nt maehara 2019 chen et al analyze different ﬁlters provide new insight achieve ﬁltering eigenvalue agc zhang et design graph ﬁlter 1 2 l according frequency response function age cui et 2020 demonstrates ﬁlter 1 λmax l could get better result λmax maximum eigenvalue laplacian matrix despitelinearﬁlters graphheat leveragesheatkernelsfor better property nt maehara nt maehara 2019 state graph convolution mainly denoising process input feature model performance heavily depend amount noise feature matrix alleviate issue chen et al present two metric measuring smoothness node representation gnn model author conclude ratio key factor generalization generalization ability gnns also received attention recently scarselli et al 2018 prove limited class gnns garg et al 2020 give much tighter generalization bound based rademacher bound neural network verma zhang 2019 analyze stability generalization property gnns different convolutional ﬁlters author conclude stability gnns depends largest eigenvalue ﬁlters knyazev et al 2019 focus generalization ability attention mechanism gnns conclusion show attention help gnns generalize larger noisy graph expressivity expressivity gnns xu et al morris et al 2019 show gcns graphsage le discriminative table 3 application graph neural network area application reference graph mining graph matching riba et 2018 li et graph clustering zhang et ying et tsitsulin et 2020 physic physical system modeling battaglia et 2016 sukhbaatar ferguset 2016 watters et 2017 hoshen 2017 kipf et 2018 sanchez et 2018 chemistry molecular fingerprint duvenaud et 2015 kearnes et 2016 chemical reaction prediction et al 2019 biology protein interface prediction fout et al 2017 side effect prediction zitnik et al 2018 disease classiﬁcation rhee et al 2018 knowledge graph kb completion hamaguchi et 2017 schlichtkrull et 2018 shang et 2019 kg alignment wang et zhang et xu et generation graph generation shchur et nowak et 2018 et 2018 et de cao kipf 2018 li et shi et 2020 liu et 2019 grover et 2019 combinatorial optimization combinatorial optimization khalil et 2017 nowak et 2018 li et kool et 2019 bello et 2017 vinyals et sutton barto 2018 dai et 2016 gasse et 2019 zheng et selsam et 2019 sato et 2019 trafﬁc network trafﬁc state prediction cui et yu et 2018 zheng et guo et 2019 recommendation system interaction prediction van den berg et 2017 ying et social recommendation wu et fan et 2019 others structural stock market matsunaga et 2019 yang et 2019 chen et li et 2020 kim et 2019 software deﬁned network rusek et al 2019 amr graph text song et beck et 2018 text text classiﬁcation peng et 2018 yao et 2019 zhang et tai et 2015 sequence labeling zhang et marcheggiani titov 2017 neural machine translation basting et 2017 marcheggiani et 2018 beck et 2018 relation extraction miwa bansal 2016 peng et 2017 song et zhang et event extraction nguyen grishman 2018 liu et 2018 fact veriﬁcation zhou et 2019 liu et 2020 zhong et 2020 question answering song et de cao et 2019 qiu et 2019 tu et 2019 ding et 2019 relational reasoning santoro et 2017 palm et 2018 battaglia et 2016 image social relationship understanding wang et al image classiﬁcation garcia bruna 2018 wang et lee et kampffmeyer et 2019 marino et 2017 visual question answering teney et 2017 wang et narasimhan et 2018 object detection hu et 2018 gu et 2018 interaction detection qi et 2018 jain et 2016 region classiﬁcation chen et al semantic segmentation liang et 2016 2017 landrieu simonovsky 2018 wang et qi et program veriﬁcation allamanis et 2018 li et 2016 zhou et al ai open 1 2020 70 wl test algorithm graph isomorphism testing xu et al also propose gin expressive gnns going beyond wl test et al 2019 discus gnns expressible fragment ﬁrst order logic author ﬁnd existing gnns hardly ﬁt logic learning graph topologic structure garg et al 2020 prove locally dependent gnn variant not capable learn global graph property including diameter cycle motif loukas 2020 dehmamy et al 2019 argue existing work only consider expressivity gnns inﬁnite layer unit work investigates representation power gnns ﬁnite depth width oono suzuki 2020 discus asymptotic haviors gnns model deepens model dynamic system invariance no node order graph output embeddings gnns supposed equivariant input feature maron et al characterize equivariant linear layer build invariant gnns maron et al prove result universal invariant gnns obtained tensorization keriven e 2019 provide alternative proof extend conclusion equivariant case chen et al 2019 build connection graph isomorphism testing prove equivalence chen et al 2019 leverage describe expressivity gnns transferability deterministic characteristic gnns parameterization untied graph suggests ability transfer across graph transferability performance guarantee levie et al 2019 investigate transferability spectral graph ﬁlters showing ﬁlters able transfer graph domain ruiz et al 2020 analyze gnn behaviour graphons graphon refers limit sequence graph also seen generator dense graph author conclude gnns transferable across graph obtained deterministically graphon different size label efﬁciency supervised learning gnns need considerable amount labeled data achieve satisfying performance improving label efﬁciency ha studied perspective active learning informative node actively selected labeled oracle train gnns cai et al 2017 gao et al hu et al demonstrate selecting informative node node uncertain node labeling efﬁciency dramatically improved empirical aspect besides theoretical analysis empirical study gnns also required better comparison evaluation include several empirical study gnn evaluation benchmark evaluation evaluating machine learning model essential step research concern experimental reproducibility replicability raised year whether extent gnn model work part model contribute ﬁnal performance investigate fundamental question study fair evaluation strategy urgently needed node classiﬁcation task shchur et al explore gnn model perform training strategy hyperparameter tune work concludes different dataset split lead dramatically different ranking model also simple model could outperform complicated one proper setting errica et al 2020 review several graph classiﬁcation model point compared inproperly based rigorous evaluation structural formation turn not fully exploited graph classiﬁcation et al 2020 discus architectural design gnn model number layer aggregation function huge amount experiment work provides comprehensive guideline gnn designation various task benchmark benchmark datasets imagenet signiﬁcant machine learning research however graph learning benchmark problematic example node classiﬁcation datasets contain only 3000 node small compared graph furthermore experimental protocol across study not uniﬁed hazardous ature mitigate issue dwivedi et al 2020 hu et al provide scalable reliable benchmark graph learning dwivedi et al 2020 build benchmark datasets multiple main task ogb hu et offer datasets furthermore work evaluate current gnn model provide leaderboards comparison application graph neural network explored wide range main across supervised unsupervised ment learning setting section generally group application two scenario 1 structural scenario data ha explicit relational structure scenario one hand emerge scientiﬁc research graph mining modeling physical system chemical system hand rise dustrial application knowledge graph trafﬁc network recommendation system 2 scenario tional structure implicit absent scenario generally include image computer vision text natural language processing two actively developing branch ai research simple illustration application fig note only list several representative application instead providing exhaustive list summary application could found table 3 structural scenario following subsection introduce gnns application structural scenario data naturally performed graph structure graph mining ﬁrst application solve basic task graph mining generally graph mining algorithm used identify useful structure downstream task traditional graph mining challenge include frequent mining graph matching graph classiﬁcation graph clustering etc although deep learning some downstream task directly solved without graph mining intermediate step basic challenge worth studied gnns perspective graph matching ﬁrst challenge graph matching traditional method graph matching usually suffer high computational complexity emergence gnns allows researcher capture structure graph using neural network thus offering another solution problem riba et al 2018 propose siamese mpnn model learn graph editing distance siamese framework two parallel mpnns structure weight sharing training objective embed pair graph small editing distance close latent space li et al design similar method periments scenario similarity search zhou et al ai open 1 2020 71 control ﬂow graph graph clustering graph clustering group vertex graph cluster based graph structure node attribute various work zhang et node representation learning oped representation node passed traditional tering algorithm apart learning node embeddings graph pooling ying et seen kind clustering recently tsitsulin et al 2020 directly target clustering task study desirable property good graph clustering method propose optimize spectral modularity remarkably useful graph clustering metric physic modeling physical system one fundamental aspect understanding human intelligence physical system modeled object system interaction object simulation physical system requires model learn law system make prediction next state tem modeling object node interaction edge system simpliﬁed graph example particle system particle interact via multiple interaction including collision hoshen 2017 spring connection electromagnetic force kipf et 2018 particle seen node teractions seen edge another example robotic system formed multiple body arm leg connected joint body joint seen node edge respectively model need infer next state body based current state system principle physic advent graph neural network work process graph representation system using available neural block tion network battaglia et 2016 utilizes mlp encode dence matrix graph commnet sukhbaatar ferguset 2016 performs node update using node previous representation average node previous representation vain hoshen 2017 introduces attention mechanism vin watters et 2017 combine cnns rnns battaglia et 2016 emergence gnns let u perform reasoning object relation physic simpliﬁed effective way nri kipf et 2018 take trajectory object input infers explicit interaction graph learns dynamic model simultaneously interaction graph learned former trajectory trajectory prediction generated decoding interaction graph sanchez et al 2018 propose graph model encode graph formed body joint robotic system learn policy stably controlling system combining gns reinforcement learning chemistry biology molecular fingerprint molecular ﬁngerprints serve way encode structure molecule simplest ﬁngerprint hot vector digit represents existence absence particular substructure ﬁngerprints used molecule searching core step drug design tional molecular ﬁngerprints ﬁxed vector however molecule naturally seen graph atom node edge therefore applying gnns molecular graph obtain better ﬁngerprints duvenaud et al 2015 propose neural graph ﬁngerprints neural fps calculate substructure feature vector via gcns sum get overall representation kearnes et al 2016 explicitly model atom atom pair independently emphasize atom interaction troduces edge representation et uv instead aggregation function ht n v p ðvþ et uv chemical reaction prediction chemical reaction product tion fundamental issue organic chemistry graph transformation policy network et 2019 encodes input molecule erates intermediate graph node pair prediction network policy network protein interface prediction protein interact using interface formed amino acid residue participating protein protein interface prediction task mine whether particular residue constitute part protein generally prediction single residue depends neighboring due letting residue node protein represented graph leverage machine learning rithms fout et al 2017 propose method learn ligand receptor protein residue representation merge classiﬁcation xu et introduces approach extract summarize local global feature better prediction biomedical engineering interaction network rhee et al 2018 leverage graph convolution relation network breast cancer subtype classiﬁcation zitnik et al 2018 also suggest model polypharmacy side effect prediction work model drug protein interaction network separately deal edge different type knowledge graph knowledge graph kg represents collection tities relational fact pair entity ha wide application question answering information retrieval knowledge guided generation task kg include learning dimensional embeddings contain rich semantics entity relation predicting missing link entity hop reasoning knowledge graph one line research treat graph collection triple proposes various kind loss function distinguish correct triple false triple bordes et 2013 line leverage graph nature kg us method various task treated graph kg seen heterogeneous graph however unlike heterogeneous graph social network logical relation tance pure graph structure schlichtkrull et 2018 ﬁrst work incorporate gnns knowledge graph embedding deal various relation proposes transformation message passing step convolutional network shang et 2019 combine gcn encoder cnn decoder together better knowledge representation challenging setting knowledge base completion ookb entity ookb entity unseen training set directly connect observed entity training set embeddings ookb entity aggregated observed entity hamaguchi et al 2017 use gnns solve lem achieve satisfying performance standard kbc setting ookb setting besides knowledge graph representation learning wang et al utilize gcn solve knowledge graph ment problem model embeds entity different language uniﬁed embedding space aligns based embedding ilarity align heterogeneous knowledge graph oag zhang et us graph attention network model various type entity representing entity surrounding graph xu et al transfer entity alignment problem graph matching problem solve graph matching network generative model generative model graph drawn signiﬁcant attention important application including modeling social teractions discovering new chemical structure constructing knowledge graph deep learning method powerful ability learn implicit distribution graph surge neural graph zhou et al ai open 1 2020 72 generative model recently netgan shchur et one ﬁrst work build neural graph generative model generates graph via random walk transforms problem graph generation problem walk generation take random walk speciﬁc graph input train walk generative model using gan architecture generated graph preserve important topological property inal graph number node unable change generating process original graph graphrnn et manages generate adjacency matrix graph ating adjacency vector node step step output network different number node li et al propose model generates edge node sequentially utilizes graph neural network extract hidden state current graph used decide action next step sequential generative process graphaf shi et 2020 also formulates graph generation sequential decision process combine generation autogressive model towards molecule generation also conduct validity check generated molecule using existing chemical rule step generation instead generating graph sequentially work generate adjacency matrix graph molgan de cao kipf 2018 utilizes discriminator solve node variant problem adjacency matrix besides applies reward network optimization towards desired chemical property et al 2018 propose constrained variational ensure semantic validity generated graph gcpn et incorporates rule reinforcement learning gnf liu et 2019 adapts normalizing ﬂow graph data normalizing ﬂow kind generative model us invertable mapping transform observed data latent vector space transforming latent vector back observed data using inverse matrix serf generating process gnf combine normalizing ﬂow graph take graph structured data input generate new graph test time graphite grover et 2019 integrates gnn variational encode graph structure feature latent variable speciﬁcally us isotropic gaussian latent iables us iterative reﬁnement strategy decode latent variable combinatorial optimization combinatorial optimization problem graph set problem attract much attention scientist ﬁelds some speciﬁc problem like traveling salesman problem tsp minimum spanning tree mst got various heuristic solution recently using deep neural network solving problem ha spot some solution leverage graph neural network graph structure bello et al 2017 ﬁrst propose approach tackle tsp method consists two part pointer network vinyals et parameterizing reward policy gradient sutton barto 2018 module training work ha proved comparable traditional approach however pointer network designed sequential data like text encoders appropriate work khalil et al 2017 kool et al 2019 improve method including graph neural network former work ﬁrst obtains node embeddings dai et 2016 feed module making decision latter one build system replacing reinforcement learning module decoder efﬁcient training work achieve better performance previous rithms prove representation power graph neural network generally gasse et al 2019 represent state combinatorial problem bipartite graph utilize gcn encode speciﬁc combinatorial optimization problem nowak et al 2018 focus quadratic assignment problem measuring ilarity two graph gnn based model learns node embeddings graph independently match using attention mechanism method offer intriguingly good performance even regime standard technique appear suffer zheng et al use generative graph neural network model learning problem also combinatorial zation problem neurosat selsam et 2019 learns message passing neural network classify satisﬁability sat problem prof learned model generalize novel tributions sat problem converted sat unlike previous work try design speciﬁc gnns solve combinatorial problem sato et al 2019 provide theoretical analysis gnn model problem establishes connection gnns distributed local algorithm group classical algorithm graph solving problem moreover strates optimal approximation ratio optimal solution powerful gnn reach also prof existing gnn model not exceed upper bound furthermore add coloring node feature improve approximation ratio trafﬁc network predicting trafﬁc state challenging task since trafﬁc network dynamic complex dependency cui et al combine gnns lstms capture spatial temporal dependency stgcn yu et 2018 construct block spatial temporal convolution layer applies residual connection bottleneck strategy zheng et al guo et al 2019 incorporate attention mechanism better model spatial temporal correlation recommendation system interaction prediction one classic problem recommendation modeling interaction graph gnns utilized area van den berg et 2017 ﬁrstly applies gcn rating graph learn user item embeddings efﬁciently adopt gnns scenario pinsage ying et build computational graph weighted sampling strategy bipartite graph reduce repeated computation social recommendation try incorporate user social network enhance recommendation performance graphrec fan et 2019 learns user embeddings item side user side wu et al go beyond static social effect attempt model homophily inﬂuence effect dual attention application structural scenario ubiquity data gnns applied larger variety task introduced list scenario brieﬂy ﬁnancial market gnns used model interaction different stock predict future trend stock matsunaga et 2019 yang et 2019 chen et li et 2020 kim et al 2019 also predict market index movement formulating graph classiﬁcation problem network sdn gnns used optimize ing performance rusek et 2019 abstract meaning tion amr graph text generation task song et al beck et al 2018 use gnns encode graph representation abstract meaning scenario section talk application narios generally two way apply gnns zhou et al ai open 1 2020 73 scenario 1 incorporate structural information domain improve performance example using information edge graph alleviate problem image task 2 infer assume relational structure task apply model solve problem deﬁned graph method zhang et model text graph common scenario include image text programming source code ni et 2018 li et 2016 however only give detailed duction ﬁrst two scenario image zero image classiﬁcation image classiﬁcation basic important task ﬁeld computer vision attracts much attention ha many famous datasets like imagenet kovsky et 2015 recently learning become popular ﬁeld image classiﬁcation learning make prediction test data sample some class only n training sample class provided training set thereby learning restricts n small quire n model must learn generalize limited training data make new prediction testing data graph neural network hand assist image classiﬁcation system challenging scenario first knowledge graph used extra information guide recognition classiﬁcation wang et kampffmeyer et 2019 wang et al make visual classiﬁers learn not only visual input also word embeddings gories name relationship category knowledge graph developed help connect related category use gcn encode knowledge graph fect happens graph convolution architecture becomes deep gcn used wang et wash much useful information representation solve smoothing problem kampffmeyer et al 2019 use single layer gcn larger borhood includes node graph proven effective building classiﬁer existing one knowledge graph large reasoning marino et al 2017 select some related entity build based result object detection apply ggnn extracted graph prediction besides lee et al also leverage knowledge graph category deﬁnes three type relation category positive correlation negative lation propagates conﬁdence relation label graph directly except knowledge graph similarity image dataset also helpful learning garcia bruna 2018 garcia bruna 2018 build weighted image network based similarity message passing graph recognition visual reasoning system usually need perform reasoning incorporating spatial semantic information natural generate graph reasoning task typical visual reasoning task visual question answering vqa task model need answer question image given text description question usually answer lie spatial relation among object image teney et al 2017 construct image scene graph question syntactic graph apply ggnn train embeddings predicting ﬁnal answer despite spatial connection among object norcliffebrown et al 2018 build relational graph conditioned question knowledge graph wang et al narasimhan et al 2018 perform ﬁner relation exploration interpretable reasoning process application visual reasoning include object detection interaction detection region classiﬁcation object detection hu et 2018 gu et 2018 gnns used calculate roi feature interaction detection qi et 2018 jain et 2016 gnns tool human object region cation chen et gnns perform reasoning graph connects region class semantic segmentation semantic segmentation crucial step towards image understanding task assign unique label category every single pixel image considered dense classiﬁcation problem however region image often not need information lead failure traditional cnn several work utilize data handle liang et al 2016 use model dependency together spatial connection building graph form superpixel map applying lstm propagate borhood information globally subsequent work improves perspective encoding hierarchical information liang et 2017 furthermore semantic segmentation rgbd semantic tion point cloud classiﬁcation utilize geometric information therefore hard model cnn qi et al construct neighbor knn graph use gnn propagation model unrolling several step prediction model take hidden state node input predicts semantic label always many point point cloud classiﬁcation task landrieu simonovsky 2018 solve point cloud mentation building superpoint graph generating embeddings classify supernodes landrieu simonovsky 2018 leverage ggnn graph convolution wang et al propose model point interaction edge calculate edge representation vector feeding coordinate terminal node node embeddings updated edge aggregation text graph neural network could applied several task based text could applied task text cation well task sequence labeling list several major application text following text classiﬁcation text classiﬁcation important classical problem natural language processing traditional text classiﬁcation us feature however representing text graph word capture semantics long distance word peng et 2018 peng et al 2018 use based deep learning model ﬁrst convert text use graph convolution operation niepert et 2016 convolve word graph zhang et al propose sentence lstm encode text view whole sentence single state consists individual word overall state use global representation classiﬁcation task method either view document sentence graph word node yao et al 2019 regard document word node construct corpus graph use text gcn learn embeddings word document sentiment classiﬁcation could also regarded text classiﬁcation problem approach proposed tai et 2015 sequence labeling given sequence observed variable word sequence labeling assign categorical label able typical task include label word sentence named entity recognition ner predict whether word sentence belongs part named entity consider variable sequence node dependency edge utilize hidden state gnns address task zhang et al utilize sentence lstm label sequence conducted experiment ner task achieves promising performance semantic role labeling another task sequence labeling ggiani titov 2017 present syntactic gcn solve problem syntactic gcn operates direct graph labeled edge special variant gcn kipf welling 2017 integrates gate let model regulate contribution individual zhou et al ai open 1 2020 74 dependency edge syntactic gcns syntactic dependency tree used sentence encoders learn latent feature representation word sentence neural machine translation neural machine translation nmt task translate text source language target language automatically using neural network usually considered task transformer vaswani et 2017 troduces attention mechanism replaces commonly used recurrent convolutional layer fact transformer assumes fully connected graph structure word graph structure explored gnns one popular application gnn incorporate syntactic semantic information nmt task basting et al 2017 utilize syntactic gcn nmt task marcheggiani et al 2018 incorporate information structure source sentence namely representation using syntactic gcn compare result incorporating only syntactic only semantic information information beck et al 2018 utilize ggnn nmt convert syntactic pendency graph new structure called levi graph levi 1942 turning edge additional node thus edge label represented embeddings relation extraction extracting semantic relation entity text help expand existing knowledge base traditional method use cnns rnns learn entity feature predict relation type pair entity sophisticated way utilize pendency structure sentence document graph built node represent word edge represent various dependency adjacency syntactic dependency discourse relation zhang et al propose extension graph convolutional network tailored relation extraction apply pruning strategy input tree relation extraction detects relation among n entity across multiple sentence peng et al 2017 explore general framework relation extraction applying graph lstms document graph song et al also use lstm model speed computation allowing parallelization event extraction event extraction important information extraction task recognize instance speciﬁed type event text always conducted recognizing event trigger predicting argument trigger nguyen grishman 2018 investigate convolutional neural network syntactic gcn exactly based dependency tree perform event detection liu et al 2018 propose jointly multiple event extraction jmee framework jointly extract multiple event trigger argument introducing syntactic shortcut arc enhance information ﬂow graph convolution network model graph information fact veriﬁcation fact veriﬁcation task requiring model extract evidence verify given claim however some claim require reasoning multiple piece evidence method like gear zhou et 2019 kgat liu et 2020 proposed conduct evidence aggregating reasoning based fully connected evidence graph zhong et al 2020 build graph formation semantic role labeling achieve promising result application text gnns also applied many task text example gnns also used question answering reading comprehension song et de cao et 2019 qiu et 2019 tu et 2019 ding et 2019 another important direction relational reasoning relational network santoro et 2017 tion network battaglia et 2016 recurrent relational network palm et 2018 proposed solve relational reasoning task based text open problem although gnns achieved great success different ﬁelds remarkable gnn model not good enough offer satisfying solution any graph any condition section list some open problem research robustness family model based neural network gnns also vulnerable adversarial attack compared adversarial tack image text only focus feature attack graph consider structural information several work proposed attack existing graph model zügner et 2018 dai et robust model proposed defend zhu et 2019 refer sun et 2018 comprehensive review interpretability interpretability also important research rection neural model gnns also lack explanation only method ying et 2019 baldassarre azizpour 2019 proposed generate explanation gnn model important apply gnn model cation trusted explanation similar ﬁelds cv nlp interpretability graph also important direction investigate graph pretraining neural model require abundant labeled data costly obtain enormous data supervised method proposed guide model learn beled data easy obtain website knowledge base method achieved great success area cv nlp idea pretraining krizhevsky et 2012 devlin et 2019 recently work focusing pretraining graph qiu et 2020 hu et zhang et 2020 different problem setting focus different aspect ﬁeld still ha many open problem requiring research effort design pretraining task effectiveness existing gnn model learning structural feature information etc complex graph structure graph structure ﬂexible plex real life application various work proposed deal complex graph structure dynamic graph heterogeneous graph discussed rapid development social network internet certainly problem challenge application scenario emerging requiring powerful model conclusion past year graph neural network become powerful practical tool machine learning task graph domain progress owes advance expressive power model ﬂexibility training algorithm survey conduct comprehensive review graph neural network gnn model introduce variant categorized computation module graph type training type moreover also summarize several general framework introduce several theoretical analysis term application taxonomy divide gnn application structural scenario scenario scenario give detailed review tions scenario finally suggest four open problem indicating major challenge future research direction graph neural network including robustness interpretability pretraining plex structure modeling declaration competing interest author declare no known competing ﬁnancial interest personal relationship could appeared inﬂuence work reported paper acknowledgement work supported national key research ment program china no national natural science foundation china nsfc beijing academy artiﬁcial intelligence baai work also supported 2019 tencent marketing solution focused research program zhou et al ai open 1 2020 75 appendix datasets many task related graph released test performance various graph neural network task based following commonly used datasets list datasets table table datasets commonly used task related graph field datasets citation network pubmed yang et 2016 cora yang et 2016 citeseer yang et 2016 dblp tang et 2008 graph mutag debnath et 1991 wale et 2008 ppi zitnik leskovec 2017 dobson doig 2003 protein borgwardt et 2005 ptc toivonen et 2003 social network reddit hamilton et blogcatalog zafarani liu 2009 knowledge graph socher et 2013 bordes et 2013 toutanova et 2015 socher et 2013 bordes et 2013 dettmers et 2018 also broader range open source datasets repository contains graph datasets list table table popular graph learning dataset collection repository introduction link network repository scientiﬁc network data repository interactive visualization mining tool graph kernel datasets benchmark datasets graph kernel relational dataset repository support growth relational machine learning stanford large network dataset collection snap library developed study large social information network open graph benchmark open graph benchmark ogb collection benchmark datasets evaluator graph machine learning pytorch appendix implementation ﬁrst list several platform provide code graph computing table table popular platform graph computing platform link reference pytorch geometric fey lenssen 2019 deep graph library wang et al aligraph zhu et al graphvite zhu et al paddle graph learning euler plato cogdl openne next list hyperlink current open source implementation some famous gnn model table table source code model mentioned survey model link ggnn 2015 neurals fps 2015 chebnet 2016 dngr 2016 sdne 2016 gae 2016 drne 2016 structural rnn 2016 dcnn 2016 gcn 2017 cayleynet 2017 graphsage 2017 gat 2017 cln 2017 ecc 2017 mpnns 2017 monet 2017 continued next column zhou et al ai open 1 2020 76 table continued model link 2018 sse 2018 lgcn 2018 fastgcn 2018 diffpool 2018 graphrnn 2018 molgan 2018 netgan 2018 dcrnn 2018 2018 rgcn 2018 2018 dgcn 2018 gaan 2018 dgi 2019 graphwavenet 2019 han 2019 research ﬁled grows rapidly recommend reader paper list published team gnnpapers gnnpapers recent paper reference allamanis brockschmidt khademi learning represent program graph proc iclr atwood towsley neural network proceeding nip pp bahdanau cho bengio neural machine translation jointly learning align translate proceeding iclr baldassarre azizpour explainability technique graph convolutional network icml workshop learning reasoning representation kostylev monet erez reutter silva logical expressiveness graph neural network proceeding iclr basting titov aziz marcheggiani simaan graph convolutional encoders neural machine translation proceeding emnlp pp battaglia pascanu lai rezende et interaction network learning object relation physic proceeding nip battaglia hamrick bapst zambaldi malinowski tacchetti raposo santoro faulkner et relational inductive bias deep learning graph network arxiv preprint beck haffari cohn learning using gated graph neural network proceeding acl bello pham le norouzi bengio neural combinatorial optimization reinforcement learning arxiv preprint bojchevski klicpera perozzi kapoor blais ozemberczki lukasik günnemann scaling graph neural network approximate pagerank proceeding kdd acm pp bordes usunier weston yakhnenko translating embeddings modeling data proceeding nip borgwardt ong onauer vishwanathan smola kriegel protein function prediction via graph kernel bioinformatics 21 boscaini masci bronstein learning shape correspondence anisotropic convolutional neural network proceeding nip pp bronstein bruna lecun szlam vandergheynst geometric deep learning going beyond euclidean data ieee spm 34 bruna zaremba szlam lecun spectral network locally connected network graph proceeding iclr buades coll morel algorithm image denoising proceeding cvpr ieee pp cai zheng chang active learning graph embedding arxiv preprint cai zheng chang comprehensive survey graph embedding problem technique application ieee tkde 30 chami perozzi e murphy machine learning graph model comprehensive taxonomy arxiv preprint chang ullman torralba tenenbaum compositional approach learning physical dynamic proceeding iclr chen zhu song stochastic training graph convolutional network variance reduction proceeding icml pp chen xiao fastgcn fast learning graph convolutional network via importance sampling proceeding iclr chen wei huang incorporating corporation relationship via graph convolutional neural network stock price prediction proceeding cikm pp chen li gupta iterative visual reasoning beyond convolution proceeding cvpr pp chen villar chen bruna equivalence graph isomorphism testing function approximation gnns proceeding neurips pp chen li peng xie cao xu zheng survey adversarial learning graph arxiv preprint chen lin li li zhou sun measuring relieving smoothing problem graph neural network topological view proceeding aaai cheng dong lapata long machine reading proceeding emnlp pp chiang liu si li bengio hsieh efﬁcient algorithm training deep large graph convolutional network proceeding kdd pp cho van merrienboer gulcehre bahdanau bougares schwenk bengio learning phrase representation using rnn statistical machine translation proceeding emnlp cui wang pei zhu survey network embedding ieee tkde cui henrickson ke wang trafﬁc graph convolutional recurrent neural network deep learning framework trafﬁc learning forecasting arxiv preprint cui zhou yang liu adaptive graph encoder attributed graph embedding proceeding kdd pp dai dai song discriminative embeddings latent variable model structured data proceeding icml dai kozareva dai smola song learning iterative algorithm graph proceeding icml dai li tian huang wang zhu song adversarial attack graph structured data proceeding icml pp de cao kipf molgan implicit generative model small molecular graph icml 2018 workshop theoretical foundation application deep generative model de cao aziz titov question answering reasoning across document graph convolutional network proceeding naacl debnath lopez de compadre debnath shusterman hansch relationship mutagenic aromatic heteroaromatic nitro compound med chem defferrard bresson vandergheynst convolutional neural network graph fast localized spectral ﬁltering proceeding nip pp dehmamy asi yu understanding representation power graph neural network learning graph topology proceeding neurips derr tang signed graph convolutional network proceeding icdm pp dettmers minervini stenetorp riedel convolutional knowledge graph embeddings proceeding aaai devlin chang lee toutanova bert deep bidirectional transformer language understanding proceeding naacl dhillon guan kulis weighted graph cut without eigenvectors multilevel approach ieee tpami 29 ding zhou chen yang tang cognitive graph reading comprehension scale proceeding acl pp tran venkatesh graph transformation policy network chemical reaction prediction proceeding sigkdd zhou et al ai open 1 2020 77 dobson doig distinguishing enzyme structure without alignment mol biol 330 duvenaud maclaurin aguileraiparraguirre gomezbombarelli hirzel aspuruguzik adam convolutional network graph learning molecular ﬁngerprints proceeding nip pp dwivedi joshi laurent bengio bresson benchmarking graph neural network arxiv preprint errica podda bacciu micheli fair comparison graph neural network graph classiﬁcation proceeding iclr fan li zhao tang yin graph neural network social recommendation proceeding www pp feng zhang ji gao hypergraph neural network proceeding aaai vol 33 pp fey lenssen fast graph representation learning pytorch geometric iclr workshop representation learning graph manifold fout byrd shariat protein interface prediction using graph convolutional network proceeding nip pp frasconi gori sperduti general framework adaptive processing data structure ieee tnn 9 fu zhang meng king magnn metapath aggregated graph neural network heterogeneous graph embedding proceeding www gallicchio micheli graph echo state network proceeding ijcnn ieee pp gao ji graph proceeding icml pp gao wang ji learnable graph convolutional network proceeding kdd acm pp gao yang zhou wu pan hu active discriminative network representation learning proceeding ijcai garcia bruna learning graph neural network proceeding iclr garg jegelka jaakkola generalization representational limit graph neural network proceeding icml gasse etelat ferroni charlin lodi exact combinatorial optimization graph convolutional neural network proceeding neurips pp gehring auli grangier dauphin convolutional encoder model neural machine translation proceeding acl 1 pp gilmer schoenholz riley vinyals dahl neural message passing quantum chemistry proceeding icml gori monfardini scarselli new model learning graph domain proceeding ijcnn ieee pp goyal ferrara graph embedding technique application performance survey knowl base sys 151 grover leskovec scalable feature learning network proceeding kdd acm pp grover zweig ermon graphite iterative generative modeling graph proceeding icml pp gu hu wang wei dai learning region feature object detection proceeding eccv pp guo lin feng song wan attention based graph convolutional network trafﬁc ﬂow forecasting proceeding aaai 33 hamaguchi oiwa shimbo matsumoto knowledge transfer entity graph neural network approach proceeding ijcai pp hamilton ying leskovec inductive representation learning large graph proceeding nip pp hamilton ying leskovec representation learning graph method application ieee data base engineering bulletin 40 hamilton zhang jurafsky leskovec loyalty online community proceeding icwsm pp hammer micheli sperduti strickert recursive network model neural network 17 hammond vandergheynst gribonval wavelet graph via spectral graph theory appl comput harmon anal 30 hassani khasahmadi contrastive representation learning graph proceeding icml pp zhang ren sun deep residual learning image recognition proceeding cvpr pp zhang ren sun identity mapping deep residual network proceeding eccv springer pp henaff bruna lecun deep convolutional network data arxiv preprint hochreiter schmidhuber long memory neural comput 9 hoshen vain attentional predictive modeling proceeding nip pp hu gu zhang dai wei relation network object detection proceeding cvpr pp hu dong wang sun heterogeneous graph transformer proceeding www pp hu dong wang chang sun generative graph neural network proceeding kdd pp hu xiong qu yuan e liu tang graph policy network transferable active learning graph proceeding neurips hu fey zitnik dong ren liu catasta leskovec open graph benchmark datasets machine learning graph proceeding neurips hu liu gomes zitnik liang pande leskovec strategy graph neural network proceeding iclr huang liu van der maaten weinberger densely connected convolutional network proceeding cvpr pp huang zhang rong huang adaptive sampling towards fast graph representation learning proceeding neurips huang xu duan ren feng wang modeling complex spatial pattern temporal feature via heterogenous graph embedding network arxiv preprint jaeger echo state approach analysing training recurrent neural erratum note vol german national research center information technology gmd technical report jain zamir savarese saxena deep learning graph proceeding cvpr pp kampffmeyer chen liang wang zhang xing rethinking knowledge graph propagation learning proceeding cvpr pp kearnes mccloskey berndl pande riley molecular graph convolution moving beyond ﬁngerprints comput aided mol de 30 keriven e universal invariant equivariant graph neural network proceeding neurips pp khalil dai zhang dilkina song learning combinatorial optimization algorithm graph proceeding nip khamsi kirk introduction metric space fixed point theory vol john wiley son kim jeong lee kim kang hat hierarchical graph attention network stock movement prediction arxiv preprint kipf welling variational graph nip bayesian deep learning workshop kipf welling classiﬁcation graph convolutional network proceeding iclr kipf fetaya wang welling zemel neural relational inference interacting system proceeding icml pmlr pp klicpera bojchevski günnemann predict propagate graph neural network meet personalized pagerank proceeding iclr knyazev taylor amer understanding attention generalization graph neural network proceeding neurips pp kool van hoof welling attention learn solve routing problem proceeding iclr krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural network proceeding nip landrieu simonovsky point cloud semantic segmentation superpoint graph proceeding cvpr lecun bottou bengio haffner learning applied document recognition proceeding ieee 86 pp lecun bengio hinton deep learning nature 521 lee rossi kim ahmed koh attention model graph survey tkdd 13 lee fang yeh wang learning structured knowledge graph proceeding cvpr pp lee lee kang graph pooling proceeding icml pp levi finite geometrical system six public lectues delivered february university calcutta university calcutta levie huang bucci bronstein kutyniok transferability spectral graph convolutional neural network arxiv preprint li tarlow brockschmidt zemel gated graph sequence neural network proceeding iclr li wang zhu huang adaptive graph convolutional neural network proceeding aaai 32 li yu shahabi liu diffusion convolutional recurrent neural network trafﬁc forecasting proceeding iclr li han wu deeper insight graph convolutional network learning proceeding aaai li vinyals dyer pascanu battaglia learning deep generative model graph arxiv preprint li chen koltun combinatorial optimization graph convolutional network guided tree search proceeding neurips li muller thabet ghanem deepgcns gcns go deep cnns proceeding iccv pp li gu dullien vinyals kohli graph matching network learning similarity graph structured object proceeding icml pp li bao harimoto chen xu su modeling stock relation graph network overnight stock movement prediction proceeding ijcai pp liang shen feng lin yan semantic object parsing graph lstm proceeding eccv pp liang lin shen feng yan xing interpretable evolving lstm proceeding cvpr pp liu luo huang jointly multiple event extraction via graph information aggregation proceeding emnlp zhou et al ai open 1 2020 78 liu kumar ba kiros swersky graph normalizing ﬂows proceeding neurips pp liu xiong sun liu fact veriﬁcation kernel graph attention network proceeding acl pp loukas graph neural network not learn depth v width proceeding iclr chen xiao constrained generation semantically valid graph via regularizing variational autoencoders proceeding neurips wang aggarwal tang graph convolutional network eigenpooling proceeding kdd pp wang aggarwal yin tang graph convolutional network proceeding sdm pp mallat wavelet tour signal processing elsevier manessi rozza manzo dynamic graph convolutional network pattern recogn 97 marcheggiani titov encoding sentence graph convolutional network semantic role labeling proceeding emnlp pp marcheggiani basting titov exploiting semantics neural machine translation graph convolutional network proceeding naacl marino salakhutdinov gupta know using knowledge graph image classiﬁcation proceeding cvpr maron shamir lipman invariant equivariant graph network proceeding iclr maron fetaya segol lipman universality invariant network proceeding icml pmlr pp masci boscaini bronstein vandergheynst geodesic convolutional neural network riemannian manifold iccv workshop matsunaga suzumura takahashi exploring graph neural network stock market prediction rolling window analysis arxiv preprint arxiv micheli neural network graph contextual constructive approach ieee tnn 20 micheli sona sperduti contextual processing structured data recursive cascade correlation ieee tnn 15 mikolov chen corrado dean efﬁcient estimation word representation vector space proceeding iclr miwa bansal relation extraction using lstms sequence tree structure proceeding acl monti boscaini masci rodola svoboda bronstein geometric deep learning graph manifold using mixture model cnns proceeding cvpr pp morris ritzert fey hamilton lenssen rattan grohe weisfeiler leman go neural graph neural network proceeding aaai 33 narasimhan lazebnik schwing box reasoning graph convolution net factual visual question answering proceeding neurips pp nguyen grishman graph convolutional network pooling event detection proceeding aaai niepert ahmed kutzkov learning convolutional neural network graph proceeding icml norcliffebrown vafeias parisot learning conditioned graph structure interpretable visual question answering proceeding neurips pp nowak villar bandeira bruna revised note learning quadratic assignment graph neural network ieee dsw ieee pp nt maehara revisiting graph neural network filter arxiv preprint oono suzuki graph neural network exponentially lose expressive power node classiﬁcation proceeding iclr palm paquet winther recurrent relational network proceeding neurips pan hu long jiang yao zhang adversarially regularized graph autoencoder graph embedding proceeding ijcai pareja domeniconi chen suzumura kanezashi kaler schardl leiserson evolvegcn evolving graph convolutional network dynamic graph proceeding aaai 34 park lee chang lee choi symmetric graph convolutional autoencoder unsupervised graph representation learning proceeding iccv pp peng poon quirk toutanova yih relation extraction graph lstms tacl 5 peng li liu bao wang song yang hierarchical text classiﬁcation recursively regularized deep proceeding www pp peng choi xu graph embedding combinatorial optimization survey arxiv preprint perozzi skiena deepwalk online learning social representation proceeding kdd acm pp pham tran phung venkatesh column network collective classiﬁcation proceeding aaai pp qi su mo guibas pointnet deep learning point set classiﬁcation segmentation proceeding cvpr qi liao jia fidler urtasun graph neural network rgbd semantic segmentation proceeding cvpr qi wang jia shen zhu learning interaction graph parsing neural network proceeding eccv pp qiu xiao qu zhou li zhang yu dynamically fused graph network reasoning proceeding acl pp qiu chen dong zhang yang ding wang tang gcc graph contrastive coding graph neural network proceeding kdd rahimi cohn baldwin user geolocation via graph convolutional network proceeding acl vol 1 pp raposo santoro barrett pascanu lillicrap battaglia discovering object relation entangled scene representation proceeding iclr rhee seo kim hybrid approach relation network localized graph convolutional filtering breast cancer subtype classiﬁcation proceeding ijcai pp riba fischer e learning graph distance message passing neural network proceeding icpr ieee pp rossi tiezzi dimitri bianchini maggini scarselli learning graph neural network iapr workshop artiﬁcial neural network pattern recognition springer pp ruiz chamon ribeiro graphon neural network transferability graph neural network proceeding neurips rusek mestres unveiling potential graph neural network network modeling optimization sdn proceeding sosr pp russakovsky deng su krause satheesh huang karpathy khosla bernstein et imagenet large scale visual recognition challenge proceeding ijcv 115 pp sanchez heess springenberg merel hadsell riedmiller battaglia graph network learnable physic engine inference control proceeding icml pp santoro raposo barrett malinowski pascanu battaglia lillicrap simple neural network module relational reasoning proceeding nip sato yamada kashima approximation ratio graph neural network combinatorial problem proceeding neurips pp scarselli gori tsoi hagenbuchner monfardini graph neural network model ieee tnn 20 scarselli tsoi hagenbuchner dimension graph recursive neural network neural network 108 schlichtkrull kipf bloem van den berg titov welling modeling relational data graph convolutional network proceeding eswc springer pp selsam lamm bünz liang de moura dill learning sat solver supervision proceeding iclr shang tang huang bi zhou convolutional network knowledge base completion proceeding aaai 33 shchur mumme bojchevski günnemann pitfall graph neural network evaluation arxiv preprint shchur zugner bojchevski gunnemann netgan generating graph via random walk proceeding icml pp shi xu zhu zhang zhang tang graphaf autoregressive model molecular graph generation proceeding iclr shuman narang frossard ortega vandergheynst emerging ﬁeld signal processing graph extending data analysis network irregular domain ieee spm 30 simonovsky komodakis dynamic ﬁlters convolutional neural network graph proceeding cvpr pp socher chen manning ng reasoning neural tensor network knowledge base completion proceeding nip pp song zhang wang gildea model text generation proceeding acl pp song zhang wang gildea relation extraction using graph state lstm proceeding emnlp pp song wang yu zhang florian gildea exploring structured passage representation reading comprehension graph neural network arxiv preprint sperduti starita supervised neural network classiﬁcation structure ieee tnn 8 sukhbaatar fergus et learning multiagent communication backpropagation proceeding nip pp sun han yan yu wu pathsim meta similarity search heterogeneous information network proceeding vldb endowment vol 4 pp sun wang yu li adversarial attack defense graph data survey arxiv preprint sun hoffmann verma tang infograph unsupervised supervised representation learning via mutual information maximization proceeding iclr sutton barto reinforcement learning introduction mit press tai socher manning improved semantic representation structured long memory network proceeding ijcnlp pp tang zhang yao li zhang su arnetminer extraction mining academic social network proceeding kdd acm pp zhou et al ai open 1 2020 79 tang qu wang zhang yan mei line information network embedding proceeding www pp teney liu den hengel representation visual question answering proceeding cvpr pp tiezzi marra melacci maggini deep lagrangian propagation graph neural network arxiv preprint toivonen srinivasan king kramer helma statistical evaluation predictive toxicology challenge bioinformatics 19 toutanova chen pantel poon choudhury gamon representing text joint embedding text knowledge base proceeding emnlp pp tsitsulin palowitch perozzi müller graph clustering graph neural network arxiv preprint tu wang huang tang zhou reading comprehension across multiple document reasoning heterogeneous graph proceeding acl pp van den berg kipf welling graph convolutional matrix completion arxiv preprint vaswani shazeer parmar jones uszkoreit gomez kaiser attention need proceeding nip pp velickovic cucurull casanova romero lio bengio graph attention network proceeding iclr velickovic fedus hamilton bengio hjelm deep graph infomax proceeding iclr verma zhang stability generalization graph convolutional neural network proceeding kdd pp vinyals bengio kudlur order matter sequence sequence set arxiv preprint vinyals fortunato jaitly pointer network proceeding nip pp wale watson karypis comparison descriptor space chemical compound retrieval classiﬁcation knowl inf syst 14 wang leskovec unifying graph convolutional neural network label propagation arxiv preprint wang pan long zhu jiang mgae marginalized graph autoencoder graph clustering proceeding cikm pp wang girshick gupta neural network proceeding cvpr pp wang lv lan zhang knowledge graph alignment via graph convolutional network proceeding emnlp wang chen ren yu cheng lin deep reasoning knowledge graph social relationship understanding proceeding ijcai wang ye gupta recognition via semantic embeddings knowledge graph proceeding cvpr wang sun liu sarma bronstein solomon dynamic graph cnn learning point cloud acm transaction graphic wang ji shi wang ye cui yu heterogeneous graph attention network proceeding www pp wang yu zheng gan gai ye li zhou huang huang guo zhang lin zhao li smola zhang deep graph library towards efﬁcient scalable deep learning graph iclr workshop representation learning graph manifold watters zoran weber battaglia pascanu tacchetti visual interaction network learning physic simulator video proceeding nip pp wu lian xu wu chen graph convolutional network markov random ﬁeld reasoning social spammer detection proceeding aaai 34 pp wu pan chen long zhang yu comprehensive survey graph neural network arxiv preprint wu souza zhang fifty yu weinberger simplifying graph convolutional network volume 97 proceeding machine learning research pmlr pp wu zhang gao weng gao chen dual graph attention network deep latent representation multifaceted social effect recommender system proceeding www pp xu li tian sonobe kawarabayashi jegelka representation learning graph jumping knowledge network proceeding icml pp xu shen cao qiu cheng graph wavelet neural network proceeding iclr xu hu leskovec jegelka powerful graph neural network proceeding iclr xu wang yu feng song wang yu knowledge graph alignment via graph matching neural network proceeding acl association computational linguistics pp xu wang chen tao zhao gnn dual graph neural network predicting structured entity interaction proceeding ijcai pp yan xiong lin spatial temporal graph convolutional network action recognition proceeding aaai yang liu zhao sun chang network representation learning rich text information proceeding ijcai pp yang cohen salakhudinov revisiting learning graph embeddings proceeding icml pmlr pp yang wei chen wu using external knowledge ﬁnancial event prediction based graph neural network proceeding cikm pp yang xiao zhang sun han heterogeneous network representation learning survey benchmark evaluation beyond arxiv preprint yao mao luo graph convolutional network text classiﬁcation proceeding aaai 33 ying chen eksombatchai hamilton leskovec graph convolutional neural network recommender system proceeding kdd update ying morris ren hamilton leskovec hierarchical graph representation learning differentiable pooling proceeding neurips ying bourgeois zitnik leskovec gnnexplainer generating explanation graph neural network proceeding neurips liu ying pande leskovec graph convolutional policy network molecular graph generation proceeding neurips ying ren hamilton leskovec graphrnn generating realistic graph deep model proceeding icml ying leskovec design space graph neural network proceeding neurips yu yin zhu graph convolutional network deep learning framework trafﬁc forecasting proceeding ijcai pp yun jeong kim kang kim graph transformer network proceeding neurips pp zafarani liu social computing data repository asu zaheer kottur ravanbakhsh poczos salakhutdinov smola deep set proceeding nip pp zang wang neural dynamic complex network proceeding kdd pp zayats ostendorf conversation modeling reddit using structured lstm tacl 6 zeng zhou srivastava kannan prasanna graphsaint graph sampling based inductive learning method proceeding iclr zhang yin zhu zhang network representation learning survey ieee tbd 6 1 zhang cui zhu deep learning graph survey ieee tkde zhang shi xie king yeung gaan gated attention network learning large spatiotemporal graph proceeding uai zhang liu song lstm text representation proceeding acl 1 pp zhang cui neumann chen deep learning architecture graph classiﬁcation proceeding aaai zhang qi manning graph convolution pruned dependency tree improves relation extraction proceeding emnlp pp zhang tong xu maciejewski graph convolutional network comprehensive review computational social network 6 1 zhang song huang swami chawla heterogeneous graph neural network proceeding kdd pp zhang liu li wu attributed graph clustering via adaptive graph convolution proceeding ijcai pp zhang liu tang dong yao zhang gu wang shao li et oag toward linking heterogeneous entity graph proceeding kdd zhang zhang sun xia only attention needed learning graph representation arxiv preprint zheng dan aragam ravikumar xing learning sparse nonparametric dag proceeding aistats pmlr pp zheng fan wang qi gman graph network trafﬁc prediction proceeding aaai 34 zhong xu tang xu duan zhou wang yin reasoning graph fact checking proceeding acl zhou han yang liu wang li sun gear evidence aggregating reasoning fact veriﬁcation proceeding acl pp zhu zhang cui zhu robust graph convolutional network adversarial attack proceeding kdd zhu zhao yang lin zhou ai li zhou aligraph proceeding vldb endowment 12 12 zhu xu qu tang graphvite hybrid system node embedding proceddings www acm pp zhuang dual graph convolutional network supervised classiﬁcation proceeding www pp zhou et al ai open 1 2020 80 zilly srivastava koutnik schmidhuber recurrent highway network proceeding icml pp zitnik leskovec predicting multicellular function tissue network bioinformatics 33 zitnik agrawal leskovec modeling polypharmacy side effect graph convolutional network bioinformatics 34 zou hu wang jiang sun gu importance sampling training deep large graph convolutional network proceeding neurips pp zügner akbarnejad günnemann adversarial attack neural network graph data proceeding kdd pp rossi frasca chamberlain eynard bronstein monti sign scalable inception graph neural network arxiv preprint zhou et al ai open 1 2020 81