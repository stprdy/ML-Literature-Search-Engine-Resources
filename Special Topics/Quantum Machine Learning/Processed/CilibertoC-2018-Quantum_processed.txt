article power data quantum machine learning michael masoud ryan sergio boixo 1 hartmut jarrod mcclean use quantum computing machine learning among exciting prospective application quantum technology however machine learning task data vided considerably different commonly studied computational task work show some problem classically hard compute easily predicted classical machine learning data using rigorous prediction error bound tion develop methodology assessing potential quantum advantage learning task bound tight asymptotically empirically predictive wide range learning model construction explain numerical result showing help data classical machine learning model competitive quantum model even tailored quantum problem propose projected quantum model provides simple rigorous quantum learning problem regime implementation demonstrate signiﬁcant prediction advantage some classical model engineered data set designed demonstrate maximal quantum advantage one largest numerical test quantum machine learning date 30 qubits open 1 google quantum ai venice ca usa 2 institute quantum information matter caltech pasadena ca usa 3 department computing mathematical science caltech pasadena ca usa jmcclean nature communication 2021 1 1234567890 quantum technology continue rapidly advance becomes increasingly important understand application beneﬁt power device time machine learning classical computer ha made great stride revolutionizing application image nition text translation even physic application computational power leading ever increasing quantum computer could accelerate machine learning potential impact enormous least two path towards quantum enhancement machine learning considered first motivated quantum application power quantum ing could principle used help improve training process existing classical enhance inference graphical could include ﬁnding better optimum training landscape ﬁnding optimum fewer query ever without structure known problem tage along line may limited quadratic small polynomial second vein interest possibility using quantum model generate correlation variable ﬁcient represent classical computation recent success theoretically experimentally demonstrating quantum computation beyond classical tractability taken evidence quantum computer sample probability distribution exponentially difﬁcult sample distribution coincide world distribution would suggest potential niﬁcant advantage typically type advantage ha sought recent work quantum neural seek parameterize distribution some set adjustable parameter quantum kernel use quantum computer deﬁne feature map map classical data quantum hilbert space justiﬁcation capability method exceed classical model often follows similar line ref quantum simulation result model leverage quantum circuit hard sample result classically potential quantum advantage work show quantitatively picture incomplete machine learning ml problem some training data provided provided data elevate classical model rival quantum model even quantum circuit generating data hard compute classically begin motivating example argument showing classical algorithm data match quantum output following provide rigorous prediction error bound training classical quantum ml method based kernel learn quantum mechanical model focus kernel method not only provide provable guarantee also ﬂexible function learn example recent advancement theoretical machine learning show training neural network large hidden layer equivalent training ml model particular kernel known neural tangent throughout refer classical ml model related theoretical development referring ml model easily associated kernel either explicitly kernel method implicitly neural tangent kernel however numerical section also include performance comparison method direct association kernel challenging random forest method quantum case also show quantum ml based kernel made equivalent training inﬁnite depth quantum neural network use prediction error bound devise ﬂowchart testing potential quantum prediction advantage separation prediction error quantum classical ml model ﬁxed amount training data important test geometric difference kernel function deﬁned classical quantum ml formally geometric difference deﬁned closest efﬁcient classical ml model practice one consider geometric difference respect suite optimized classical ml model geometric difference small classical ml method guaranteed provide similar better performance prediction dataset independent function value label hence sent powerful function independent prescreening allows one evaluate any possibility better performance hand geometry differs greatly show existence dataset exhibit large prediction advantage using quantum ml model one construct efﬁciently tool develop could used compare construct hard classical model like hash function enforce restriction allow u say something quantum separation particular feature map white box quantum circuit speciﬁcation available ideal feature map feature map made tionally hard evaluate classically constructive example discrete log feature map provable separation kernel given supplementary section additionally minimum classical model mean classical hash tions reproduced formally deﬁnition moreover application tool existing model literature rule many immediately providing powerful sieve focusing development new data encoding following construction numerical experiment ﬁnd variety common quantum model literature perform similarly worse classical ml classical quantum datasets due small geometric difference small geometric difference consequence exponentially large hilbert space employed existing quantum model input far apart circumvent setback propose improvement enlarges geometric difference projecting quantum state embedded classical data back approximate classical large metric difference endowed projected quantum model able construct engineered datasets demonstrate large prediction advantage common classical ml model numerical experiment 30 qubits despite construction based method associated kernel ﬁnd empirically prediction advantage remains robust across tested classical method including without easily determined kernel open possibility use small quantum computer generate efﬁciently veriﬁable machine learning problem could challenging classical ml model result setup motivating example begin setting problem method interest classical quantum model provide simple motivating example studying data increase power classical model quantum data focus supervised learning task collection n training example xi yi xi input data yi associated label value assume xi sampled independently data distribution theoretical analysis consider yi 2 r generated some quantum model particular consider article nature communication 2 nature communication 2021 continuous encoding unitary map classical input data xi quantum state xi uencðxiþ 0 j refer corresponding density matrix ρ xi expressive power embeddings investigated functional analysis point however setting data provided requires special attention encoding unitary followed unitary uqnn θ measure observable quantum neural network produce input xi given yi f ðxiþ xi qnnouqnn xi quantum model considered also referred quantum neural network qnn goal understand easy predict function f x training machine learning model notation place turn simple motivating example understand availability data machine learning task change computational hardness consider data point fxign classical vector 1 use amplitude encode data state xi xk k j xk individual coordinate vector xi uqnn hamiltonian function f ðxþ x h juy qnnouqnn x j general hard compute even single input state particular following proposition showing classical algorithm compute f x efﬁciently quantum computer no powerful classical computer see supplementary section 1 proof proposition classical algorithm without training data compute f x efﬁciently any uqnn bpp bqp nevertheless incorrect conclude training classical model data learn evolution hard see write expectation value f ðxiþ p k h j uy qnnouqnn p xl l j p p xl quadratic function coefﬁcients bkl k h juy qnnouqnn l j using theory developed later work show any uqnn training speciﬁc classical ml model collection n training example xi yi f xi would give rise prediction model h xi f ðxþj ﬃﬃﬃﬃ ﬃ n r constant c refer supplementary section 1 proof result hence n training data one train classical ml model predict function f x additive prediction error elevation classical model some training sample illustrative power data supplementary section 2 give rigorous theoretic argument computational power provided data cartoon depiction complexity separation induced data provided fig 1 simple example make basic point sufﬁcient data change complexity consideration perhaps open question answer example us rather weak encoding amplitude assumes one ha access amount data par dimension model interesting case occur strengthen data encoding include modern classical ml model consider number data n much le dimension model interesting case one quantitatively answer fig 1 illustration relation complexity class ﬂowchart understanding prescreening potential quantum advantage cartoon separation problem complexity created addition data problem classical algorithm learn data deﬁne complexity class solve problem beyond classical computation bpp still expected quantum computation efﬁciently solve problem classical ml algorithm data not rigorous deﬁnition proof separation classical algorithm learn data given supplementary section b ﬂowchart develop understanding potential quantum prediction advantage n sample data potentially inﬁnite depth qnn made encoding function circuit uenc uqnn provided input along quantum classical method associated kernel test given function n emphasize role data possibility prediction advantage one ﬁrst evaluate geometric quantity gcq measure possibility advantageous prediction separation without yet considering actual function learn show one efﬁciently construct adversarial function saturates limit test passed otherwise classical approach guaranteed match performance any function data subsequently consider actual function provided test may run using model complexity sc sq one speciﬁcally us quantum kernel qk method red dashed arrow evaluate possible choice uqnn lead easy classical function chosen encoding data nature communication article nature communication 2021 3 primary interest ml algorithm much stronger ﬁtting quadratic function input data provided interesting way amplitude encoding work focus classical quantum ml model based kernel function k xi xj high level kernel function seen measure similarity k xi xj large xi xj close considered ﬁnite input data kernel function may represented matrix kij k xi xj condition required kernel method satisﬁed matrix representation hermitian positive given kernel function corresponds nonlinear feature mapping ϕ x map x possibly feature space kðxi xjþ ϕðxiþyϕðxjþ basis kernel trick intricate powerful map ϕ xi implemented evaluation relatively simple kernel function simple case example using kernel k xi xj corresponds feature map ϕðxiþ xl k j l j capable learning quadratic function amplitude kernel based ml algorithm trained model always written h x x w vector feature space deﬁned kernel example training convolutional neural network large hidden equivalent using corresponding neural tangent kernel kcnn feature map ϕcnn kernel kcnn nonlinear mapping extract local property quantum mechanic similarly kernel function deﬁned using native geometry quantum state space x j example deﬁne kernel function using output kernel method like classical support vector deﬁnes quantum kernel method wide class function learned sufﬁciently large amount data using right kernel function example contrast perhaps natural kernel quantum kernel kq xi xj tr ρ xi ρ xj learn arbitrarily deep quantum neural network uqnn measure any observable shown supplementary section 3 gaussian kernel kγðxi xjþ hyperparameter γ learn any continuous function compact includes learning any qnn theless required amount data n achieve small prediction error could large worst case although work kernel deﬁned quantum space due expressive property terminology past work refer kqðxi xjþ tr ρðxiþρðxjþ quantum kernel method throughout work also deﬁnition given testing quantum advantage construct general framework assessing potential quantum prediction advantage machine learning task beginning general result build intuition practical test based metry learning space framework summarized fig foundation general prediction error bound training ml model predict some quantum model deﬁned f x tr ouρ x derived concentration ities ou uy qnnouqnn suppose obtained n training example xi yi f xi training data exists ml algorithm output h x x using kernel kðxi xjþ kij ϕðxiþyϕðxjþ ha simpliﬁed prediction error bounded f ðxþj ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ skðnþ n r constant c 0 n independent sample data distribution note subsequent bound key dependence quantity data n reﬂecting role data improve prediction performance due scaling freedom αϕ x assumed ϕðxiþyϕðxiþ tr ðkþ derivation result given supplementary section given core prediction error bound seek understand implication main quantity determines prediction error skðnþ n n tr ðouρðxiþþ tr ðouρðxjþþ quantity sk n equal model complexity trained function h x x sk n training smaller value sk n implies better generalization new data x sampled distribution intuitively sk n measure whether closeness xi xj deﬁned kernel function k xi xj match well closeness observable expectation quantum state ρ xi ρ xj recalling larger kernel value indicates two point closer computation sk n performed efﬁciently classical computer inverting n n matrix k obtaining n value tr ouρ xi performing order n experiment physical quantum device time complexity scale order due connection model complexity regularization term often added optimization problem training h x x see ref regularization prevents sk n becoming large expense not completely ﬁtting training data detailed discussion proof tion given supplementary section 4 prediction error upper bound often shown asymptotically tight proving matching lower bound example k xi xj quantum kernel tr ρ xi ρ xj deduce sk n hence one would need number data n scaling tr supplementary section 8 give matching lower bound showing scaling tr unavoidable assume large hilbert space dimension lower bound hold any learning algorithm not only quantum kernel method lower bound proof us mutual information analysis could easily extend kernel proof strategy also employed extensively devise upper lower bound classical quantum ml learning quantum model furthermore not only bound asymptotically tight numerical experiment given supplementary section 13 ﬁnd prediction error bound also capture performance classical ml model not based kernel constant factor observed quite modest given some set data sk n found small relative n training classical ml model quantum model f x predicted accurately even f x hard compute classically any given order formally evaluate potential quantum prediction advantage generally one must take sk n minimal efﬁcient classical model however focused minimally attainable value reasonable set classical method tuned meter prescribes effective method evaluating potential quantum advantage practice already rule considerable number example literature bound see potential advantage one ml algorithm deﬁned predict better another ml algorithm deﬁned depends largest possible separation dataset separation characterized deﬁning asymmetric geometric difference article nature communication 4 nature communication 2021 depends dataset independent function value label hence evaluating quantity good ﬁrst step understanding potential quantum advantage shown fig quantity deﬁned ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ jj ﬃﬃﬃﬃﬃﬃ p ﬃﬃﬃﬃﬃﬃ p q spectral norm resulting matrix assume tr tr one show implies prediction error bound c ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ p ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ p detailed derivation given supplementary section c illustration found fig geometric difference g computed classical computer performing singular value tion n n matrix standard numerical analysis provide highly efﬁcient computation singular value decomposition time order intuitively xi xj xi xj large geometric difference small value grows kernel deviate see explicitly geometric difference allows one make statement possibility one ml model make different prediction another consider geometric difference gcq g classical ml model kernel kc xi xj quantum ml model kq xi xj tr ρ xi ρ xj gcq small sc cqsq classical ml model always similar better model complexity sk n compared quantum ml model implies prediction performance classical ml likely competitive better quantum ml model one likely prefer using classical model captured ﬁrst step ﬂowchart fig contrast gcq large show exists dataset sc cqsq quantum model exhibiting superior prediction performance efﬁcient method explicitly construct maximally divergent dataset given supplementary section 7 numerical demonstration stability separation provided next section formal statement classical method generally requires deﬁning overall efﬁcient classical method practice consider gcq minimum geometric difference among suite optimized classical ml model engineered approach minimizes value hyperparameter search ﬁnd best classical adversary show remarkable robustness across classical method including without associated kernel random speciﬁc case quantum kernel method kq ij kqðxi xjþ tr ðρðxiþρðxjþþ gain additional insight model complexity sk sometimes make conclusion classically learnability possible uqnn given encoding data let u deﬁne vec x hermitian matrix x vector containing real imaginary part entry case ﬁnd sq vecðouþ tpqvecðouþ pq projector onto subspace formed vec ρ vec ρ xn highlight dim ðpqþ rank ðkqþ deﬁnes effective dimension quantum state space spanned training data illustration dimension found fig pq projector ha eigenvalue 0 1 sq vecðouþ tvecðouþþ minðd tr assuming hence case quantum kernel method prediction error bound may written f ðxþj ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ minðd tr n r detailed derivation given supplementary section also consider approximate dimension small lues kq truncated incurring small training error fig 2 cartoon geometry kernel function deﬁned classical quantum ml model letter b represent data point xi different space arrow representing similarity measure kernel function data geometric difference g difference similarity measure arrow different ml model effective dimension dataset quantum hilbert space nature communication article nature communication 2021 5 obtaining kq quantum device dimension computed efﬁciently classical machine performing singular value decomposition n n matrix kq estimation tr performed sampling random state ψ quantum measuring ψ performing statistical analysis measurement prediction error bound show quantum kernel method learn any uqnn dimension training set space squared frobenius norm observable tr much smaller amount data supplementary section 8 show quantum kernel method optimal learning quantum model bounded tr saturate fundamental lower bound however practice observables pauli operator exponentially large tr central quantity dimension using prediction error bound quantum kernel method gcq minðd tr small classical ml would also able learn any uqnn case one must conclude given encoding data classically easy not affected arbitrarily deep uqnn constitutes bottom left part ﬂowchart fig ultimately see prediction advantage particular dataset speciﬁc function need large separation sc sq happens input xi xj considered close quantum ml model actually close target function f x far classical ml represented ﬁnal test fig 1 methodology outline result achieved term essential component projected quantum kernel addition analyzing existing quantum model analysis approach introduced also provides suggestion new quantum model improved property address example start original quantum kernel effective dimension large kernel tr ρ xi ρ xj based metric regard data far kernel matrix kq close identity result small geometric ence gcq leading classical ml model competitive outperforming quantum kernel method supplementary section 9 present simple quantum model requires exponential amount sample learn using quantum kernel tr ρ xi ρ xj only need linear number sample learn using classical ml model circumvent setback propose family projected quantum kernel solution kernel work projecting quantum state approximate classical representation using reduced physical observables classical even training set space ha large dimension n projection allows u reduce dimensional classical space generalize better going exponentially large quantum hilbert space projected quantum kernel challenging evaluate without quantum computer numerical experiment ﬁnd classical projection increase rather decrease geometric difference classical ml model construction foundation best performing quantum method later one simplest form projected quantum kernel measure reduced density matrix qubits encoded state ρk xi ρ xi deﬁne kernel kpqðxi xjþ exp k jjρkðxiþ f kernel deﬁnes feature map function space capable expressing arbitrary function power quantum state nonintuitive result density functional theory know even one body density sufﬁcient determining exact ground property system modest assumption supplementary section 10 provide example projected quantum kernel includes efﬁcient method computing kernel function contains order rdms using local randomized measurement formalism classical classical shadow formalism allows efﬁcient construction rdms ments supplementary section 11 show projected version quantum kernel lead simple rigorous quantum recently proposed learning problem based discrete numerical study provide numerical evidence 30 qubits support theory relation dimension geometric difference g prediction formance using projected quantum kernel geometric difference g much larger see strongest empirical advantage scalable quantum model quantum datasets date largest combined simulation analysis digital quantum machine learning aware make use tensorflow reaching peak throughput quadrillion ﬂoating point operation per second trend quantum simulation 800 classical analysis observed maximum experiment size overall ﬂoating point operation across experiment totalling quintillion exaﬂop order mimic data distribution pertains data conduct experiment around image classiﬁcation distinguishing clothing item challenging original based mnist preprocess data using principal component transform image vector data provided quantum classical model classical case data input vector quantum case us given circuit embed vector space n qubits quantum embeddings explore three option separable rotation embedding hamiltonian evolution circuit explicit construction supplementary section classical ml task c goal correctly identify image shirt dress original dataset quantum ml task use source data embeddings take function value expectation value local observable ha evolved quantum neural network resembling trotter evolution model random coupling case embedding taken part ground truth resulting function different depending quantum embedding ml task compare best performing model list standard classical ml algorithm properly tuned hyperparameters see supplementary tion 12 detail fig 3 give comparison prediction performance classical quantum ml model one see not only classical ml model perform best original classical dataset prediction performance classical method quantum datasets also competitive even outperform existing quantum ml model despite quantum ml model access training embedding article nature communication 6 nature communication 2021 classical method not performance classical ml model especially strong dataset q dataset q elevation classical performance evidence power data moreover intriguing behavior lack quantum advantage may explained considering effective dimension geometric difference g following theoretical construction fig see dimension original quantum state space grows rather quickly geometric difference g becomes small dimension becomes large standard quantum kernel saturation dimension coincides decreasing statistical ﬂuctuations performance seen fig moreover given poor ml performance natural instinct throw resource problem qubits demonstrated naïve quantum kernel method likely lead tiny inner product even worse performance contrast projected quantum space ha low dimension even grows yield higher geometric difference g embeddings system size methodology predicts g small classical ml model competitive outperform quantum ml model veriﬁed fig original projected quantum kernel small geometric difference g lead good performance classical ml model no large quantum advantage seen only geometric difference g large projected kernel method embedding see some mild advantage best classical method result hold disregarding any detail quantum evolution trying learn even one hard simulate classically fig 3 relation dimension geometric difference g prediction performance shaded region standard deviation 10 independent run n number qubits quantum encoding dimension input classical encoding approximate dimension geometric difference g classical ml model quantum kernel q projected quantum kernel pq different embeddings system size b prediction error lower better quantum kernel method q projected quantum kernel method pq classical ml model classical c quantum q datasets number data n grows large geometric difference g quantum kernel becomes small see small geometric difference g always result classical ml competitive outperforming quantum ml model g large potential improvement classical ml example projected quantum kernel improves upon best classical ml dataset q fig 4 prediction accuracy higher better engineered datasets label function engineered match geometric difference g projected quantum kernel classical approach demonstrating signiﬁcant gap quantum best classical model 30 qubits g large consider best performing classical ml model among gaussian svm linear svm adaboost random forest neural network gradient boosting only report accuracy quantum kernel method system size n 28 due high simulation cost inferior performance nature communication article nature communication 2021 7 order push limit separation quantum classical approach learning setting consider set engineered datasets function value designed saturate geometric inequality sc classical ml model associated kernel projected quantum kernel method particular design dataset spq 1 sc gðkcjjkpqþ recall eq 3 dataset hence show largest separation prediction error bound ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ p engineered dataset constructed via simple eigenvalue problem exact procedure described supplementary section 7 result shown fig quantum nature encoding increase corresponding increasing g performance best classical method original quantum kernel decline precipitously advantage projected quantum kernel closely follows geometric difference g reach 20 large size despite optimization g only possible classical method associated kernel performance advantage remains stable across common classical method note also constructed engineered datasets saturating geometric inequality classical ml original quantum kernel small geometric difference g presented no empirical advantage large system size see supplementary section 13 keeping argument role data increase number training data n method improve advantage gradually diminish dataset engineered show strongest empirical separation largest system size date conjecture procedure could used quantum computer create challenging datasets easy learn quantum device hard learn classically still easy verify classically given correct label moreover size margin implies separation may even persist moderate amount noise quantum device discussion use quantum computing machine learning remains exciting prospect quantifying quantum advantage application ha some subtle issue one must approach carefully constructed foundation understanding opportunity quantum advantage learning setting showed quantitatively classical ml algorithm data become computationally powerful prediction advantage quantum model not guaranteed even data come quantum process challenging dently simulate motivated test introduced projected quantum kernel engineered datasets projected quantum kernel outperform tested classical model prediction error author knowledge ﬁrst empirical stration large separation quantum classical ml model work suggests simple guidebook generating ml problem give large separation quantum classical model even modest number qubits size separation trend 30 qubits suggests existence learning task may easy verify hard model classically requiring modest number qubits allowing device noise claim true advantage quantum machine learning setting require not only benchmarking classical machine learning model also classical approximation quantum model additional work needed identify embeddings satisfy sometimes conﬂicting requirement hard approximate classically exhibiting meaningful signal local observables large number qubits research required ﬁnd use case datasets closer practical interest evaluate potential claim advantage believe tool developed work help pave way exciting frontier data availability data support plot within paper ﬁndings study available upon reasonable request source data provided paper code availability tutorial reproducing smaller numerical experiment available received 18 november 2020 accepted 16 march 2021 reference halevy norvig pereira unreasonable effectiveness data ieee intell syst 24 8 2009 grover fast quantum mechanical algorithm database search proc annual acm symposium theory computing 1996 durr hoyer quantum algorithm ﬁnding minimum http 1996 farhi et al quantum adiabatic evolution algorithm applied random instance problem science 292 472 2001 neven denchev rose macready training large scale classiﬁer quantum adiabatic algorithm 2009 rebentrost mohseni lloyd quantum support vector machine big data classiﬁcation phys rev lett 113 130503 2014 leifer poulin quantum graphical model belief propagation ann phys 323 1899 2008 aaronson ambainis need structure quantum speedup 2009 mcclean et al low depth mechanism quantum optimization http 2020 boixo et al characterizing quantum supremacy device nat phys 14 595 2018 arute et al quantum supremacy using programmable superconducting processor nature 574 505 2019 peruzzo et al variational eigenvalue solver photonic quantum processor nat commun 5 4213 2014 mcclean romero babbush theory variational hybrid algorithm phys 18 023023 2016 farhi neven classiﬁcation quantum neural network near term processor arxiv preprint 2018 havlíček et al supervised learning feature space nature 567 209 2019 cortes vapnik network mach learn 20 273 1995 schölkopf et al learning kernel support vector machine regularization optimization beyond 2002 mohri rostamizadeh talwalkar foundation machine learning 2018 jacot gabriel hongler neural tangent kernel convergence generalization neural network nip 18 proceeding international conference neural information processing system pp 2018 novak et al neural tangent fast easy inﬁnite neural network python arxiv preprint 2019 arora et al exact computation inﬁnitely wide neural net advance neural information processing system 2019 blank park rhee petruccione quantum classiﬁer tailored quantum kernel npj quantum inf 6 1 2020 bartkiewicz experimental quantum machine learning ﬁnite feature space sci 10 1 2020 article nature communication 8 nature communication 2021 liu arunachalam temme rigorous robust quantum supervised machine learning arxiv preprint http 2020 huang kueng preskill predicting many property quantum system measurement nat phys 2020 cotler j wilczek quantum overlapping tomography phys rev lett 124 100401 2020 paini kalev approximate description quantum state arxiv preprint 2019 lloyd schuld ijaz izaac j killoran quantum embeddings machine learning arxiv preprint 2020 schuld sweke meyer effect data encoding expressive power variational quantum machine learning model phys rev 103 032430 2021 mcclean boixo smelyanskiy babbush neven barren plateau quantum neural network training landscape nat commun 9 1 2018 grant wossnig ostaszewski benedetti initialization strategy addressing barren plateau parametrized quantum circuit quantum 3 214 2019 schuld bocharov svore wiebe quantum classiﬁers phys rev 101 032308 larose coyle robust data encoding quantum classiﬁers phys rev 102 032420 2020 harrow montanaro quantum computational supremacy nature 549 203 2017 li et al enhanced convolutional neural tangent kernel arxiv preprint 2019 micchelli xu zhang universal kernel mach learn 7 2651 2006 krogh hertz simple weight decay improve generalization adv neural inf process syst 1992 suykens vandewalle least square support vector machine classiﬁers neural process lett 9 293 1999 huang kueng preskill bound quantum advantage machine learning arxiv preprint 2021 anderson et al lapack user guide edn society industrial applied mathematics 1999 breiman random forest mach learn 45 5 2001 gosset smolin compressed classical description quantum state arxiv preprint 2018 aaronson shadow tomography quantum state siam comput http 2020 aaronson rothblum gentle measurement quantum state differential privacy proc annual acm sigact symposium theory computing 2019 hohenberg kohn inhomogeneous electron gas phys rev 136 1964 runge gross theory system phys rev lett 52 997 1984 broughton et al tensorﬂow quantum software framework quantum machine learning arxiv preprint 2020 xiao rasul vollgraf novel image dataset benchmarking machine learning algorithm arxiv preprint 2017 lecun cortes burges mnist handwritten digit database http 2010 jolliffe principal component analysis springer 1986 schuld killoran quantum machine learning feature hilbert space phys rev lett 122 040504 2019 skolik mcclean mohseni van der smagt leib layerwise learning quantum neural network quantum machine intelligence 3 5 2021 acknowledgement want thank richard kueng john platt john preskill thomas vidick nathan wiebe wu valuable input inspiring discussion thank bálint pató crucial contribution setting simulation author contribution developed theoretical aspect work conducted numerical experiment wrote open source code contributed technical discussion writing manuscript competing interest author declare no competing interest additional information supplementary information online version contains supplementary material available correspondence request material addressed peer review information nature communication thanks nana liu anonymous reviewer contribution peer review work reprint permission information available publisher note springer nature remains neutral regard jurisdictional claim published map institutional afﬁliations open access article licensed creative common attribution international license permit use sharing adaptation distribution reproduction any medium format long give appropriate credit original author source provide link creative common license indicate change made image third party material article included article creative common license unless indicated otherwise credit line material material not included article creative common license intended use not permitted statutory regulation exceeds permitted use need obtain permission directly copyright holder view copy license visit author 2021 nature communication article nature communication 2021 9