machine learning vedran jacob han für theoretische physik universität innsbruck technikerstraße innsbruck austria quantum institute national institute standard technology gaithersburg maryland 20899 usa center quantum information computer science university maryland college park maryland 20742 usa received 15 april 2016 published 20 september 2016 emerging field quantum machine learning ha potential substantially aid problem scope artificial intelligence only enhanced recent success field classical machine learning work propose approach systematic treatment machine learning perspective quantum information approach general cover three main branch machine learning supervised unsupervised reinforcement learning quantum improvement supervised unsupervised learning reported reinforcement learning ha received much le attention within approach tackle problem quantum enhancement reinforcement learning well propose systematic scheme providing improvement example show quadratic improvement learning efficiency exponential improvement performance limited time period obtained broad class learning problem doi field artificial intelligence ai ha lately remarkable success especially area machine learning recent milestone recently believed decade computer beating expert human player game go 3 illustrates potential learning machine parallel witnessing emergence new field quantum machine learning qml ha profound potential tionize field ai much like quantum information processing ha influenced classical counterpart 4 evidence already substantiated improvement reported classification clustering problem task representative two three main branch machine learning first supervised learning considers problem learning conditional distribution pðyjxþ function fðxþ assigns label data x classifies data based correctly labeled example called training set provided distribution pðx yþ second pervised learning us sample identify structure distribution pðxþ identifies cluster quantum analog first task corresponds problem conditional state ρx state partition system given measurement outcome another partition reconstructed measurement statistic joint state ρxy encodes bution pðx yþ unsupervised case similar third branch reinforcement learning rl tutes interactive mode learning general learning agent learning algorithm learns behave correctly use reinforcement punishment rl ha le investigated quantum information perspective although some result reported key question quantum processing help learning requires u clarify constitutes good learning model involved two istics typically considered first tional complexity algorithm learner second sample complexity standard supervised learning quantifies large training set ha algorithm learn distribution pðyjxþ tomography context count number copy ρxy required learning algorithm reconstruct state ρx desired confidence rl sample complexity usually substituted learning number interaction step needed agent learn obtain reward high probability recent result qml focused improving computational complexity only recent work considering sample complexity aspect 11 supervised computational learning however broader question extent ai ultimately benefit quantum mechanic general learning setting remains largely open inthisworkweaddressthisquestion withemphasisonthe general le explored rl setting propose paradigm considering qml allows u better understand limit power using present schema identifying setting quantum effect help illustrate schema work provide methodforachievingquantumimprovements polynomial required number interaction round exponential improvement success rate many rl setting paradigm three learning setting fit paradigm learning agent 14 standard field artificial intelligence consider learning agent equivalently learning program prl 117 130501 2016 p h c l r e v e w l e e r week ending 23 september 2016 13 6 2016 american physical society interacts unknown environment e task environment problem setting via exchange message interchangeably issued called action faig e called percept fsjg quantum extension set become hilbert space ha spanfjaiig h spanfjsiig form mal base percept action state mixture referred classical state any figure merit performance agent e function history interaction collecting exchanged percept action history action thus central concept learning correct quantum generalization history not trivial deal momentarily either e stochastic interaction e described distribution history length denoted figure merit extended distribution convex linearity recover supervised learning paradigm take e characterized distribution pðx yþ agent given training labeled data point pair ðx yþ sampled pðx first n percept agent respond correct label action response presented percept unlabeled data point reinforcement learning naturally phrased interaction percept space also contains reward denote percept space including reward status reward binary formally paradigm interactive setting thus convenient quantum information treatment qml existing result group four category 15 cc cq qc qq depending whether agent first symbol environment second symbol classical c quantum q cc scenario cover classical machine learning cq setting asks classical learning technique may aid quantum task quantum control quantum metrology 18 adaptive quantum computing 19 design quantum experiment 20 deal example nonconvex nonlinear mization problem arising quantum experiment tackled machine learning technique qc corresponds tum variant learning algorithm facing classical environment figuratively speaking study potential learning robot enhanced quantum qq setting focus work e quantum system interaction fully quantum even question mean learn becomes problematic instance agent environment may become entangled learning constitutes interaction standard quantum extension applied action percept set represented mentioned hilbert space ha h agent environment act common communication register rc capable ofrepresenting bothpercepts action thus agent environment isdescribed asa sequence ofcompletely positive map fmt agðfmt act register rc also private register ra constitutes internal memory agent environment illustrated fig 1 dashed line central object characterizing interaction namely history quantum case generated ing periodic measurement rc classical often called computational basis generalization process quantum case tested interaction define tester sequence controlled map form ut ðjxirc jxirc jψirt x fux gx unitary map acting tester register rt step history relative given tester defined state register rt tested interaction shown fig restriction tester controlled map relative classical basis guarantee any choice local map ux interaction classical e remains unchanged classical tester copy content rc relative classical basis ha essentially effect measuring rc copying outcome word interface e classical shown latter case any quantum agent environment exist classical e generate history any tester 22 word classical agent qc setting alently classically tested qq setting achieve performance quantum agent term any dependent figure merit thus only improvement term computational complexity scope limit quantum ultimate potential quantum improvement learning qc classically tested setting bound computational complexity improvement achieved certain case improvement learning ciency require special type access environment not fully tested exactly done ref purposeofimproving computational complexity great success improvement exponential classical source sample substituted quantum ram 32 architecture allows accessing many sample superposition substitution come fig tested interaction general map tester ut k act fresh subsystem register rt not control agent environment crossed wire represent multiple system prl 117 130501 2016 p h c l r e v e w l e e r week ending 23 september 2016 naturally un supervised setting basic interaction comprises only two step agent request sample environment provides however general setting environment ill suited quantum parallel approach general environment store action agent memory never return effectively break entanglement agent register ra prohibits interference effect nonetheless many environmental setting still possible dissect map environment provide oracular variant use help agent learn approach quantum improvement reinforcement brings u schema improving rl agent first given classical environment e define fair unitary oracular equivalent eq fair meant sense quantum oracle boolean function fair analogue classical boolean not provide information e classical access guaranteed eq realizable reversible version second access any quantum environment eq not generically speed aspect interaction quantum walk find target vertex faster price traversed path undefined identify particular environmental property efficiently ascertained using eq relevant learning third construct improved agent us property previous point illustrate approach restricted scenario ease presentation show example generalize later application any task ment separately consider map specifies next percept environment general stochastic function mapping elapsed history onto next reward function latter described map also depends history andcomplementstheperceptbysettingitsreward status environment simple strictly epochal meaning environment reset step one reward given although interaction turn based represented sequence map ami smi bar sm highlight includes reward status moreover deterministic environment themaps fe λ only depend action agent percept response fixed deterministic simple strictly epochal environment construction appropriate oracle dramatically simplified action returned agent block step next block independent moreover using phase kickback reward map modified 22 influence global phase returned action state lead oracle realizing eq ami one use oracle requires interaction step constitutes first step proposed schema next focus step 2 obtaining useful property environment identifying setting provably help constructed oracle point towards use search find rewarding action sequence alone suffices improvement only special environment learning reduces searching better combining fast searching classical learning model canonical rl setting agent learns learn not correct sequence move per se rather correct association action given percept illustrate imagine navigating maze percept encode correct direction movement correct association learned agent perform well fortheagent learn correct association first must encounter instance rewarding sequence quantum access help thus aim assisting exploration phase balancing act exploration trying behavior find optimum exploitation reaping reward using learned information characteristic rl 33 idea made fully precise considering class environment successful exploration phase guaranteed lead better overall learning mance whether case however also depends learning model agent thus identify environment pair better performance past exploration implies better performance future average call setting formally consider environment e agent ht history rateðhtþ tþ ht history better performance implies rate eðhtþ aðhtþ tþ tþ some future period eðhþ aðhþ denote environment agent respectively undergone history h note aðhþ technically different agent say aðhtþ luckier tþ pair ða eþ satisfying formal condition thus luck favoring may additionally specify period implication 3 hold brings u step 3 schema given theorem theorem 1 let e deterministic strictly epochal environment exists oracular variant eq e any classical learning model luck favoring relative e figure merit rate monotonically increasing number reward history construct quantum agent aq aq interacting eq outperforms term figure merit rate relative chosen tester prl 117 130501 2016 p h c l r e v e w l e e r week ending 23 september 2016 theorem state restricted setting deterministic epochal environment possible cally improve learning efficiency learning agent providedthe environmentsareluck favoringfor thoseagents note reasonable learning model luck favoring relative typically considered task ments see ref 22 longer discussion statement theorem 1 omitted additional specification pertaining understood property hold improved performance hold relative period prove theorem 1 construct aq given construction illustrated step step fig 2 illustrative purpose classical interaction agent contrasted quantum interaction agent aq aq use quantumoraclevariantofe eq oracle time ﬃﬃﬃﬃﬃﬃﬃﬃﬃ jajm p þ epoch length jaj numberofactions tofindarewardingactionsequencear using grover search period interaction untested interaction fully classically tested step 2 aq play one epoch outputting action ar sequentially classical ronment obtain response environment recall eq oracle not provide obtaining entire rewarding history hr thus far aq used oðm ﬃﬃﬃﬃﬃﬃﬃﬃﬃ jajm p þ interaction step step 3 aq train internal simulation simulating interactionbetweena ande andrestartingthe simulation thehistoryhr occurs weassumesuchanoccurrencehas nonzero probability may require many internally simulated interaction no interaction real environment step 4 internal simulation aðhrþ corresponds luckiest agent possible aq quishes control finally consider happens time period unless additional information environment given oðtþ step ha only exponentially small fo probability seen rewarding sequence thus quantum agent luckier classical setting implies aq continue outperform step statement theorem 1 not quantitative due generality definition setting however trade generality exactness agent employ variant 33 output rewarding sequence exploit probability ϵ explores probability 1 ratio performance aq exponential constant reward probability ϵ aq versus exponentially diminishing step exponential gap hold time scale however improvement term learning efficiency number interaction step quadratic result achieve solid improvement using simple technique cost restricting task environment however example generalized two direction first long reset occurs step multiple multivalued reward also handled defining oracle reversibly count reward highly rewarding sequence found quantum optimization technique 34 worked ref 22 second stronger assumption eq using involved quantum subroutine deal stochastic environment instance setting one reward per epoch oracle uejaiðcos þ sin θa probability reward given action sequence constructed reversible mentation environment randomness sented subsystem entangled state 22 using phase kickback phase tion agent realize mapping θai θa precision estimate reward probability specified angle θa next amplitude amplification used amplitude amplify sequence reward probability prðaþ given sequence threshold pmin given nmin sequence ntot ce total overall number interaction step multiplies amplitude amplification cost phase estimation cost overall min interaction step classical agent interaction cost process minimal relevant success probability constant family task environment constitutes quadratic improvement finding good action sequence approach also generalized wider class setting 22 fig difference interaction aq step 1 2 aq us access eq oracle oðtþ step obtains rewarding sequence hr step 3 aq simulates agent train simulation produce rewarding sequence step 4 aq us aðhrþ remainder classically tested interaction classical environment prl 117 130501 2016 p h c l r e v e w l e e r week ending 23 september 2016 many setting robotics classical ments not allow nonetheless presented construction used ing 14 agent construct internal tation environment facilitate better learning simulation quantum chip help speeding internal processing done qc setting tantalizing exception may nanoscale robot intelligent version situ probe ref 19 future quantum experiment scale environment manifestly quantum exquisite control becomes possibility work extended eral framework artificial intelligence 14 quantum domain based established schema quantum improvement ing beyond computational complexity using schema given explicit construction reinforcement learning agent outperform classical counterpart quadratically term learning efficiency even exponentially performance limited period constitutes important step towards systematic investigation full potential quantum machine learning first step context reinforcement learning quantum interaction acknowledge support austrian science fund fwf grant no sfb foqus f 4012 templeton world charity foundation grant no thanks christopher portmann petros wallden peter wittek useful discussion helped part work 1 chouard venema machine intelligence nature 521 435 2015 2 stajic stone chin wilbe rise machine science 349 248 2015 3 silver et mastering game go deep neural network tree search nature london 529 484 2016 4 nielsen chuang quantum computation quantum information cambridge university press cambridge england 2000 5 wittek quantum machine learning quantum computing mean data mining academic press new york 2014 6 lloyd mohseni rebentrost quantum rithms supervised unsupervised machine learning 7 aïmeur brassard gambs quantum unsupervised learning mach learn 90 261 2013 8 rebentrost mohseni lloyd quantum support vector machine big data classification phys rev lett 113 130503 2014 9 dong chen li tarn quantum reinforcement learning ieee trans syst man cybern b cybern 38 1207 2008 10 paparo dunjko makmal delgado briegel quantum speedup active learning agent phys rev x 4 031002 2014 11 arunachalam de wolf optimal quantum sample complexity learning algorithm 12 servedio gortler quantum versus classical learnability annual ieee conference tional complexity chicago illinois 2001 ieee computer society los alamitos 2001 138 13 atıcı advance quantum computational learning theory thesis columbia university 2006 14 russel norvig artificial modern approach ed prentice hall new jersey 2003 15 aïmeur brassard gambs machine learning quantum world advance artificial intelligence conference canadian society computational study intelligence canadian ai 2006 springer berlin heidelberg 2006 pp 16 zahedinejad schirmer sander tionary algorithm hard quantum control phys rev 90 032310 2014 17 wiseman milburn quantum measurement control cambridge university press cambridge england 2010 18 lovett crosnier sander differential evolution adaptive quantum metrology phys rev lett 110 220501 2013 19 tiersch ganahl briegel adaptive quantum computation changing environment using projective simulation sci 5 12874 2015 20 krenn malik fickler lapkiewicz zeilinger automated search new quantum experiment phys rev lett 116 090405 2016 21 schuld sinayskiy petruccione quest quantum neural network quant inf process 13 2567 2014 22 see supplemental material cludes ref full detailed proof ments made main text also extension result presented 23 dunjko taylor briegel framework learning agent quantum environment 24 grover fast quantum mechanical algorithm database search proceeding annual acm posium theory computing philadelphia 1996 acm press philadelphia 1996 212 25 baritompa bulger wood grover quantum algorithm applied global optimization siam optim 15 1170 2005 26 sutton integrated architecture learning planning reacting based approximating dynamic ming proceeding seventh international workshop machine learning austin tx 1990 morgan kaufmann san francisco 1990 pp prl 117 130501 2016 p h c l r e v e w l e e r week ending 23 september 2016 27 briegel de la cuevas projective simulation artificial intelligence sci 2 400 2012 28 makmal melnikov dunjko briegel within projective simulation ieee access 4 2110 2016 29 boyer brassard høyer tapp tight bound quantum searching fortschr phys 46 493 1998 30 yoder low chuang quantum search optimal number query phys rev lett 113 210501 2014 31 brassard hoyer mosca tapp quantum amplitude amplification estimation arxiv 0005055 32 giovannetti lloyd maccone quantum random access memory phys rev lett 100 160501 2008 33 sutton barto reinforcement learning introduction ed mit press cambridge massachusetts 1998 34 durr hoyer quantum algorithm finding minimum arxiv prl 117 130501 2016 p h c l r e v e w l e e r week ending 23 september 2016