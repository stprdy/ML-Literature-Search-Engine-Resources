quantum machine learning feature hilbert space maria nathan killoran xanadu 372 richmond st w toronto canada dated march 21 2018 basic idea quantum computing surprisingly similar kernel method machine learning namely eﬃciently perform computation intractably large hilbert space paper explore some theoretical foundation link show open new avenue design quantum machine learning algorithm interpret process encoding input quantum state nonlinear feature map map data quantum hilbert space quantum computer analyse input data feature space based link discus two approach building quantum model classiﬁcation ﬁrst approach quantum device estimate inner product quantum state compute classically intractable kernel kernel fed any classical kernel method support vector machine second approach use variational quantum circuit linear model classiﬁes data explicitly hilbert space illustrate idea feature map based squeezing system visualise working principle datasets introduction goal many quantum algorithm perform eﬃcient computation hilbert space grows rapidly size quantum system eﬃcient mean number operation applied system grows polynomially system size illustration famous quantum fourier transform applied system us poly n operation perform discrete fourier transform amplitude system pushed extreme single operation ple squeezing applied mode formally manipulates quantum state hilbert space sense quantum computing understood technique perform implicit computation intractably large hilbert space eﬃcient manipulation quantum system machine learning kernel method ﬁeld surprisingly similar logic nutshell idea kernel method formally embed data sometimes dimensional feature space becomes easier analyse see figure 1 popular example support vector machine draw decision boundary two class datapoints mapping data feature space becomes linearly separable trick algorithm never explicitly performs computation vector feature space us kernel function deﬁned domain original input data like quantum computing kernel method therefore perform implicit computation possibly intractably large hilbert space eﬃcient manipulation data input besides apparent link kernel method hardly studied quantum machine learning literature ﬁeld deﬁnition employ investigates use quantum computing resource machine learning across approach young ﬁeld vary sampling quantum optimisation 6 7 linear algebra solver using quantum circuit trainable model inference 11 12 lot attention ha paid recent trend machine learning deep learning neural network kernel method successful only mentioned reference 9 13 besides single study connection coherent state gaussian kernel 14 potential quantum computing remains widely unexplored aim paper investigate relationship feature map kernel method quantum computing interpret process encoding input quantum state feature map map data potentially vastly feature space hilbert space quantum system data analysed feature hilbert space simple classiﬁers linear model may gain enormous power furthermore well known inner product two data input mapped feature space give rise kernel function measure distance data point kernel method use kernel function create model successful pattern recognition switching kernel one tively switch diﬀerent model known kernel trick quantum case kernel trick corresponds changing data encoding strategy two perspective namely kernel one hand feature space one hand naturally lead two way building quantum classiﬁers supervised learning implicit approach take 19 mar 2018 2 original space feature space fig original space training input data two class blue square red circle not separable simple linear model left map higher dimensional feature space linear model indeed suﬃcient deﬁne separating hyperplane act decision boundary right classical model depends kernel function us quantum device evaluate kernel computed inner product quantum state feature hilbert space explicit approach us quantum device directly learn linear decision boundary feature space optimising variational quantum circuit central result paper idea bedding data quantum hilbert space open promising avenue quantum machine learning generically use quantum device tern recognition implicit explicit approach not only also suitable quantum technology allows u test generation quantum er currently developed nonlinear feature map also circumvent need implement nonlinear transformation data thereby solve outstanding problem quantum machine ing come back conclusion ii feature map kernel quantum computing machine learning typically given dataset input xm certain input set x recognise pattern evaluate produce previously unseen data kernel method use distance measure κ x any two input x der construct model capture property data distribution distance measure connected inner product certain space feature space side many practical application famous support vector machine method rich theoretical foundation 15 want light some relevant point feature map kernel let u start deﬁnition feature map deﬁnition let f hilbert space called feature space x input set x sample input set feature map map φ x input vector hilbert space vector φ x called feature vector feature map play important role machine learning since map any type input data space metric space usually much higher dimension feature map nonlinear function change relative position data point example figure 1 dataset become lot easier classify feature space feature map intimitely connected kernel 16 deﬁnition let x nonempty set called input set function κ x x called kernel gram matrix k entry km κ xm positive semideﬁnite word any ﬁnite subset xm cm x xm deﬁnition inner product every feature map give rise kernel theorem let φ x feature map inner product two input mapped feature space deﬁnes kernel via κ x x φ 1 inner product deﬁned proof must show gram matrix kernel positive deﬁnite arbitrary cm any xm ﬁnd x xm x cmφ xm x x cmφ xm connection feature map kernel mean every feature map corresponds distance measure input space mean inner product feature vector also mean compute inner product vector mapped much higher sional space computing kernel function may computationally lot easier 3 feature map kernel rkhs thm 1 def 3 thm 2 fig relationship concept feature map kernel reproducing kernel hilbert space reproducing kernel hilbert space kernel theory go deﬁnes unique hilbert space associated kernel reproducing kernel hilbert space rkhs 17 18 although rather stract concept useful order understand signiﬁcance kernel machine learning well connection linear model feature space deﬁnition let x input set r hilbert space function f x map input real number let inner product deﬁned r give rise norm via p r reproducing kernel hilbert space every point evaluation continuous functional f f x x equivalent condition exists function κ x x κ x f x 2 κ x f x function κ unique reproducing kernel r eq 2 reproducing property note diﬀerent isometrically isomorphic hilbert space derived mercer kernel 19 since feature map give rise kernel kernel give rise reproducing kernel hilbert space construct unique reproducing kernel hilbert space any given feature map see figure 2 theorem let φ x feature map input set x giving rise complex kernel κ x x φ corresponding reproducing kernel hilbert space ha form rκ f x f x φ x w 3 function rkhs associated feature map φ interpreted linear model w deﬁnes hyperplane feature space machine learning rather formal concept gain relevance no le formal representer orem 20 theorem let x input set κ x x kernel data set consisting data pair xm ym x r f x class model function live reproducing kernel hilbert space rκ thermore assume cost function c tiﬁes quality model comparing predicted put f xm target ym ha tion term form g g 0 strictly monotonically increasing function any function f minimises cost function c written f x x αmκ x xm 4 some parameter αm representer theorem implies common family machine learning optimisation problem function rkhs r solution represented expansion kernel function eq 4 quently instead explicitly optimising dimensional rkhs directly start implicit ansatz eq 4 solve convex optimisation lem ﬁnding parameter αm combination theorem 2 theorem 3 show another facet link kernel feature map model deﬁnes hyperplane feature space often written model depends kernel evaluation section iii translate two viewpoint two way designing quantum machine learning algorithm input encoding feature map immediate approach combine quantum chanics theory kernel associate hilbert space quantum system reproducing kernel hilbert space ﬁnd reproducing kernel system show appendix hilbert space discrete base well special case hilbert space coherent state reproducing kernel given inner product basis vector insight lead interesting result example chatterjee et al 14 show inner product optical coherent state turned gaussian kernel also called radial basis function kernel widely used machine learning however widen framework choose another route instead asking kernel associated quantum hilbert space associate quantum hilbert space feature space derive kernel given inner product quantum state seen previous section automatically give rise rkhs entire apparatus kernel theory applied assume want encode some input x input set x quantum state described vector x life hilbert space procedure input encoding fulﬁlls deﬁnition feature map 4 φ x call quantum feature map according theorem 1 derive kernel κ feature map via eq 1 virtue theorem 2 kernel reproducing kernel rkhs rκ deﬁned eq 3 function rκ inner product input data vector deﬁnes linear model f x w x 5 note use dirac bracket inner product signify calculating inner ucts quantum hilbert space finally representer theorem 3 guarantee minimiser minw c w empirical risk c w x xm w expressed equation 4 simple idea terpreting x x feature map therefore allows u make use rich theory kernel method give rise machine learning model whose trained didates expressed inner product quantum state note state x complex tudes always construct real kernel taking absolute square inner product iii quantum machine learning feature hilbert space let u enter realm quantum computing quantum machine learning show use idea section ii c design two type quantum machine learning algorithm illustrate approach example system circuit perspective quantum computing quantum feature map x x state preparation circuit uφ x act ground vacuum state hilbert space f uφ x x call uφ x embedding circuit model eq 5 reproducing hilbert space deﬁnition 2 inner product x general quantum state therefore consider second circuit w call model circuit model circuit speciﬁes hyperplane linear model feature hilbert space feature state x orthogonal x lie sion boundary whereas state positive negative inner product lie left right side hyperplane show some example circuit associated kernel let u look popular input encoding technique quantum machine learning basis encoding many quantum machine learning algorithm assume input x computation encoded binary string represented tational basis state qubits 12 21 ple x 01001 represented basis state computational basis state corresponds standard basis vector integer resentation bitstring hilbert space f eﬀect circuit given uφ x 0 1 n feature map map data input state orthonormal basis equivalent generic dimensional case discussed appendix shown generic kernel kronecker delta κ x δij binary similarity measure only nonzero two identical input amplitude encoding another approach mation encoding associate normalised input vector x dimension n amplitude n qubit state 8 13 uφ x x th computational basis state choice corresponds linear kernel κ x xt copy quantum state slight variation amplitude encoding implement polynomial nels 9 taking copy amplitude encoded tum state uφ x corresponds kernel κ x xt product encoding one also use tensor product encoding feature input x xn encoded amplitude one separate qubit example encode xi xi co xi sin xi 1 n 22 23 corresponds circuit eﬀect uφ x co sin co xn sin xn implies cosine kernel κ x n co xi 5 implicit approach quantum device kernel explicit approach quantum device model prediction prediction new input training input fig illustration two approach use quantum ture map supervised learning implicit approach us quantum device evaluate kernel function part hybrid model trained classical method explicit approach model solely computed quantum device consists tional circuit trained hybrid method building quantum classiﬁer formulated idea section ii c language quantum computing identify two diﬀerent strategy designing quantum machine learning algorithm see figure 3 one hand use quantum computer estimate inner product κ x x model eq 4 call implicit approach since use quantum system estimate distance measure input space strategy requires tum computer two thing implement uφ x any x estimate inner product quantum state example using swap test routine computation model kernel estimate well training algorithm left classical device excellent strategy context quantum technology 24 interested using quantum computer only small routine limited gate count compute much possible classical hardware note long term quantum computer could also used learn parameter αm computing inverse kernel gram matrix ha investigated ref 9 25 hand motivated troduction one bypass representer theorem explicitly perform classiﬁcation feature hilbert space quantum system call explicit approach example mean ﬁnd deﬁnes model make model circuit trainable w w θ hybrid training 23 26 θ learn optimal model θ w θ ansatz c c c fig shape squeezing kernel function κsq x equation 7 diﬀerent squeezing strength hyperparameters input x ﬁxed 0 0 varied plot show interval 1 horizontal ax model circuit architecture deﬁnes space possible model act regularisation see also 22 follow slightly general strategy compute state w θ measurement determine output model depending measurement not necessarily linear model feature hilbert space could even go include postselection model circuit might give classiﬁer feature hilbert space even power using quantum computer learning task two approach desirable various setting example implicit approach may interesting case quantum device evaluates kernel model faster term absolute runtime speed another interesting example setting kernel one want use classically intractable runtime grows exponentially even faster input dimension explicit approach may useful want leave limit rkhs framework construct classiﬁers directly hilbert space remainder work want explore two approach several example use ing quantum system ture map hilbert space f dimensional fock space construct based quantum machine learning classiﬁer example implemented optical quantum computer squeezing feature map squeezed vacuum state deﬁned 1 p cosh r x p tanh r denotes fock basis z reiϕ complex squeezing factor absolute value r phase useful introduce notation r ϕ interpret x x c x ture map real input space x 6 train 0 test 0 c train 1 test 1 c c c c c fig decision boundary support vector machine custom kernel eq 7 shaded area show decision region class 0 blue class 1 red plot show rate correct classiﬁcations training set ﬁrst row plot three standard datasets circle moon blob 150 test 50 training sample second row illustrates ing squeezing hyperparameter c change classiﬁcation performance use dataset 500 training 100 test sample training wa performed python learn svc classiﬁer using custom kernel implement overlap eq 8 hilbert space fock state short fock space c constant hyperparameter mine strength squeezing x associated phase moreover given input dataset vector x xn deﬁne joint state n squeezed vacuum mode φ x c x 6 c x c c xn feature map f multimode fock space call feature map squeezing feature map phase encoding kernel κ x c n c xi c 7 c xi c r sech c sech c 1 tanh c tanh c 8 derived feature map 27 easy compute classical computer plotted figure 4 see hyperparameter c determines variance kernel function note also encode train 0 test 0 epoch 1 epoch 500 epoch 5000 train 1 test 1 fig decision boundary perceptron classiﬁer fock space mapping data point via squeezing feature map phase encoding eq 6 c perceptron only act real space without regularisation blob dataset ha only 70 training 20 test sample perceptron achieves training accuracy 1 le 5000 epoch mean data linearly separable fock space interestingly example test performance remains exactly simulation performed strawberry field simulator well perceptron classiﬁer feature absolute value squeezing ﬁne squeezing feature map absolute value encoding x x x c however version not vary variance kernel function use phase encoding following investiagtions implicit classiﬁer implicit approach evaluate kernel eq 7 quantum computer feed classical kernel method instead using real quantum device exploit fact case squeezing kernel eﬃciently computed classically use custom kernel support vector machine figure 5 show model easily learns decision boundary datasets since idea support vector machine ﬁnd hyperplane feature space want know whether always ﬁnd hyperplane training accuracy word ask data becomes linearly separable fock space squeezing feature map easy way apply perceptron classiﬁer data feature space perceptron guaranteed ﬁnd separating hyperplane exists figure 6 show performance perceptron classiﬁer fock space blob data data wa mapped space squeezing feature map phase encoding one see 5000 epoch run dataset decision boundary perfectly ﬁts training data achieving accuracy number iteration train perceptron known increase γ margin two class 28 indeed ﬁnd simulation moon 7 circle data only take epoch reaching full accuracy although perfect ﬁt training data course not useful machine learning seen accuracy test set result clue fact squeezing feature map make data linearly separable feature space fact prove appendix result simulation promising goal ﬁnd sophisticated kernel although quantum computer could oﬀer constant speed tages become indispensable feature map cuit classically intractable however squeezed state example gaussian state well known gaussian state although living hilbert space eﬃciently ulated classical computer 29 used simulation order something interesting one need element circuit ample one extend standard linear optical network beamsplitters cubic phase gate 30 31 use ton number measurement 32 end let vφ x feature map circuit quantum algorithm take vacuum state prepares dependent state kernel κ x φ x vφ general not simulated classical computer any therefore interesting open question type feature map circuit vφ classically tractable time lead powerful kernel classical model support vector machine explicit quantum classiﬁer explicit approach deﬁned use parametrised circuit w θ build classiﬁer squeezing example done follows start two vacuum mode classify data input x ﬁrst map input quantum state performing squeezing operation mode second ply model circuit w θ third interpret probability p measuring certain fock state output machine learning model since probability depends displacement squeezing intensity better deﬁne two tie say p 2 0 p 0 2 encoded output vector output vector normalised 33 new vector 1 p 0 p 1 p 0 p 1 interpreted probability model predict class 0 1 respectively ﬁnal label class feature map circuit model circuit w θ p 0 p 1 x f b c w θ p c p b p v p v fig 7 representation graphical language quantum neural network vector input space x get mapped feature space f fock space quantum system model circuit including photon detection measurement implement linear model feature space reduces inﬁnite hidden layer two output b model circuit explicit classiﬁer described text us only 2 mode instantiate hidden layer variational circuit w θ consists titions gate block use gate block shown beamsplitter b displacement quadratic p cubic phase gate c described text higher probability interpret circuit graphical representation neural network shown top figure let u assume could represent any possible tum circuit feature hilbert space circuit w θ since data f linearly separable w obtain 100 accuracy training set saw figure however goal machine learning not perfectly ﬁt data generalise therefore not desirable ﬁnd optimal sion boundary training data f ﬁnd good candidate class decision boundary capture structure data well restricted class decision boundary deﬁned using ansatz model circuit w θ not represent any circuit yet still ﬂexible enough reach interesting candidate figure 7 show model circuit 2 input mode example architecture consists repetition general gate 8 train 0 test 0 1 detail iteration loss train 1 test 1 fig fock space classiﬁer presented figure 7 text moon dataset shaded area show ability p 1 predicting class datasets consist 150 training 50 test sample ha trained 5000 step stochastic gradient descent 5 adaptive learning rate cost function gentle regularisation applied weight loss drop predominantly ﬁrst 200 step left block denote ˆ ˆ creation lation operator mode 1 2 ˆ ˆ corresponding quadrature operator see 34 entangling beam splitter gate b u v eu eivˆ 2 u v circuit consists gate ﬁrst second third order quadrature gate implemented displacement gate z e im z ˆ z ˆ p complex displacement factor use quadratic phase gate second order p u ei u 2 ˆ cubic phase gate third order operator v u ei u 3 ˆ principle construct any quantum circuit gate set basic circuit block easily generalised circuit mode replacing single beam splitter full optical network beam splitter 35 show fock space classiﬁer work plot decision boundary moon data figure 8 using 4 repetition gate block figure 7 32 parameter total training loss show 200 iteration stochastic gradient descent algorithm loss converges almost zero iv conclusion paper introduced number new idea area quantum machine learning based theory feature space kernel interpreting encoding input quantum state feature map associate quantum hilbert space feature space inner product quantum state feature space used evaluate kernel function alternatively train variational quantum circuit explicit classiﬁer feature space learn decision boundary introduced squeezing feature map example motivated simulation two approach lead interesting result work many avenue research example raised question whether interesting kernel function computed estimating inner product tum state state preparation classically intractable another open question detail design training variational circuit learning algorithm tailormade use hybrid training scheme topic ha begun investigated quantum machine learning community 1 12 last not least want come back point raised introduction quantum machine ing lot model use amplitude encoding mean data vector represented amplitude quantum state especially trying reproduce neural dynamic one would like perform nonlinear transformation data linear transformation natural quantum theory earities diﬃcult design context ing workarounds based postselection success circuit proposed 23 36 siderable cost making circuit probability failure grows size architecture feature map approach source nonlinearity procedure encoding input quantum state therefore oﬀers gant solution problem nonlinearities tude encoding 1 verdon broughton biamonte arxiv preprint 2017 2 amin physical review 92 1 2015 3 benedetti omez biswas 9 arxiv preprint 2016 4 guang hao low chuang physical review 89 062315 2014 5 wittek gogolin scientiﬁc report 7 2017 6 denchev ding neven vishwanathan proceeding international conference machine learning 2012 pp 7 ogorman babbush smelyanskiy pean physical journal special topic 224 163 2015 8 wiebe braun lloyd physical review letter 109 050505 2012 9 rebentrost mohseni lloyd physcial view letter 113 130503 2014 10 schuld sinayskiy petruccione physical review 94 022342 2016 11 wan dahlsten ansson gardner kim npj quantum information 3 36 2017 12 farhi neven arxiv preprint 2018 13 schuld fingerhuth petruccione physic letter 119 60002 2017 14 chatterjee yu quantum information communication 17 1292 2017 15 olkopf smola learning kernel support vector machine regularization optimization beyond mit press 2002 16 berg christensen ressel harmonic analysis semigroups 1984 17 hofmann olkopf smola annals statistic 1171 2008 18 aronszajn transaction american ical society 68 337 1950 19 j mercer phil trans soc lond 209 415 1909 20 olkopf herbrich smola tional learning theory springer 2001 pp 21 wang journal mathematics research 7 175 2015 22 stoudenmire schwab advance neural information processing system 2016 pp 23 guerreschi smelyanskiy arxiv preprint 2017 24 preskill arxiv preprint 2018 25 schuld sinayskiy petruccione physical review 94 022342 2016 26 mcclean romero babbush guzik new journal physic 18 023023 2016 27 barnett radmore method theoretical quantum optic vol 15 oxford university press 2002 28 novikoﬀ convergence proof perceptrons tech stanford research institute 1963 29 bartlett sander braunstein nemoto quantum information continuous variable springer 2002 pp 30 gottesman kitaev preskill physical view 64 012310 2001 31 lloyd braunstein quantum information continuous variable springer 1999 pp 32 bartlett sander physical review 65 042304 2002 33 contrast standard technique machine learning not advisable use softmax layer purpose since small lead almost form probability 34 weedbrook pirandola cerf ralph shapiro lloyd review modern physic 84 621 2012 35 flamini spagnolo viggianiello crespi osellame sciarrino scientiﬁc report 7 15133 2017 36 wiebe granade arxiv preprint 2015 37 berlinet reproducing kernel hilbert space probability statistic springer ence business medium 2011 38 griﬃths yuille probabilistic mind prospect bayesian cognitive science 33 2008 39 la madrid european journal physic 26 287 2005 40 klauder skagerstam coherent state plication physic mathematical physic world scientiﬁc 1985 41 hogben handbook linear algebra crc press 2006 appendix reproducing kernel quantum system section appendix try ﬁnd answer question reproducing kernel hilbert space generic quantum system give rise quantum theory prescribes state quantum system modelled vector hilbert space h typical setting hilbert space constructed complete basis eigenvectors complete set commuting hermitian operator corresponds physical observables due hermiticity observables basis orthogonal continuous observable position operator describing location particle countably inﬁnite observing number photon electric ﬁeld ﬁnite observing spin electron vector hilbert space abstractly referred dirac notation however every hilbert space ha functional tation case discrete basis dimension n functional representation hf h given hilbert space square summable sequence ψ si n inner product p si ψ si si continuous case space square summable equivalence class function ψ inner product r dsψ preceding formulation quantum theory therefore associate every quantum system hilbert space function mapping set complex number question hilbert space give rise reproducing kernel make rkhs respect input set resolution identity 1 r continuous 1 p discrete case immediately create reproducing property 10 eq 2 consider ﬁrst discrete case ψ si x sj identify reproducing kernel since basis orthonormal κ si sj δi j continuous case subtle inserting tity get ψ z ψ reproducing kernel property ducing kernel κ however function δ not square integrable mean not part hf property deﬁnition 3 not fulﬁlled no surprise space square integrable function frequent example hilbert space not rkhs 37 inconsistency dirac formalism functional analysis also issue quantum theory usually glossed physical context 38 mathematical rigour needed physicist usually refer theory rigged hilbert space 39 quantum system inﬁnite basis naturally give rise reproducing kernel not delta function system described generalised coherent state 40 context quantum machine learning ha discussed ref 14 generalised coherent state vector hilbert space hc ﬁnite countably inﬁnite sion index l some topological space l allowing u deﬁne norm p two fundamental property first strongly continuous function l lim 0 note excludes example discrete fock basis also any orthonormal set state continuous label z since 1 1 second exists measure µ l resolution identity 1 r l dµ l lead functional representation hilbert space vector expressed via p l ψ l ψ l inserting resolution identity right hand side expression yield ψ l z l exactly reproducing property deﬁnition 3 reproducing kernel κ l since ﬁnite overlap any two state basis kernel not dirac delta tion not run problem continuous orthogonal base hence hilbert space coherent state rkhs input set l type coherent state optical coherent state 2 x αn n eigenstates bosonic creation operator ˆ associated kernel κ α β e 2 2 whose square radial basis function gaussian kernel remarked 14 appendix b linear separability fock space map input dataset new dataset c c xm using squeezing feature map phase encoding eq 6 feature mapped data vector always linearly separable mean any assignment two class label data separated hyperplane f see figure 1 show ﬁrst consider following proposition set vector rn linearly separable linear independent proof found appendix proposition 1 tell u data linearly independent linearly separable result fact known statistical learning theory vc dimension measure ﬂexibility expressive power linear model k dimension k 1 mean linear model separate shatter k 1 data point choose strategy range not strategy labelled show squeezing feature map map vector linearly independent state fock space know any dataset becomes linearly separable fock space simplify let ﬁrst see look squeezing map single mode proposition given set squeezing phase ϕm ϕm 1 hyperparameter c squeezed vacuum fock state c c ϕm linearly independent proof found appendix similar proof conﬁrms proposition also hold true sueezing map absolute value encoding described section iii symbolic computation rank design matrix feature space mathematica conﬁrms result randomly selected squeezing factor 11 10 cutoﬀdimension truncates fock space 40 dimension multimode feature map dealing input data dimension higher one c ϕm c ϕm 1 c ϕm c c 1 c c ϕm c n c ϕm c 1 ϕm 1 n value zero else linear independence therefore carry feature map appendix c proof proposition 1 let xm ym dataset vector xm 1 1 vector guaranteed linearly separable any assignment class 1 label ym hyperplane deﬁned parameter wn b sgn n x wixm b ym 1 sign function bit tricky instead show stronger condition n x wixm b ym 1 hold some parameter eq must automatically satisﬁed equation deﬁnes system linear equation n 1 unknown namely variable theorey linear algebra know 41 least one solution only rank coeﬃcient matrix 1 n 1 xm 1 xm n 1 equal rank augmented matrix 1 n 1 xm 1 xm n 1 ym remember rank matrix number linearly independent row column vector data vector linearly independent n n would some vector depend others vector dimension rank x min n augmenting x stacking any number column vector simply increase n mean doe not change rank matrix follows linearly independent data point embedded n dimensional space system ha solution data therefore linearly separable argument add vector linearly dependent fact add one only one data point linearly depends others still guarantee linear bility adding one data point make row number equal column number adding column doe not change rank trast adding two data point mean column row adding column indeed change rank appendix proof proposition 2 let consider matrix squeezed state fock basis form row mjn 1 p cosh rj tanh rj p n introduce two auxiliary diagonal matrix diag q cosh rj diag n p multiplying ﬁnd matrix v ha matrix element vjn tanh rj importantly v ha structure vandermonde trix particular ha determinant det v 1 2 tanh ri eiφj tanh rj only way det v 0 ei tanh ri tanh rj some squeezing feature map phase encoding prescribes ri rj c assume 12 c 0 thus only solution equation ϕi ϕj only true two feature vector describe datapoint excluded proposition thus det v 0 mean det 0 hence full rank mean column feature vector linearly independent note proof also scribe squeezing feature map absolute value encoding make distinct data point linearly independent fock space