article understanding quantum machine learning also requires rethinking generalization elies jens eisert carlos 1 quantum machine learning model shown successful generalization performance even trained data work tematic randomization experiment show traditional approach understanding generalization fail explain behavior quantum model experiment reveal quantum neural network accurately ﬁt random state random labeling training data ability memorize random data deﬁes current notion small generalization error problematizing approach build complexity measure vc dimension rademacher complexity uniform relative complement empirical result theoretical construction showing quantum neural network ﬁt arbitrary label quantum state hinting memorization ability result not preclude possibility good generalization training data rather rule any possible antees based only property model family ﬁndings expose fundamental challenge conventional understanding eralization quantum machine learning highlight need paradigm shift study quantum model machine learning task quantum device promise application solving computational problem beyond capability classical given paramount importance machine learning wide variety algorithmic application make prediction based training data natural thought investigate extent quantum computer may assist tackling machine learning task indeed task commonly listed among promising candidate application quantum date within emergent ﬁeld quantum machine learning qml body erature available heuristically explores potential improving learning algorithm access quantum among model considered parameterized quantum circuit pqcs also known quantum neural network qnns take center stage blems quantum machine learning quantum advantage putational complexity proven classical date advantage rely availability quantum computer not within reach architecture pqcs advantage ha not shown yet growing body literature available investigates aimed understanding expect quantum model among study latter notion generalization cularly important since aimed providing guarantee performance qml model unseen data training process importance notion generalization pqcs actually reﬂecting development classical machine learning vapnik laid groundwork formal study tistical learning system methodology wa considered standard classical machine learning theory roughly last decade however mindset put forth work ha disrupted seminal demonstrating conventional understanding generalization unable explain greatsuccess deep convolutional neural network network display order magnitude trainable parameter dimension received 3 july 2023 accepted 6 february 2024 check update center complex quantum system freie universität berlin berlin germany heinrich hertz institute berlin germany zentrum berlin für materialien und energie berlin germany jense nature 2024 1 1234567890 1234567890 image process deﬁed conventional wisdom concerning generalization employing clever randomization test derived parametric author ref 62 exposed crack foundation vapnik theory least applied speciﬁc large network established plexity measure vc dimension macher among others inadequate explaining generalization behavior large classical neural network ings form numerical experiment directly challenge many uniform generalization bound learning model derived ref uniform ization bound apply uniformly hypothesis across entire function family consequently fail distinguish hypothesis good performance completely overﬁt training data moreover uniform generalization bound oblivious difference data randomly corrupted pattern inherent uniformity grant long reach randomization test exposing single instance poor generalization sufﬁcient reduce statement matical theorem mere trivially loose bound state affair ha important consequence emergent ﬁeld qml explore noteworthy current generalization bound quantum machine learning model essentially uniquely focused uniform variant consequently present sion remains akin classical machine learning canon advent ref observation raise natural question whether randomization test would yield analogous come applied quantum model classical machine learning widely acknowledged scale deep neural network play crucial role generalization analogously widely accepted current qml model considerably distant size scale context one would not anticipate similarity current qml model classical learning article provide empirical evidence unexpected behavior ﬁeld generalization quite arresting conclusion fact position challenge notion generalization building similar randomization test used ref turn already yield surprising result applied qml model employing quantum state input empirical ﬁndings also form numerical ments reveal uniform generalization bound may not right approach qml corroborate body ical work rigorous underpinning show qml model assign arbitrary label quantum state speciﬁcally show pqcs able perfectly ﬁt training set polynomial size number qubits revealing ability memorize random data result rule good generalization guarantee training data uniform clarify experiment not study generalization capacity qml instead expose limitation uniform generalization bound applied model qml model demonstrated good generalization performance some tributions not explain achieve highlight reason behind successful generalization remain elusive result statistical learning theory background begin brieﬂy introducing necessary terminology cussing ﬁndings framework supervised learning denote x input domain set possible label assume unknown ﬁxed distribution dðx yþ data originate let f represent family function map x expected risk functional r quantiﬁes dictive accuracy given function f data sampled according training set denoted comprises n sample drawn empirical risk rsðf þ evaluates performance function f restricted set difference r f rsðf þ referred generalization gap deﬁned genðf þ jrðf þ rsðf þj dependence gen f implied evident context similarly dependence r f rsðf þ gen f also implicit employ cðfþ represent any complexity measure function family vc dimension rademacher complexity important note measure property whole function family f not single function f 2 traditional framework statistical learning way aforementioned concept relate one another follows primary goal supervised learning minimize expected risk r associated learning task unattainable goal construction stem ing expected risk sum two term rðf þ rsðf þ zﬄ empiricalrisk bias rðf þ rsðf þ ﬄ zﬄﬄﬄﬄﬄﬄﬄﬄ ﬄ generalization gap variance characterization arises conventional understanding diminishing one component invariably lead increase two negative scenario exist extreme underﬁtting occurs model exhibit high bias resulting imperfect classiﬁcation training set conversely overﬁtting arises model display high variance leading perfect classiﬁcation training set overﬁtting considered detrimental may cause learning model learn spurious correlation induced noise training data modating noise data would consequently lead suboptimal performance new data poor generalization concerning model selection problem practitioner thus tasked ing model appropriate model capacity learning task aiming strike balance notion explained extensively ref previously described scenario no longer applicable demonstrated quantum learning model display good generalization performance able completely overﬁt data phenomenon sometimes linked ability learning model memorize data term memorization deﬁned occurrence overﬁtting without concurrent ization essential clarify overﬁtting context mean perfect ﬁtting training set regardless generalization formance furthermore model considered memorized training set overﬁtting poor generalization occur simultaneously overall high model capacity particularly relation memorization ability found addressing learning task practical signiﬁcance phenomenon wa initially characterized large overparameterized deep neural network ref manuscript present analogous unexpected behavior parameterized quantum circuit randomization test goal improve understanding pqcs learning model particular tread domain generalization interplay ability memorize random data main idea work build theory randomization test figure 1 contains visualization framework initially train qnns quantum state whose label randomized compare training accuracy achieved learning model trained true label result reveal article nature 2024 2 many case model learn classify training data perfectly regardless whether label randomized altering input data reach ﬁrst ﬁnding observation 1 fitting random label existing qml model accurately ﬁt random label quantum state next randomize only fraction label observe steady increase generalization error label noise rise suggests qnns capable extracting residual signal data simultaneously ﬁtting noisy portion using memorization observation 2 fitting partially corrupted label existing qml model accurately ﬁt partially corrupted label quantum state addition randomizing label also explore effect randomizing input quantum state conclude observation 3 fitting random quantum state existing qml model accurately ﬁt label random quantum state randomization experiment result remarkably large generalization gap training without changing circuit ture number parameter number training example learning algorithm highlighted ref 62 classical learning model straightforward experiment implication quantum neural network already show memorization capability quantum data trainability model remains largely unaffected absence correlation input state label randomizing label doe not change any property learning task data following present experimental design formal interpretation result even though would seem result contradictestablished theorem elucidate prove uniform generalization bound vacuous currently tested model numerical result show numerical result randomization test focusing candidate architecture tion problem quantum convolutional neural network qcnn 69 classiﬁcation quantum phase matter classifying quantum phase matter accurately relevant task study moreover due signiﬁcance frequently appears benchmark problem experiment consider generalized cluster hamiltonian h x n j 1 zj jxj 1 jxj 1 n number qubits xi zi pauli operator acting ith qubit coupling strength speciﬁcally fig 1 visualization framework empirical experiment tribution labeled quantum data undergoes randomization process leading corrupted data distribution training test set drawn dently distribution training set fed optimization algorithm employed identify best ﬁt data set individually family parameterized quantum circuit f process generates two hypothesis one original data foriginal another corrupted data fcorrupted empirically ﬁnd thatthe labeling function perfectlyﬁt training data leading small training error parallel foriginal achieves small test error indicating good learning performance quantiﬁed small generalization gap gen foriginal small contrary randomization process cause fcorrupted achieve large test error turn result large generalization gap gen fcorrupted large b regarding uniform generalization bound worth ing corner qml literature assigns upper bound gunif entire function family without considering speciﬁc characteristic individual function finally combine two signiﬁcant ﬁndings 1 identiﬁed hypothesis large empirical generalization gap 2 form generalization bound impose identical upper bound hypothesis consequently conclude any uniform generalization bound derived literature must regarded large indicating bound loose training data size notion loose generalization bound doe not exclude possibility achieving good generalization rather fails explain predict successful behavior article nature 2024 3 classify state according one four topological phase display demonstrated ref 75 depicted fig 2 phase diagram comprises phase topological ii ferromagnetic iii iv trivial learning task undertake involves identifying correct quantum phase given ground state generalized cluster hamiltonian some choice generate training set yiþg n 1 sampling coupling coefﬁcients uniformly random domain 4 n number training data point representing ground state vector h corresponding sampled yi denoting corresponding phase label among aforementioned phase particular label bit string yi 0 0 0 1 1 0 1 1 employ qcnn architecture presented ref 69 address classiﬁcation problem adapting classical convolutional neural network quantum setting qcnns particularly task involving spatial temporal pattern make architecture natural choice phase classiﬁcation problem unique feature qcnn architecture interleaving volutional pooling layer convolutional layer consist parameterized unitaries applied neighboring qubits functioning ﬁlters feature map across different layer qcnn following convolutional layer pooling layer introduced reduce dimensionality quantum state retaining relevant feature data achieved measuring subset qubits applying translationally invariant parameterized unitaries based corresponding measurement outcome thus pooling layer consistently ce number qubits constant factor leading quantum circuit logarithmic depth relative initial system size circuit share structural similarity multiscale entanglement renormalization nevertheless instance input state qcnn exhibit high degree entanglement efﬁcient classical simulation circuit becomes infeasible operation qcnn interpreted quantum channel cϑ speciﬁed parameter ϑ mapping input state ρin output state ρout represented ρout cϑ ρin subsequently expectation value hermitian operator measured utilizing resulting ρout implementation follows presented ref qcnn map input state vector consisting n qubits output state labeling function given output state use probability outcome bit string state measured computational basis particular predict label according measurement outcome lowest probability according ψ 7 arg min pb experiment repetition generate data sponding distribution training use loss function ϑ ψi classiﬁcation rule loss function involve selecting outcome lowest probability wa already utilized ref author found employing seemingly loss function lead good generalization performance thus given training set minimize empirical risk rsðϑþ 1 n x n 1 yi ψi consider three way altering original data distribution data sampled namely data wherein true label replaced random label b randomization only fraction r 0 1 data mixing real corrupted label distribution dr c replacing input quantum state dom state dst instead randomizing label randomization experiment generalization gap risk functionals deﬁned according relevant distribution 2 dr dstg case correlation state label gradually lost mean control much signal learned experiment correlation vanished entirely learning impossible one could expect impossibility learning manifest training process lack convergence observe training qcnn model random data result almost perfect classiﬁcation performance training set face value mean qcnn able memorize noise following experiment approximate expected risk r empirical risk rt using large test set test set sampled independently distribution training set particular test set contains 1000 point ments additionally report result using probability error elucidated consequently employ term error instead risk henceforth refer test accuracy test error accurate proxy true accuracy expected risk respectively experiment follow process create training set test set find function f approximately minimizes empirical risk eq 6 compute training error rsðf þ test error rtðf þ empirical generalization gap gentðf þ rtðf þ rsðf þj ease notation shall employ gen f instead gent f discussing generalization gap without reiterating empirical nature random label start randomization test drawing data wherein true label replaced random label sampled uniformly 0 0 0 1 1 0 1 1 order sample labeled pair obtained original data tribution yþ label randomly replaced experiment employed qcnns varying number qubits n 8 16 32 qubit number generated training set different size n 5 8 10 14 20 random fig 2 phase diagram generalized cluster hamiltonian phase diagram hamiltonian eq 3 comprises phase protected topological ii ferromagnetic iii iv trivial article nature 2024 4 real label model trained individually n n combination fig illustrate result obtained ﬁtting random real label well random state discussed later data point ﬁgure represents average generalization gap achieved ﬁxed training set size n different qubit number observe large gap random label close seen effectively maximal perfect training accuracy test accuracy random guessing would yield ﬁnding suggests qcnn adjusted ﬁt random label training set despite label bearing no correlation input state training set size increase since capacity qcnn ﬁxed achieving perfect classiﬁcation accuracy entire training set becomes increasingly challenging consequently generalization gap diminishes worth noting decrease training accuracy also observed true labeling corrupted label next randomization label investigate qcnn ﬁtting behavior data come varying level label corruption dr ranging no label altered r 0 corrupted r 1 experiment sider different number training point n 4 6 8 ﬁxed number qubits n combination n n start experiment no randomized label r 0 gradually increase ratio randomized label label altered r 0 1 figure show test error convergence repetition experiment reach 100 ing accuracy observe steady increase test error noise level intensiﬁes suggests qcnns capable extracting remaining signal data simultaneously ting noise brute force label corruption approach 1 test error converges 75 corresponding performance random guessing inset fig focus experiment conducted n 6 training point particular examine relationship learning speed ratio random label plot show average ﬁve experiment repetition remarkably individual run exhibit consistent pattern training error initially remains high converges quickly decrease start behavior wa also reported classical neural precise moment training error begin decrease seems heavily dependent random initialization parameter however also relates ratio r training data notably observe long stable plateau mediate case r r roughly halfway starting training error zero plateau represents average run rapid decrease ha not yet started convergence ha already achieved leading niﬁcant variance interestingly complete absence correlation state label r 1 qcnn average perfectly ﬁts training data even slightly faster real label r 0 random state scenario introduce randomness input ground state vector rather label goal introduce certain degree randomization quantum state preserving some inherent structure problem achieve deﬁne data distribution dst random quantum state speciﬁc manner instead drawing pure random state uniformly sample data dst ﬁrst draw pair original distribution yþ apply following mation state vector compute mean μψ variance fig 3 randomization test quantum phase recognition generalization gap function training set size achieved quantum convolutional neural network qcnn architecture qcnn trained real data random label data random state data horizontal dashed line largest eralization gap attainable characterized zero training error test error equal random guessing due task four possible class shaded area corresponds standard deviation across different experiment tions real data random label employed 8 16 32 qubits random state employed 8 10 12 qubits observe random label random state exhibit similar trend generalization gap slight discrepancy height due different relative frequency four class respective randomization protocol case test accuracy fails surpass random guessing notably largest ization gap occurs random label experiment using training set size n 10 highlighting memorization capacity particular qcnn training uncorrupted data yield behavior accordance previous b test error function ratio label corruption training qcnn training set size n 6 8 n plot illustrates polation uncorrupted data r 0 random label r 1 label corruption approach 1 test accuracy drop level random guessing dependence test error label corruption reveals ability qcnn extract remaining signal despite noise initial training set inset focus case n conveys optimization speed four different level corruption namely 0 2 4 6 6 label corrupted provides insight average convergence time shaded area denotes variance ﬁve experiment repetition independently initialized qcnn parameter surprisingly average ﬁtting completely random noise take le time ﬁtting unperturbed data phenomenon emphasizes qcnns accurately memorize random data article nature 2024 5 σψ amplitude sample new amplitude randomly gaussian distribution n ðμψ σψþ new amplitude obtained normalize random state experiment performed varying number qubits n 8 10 12 training set size n 5 8 10 14 20 fig show result ﬁtting random input state together random real label experiment outcome empirical generalization gap achieved qcnn random state exhibit similar shape obtained random label indeed slight difference relative occurrence four class lead improved performance biased random guessing observe qcnn perfectly ﬁt training set data generalization gap decrease analogously scenario random label case random state present intriguing aspect qcnn architecture wa initially designed unveil exploit local correlation input quantum however randomization protocol experiment remove precisely local information leaving only global information original data mean variance amplitude wa not case random label experiment input ground state remained unaltered only label modiﬁed ability qcnn memorize random data seems unaffected despite structure exploit local information implication ﬁndings indicate novel approach required studying capability quantum neural network elucidate experimental result ﬁt statistical learning theoretic work main goal machine learning ﬁnd expected risk minimizer fopt associated given learning task f opt arg min f rðf þ however given unknown nature complete data distribution evaluation r becomes infeasible consequently must resort unbiased estimator empirical risk r let mization algorithm obtain f approximate empirical risk minimizer f min f rsðf þ nonetheless although rsðf þ unbiased estimator r f remains uncertain whether empirical risk minimizer f yield low expected risk r f generalization gap gen f come critical quantity interest quantifying difference mance training set rsðf þ expected performance entire domain r f literature extensive effort invested providing robust guarantee magnitude generalization gap qml model generalization theorem assert reasonable assumption ization gap given model upper bounded quantity depend various parameter include property function family optimization algorithm used data tribution derivation generalization bound learning model typically involves rigorous mathematical calculation often considers restricted scenario many result literature ﬁt following template generic uniform generalization bound let f hypothesis class let any distribution let r risk functional associated r empirical version given set n labeled data let cðfþ complexity measure any function f 2 f generalization gap gen f upper bounded high probability genðf þ usually gunifðfþ 2 given explicitly make dependence gunif n implicit clarity high probability taken respect repeated sampling set size refer uniform generalization bound virtue equal element f class also bound apply irrespective probability distribution exists singular example doe not ﬁt template ref ticular case author introduce complexity measure resulting bound depends data tribution learned hypothesis albeit indirectly result present difﬁculties quantitative prediction usefulness uniform generalization bound lie ability provide performance guarantee model taking any computationally expensive training thus becomes interest identify range value cðfþ n result diminishing entirely vanishing generalization gap limit n bound usually deal asymptotic regime thus sometimes unclear tight statement practical scenario case risk functional bounded reﬁne bound example take probability error reðf þ pðx yþ f 2 immediately say any f trivial upper bound generalization gap gen f thus generalization bound could rewritten genðf þ 1 gunifðfþ additional threshold render actual value gunifðfþ siderable signiﬁcance necessary tool discus result experiment properly randomizing data simply involves changing distribution original domized 2 dr dstg remarked eq 9 doe not change different distribution implying upper bound generalization gap applies data coming corrupted data data input label uncorrelated any hypothesis not better random guessing expectation result expected risk value close maximum instance case probability error classiﬁcation task class input assigned class uniformly random must hold any hypothesis f reðf þ 1 indicating expected risk must always large large risk particular example doe not generally imply large generalization gap gen f f instance learning model unable ﬁt corrupted training set r e sðf þ one would small generalization gap gen f conversely eralization gap f large gen f learning algorithm must ﬁnd function actually ﬁt r e sðf þ yet even last scenario uniform generalization bound still applies let u denote size largest training set found function fr able ﬁt random data r e sðf rþ lead article nature 2024 6 large generalization gap gen fr since uniform eralization bound applies function class f 2 f found gunifðfþ 1 empirical lower bound generalization bound reveals generalization bound vacuous training set size noteworthy also regime generalization bound remains impractically large strength result resides fact not need specify complexity measure cðfþ empirical ﬁndings apply every uniform generalization bound irrespective derivation give strong evidence need perspective shift study generalization quantum machine learning analytical result previous section provided evidence qnns rately ﬁt random label experimental empirical ﬁndings restricted number qubits training sample tested limitation seem restrictive actually relevant regime interest considering empirical evidence section formally study memorization ability qml model arbitrary size beyond nisq era term ﬁnite sample expressivity goal establish sufﬁcient tions demonstrating qml model could ﬁt arbitrary training set not establish always possible scenario finite sample expressivity refers ability function family memorize arbitrary data general expressivity ability hypothesis class approximate function entire domain conversely ﬁnite sample expressivity study ability imate function subset although ﬁnite sample expressivity weaker notion expressivity seen stronger alternative hypothesis importance ﬁnite sample expressivity lie fact machine learning task always deal ﬁnite training set suppose given model found able realize any possible labeling available training set reasonably one would not expect model learn meaningful insight training data sible some form learning may still occur albeit without clear understanding underlying mechanism however circumstance uniform generalization bound would inevitably become trivial theorem 1 finite sample expressivity quantum circuit let ρn unknown quantum state n 2 n qubits n 2 oðpolyðnþþ let w gram matrix j trðρiρjþ w any yn 2 r real number construct quantum circuit depth poly n observable trðρimyþ yi proof given supplementary note theorem 1 give u constructive approach given ﬁnite set quantum state real label ﬁnd quantum circuit produce label expectation value input state give intuition qml model seem capable learning random label random quantum state nevertheless stated theorem fall short applying speciﬁcally pqcs construction propose requires query access set input state every time circuit executed estimate value trðρiρjþ employing swap test circuit realizes swap test bear little relation usual qml ansätze ideally possible one impose familiar pqc ture drop need use input state next propose alternative restricted version statement keeping qml mind desired application need sense distinguishability quantum state deﬁnition 1 distinguishability condition say quantum state ρn fulﬁll distinguishability condition ﬁnd intermediate state ρi based some generic quantum state approximation protocol fulﬁll following n ρi efﬁciently preparable pqc matrix w efﬁciently constructed entry w j ρjþ matrix w notable example approximation protocol inspired classical tensor instance similarly classical shadow one could draw unitaries approximate poly n using brickwork ansatz poly n layer haar random gate given quantum state ρ one produce several pair u b u randomly drawn unitary b outcome performing computational basis measurement one refers individual pair snapshot notice approach doe not follow exactly ditional classical shadow protocol end goal prepare approximation pqc rather utilizing classical simulation purpose particular not employ inverse measurement channel since would break complete positivity thus corresponding approximation would not quantum state snapshot one efﬁciently prepare corresponding quantum state b undoing unitary wa drawn preparing corresponding computational basis state vector given lection snapshot um bm approximation protocol would consist preparing mixed state 1 pm 1 uy bm since bm prepared n gate um brickwork pqc architecture approximation protocol fulﬁlls restriction efﬁcient preparation deﬁnition whether not any generic approximation protocol accurate enough speciﬁc choice quantum state discus method section analytical method present algorithm box 1 together correctness statement theorem given input state ρn box 1 moreover allows combine several quantum state approximation protocol order produce matrix inner product theorem 2 finite sample expressivity pqcs let ρn unknown quantum state n 2 n qubits n 2 oðpolyðnþþ fulﬁlling distinguishability condition deﬁnition construct pqc poly n depth parameterized observable mðϑþ any ynþ 2 r real number efﬁciently ﬁnd speciﬁcation parameter ϑy trðρi mðϑyþþ yi proof given supplementary note 2 us idea reminiscent formalism linear combination unitary theorem 2 understand pqcs produce any labeling arbitrary set quantum state provided fulﬁll distinguishability condition article nature 2024 7 notice deﬁnition 1 needed correctness theorem require knowledge efﬁcient classical description quantum state two main reason one hand pqcs object study hence need prepare approximation efﬁciently pqc addition hand ability condition also enough prevent u running like arising tributed inner product estimation result ref discussion next discus implication result suggest research avenue explore future shown quantum neural network qnns ﬁt random data including randomized label quantum state provided detailed explanation place ﬁndings statistical learning theory context not claim uniform generalization bound wrong any prior result false instead show statement theorem ﬁt generic uniform template must vacuous regime model able ﬁt large fraction random data brought randomization test ref 62 quantum level selected one promising qml architecture experiment known quantum convolutional neural network qcnn considered task classifying quantum phase matter application qml numerical result suggest must reach uniform generalization bound fully understand quantum machine learning qml model particular experiment like ately problematize approach based complexity measure like vc dimension rademacher complexity uniform relative best knowledge essentially generalization bound derived qml far uniform kind therefore ﬁndings highlight need perspective shift generalization qml future interesting conduct causation ments qnns using generalization measure ing candidate good generalization measure qml include time convergence training procedure geometric ness minimum algorithm converged robustness noise structure qcnn equivariant pooling er result ansatz restricted expressivity core feature including intermediate measurement rithmic depth make qcnn smaller model deeper pqcs like brickwork ansatz completely unrestricted meter language traditional statistical learning translates higher bias lower variance consequently task qcnn tends towards underﬁtting posing greater challenge achieving perfect ﬁtting training set compared expressive model result qcnn anticipated exhibit better generalization behavior compared usual efﬁcient qcnn thus assigned lower generalization bound larger model due higher variance latter therefore demonstration uniform generalization bound applied qcnn family trivially loose immediately implies bound applied le restricted model must also vacuous stated differently ﬁndings small model qcnn inherently apply larger model including efﬁcient ansatz furthermore study add evidence porting need proper understanding symmetry equivariance addition numerical experiment analytically shown qnns able ﬁt arbitrary labeling data set seems contradict claim training data provably sufﬁcient guarantee good generalization qml raised ref analytical numerical result not preclude possibility good generalization training data rather indicate not guarantee argument based uniform generalization bound reason successful generalization might occur yet discovered employ rest section comment signiﬁcance randomization experiment also describe parallelism difference work seminal ref 62 served basis experimental design particular two primary factor warrant consideration size model size training set learning task quantum phase recognition problem system 32 qubits use training set comprised 20 labeled pair following paragraph elucidate whether considered large small capable ﬁtting memorizing whether result experiment due ﬁnite sample size artifact upon ﬁrst glance training set size employed domization experiment may seem relatively small however essential consider randomization study within relevant context previously mentioned good generalization performance ha reported qml particularly classifying quantum phase matter using qcnn present bination model task also among best leading ches concerning generalization within qml literature key fact randomization test use training set size original experiment reported good generalization formance question whether randomization result caused relative ease ﬁnd pattern ﬁt given label small set data ruled fact small set size sufﬁce solve original problem qcnn able ﬁt random data only ﬁnite sample size artifact would anticipate expected risk generalization gap considerably large even original data given observation successful generalization data sampled original tribution conclude training set not small rather large enough study ref 62 common learning model considered regarded among best term generalization benchmark task also mization experiment case employed datasets taken experiment time yet spite larities imperative recognize learning model employed study fundamentally different not only operate distinct computing platform physically different nature also function produced neural network cally different produced parameterized quantum cuits consequence caution warranted expecting two different learning model behave equally faced mization experiment based unrelated learning task fact quantum classical learning model display similar result not taken granted key distinction lie notion overparameterization play critical role classical machine learning important distinguish notion overparameterization classicalml recently introduced deﬁnition overparameterization name deal different concept deep network studied ref 62 far parameter dimension input image training set size brings u refer large model conversely argue qcnn qualiﬁes small model although number parameter considered architecture larger size training set exhibit logarithmic scaling number qubits number dimension quantum state scale nentially hence inappropriate categorize model investigated large way classical model ref ﬁnd ability small quantum learning model ﬁt random data unexpected witnessed many work uniform article nature 2024 8 generalization bound quantum model published aftermath ref observation reveals promising research direction not only must rethink approach studying eralization qml must also recognize mechanism leading successful generalization qml may differ entirely classical machine learning higher level work exempliﬁes necessity establishing connection erature classical machine learning evolving ﬁeld tum machine learning method numerical method section provides comprehensive description numerical experiment including computation technique employed random real label implementation well random state partially corrupted label implementation random real label implementation test training ground state vector cluster hamiltonian eq 3 obtained variational principle matrix product state reading density matrix renormalization group software package utilized matrix product state backend simulate quantum cuits particular bond dimension χ 40 wa employed simulation qcnns ﬁnd increasing bond dimension doe not lead any noticeable change result random state label implementation scenario test training ground state vector obtained directly diagonalizing hamiltonian note qcnn comprised smaller number qubits example namely n 8 10 12 simulation quantum circuit wa performed using software framework allows faster simulation quantum circuit implementation training parameter initialized randomly optimization method employed update meter qcnn training stochastic optimization strategy code generated current study also available ref analytical method shed light practicality deﬁnition 1 requirement central theorem algorithm box 1 allows several approximation protocol combined increase chance fulﬁlling assumption deﬁnition indeed allow auxiliary state ρn linear combination several approximation state staying mindset deﬁnition cast problem ﬁnding optimal weighting linear combination linear optimization problem positive deﬁnite constraint theorem 3 ass distinguishability condition deﬁnition 1 speciﬁc state ρn speciﬁc approximation protocol theorem 3 also considers case different approximation protocol combined doe not contradict requirement theorem theorem 3 conditioning convex program 1 let ρn unknown quantum state n qubits n 2 oðpolyðnþþ any n let σi ðσi 1 σi mþ approximation ρi efﬁciently prepared using pqc assume computation trðρiσj kþ polynomial time any choice j call σ σn real number α αi k 2 rnm deﬁne auxiliary state ρn ρiðα σiþ x k 1 αi kσk matrix inner product wðα σþ entry wðα σþi j h tr ρjðα σjþ x k 1 αj ktr ρiσj k k wðα σþ k one decide polynomial time whether given ρn σ κ 2 r exists speciﬁcation α 2 rnm wðα σþ sense k wðα exists speciﬁcation convex problem sdp output instance α ρ σ κ w exists one also ﬁnd polynomial time α smallest k k norm proof inequality k wðα σþ k follows gershgorin circle given entry w bounded 0 1 particular largest singular value matrix w reach value n entry expression w j x k 1 αj ktr ρiσj k linear constraint α w j n κi w box 1 convex optimization state approximation require 1 ρ ρn state 2 approximation algorithm 3 κ number ensure α w possible 0 otherwise 4 5 n k 6 σi k akðρiþ 7 end 8 9 σ ðσi 10 11 α sdp ρ σ κ proof theorem 3 12 13 sdp fails 14 return 0 suitable α found 15 16 else 17 return α w 18 end article nature 2024 9 matrix ordering positive constraint w equivalent k w k κi w mean smallest singular value w lower bounded κ equivalent k wðα invertible wðα σþ test whether sucha w hence take form feasibility one additionally minimize objective function k k linear convex quadratic hence problem overall problem solved problem solved polynomial effort interior point method duality theory readily provides rigorous certiﬁcate proof refrain explicitly specifying deﬁnition σ relation original state ρn indeed success criterion resulting matrix w sufﬁcient condition could required gram matrix w j trðρiρjþ conditioned n least one k σk close ρi some distance metric condition would expect w nonetheless condition not necessary general plausible state ρi structed σ not close original state ρi resulting w not close w w still situation construction theorem 2 still hold propose using box 1 construct optimal auxiliary state ρn given unknown input state ρn collection available approximation protocol algorithm produce output either 0 case no combination imation state satisﬁes distinguishability condition provides weight α necessary construct auxiliary state sum approximation state theorem 3 prove correctness algorithm construction σ input state ρn play intuitive role success box let u consider two scenario first assume gram matrix initial state w tioned n least one k ρi σk instance exists least one speciﬁcation real value α w sufﬁces set αj k δj k latter denoting kronecker delta guarantee algorithm box 1 output satisfactory α potentially minimal norm polynomial time conversely consider scenario approximation protocol employed construct σ yield failure resulting σk 0 h n k case no choice α w well conditioned box 1 necessarily output 0 also within polynomial time refer proof theorem 2 supplementary note 2 explanation construct intermediate state ρi linear combination auxiliary state σi without giving pqc framework data availability data used study available zenodo database ref code availability code used study available zenodo database ref reference shor algorithm quantum computation discrete rithms factoring proceeding annual symposium foundation computer science ieee santa fe nm usa 1994 montanaro quantum algorithm overview npj quant inf 2 15023 2016 arute et al quantum supremacy using programmable conducting processor nature 574 2019 wu et al strong quantum computational advantage using superconducting quantum processor phys rev lett 127 180501 2021 hangleiter eisert computational advantage quantum random sampling rev mod phys 95 035001 2023 biamonte et al quantum machine learning nature 549 2017 dunjko briegel machine learning artiﬁcial intelligence quantum domain review recent progress prog phys 81 074001 2018 schuld petruccione machine learning quantum computer springer international publishing 2021 carleo et al machine learning physical science rev mod phys 91 045002 2019 schuld fingerhuth petruccione implementing classiﬁer quantum interference circuit europhys lett 119 60002 2017 havlíček et al supervised learning feature space nature 567 2019 schuld killoran quantum machine learning feature hilbert space phys rev lett 122 040504 2019 benedetti et al generative modeling approach marking training shallow quantum circuit npj quant inf 5 45 2019 zhu et al training quantum circuit hybrid quantum computer sci adv 5 2019 latorre data universal quantum classiﬁer quantum 4 226 2020 coyle mill danos kasheﬁ born supremacy quantum advantage training ising born machine npj quant inf 6 60 2020 lloyd schuld ijaz izaac j killoran quantum embeddings machine learning 2020 hubregtsen et al training quantum embedding kernel term quantum computer phys rev 106 042431 2022 rudolph et al generation handwritten digit quantum computer phys rev x 12 031010 2022 et al quantum generative adversarial network monte carlo event quantum 6 777 2022 benedetti lloyd sack fiorentini parameterized quantum circuit machine learning model quant sci tech 4 043001 2019 cerezo et al variational quantum algorithm nat rev phys 3 2021 bharti et al noisy quantum algorithm rev mod phys 94 015004 2022 sweke seifert hangleiter eisert quantum versus classical learnability discrete distribution quantum 5 417 2021 liu arunachalam temme rigorous robust tum supervised machine learning nat phys 17 1013 2021 article nature 2024 10 jerbi trenkwalder poulsen nautrup briegel j dunjko quantum enhancement deep reinforcement ing large space prx quantum 2 010328 2021 pirnay sweke eisert j seifert separation density modelling phys rev 107 042416 2023 sim johnson expressibility entangling capability parameterized quantum circuit hybrid algorithm adv quant tech 2 1900070 2019 tagliacozzo latorre scaling variational quantum circuit depth condensed matter system quantum 4 272 2020 wu yao zhang zhai expressivity quantum neural network phys rev 3 2021 herman et al expressivity variational quantum machine learning boolean cube ieee trans quant eng 4 2023 hubregtsen pichlmeier stecher bertels evaluation parameterized quantum circuit relation cation accuracy expressibility entangling capability quant mach intell 3 2021 haug bharti kim capacity quantum metry parametrized quantum circuit prx quantum 2 040309 2021 holmes sharma cerezo cole connecting ansatz expressibility gradient magnitude barren plateau prx quantum 3 010313 2022 mcclean boixo smelyanskiy babbush neven barren plateau quantum neural network training landscape nat commun 9 4812 2018 cerezo sone volkoff cincio cole cost function dependent barren plateau shallow parametrized quantum circuit nat commun 12 1791 2021 arrasmith cerezo czarnik cincio cole effect barren plateau optimization quantum 5 558 2021 kim kim j rosa universal effectiveness circuit variational eigenproblems phys rev 3 023203 2021 wang et al barren plateau variational tum algorithm nature comm 12 6961 2021 pesah et al absence barren plateau quantum tional neural network phys rev x 11 041011 2021 marrero kieferová wiebe barren plateau prx quantum 2 040316 2021 larocca ju cole j cerezo theory overparametrization quantum neural network nat comp sci 3 2023 sharma cerezo cincio cole trainability sipative quantum neural network phys rev lett 128 180505 2022 rudolph et al trainability barrier opportunity quantum generative modeling 2023 caro datta quantum circuit quant mach intell 2 14 2020 abbas et al power quantum neural network nature comp sci 1 2021 banchi pereira j pirandola generalization quantum machine learning quantum information standpoint prx quantum 2 040321 2021 bu koh li luo q zhang effect quantum resource noise statistical complexity quantum cuits quant sci technol 8 025013 2023 bu koh li luo q zhang rademacher complexity noisy quantum circuit 2021 bu koh li luo q zhang statistical complexity quantum circuit phys rev 105 062431 2022 du tu yuan x tao efﬁcient measure expressivity variational quantum algorithm phys rev lett 128 080506 2022 gyurik dunjko structural risk minimization quantum linear classiﬁers quantum 7 893 2023 caro meyer eisert j sweke generalization bound parametrized quantum circuit quantum 5 582 2021 caro et al generalization quantum machine learning training data nat commun 13 4919 2022 caro et al generalization learning quantum dynamic nat commun 14 3751 2023 qian wang du wu x tao dilemma quantum neural network ieee trans neu net learn syst 2022 du yang tao hsieh power quantum neural network multiclass classiﬁcation phys rev lett 131 140601 2023 schatzki larocca nguyen sauvage cerezo theoretical guarantee quantum neural network npj quantum inf 10 12 2024 peter schuld generalization despite overﬁtting tum machine learning model quantum 7 1210 2023 haug kim generalization quantum geometry learning unitaries 2023 vapnik chervonenkis uniform convergence relative frequency event probability th prob appl 16 1971 zhang bengio hardt recht b vinyals standing deep learning still requires rethinking generalization commun acm 64 2021 edgington onghena randomization test statistic series textbook monograph 4 edn chapman crc philadelphia pa 2007 valiant theory learnable commun acm 27 1984 understanding machine learning cambridge university press cambridge england 2014 http vapnik nature statistical learning theory springer ence business medium 1999 bartlett mendelson rademacher gaussian plexities risk bound structural result mach learn 3 2003 mukherjee niyogi poggio rifkin learning theory stability sufﬁcient generalization necessary sufﬁcient consistency empirical risk minimization adv comput math 25 2006 cong choi lukin quantum convolutional neural network nat phys 15 2019 kottmann metz fraxanet j baldelli variational tum anomaly detection unsupervised mapping phase diagram physical quantum computer phys rev 3 043184 2021 jerbi et al power limitation learning quantum dynamic incoherently 2023 carrasquilla j melko machine learning phase matter nat phys 13 2017 article nature 2024 11 sachdev quantum phase matter cambridge university press massachusetts 2023 9781009212717 broecker carrasquilla melko trebst machine learning quantum phase matter beyond fermion sign blem sci 7 8823 2017 verresen moessner pollmann metry protected topological phase transition phys rev b 96 165124 2017 vidal class quantum state efﬁciently simulated phys rev lett 101 110501 2008 huang kueng preskill predicting many property quantum system measurement nat phys 16 2020 rudolph chen miller acharya decomposition matrix product state shallow quantum cuits 00595 2022 child wiebe hamiltonian simulation using linear combination unitary operation quantum inf comput 12 2012 anshu landau z liu distributed quantum inner product estimation proceeding annual acm sigact posium theory computing stoc 2022 association computing machinery 2022 3519974 jiang neyshabur mobahi krishnan bengio tastic generalization measure ﬁnd 2019 kandala et al variational quantum solver small molecule quantum magnet nature 549 2017 meyer et al exploiting symmetry variational quantum machine learning prx quantum 4 010328 2023 skolik cattelan yarkoni bäck dunjko equivariant quantum circuit learning weighted graph npj quant inf 9 47 2023 larocca et al quantum machine learning prx quantum 3 030341 2022 white density matrix formulation quantum renormalization group phys rev lett 69 2863 1992 gray quimb python library quantum information body calculation open source soft 3 819 2018 zhang et al tensorcircuit quantum software framework nisq era quantum 7 912 2023 efthymiou et al qibo framework quantum simulation hardware acceleration quantum sci tech 7 015018 2021 hansen akimoto baudis github 2019 eisert j understanding quantum machine learning also requires rethinking generalization zenodo database 2023 gershgorin über die abgrenzung der eigenwerte einer matrix bulletin de l académie de science de l ur classe de science mathématiques et naturelles 6 1931 boyd vanderberghe convex optimization cambridge versity press cambridge 2004 acknowledgement would like thank matthias caro vedran dunjko johannes jakob meyer ryan sweke useful comment earlier version manuscript christian bertoni josé carrasco soﬁene jerbi insightful discussion also acknowledge bmbf hybrid bmwk eniqma planqk quantera hqcc quantum flagship cluster excellence dfg crc 183 einstein foundation einstein research unit quantum device erc debuqc ﬁnancial support author contribution project ha conceived experimental design ha laid analytical result proven numerical experiment performed project ha supervised author contributed writing manuscript funding open access funding enabled organized projekt deal competing interest author declare no competing interest additional information supplementary information online version contains supplementary material available correspondence request material addressed jens eisert carlos peer review information nature communication thanks deng anonymous reviewer contribution peer review work peer review ﬁle available reprint permission information available publisher note springer nature remains neutral regard jurisdictional claim published map institutional afﬁliations open access article licensed creative common attribution international license permit use sharing adaptation distribution reproduction any medium format long give appropriate credit original author source provide link creative common licence indicate change made image third party material article included article creative common licence unless indicated otherwise credit line material material not included article creative common licence intended use not permitted statutory regulation exceeds permitted use need obtain permission directly copyright holder view copy licence visit author 2024 article nature 2024 12