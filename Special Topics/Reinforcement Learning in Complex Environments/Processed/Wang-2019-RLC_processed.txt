ieee transact vehicular technolog vol march autonom navig uav complex environ deep reinforc learn approach chao wang student member ieee jian wang senior member ieee yuan shen member ieee xudong zhang member ieee thi paper propos deep reinforc ing drl method allow unman aerial vehicl uav execut navig task complex ronment thi techniqu import mani applic good deliveri remot surveil problem late partial observ markov decis process pomdp solv novel onlin drl algorithm design base two strictli prove polici gradient theorem within framework contrast convent simultan local sens approach method directli map uav raw sensori measur control signal navig experi result demonstr method enabl uav autonom perform navig virtual complex environ aliz complex ronment besid propos onlin drl algorithm address pomdp outperform index navig deep reinforc learn partial observ markov decis process introduct ver past year seen unpreced growth unman aerial vehicl uav appli variou area aerial photographi rescu crop tection long term goal uav applic build ligent system implement variou task without human intervent thi work seek develop techniqu allow uav autonom navig arbitrari ture place destin complex environ urban area crowd skyscrap techniqu import mani applic good deliveri genci aid remot surveil creat safer smarter citi multitud method rang propos address uav tion problem common approach manuscript receiv may revis septemb ber accept decemb date public januari date current version march thi work wa support nation key research develop program china grant review thi paper wa coordin terje correspond author jian wang author depart electron engin inghua univers beij china cn zhangxd digit object identiﬁ sens avoid avoid collis steer vehicl opposit direct perform navig path plan peng instanc use optic ﬂow techniqu detect obstacl rapidli explor dom tree algorithm implement path plan jason bine reactiv algorithm algorithm implement navig anoth class base method make use simultan local map slam intuit struct map environ use slam navig achiev path plan cui combin grahpslam onlin path plan modul ing approach enabl uav ﬁnd trajectori foliag environ apart two class method navig also done retrac rie method thi categori gener fall cise local uav use variou posit techniqu kalman ﬁltere variat navig achiev minim error estim posit posit common featur base method requir explicit path plan may lead unexpect failur environ highli dynam complex thi respect work resort machin learn approach imit learn inforc learn rl nursultan ple develop rl algorithm name texplor high level control method uav navig grid map barrier stephan build imit base control use small set human demonstr achiev good perform natur forest environ new challeng aris uav perform navig task complex environ environ scale thi case method lose efﬁcienc sinc infeas build map environ environ complex crowd dens obstacl sens approach gener design address navig problem environ spars obstacl may lose efﬁcienc appli complex environ environ often dynam evid plan incap handl situat rl adapt dynam environ need improv order cope complex environ sensor limit sens capac uav navig fulli depend ieee person use permit requir ieee permiss see http inform author licens use limit queen mari univers london download march utc ieee xplore restrict appli wang et al autonom navig uav complex environ observ return sensor sensor limit sens capac uav might confus situat encount overcom challeng uav need resort entir histori observ help distinguish tere situat decid action take thi respect decis make process ﬁt framework partial observ markov decis process pomdp teriz process agent hidden state obtain observ state take action transit anoth den state obtain reward therefor thi paper model navig problem pomdp design deep reinforc learn drl algorithm settl pomdp structur uav navig problem must meet sever requir firstli sinc infeas know priori state transit biliti observ probabl secondli action space continu sinc uav control signal ou lastli observ space reward function inform enough order achiev certain goal respondingli design observ space synthes multipl sensori output character uav intern state relationship environ destin besid domain reward function design award penal certain action taken certain state rl solv pomdp maintain memori past observ action stochast determinist control polici deriv stochast polici map memori past observ action tion action space determinist polici map memori action method polici gradient od method method deriv parameter stochast polici pomdp continu action space ing gradient descent paramet space algorithm suffer major drawback implement fashion besid rl known unstabl nonlinear function approxim use mate valu function thi respect drl combin deep learn rl directli project raw sensori put control signal develop provid solut concept markov decis process mdp pomdp mnih ﬁrstli util drl play electron game discret control proﬁl afterward lillicrap appli insight problem continu action space design deep determinist polici gradient algorithm ddpg within framework heess take partial observ state mdp consider design recurr determinist polici gradient algorithm rdpg show littl perform differ stochast determinist polici control pomdp becaus drl obtain optim control polici learn might lead irrevers damag agent shall capabl learn small sampl real world rdpg sinc updat polici paramet episod end episod agent interact environ execut action determin learn polici episod end agent reach ani termin state obvious updat ramet polici make immedi use newest experi facilit agent better plore unknown environ learn faster besid unlik ddpg conduct paramet optim base state transit tupl state action reward next state rdpg updat paramet base entir episod break episod state transit tupl given pomdp hold markov properti show optim procedur inefﬁci stochast gradient descend sgd appli conduct paramet tion thi regard propos efﬁcient onlin drl algorithm name solv pomdp main contribut thi paper follow develop drl framework uav navig scale complex environ navig problem model pomdp solv novel onlin drl algorithm prove pomdp set polici gradient within framework determin histori servat action instead entir episod polici gradient determinist polici special case stochast polici without precondit design efﬁcient onlin drl algorithm approach pomdp continu action space base two rem build stochast complex environ valid effect propos method remaind thi manuscript structur follow section ii introduc background knowledg mdp pomdp two drl algorithm solv pomdp continu control space section iii formul tion problem section iv elabor pomdp model process navig problem propos algorithm demonstr section simul result discuss present section vi section vii conclud thi paper envisag futur work ii background thi section ﬁrst give brief introduct mdp pomdp follow two drl algorithm solv pomdp continu action space mdp pomdp pomdp extens mdp partial observ state allow decis make uncertain condit mdp compos state space initi state space initi state distribut p action space state transit probabl distribut p isfi markov properti reward function r st character environ feedback execut action state state space action space mdp could either discret continu sinc uav control signal continu thi paper focu mdp pomdp continu state action space eas author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact vehicular technolog vol march denot leav implicit follow context summat oper actual integr oper mdp gener address via rl learn mal polici π learn polici could either determinist stochast determinist ici denot μ project state action stochast polici denot p set probabl measur return abil densiti avail state action pair action space continu stochast determinist ici parameter function θ μ θ θ refer paramet function keep notat simpl leav implicit case polici π μ function θ gradient respect rl involv estim valu function state valu function pair stochast polici valu function deﬁn vπ st e γlr γ discount factor rang function qπ st e γlr stochast polici optim agent receiv mum expect futur discount reward also refer target function execut η π e γtr st determinist polici valu function function target function obtain replac π μ st mdp becom pomdp agent could observ state st instead receiv observ ot distribut p immedi servat sequenc longer satisﬁ markov properti p ot p quenc agent need access entir histori trajectori ht ot infer current state st make decis base goal rl partial observ set thu learn optim polici project histori trajectori action distribut maxim ddpg rdpg categori polici gradient method dress mdp continu control space seek learn optim polici θ μ θ ment polici gradient paramet space π π qπ π state distribut domli gener episod start initi state follow framework polici π function qπ call actor critic respect critic parameter function qω estim td method ωt ϵ γqωt st st γqω refer target valu ϵ learn rate learn method ddpg deriv determinist polici gradient theorem mdp say mdp continu action space minist polici gradient exist comput μ qμ dμ denot state distribut randomli gener episod rdpg extend framework ddpg pomdp date polici paramet use follow polici gradient μ e ht qμ ht ht dure learn rdpg agent interact environ use current learn polici end episod agent cach entir episod replay memori design random observ stabil learn process sampl batch episod conduct paramet optim rdpg show effect mani simul tinuou control task partial observ state ha two limit rdpg ofﬂin learn algorithm onli execut paramet optim end episod tribut insufﬁci util experi ration environ rdpg converg slowli demonstr section v paramet date base histori trajectori instead entir episod observ stochast gradient actual sum sequenc histori gradient episod rdpg converg slowli use sgd mizat method thi respect propos new approach address pomdp iii problem formul thi section formul uav navig problem complex environ coordin frame uav typic control proﬁl uav ha three degre freedom includ throttl heav steer refer speed chang vertic displac rotat around vers axe respect correspondingli altogeth author licens use limit queen mari univers london download march utc ieee xplore restrict appli wang et al autonom navig uav complex environ fig illustr partial observ uav navig complex environ solid black circl gray circl repres uav perceptron rang arrow denot possibl trajectori gener uav six independ coordin determin posit tion motion uav coordin frame use describ uav absolut posit orient denot ϕ x z ϑ ϑy ϑz besid dinat frame use describ uav linear angular veloc denot μ b c ϑa ϑb ϑc coordin frame togeth coordin frame character uav posit orient motion refer uav intern state descript uav navig simplic assum uav ﬂy ﬁxed speed ﬁxed height unless state furthermor omit uav momentum ﬂy suppos steer action take effect time consequ activ uav restrict x plane vector describ uav posit orient motion simpliﬁ ζ x ϑ dynam uav formul ϑt xt v co yt v sin steer signal v constant speed goal control uav ﬂy arbitrari ture place φdep destin φde nate frame complex environ without trap collid obstacl iv pomdp model thi section ﬁrst explain exampl reason whi state navig problem partial observ present detail pomdp model procedur uav navig pomdp depict fig suppos uav locat point approach target posit shall head north point thi point uav ﬁnd path target block obstacl shall turn back turn left turn right fig b observ uav environ denot distanc return nine virtual rang ﬁnder ϖ repres angl distanc uav current posit destin ϑ denot angl uav direct north direct c deploy addit sensor dimension environ denot distanc return rang ﬁnder outsid horizont plane given horizont detect direct ϑz denot angl uav movement direct perspect direct turn back reach point limit sens capac immedi ﬁnd surround due limit sens capac consequ turn back eventu stray similarli turn left right uav also end cycl trap contrast uav could rememb ha experienc shall gradual construct structur local environ base experi accordingli make better decis mdp assum state fulli observ would fail captur complex structur environ comparison pomdp abl obtain inform histori trajectori therefor shall work efﬁcient observ space action space speciﬁc purpos navig uav shall least capabl receiv inform three sourc inform indic intern state relationship ronment relationship destin firstli sinc environ often dynam abandon uav absolut posit x onli use ϑ tion describ intern state illustr fig b practic ϑ measur devic secondli uav relationship surround acter imag return camera radar signal return radar distanc return rang ﬁnder thi work use nine rang ﬁnder character relat denot ψ depict fig lastli tor ξ ϖ repres distanc angl uav current posit target use describ connect destin depict fig b practic ξ measur devic compos three kind inform bring us ﬁnal tion observ space ϑ ϖ ϑ ϖ π given uav ﬂight altitud speed preﬁx constant use ρ rang π denot steer signal reward design incorpor domain knowledg reward act signal evalu good take action state uav navig complex author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact vehicular technolog vol march environ straightforward way spars reward mean uav would reward onli arriv target posit howev initi polici randomli gener uav strike destin probabl zero environ full barrier accordingli learn algorithm would take extrem long time converg altern approach reward reward shape provid learn agent speciﬁc form reward guarante polici invari nonetheless sinc onli make sens condit shape reward differ potenti function hard implement practic tend block way incorpor domain knowledg thi paper design reward incorpor domain knowledg navig problem preserv rel satisfactori polici reward consist four part name transit reward cle penalti reward step penalti transit reward design rtran σddist σ posit constant ddist reduc distanc tween uav current posit destin transit transit reward incorpor knowledg encourag uav head destin ize distanc target besid practic collid obstacl could catastroph uav prevent get close ani obstacl design obstacl penalti rbar n α β two posit constant dmin imum distanc uav obstacl courag avoid barrier would obtain constant reward rf ree direct point place lastli ensur uav arriv tinat soon possibl transit would get constant penalti rstep summar ﬁnal reward formul rf inal rtran rbar rf ree rstep differ instanc may result differ polici obtain polici expect paramet must make deep survey thi relationship section vi new approach address pomdp thi section deriv novel onlin drl algorithm solv pomdp begin deriv state basic deﬁnit deﬁn valu function underli state st observ ht v ht π st e st ht π function underli pair st observ ht qht π st e st ht π expect reward take action state st observ ht rht π st e st ht π take expect respect state distribut st obtain valu function tori trajectori ht vπ ht est v ht π st function histori trajectori action pair ht qπ ht est qht π st reward histori trajectori action pair ht rπ ht est rht π st base deﬁnit perform object η π vπ expect initi histori trajectori equal initi observ take gradient perform object spect paramet θ bring us stochast polici gradient theorem pomdp theorem consid learn stochast polici pomdp continu action space gradient stochast polici exist calcul theorem stochast polici gradient theorem pomdp consid learn stochast polici pomdp continu action space gradient stochast polici exist calcul π h log π qπ h dπ h expect histori trajectori distribut proof see appendix remark stochast gradient relat tori trajectori instead entir episod result everi time agent interact environ perform onlin paramet optim without wait end episod facilit agent better explor unknown environ learn faster analog structur reﬂect pomdp convert mdp inform regard histori jectori pomdp fulli observ state silver prove determinist polici gradient mdp exist special case stochast polici gradient tic polici satisﬁ certain condit nonetheless argu author licens use limit queen mari univers london download march utc ieee xplore restrict appli wang et al autonom navig uav complex environ fig schemat view simul stochast complex environ obstacl build differ type environ distinguish shape size mutual space random environ manifest two aspect four type environ uniformli randomli select learn agent everi time befor prepar interact environ onc environ chosen agent build immedi randomli regener theorem extend determinist polici make ani assumpt extens accomplish reformul determinist polici dirac delta function theorem ii determinist polici gradient theorem pomdp consid learn determinist polici pomdp continu action space gradient determinist polici μ h exist calcul μ h h qμ h h dμ h denot histori trajectori distribut induc determinist polici proof see appendix b remark stochast gradient equal count sum term entir episod paramet actor critic rdpg actual mize base sequenc highli correl histori rie framework actor critic approxim function paramet updat use sgd polici gradient algorithm address pomdp base might potenti stabl converg faster base sgd quir input data sequenc valid conclus experi section vi sinc gradient involv take expect respect unknown histori trajectori tribut dπ h must estim sampl state action space thi regard rl algorithm use stochast polici gener converg much slowli use terminist polici determinist polici onli requir sampl state space stochast polici need ple state space action space faster converg design drl algorithm name base theorem ii propos algorithm demonstr tabl use two target actor critic network replay memori aim stabil learn process addit stochast process explor nois current learn polici encourag agent keep explor state action space compar rdpg perform onlin learn strategi updat paramet term histori trajectori instead entir episod tabl vi simul result thi section present experiment set ulat result along discuss experiment set construct four type complex environ stochast environ depict fig ment cover one half squar kilomet crowd build random height dure tation agent uniformli randomli choos one environ rollout episod follow current learn ici corrupt explor nois everi time befor agent start interact environ build height uniform distribut u regener analog rdpg actor critic imat two lstm summar inform encod author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact vehicular technolog vol march fig network structur actor critic two network take exactli form size ﬁrst three layer respect lstm cell output layer contrast critic network input action sequenc actor network begin design keep uniform structur network fold form zero histori trajectori network structur demonstr fig observ uav normal steer signal normal uav speed ﬂight altitud set respect reward instanti σ α β rf ree rstep maximum episod length time step set besid adam optim ploy learn network paramet learn rate actor critic respect eter critic network regular weight decay discount factor γ soft target date rate ε addit explor nois uniform distribut u use explor state action space navig behavior valid effect propos method train agent dispatch execut multipl gation task simul environ depict fig project histori trajectori control signal agent success ﬂie departur place destin manlik navig behavior suggest drl efﬁcient approach address uav navig problem scale complex environ verifi memor agent cient perform navig task memoryless ddpg agent select two pair start posit target sition difﬁcult environ let two agent execut navig task depict fig learn navig complex ment nevertheless sinc agent make sion base histori observ action trajectori perceiv inform local structur environ accordingli capabl escap trap contrari ddpg agent rememb ha experienc make decis onli base current ception environ consequ percept local structur environ tend caught trap conclus also veriﬁ converg curv ddpg fig obtain evalu target function use mont carlo method speciﬁc independ navig task perform everi ﬁve hundr train episod use learn polici target function approxim averag count cumul reward navig episod seen two algorithm manifest similar converg speed estim normal return ddpg much lower converg also make quantit survey success rate stray rate crash rate navig mission ddpg agent well agent four environ agent success rate stray rate crash rate obtain calcul percentag success navig mission percentag fail mission end trap local environ percentag fail mission due crash obstacl complet igat mission environ respect result illustr tabl ii see success rate ddpg agent four environ lower onli averag contrast agent ﬁnish navig mission success rate environ averag signiﬁc perform improv addit two agent exhibit low crash rate stray rate ddpg agent reach env iv environ largest amount trap much higher agent valid ddpg agent rememb ha experienc prone trap agent make decis base histori rienc ha abil escap trap signiﬁc perform improv ddpg come cost increas comput complex ddpg use two feedforward neural network approxim actor critic use two recurr neural network lstm experi comput complex optim recurr neural network higher compar optim feedforward neural network perform valid efﬁcient rdpg rdpg equal illustr fig onli use around episod converg rdpg use around episod reach converg point sinc tice interact real environ could catastroph compar rdpg requir less time interact environ therefor efﬁcient learn believ manc improv come two aspect onlin learn algorithm compar rdpg perform paramet optim end entir episod updat paramet everi time agent interact author licens use limit queen mari univers london download march utc ieee xplore restrict appli wang et al autonom navig uav complex environ fig navig trajectori agent simul environ solid circl star denot departur place destin respect curv connect repres navig trajectori clariti ani build lower altitud uav ﬂie drawn imag besid build obstruct view trajectori clip lower one height equal uav ﬂight altitud fig schemat view navig trajectori agent ddpg agent sake clariti onli top view environ drawn build lower uav ﬂight altitud omit subﬁgur c depict navig trajectori agent subﬁgur b correspond trajectori ddpg agent fig converg curv ddpg rdpg repres number episod use train ize return train stage return formul approxim averag return navig trajectori gener learn polici environ make immedi use newest perienc efﬁcient data util high correl among histori trajectori episod reduc efﬁcienc rdpg data util trast use mutual independ histori trajectori sampl replay memori perform paramet mizat therefor work efﬁcient although rdpg achiev similar maliz return converg learn polici deviat lot see tabl ii cess rate rdpg agent four environ lower onli averag around lower agent besid stray rate rdpg agent much higher agent consequ though two rithm design settl pomdp efﬁcient superior rdpg also demonstr strong gener abil height build stochast environ ha uniform distribut interv lower height denser obstacl test gener abil two algorithm agent rdpg agent dispatch execut navig mission differ ﬂight altitud complex environ besid also ﬁx ﬂight altitud let two agent execut navig task even complex ment result illustr fig fig see fig b c decreas ﬂight altitud success rate agent doe degrad wherea rdpg agent manifest rel cant decreas besid contrast agent stray rate crash rate ddpg agent tend increas author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact vehicular technolog vol march tabl ii statist quantiti ddpg rdpg differ environ fig success rate stray rate crash rate averag ﬂight distanc navig mission agent rdpg agent differ ﬂight altitud ﬂight altitud success rate stray rate crash rate obtain way tabl ii averag ﬂight distanc estim averag length success arriv trajectori environ ﬂight altitud decreas higher success rate lower stray rate crash rate agent differ ﬂight tude demonstr doe simpli rememb structur complex environ deriv abil cope complex environ fig averag ﬂight distanc two agent see averag ﬂight distanc rdpg agent also uniformli longer agent differ ﬂight altitud illustr propos efﬁcient ddpg escap trap complex environ besid see lower ﬂight altitud step two agent spend head destin result accord intuit becaus decreas ﬂight altitud obstacl environ becom denser path destin becom twisti addit illustr fig increas area complex environ two agent manifest tendenc decreas success fig success rate stray rate crash rate averag ﬂight distanc navig mission agent well rdpg agent complex environ differ size environ differ size obtain simpli extend origin environ shown fig larger size tabl iii statist quantiti train differ reward rate increas stray rate crash rate averag ﬂight distanc nonetheless success rate agent environ still higher around higher ddpg agent scale environ besid rdpg agent averag ﬂight distanc much shorter agent especi environ consid distanc uav destin one dimens servat space agent never train complex environ high perform illustr ha strong gener abil author licens use limit queen mari univers london download march utc ieee xplore restrict appli wang et al autonom navig uav complex environ fig schemat view inﬂuenc reward navig behavior breviti onli choos two pair start point target point env env iv subﬁgur c show navig trajectori agent rf r ee rstep b correspond navig trajectori rf r ee rstep fig navig trajectori agent environ clariti draw build obstruct view trajectori effect reward addit partial observ state reward also fect navig behavior agent vestig inﬂuenc navig behavior vari reward step penalti reset reward step penalti rf ree rstep respect keep paramet unchang retrain agent tabl iii show success rate stray rate crash rate averag ﬂight distanc agent befor ter retrain see train low step penalti high reward agent show tic success rate decreas around besid stray rate averag ﬂight distanc signiﬁcantli increas respect ing retrain agent tend stray free space rather head destin fig give schemat view result see fed higher reward lower step penalti agent stray free space fail strike destin result tent intuit sinc valu reward step penalti exactli modul much want uav head free space avoid obstacl head destin soon possibl result though reward appropri way encod domain knowledg problem speed learn procedur must design care thi might main drawback reward without reward shape navig environ demonstr method also work dimension environ sinc uav gener permit ﬂy neither high low urban area restrict cruis altitud ing descend process begin end navig procedur taken consider uav take land vertic thu long uav rive overhead space target posit navig mission complet horizont detect direct add four rang ﬁnder sens obstacl outsid horizont plane illustr fig c assum uav direct parallel horizont plane adjust also made reward function action observ space uav get extra penalti altitud near either end altitud interv penalti obstacl penalti except dmin repres minimum vertic distanc end altitud interv besid illustr fig c new dimens ad control vector maneuv vertic angl ϑz uav movement direct direct observ space adjust ad uav altitud hz vertic angl ϑz distanc return addit rang ﬁnder adjust lustrat fig uav also perform navig task environ contrast set uav environ avoid obstacl chang horizont direct well altitud success rate stray rate crash rate agent respect compar set crash rate increas think perform degrad result poor sens capac rang author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact vehicular technolog vol march ﬁnder differ camera rang ﬁnder onli detect obstacl sever direct therefor sibl obstacl veri close uav uav think keep proper distanc situat come even wors environ sinc environ even complex environ vii conclus futur work thi work develop drl framework uav igat complex environ problem formul pomdp gener drl algorithm propos address without map reconstruct path plan method enabl uav ﬂy arbitrari partur place destin complex ment theoret deriv simul result strate efﬁcient rdpg besid could gener complex ronment onli minor perform degrad less still work futur stanc reward function must design otherwis may yield unsatisfactori perform tial way settl thi problem directli handl spars reward appendix proof follow along similar line standard polici gradient theorem mdp deriv shown bottom thi page valu function vπ h valu function qπ h satisfi recurs relationship similar mdp use p ht denot observ dynam histori trajectori analog environ dynam note p ht fact refer p ht set p ht zero ht preﬁx extend repres transit probabl ani pair histori trajectori sinc explicitli reformul target function term valu function histori trajectori gradient involv take deriv valu function vπ h term polici weight equip recurs relationship left part thi proof becom straightforward appli follow line deriv write gradient valu function vπ h γkp k π qπ h h π h qπ h pr k π denot probabl ing histori trajectori h k step follow polici π h could ani histori trajectori sinc extend p ht entir histori trajectori space besid π h γkp k π repres histori trajectori distribut randomli erat episod start initi histori trajectori follow polici immedi gradient target function written π π h π qπ h h π qπ h dπ h deﬁn theorem appendix b ﬁrst introduc lemma lemma deriv dirac delta function suppos f x ani continu function δ x dirac delta function deriv δ x hold follow properti x f x dx recal main context replac integr tion summat oper case eas denot rewrit π h dπ h π π qπ h dadh qπ ht est st qht π st est st est st γest st eot st st p ot v ht π rπ ht γ est st ot st st ot p ot ht v ht π rπ ht γeht ht ht est st st st v ht π rπ ht γeht ht ht est st v ht π rπ ht γ ht p ht vπ author licens use limit queen mari univers london download march utc ieee xplore restrict appli wang et al autonom navig uav complex environ π h dπ h π π qπ h dadh h dπ h f h f h qπ h μ h h f h dh h dμ h f h f h h h qπ h μ h h f h dh h dμ h h qμ h μ h h h f h dh h dμ h h h qπ h μ h h f h dh h dμ h h μ h h h h f h dh h dμ h h h h dh determinist polici repres δ μ h let f h μ h polici denot δ f h substitut employ chang variabl niqu properti present lemma obtain ﬁnal result shown top thi page refer moham idri moham jawhar uav smart citi opportun challeng proc ieee int conf unman aircraft pp israelsen beall bareiss stuart keeney van den berg automat collis avoid manual man aerial vehicl proc ieee int conf robot pp chee zhong control navig collis avoid unman aerial vehicl sen actuat vol pp agraw ratnoo ghose invers optic ﬂow base guidanc uav navig urban canyon aerosp sci vol pp gageik benz montenegro obstacl detect collis avoid uav complementari sensor ieee access vol pp peng lin dai path plan obstacl anc vision guid quadrotor uav navig proc ieee int conf control pp roelofsen gillet martinoli reciproc collis avoid quadrotor use visual detect proc int conf intel robot pp belkhouch model calcul collis risk air hicl ieee tran veh vol pp jun luo mcclean parr teaci de nardi uav posit estim collis avoid use extend kalman ﬁlter ieee tran veh vol pp jul cui lai dong chen autonom navig uav foliag environ intel robot vol pp gee jame van der mark delma gimel farb lidar guid stereo simultan local map slam uav outdoor scene reconstruct proc ieee int conf imag vi comput new zealand pp li liu zhang hang imu integr navig slam method small uav indoor environ proc ieee conf inerti sensor syst pp fu campoy monocular collis avoid strategi uav use fuzzi logic control intel robot vol pp oguz temelta consist analysi uav navig proc spie vol pp thrun montemerlo graph slam algorithm cation map urban structur int robot vol pp zhou zou pei ying liu yu structslam visual slam build structur line ieee tran veh vol pp apr goh abdelkhalik zekavat weight ment fusion kalman ﬁlter implement uav navig aerosp sci vol pp zhang kleeman robust appear base visual rout low navig outdoor environ int robot vol pp karpenko konovalenko miller miller nikolaev uav control basi landmark observ sensor vol pp konovalenko miller miller nikolaev uav navig basi featur point detect underli face proc eur conf model pp prieto visual base navig power line inspect use virtual environ proc spie vol art strydom thurrowgood srinivasan visual odometri autonom uav navig use optic ﬂow stereo proc trala conf robot pp ross et learn monocular reactiv uav control clutter natur environ proc ieee int conf robot pp faust palunko cruz fierro tapia autom aerial suspend cargo deliveri reinforc learn artif vol pp imanberdiyev fu kayacan chen autonom igat uav use reinforc ing proc ieee int conf control robot pp wang wang zhang zhang autonom tion uav unknown complex environ deep reinforc learn proc ieee int conf globalsip pp chrisman reinforc learn perceptu alias ceptual distinct approach proc assoc advanc artif pp author licens use limit queen mari univers london download march utc ieee xplore restrict appli ieee transact vehicular technolog vol march lin mitchel memori approach reinforc learn domain school comput carnegi mellon pittsburgh pa usa tech may william simpl statist algorithm nectionist reinforc learn mach vol pp wierstra foerster peter schmidhub solv deep memori pomdp recurr polici gradient proc int conf artif neural pp jurˇ ıˇ cek thomson young natur actor belief critic reinforc algorithm learn paramet dialogu system model pomdp acm tran speech lang vol pp wierstra schmidhub polici gradient critic proc eur conf mach pp mnih et control deep reinforc ing natur vol pp sutton barto reinforc learn introduct vol cambridg usa mit press lillicrap et continu control deep reinforc ing konda tsitsikli algorithm proc neural inf process pp heess hunt lillicrap silver control recurr neural network silver lever heess degri wierstra riedmil determinist polici gradient algorithm proc int conf mach pp ng harada russel polici invari reward transform theori applic reward shape proc int conf mach vol pp harutyunyan devlin vrancx e express arbitrari reward function advic proc assoc advanc artif pp kingma ba adam method stochast optim proc int conf learn sutton mcallest singh mansour polici dient method reinforc learn function approxim proc advanc neural inf process pp chao wang receiv degre electron engin tsinghua univers beij china current work toward degre depart tronic engin tsinghua univers supervis professor zhang hi research terest includ signal process machin learn intellig control jian wang sm receiv degre electron engin tsinghua univers beij china join faculti tsinghua univers current sociat professor depart electron engin hi research interest includ intellig collabor system inform secur enhanc technolog signal process encrypt domain wireless network yuan shen receiv degre highest honor electron engin tsinghua univers beij china degre electr engin comput scienc mit cambridg usa respect associ professor ment electron engin tsinghua univers prior wa research assist postdoctor associ wireless tion network scienc laboratori mit dure hi research interest includ statist infer commun theori inform theori optim hi current research focus network local navig infer techniqu resourc alloc cooper network shen chair also serv vice chair secretari ieee comsoc radio tion committe tpc symposium ieee icc also wa ieee globecom eusipco ieee icc advanc network local navig anln workshop ha editor ieee transact wireless commun sinc ieee wireless cation letter sinc ieee china commun sinc ieee commun letter intern journal distribut sensor network wa recipi ieee comsoc board outstand young scholar award qiu shi outstand young scholar award china youth talent program marconi societi paul baran young scholar award hi paper receiv ieee comsoc fred ellersick prize three best paper award ieee intern confer xudong zhang receiv degre tsinghua univers beij china ha depart electron gineer tsinghua univers sinc ha author coauthor paper three book ﬁeld signal process chine learn hi research interest includ tical signal process machin learn theori multimedia signal process author licens use limit queen mari univers london download march utc ieee xplore restrict appli