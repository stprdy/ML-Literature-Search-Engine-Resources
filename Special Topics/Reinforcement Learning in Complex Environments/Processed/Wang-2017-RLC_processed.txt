autonom navig uav unknown complex environ deep reinforc learn chao wang jian wang xudong zhang xiao zhang depart electron engin tsinghua univers china abstract unman aerial vehicl uav base deliveri thrive thi paper model autonom navig uav unknown complex environ continu control problem solv use deep reinforc learn without path plan map construct method enabl uav navig arbitrari departur place destin use onli sensori inform local environ gp signal argu navig task partial observ markov decis process pomdp extant recurr determinist polici gradient algorithm less effici consequ deriv faster polici learn algorithm pomdp base architectur valid idea simul five virtual environ virtual uav fli fix altitud constant speed cognit local environ achiev measur distanc uav obstacl multipl direct simul result demonstr effect method index autonom navig uav deliveri deep reinforc learn partial observ markov decis process introduct remot deliveri uav burgeon techniqu ha attract mani research use last resort deliveri hierarchi promot develop smart citi thi envis attract research face two challeng autonom navig navig unknown complex environ urban area cui bachrach employ slam simultan local map algorithm attack challeng basic idea increment build map unknown environ use simultan local nevertheless argu unnecessari learn map unknown environ stretch mile plan path base instanc finish deliveri task within area uav may dispatch anoth unknown area israelsen provid anoth solut concept sens avoid empow uav sens obstacl path destin evad go back autonom manual trajectori howev thi techniqu work simpl environ countrysid dens obstacl complex environ urban area crowd sky scraper signal tower may tremend reduc effici must continu cope obstruct put uav back trajectori new trajectori zhang propos approach enabl robot retrac rout control men base visual appear match howev intract uav fli random start posit random target posit aforement method partial solv navig problem unknown complex environ includ learn map unknown environ rout plan interact human oper thi paper aim empow uav without construct local global map path plan use onli gp signal sensori inform local environ navig autonom intellig arbitrari departur place arbitrari target posit unknown complex environ model autonom navig problem continu control problem employ deep reinforc learn drl solv drl appli deep learn tradit reinforc learn rl function approxim provid solut concept markov decis process mdp partial observ markov decis process pomdp directli project sensori input control signal bridg gap cognit natur world control strategi firstli introduc mnih creat virtual agent surpass play multipl electron game discret control afterward lillicrap use two neural network approxim actor critic algorithm base design deep determinist polici gradient ddpg algorithm solv mdp continu control heess extend pomdp approxim actor critic use two recurr neural network develop algorithm name recurr determinist polici gradient rdpg work uav regard distanc obstacl multipl direct orient angl ieee globalsip author licens use limit queen mari univers london download march utc ieee xplore restrict appli distanc angl present posit destin deriv gp signal sensori inform learn navig target posit use drl propos method includ learn local global map unknown complex environ path plan besid argu navig problem pomdp extant rdpg algorithm effici framework learn use memori replay consequ base architectur function approxim deriv effici polici learn algorithm pomdp continu control remind thi manuscript organ follow section ii elucid navig model briefli introduc basic theori rl drl deriv rdpg algorithm section iii valid method simul section iv conclud thi paper envisag futur work method model descript autonom navig real environ could complex sake breviti without loss gener creat virtual uav fli fix altitud fix speed control profil onli contain turn left right besid simul sever type environ unknown complex environ depict fig goal virtual uav fli arbitrari start posit arbitrari destin use gp signal percept local environ model navig problem control problem employ drl solv first describ profil sensori inform state virtual uav suppos uav mount five virtual rang finder illustr fig regard distanc uav obstacl five direct uav sensori inform environ nevertheless obviou five distanc insuffici finish navig task qualifi uav navig combin five distanc uav orient angl distanc angl present posit destin depict fig b final state anoth import issu reward specif work reward compos four part first part environ penalti uav close ani obstacl dure fli penal regard minimum distanc obstacl use exponenti function formul penalti minimum distanc decreas penalti grow exponenti second part transit reward distanc uav target posit decreas time step uav reward proport reduc distanc third part direct reward uav would get constant reward direct accord correspond direct biggest distanc among five distanc forth part step penalti everi time step uav would given constant penalti encourag reach target posit soon possibl till ddpg directli employ solv navig problem howev depict fig uav could easili trap local area argu aforement state descript partial observ real state uav real state determin intern state posit inform local environ uav onli ha limit abil percept local environ rdpg algorithm drl provid solut concept mdp pomdp mdp describ set environ state set initi state distribut p transit function p reward function r st goal drl agent learn optim stochast polici π st fig five type virtual environ cover around one squar kilomet obstacl obstacl obstacl obstacl target posit north first perspect b fig rang finder sens obstacl five differ direct b angl present posit target posit direct angl uav author licens use limit queen mari univers london download march utc ieee xplore restrict appli determinist polici μ st maxim cumul discount reward mdp becom pomdp agent could directli observ state st instead obtain observ ot ot p consequ polici function trajectori ht ot state differ rl need handcraft featur state represent drl automat abstract state descript raw sensori input use deep network structur rdpg base algorithm solv pomdp approxim actor critic polici use two recurr neural network function qμ μθ dure learn actor network updat follow gradient back propag time bptt h q h h e γ discount factor note expect explicitli entir trajectori τ argu thi polici updat suitabl framework learn use memori replay design reduc correl present observ sequenc stabil learn process speed converg procedur specif suppos underli real state ht st rdpg actual updat actor base sequenc highli correl state break correl make use architectur function approxim deriv polici updat directli maxim expect accumul discount reward obtain new polici updat pomdp continu control new polici explicitli updat histori trajectori ht rather entir trajectori follow detail deriv defin stochast polici function parameter θ h denot underli real state observ histori trajectori ht st st p st ht defin valu function function expect reward observ histori trajectori h p h v h e e r h h k k p h p p k q h e e r h h h p h p r e e r h h h p p relat valu function function describ h h q h r p h h v h p repres probabl transit trajectori h trajectori h h obvious extend thi transit probabl ani two trajectori let p h h h defin target function p h j e v h p p repres initi observ distribut h v h h v h q h h p h h note eq possess structur eq therefor rewritten h v h h h h q h ρπ repres histori distribut induc polici polici updat deriv log p h h h j e e h q h elucid eq expect histori trajectori h rather entir trajectori note deriv polici stochast one lever prove determinist polici special case stochast polici follow hi idea obtain updat determinist polici μθ ht pomdp continu control p h h h q h h j h e outlin tabl design new algorithm base eq name simul result author licens use limit queen mari univers london download march utc ieee xplore restrict appli depict fig abstract real complex environ fix altitud five virtual environ includ mani trap dens obstacl addit virtual uav speed set meter per time step befor learn sensori input normal control signal normal mean turn left degre turn right degre learn algorithm specifi exactli except explor nois experi use explor nois follow uniform distribut u order speed state space search process dure learn episod five environ chosen equal chanc start posit target posit gener uniformli randomli chosen environ afterward learn agent run episod whose maximum length simultan updat paramet ddpg employ solv navig problem valid effici propos method randomli gener four pair start posit destin complex environ fifth virtual environ let agent rdpg agent fli start posit target posit depict fig two agent learn navig without map construct path plan nevertheless sinc rdpg agent know local inform rdpg agent learn navig polici less greedi rdpg agent possess abil avoid escap trap ddpg agent easili trap valid effici propos draw converg curv rdpg depict fig expect due high correl present observ sequenc converg much faster ddpg conclus futur work challeng uav autonom navig complex unknown environ work drl employ solv problem base gp signal sensori inform local environ though uav environ virtual simul result demonstr feasibl propos approach furthermor compar origin rdpg algorithm converg much faster howev still much work futur first propos method need test real environ includ complex obstacl tree electr wire second critic choos proper sensor work use five virtual rang finder realiti camera radar seem feasibl third effect sensor nois taken consider though alreadi regard navig problem pomdp effect introduc sensor nois still unknown tabl algorithm initi critic network qw ht actor μθ ht paramet w initi target network qw μθ weight w w θ initi replay buffer initi process n action explor receiv initi observ obtain action μθ ht nt execut action receiv reward rt obtain new observ ot store transit ot rt r updat histori trajectori ht ot sampl minibatch l transit hi ai oi ri set yi ri γqw hi ai oi μθ hi ai oi comput critic updat use bptt q h q h l comput actor updat use bptt q h h h l updat actor critic use adam updat two target network w εw w θ εθ θ end end b fig illustr trajectori circl repres start posit star repres target posit trajectori gener ddpg agent b trajectori gener agent number histori trajectori reward rdpg fig converg curv rdpg author licens use limit queen mari univers london download march utc ieee xplore restrict appli refer amazon prime air onlin avail http access may moham idri moham jawhar may uav smart citi opportun challeng unman aircraft system icua intern confer pp ieee cui lai dong chen autonom navig uav foliag environ journal intellig robot system bachrach prentic roy robust autonom navig environ journal field robot dissanayak newman clark csorba solut simultan local map build slam problem ieee transact robot autom israelsen beall bareiss stuart keeney van den berg j may automat collis avoid manual unman aerial vehicl robot autom icra ieee intern confer pp ieee zhang kleeman robust appear base visual rout follow navig outdoor environ intern journal robot research mnih kavukcuoglu silver rusu veness bellemar petersen control deep reinforc learn natur lillicrap hunt pritzel heess erez tassa wierstra continu control deep reinforc learn arxiv preprint konda tsitsikli algorithm advanc neural inform process system pp silver lever heess degri wierstra riedmil determinist polici gradient algorithm intern confer machin learn heess hunt lillicrap silver base control recurr neural network arxiv preprint sutton mcallest singh mansour polici gradient method reinforc learn function approxim advanc neural inform process system pp kingma ba j adam method stochast optim arxiv preprint author licens use limit queen mari univers london download march utc ieee xplore restrict appli