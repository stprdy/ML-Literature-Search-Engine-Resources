autonomous navigation uav unknown complex environment deep reinforcement learning chao wang jian wang xudong zhang xiao zhang department electronic engineering tsinghua university china abstract unmanned aerial vehicle uavs based delivery thriving paper model autonomous navigation uav unknown complex environment continuous control problem solve using deep reinforcement learning without path planning map construction method enables uavs navigate arbitrary departure place destination using only sensory information local environment gps signal argue navigation task partially observable markov decision process pomdp extant recurrent deterministic policy gradient algorithm le efficient consequently derive faster policy learning algorithm pomdp based architecture validate idea simulate five virtual environment virtual uav flying fixed altitude constant speed cognition local environment achieved measuring distance uav obstacle multiple direction simulation result demonstrate effectiveness method index autonomous navigation uav delivery deep reinforcement learning partially observable markov decision process introduction remote delivery uavs burgeoning technique ha attracted many researcher used last resort delivery hierarchy 1 promote development smart city 2 envision attractive researcher face two challenge autonomous navigation navigation unknown complex environment urban area cui 3 bachrach 4 employ slam simultaneously localization mapping algorithm 5 attack challenge basic idea incrementally building map unknown environment using simultaneously localize nevertheless argue unnecessary learn map unknown environment stretch mile plan path based instance finishing delivery task within some area uav may dispatched another unknown area israelsen 6 1 provide another solution concept sense avoidance empowers uavs sense obstacle path destination evade go back autonomously manually trajectory however technique work simple environment countryside dense obstacle complex environment urban area crowded sky scraper signal tower may tremendously reduce efficiency must continuously cope obstruction put uavs back trajectory new trajectory zhang 7 proposes approach enables robot retrace route control men based visual appearance matching however intractable uavs fly random starting position random target position aforementioned method partially solve navigation problem unknown complex environment include learning map unknown environment route planning interaction human operator paper aim empower uavs without constructing local global map path planning using only gps signal sensory information local environment navigate autonomously intelligently arbitrary departure place arbitrary target position unknown complex environment model autonomous navigation problem continuous control problem employ deep reinforcement learning drl solve drl applies deep learning traditional reinforcement learning rl function approximation provides solution concept markov decision process mdp partially observable markov decision process pomdp directly project sensory input control signal bridge gap cognition natural world control strategy firstly introduced mnih 8 create virtual agent surpass playing multiple electronic game discrete control afterwards lillicrap 9 us two neural network approximate actor critic algorithm 10 based 11 design deep deterministic policy gradient ddpg algorithm solve mdp continuous control heess 12 extends 9 pomdp approximating actor critic using two recurrent neural network develops algorithm named recurrent deterministic policy gradient rdpg work uav regard distance obstacle multiple direction orientation angle 858 ieee globalsip 2017 authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply distance angle present position destination derived gps signal sensory information learns navigate target position using drl proposed method include learning local global map unknown complex environment path planning besides argue navigation problem pomdp extant rdpg algorithm not efficient framework learning using memory replay 8 consequently based architecture function approximation 13 derive efficient policy learning algorithm pomdp continuous control reminder manuscript organized follows section ii elucidates navigation model briefly introduces basic theory rl drl derives rdpg algorithm section iii validates method simulation section iv concludes paper envisages future work method model description autonomously navigate real environment could complex sake brevity without loss generality create virtual uav fly fixed altitude fixed speed control profile only contains turning left right besides simulate several type environment unknown complex environment depicted fig goal virtual uav fly arbitrary starting position arbitrary destination using gps signal perception local environment model navigation problem control problem employ drl solve first describe profile sensory information state virtual uav supposing uav mounted five virtual range finder illustrated fig 2 regard distance uav obstacle five direction uav sensory information environment nevertheless obvious five distance insufficient finishing navigation task qualify uavs navigation combine five distance uav orientation angle distance angle present position destination depicted fig 2 b final state another important issue reward specification work reward composed four part first part environment penalty uav close any obstacle flying penalized regarding minimum distance obstacle use exponential function formulate penalty minimum distance decrease penalty grows exponentially second part transition reward distance uav target position decrease time step uav rewarded proportional reduced distance third part direction reward uav would get constant reward direction accordance corresponding direction biggest distance among five distance forth part step penalty every time step uav would given constant penalty encourage reach target position soon possible till ddpg directly employed solve navigation problem however depicted fig 3 uav could easily trapped some local area argue aforementioned state description partial observation real state uav real state determined internal state position information local environment uav only ha limited ability percept local environment rdpg algorithm drl provides solution concept mdp pomdp mdp described set environment state set initial state distribution p transition function p reward function r st goal drl agent learn optimal stochastic policy π st fig five type virtual environment cover around one square kilometer obstacle obstacle obstacle obstacle 1 2 target position north first perspective b fig 2 range finder sensing obstacle five different direction b angle present position target position direction angle uav 859 authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply deterministic policy μ st maximizes cumulated discounted reward mdp becomes pomdp agent could not directly observe state st instead obtains observation ot ot p consequently policy function trajectory ht ot state different rl need handcrafted feature state representation drl automatically abstract state description raw sensory input using deep network structure rdpg based algorithm solves pomdp approximating actor critic policy using two recurrent neural network function qμ μθ learning actor network updated following gradient 12 back propagation time bptt 1 h q h h e γ discounted factor note expectation explicitly entire trajectory τ argue policy update not suitable framework learning using memory replay 8 designed reduce correlation present observation sequence stabilize learning process speed convergence procedure specific supposing underlying real state ht st rdpg actually update actor based sequence highly correlated state break correlation make use architecture function approximation 13 derives policy update directly maximizing expected accumulated discounted reward obtain new policy update pomdp continuous control new policy explicitly updated history trajectory ht rather entire trajectory follows detailed derivation define stochastic policy function parameterized θ h denote underlying real state observing history trajectory ht st st p st ht define value function function expected reward observing history trajectory h 1 1 1 1 p h v h e e r h h 1 1 1 1 k k p h p p k q h e e r h h 1 1 h p h p r e e r h h 1 1 1 1 1 h p p relation value function function described h h q h r p h h v h p represents probability transition trajectory h trajectory h h obviously extend transition probability any two trajectory letting p 0 h h h define target function 0 0 p h j e v h p p represents initial observation distribution 0 0 0 0 0 h v h h v h q h h p h h note eq 8 posse structure eq 7 13 therefore rewritten 0 0 h v h h h h q h ρπ represents history distribution induced policy policy update derived 0 0 log p h h h j e e h q h elucidated eq 10 expectation history trajectory h rather entire trajectory note derived policy stochastic one lever 11 prof deterministic policy special case stochastic policy following idea obtain update deterministic policy μθ ht pomdp continuous control 0 0 p h h h q h h j h e outlined table 1 design new algorithm based eq 11 9 name simulation result 860 authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply depicted fig 1 abstract real complex environment fixed altitude five virtual environment include many trap dense obstacle additionally virtual uav speed set 2 meter per time step learning sensory input normalized control signal normalized mean turning left 180 degree 1 turning right 180 degree learning algorithm specify exactly 9 except exploration noise experiment use exploration noise following uniform distribution u order speed state space searching process learning episode five environment chosen equal chance starting position target position generated uniformly randomly chosen environment afterwards learning agent run episode whose maximum length simultaneously update parameter ddpg employed solve navigation problem validate efficiency proposed method randomly generate four pair starting position destination complex environment fifth virtual environment let agent rdpg agent fly starting position target position depicted fig 3 two agent learned navigate without map construction path planning nevertheless since rdpg agent know local information rdpg agent learned navigation policy le greedy rdpg agent posse ability avoid escape trap ddpg agent easily trapped validate efficiency proposed draw convergence curve rdpg depicted fig expected due high correlation present observation sequence converges much faster ddpg conclusion future work challenging uavs autonomously navigate complex unknown environment work drl employed solve problem based gps signal sensory information local environment though uav environment virtual simulation result demonstrate feasibility proposed approach furthermore compared original rdpg algorithm converges much faster however still much work future first proposed method need tested real environment includes complex obstacle tree electric wire second critical choose proper sensor work use five virtual range finder reality camera radar seem feasible third effect sensor noise taken consideration though already regard navigation problem pomdp effect introduced sensor noise still unknown table algorithm initialize critic network qw ht actor μθ ht parameter w initialize target network qw μθ weight w w θ initialize replay buffer initialize process n action exploration receive initial observation obtain action μθ ht nt execute action receive reward rt obtain new observation ot store transition ot rt r update history trajectory ht ot sample minibatch l transition hi ai oi ri set yi ri γqw hi ai oi μθ hi ai oi compute critic update using bptt 1 q h q h l compute actor update using bptt 1 q h h h l update actor critic using adam 14 update two target network w εw w θ εθ θ end end b fig illustration trajectory circle represents starting position star represent target position trajectory generated ddpg agent b trajectory generated agent 0 2000 4000 6000 8000 10000 12000 0 20 40 60 80 100 120 number history trajectory reward rdpg fig convergence curve rdpg 861 authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply reference 1 amazon prime air online available accessed 7 may 2017 2 mohammed idries mohamed jawhar 2014 may uavs smart city opportunity challenge unmanned aircraft system icuas 2014 international conference pp ieee 3 cui lai dong chen 2016 autonomous navigation uav foliage environment journal intelligent robotic system 84 4 bachrach prentice roy 2011 robust autonomous navigation environment journal field robotics 28 5 5 dissanayake newman clark csorba 2001 solution simultaneous localization map building slam problem ieee transaction robotics automation 17 3 6 israelsen beall bareiss stuart keeney van den berg j 2014 may automatic collision avoidance manually unmanned aerial vehicle robotics automation icra 2014 ieee international conference pp ieee 7 zhang kleeman 2009 robust appearance based visual route following navigation outdoor environment international journal robotics research 28 3 8 mnih kavukcuoglu silver rusu veness bellemare petersen 2015 control deep reinforcement learning nature 518 7540 9 lillicrap hunt pritzel heess erez tassa wierstra 2015 continuous control deep reinforcement learning arxiv preprint 10 konda tsitsiklis 2000 algorithm advance neural information processing system pp 1014 11 silver lever heess degris wierstra riedmiller 2014 deterministic policy gradient algorithm international conference machine learning 12 heess hunt lillicrap silver 2015 based control recurrent neural network arxiv preprint 13 sutton mcallester singh mansour 2000 policy gradient method reinforcement learning function approximation advance neural information processing system pp 14 kingma ba j 2014 adam method stochastic optimization arxiv preprint 862 authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply