journal machin learn research submit revis publish simpl agent complex environ eﬃcient reinforc learn agent state shi dong benjamin van roy bvr stanford univers zhengyuan zhou zzhou new york univers editor alekh agarw abstract design simpl reinforc learn rl agent implement optimist version establish regret analysi thi agent oper level compet ani environ leverag concept literatur provabl eﬃcient rl consid gener interfac provid novel agent design analysi thi level gener posit result inform design futur agent oper complex real environ establish time progress agent perform competit rel polici requir longer time evalu time take approach asymptot perform polynomi complex agent state represent time requir evalu best polici agent repres notabl depend complex environ ultim perform loss agent bound constant multipl measur distort introduc agent state represent thi work ﬁrst establish algorithm approach thi asymptot condit within tractabl time frame keyword reinforc learn dynam program regret analysi agent design introduct reinforc learn agent demonstr remark success simul ronment exampl recent develop muzero agent schrittwies et learn interact eﬀect ani broad rang environ simul deliv superhuman perform play chess go shogi arcad game continu innov thi area aim produc agent engag increasingli complex environ ultim environ like physic world world wide web pose far greater complex agent repres grow mathemat literatur focus establish eﬃcienc guarante typic term sampl complex regret bound kearn singh jaksch et al repres earli instanc inde data eﬃcienc remain impedi c shi dong benjamin van roy zhengyuan zhou licens see http attribut requir provid http dong van roy zhou carri success reinforc learn simul real environ agent must learn within reason time frame mathemat literatur ought inform futur agent design howev work thi area ha tend focu restrict class environ produc bound depend number environ state eﬀect inﬁnit complex environ thi paper aim bridg divid particular extend idea mathemat literatur relax common restrict establish result oﬀer insight simpl agent oper eﬀect arbitrarili complex environ consid problem formul ha previous studi thi work contribut multipl front frame learn object agent design perform analysi figur bridg divid provabl eﬃcient reinforc learn versu practic agent design complex environ consid interfac illustr figur deﬁn ﬁnite action set ﬁnite observ set agent interact environ execut time action regist observ gener singl stream experi time agent select base histori ht ot initi histori empti interfac describ veri gener agent engag thi manner arbitrarili complex environ exampl consid agent interact world wide web via comput termin action could encod keystrok mous click movement observ could take form pixel render monitor context environ would like far complex agent simpl agent complex environ environ dynam character function ρ assign probabl ρ p ht observ henc environ speciﬁ tupl e ρ ﬁxed set observ probabl function order accommod complex real environ formul enabl us relax sever restrict assumpt commonli made literatur figur interfac assum environ markov decis process mdp would requir observ probabl ρ depend histori onli recent observ assum environ exhibit episod behavior would requir environ occasion assum perform optim polici accur estim within manag time frame complex environ requir time intract larg even inﬁnit requir agent suppli durat oper input consid instead singl endless stream experi call agent perform well ani long horizon polici perform polici π map histori action probabl probabl assign action histori h denot π let p denot set polici denot b π polici execut agent agent design amount specifi thi polici typic term algorithm sampl action accord b π design prefer express term reward function histori ht action observ thi function prescrib reward r ht character perform polici term expect reward formal thi notion build gener probabilist framework detail present appendix thi framework action observ random variabl observ probabl function ρ also random variabl unknown agent design consequ environ e random variabl formal deﬁn appendix use subscript indic probabl expect evalu action select particular polici exampl action probabl dong van roy zhou satisfi pπ π expect return timestep polici π written eπ express probabl expect b π suppress subscript exampl p pb π b π e eb π thi notat denot averag reward polici π λπ lim inf x e optim averag reward λπ quantifi agent perform rel refer polici π timestep environ e term regret regretπ e x e also consid notion regret rel refer polici class sup regretπ note regretp simpli regret rel optim averag reward exist optim polici regretp note express deﬁn regret random variabl depend environ perspect agent design reason goal would attain low expect regret e regretp long durat expect taken random howev thi paper examin perform ﬁxed agent case distribut e point mass p e e e ani environ establish almost sure upper bound random variabl regretp natur bound would impli upper bound e regretp note practic agent design restrict ﬁxed agent abl leverag prior knowledg e improv agent design thi topic nevertheless beyond scope thi work regret bound necessarili depend time requir assess polici complex environ time requir assess optim polici arbitrarili larg even inﬁnit develop bound depend instead time requir assess polici refer class particular bound indic time progress agent accumul experi perform well rel polici take longer assess simpl agent practic agent must oper bound memori comput constraint agent retain repeatedli process histori rather agent maintain onli agent state xt suﬃc produc action sinc xt repres agent retain histori must updat increment accord b f xt simpl agent complex environ agent state updat function b f repres algorithm random discuss lu et al popular agent design dqn mnih et muzero schrittwies et mpo abdolmaleki et song et agent state partit three compon agent state xt situat state st epistem state pt algorithm state zt situat state meant captur salient inform agent current situat environ epistem state retain agent knowledg environ algorithm state record inform unrel environ read agent intern clock intern gener random number reward comput agent depend histori situat state let denot ﬁnite set situat state reward function take form r r gener reward accord r st note reward function special case introduc section r ht thi work focu case reward comput situat state instead directli histori sinc set situat state ﬁnite allow reward depend situat state ensur comput reward function r doe requir inﬁnit memori thi paper design analyz simpl agent engag ani environ instanti follow input initi situat state updat function f reward function r provid precis speciﬁc later paper oﬀer rough descript agent oper agent updat situat state accord f st use thi comput reward illustr figur note function f assum given agent begin ﬁxed throughout agent lifetim sequenc situat state deﬁn recurs f situat state dynam need markovian particular p st p ht agent epistem state pt qt nt compris action valu function qt count function nt agent algorithm state includ onli current time action valu qt updat via optimist discount algorithm discount factor degre optim increas time agent updat nt track visit count use determin suitabl degre optim action sampl uniformli set arg qt st greedi action henc ani time agent seen execut polici πt action probabl depend histori ht onli situat state dong van roy zhou figur agent maintain situat state use comput reward worth emphas agent design oﬀer perform simul real environ rather motiv design agent amen theoret analysi aim gener insight inform design futur agent exampl servic rate control let us consid didact exampl exceedingli simpl serv elucid notat framework exampl involv agent oper servic station illustr figur time one custom present agent appli servic mode fast slow custom pay upon arriv cost incur slow mode appli custom serv fast mode servic incur cost per timestep maxim averag reward agent must make choic balanc revenu cost servic figur servic station serv custom thi problem one servic rate control studi oper research see weber stidham jr stidham jr weber jo sennott howev work ha tend focu agent eﬀect appli particular styliz model govern arriv servic rate approach instead adapt ani statist structur doe suﬀer misspeciﬁc work reinforc learn control queue system moallemi et raei et share thi spirit note onli order convey idea simpl transpar manner focu simpl servic system agent appli much complex environ exampl involv multipl server queue simpl agent complex environ interfac agent perspect servic station view environ e ρ action fast slow identifi servic mode observ arriv departur indic arriv departur henc servic mode appli timestep indic ani arriv departur occur end timestep function ρ speciﬁ observ probabl condit histori initi unknown exampl design may uncertain custom arriv rate depend histori situat state dynam consid situat state st simpli indic presenc custom sinc observ record arriv departur function f f st servic initi vacant proﬁt written r st function special interest polici select action base onli situat state set polici pπ pπ let us denot thi set baselin agent consid agent design learn polici within thi class polici simpl enough agent could perform nearli exhaust search restrict attent approach scale set involv much larger set situat state two simpl agent thi kind serv baselin comparison let πϵ p polici absenc custom appli slow mode otherwis sampl slow fast probabl baselin agent begin execut πϵ ϵ appli slow servic mode everi timestep ﬁrst agent increas ϵ gather data long durat use data estim arriv rate estim warrant increas servic rate thi agent analysi static sens doe entail ani experiment instead assum arriv rate remain ﬁxed second agent addit tri small valu ϵ durat order estim deriv averag reward thi deriv posit increas one signiﬁc diﬀer rel ﬁrst agent use deriv thi second agent anticip impact small increas ϵ bear arriv rate second agent repres approach use polici gradient literatur discuss sutton barto refer therein environ dynam studi behavior agent given environ dynam character speciﬁc observ probabl function correspond realiz environ denot provid detail speciﬁc appendix b assum purpos thi analysi p e thi environ custom arriv rate depend maximum servic time experienc among recent dozen custom serv idea long servic time hurt reput turn reduc number custom seek servic servic time impact agent choic dong van roy zhou fast mode servic alway complet singl timestep slow mode servic complet next timestep probabl given speciﬁc maxim averag reward per timestep thi achiev appli fast mode servic everi timestep case custom serv singl timestep new custom arriv soon previou one depart perform figur plot cumul move averag reward attain optimist agent later present averag two hundr independ simul ﬁgure also plot maximum averag reward averag reward attain alway appli slow servic mode baselin agent never choos deviat slow servic mode therefor realiz averag reward close latter figur cumul move averag reward attain optimist agent maxim averag reward averag reward attain alway appli slow servic mode approxim behavior baselin agent result convey potenti beneﬁt agent design address gener environ optimist agent eventu ﬁgure choic drive futur arriv rate suﬃcient improv perform baselin agent demonstr level sophist contribut relat literatur formul interact studi thi paper veri gener involv singl stream experi without restrict assumpt commonli made literatur discuss section import theoret work relax assumpt inform design agent oper complex real environ thi formul bear close resembl studi mccallum hutter daswani et al lu et al formul focu work provabl eﬃcient reinforc learn work ﬁrst extend regret analysi tool thi set simpl agent complex environ build upon gener model interact thi paper make rang contribut innov frame learn object agent design perform analysi well gener qualit insight inform practic agent design thi section summar contribut relat prior literatur frame learn object literatur provabl eﬃcient reinforc learn common studi agent perform regret analysi howev manner regret bound typic frame doe suitabl accommod complex environ develop concept allow us frame meaning learn object context averag time intellig choos polici agent must assess rel perform regret bound establish literatur typic reﬂect thi requir via depend statist bound time requir assess optim polici exampl includ episod durat osband et azar et jin et diamet jaksch et span bartlett tewari ouyang et wei et complex environ time requir assess optim polici intract larg even inﬁnit deriv bound instead depend reward averag time polici refer class let λπ h denot expect averag reward timestep start histori h λπ lim deﬁn reward averag time τπ polici π smallest valu τ h h thi close relat concept introduc kearn singh deﬁn notion averag time function toler paramet associ error h deﬁnit reli instead singl scalar statist τπ let optim polici essenti equival notion span introduc bartlett tewari note might exist histori h polici π λπ h doe converg λπ result τπ need rule case analysi see later certain polici π τπ vacuou regret bound regret thi make intuit sens sinc inﬁnit reward averag time impli agent might abl revers previou mistak consequ could get stuck suboptim indeﬁnit distort distinct element formul agent instanti situat state updat function thi serv simplifi agent experi extract use featur histori enabl product behavior arbitrarili complex environ dong van roy zhou particular instead number environ state regret bound depend number situat state distort incur use predict optim discount valu follow let φ h denot situat state would gener experienc histori h speciﬁc φ h map deﬁn h ot histori sequenc contain timestep st sequenc situat state gener h f f st f ot φ h worth emphas map φ determin avoid clutter suppress depend notat discount factor γ histori h action denot optim discount action valu qγ h discount factor γ weight reward realiz k timestep γk thought prescrib eﬀect plan horizon τ deﬁn distort eﬀect plan horizon τ max sup φ h qγ h inf φ h qγ h γ thi maximum diﬀer optim action valu across histori lead situat state oﬀer measur error introduc predict optim action valu base situat state instead histori thi sort distort measur ha long use analysi approxim dynam program algorithm aggreg environ state gordon tsitsikli van roy van roy though thi case instead aggreg histori frame requir stronger notion distort deﬁn sup τ thi quantiﬁ accuraci situat state predict optim action valu plan horizon durat τ greater thi distort measur oﬀer use statist character perform agent abl plan eﬀect increas horizon data accumul worth mention aggreg histori deﬁnit distort bear resembl notion abstract deﬁn li et li et al presum environ model mdp thi work set histori inﬁnit would like highlight conceptu valu simpl agent complex environ distinct sinc goal analys concret agent oper ﬁnite memori whi thi work instead simpli assum exist abstract function map histori situat state consid situat state updat function f allow agent recurs comput next situat state base previou one way abl ensur agent abl maintain updat situat state ﬁnite memori worth mention distinct doe aﬀect regret analysi reader familiar rl theori might also connect thi work li comprehens studi abstract framework note result thi work appli gener set sinc doe impos ﬁxed discount factor sequenc reward moreov theori show compar delay algorithm studi li ultim perform loss agent enjoy much grace depend abstract gap deﬁn section li note environ deﬁn view mdp inﬁnit mani mdp state aggreg histori consid amount state aggreg broader context state aggreg view form function approxim larg bodi theoret literatur ha dedic function approxim mdp one relev work thi categori jiang et author show environ dynam approxim admit factor class function approxim state valu accord factor number sampl requir learn polici environ polynomi respect rank speciﬁc thi result impli sampl complex learn polici state aggreg polynomi number aggreg recent work along thi line includ jin et agarw et wang et zanett et howev algorithm work sampl complex guarante mainli serv theoret purpos littl bear practic agent design wherea one main target work shed light element empir success agent reader interest theoret result function approxim refer recent paper zhou et provid comprehens review limit frame use ﬁxed situat state updat function thi consist manner practic agent oper exampl dqn agent mnih et take situat state number recent video frame like valu adapt way situat state updat base learn environ encod agent epistem state muzero agent schrittwies et doe adapt updat function thi way agent repres updat function term recurr neural network weight adapt time base interact environ despit thi limit frame repres signiﬁc step advanc mathemat literatur direct may inform futur agent design dong van roy zhou refer class frame agent design object notion compet eﬀect polici particular refer class eﬀect measur len regret function averag time distort oppos singl scalar object spirit oﬀer framework studi deriv interpret regret bound gener insight inform agent design understand thi spirit may help draw analog ﬁeld optim optim problem frame term precis scalar object design optim algorithm tend formul term measur comput complex solut qualiti function number decis variabl constraint well salient problem characterist one refer class introduc earlier denot p consist polici π pπ pπ word polici select action base situat state instead histori ideal situat state suﬃc predict agent requir make optim decis case p would includ optim polici let π p polici π p λπ think agent tri learn polici within p natur expect λb π π b π polici execut agent deﬁn section discuss appendix exist environ situat state dynam π π consequ λb π π averag regret satisﬁ lim inf regretp π π π light thi fundament limit polici class p frame object optim depend averag regret distort aforement object call agent eventu compet eﬀect best polici among select action base situat state second object frame call agent attain eventu level perform quickli time requir depend time take compar polici bound averag time discuss earlier import avoid depend averag time optim polici well number environ state intract larg even inﬁnit complex environ instead consid regret bound depend number situat state averag time particular consid regret bound form regretp π π foobar metasyntact function abus notat use denot set cardin understand regret depend argument guid design quickli learn perform well rel addit consid τ refer class pτ π τπ consist polici averag time greater class consid bound form regretpτ τ simpl agent complex environ diﬀer function foobar bound oﬀer insight agent quickli learn perform well rel polici ani particular averag time agent ought abl compet polici pτ within time grow τ regret bound reﬂect relationship draw attent balanc associ agent design agent implement variant watkin earli analys focus asymptot converg guarante assumpt agent tri action environ state inﬁnit often watkin watkin dayan tsitsikli jaakkola et recent research ha merg concept literatur regret analysi lead provabl eﬃcient variat jin et wei et optimist agent ensur level eﬃcienc use care chosen step size perturb action valu updat maintain optimist estim thi merg present opportun bridg eﬃcient reinforc learn literatur practic agent design align algorithm studi mathemat literatur build thi line work design new optimist agent suitabl complex environ agent reli sever algorithm innov agent jin et wei et maintain action valu environ state maintain action valu situat state algorithm jin et al design episod environ wei et al oper ﬁxed discount factor depend horizon algorithm design gener environ doe make use discount factor discount factor increas time gener eﬀect behavior increasingli long plan horizon step size use jin et wei et depend horizon agent design oper indeﬁnit rather predetermin horizon use step size depend perform analysi critic contribut thi paper lie perform analysi result present section discuss key implic firstli establish π agent attain averag regret lim sup regretp π π thi exactli four time lower bound also interest relat thi upper bound theorem section indic ϵ exist environ set situat state situat state updat function reward function particular approxim dynam program adp method one might appli whitt gordon tsitsikli van roy muno ari yield polici µ π gener far wors van roy suggest ﬁxed point would yield polici satisﬁ π known whether dong van roy zhou ﬁxed point determin comput tractabl algorithm intrigu agent comput tractabl base method attain averag regret within factor four provid brief discuss thi topic end appendix special case distort π analysi impli follow regretp sa π sat τ π omiss constant factor thi case sinc situat state enabl exact predict optim valu regret grow sublinearli mean agent eventu learn global optim polici depend wors usual scale appear result pertain episod environ jin et al zhang et al formul scale unachiev without addit term regret bound scale exponenti jaksch et al wei et al worth note wei et al consid averag reward object though zero distort provid regret bound scale rather algorithm crucial reli knowledg ﬁxed durat agent analysi also modiﬁ attain scale given ﬁxed durat combin see besid bound onli depend π reward averag time best polici refer class previou regret bound tabular reinforc learn scale number state reward averag time optim polici complex environ quantiti arbitrarili larg inﬁnit interestingli bound ensur agent abl learn eﬃcient spite establish τ regretpτ sa τ sat τ recal pτ class polici reward averag time greater τ regretpτ quantiﬁ regret rel class thi bound oﬀer insight time agent learn perform competit polici larger reward averag time understand thi let us focu special case thi case bound impli ϵ set τ ϵt lim sup regretpτ henc suﬃcient larg agent averag reward approxim best polici reward averag time greater ϵt qualit insight share element common agent agent far simpler motiv wa produc anoth agent rather oﬀer context simpl agent complex environ amen analys inform design futur agent discuss key insight support result first result demonstr possibl agent oper eﬀect within tractabl time frame singl endless stream interact arbitrarili complex environ previou result either reli fact environ mix modest amount time jin et jaksch et zhang et horizon oper ﬁxed known agent wei et previou result focu mdp ha also relat work pomdp et kara yuksel subramanian et result relev onli tractabl number environ state bound depend environ mix time number state among thing result impli agent perform well even environ complex perform optim polici would take forev estim secondli ﬁrst establish algorithm averag regret bound constant multipl distort approach asymptot perform within tractabl time frame exampl van roy impli certain common adp algorithm requir environ dynam known output polici π p within constant multipl inde previou analys adp algorithm instead bound multipl τ notion averag time depend environ complex whitt gordon tsitsikli van roy thi scale τ far wors constant τ becom arbitrarili larg complex environ real environ impract attain zero distort therefor degre impact perform inevit result oﬀer insight avoid scale intrigu aspect agent design eﬀect plan horizon increas time allow agent eventu optim perform arbitrarili long horizon agent eﬀect plan horizon scale thi rate lead regret bound notion plan may beneﬁt restrict eﬀect horizon base quantiti data gather ha also observ jiang et al regret bound depend distort induc ﬁxed situat state updat function howev agent leverag abil neural network adapt thi updat function nachum et schrittwies et result directli address adapt oﬀer insight way inﬂuenc agent perform valu function central theori mdp valu function valu function typic consid function environ state consid instead function histori thi section deﬁn valu function character solut bellman equat dong van roy zhou throughout thi section consid ﬁxed discount factor γ environ e ρ simplifi notat use h denot histori gener concaten action observ histori deﬁn h h transit matrix pa entri ρ h otherwis h similarli polici π deﬁn transit matrix pπ x π h action polici π let ra rπ vector compon given rah x ρ r h rπh x π rah polici π let v γ π h x γt πrπ h qγ π h rah x v γ π function repres expect discount reward start histori h either subsequ action select π onli action execut take supremum polici obtain optim valu v γ h sup v γ π h qγ h sup qγ π h follow proposit follow proposit bertseka teriz v γ qγ uniqu solut among set bound function bellman equat proposit pair v γ qγ uniqu solv system equat v h q h q h rh γ p v among pair bound function v h q h close thi section import lemma tie togeth three concept relat polici π expect averag reward λπ reward averag time τπ discount valu function v γ π lemma close resembl theorem de faria van roy omit proof lemma π h γ v γ π h λπ thi result establish reward averag time bound diﬀer discount valu v γ π h averag reward λπ scale eﬀect horizon simpl agent complex environ agent design perform analysi thi section present studi optimist agent similarli agent demonstr success simul learn predict action valu howev rather neural network represent agent maintain lookup tabl contain one predict per situat pair action select greedili respect predict upon observ agent increment adjust predict assign previou pair base tempor diﬀer agent predict discount valu howev order eventu maxim averag reward associ discount factor increas time approach one idea agent plan ani given time particular eﬀect horizon thi horizon increas agent gather data enabl plan longer horizon greater conﬁdenc discount prelud primari agent introduc simpler one serv didact purpos thi simper agent plan ﬁxed eﬀect horizon τ design oper ﬁxed durat variabl requir input instanti agent particular agent execut algorithm discount q learn eﬀect horizon τ prescrib discount factor γ agent start initi situat state timestep agent increment visit count n comput next situat state f updat predict q via discount iter discount factor γ updat line adjust action valu respons tempor diﬀer two element thi updat warrant discuss one step size α given n thi step size sequenc adapt use jin et al ha number desir properti establish lemma particular properti ensur estim error accumul exponenti agent updat action valu second key element optimist boost ad tempor diﬀer given p n thi term inject optim ensur predict like optimist term domin qγ number visit n pair increas uncertainti around predict decreas thi reﬂect denomin p n let b πτ polici implement agent execut algorithm eﬀect plan horizon τ oper durat let regret rel refer polici π experienc thi agent timestep denot regretτ π eb πτ x e recal τπ h reward averag time polici π max sup φ h qγ h inf φ h qγ h dong van roy zhou algorithm discount q learn input initi situat state f situat state updat function r reward function τ eﬀect plan horizon durat oper γ β p log q unif q execut action regist observ n α n q α r γ q β n q q τ end γ distort introduc predict optim valu eﬀect horizon τ base situat state instead histori follow regret bound theorem τ π regretτ π p sat log h sa log algorithm requir eﬀect plan horizon τ durat oper input establish section regret bound sophist agent doe requir τ input agent relax need paramet oper eﬀect plan horizon increas time grow horizon rather target ﬁxing durat oper eﬀect plan horizon done algorithm discount q learn design agent oper eﬀect ani durat plan grow horizon studi primari agent execut algorithm grow horizon q learn accomplish thi agent instanti onli three input initi situat state situat state updat function reward function worth note agent interact environ singl stream experi reset reiniti situat state updat line look ident simpl agent complex environ algorithm eﬀect horizon τ thu discount factor γ optim coeﬃcient β chang time algorithm grow horizon q learn input initi situat state f situat state updat function r reward function q n τ β q n unif q execut action regist observ n α n γ q α r γ q β n q q τ end algorithm call subroutin govern evolut eﬀect plan horizon τ optim coeﬃcient β suitabl adjust action valu q visit count n tandem chang τ particular prescrib eﬀect plan horizon prescrib optim coeﬃcient increas action valu remain optimist eﬀect plan horizon increas deemphas less recent tempor diﬀer base tialli diﬀer discount factor sequenc chang point begin continu tk k underli function specifi function help deﬁn notat recent chang point time particular index recent chang point given kt max k tk recent chang point tkt ﬁrst function provid eﬀect plan horizon τ kt dong van roy zhou optim coeﬃcient β similarli updat chang point accord kt q log kt note kt optim coeﬃcient scale eﬀect plan horizon rais power time logarithm term ensur action valu remain optimist increment amount eﬀect horizon thi accomplish kt final simplifi analysi reset count chang point multipli tkt count becom one upon next visit ani pair result step size α replac action valu tempor diﬀer eﬀect forc agent forget experi preced chang point import note choic design facilit analysi rather produc eﬀect agent discuss altern choic next section may improv perform denot b π polici execut algorithm subroutin speciﬁ abov recal regretπ regret experienc b π rel refer polici π maximum distort eﬀect horizon equal exceed reward averag time τπ follow theorem main theoret result thi paper theorem π regretπ p sa log log algorithm view oper sequenc episod delin chang point eﬀect plan horizon optim coeﬃcient ﬁxed thought instanti appli episod despit theorem follow directli theorem reason latter appli agent begin empti histori wherea agent instanti chang point doe theorem appendix bridg thi gap oﬀer gener theorem appli agent start arbitrari histori h long situat state initi φ h two corollari theorem facilit interpret implic ﬁrst teriz regret rel refer class pτ consist polici reward averag time exceed sinc regretpτ regretπ follow corollari simpl agent complex environ corollari τ regretpτ p sa log log follow thi corollari ϵ τ polynomi poli agent attain averag reward within ϵ λπ within τ poli timestep henc within time scale τ agent attain averag reward competit ani polici reward averag time also implicit thi observ time agent becom competit polici requir longer time evalu second corollari bound regret rel optim averag reward thi follow theorem next lemma consequ corollari van roy lemma τ π appli lemma take π π arriv follow corollari theorem corollari regretp p sa log π πt log π thi corollari convey anoth intrigu properti result agent approach asymptot perform time scale τ particular thi time doe depend reward averag time optim polici complex environ could intract larg even inﬁnit depend instead reward averag time π determin situat rather environ state dynam schedul scheme function prescrib schedul adjust eﬀect plan horizon optim coeﬃcient valu count function particular choic speciﬁ previou section part algorithm design facilit regret analysi inde irregular structur abrupt adjust occur particular chang point wa introduc sole simplifi analysi partit stream episod natur choic involv smooth schedul may substanti improv realiz perform satisfi similar improv regret bound rate eﬀect horizon grow time play import role rate may oner slow requir veri long time develop plan span reason horizon illustr import schedul let us revisit servic rate control exampl section simul result report section demonstr capabl optimist improv perform time made use particular smooth dong van roy zhou schedul log note may beneﬁci modifi rate plan horizon grow purpos current studi retain rate onli tune aspect schedul figur compar result report section schedul algorithm plot repres averag two hundr simul trajectori latter agent eventu improv perform requir veri long time due impract schedul figur perform algorithm origin schedul versu improv smooth schedul may surpris algorithm perform poorli despit satisfi regret bound previou section inde common mathemat result eﬃcient reinforc learn regret bound tend veri weak typic oﬀer accur predict realiz perform given level inaccuraci oﬀer precis guidanc agent design howev bound analys lead use develop qualit understand insight discuss earlier section close remark present studi simpl agent gener interfac interact singl stream experi result bound regret realiz agent bound bear implic asymptot perform rate agent simpl agent complex environ approach level perform importantli bound depend number environ state mix time one interest insight emerg involv relat agent eﬀect plan horizon durat past experi agent plan eﬀect horizon grow number direct result thi work strengthen extend discuss thi section agent use particularli simpl represent action valu function compris ﬁxed prespeciﬁ situat state updat function lookup tabl situat state agent adapt situat state updat function base agent experi gener situat state typic use neural network instead lookup tabl extens allow much larger situat state space greatli improv perform agent analyz discard previou experi whenev eﬀect plan horizon increas thi impract done onli facilit analysi ought possibl analyz variat gradual phase inﬂuenc past data along line discuss section maxim averag reward may natur agent learn diﬀerenti valu function directli studi wan et al rather discount valu function doe agent howev open issu whether agent explor environ eﬃcient believ thi problem worth investig final suspect term regret bound reﬂect rate agent approach asymptot perform fundament improv better agent design nuanc analysi note thi depend stem subroutin agent use adjust plan horizon mention section set induc eﬀect plan horizon grow suggest take time τ plan eﬀect horizon length conjectur exist environ ani agent requir time τ thi would translat instead term regret lower bound lower bound could shed light limit learn could oﬀer use insight agent design long agent ought plan given durat past experi acknowledg financi support armi research oﬃc aro grant nation scienc foundat grant digit twin research grant bain compani grate acknowledg shi dong wa partli support herb jane dwight stanford graduat fellowship zhengyuan zhou acknowledg gener support new york univers spring center global economi busi faculti research grant thank michael harrison satind singh john tsitsikli xiuyuan lu stimul discuss help feedback also thank alex cloud point mistak previou version thi paper dong van roy zhou appendix probabilist framework thi appendix deﬁn probabilist framework notat deﬁn random quantiti respect probabl space ω f p probabl event f denot p f event f g p g probabl f condit g denot p random variabl function set outcom ωa domain random variabl z p z denot probabl event z lie within set probabl p z event f condit event z z take valu rk ha densiti pz though p z z z condit probabl p z whenev pz z denot p z ﬁxed f thi function denot valu evalu z z p random variabl even p z z p becaus problemat event occur zero probabl possibl realiz z probabl p z z z z function denot valu thi function evalu z p z note p z random variabl becaus depend random variabl z possibl realiz z probabl p z condit z z function z evalu thi function z yield random variabl denot p particular random variabl appear routin throughout paper one environ e ρ determinist set deﬁn interfac observ probabl function ρ random variabl thi random reﬂect agent design epistem uncertainti environ often consid probabl p event f condit environ main result involv upper bound random variabl form e function establish e probabl one suﬃcient argument environ e distribut e ha posit mass e e e appendix c reader encount notat like e e may help think accompani quantiﬁ distribut e e enjoy posit polici π assign probabl π action histori polici π random variabl aπ oπ aπ oπ repres sequenc interact erat select action accord particular hπ aπ oπ oπ denot histori interact time p aπ π aπ p oπ aπ e ρ oπ aπ shorthand gener suppress superscript π instead indic polici subscript exampl pπ p aπ π aπ pπ e p oπ aπ e ρ oπ aπ depend π extend algorithm state zπ situat state sπ epistem state p π use convent suppress superscript appropri simpl agent complex environ figur custom arriv probabl decreas function maximum servic time experienc among recent dozen custom serv express expect use subscript notat probabl exampl expect reward rπ r sπ aπ oπ condit ment e state sπ action aπ written e rπ sπ aπ eπ st much paper studi properti interact speciﬁc polici b clear context suppress superscript subscript indic thi exampl ht hb π ab π ob π p pb π b π appendix servic rate control exampl thi appendix supplement discuss section particular provid precis character environ dynam establish two baselin agent describ section deviat slow mode servic also present third sophist baselin agent establish even doe learn deviat slow mode environ dynam servic station initi vacant custom may arriv start end ﬁrst timestep time arriv probabl depend maximum servic time experienc among recent custom serv denot thi statist wt initi custom arriv probabl decreas wt increas illustr figur particular condit wt probabl custom arriv time servic station vacant pt choic servic mode impact servic time fast mode servic alway complet singl timestep slow mode servic complet dong van roy zhou next timestep probabl observ probabl condit st given departur arriv pt condit st fast departur arriv pt slow departur arriv easi verifi averag reward maxim agent appli fast mode servic everi timestep thi polici minim servic time custom wait precis one timestep consequ thi polici wt converg doe arriv probabl averag reward therefor analysi baselin agent studi perform two baselin agent introduc section well sophist variant recal agent time appli polici πϵ select fast mode probabl ϵ vari time agent begin knowledg servic complet probabl slow fast mode respect throughout discuss let random variabl sampl distribut wt pt respect let cϵ servic complet probabl ani timestep custom serv polici πϵ particular cϵ ϵ averag reward λπϵ given λπϵ proﬁt per custom mean interarriv time ϵ cϵ cϵ eπϵ ﬁrst agent appli onli deviat warrant observ data long durat assum arriv probabl ﬁxed whether decid increas ϵ depend arriv probabl estim polici custom servic time probabl otherwis least sinc wt largest among servic time e e consequ e h e keep thing simpl suppos agent estim thi arriv probabl exactli e agent estim averag reward πϵ ˆ λπϵ proﬁt per custom mean interarriv time cϵ cϵ simpl agent complex environ note diﬀer equat equat due ﬁrst agent use arriv probabl result rather πϵ sinc e ˆ λπϵ strictli decreas ϵ agent doe deviat slow mode second agent addit tri small valu ϵ durat order estim deriv averag reward ϵ thi deriv posit increas show thi deriv neg henc second agent doe deviat slow mode one easili check posit integ w ϵ pπϵ e recal cϵ follow equat λπϵ cϵ cϵ eπϵ g ϵ g ϵ ϵ g ϵ eπϵ e pπϵ w e consequ take deriv respect ϵ evalu ϵ yield dλπϵ dϵ dϵg ϵ g g second agent deviat final even consid third agent similar second agent except addit estim second deriv choos ϵ maxim taylor expans λπϵ around ϵ subject constraint thi agent end alway select slow mode servic becaus even exploit inform suggest stay best thing see thi evalu second deriv λπϵ ϵ yield use polynomi extrapol one would get λπϵ dϵ therebi yield strictli smaller valu ϵ summari ﬁrst baselin agent repres might produc conserv design demand see empir evid justifi fast servic befor ever tri second agent repres approach use polici gradient literatur discuss sutton barto refer therein third agent pursu sophist approach entail estim use second deriv addit gradient per analysi given abov three agent end choos polici henc perform poorli rel optimist agent adapt action valu qt st predict futur return select action dong van roy zhou appendix proof proof lemma lemma restat lemma π h γ v γ π h λπ prove thi lemma recal ﬁxed polici π discount factor γ histori h v γ π h x γt πrπ h simplic let ℓ πrπ h v γ π h x γℓrℓ deﬁnit τπ τπ ℓ x henc v γ π h λπ x x ℓ x x ℓ x x τπ τπ desir result properti learn rate thi subsect gener use lemma jin et properti learn rate let αi k αi k k simpl agent complex environ αℓ learn rate sequenc k k natur pk αi k also follow lemma k k αi k k b k k αi k k pk αi k k c αi k proof proof lemma jin et cover case h posit integ note proof part b also appli h thu left us show part c hold real number h thi end ﬁrst establish posit real number b b x j b j fact show induct posit integ ℓthat b ℓ x j b j b ℓ j b j b b b b b b henc hold suppos hold b x j b j b ℓ x j b j j b j b ℓ j b j j b j b ℓ j b j b b b j b j dong van roy zhou follow induct hypothesi thu also hold conclud induct follow b x j b j lim b x j b j lim b ℓ j b j howev log ℓ j b j ℓ x log b j ℓ x b b j side goe impli lim ℓ j b j thu follow x αi k h h x j h j h h x k j h j h h h h h claim part c regret analysi discount agent thi section focu ourselv discount variant agent section throughout thi section assum discount factor γ ﬁxed also consid hypothet set histori start arbitrari h agent start situat state φ h sinc everi time agent chang discount factor environ histori reset main goal demonstr result hold regardless initi histori long agent start correspond agent state order simplifi notat thi section alway condit environ gener ﬁxed environ e ρ p e e e shorten p e respect let h ﬁxed possibl histori φ h also omit superscript γ valu simpl agent complex environ algorithm discount subroutin input f r γ β initi histori h h q n v q true unif arg q n α execut action regist observ q q α h r γ v β n q n q v q end function v γ qγ v γ π qγ reader keep mind valu function thi section respect discount factor speciﬁc consid algorithm ident algorithm except initi histori arbitrari use initi q let histori trajectori algorithm ht ot let st φ ht rt r ot also let vt valu situat state timestep immedi updat q q α rt γ v st β p n q q v q dong van roy zhou let vt h shorthand vt φ h similarli qt qt h deﬁn note sinc action select greedili qt ht max qt ht vt ht final let p transit oper function g h pair h pg h x g let π polici correspond algorithm recal distort respect eﬀect plan horizon τ deﬁn max sup φ h h inf φ h h avoid clutter γ simpli use repres τ aim show follow result theorem algorithm execut γ β p log ι h ι h initi histori h π x ht π ht p sat log sa max ι regret decomposit thi subsect prove follow lemma decompos side pave way analysi lemma π x ht π ht π x ht ht simpl agent complex environ proof π x ht π ht π x vt ht π ht π x vt ht ht take closer look ﬁrst term side sinc π greedi respect vt π x vt ht π ht π x vt ht π ht π x vt ht ht π x ht π ht π x vt ht ht γ π x π combin π x ht π ht π x ht ht ht π ht divid side consid ht π ht arriv simplic let χk vk hk hk ξk qk hk ak hk ak k use notat written equival π x ht π ht π x dong van roy zhou establish thi subsect show timestep valu function vt almost optimist uniformli across histori follow result lemma algorithm execut γ βδ r log δ ι qγ h ι h probabl least h vt h h qt h h max ι proof moment let us ﬁx h let ˆ qk h ha updat k time ˆ initi valu k let tk timestep h updat note φ htk φ h atk updat rule algorithm n ˆ qn n x αi n γ vti βδ simpl agent complex environ thu n ˆ qn h n x αi n γ vti βδ h n x αi n γ vti βδ n x αi n hti ati n x αi n γ vti hti ati n x αi n βδ n x αi n vti n x αi n hti ati βδ follow φ hti φ h h hti follow fact hti ati γ hti ati consid follow sequenc gk k x αi n hti ati k n k e e αk n htk atk e e αk n htk atk htk atk impli gk k n martingal result follow hoeﬀd inequ probabl least gn n x αi n hti ati r log δ dong van roy zhou use assert b lemma fact hti ati scale δ appli union bound probabl least simultan h n long h updat n time timestep n x αi n hti ati r log δ denot abov event recal choos βδ r log δ result follow condit event e ˆ qn n x αi n show desir result induct assum event e occur requir obvious h h h suppos result hold h long h updat n time timestep h h γ n x αi n γ otherwis h updat timestep h h h thi lead h h min max h h h therefor result hold note one direct implic lemma χt simpl agent complex environ bound thi subsect prove follow lemma lemma algorithm execut γ βδ speciﬁ lemma probabl least x sa r sat log δ ξt χt deﬁn respect proof moment let us ﬁx consid situat pair ht ha updat n time befor includ timestep let tn timestep ht updat recal φ hti φ ht ati condit event e qt ht ht n x αi n γ vti βδ ht n x αi n h γ vti ati n x αi n βδ γ n x αi n h vti ati γ n x αi n h vti γ n x αi n h ati γ n x αi n h vti follow fact φ hti φ ht ht hti dong van roy zhou inequ follow assert lemma follow condit event e n x αi n h ati combin vt ht ht vt ht ht qt ht ht γ n x αi n h vti n φ φ hti vti vti vti otherwis φ φ hti φ updat timestep ti lead vti combin two case claim exist wn vt ht ht qt ht ht γ n x αi n h vwi note onli appli time ha updat least onc suppos ht ﬁrst updat time mean ha updat even onc prior timestep natur vt ht ht recal n number time valu ht updat befor includ timestep let take valu replac n wi nt wt respect reﬂect depend sum side consid x vt ht ht x qt ht ht γ x nt x αi nt h vwt sa x simpl agent complex environ note sa mani ﬁrst want determin whether vt ht ht appear summat side base appear must nt mean situat pair ht ha never visit lead vt ht ht therefor claim x vt ht ht γ x nt x αi nt h vwt sa x wt shown condit event e χk inequ impli x ξt γ x nt x αi nt χwt sa x γ x nt x αi nt χwt sa x examin ﬁrst term side lemma c x nt x αi nt χwt x χt term last term let nt number time situat pair updat time x x x nt x x x p nt sa x x nt sat ﬁnal step use fact x x nt dong van roy zhou revisit start x ξt γ x nt x αi nt χwt sa x γ x χt sa sat equival x x χt sa sat recal condit event e χt γ thu drop ﬁrst term deduc probabl x sa r sat log δ finish proof theorem remain show bound lemma also impli bound expect sum ξt δ let eδ denot event becaus ξt ht ht e x ec eδ δt therefor e x e x e e x ec r sat log δ sa δt let δ β p log simpl agent complex environ e x p sat log sa plug abov inequ arriv π x ht π ht p sat log sa p sat log sa conclud proof theorem discount return averag reward thi section still continu studi discount subroutin algorithm shift focu learn perform respect averag reward goal show follow stronger version theorem theorem τ τ log algorithm execut γ β log ι qγ h h initi histori h π x p sat log h h sa log τ max ι proof first notic π h v γ π ht e x dong van roy zhou thu e x v γ ht γ π ht e x x x e x x e x e x result lemma notic sinc e x x γ let γ γ γt log e therefor e x γ e x γ x γ e x γ x γ e x γ e x γ simpl agent complex environ hand e γ x γ theorem also e x v γ ht γ π ht e x v γ ht γ π ht p sat log τ h sa τ note sinc γ combin τ e x γ p sat log τ h sa τ h log τ τ τ log γ e x p sat log h sa τ h log τ γ p sat log h sa τ log τ claim theorem proof lemma lemma restat lemma τ π fact ﬁxed τ let γ γ long φ φ v theorem van roy ϵ γ exist polici ϵ p distribut µϵ h h µϵ h dong van roy zhou x µϵ h v h ϵ h ϵ take limit notic π h lim v π h λπ arriv lim sup h ϵ h ϵ thi mean ι exist γι γ λπγι h πγι ϵ h ϵ ι sinc πγι ϵ π abov inequ impli ϵ ι λπγι h π h ϵ ι notic follow result blackwel γι λπγι h h henc take ϵ ι give us h π h desir conclud proof theorem thi section complet ﬁnal step toward theorem restat theorem π regretπ p sa log log set subroutin speciﬁ interact agent environ view epoch k epoch correspond execut algorithm ﬁxed ﬁxed discount factor γk k equival ﬁxed eﬀect plan horizon τk k tk chang point tk k previou subsect analys regret algorithm discount factor ﬁxed thi subsect allow discount factor chang use doubl trick bound total regret b throughout thi section assum total number timestep ﬁxed start follow use lemma simpl agent complex environ lemma let b integ ζ f f x consid sequenc tj j let k minimum index tk k f f tk sζ proof first notic tk thi becaus base deﬁnit henc pk tk tk contradict minim sinc pk tk k therefor f f tk k x tζ k ζ sζ desir light requir algorithm also need follow result lemma h h proof let πi h πi uniform distribut arg qγi h proposit πi exist qγi h rh x x πirπi h result h h h h x x x x dong van roy zhou desir abov lemma togeth lemma subroutin deﬁn ensur begin epoch k q h h τk h let tk denot timestep epoch k l l index epoch contain timestep last epoch concern tl let length epoch tk ﬁxed refer polici π let rπ tk e e k let γk discount factor use epoch discount factor epoch theorem either rπ tk k p log h h sa log τk p sa log τπ h log τk rπ tk let ik τk τπ otherwis simpl agent complex environ let g p sa log log arriv regretπ l x rπ tk l x ik g ik l x τk g τk τπ l x π g τ π π l x g τ π lemma l x g τ π p sa log log therefor regretπ p sa log log π justiﬁ claim theorem appendix result approxim dynam program theorem exist environ ρ set situat state situat state updat function f reward function r condit e π proof consid environ two action two observ observ probabl given ρ h otherwis dong van roy zhou ρ word observ determinist onli agent take diﬀer action one took previou timestep observ alway ﬁrst timestep onli one situat state situat state updat function f reward equal observ r notic thi environ attain altern two action timestep let thi environ claim condit e γ sinc onli one situat state onli verifi histori pair action qγ inde histori h action qγ h h γ otherwis sinc qγ h onli take abov two valu obvious hold thu howev onli two polici p one alway take action alway take action either polici result reward sequenc impli π thu π desir next show intuit learn algorithm may gener polici whose perform much wors π thi happen even environ dynam known learn algorithm proce follow iter k algorithm independ sampl k k accord uniform distribut valu function updat via v k min v x v k n rao k γ p k simpl agent complex environ γ ﬁxed discount factor note environ dynam function ρ known algorithm abl comput r p otherwis ha use ﬁtted valu r p thi algorithm version ﬁtted valu iter algorithm loss function studi muno ari worth mention gener ﬁtted valu iter need converg howev sinc updat rule amont ﬁtted valu iter aggreg state sequenc k k alway converg probabl v per muno ari use exampl adapt van roy follow result theorem valu function updat via γ ϵ exist integ environ e condit e e τ π greedi respect v γ proof integ n consid mdp state two action note thi mdp view environ en ρ observ correspond mdp state simpli call state hereaft ρ depend histori onli recent observ observ state induc equival class set histori h h onli last observ henceforth use observ denot equival class h induc also distinguish observ everi timestep probabl system reset next state drawn uniformli condit system doe reset state system transit determinist state state system transit determinist state regardless action state system transit state probabl action state system transit state determinist action stay state determinist action environ dynam ρ thu written ρ otherwis ρ otherwis ρ dong van roy zhou let δ κ everi state agent receiv reward δ take action reward take action everi state agent receiv reward regardless action addit agent also receiv reward take action state scenario agent receiv zero reward situat state space φ whenev φ whenev condit e en ﬁxed γ verifi qγ qγ k also qγ qγ qγ k δ qγ k sinc state map situat state state map situat state δ τ optim averag reward environ en attain appli action everi state consid p choos action situat state action situat state appar addit van roy establish whenev κ exist n π thu sinc thi environ follow τπ γ γ γδ therefor ϵ choos κ allow final would like provid brief discuss depend distort factor van roy shown environ mdp simpl agent complex environ consid solut version approxim valu iter take account invari distribut appropri polici greedi polici respect solut alway attain optim depend distort speciﬁc project approxim valu iter ha incorpor invari distribut greedi polici respect current valu howev approxim valu iter plan algorithm valu pair mdp updat simultan come learn algorithm valu updat depend pair visit timestep learn constitut core agent implicitli carri project reader interest thi connect refer section van roy set lectur slide valu iter aggreg state russo refer abba abdolmaleki jost tobia springenberg yuval tassa remi muno nicola heess martin riedmil maximum posteriori polici optimis intern confer learn represent alekh agarw sham kakad akshay krishnamurthi wen sun flamb tural complex represent learn low rank mdp arxiv preprint mohammad gheshlaghi azar ian osband emi muno minimax regret bound reinforc learn intern confer machin learn page pmlr peter l bartlett ambuj tewari regal regular base algorithm forcement learn weakli commun mdp arxiv preprint dimitri p bertseka abstract dynam program athena scientiﬁc david blackwel discret dynam program annal mathemat statist page mayank daswani peter sunehag marcu hutter forcement learn asian confer machin learn page pmlr mayank daswani peter sunehag marcu hutter et al featur reinforc learn state art sequenti big data paper workshop associ advanc artiﬁci intellig daniela pucci de faria benjamin van roy linear program cost approxim dynam program perform guarante mathemat oper research dong van roy zhou geoﬀrey j gordon stabl function approxim dynam program machin learn proceed page elsevi marcu hutter univers artiﬁci intellig sequenti decis base algorithm probabl springer scienc busi media tommi jaakkola michael jordan satind p singh converg stochast iter dynam program algorithm neural comput mehdi rahul jain ashutosh nayyar onlin learn unknown partial observ mdp arxiv preprint thoma jaksch ronald ortner peter auer regret bound ment learn journal machin learn research nan jiang alex kulesza satind singh richard lewi depend eﬀect plan horizon model accuraci proceed intern confer autonom agent multiag system page cites nan jiang akshay krishnamurthi alekh agarw john langford robert e schapir contextu decis process low bellman rank intern confer machin learn page pmlr chi jin zeyuan sebastien bubeck michael jordan provabl eﬃcient arxiv preprint chi jin zhuoran yang zhaoran wang michael jordan provabl eﬃcient reinforc learn linear function approxim confer learn theori page pmlr kyung jo lagrangian algorithm comput optim servic rate jackson queu network comput oper research ali devran kara serdar yuksel near optim ﬁnite memori feedback polici partial observ markov decis process arxiv preprint michael kearn satind singh reinforc learn polynomi time machin learn lihong li unifi framework comput reinforc learn theori rutger state univers new brunswick lihong li thoma j walsh michael l littman toward uniﬁ theori state abstract mdp isaim xiuyuan lu benjamin van roy vikranth dwaracherla morteza ibrahimi ian osband zheng wen reinforc learn bit bit arxiv preprint r andrew mccallum util distinct reinforc learn hidden state machin learn proceed page elsevi simpl agent complex environ volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc g bellemar alex grave martin riedmil andrea k fidjeland georg ostrovski et al control deep reinforc learn natur ciamac c moallemi sunil kumar benjamin van roy approxim dynam program queue network unpublish manuscript emi muno csaba ari bound ﬁtted valu iter journal machin learn research oﬁr nachum shixiang gu honglak lee sergey levin represent learn hierarch reinforc learn arxiv preprint ian osband daniel russo benjamin van roy eﬃcient reinforc learn via posterior sampl arxiv preprint ian osband benjamin van roy daniel j russo zheng wen et al deep explor via random valu function journal machin learn research yi ouyang mukul gagrani ashutosh nayyar rahul jain learn unknown markov decis process thompson sampl approach arxiv preprint majid raei ali tizghadam alberto reinforc learn approach provid qualiti servic arxiv preprint daniel russo onlin variant valu iter approxim via state gation http pdf lectur slide julian schrittwies ioanni antonogl thoma hubert karen simonyan laurent sifr simon schmitt arthur guez edward lockhart demi hassabi thore graepel et al master atari go chess shogi plan learn model natur linn sennott stochast dynam program control queue system volum john wiley son franci song abba abdolmaleki jost tobia springenberg aidan clark hubert soyer jack rae seb nouri arun ahuja siqi liu dhruva tirumala nicola heess dan belov martin riedmil matthew botvinick maximum posteriori polici optim discret continu control intern confer learn represent url http shaler stidham jr richard r weber monoton insensit optim polici control queue undiscount cost oper research jayakumar subramanian amit sinha raihan seraj aditya mahajan approxim inform state approxim plan reinforc learn partial observ system journal machin learn research dong van roy zhou richard sutton andrew g barto reinforc learn introduct john n tsitsikli asynchron stochast approxim machin learn john n tsitsikli benjamin van roy method larg scale dynam program machin learn benjamin van roy perform loss bound approxim valu iter state aggreg mathemat oper research yi wan abhishek naik richard sutton learn plan markov decis process arxiv preprint ruosong wang russ r salakhutdinov lin yang reinforc learn gener valu function approxim provabl eﬃcient approach via bound elud dimens advanc neural inform process system christoph watkin learn delay reward phd thesi christoph watkin peter dayan machin learn richard r weber shaler stidham optim control servic rate network queue advanc appli probabl page wei mehdi jafarnia jahromi haipeng luo hiteshi sharma rahul jain reinforc learn markov decis process intern confer machin learn page pmlr ward whitt approxim dynam program mathemat oper research andrea zanett alessandro lazar mykel kochenderf emma brunskil learn near optim polici low inher bellman error intern confer machin learn page pmlr zihan zhang xiangyang ji simon du reinforc learn diﬃcult bandit algorithm escap curs horizon arxiv preprint dongruo zhou quanquan gu csaba szepesvari nearli minimax optim reinforc learn linear mixtur markov decis process confer learn theori page pmlr