review confer paper iclr learn navig complex environ piotr razvan fabio viola hubert soyer andrew ballard andrea banino misha denil ross goroshin laurent sifr koray kavukcuoglu dharshan kumaran raia hadsel deepmind london uk piotrmirowski razp fviola soyer aybd abanino mdenil goroshin sifr korayk dkumaran raia abstract learn navig complex environ dynam element tant mileston develop ai agent thi work formul navig question reinforc learn problem show data efﬁcienc task perform dramat improv reli addit auxiliari task leverag multimod sensori input particular consid jointli learn reinforc learn problem auxiliari depth predict loop closur classiﬁc task thi approach learn navig raw sensori input complic maze approach perform even condit goal locat chang frequent provid detail analysi agent abil localis network activ dynam show agent implicitli learn key navig abil introduct abil navig efﬁcient within environ fundament intellig behavior whilst convent robot method simultan localis map slam tackl navig explicit focu posit infer map dissanayak et follow recent work deep reinforc learn mnih et propos navig abil could emerg agent learn polici maxim reward one advantag intrins approach action divorc represent rather learnt togeth thu ensur featur present represent learn navig reinforc learn partial observ environ howev pose sever challeng first reward often spars distribut environ may onli one goal locat second environ often compris dynam element requir agent use memori differ timescal rapid memori goal locat togeth short term memori subserv tempor integr veloc signal visual observ longer term memori constant aspect environ boundari cue improv statist efﬁcienc bootstrap reinforc learn procedur augment loss auxiliari task provid denser train signal support represent learn consid two addit loss ﬁrst one involv reconstruct depth map time step predict one input modal depth channel colour channel thi auxiliari task concern geometri environ aim encourag learn represent aid obstacl avoid trajectori plan second task directli invok loop closur slam agent train predict current locat ha previous visit within local trajectori equal contribut video illustr navig agent avail http jan review confer paper iclr figur view small maze larg maze correspond maze layout sampl agent trajectori maze made public differ textur visual cue well explor reward goal shown right address memori requir task reli stack lstm architectur grave et pascanu et evalu approach use ﬁve maze environ demonstr acceler learn increas perform propos agent architectur environ featur complex geometri random start posit orient dynam goal locat long episod requir thousand agent step see figur also provid detail analysi train agent show critic navig skill acquir thi import neither posit infer map directli part loss therefor raw perform goal ﬁnding task necessarili good indic skill acquir particular show propos agent resolv ambigu observ quickli local complex maze thi local capabl correl higher task reward approach reli learn framework incorpor multipl object firstli tri maxim cumul reward use approach secondli minim auxiliari loss infer depth map rgb observ final agent train detect loop closur addit auxiliari task encourag implicit veloc integr reinforc learn problem address asynchron advantag algorithm mnih et reli learn polici π θ valu function v st θv given state observ polici valu function share intermedi represent comput use separ linear layer topmost layer model agent setup close follow work mnih et refer thi work detail use convolut encod follow either mlp lstm use action repetit entropi regular prevent polici satur detail also found appendix baselin consid thi work agent mnih et receiv onli rgb input environ use either recurr pure model see figur b encod rgb input use consid architectur layer convolut network support navig capabl approach also reli nav agent figur employ stack lstm convolut encod expand observ agent includ veloc action sampl stochast polici immedi reward previou time step opt feed veloc previous select action directli second recurr layer ﬁrst layer onli receiv reward postul ﬁrst layer might abl make associ reward visual observ provid context second layer polici comput thu observ st may includ imag xt w h width review confer paper iclr xt vt enc ᵨ ᬭ xt enc ᵨ ᬭ enc ᵨ ᬭloop l depth ff nav nav xt vt enc ᵨ ᬭ xt lstm depth figur differ architectur convolut encod follow feedforward layer polici π valu function output b ha lstm layer c use addit input veloc reward action well stack lstm ha addit output predict depth loop closur height imag later rotat veloc vt previou action previou reward figur show augment nav differ possibl auxiliari loss particular consid predict depth convolut layer refer thi choic top lstm layer predict loop closur l auxiliari loss comput current frame via singl layer mlp agent train appli weight sum gradient come gradient depth predict multipli gradient loop closur scale βl detail onlin learn algorithm given appendix b depth predict primari input agent form rgb imag howev depth inform cover central ﬁeld view agent might suppli valuabl inform structur environ depth could directli use input argu present addit loss actual valuabl learn process particular predict loss share represent polici could help build use featur rl much faster bootstrap learn sinc know eigen et singl frame enough predict depth know thi auxiliari task learnt comparison depth input versu addit loss given appendix c show signiﬁc gain depth loss sinc role auxiliari loss build represent model necessarili care speciﬁc perform obtain natur predict care data efﬁcienc aspect problem also comput complex loss use main task converg faster compar solv rl problem use less data sampl addit comput cost minim achiev thi use low resolut variant depth map reduc screen resolut explor two differ variant loss ﬁrst choic phrase regress task natur choic thi formul combin higher depth resolut extract inform mean squar error impos unimod distribut van den oord et address thi possibl issu also consid classiﬁc loss depth posit discretis differ band band distribut pay attent object detail appendix b motiv classiﬁc formul greatli reduc resolut depth ﬂexibl learn perspect result faster converg henc faster bootstrap imag crop befor subsampl lessen ﬂoor ceil littl relev depth inform review confer paper iclr loop closur predict loop closur like depth valuabl navig agent sinc use efﬁcient explor spatial reason produc train target detect loop closur base similar local posit inform dure episod obtain integr veloc time speciﬁc trajectori note pt pt posit agent time deﬁn loop closur label lt equal posit pt agent close posit earlier time order avoid trivial loop closur consecut point trajectori add extra condit intermediari posit far pt threshold provid two limit learn predict binari loop label done minim bernoulli loss lt output output hidden represent ht last hidden layer model follow sigmoid activ relat work rich literatur navig primarili robot literatur howev focu relat work deep rl deep dqn breakthrough extrem challeng domain atari mnih et recent work ha develop rl method advantag use asynchron train multipl agent parallel mnih et recurr network also success incorpor enabl state disambigu partial observ environ koutnik et hausknecht stone mnih et narasimhan et deep rl ha recent use navig domain kulkarni et al use feedforward architectur learn deep successor represent enabl behavior ﬂexibl reward chang mazebas gridworld provid mean detect bottleneck vizdoom zhu et al use feedforward siames architectur incorpor pretrain resnet support navig target discretis environ oh et al investig perform varieti network extern memori weston et simpl navig task minecraft block world environ tessler et al also use minecraft domain show beneﬁt combin feedforward network learn resuabl skill modul cf option sutton et transfer navig task tai liu train convnet agent use depth channel input obstacl avoid environ barron et al investig well convnet predict depth channel rgb minecraft environ use depth train agent auxiliari task often use facilit represent learn suddarth kergosien recent incorpor addit object design augment represent learn auxiliari reconstruct decod pathway zhang et rasmu et zhao et mirowski et ha yield beneﬁt larg scale classiﬁc task deep rl set howev onli two previou paper examin beneﬁt auxiliari task speciﬁc li et al consid supervis loss ﬁtting recurr model hidden represent predict next observ state context imit learn sequenc provid expert lampl chaplot show perform dqn agent shooter game vizdoom environ substanti enhanc addit supervis auxiliari task wherebi convolut network wa train task inform presenc enemi weapon provid game engin contrast contribut address fundament question learn intrins sentat space geometri movement simultan maximis reward reinforc learn method valid challeng maze domain random start goal locat experi consid set maze deepmind lab environ beatti et see fig visual rich addit observ avail agent inerti review confer paper iclr static maze small b static maze larg c random goal random goal maze small e random goal maze larg f random goal maze larg differ lation depth predict figur reward achiev agent differ task two static maze small larg ﬁxed goal two static maze compar layout dynam goal result averag top random hyperparamet conﬁgur star label indic use reward clip pleas see text detail inform local depth action space discret yet allow ﬁnegrain control compris action agent rotat small increment acceler forward backward sideway induc rotat acceler move reward achiev environ reach goal random start locat orient goal reach agent respawn new start locat must return goal episod termin ﬁxed amount time expir afford agent enough time ﬁnd goal sever time spars fruit reward serv encourag explor appl worth point strawberri point goal point video agent solv maze link appendix static variant maze goal fruit locat ﬁxed onli agent start locat chang dynam random goal variant goal fruit randomli place everi episod within episod goal appl locat stay ﬁxed episod end thi encourag strategi agent initi explor maze retain goal locat quickli reﬁnd respawn variant static random goal consid small larg map small maze episod last timestep larg maze step see figur rgb observ environ see figur right inspir classic use investig navig rodent olton et layout remain ﬁxed throughout agent spawn central corridor appl reward ha locat goal place alcov one four arm becaus goal hidden alcov optim agent behaviour must reli memori goal locat order return goal use direct rout goal locat constant within episod vari randomli across episod differ agent architectur describ section evalu train ﬁve maze figur show learn curv averag top perform agent agent feedforward model ff recurr model lstm stack lstm version veloc previou action reward input nav nav depth predict convolut layer nav nav depth predict last lstm layer nav nav loop closur predict nav well nav environ use thi paper publicli avail http review confer paper iclr figur left exampl depth predict pair ground truth predict depth sampl everi step right exampl loop closur predict agent start gray squar trajectori plot gray blue dot correspond true posit output loop closur detector red cross correspond fals posit green cross fals neg note fals posit occur agent actual squar away actual loop closur auxiliari loss consid togeth nav case ran experi randomli sampl rang detail pleas see appendix mean top run well top curv plot expert human score establish profession game player compar result nav agent reach perform static attain human score random goal mnih et al reward clip use stabil learn techniqu employ thi work well unfortun particular task thi yield slightli suboptim polici becaus agent doe distinguish appl point goal point remov reward clip result unstabl behaviour base agent see appendix c howev seem auxiliari signal depth predict mediat thi problem extent result stabl learn dynam figur nav vs nav clearli indic whether reward clip use ad asterisk agent name figur also explor differ two formul depth predict regress task classiﬁc task see regress agent nav mse perform wors one doe classiﬁc nav thi result extend map therefor onli use classiﬁc formul also see predict depth last lstm layer henc provid structur recurr layer convolut one perform better note particular result learn curv figur b consid feedforward model red curv versu lstm version pink curv even though navig seem intrins requir memori singl observ could often ambigu forward model achiev competit perform static maze thi suggest might good strategi involv tempor memori give good result name reactiv polici held weight encod learn strategi thi motiv dynam environ encourag use memori gener navig strategi figur also show advantag ad veloc reward action input well impact use two layer lstm orang curv vs red pink though thi agent nav better simpl architectur still rel slow train maze believ thi mainli due slower data inefﬁci learn gener seen pure rl approach support thi see ad auxiliari predict target depth loop closur nav black curv speed learn dramat maze see tabl auc metric ha strongest effect static maze becaus acceler learn also give substanti last perform increas random goal maze although place valu task perform auxiliari loss report result loop closur predict task test episod step within larg maze random goal nav agent demonstr veri success loop detect reach score sampl trajectori seen figur right except nav agent figur use depth regress reward clip doe wors includ becaus analysi base thi agent review confer paper iclr mean top agent highest reward agent maze agent auc score human goal posit acc latenc score ff lstm nav nav nav static ff lstm nav nav static ff lstm nav nav random goal ff lstm nav nav random goal ff lstm nav nav nav tabl comparison four agent architectur ﬁve maze conﬁgur includ random static goal auc area learn curv score human averag best hyperparamet evalu singl best perform agent done analysi test episod goal give number episod goal wa reach one time posit accuraci classiﬁc accuraci posit decod latenc averag time ﬁrst goal acquisit averag time subsequ goal acquisit score mean score test episod analysi posit decod order evalu intern represent locat within agent either hidden unit ht last lstm case ff agent featur ft last layer train posit decod take represent input consist linear classiﬁ multinomi probabl distribut discret maze locat small maze locat larg maze locat ha locat note backpropag gradient posit decod rest network posit decod onli see represent expos model chang exampl posit decod nav agent shown figur initi uncertainti posit improv near perfect posit predict observ acquir agent observ posit entropi spike respawn decreas onc agent acquir certainti locat addit video agent posit decod link appendix complex maze local import purpos reach goal seem posit accuraci ﬁnal score correl shown tabl pure architectur still achiev accuraci static maze static goal suggest encod memor posit weight thi small maze solvabl agent sufﬁcient train time random goal nav achiev best posit decod perform accuraci wherea ff lstm architectur approxim opposit branch maze nearli ident except veri spars visual cue observ onc goal ﬁrst found nav agent capabl directli return correct branch order achiev maxim score howev linear posit decod thi agent onli accur wherea plain lstm agent hypothes symmetri induc symmetr polici need sensit exact posit agent see analysi review confer paper iclr figur trajectori nav agent left nav random goal maze right cours one episod begin episod gray curv map agent explor environ ﬁnd goal unknown locat red box dure subsequ respawn blue path agent consist return goal valu function plot episod rise agent approach goal goal plot vertic red line figur trajectori nav agent random goal maze overlaid posit probabl predict predict decod train lstm hidden activ taken step dure episod initi uncertainti give way accur posit predict agent navig desir properti navig agent random goal task abl ﬁrst ﬁnd goal reliabl return goal via efﬁcient rout subsequ latenc column tabl show nav agent achiev lowest latenc goal onc goal ha discov ﬁrst number show time second ﬁnd goal ﬁrst time second number averag time subsequ ﬁnd figur show clearli agent ﬁnd goal directli return goal rest episod random goal none agent achiev lower latenc initi goal acquisit thi presum due larger challeng environ stack lstm goal analysi figur show show trajectori travers agent four goal locat initi exploratori phase ﬁnd goal agent consist return goal locat visual agent polici appli tsne dimens reduct maaten hinton cell activ step agent four goal locat whilst cluster correspond four goal locat clearli distinct lstm agent main cluster nav agent trajectori diagon opposit arm maze repres similarli given action sequenc opposit arm equival straight turn left twice top left bottom right goal locat thi suggest nav lstm maintain efﬁcient represent rather independ polici critic inform current relev goal provid addit lstm investig differ combin auxiliari task result suggest depth predict polici lstm yield optim result howev sever auxiliari task concurr introduc jaderberg et thu provid comparison reward predict depth predict follow paper implement two addit agent architectur one perform reward predict convnet use replay buffer call nav one combin reward predict convnet depth predict lstm nav tabl suggest reward predict nav improv upon plain stack lstm architectur nav much depth predict polici lstm nav combin reward predict depth predict nav yield compar result depth predict alon nav normalis averag auc valu respect futur work explor auxiliari task review confer paper iclr agent trajectori episod differ goal locat b lstm activ agent c lstm activ nav agent figur lstm cell activ lstm nav agent collect multipl episod reduc dimens use tsne colour repres goal locat lstm nav agent shown navig agent architectur maze nav nav nav nav nav nav static static random goal random goal tabl comparison ﬁve navig agent architectur ﬁve maze conﬁgur random static goal includ agent perform reward predict nav nav reward predict implement follow jaderberg et report auc area learn curv averag best hyperparamet conclus propos deep rl method augment memori auxiliari learn target train agent navig within larg visual rich environ includ frequent chang start goal locat result analysi highlight util auxiliari object name depth predict loop closur provid richer train signal bootstrap learn enhanc data efﬁcienc examin behavior train agent abil localis network activ dynam order analys navig abil approach augment deep rl auxiliari object allow learn may encourag develop gener navig strategi notabl work auxiliari loss relat jaderberg et independ look data efﬁcienc exploit auxiliari loss one differ two work auxiliari loss onlin current frame reli ani form replay also explor loss veri differ natur final focu navig domain understand navig emerg solv rl problem jaderberg et al concern data efﬁcienc ani whilst best perform agent rel success navig abil would stretch larger demand place rapid memori procedur gener maze due limit capac stack lstm thi regard import futur combin visual complex environ architectur make use extern memori grave et weston et olton et enhanc navig abil agent whilst thi work ha focus investig beneﬁt auxiliari task develop abil navig deep reinforc learn would interest futur work compar techniqu approach acknowledg review confer paper iclr would like thank alexand pritzel thoma degri joseph modayil use discuss charl beatti julian schrittwies marcu wainwright stig petersen environ design develop amir sadik sarah york expert human game test refer trevor barron matthew whitehead alan yeung deep reinforc learn world environ deep reinforc learn frontier challeng ijcai charl beatti joel leibo deni teplyashin tom ward marcu wainwright heinrich kãijttler andrew lefrancq simon green victor vald amir sadik julian schrittwies keith anderson sarah york max cant adam cain adrian bolton stephen gaffney helen king demi hassabi shane legg stig petersen deepmind lab arxiv url http mwm gamini dissanayak paul newman steve clark hugh michael csorba solut simultan local map build slam problem ieee transact robot autom david eigen christian puhrsch rob fergu depth map predict singl imag use deep network proc neural inform process system nip alex grave moham abdelrahman geoffrey hinton speech recognit deep recurr neural network proceed intern confer acoust speech signal process icassp alex grave greg wayn malcolm reynold tim harley ivo danihelka agnieszka nska sergio gómez colmenarejo edward grefenstett tiago ramalho john agapi et al hybrid comput use neural network dynam extern memori natur matthew hausknecht peter stone deep recurr partial observ mdp proc conf artiﬁci intellig aaai max jaderberg volodymir mnih wojciech czarnecki tom schaul joel leibo david silver koray kavukcuoglu reinforc learn unsupervis auxiliari task submit int l confer learn represent iclr jan koutnik giusepp cuccu jãijrgen schmidhub faustino gomez evolv neural network reinforc learn proceed annual confer genet evolutionari comput gecco teja kulkarni ardavan saeedi simanta gautam samuel gershman deep successor reinforc learn corr url http guillaum lampl devendra singh chaplot play fp game deep reinforc learn corr url http xiujun li lihong li jianfeng gao xiaodong jianshu chen li deng ji recurr reinforc learn hybrid approach proceed intern confer learn represent iclr url http lauren van der maaten geoffrey hinton visual data use journal machin learn research nov piotr mirowski marc aurelio ranzato yann lecun dynam semant index nip deep learn unsupervis learn workshop volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness et al control deep reinforc learn natur volodymyr mnih lnech badia mehdi mirza alex grave timothi lillicrap tim harley david silver koray kavukcuoglu asynchron method deep reinforc learn proc int l conf machin learn icml review confer paper iclr arun nair praveen srinivasan sam blackwel cagda alcicek rori fearon et al massiv parallel method deep reinforc learn proceed intern confer machin learn deep learn workshop icml karthik narasimhan teja kulkarni regina barzilay languag understand game use deep reinforc learn proc empir method natur languag process emnlp junhyuk oh valliappa chockalingam satind singh honglak lee control memori activ percept action minecraft proc intern confer machin learn icml david olton jame becker gail e handelmann hippocampu space memori behavior brain scienc razvan pascanu caglar gulcehr kyunghyun cho yoshua bengio construct deep recurr neural network arxiv preprint antti rasmu mathia berglund mikko honkala harri valpola tapani raiko learn ladder network advanc neural inform process system nip steven c suddarth yl kergosien hint mean improv network perform learn time neural network pp springer richard sutton doina precup satind singh mdp framework tempor abstract reinforc learn artiﬁci intellig lei tai ming liu toward cognit explor deep reinforc learn mobil robot arxiv url http chen tessler shahar givoni tom zahavi daniel mankowitz shie mannor deep hierarch approach lifelong learn minecraft corr url http tijmen tieleman geoffrey hinton lectur rmsprop divid gradient run averag recent magnitud coursera neural network machin learn volum van den oord kalchbrenn kavukcuoglu pixel recurr neural network jason weston sumit chopra antoin bord memori network arxiv preprint yute zhang kibok lee honglak lee augment supervis neural network pervis object imag classiﬁc proc intern confer machin learn icml junbo zhao michaël mathieu ross goroshin yann lecun stack int l conf learn represent workshop iclr url http yuke zhu roozbeh mottaghi eric kolv joseph lim abhinav gupta li ali farhadi visual navig indoor scene use deep reinforc learn corr url http review confer paper iclr supplementari materi video train navig agent show behaviour nav agent video correspond navig environ small static larg static small random goal larg random goal video show video actual input agent rgb imag valu function time fruit reward goal acquisit layout maze consecut trajectori agent mark differ colour output train posit decod overlay top maze layout b network architectur train onlin algorithm learn introduc class neural agent modular structur train multipl task input come differ modal vision depth past reward past action implement agent architectur simpliﬁ modular natur essenti construct multipl network one per task use share build block optimis network jointli modul use perceiv visual input lstm use learn navig polici share among multipl task modul depth predictor gd loop closur predictor gl navig network output polici valu function train use reinforc learn depth predict loop closur predict network train use learn within thread asynchron train environ agent play episod game environ therefor see observ reward pair st rt take action differ experienc agent parallel thread within thread multipl task navig depth loop closur predict train schedul add gradient share paramet vector arriv within thread use system subordin gradient updat reinforc learn procedur network train detail experi use encod model convolut layer follow fulli connect layer recurr layer predict polici valu function architectur similar one mnih et convolut layer follow ﬁrst convolut layer ha kernel size stride featur map second layer ha kernel size stride featur map fulli connect layer ff architectur figur ha hidden unit output visual featur ft lstm lstm architectur ha hidden unit output lstm hidden activ ht lstm figur fed extra input past reward previou action express vector dimens later rotat veloc vt encod vector concaten vector nav architectur figur ﬁrst lstm hidden second lstm hidden depth predictor modul gd loop closur detect modul gl mlp hidden unit depth mlp follow independ softmax output one per depth pixel loop closur mlp follow softmax output illustr figur architectur nav agent depth taken labyrinth environ valu divid taken power spread valu interv empir decid use follow quantiz ensur uniform nav agent http nav agent static maze http nav agent static maze http nav agent random goal maze http nav agent random goal maze http review confer paper iclr xt vt v ft ht gl ht gd ft gd ft gl ht figur detail architectur nav agent take rgb visual input xt past reward previou action well veloc vt produc polici π valu function v depth predict gd ft ht well loop closur detect gl ht bin across class previou version agent singl depth predict mlp gd regress depth pixel convnet output paramet modul point subset common vector paramet optimis paramet use asynchron version rmsprop tieleman hinton nair et wa recent exampl asynchron parallel gradient updat deep reinforc learn case focu speciﬁc asynchron advantag actor critic reinforc learn procedur mnih et learn follow close paradigm describ mnih et use worker rmsprop algorithm without momentum center varianc gradient comput chunk episod score point train curv averag episod model get ﬁnish environ step whole experi run maximum environ step agent ha action repeat mnih et mean consecut step agent use action pick begin seri thi reason paper actual report result term agent perceiv step rather environ step maxim number agent perceiv step ani particular run grid sampl categor distribut learn rate wa sampl strength entropi regular reward scale clip new set experi previou set experi reward scale factor clip prior advantag algorithm gradient comput chunk step episod previou set experi use chunk step auxiliari task use hyperparamet sampl coefﬁcient βd depth predict loss convnet featur ld sampl coefﬁcient depth predict loss lstm hidden sampl coefﬁcient βl loop closur predict loss sampl loop closur use follow threshold maximum distanc posit similar squar minimum distanc remov trivial squar review confer paper iclr random goal maze small comparison reward clip b random goal maze small comparison depth predict figur result averag top random hyperparamet conﬁgur star label indic use reward clip pleas see text detail c addit result reward clip figur show addit learn curv particular left plot show baselin ff lstm well nav agent without auxiliari loss perform wors without reward clip reward clip seem remov reward clip make learn unstabl absenc auxiliari task thi particular reason chose show baselin reward clip main result depth predict regress classif task right subplot figur compar depth input versu target note use rgbd input nav agent perform even wors predict depth regress task gener wors predict depth classiﬁc task task maze environ evalu behaviour agent introduc thi paper well agent reward predict introduc jaderberg et nav combin reward predict convnet depth predict polici lstm nav differ maze environ speciﬁc task ﬁrst environ arena appl yield point lemon yield point dispos arena agent need pick appl befor respawn episod last second second environ stairway melon thin squar corridor one direct lemon follow stairway melon point reset level direct appl dead end melon visibl reachabl agent spawn lemon appl random orient environ releas deepmind lab beatti et environ requir navig skill shortest path plan simpl reward identiﬁc lemon appl melon persist explor figur show major differ auxiliari task relat depth predict reward predict depth predict boost perform agent beyond stack lstm architectur hint gener applic depth predict beyond navig task sensit toward sampl experi thi paper replica run hyperparamet learn rate entropi cost sampl interv figur show nav architectur review confer paper iclr learn curv b stairway melon learn curv c layout stairway melon layout figur comparison agent architectur maze conﬁgur arena stairway melon describ detail beatti et imag credit c jaderberg et static maze small b random goal maze larg c random goal figur plot area curv auc reward achiev agent across differ experi differ task larg static maze ﬁxed goal larg static maze compar layout dynam goal reward auc valu comput replica replica run per experi reward auc valu sort decreas valu auxiliari task achiev higher result compar larger number replica hint fact auxiliari task make learn robust choic hyperparamet asymptot perform agent final compar asymptot perform agent term navig ﬁnal reward obtain end episod term represent polici lstm rather visualis convolut ﬁlter quantifi chang represent review confer paper iclr agent architectur frame perform lstm nav score mean top posit acc score mean top posit acc tabl asymptot perform analysi two agent random goal maze compar train labyrinth frame frame without auxiliari task term posit decod follow approach explain section speciﬁc compar baselin agent lstm navig agent one auxiliari task depth predict get twice mani gradient updat number frame seen environ onc rl task onc auxiliari depth predict task tabl show perform baselin agent well posit decod accuraci signiﬁcantli increas twice number train step go point point reach perform posit decod accuraci nav agent half number train frame thi reason believ auxiliari task simpli acceler train