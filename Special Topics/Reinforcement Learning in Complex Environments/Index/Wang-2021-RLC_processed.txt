expert system application 180 2021 115128 available online 29 april 2021 2021 elsevier right reserved multirobot coordination deep reinforcement learning complex environment di wang hongbin deng school mechatronical engineering beijing institute technology beijing 10081 china r c l e n f keywords multirobot coordination reinforcement learning deep learning visual perception b r c multiple autonomous robot system important complete path planning coordinately effectively process interference avoidance resource allocation information sharing traditional multirobot coordination algorithm solution known environment target position robot need move robot priority set limit autonomy robot only using visual information solve problem multirobot coordination still le paper proposes cooperative algorithm based deep reinforcement learning make robot autonomous process selecting target position moving use approach using only top view top view view image information collected perspective robot input proposed algorithm includes dueling neural network structure solve task allocation path planning call algorithm tfdueling perception derstanding environment robot reach target position without collision robot move any target position compare proposed algorithm tfdueling different input structure algorithm tdueling fdueling different neural network structure tfdqn tfddqn experiment show proposed tfdueling algorithm ha highest accuracy robustness introduction field robotics mature enough perform diverse plex task recent year multirobots become popular some application surveying monitoring search rescue application robot need establish coordination interaction achieve individual group goal key problem make individual decision whole system achieve greatest reward problem often called coordination farinelli zanotto pagello 2017 however multirobot coordination cause problem robot complete path planning coordinately effectively process interference avoidance resource allocation information sharing reliable coordination method important part multiple autonomous robot system present research multirobot coordination algorithm ha achieved positive result including centralized method azarm schmidt 1996 decoupling method cirillo uras koenig 2014 reinforcement learning method wang silva 2008 multirobot coordination autonomous mobile robot need identify team member consider path robot path planning robot handle complex task easily several autonomous robot multiple coordinated robot accomplish given task faster efficiently single robot spaan gonalves sequeira 2010 proposed multirobot nation algorithm based auction algorithm markov sion model auctioning provides flexible method assigning task robot combine observable markov decision model framework calculate execution strategy agent task bidding value obtained directly corresponding value function however auction algorithm solves problem task assignment beginning doe not make different decision based state multiple robot azarm schmidt 1996 proposed method dynamically assigning robot priority negotiation process solve coordination problem two robot space method separate assignment task path plan set priority robot work precise grid world limiting autonomy robot corresponding author address denghongbin deng content list available sciencedirect expert system application journal homepage received 26 february 2020 received revised form 29 january 2021 accepted 24 april 2021 expert system application 180 2021 115128 2 many method multirobot path planning finding optimal path problem adler halperin solovey 2015 demonstrated moving disk simple polygon problem disk allowed move any target position long target position eventually occupied known problem difficult move several object starting configuration target configuration find coordinated trajectory contrast general path planning problem requires robot move fixed target position simulated environment allows multiple robot move random initial position any target position distributed artificial intelligence dai subfield artificial telligence dai focus system consisting multiple independent entity interact domain traditionally dai divided two subdisciplines distributed problem solution dp focus information management system multiple component working towards common goal multiagent system deal behavior several independent entity agent stone veloso 2000 simulation environment designed multirobot system proposed algorithm different heuristic cap 2015 leader vrohidis vlantis bechlioulis kyriakopoulos 2018 method equivalent solving task assignment path planning among multiple leader robot priority fully achieve autonomy vrohidis et al 2018 proposed distributed multirobot system multirobot path planning problem solved workspace reinforcement learning used solve problem le no communication maze problem uwano tatebe tajima 2018 eaqr zhang wang 2018 reinforcement learning algorithm solve distributed sensor network problem mrcdrl wang deng pan 2020 solves problem multirobot coordination relies vision scene complex scene relatively multirobot coordination algorithm rely vision highly dynamic environment multirobot coordination need solve problem effective environmental perception task ment path planning environment robot often doe not know target position need determine target position perceiving understanding scene highly dynamic environment include dynamic static obstacle complex background tive path planning without collision also important robot earns different reward different state final moving path not determined beginning robot need complete task assignment path planning problem according state simulation environment designed top view contain multiple controllable agent isomorphic agent center image preprocessed top view image captured agent perspective used input neural network therefore deep neural network designed multiinput structure training process necessary consider action robot process coordination achieve greater reward paper propose multirobot coordination algorithm based deep ment learning dynamic environment solve problem task assignment path planning effectiveness algorithm verified simulation contribution rized follows 1 dynamic environment starting tabula rasa only two part image data used input top view robot centric top view view image information collected perspective robot doe not need know distance robot surrounding obstacle distance robot target position no image annotation required training corresponding action current state robot obtained 2 method based deep reinforcement learning proposed solve multirobot task assignment path planning problem process training not necessary specify target position robot go repeated experiment reward weight automatically updated finally multiple robot reach target position without collision multiple robot not reach target position rest paper organized follows section 2 provides review related work literature section 3 contains description experimental enviroment approach described section section 5 contains neural network structure neural network training neural network testing finally conclusion summarized section 6 related work challenging design appropriate coordination strategy multirobot environment robot operate effectively complex environment yan jouandeau cherif 2013 many study centralized control method still used considers single robot part coupling system us single central planning unit calculate path algorithm used obtained single robot version including method spaan et 2010 artificial potential field method schultz parker schneider 2003 nazarahari khanmirza doostie 2019 spatial decomposition method koo et 2012 approach make use principle market economy enable robot coordinate essence trade task resource robot maximize wealth improving overall effectiveness kalra zlot dia stentz 2005 murdoch gerkey mataric 2002 task allocation system based principled publishing subscribing communication model artificial potential field method map every point space real value using derivative function simple case robot must simply follow gradient reach target position yamashita et al 2003 proposed path planning method multimobile robot porting large object based artificial potential field method algorithm known environment problem multirobot cooperation starting area target area wa solved spatial decomposition method divide space stacles small area us group adjacent area represent path multiple robot try enter area area subdivided assigned different robot koo et al 2012 proposed multirobot coordination framework based regular tessellation framework regular tessellation used divide space interest union disjoint region equal cell occupied only robot obstacle robot modeled hybrid automaton capturing limited mode operation maintain within current unit reach adjacent unit sponding cell task survey supervision search rescue robot often work complex environment making accurate division space impossible although existing method effectively calculate trajectory single robot no effective method dinating multirobot movement avoiding deadlock multirobot path planning problem decoupled subproblems avoid deadlock cirillo et al 2014 van snoeyink lin manocha 2009 although research multirobot system ha made substantial progress multirobot coordination still difficult particular dealing spatial task distribution exponential explosion due wang deng expert system application 180 2021 115128 3 connected action state centralized control multiple robot come impossible some application heuristic method also better solution gorithms priority planning algorithm solving multirobot coordination known environment cap 2015 proposed modified priority planning algorithm adjusted executed asynchronous decentralized manner robot search set globally coordinated trajectory starting position target position exchanging message using heuristic estimation algorithm mean bidding price not always exactly correct accurate bidding result not giving greatest reward effective robot kalra et 2005 also several different coordination approach including graph theory method hernndez barrientos cerro 2014 reactive behavioral method mataric 1993 swarm method alers tuyls claes wei 2014 leader method mourikis roumeliotis 2006 focus reactive behavior avoid collision using highly reactive behavior simplify planning avoid conflict shortly conflict occur swarm method doe not consider every robot reach target position robot swarm exhibit some behavior alers et al 2014 proposed collaborative algorithm based foraging behavior bee ant solve problem maximizing environmental coverage multirobot system leader approach one leader followed robot some form many uncertain resource unstructured dynamic environment difficult design controller controller need guarantee performance local perception li chen tee li 2015 studied coordination control multirobots based reinforcement learning solve problem path planning process moving object wang silva 2008 proposed learning algorithm solve conflict caused multirobot orative transportation task dynamic environment simulation environment 950 700 pixel wa established training process accurate location information obstacle wa used state input path planning reinforcement learning sutton barto 2018 adaptive flexible method automatically update weight repeated experiment corresponding reward obtain sponding action current state however increase system complexity learning cost reinforcement learning increase exponentially deep neural network lecun bengio hinton 2015 applied visual path prediction huang et 2016 deep understanding image learned training combination deep neural network reinforcement learning strategy successfully obtained perceptual input mnih kavukcuoglu silver 2015 used solve exponential growth learning cost reinforcement learning deep reinforcement learning li 2017 performs better human single agent video game atari mnih et 2015 guo singh lee lewis wang 2014 virtual environment mnih et 2016 jaderberg et 2016 deep reinforcement learning ha widely used field game strategy path planning single robot levine finn darrell abbeel 2016 proposed training method based depth reinforcement learning trained ceptron controller learn robot motor torque strategy original image monocular output image mirowski pascanu viola 2016 proposed using deep reinforcement learning solve igation problem complex environment mnih et al 2015 proposed deep q network dqn us only image score input learns strategy successfully single agent scenario dimensional perceptual input double dqn ddqn hasselt guez silver 2016 based dqn algorithm not only reduces overestimation problem also achieves higher efficiency some game wang et al 2015 proposed new neural network structure dueling network chitectures dueling based dqn network structure expressed two part one estimated state value function advantage function dueling structure also improves formance dqn apply dueling neural network structure multirobot coordination experimental environment verify effectiveness proposed multirobot coordination algorithm complex environment use unity juliani et 2018 design experimental environment shown fig 1 experimental environment includes robot static cles ground stain target location static obstacle include rock heap different shape surrounding rock wall different plant robot represented black vehicle target position resented white cube number robot target position fig 1 b c e image collected fig experimental environment experimental environment contains dynamic static obstacle ground stain target position b c e image collected perspective robot wang deng expert system application 180 2021 115128 4 person perspective robot experimental environment algorithm connected juliani et 2018 easily obtain state reward experimental ment action given algorithm applied experimental environment proposed algorithm us deep reinforcement learning solve problem multiple robot reaching target position without collision training process only current status sponding reward robot used input no human ance required robot moving path target position robot performs current action set different reward rh depending whether robot collides 1 robot doe not collide doe not reach target tion set reward rh 2 robot collides static obstacle set reward rh 3 robot collides robot set reward rh 4 robot reach target position different reward rh 1 rp set according distance h center robot target position shown fig 2 rp rv half target position robot length respectively 5 reward rh robot robot considered reached target position larger reward rh rh 120 set multirobot coordination algorithm section introduce multirobot coordination algorithm based deep reinforcement learning complex environment fig distance robot target position robot contact target position distance h center calculated maximum distance set rp rv regardless longest distance formed oblique diagonal robot target position rp rv half target location robot length respectively fig overview multirobot coordination algorithm top view view robot time step preprocessed combined form empirical data set neural network us training sample randomly selected empirical dataset obtain ing action fig preprocessing top view rectangle centered robot segmented rectangle width w slightly larger top view simulation environment wang deng expert system application 180 2021 115128 5 includes data preprocessing experience replay training tfdueling neural network shown fig 3 data preprocessing simulated environment two piece image information returned top view simulation environment image f collected perspective robot need preprocess top view ensure different robot correspond different state taking robot side example fig 4 taking robot center top view segmented square size process segmentation position beyond top view filled gray robot processed way state st robot corresponding top view obtained shown fig image f directly used state sf corresponding perspective robot using continuous state algorithm input robot better understand current scene mnih et 2015 algorithm use four continuous frame input frame consists st sf reduce quantity data input neural network perform grayscale processing shown top view view fig current input state algorithm sj sj sj sj sj sj f sj f sj f sj f j number robot time step j 1 n n number robot deep q network dqn main advantage dqn obtain possible action calculating q value according input state network mnih et 2015 process training use experience replay mnih et 2015 improve stability weight updating neural network experience ej sj aj rj sj end generated time step added data set ej 1 ej aj action corresponding state robot time step end 0 1 indicates whether current state last current episode episode number collision robot 5 considered end neural network us size 32 training sample randomly selected empirical data target value yq online defined yq rj γmax aj q sj aj θi 1 γ 0 1 discount factor use discount factor γ sj state corresponding time step aj action corresponding state sj θi weight ith time step neural network rj rh p 1 rt p composed reward introduced simulation environment chapter generated distance robot target location reward rt rt 1 generated time step episode tmax 200 average time step robot random initial position target position episode target network improve stability learning updating process mnih et 2015 parameter target network copied online network rameters every time step target value ydqn dqn defined ydqn rj γmax aj q sj aj 2 double dqn ddqn standard dqn value used select evaluate action value lead overoptimistic value estimate hasselt et 2016 ddqn us online network target network reduce overestimation target value yddqn ddqn defined yddqn rj γq sj argmaxaj sj aj θi 3 argmaxaj sj aj θi use parameter θi online network obtain action corresponding state sj stream network tfdueling based dqn dueling neural network wang et 2015 timizes fully connected layer neural network structure two part state value function action fig tfdueling neural network structure top view stream us convolutional neural network extract feature top view view stream us convolution network extract feature view dueling stream combine two feature output corresponding action wang deng expert system application 180 2021 115128 6 advantage function atari domain dueling neural network obtain better strategy estimate represents many action state value advantage value use convolution feature merged 4 dueling stream fig 5 q sj aj θi αi βi v sj θi βi sj aj θi αi sj aj θi αi 4 v sj θi βi value function βi parameter value function sj aj θi αi advantage function αi parameter advantage function sj aj θi αi mean advantage function training process tfdueling neural network shown tfdueling algorithm consists three part namely top view stream view stream dueling stream shown fig combining ddqn dueling network structure dueling stream target value ytfdueling j tfdueling defined ytfdueling rj γq sj argmaxaj sj aj θi αi βi 5 parameter target network advantage function value function respectively reward function often deceptive only reward mized no mechanism encourage intelligent detection function fall local optimum cause agent unable learn correctly proposed algorithm agent explores new area environment tfdueling free directly us sample input simulator solve reinforcement learning task learning greedy strategy also tfdueling network training initialize replay memory initialize time step initialize online network random weight θ α β initialize target network weight θ α β episode 1 initialize state sequence sj 1 sj sj sj sj sj sj sj sj not dead probability ε select random action aj otherwise select aj argmaxaj q sj aj θi αi βi execute action aj observe reward rj next state sj save experience ej sj aj rj sj end sample random ej k sj k aj k rj k sj end k subscript sample randomly selected set ytfdueling k rj k end 1 rj k γq sj argmaxaj q sj aj θi αi βi end 0 perform gradient descent step ytfdueling k sj k aj k θi αi βi 2 respect network parameter θi αi βi every c time step copy θi αi βi end end experiment validate proposed multirobot coordination algorithm tfdueling experiment simulation platform us ubuntu nvidia gtx 1080 ti graphic card chapter compare number different robot structure different neural network illustrate effectiveness proposed algorithm fig score number success training process different number robot score number success two robot b e result three robot c f result four robot wang deng expert system application 180 2021 115128 7 neural network structure structure neural network used proposed algorithm shown fig input layer neural network processed image top view view size 64 64 first hidden layer top view stream volutional layer convolves 64 filter 5 5 stride 2 activation function rectifier nonlinearity nair hinton 2010 pooling layer size 2 2 added first hidden layer not shown picture second hidden layer convolutional layer 128 3 3 filter stride 2 applies rectifier next two convolutional layer convolve 256 128 filter filter size 3 3 stride 2 rectifier applied first hidden layer view stream convolutional layer convolves 64 filter 5 5 stride 2 applies rectifier followed pooling layer size 2 2 also not shown picture next two convolutional layer convolve 128 64 filter filter size 3 3 stride 2 rectifier applied first layer dueling stream composed feature top view stream view stream output second layer fully connected layer 512 unit last hidden layer fully connected value stream one output advantage size output layer size output layer 3 consisting forward left right neural network training training process robot move random initial position without human interference satisfy number stored experience experience replay wa larger batch diversity training wa started number periences stored experience replay wa greater 50 thousand experience replay storage space wa 400 thousand policy ε wa reduced first 800 thousand time step remained used three different neural network structure tfdueling tfdqn stream network tfddqn stream network train simulation environment different number robot experimental result shown fig fig 6 b c training score two three four robot different neural network structure given respectively line light color score episode average score adjacent one thousand episode represented dark line score tends stabilize fig 6 e f show number success two three four robot different neural network structure respectively according fig 6 e f late maximum max average avg variance var one thousand episode near max shown table case two robot max avg var number success three neural network algorithm similar case three robot tfdueling algorithm better two algorithm case four robot only var tfdueling algorithm slightly larger tfddqn algorithm max avg better two algorithm validation using top view view input effective using top view view alone illustrated fig tdueling us only top view stream dueling stream fdueling us only view stream dueling stream case three robot use tfdueling tdueling fdueling neural network structure training number ce different neural network structure shown fig using top view view input number success eling much higher tdueling fdueling neural network structure show tfdueling effectively solve multirobot coordination problem complex environment algorithm proposed different single agent algorithm ha proposed use top view get global perception scene view get local perception scene process training agent get reward together table 1 statistic neural network training type tfdqn tfddqn tfdueling max 98 100 98 avg var max 86 88 94 avg var max 74 77 81 avg var fig number success generated neural network different input structure training part neural network structure tdueling fdueling hyperparameters tfdueling fig number success generated different neural network structure testing process case four robot successful episode first episode counted wang deng expert system application 180 2021 115128 8 neural network testing process testing adjust policy ε reduce impact exploration using neural network shown fig 6 f set initial 100 position randomly count number success end episode cumulatively shown fig proposed tfdueling algorithm also ha best performance test process case four robot effect diagram multirobot ation tfdueling algorithm shown fig ball different color represent trajectory robot fig 9 b show robot learn reach target position according distance c e show multiple robot reach target position avoiding near target position f g show multiple robot directly reach target position h show robot encounter protuberance rock course detour lead failure experimental condition designed strict collision occurs cause failure only image data used perceive understand scene robot arrive target position random initial position without collision effectively complete path planning multirobots doe not need specify target position robot need go completes task assignment multirobots autonomously conclusion complex environment propose robot coordination algorithm based deep reinforcement learning only top view top view person view image collected perspective robot used input solve task allocation path planning problem multiautonomous robot robot start random initial position training time step crease multirobot learns multiple robot beginning occupy target location mutually finally predict target location robot ha reach beginning training robot run randomly environment increase training time robot begin move target position time multiple robot move target position finally robot predict target position go without collision compare proposed algorithm tfdueling different neural network structure tdueling fdueling tfdqn tfddqn ments show tfdueling algorithm effectively robustly solve problem multirobot coordination complex dimensional environment future research improve curacy algorithm following two aspect crease training time step state stored experience database close target location accuracy improvement slow accuracy improves balancing state experience database however due limitation visual perception perception range impossible accurately mine distance local obstacle state improve racy improving local perception robot currently not consider improving accuracy algorithm considerably increasing training time credit authorship contribution statement di wang conceptualization methodology software validation formal analysis investigation data curation writing original draft writing review editing visualization hongbin deng resource supervision project administration funding acquisition declaration competing interest author declare no known competing financial interest personal relationship could appeared influence work reported paper acknowledgement would like thank anonymous reviewer zhenhua pan whose insightful comment greatly improved quality paper work described paper wa supported part national natural science foundation china grant no 5177041109 reference adler halperin solovey 2015 efficient motion planning unlabeled disc simple polygon algorithmic foundation robotics xi pp fig result multirobot collaboration based tfdueling algorithm ball different color represent trajectory robot wang deng expert system application 180 2021 115128 9 alers tuyls claes wei 2014 robot coordination foraging coverage artificial life conference proceeding 14 pp azarm schmidt 1996 decentralized approach motion multiple mobile robot advanced robotics 11 cap 2015 algorithm trajectory planning infrastructure association advancement artificial intelligence pp cirillo uras koenig 2014 approach motion planning vehicle intelligent robot system iros pp van snoeyink lin manocha 2009 centralized path planning multiple robot optimal decoupling sequential plan robotics science system 2 farinelli zanotto pagello 2017 advanced approach coordination logistic scenario robotics autonomous system gerkey mataric j 2002 sold auction method multirobot coordination ieee transaction robotics automation 18 guo singh lee lewis wang x 2014 deep learning atari game play using offline tree search planning international conference neural information processing system pp hasselt guez silver 2016 deep reinforcement learning double learning aaai hernndez barrientos cerro 2014 selective smooth fictitious play approach based game theory patrolling infrastructure system expert system application 41 huang li wu liu tang zhuang 2016 deep learning driven visual path prediction single image ieee transaction image processing 25 jaderberg mnih czarnecki schaul leibo silver kavukcuoglu 2016 reinforcement learning unsupervised auxiliary task arxiv preprint juliani berges vckay gao henry mattar lange 2018 unity general platform intelligent agent arxiv preprint kalra zlot dia stentz 2005 multirobot coordination comprehensive survey analysis univ pittsburgh pa robotics inst koo li quottrup clifton bak 2012 framework motion planning temporal logic specification pp science china information science lecun bengio hinton 2015 deep learning nature 521 levine finn darrell abbeel 2016 training deep visuomotor policy journal machine learning research 17 li 2017 deep reinforcement learning overview arxiv preprint arxiv li chen tee li q 2015 reinforcement learning control coordinated manipulation neurocomputing 170 mataric j 1993 designing emergent behavior local interaction collective intelligence proceeding second international conference simulation adaptive behavior pp mirowski pascanu viola et al 2016 learning navigate complex environment arxiv preprint mnih badia mirza graf lillicrap harley silver kavukcuoglu 2016 asynchronous method deep reinforcement learning international conference machine learning pp mnih kavukcuoglu silver et al 2015 control deep reinforcement learning nature mourikis roumeliotis 2006 optimal sensor scheduling constrained localization mobile robot formation ieee transaction robotics nair hinton 2010 rectified linear unit improve restricted boltzmann machine proc int conf mach pp nazarahari khanmirza doostie 2019 path planning continuous environment using enhanced genetic algorithm expert system application pp schultz parker schneider 2003 system swarm intelligent automaton 2 springer spaan gonalves sequeira j 2010 multirobot coordination auctioning pomdps robotics automation icra pp stone veloso 2000 multiagent system survey machine learning perspective autonomous robot 8 sutton barto 2018 reinforcement learning introduction mit press uwano tatebe tajima et al 2018 cooperation based reinforcement learning internal reward maze problem sice journal control measurement system integration 11 vrohidis vlantis bechlioulis kyriakopoulos j 2018 reconfigurable coordination guaranteed convergence obstacle cluttered environment local communication autonomous robot 42 wang deng pan z 2020 mrcdrl coordination deep reinforcement learning neurocomputing 406 wang silva 2008 approach coordination engineering application artificial intelligence 21 wang schaul hessel hasselt lanctot freitas 2015 dueling network architecture deep reinforcement learning arxiv preprint arxiv yamashita arai asama 2003 motion planning multiple mobile robot cooperative manipulation transportation ieee transaction robotics automation 19 yan jouandeau cherif 2013 survey analysis coordination international journal advanced robotic system 10 zhang wang 2018 eaqr multiagent algorithm coordination multiple agent complexity di wang received degree computer science technology lin yi university linyi china 2014 degree beijing union university beijing china 2017 currently working toward degree beijing institute technology current research interest include machine learning robotics hongbin deng received degree beijing institute technology beijing china 1997 2000 2008 respectively associate professor beijing institute technology current research interest include robotics control wang deng