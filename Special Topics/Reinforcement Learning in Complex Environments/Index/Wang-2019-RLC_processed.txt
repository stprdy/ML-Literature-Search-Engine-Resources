2124 ieee transaction vehicular technology vol 68 no 3 march 2019 autonomous navigation uavs complex environment deep reinforcement learning approach chao wang student member ieee jian wang senior member ieee yuan shen member ieee xudong zhang member ieee paper propose deep reinforcement ing drl method allows unmanned aerial vehicle uavs execute navigation task complex ronments technique important many application good delivery remote surveillance problem lated partially observable markov decision process pomdp solved novel online drl algorithm designed based two strictly proved policy gradient theorem within framework contrast conventional simultaneous localization sensing approach method directly map uavs raw sensory measurement control signal navigation experiment result demonstrate method enable uavs autonomously perform navigation virtual complex environment alized complex ronments besides proposed online drl algorithm addressing pomdps outperforms index navigation deep reinforcement learning partially observable markov decision process introduction ver past year seen unprecedented growth unmanned aerial vehicle uavs applied various area aerial photography rescuing crop tection long term goal uav application build ligent system implement various task without human intervention work seek develop technique allows uavs autonomously navigate arbitrary ture place destination complex environment urban area crowded skyscraper technique important many application good delivery gency aid remote surveillance creating safer smarter city 1 multitude method ranging proposed address uav tion problem common approach manuscript received may 13 2018 revised september 22 2018 ber 24 2018 accepted december 28 date publication january 3 2019 date current version march 14 work wa supported national key research development program china grant review paper wa coordinated terjee corresponding author jian wang author department electronic engineering inghua university beijing 100084 china cn zhangxd digital object identiﬁer sensing avoidance avoids collision steering vehicle opposite direction performs navigation path planning 2 9 peng 6 instance us optical ﬂow technique detect obstacle rapidly exploring dom tree algorithm implement path planning jason 2 bine reactive algorithm algorithm implement navigation another class based method make use simultaneously localization mapping slam intuition structing map environment using slam navigation achieved path planning 10 16 cui 10 combine grahpslam 14 online path planning module ing approach enabling uavs ﬁnd trajectory foliage environment apart two class method navigation also done retracing ries 17 22 method category generally fall cisely localizing uavs using various position technique kalman ﬁltering variation 17 navigation achieved minimizing error estimated position position common feature based method require explicit path planning may lead unexpected failure environment highly dynamic complex respect some work resort machine learning approach imitation learning inforcement learning rl 23 25 nursultan 25 ple develops rl algorithm named texplore high level control method uav navigation grid map no barrier stephane 23 build imitation based controller using small set human demonstration achieves good performance natural forest environment new challenge arise uavs perform navigation task complex environment 1 environment scale case method lose efﬁciency since infeasible build map environment 2 environment complex crowded dense obstacle sensing approach generally designed address navigation problem environment sparse obstacle may lose efﬁciency applied complex environment 3 environment often dynamic evident planning incapable handling situation rl adapts dynamic environment need improvement order cope complex environment 4 sensor limited sensing capacity uav navigation fully depends 2019 ieee personal use permitted requires ieee permission see information authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply wang et al autonomous navigation uavs complex environment 2125 observation returned sensor sensor limited sensing capacity uavs might confused situation encountering overcome challenge uavs need resort entire history observation help distinguish tered situation decide action take respect decision making process ﬁts framework partially observable markov decision process pomdp terizes process agent some hidden state obtains observation state take action transit another den state obtains some reward therefore paper model navigation problem pomdp design deep reinforcement learning drl algorithm settle 26 pomdp structuring uav navigation problem must meet several requirement firstly since infeasible know priori state transition bilities observation probability secondly action space continuous since uavs control signal ous lastly observation space reward function informative enough order achieve certain goal respondingly design observation space synthesizes multiple sensory output characterizing uavs internal state relationship environment destination besides domain reward function designed award penalize certain action taken certain state rl solves pomdps maintaining memory past observation action 27 31 stochastic deterministic control policy derived stochastic policy map memory past observation action tions action space deterministic policy map memory action method policy gradient od method 29 30 method 31 32 derive parameterized stochastic policy pomdps continuous action space ing gradient descent parameter space algorithm suffer major drawback not implemented fashion besides rl known unstable nonlinear function approximators used mate value function 33 respect drl combination deep learning rl directly project raw sensory put control signal developed provide solution concept markov decision process mdps 34 pomdps mnih 33 ﬁrstly utilizes drl play electronic game discrete control proﬁles afterwards lillicrap 35 applies insight problem continuous action space design deep deterministic policy gradient algorithm ddpg within framework 36 heess 37 take partial observability state mdps consideration design recurrent deterministic policy gradient algorithm rdpg show little performance difference stochastic deterministic policy controlling pomdps drl obtains optimal control policy learning might lead irreversible damage agent shall capable learning small sample real world rdpg not since not update policy parameter episode end episode agent interacts environment executing action determined learned policy episode end agent reach any terminal state obviously updating rameters policy make immediate use newest experience facilitating agent better plore unknown environment learn faster besides unlike ddpg conduct parameter optimization based state transition tuples state action reward next state rdpg update parameter based entire episode not break episode state transition tuples given pomdps not hold markov property show optimization procedure inefﬁcient stochastic gradient descend sgd applied conducting parameter tion regard propose efﬁcient online drl algorithm named solve pomdps main contribution paper follows 1 develop drl framework uav navigation scale complex environment navigation problem modeled pomdp solved novel online drl algorithm 2 prove pomdp setting policy gradient within framework determined history servations action instead entire episode policy gradient deterministic policy special case stochastic policy without precondition 3 design efﬁcient online drl algorithm approaching pomdps continuous action space based two rem build stochastic complex environment validate effectiveness proposed method remainder manuscript structured follows section ii introduces some background knowledge mdps pomdps two drl algorithm solving pomdps continuous control space section iii formulates tion problem section iv elaborates pomdp modeling process navigation problem proposed algorithm demonstrated section simulation result discussion presented section vi section vii concludes paper envisages some future work ii background section ﬁrst give brief introduction mdps pomdps followed two drl algorithm solving pomdps continuous action space mdps pomdps pomdps extension mdps partially observable state allows decision making uncertain condition mdp composed state space initial state space initial state distribution p action space state transition probability distribution p isfying markov property reward function r st characterizing environment feedback executing action state state space action space mdp could either discrete continuous since uavs control signal continuous paper focus mdps pomdps continuous state action space ease authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 2126 ieee transaction vehicular technology vol 68 no 3 march 2019 denotation leave implicit following context summation operation actually integral operation mdps generally addressed via rl learns mal policy π learning policy could either deterministic stochastic deterministic icy denoted μ project state action stochastic policy denoted p set probability measure return ability density available state action pair action space continuous stochastic deterministic icy parameterized function θ μ θ θ refers parameter function keep notation simple leave implicit case policy π μ function θ gradient respect rl involves estimating value function state value function pair stochastic policy value function deﬁned vπ st e γlr 1 γ discount factor ranging 0 1 function qπ st e γlr 2 stochastic policy optimal agent receives mum expected future discounted reward also referred target function executing η π e γtr st 3 deterministic policy value function function target function obtained replacing π μ st 1 2 3 mdp becomes pomdp agent could not observe state st instead receives observation ot distribution p immediate servation sequence no longer satisﬁes markov property p ot p quence agent need access entire history trajectory ht ot infer current state st make decision based goal rl partially observable setting thus learn optimal policy project history trajectory action distribution maximizing 3 ddpg rdpg 36 category policy gradient method dressing mdps continuous control space seek learn optimal policy θ μ θ menting policy gradient parameter space 0 π π qπ 4 π state distribution domly generated episode starting initial state following framework policy π function qπ called actor critic respectively critic parameterized function qω estimated td method ωt ϵ γqωt st st 5 γqω referred target value ϵ learning rate learning method ddpg derived deterministic policy gradient theorem mdps 38 say mdps continuous action space ministic policy gradient exists computed μ qμ 6 dμ denotes state distribution randomly generated episode rdpg extends framework ddpg pomdps dating policy parameter using following policy gradient μ e ht qμ ht ht 7 learning rdpg agent interacts environment using currently learned policy end episode agent cache entire episode replay memory designed randomize observation stabilize learning process 33 sample batch episode conduct parameter optimization rdpg show effectiveness many simulated tinuous control task partially observable state ha two limitation 1 rdpg ofﬂine learning algorithm only executes parameter optimization end episode tributing insufﬁcient utilization experience ration environment 2 rdpg converges slowly demonstrated section v parameter dated based history trajectory instead entire episode observing stochastic gradient 7 actually sum sequence history gradient episode rdpg converge slowly using sgd mization method respect propose new approach address pomdps iii problem formulation section formulate uav navigation problem complex environment coordinate frame uavs typically control proﬁle uavs ha three degree freedom including throttle heave steering refer speed change vertical displacement rotation around verse ax respectively correspondingly altogether authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply wang et al autonomous navigation uavs complex environment 2127 fig illustration partial observability uav navigation complex environment solid black circle gray circle represent uav perceptron range arrow denote possible trajectory generated uav six independent coordinate determining position tion motion uavs coordinate frame used describe uavs absolute position orientation denoted ϕ x z ϑ ϑy ϑz besides dinate frame used describe uavs linear angular velocity denoted μ b c ϑa ϑb ϑc coordinate frame together coordinate frame characterize uavs position orientation motion referred uavs internal state description uav navigation simplicity assume uavs ﬂy ﬁxed speed ﬁxed height unless stated furthermore omit uavs momentum ﬂying suppose steering action take effect no time consequence activity uavs restricted x plane vector describing uavs position orientation motion simpliﬁed ζ x ϑ dynamic uavs formulated ϑt xt v co yt v sin 8 steering signal v constant speed goal control uav ﬂy arbitrary ture place φdep destination φdes nate frame complex environment without trapped colliding obstacle iv pomdp modeling section ﬁrst explain example reason state navigation problem partially observable present detail pomdp modeling procedure uav navigation pomdp depicted fig 1 supposing uav locates point approach target position shall head north point point uav ﬁnds path target blocked obstacle shall turn back turn left turn right fig 2 b observation uavs environment denote distance returned nine virtual range ﬁnders ϖ represent angle distance uavs current position destination ϑ denotes angle uavs direction north direction c deployment additional sensor dimensional environment denote distance returned range ﬁnders outside horizontal plane given horizontal detection direction ϑz denotes angle uav movement direction perspective direction turning back reach point limited sensing capacity immediately ﬁnds surrounding due limited sensing capacity consequence turn back eventually stray similarly turning left right uav also end cycling trap contrast uav could remember ha experienced shall gradually construct structure local environment based experience accordingly make better decision mdps assume state fully observable would fail capture complex structure environment comparison pomdps able obtain information history trajectory therefore shall work efﬁciently observation space action space speciﬁcation purpose navigation uavs shall least capable receiving information three source information indicating internal state relationship ronment relationship destination firstly since environment often dynamic abandon uavs absolute position x only use ϑ tion describe internal state illustrated fig 2 b practice ϑ measured device secondly uavs relationship surrounding acterized image returned camera radar signal returned radar distance returned range ﬁnders work use nine range ﬁnders characterize relation denoted ψ depicted fig 2 lastly tor ξ ϖ represents distance angle uavs current position target used describe connection destination depicted fig 2 b practice ξ measured device composing three kind information brings u ﬁnal tion observation space ϑ ϖ ϑ ϖ π 0 100 0 given uavs ﬂight altitude speed preﬁxed constant use ρ ranging π denote steering signal reward design incorporating domain knowledge reward act signal evaluating good taking action state uav navigation complex authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 2128 ieee transaction vehicular technology vol 68 no 3 march 2019 environment straightforward way sparse reward mean uavs would rewarded only arrive target position however initial policy randomly generated uavs strike destination probability zero environment full barrier accordingly learning algorithm would take extremely long time converge alternative approach reward reward shaping 39 provides learning agent speciﬁc form reward guarantee policy invariance nonetheless since only make sense condition shaped reward difference potential function hard implement practice tends block way incorporating domain knowledge 40 paper design reward incorporates domain knowledge navigation problem preserving relatively satisfactory policy reward consists four part namely transition reward cle penalty reward step penalty transition reward designed rtrans σddist 9 σ positive constant ddist reduced distance tween uavs current position destination transition transition reward incorporates knowledge encourage uavs head destination ize distance target besides practice colliding obstacle could catastrophic uavs prevent getting close any obstacle design obstacle penalty rbar n 10 α β two positive constant dmin imum distance uavs obstacle courage avoid barrier would obtain constant reward rf ree direction point place lastly ensure uavs arrive tination soon possible transition would get constant penalty rstep summarize ﬁnal reward formulated rf inal rtrans rbar rf ree rstep 11 different instance 11 may result different policy obtain policy expect parameter must make deep survey relationship section vi new approach addressing pomdps section derive novel online drl algorithm solve pomdps begin derivation stating some basic deﬁnitions deﬁne value function underlying state st observing ht v ht π st e st ht π 12 function underlying pair st observing ht qht π st e st ht π 13 expected reward taking action state st observing ht rht π st e st ht π 14 take expectation 12 13 14 respect state distribution st obtain value function tory trajectory ht vπ ht est v ht π st 15 function history trajectory action pair ht qπ ht est qht π st 16 reward history trajectory action pair ht rπ ht est rht π st 17 based deﬁnitions performance objective 3 η π vπ 18 expectation initial history trajectory equal initial observation taking gradient performance objective spect parameter θ brings u stochastic policy gradient theorem pomdps theorem 1 consider learning stochastic policy pomdp continuous action space gradient stochastic policy exists calculated theorem stochastic policy gradient theorem pomdps consider learning stochastic policy pomdp continuous action space gradient stochastic policy exists calculated π h log π qπ h 19 dπ h expected history trajectory distribution proof see appendix remark 1 stochastic gradient 19 related tory trajectory instead entire episode result every time agent interacts environment perform online parameter optimization without waiting end episode facilitating agent better explore unknown environment learn faster analogous structure 19 4 reﬂects pomdps converted mdps informally regarding history jectories pomdps fully observable state silver 38 prof deterministic policy gradient mdps exists special case stochastic policy gradient tic policy satisﬁes certain condition nonetheless argue authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply wang et al autonomous navigation uavs complex environment 2129 fig schematic view simulated stochastic complex environment obstacle building different type environment distinguished shape size mutual space randomness environment manifest two aspect 1 four type environment uniformly randomly selected learning agent every time prepares interact environment 2 environment chosen agent building immediately randomly regenerated theorem extended deterministic policy making any assumption extension accomplished reformulating deterministic policy dirac delta function theorem ii deterministic policy gradient theorem pomdps consider learning deterministic policy pomdp continuous action space gradient deterministic policy μ h exists calculated μ h h qμ h h 20 dμ h denotes history trajectory distribution induced deterministic policy proof see appendix b remark 2 stochastic gradient 7 equal counted sum 20 term entire episode parameter actor critic rdpg actually mized based sequence highly correlated history ries framework actor critic approximated function parameter updated using sgd policy gradient algorithm addressing pomdps based 20 might potentially stable converge faster based 7 sgd quire input data sequence validate conclusion experiment section vi since gradient 19 20 involve taking expectation respect unknown history trajectory tribution dπ h must estimate sampling state action space regard rl algorithm using stochastic policy generally converge much slowly using terministic policy deterministic policy only require sample state space stochastic policy need ples state space action space faster convergence design drl algorithm named based theorem ii proposed algorithm demonstrated table use two target actor critic network replay memory aim stabilize learning process addition stochastic process exploration noise currently learned policy encourage agent keep exploring state action space 37 compared rdpg performs online learning strategy update parameter term history trajectory instead entire episode table vi simulation result section present experimental setting ulation result along some discussion experimental setting construct four type complex environment stochastic environment depicted fig ment cover one half square kilometer crowded building random height tation agent uniformly randomly chooses one environment rollouts episode following currently learned icy corrupted exploration noise every time agent start interacting environment building height uniform distribution u regenerated analogous rdpg actor critic 20 imated two lstms summarizing information encoded authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 2130 ieee transaction vehicular technology vol 68 no 3 march 2019 fig network structure actor critic two network take exactly form size ﬁrst three layer 11 400 300 respectively lstm cell 300 output layer contrast critic network input action sequence actor network begin designed keep uniform structure network folded form zero history trajectory network structure demonstrated fig observation uav normalized 0 1 steering signal normalized 1 uav speed ﬂight altitude set respectively reward instantiated σ α β rf ree rstep maximum episode length time step set besides adam optimizer 41 ployed learn network parameter learning rate actor critic respectively eters critic network regularized weight decay discounted factor γ soft target date rate ε addition exploration noise uniform distribution u used explore state action space navigation behavior validate effectiveness proposed method trained agent dispatched execute multiple gation task simulated environment depicted fig 5 projecting history trajectory control signal agent successfully ﬂies departure place destination manlike navigation behavior suggests drl efﬁcient approach addressing uav navigation problem scale complex environment verify memorable agent cient performing navigation task memoryless ddpg agent select two pair starting position target sitions difﬁcult environment let two agent execute navigation task depicted fig 6 learned navigate complex ment nevertheless since agent make sion based history observation action trajectory perceives information local structure environment accordingly capable escaping trap contrary ddpg agent not remember ha experienced make decision only based current ception environment consequence not percept local structure environment tends caught trap conclusion also veriﬁed convergence curve ddpg fig 7 obtained evaluating target function 3 using monte carlo method speciﬁcally 200 independent navigation task performed every ﬁve hundred training episode using learned policy target function approximated averaging counted cumulated reward 200 navigation episode seen two algorithm manifest similar convergence speed estimated normalized return ddpg much lower converge also make quantitative survey success rate stray rate crash rate navigation mission ddpg agent well agent four environment agent success rate stray rate crash rate obtained calculating percentage successful navigation mission percentage failed mission ending trapped local environment percentage failed mission due crashing obstacle 300 complete igation mission environment respectively result illustrated table ii see success rate ddpg agent four environment lower 54 only average contrast agent ﬁnishes navigation mission success rate 94 environment average signiﬁcant performance improvement additionally two agent exhibit low crash rate stray rate ddpg agent reach 69 env iv environment largest amount trap much higher agent validating ddpg agent not remember ha experienced prone trapped agent make decision based history rience ha ability escape trap signiﬁcant performance improvement ddpg come cost increased computational complexity ddpg us two feedforward neural network approximate actor critic us two recurrent neural network lstm experiment computational complexity optimizing recurrent neural network higher compared optimizing feedforward neural network performance validate efﬁcient rdpg rdpg equal illustrated fig 7 only us around 2 500 episode converge rdpg us around 8 000 episode reach convergence point since tice interaction real environment could catastrophic compared rdpg requires le time interaction environment therefore efﬁcient learning believe mance improvement come two aspect 1 online learning algorithm compared rdpg performs parameter optimization end entire episode update parameter every time agent interacts authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply wang et al autonomous navigation uavs complex environment 2131 fig navigation trajectory agent simulated environment solid circle star denote departure place destination respectively curve connecting represent navigation trajectory clarity any building lower altitude uav ﬂies not drawn image besides building obstructing view trajectory clip lower one height equal uav ﬂight altitude fig schematic view navigation trajectory agent ddpg agent sake clarity only top view environment drawn building lower uav ﬂight altitude omitted subﬁgures c depict navigation trajectory agent subﬁgures b corresponding trajectory ddpg agent fig convergence curve ddpg rdpg represents number episode used training ized return training stage return formulated 3 approximated averaging return 200 navigation trajectory generated learned policy environment making immediate use newest perience 2 efﬁcient data utilization high correlation among history trajectory episode reduces efﬁciency rdpg data utilization trast us mutually independent history trajectory sampled replay memory perform parameter mization therefore work efﬁciently although rdpg achieve similar malized return converge learned policy deviate lot see table ii ce rate rdpg agent four environment lower 87 only average around 21 lower agent besides stray rate rdpg agent much higher agent consequently though two rithms designed settle pomdps efﬁcient superiority rdpg also demonstrated strong generalization ability height building stochastic environment ha uniform distribution interval 30 200 lower height denser obstacle test generalization ability two algorithm agent rdpg agent dispatched execute navigation mission different ﬂight altitude complex environment besides also ﬁx ﬂight altitude 90 let two agent execute navigation task even complex ments result illustrated fig 8 fig see fig 8 b c decrease ﬂight altitude success rate agent doe not degrade whereas rdpg agent manifest relatively cant decrease besides contrast agent stray rate crash rate ddpg agent tend increase authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 2132 ieee transaction vehicular technology vol 68 no 3 march 2019 table ii statistical quantity ddpg rdpg different environment fig success rate stray rate crash rate average ﬂight distance navigation mission agent rdpg agent different ﬂight altitude ﬂight altitude success rate stray rate crash rate obtained way table ii average ﬂight distance estimated averaging length successfully arrived trajectory environment ﬂight altitude decrease higher success rate lower stray rate crash rate agent different ﬂight tude demonstrates doe not simply remember structure complex environment derives ability cope complex environment fig 8 average ﬂight distance two agent see average ﬂight distance rdpg agent also uniformly longer agent different ﬂight altitude illustrating proposed efﬁcient ddpg escaping trap complex environment besides see lower ﬂight altitude step two agent spend heading destination result accordance intuition decrease ﬂight altitude obstacle environment become denser path destination becomes twisty addition illustrated fig 9 increase area complex environment two agent manifest tendency decrease success fig success rate stray rate crash rate average ﬂight distance navigation mission agent well rdpg agent complex environment different size environment different size obtained simply extending original environment shown fig 3 larger size table iii statistical quantity trained different reward rate increase stray rate crash rate average ﬂight distance nonetheless success rate agent environment still higher 90 around 13 higher ddpg agent scale environment besides rdpg agent average ﬂight distance much shorter agent especially environment considering distance uavs destination one dimension servation space agent never trained complex environment high performance illustrates ha strong generalization ability authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply wang et al autonomous navigation uavs complex environment 2133 fig schematic view inﬂuence reward navigation behavior brevity only choose two pair starting point target point env env iv subﬁgure c show navigation trajectory agent rf r ee rstep b corresponding navigation trajectory rf r ee rstep fig navigation trajectory agent environment clarity not draw building obstructing view trajectory effect reward addition partial observability state reward also fects navigation behavior agent vestigate inﬂuence navigation behavior varying reward step penalty reset reward step penalty rf ree rstep respectively keep parameter unchanged retrain agent table iii show success rate stray rate crash rate average ﬂight distance agent ter retraining see trained low step penalty high reward agent show tic success rate decrease around 85 besides stray rate average ﬂight distance signiﬁcantly increase respectively ing retrained agent tends stray free space rather head destination fig 10 give schematic view result see fed higher reward lower step penalty agent stray free space fails strike destination result tent intuition since value reward step penalty exactly modulates much want uavs head free space avoid obstacle head destination soon possible result though reward appropriate way encode domain knowledge problem speed learning procedure must designed carefully might main drawback reward without reward shaping navigation environment demonstrate method also work dimensional environment since uavs generally not permitted ﬂy neither high low urban area restrict cruising altitude 50 130 ing descending process beginning end navigation procedure not taken consideration uavs take land vertically thus long uav rives overhead space target position navigation mission completes horizontal detection direction add four range ﬁnders sense obstacle outside horizontal plane illustrated fig 2 c assume uav direction parallel horizontal plane adjustment also made reward function action observation space uav get extra penalty altitude near either end altitude interval penalty obstacle penalty except dmin represents minimum vertical distance end altitude interval besides illustrated fig 2 c new dimension added control vector maneuver vertical angle ϑz uav movement direction direction observation space adjusted adding uav altitude hz vertical angle ϑz 36 distance 9 returned additional range ﬁnders adjustment lustrated fig 11 uav also perform navigation task environment contrast setting uav environment avoid obstacle changing horizontal direction well altitude success rate stray rate crash rate agent respectively compared setting crash rate increase think performance degradation resulted poor sensing capacity range authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 2134 ieee transaction vehicular technology vol 68 no 3 march 2019 ﬁnders different camera range ﬁnders only detect obstacle several direction therefore sible obstacle close uav uav think keep proper distance situation come even worse environment since environment even complex environment vii conclusion future work work develop drl framework uav igation complex environment problem formulated pomdp general drl algorithm proposed address without map reconstruction path planning method enables uavs ﬂy arbitrary parture place destination complex ments theoretical derivation simulation result strate efﬁcient rdpg besides could generalize complex ronments only minor performance degradation le still some work future stance reward function must designed otherwise may yield unsatisfactory performance tial way settle problem directly handling sparse reward appendix proof follows along similar line standard policy gradient theorem mdps 42 derived 21 shown bottom page value function vπ h value function qπ h satisfy recursive relationship similar mdps use p ht denote observation dynamic history trajectory analogous environment dynamic note p ht fact refers p ht setting p ht zero ht not preﬁx extended represent transition probability any pair history trajectory since explicitly reformulate target function 18 term value function history trajectory gradient involves taking derivative value function vπ h term policy weight equipped recursive relationship left part proof becomes straightforward applying 21 following line derivation 42 write gradient value function vπ h γkp k π qπ h h π h qπ h 22 pr k π denotes probability ing history trajectory h k step following policy π h could any history trajectory since extended p ht entire history trajectory space besides π h γkp k π represents history trajectory distribution randomly erated episode starting initial history trajectory following policy immediate gradient target function 18 written π 0 π h π qπ h h π qπ h 23 dπ h deﬁned theorem appendix b ﬁrst introduce lemma lemma derivative dirac delta function supposing f x any continuous function δ x dirac delta function derivative δ x hold following property x f x dx 0 24 recall main context replace integral tions summation operation case ease denotation rewrite 19 π h dπ h π π qπ h dadh 25 qπ ht est st qht π st est st est st γest st eot 1 st st p ot 1 v ht 1 π rπ ht γ est st 1 ot st st ot 1 p ot ht v ht 1 π rπ ht γeht ht ht est st st st 1 v ht 1 π rπ ht γeht ht ht est st 1 v ht 1 π rπ ht γ ht 1 p ht vπ 21 authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply wang et al autonomous navigation uavs complex environment 2135 π h dπ h π π qπ h dadh h dπ h f h f h qπ h μ h h f h dh h dμ h f h f h h h qπ h μ h h f h dh h dμ h h qμ h μ h h h f h dh h dμ h h h qπ h μ h h f h dh h dμ h h μ h h h h f h dh h dμ h h h h dh 26 deterministic policy represented δ μ h let f h μ h policy denoted δ f h substituting 25 employing change variable nique property present lemma obtain ﬁnal result shown 26 top page reference 1 mohammed idries mohamed jawhar uavs smart city opportunity challenge proc ieee int conf unmanned aircraft 2014 pp 2 israelsen beall bareiss stuart keeney van den berg automatic collision avoidance manually manned aerial vehicle proc ieee int conf robot 2014 pp 3 chee zhong control navigation collision avoidance unmanned aerial vehicle sen actuator vol 190 pp 2013 4 agrawal ratnoo ghose inverse optical ﬂow based guidance uav navigation urban canyon aerosp sci vol 68 pp 2017 5 gageik benz montenegro obstacle detection collision avoidance uav complementary sensor ieee access vol 3 pp 2015 6 peng lin dai path planning obstacle ance vision guided quadrotor uav navigation proc ieee int conf control 2016 pp 7 roelofsen gillet martinoli reciprocal collision avoidance quadrotors using visual detection proc int conf intell robot 2015 pp 8 belkhouche modeling calculating collision risk air hicles ieee trans veh vol 62 no 5 pp jun 2013 9 luo mcclean parr teacy de nardi uav position estimation collision avoidance using extended kalman ﬁlter ieee trans veh vol 62 no 6 pp jul 2013 10 cui lai dong chen autonomous navigation uav foliage environment intell robot vol 84 no pp 2016 11 gee james van der mark delmas gimel farb lidar guided stereo simultaneous localization mapping slam uav outdoor scene reconstruction proc ieee int conf image vi comput new zealand 2016 pp 12 li liu zhang hang imu integrated navigation slam method small uav indoor environment proc ieee conf inertial sensor syst 2014 pp 13 fu campoy monocular collision avoidance strategy uav using fuzzy logic controller intell robot vol 73 no pp 2014 14 oguz temeltas consistency analysis uav navigation proc spie vol 9084 no 18 pp 2014 15 thrun montemerlo graph slam algorithm cation mapping urban structure int robot vol 25 no pp 2006 16 zhou zou pei ying liu yu structslam visual slam building structure line ieee trans veh vol 64 no 4 pp apr 2015 17 goh abdelkhalik zekavat weighted ment fusion kalman ﬁlter implementation uav navigation aerosp sci vol 28 no 1 pp 2013 18 zhang kleeman robust appearance based visual route lowing navigation outdoor environment int robot vol 28 no 3 pp 2009 19 karpenko konovalenko miller miller nikolaev uav control basis landmark observation sensor vol 15 no 12 pp 2015 20 konovalenko miller miller nikolaev uav navigation basis feature point detection underlying face proc eur conf model 2015 pp 21 prieto visual based navigation power line inspection using virtual environment proc spie vol 9406 2015 art no 22 strydom thurrowgood srinivasan visual odometry autonomous uav navigation using optic ﬂow stereo proc tralas conf robot 2014 pp 23 ross et learning monocular reactive uav control cluttered natural environment proc ieee int conf robot 2013 pp 24 faust palunko cruz fierro tapia automated aerial suspended cargo delivery reinforcement learning artif vol 247 pp 2017 25 imanberdiyev fu kayacan chen autonomous igation uav using reinforcement ing proc ieee int conf control robot 2016 pp 26 wang wang zhang zhang autonomous tion uav unknown complex environment deep reinforcement learning proc ieee int conf globalsip 2017 pp 27 chrisman reinforcement learning perceptual aliasing ceptual distinction approach proc assoc advancement artif 1992 pp authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply 2136 ieee transaction vehicular technology vol 68 no 3 march 2019 28 lin mitchell memory approach reinforcement learning domain school comput carnegie mellon pittsburgh pa 15213 usa tech may 1992 29 williams simple statistical algorithm nectionist reinforcement learning mach vol 8 no pp 256 1992 30 wierstra foerster peter schmidhuber solving deep memory pomdps recurrent policy gradient proc int conf artif neural 2007 pp 31 jurˇ ıˇ cek thomson young natural actor belief critic reinforcement algorithm learning parameter dialogue system modelled pomdps acm trans speech lang vol 7 no 3 pp 2011 32 wierstra schmidhuber policy gradient critic proc eur conf mach 2007 pp 33 mnih et control deep reinforcement ing nature vol 518 no 7540 pp 2015 34 sutton barto reinforcement learning introduction vol 1 no cambridge usa mit press 1998 35 lillicrap et continuous control deep reinforcement ing 2015 36 konda tsitsiklis algorithm proc neural inf process 2000 pp 37 heess hunt lillicrap silver control recurrent neural network 2015 38 silver lever heess degris wierstra riedmiller deterministic policy gradient algorithm proc int conf mach 2014 pp 39 ng harada russell policy invariance reward transformation theory application reward shaping proc int conf mach 1999 vol 99 pp 40 harutyunyan devlin vrancx e expressing arbitrary reward function advice proc assoc advancement artif 2015 pp 41 kingma ba adam method stochastic optimization proc int conf learn 2015 42 sutton mcallester singh mansour policy dient method reinforcement learning function approximation proc advance neural inf process 2000 pp chao wang 16 received degree electronic engineering tsinghua university beijing china currently working toward degree department tronic engineering tsinghua university supervision professor zhang research terests include signal processing machine learning intelligent control jian wang sm 18 received degree electronic engineering tsinghua university beijing china 2006 joined faculty tsinghua university currently sociate professor department electronic engineering research interest include intelligent collaborative system information security enhancing technology signal processing encrypted domain wireless network yuan shen 14 received degree highest honor electronic engineering tsinghua university beijing china 2005 degree electrical engineering computer science mit cambridge usa 2008 2014 respectively associate professor ment electronic engineering tsinghua university prior wa research assistant postdoctoral associate wireless tion network science laboratory mit research interest include statistical inference communication theory information theory optimization current research focus network localization navigation inference technique resource allocation cooperative network shen chair also served vice chair 2018 secretary ieee comsoc radio tions committee tpc symposium ieee icc 2020 also wa ieee globecom 2018 2016 eusipco 2016 ieee icc advanced network localization navigation anln workshop ha editor ieee transaction wireless communication since 2018 ieee wireless cation letter since 2018 ieee china communication since 2017 ieee communication letter international journal distributed sensor network 2015 wa recipient ieee comsoc board outstanding young scholar award qiu shi outstanding young scholar award china youth talent program marconi society paul baran young scholar award paper received ieee comsoc fred ellersick prize three best paper award ieee international conference xudong zhang 04 received degree tsinghua university beijing china ha department electronics gineering tsinghua university since ha authored coauthored 150 paper three book ﬁeld signal processing chine learning research interest include tical signal processing machine learning theory multimedia signal processing authorized licensed use limited queen mary university london downloaded march utc ieee xplore restriction apply