journal machine learning research 23 2022 submitted revised published simple agent complex environment eﬃcient reinforcement learning agent state shi dong benjamin van roy bvr stanford university zhengyuan zhou zzhou new york university editor alekh agarwal abstract design simple reinforcement learning rl agent implement optimistic version establish regret analysis agent operate some level competence any environment leverage concept literature provably eﬃcient rl consider general interface provide novel agent design analysis level generality position result inform design future agent operation complex real environment establish time progress agent performs competitively relative policy require longer time evaluate time take approach asymptotic performance polynomial complexity agent state representation time required evaluate best policy agent represent notably no dependence complexity environment ultimate performance loss agent bounded constant multiple measure distortion introduced agent state representation work ﬁrst establish algorithm approach asymptotic condition within tractable time frame keywords reinforcement learning dynamic programming regret analysis agent design introduction reinforcement learning agent demonstrated remarkable success simulated ronments example recently developed muzero agent schrittwieser et 2020 learns interact eﬀectively any broad range environment simulator delivers superhuman performance playing chess go shogi arcade game continuing innovation area aim produce agent engage increasingly complex environment ultimately environment like physical world world wide web pose far greater complexity agent represent growing mathematical literature focus establishing eﬃciency guarantee typically term sample complexity regret bound kearns singh 2002 jaksch et al 2010 represent early instance indeed data eﬃciency remains impediment c shi dong benjamin van roy zhengyuan zhou license see attribution requirement provided dong van roy zhou carrying success reinforcement learning simulated real environment agent must learn within reasonable time frame mathematical literature ought inform future agent design however work area ha tended focus restrictive class environment produce bound depend number environment state eﬀectively inﬁnite complex environment paper aim bridge divide particular extend idea mathematical literature relaxing common restriction establish result oﬀer insight simple agent operate eﬀectively arbitrarily complex environment consider problem formulation ha previously studied work contributes multiple front framing learning objective agent design performance analysis figure 1 bridging divide provably eﬃcient reinforcement learning versus practical agent design complex environment consider interface illustrated figure 2 deﬁned ﬁnite action set ﬁnite observation set agent interacts environment executing time action registering observation generating single stream experience time agent selects based history ht ot initial history empty interface described general agent engage manner arbitrarily complex environment example consider agent interacts world wide web via computer terminal action could encode keystroke mouse click movement observation could take form pixel rendered monitor context environment would likely far complex agent 2 simple agent complex environment environment dynamic characterized function ρ assigns probability ρ p ht observation hence environment speciﬁed tuple e ρ ﬁxed set observation probability function order accommodate complex real environment formulation enables u relax several restrictive assumption commonly made literature figure 2 interface not assume environment markov decision process mdp would require observation probability ρ depend history only recent observation not assume environment exhibit episodic behavior would require environment occasionally not assume performance optimal policy accurately estimated within manageable time frame complex environment required time intractably large even inﬁnite not require agent supplied duration operation input consider instead single endless stream experience calling agent perform well any long horizon policy performance policy π mapping history action probability probability assigned action history h denoted π let p denote set policy denote b π policy executed agent agent design amount specifying policy typically term algorithm sample action according b π designer preference expressed term reward function history ht action observation function prescribes reward r ht characterize performance policy term expected reward formalize notion build general probabilistic framework detail presented appendix framework action observation random variable observation probability function ρ also random variable unknown agent designer consequently environment e random variable formally deﬁned appendix use subscript indicate probability expectation evaluated action selected particular policy example action probability 3 dong van roy zhou satisfy pπ π expected return timesteps policy π written eπ expressing probability expectation b π suppress subscript example p pb π b π e eb π notation denote average reward policy π λπ lim inf 1 x e 1 optimal average reward λπ quantify agent performance relative reference policy π timesteps environment e term regret regretπ e x e 2 also consider notion regret relative reference policy class sup regretπ 3 note regretp simply regret relative optimal average reward exists optimal policy regretp note expression deﬁning regret random variable depend environment perspective agent designer reasonable goal would attain low expected regret e regretp long duration expectation taken randomness however paper examine performance ﬁxed agent case distribution e point mass p e e 1 e any environment establish almost sure upper bound random variable regretp naturally bound would imply upper bound e regretp note practice agent designer not restricted ﬁxed agent able leverage prior knowledge e improve agent design topic nevertheless beyond scope work regret bound necessarily depend time required ass policy complex environment time required ass optimal policy arbitrarily large even inﬁnite develop bound depend instead time required ass policy reference class particular bound indicate time progress agent accumulates experience perform well relative policy take longer ass simple agent practical agent must operate bounded memory computation constraint agent not retain repeatedly process history rather agent maintains only agent state xt suﬃces produce action since xt represents agent retains history must updated incrementally according b f xt 4 simple agent complex environment some agent state update function b f represents algorithmic randomness discussed lu et al 2021 popular agent design dqn mnih et 2015 muzero schrittwieser et 2020 mpo abdolmaleki et 2018 song et 2020 agent state partitioned three component agent state xt situational state st epistemic state pt algorithmic state zt situational state meant capture salient information agent current situation environment epistemic state retains agent knowledge environment algorithmic state record information unrelated environment reading agent internal clock internally generated random number reward computed agent depend history situational state letting denote ﬁnite set situational state reward function take form r r generating reward according r st note reward function special case introduced section r ht work focus case reward computed situational state instead directly history since set situational state ﬁnite allowing reward depend situational state ensures computation reward function r doe not require inﬁnite memory paper design analyze simple agent engage any environment instantiated following input initial situational state update function f reward function r 0 1 provide precise speciﬁcation later paper oﬀer rough description agent operates agent update situational state according f st us compute reward illustrated figure note function f assumed given agent beginning ﬁxed throughout agent lifetime sequence situational state deﬁned recursively f situational state dynamic need not markovian particular p st p ht agent epistemic state pt qt nt comprised action value function qt count function nt agent algorithmic state includes only current time action value qt updated via optimistic discounted algorithm discount factor degree optimism increasing time agent update nt track visitation count used determine suitable degree optimism action sampled uniformly set arg qt st greedy action hence any time agent seen executing policy πt action probability depend history ht only situational state 5 dong van roy zhou figure 3 agent maintains situational state us compute reward worth emphasizing agent not designed oﬀer performance simulated real environment rather motivation design agent amenable theoretical analysis aim generate insight inform design future agent example service rate control let u consider didactic example exceedingly simple serf elucidate notation framework example involves agent operating service station illustrated figure time one customer present agent applies service mode fast slow customer pay 1 upon arrival no cost incurred slow mode applied no customer served fast mode service incurs cost per timestep maximize average reward agent must make choice balance revenue cost service figure 4 service station serving customer problem one service rate control studied operation research see weber stidham jr 1987 stidham jr weber 1989 jo 1989 sennott 2009 however work ha tended focus agent eﬀective applied particular stylized model govern arrival service rate approach instead adapts any statistical structure doe not suﬀer misspeciﬁcation work reinforcement learning control queueing system moallemi et 2008 raeis et 2021 share spirit note only order convey idea simple transparent manner focus simple service system agent applied much complex environment example involving multiple server queue 6 simple agent complex environment interface agent perspective service station viewed environment e ρ action fast slow identify service mode observation arrival departure indicate arrival departure hence service mode applied timestep indicates any arrival departure occurring end timestep function ρ speciﬁes observation probability conditioned history initially unknown example designer may uncertain customer arrival rate depend history situational state dynamic consider situational state st 0 1 simply indicates presence customer since observation record arrival departure function f f st service initially vacant proﬁt written r st some function special interest policy select action based only situational state set policy pπ pπ let u denote set baseline agent consider agent designed learn policy within class policy simple enough agent could perform nearly exhaustive search restrict attention approach scale setting involving much larger set situational state two simple agent kind serve baseline comparison let πϵ p policy absence customer applies slow mode otherwise sample slow fast probability 1 baseline agent begin executing πϵ ϵ 0 applying slow service mode every timestep ﬁrst agent increase ϵ gathering data long duration using data estimate arrival rate estimate warrant increasing service rate agent analysis static sense doe not entail any experimentation instead assumes arrival rate remain ﬁxed second agent additionally try small value ϵ 0 some duration order estimate derivative average reward derivative positive increase one signiﬁcant diﬀerence relative ﬁrst agent use derivative second agent anticipates impact small increase ϵ bear arrival rate second agent representative approach used policy gradient literature discussed sutton barto 2018 reference therein environment dynamic study behavior agent given environment dynamic characterized speciﬁc observation probability function corresponding realized environment denoted provide detailed speciﬁcation appendix b assume purpose analysis p e environment customer arrival rate depends maximum service time experienced among recent dozen customer served idea long service time hurt reputation turn reduces number customer seeking service service time impacted agent choice 7 dong van roy zhou fast mode service always completed single timestep slow mode service completed next timestep probability given speciﬁcation maximal average reward per timestep achieved applying fast mode service every timestep case customer served single timestep new customer arrives soon previous one departs performance figure 5 plot cumulative moving average reward attained optimistic agent later present averaged two hundred independent simulation ﬁgure also plot maximum average reward average reward attained always applying slow service mode baseline agent never choose deviate slow service mode therefore realize average reward close latter figure 5 cumulative moving average reward attained optimistic agent maximal average reward average reward attained always applying slow service mode approximates behavior baseline agent result convey potential beneﬁts agent designed address general environment optimistic agent eventually ﬁgures choice drive future arrival rate suﬃciently improve performance baseline agent not demonstrate level sophistication contribution related literature formulation interaction studied paper general involving single stream experience without restrictive assumption commonly made literature discussed section important theoretical work relax assumption inform design agent operate complex real environment formulation bear close resemblance studied mccallum 1995 hutter 2004 daswani et al 2013 2014 lu et al 2021 formulation not focus work provably eﬃcient reinforcement learning work ﬁrst extend regret analysis tool setting 8 simple agent complex environment building upon general modeling interaction paper make range contribution innovating framing learning objective agent design performance analysis well generating qualitative insight inform practical agent design section summarize contribution relation prior literature framing learning objective literature provably eﬃcient reinforcement learning common study agent performance regret analysis however manner regret bound typically framed doe not suitably accommodate complex environment develop concept allow u frame meaningful learning objective context averaging time intelligently choose policy agent must ass relative performance regret bound established literature typically reﬂect requirement via dependence statistic bound time required ass optimal policy example include episode duration osband et 2013 2019 azar et 2017 jin et 2018 diameter jaksch et 2010 span bartlett tewari 2012 ouyang et 2017 wei et 2020 complex environment time required ass optimal policy intractably large even inﬁnite derive bound instead depend reward averaging time policy reference class let λπ h denote expected average reward timesteps starting history h λπ lim deﬁne reward averaging time τπ policy π smallest value τ 0 h 4 h closely related concept introduced kearns singh 2002 deﬁnes notion averaging time function tolerance parameter associated error h deﬁnition relies instead single scalar statistic τπ let optimal policy essentially equivalent notion span introduced bartlett tewari 2012 note might exist history h policy π λπ h doe not converge λπ resulting τπ not need rule case analysis see later certain policy π τπ vacuous regret bound regret make intuitive sense since inﬁnite reward averaging time implies agent might not able reverse previous mistake consequently could get stuck suboptimality indeﬁnitely distortion distinctive element formulation agent instantiation situational state update function serf simplify agent experience extracting useful feature history enables productive behavior arbitrarily complex environment 9 dong van roy zhou particular instead number environment state regret bound depend number situational state distortion incurred using predict optimal discounted value follows let φ h denote situational state would generated experiencing history h speciﬁcally φ h mapping deﬁned h ot history sequence containing timesteps st sequence situational state generated h f f st f ot φ h worth emphasizing mapping φ determined avoid cluttering suppress dependence notation discount factor γ 0 1 history h action denote optimal discounted action value qγ h discount factor γ weight reward realized k timesteps γk thought prescribing eﬀective planning horizon τ 1 deﬁne distortion eﬀective planning horizon τ max sup φ h qγ h inf φ h qγ h 5 γ 1 maximum diﬀerence optimal action value across history lead situational state oﬀers measure error introduced predicting optimal action value based situational state instead history sort distortion measure ha long used analysis approximate dynamic programming algorithm aggregate environment state gordon 1995 tsitsiklis van roy 1996 van roy 2006 though case instead aggregate history framing requires stronger notion distortion deﬁned sup τ 6 quantiﬁes accuracy situational state predict optimal action value planning horizon duration τ greater distortion measure oﬀers useful statistic characterizing performance agent able plan eﬀectively increasing horizon data accumulates worth mentioning aggregation history deﬁnition distortion bear resemblance notion abstraction deﬁned li et 2006 li et al 2006 presume environment modeled mdp work set history inﬁnite would like highlight conceptual value 10 simple agent complex environment distinction since goal analysing concrete agent operates ﬁnite memory work instead simply assuming exists abstraction function mapping history situational state consider situational state update function f allows agent recursively compute next situational state based previous one way able ensure agent able maintain update situational state ﬁnite memory worth mentioning distinction doe not aﬀect regret analysis reader familiar rl theory might also connect work li 2009 comprehensively study abstraction framework note result work applies general setting since doe not impose ﬁxed discount factor sequence reward moreover theory show compared delayed algorithm studied li 2009 ultimate performance loss agent enjoys much graceful dependence abstraction gap deﬁned section li 2009 note environment deﬁned viewed mdp inﬁnitely many mdp state aggregation history considering amount state aggregation broader context state aggregation viewed form function approximation large body theoretical literature ha dedicated function approximation mdps one relevant work category jiang et 2017 author show environment dynamic approximately admit factorization class function approximate state value accordance factorization number sample required learn policy environment polynomial respect rank speciﬁcally result implies sample complexity learning policy state aggregation polynomial number aggregate recent work along line include jin et 2020 agarwal et 2020 wang et 2020 zanette et 2020 however algorithm work sample complexity guarantee mainly serve theoretical purpose little bearing practical agent design whereas one main target work shed light element empirically successful agent reader interested theoretical result function approximation refer recent paper zhou et 2021 provides comprehensive review limitation framing use ﬁxed situational state update function consistent manner some practical agent operate example dqn agent mnih et 2015 take situational state some number recent video frame likely value adapting way situational state updated based learned environment encoded agent epistemic state muzero agent schrittwieser et 2020 doe adapt update function way agent represents update function term recurrent neural network weight adapted time based interaction environment despite limitation framing represents signiﬁcant step advancing mathematical literature direction may inform future agent design 11 dong van roy zhou reference class frame agent design objective notion competing eﬀectively policy particular reference class eﬀectiveness measured lens regret function averaging time distortion opposed single scalar objective spirit oﬀer framework studying derive interpretable regret bound generate insight inform agent designer understand spirit may helpful draw analogy ﬁeld optimization optimization problem framed term precise scalar objective design optimization algorithm tends formulated term measure computational complexity solution quality function number decision variable constraint well salient problem characteristic one reference class introduced earlier denoted p consists policy π pπ pπ word policy select action based situational state instead history ideally situational state suﬃce predicting agent requires make optimal decision case p would include optimal policy let π p policy π p λπ think agent trying learn policy within p natural expect λb π π b π policy executed agent deﬁned section discus appendix exist environment situational state dynamic π π consequently λb π π average regret satisﬁes lim inf regretp π π π 7 light fundamental limitation policy class p frame objective optimizing dependence average regret distortion aforementioned objective call agent eventually compete eﬀectively best policy among select action based situational state second objective frame call agent attain eventual level performance quickly time required depends time take compare policy bounded averaging time discussed earlier important avoid dependence averaging time optimal policy well number environment state intractably large even inﬁnite complex environment instead consider regret bound depend number situational state averaging time particular consider regret bound form regretp π π foobar metasyntactic function some abuse notation use denote set cardinality understanding regret depends argument guide design quickly learn perform well relative additionally consider τ reference class pτ π τπ consisting policy averaging time no greater class consider bound form regretpτ τ 12 simple agent complex environment diﬀerent function foobar bound oﬀer insight agent quickly learn perform well relative policy any particular averaging time agent ought able compete policy pτ within some time grows τ regret bound reﬂect relationship draw attention balancing associated agent design agent implement variant watkins 1989 early analysis focused asymptotic convergence guarantee assumption agent try action environment state inﬁnitely often watkins 1989 watkins dayan 1992 tsitsiklis 1994 jaakkola et 1994 recently research ha merged concept literature regret analysis leading provably eﬃcient variation jin et 2018 wei et 2020 optimistic agent ensure level eﬃciency using carefully chosen step size perturbing action value update maintain optimistic estimate merging present opportunity bridge eﬃcient reinforcement learning literature practical agent design aligned algorithm studied mathematical literature build line work design new optimistic agent suitable complex environment agent relies several algorithmic innovation agent jin et 2018 wei et 2020 maintain action value environment state maintains action value situational state algorithm jin et al 2018 designed episodic environment wei et al 2020 operates ﬁxed discount factor depends horizon algorithm designed general environment doe make use discount factor discount factor increase time generate eﬀective behavior increasingly long planning horizon step size used jin et 2018 wei et 2020 depend horizon agent designed operate indeﬁnitely rather predetermined horizon us step size not depend performance analysis critical contribution paper lie performance analysis result presented section 4 discus key implication firstly establish π agent attains average regret lim sup regretp π π 8 exactly four time lower bound 7 also interesting relate upper bound theorem 19 section indicates ϵ 0 exists environment set situational state situational state update function reward function particular approximate dynamic programming adp method one might apply whitt 1978 gordon 1995 tsitsiklis van roy 1996 munos ari 2008 yield policy µ π generally far worse van roy 2006 suggests ﬁxed point would yield policy satisﬁes π not known whether 13 dong van roy zhou ﬁxed point determined computationally tractable algorithm intriguing agent computationally tractable based method attains average regret within factor four provide brief discussion topic end appendix specialized case distortion π 0 analysis implies following regretp sa π sat τ 5 π 9 omission constant factor case since situational state enable exact prediction optimal value regret grows sublinearly meaning agent eventually learns globally optimal policy dependence worse usual scaling appears result pertaining episodic environment jin et al 2018 zhang et al 2020 formulation scaling unachievable without additional term regret bound scale exponentially jaksch et al 2010 wei et al 2020 worth noting wei et al 2020 considers average reward objective though zero distortion provides regret bound scale rather algorithm crucially relies knowledge ﬁxed duration agent analysis also modiﬁed attain scaling given ﬁxed duration combining 8 9 see besides bound only depends π reward averaging time best policy reference class previous regret bound tabular reinforcement learning scale number state reward averaging time optimal policy complex environment quantity arbitrarily large inﬁnite interestingly bound ensures agent able learn eﬃciently spite establish τ regretpτ sa τ sat τ 5 10 recall pτ class policy reward averaging time no greater τ regretpτ quantiﬁes regret relative class bound oﬀers insight time agent learn perform competitively policy larger reward averaging time understand let u focus special case 0 case bound implies ϵ 0 1 setting τ ϵt lim sup regretpτ 11 hence suﬃciently large agent average reward approximates best policy reward averaging time no greater ϵt qualitative insight share element common agent agent far simpler motivation wa not produce another agent rather oﬀer context 14 simple agent complex environment amenable analysis inform design future agent discus some key insight supported result first result demonstrate possible agent operate eﬀectively within tractable time frame single endless stream interaction arbitrarily complex environment previous result either rely fact environment mix modest amount time jin et 2018 jaksch et 2010 zhang et 2020 horizon operation ﬁxed known agent wei et 2020 previous result focus mdps ha also related work pomdps et 2021 kara yuksel 2020 subramanian et 2022 result relevant only tractable number environment state bound not depend environment mixing time number state among thing result imply agent perform well even environment complex performance optimal policy would take forever estimate secondly ﬁrst establish algorithm average regret bounded constant multiple distortion approach asymptotic performance within tractable time frame example van roy 2006 implies certain common adp algorithm require environment dynamic known not output policy π p within constant multiple indeed previous analysis adp algorithm instead bound multiple τ some notion averaging time depends environment complexity whitt 1978 gordon 1995 tsitsiklis van roy 1996 scaling τ far worse constant τ becoming arbitrarily large complex environment real environment impractical attain zero distortion therefore some degree impact performance inevitable result oﬀers insight avoid scaling intriguing aspect agent design eﬀective planning horizon increase time allowing agent eventually optimize performance arbitrarily long horizon agent eﬀective planning horizon scale rate lead regret bound notion planning may beneﬁt restricting eﬀective horizon based quantity data gathered ha also observed jiang et al 2015 regret bound depend distortion induced ﬁxed situational state update function however some agent leverage ability neural network adapt update function nachum et 2018 schrittwieser et 2020 result not directly address adaptation oﬀer insight way inﬂuence agent performance value function central theory mdps value function value function typically considered function environment state consider instead function history section deﬁne value function characterize solution bellman equation 15 dong van roy zhou throughout section consider ﬁxed discount factor γ 0 1 environment e ρ simplify notation use h denote history generated concatenating action observation history deﬁne h h transition matrix pa entry ρ h 0 otherwise 12 h similarly policy π deﬁne transition matrix pπ x π 13 h action policy π let ra rπ vector component given rah x ρ r h rπh x π rah 14 policy π let v γ π h x γt πrπ h qγ π h rah x v γ π 15 function represent expected discounted reward starting history h either subsequent action selected π only action executed taking supremum policy obtain optimal value v γ h sup v γ π h qγ h sup qγ π h 16 following proposition follows proposition bertsekas 2018 terizes v γ qγ unique solution among set bounded function bellman equation proposition 1 pair v γ qγ uniquely solves system equation v h q h q h rh γ p v among pair bounded function v h q h close section important lemma tie together three concept relating policy π expected average reward λπ reward averaging time τπ discounted value function v γ π lemma closely resembles theorem de farias van roy 2006 omit proof lemma 2 π h γ 0 1 v γ π h λπ result establishes reward averaging time bound diﬀerence discounted value v γ π h average reward λπ scaled eﬀective horizon 16 simple agent complex environment agent design performance analysis section present study optimistic agent similarly agent demonstrated success simulation learns predict action value however rather neural network representation agent maintains lookup table containing one prediction per situational pair action selected greedily respect prediction upon observation agent incrementally adjusts prediction assigned previous pair based temporal diﬀerence agent predicts discounted value however order eventually maximize average reward associated discount factor increase time approach one idea agent plan any given time particular eﬀective horizon horizon increase agent gather data enables planning longer horizon greater conﬁdence discounted prelude primary agent introduce simpler one serf didactic purpose simper agent plan ﬁxed eﬀective horizon τ designed operate ﬁxed duration variable required input instantiating agent particular agent executes algorithm 1 discounted q learning eﬀective horizon τ prescribes discount factor γ 1 agent start initial situational state timestep agent increment visitation count n computes next situational state f update prediction q via discounted iteration discount factor γ 1 update line 11 adjusts action value response temporal diﬀerence two element update warrant discussion one step size α given 1 n step size sequence adapted used jin et al 2018 ha number desirable property established lemma particular property ensure estimation error not accumulate exponentially agent update action value second key element optimistic boost added temporal diﬀerence given p n term injects optimism ensure prediction likely optimistic term dominating qγ number visit n pair increase uncertainty around prediction decrease reﬂected denominator p n let b πτ policy implemented agent executes algorithm 1 eﬀective planning horizon τ operation duration let regret relative reference policy π experienced agent timesteps denoted regretτ π eb πτ x e 17 recall τπ h reward averaging time policy π max sup φ h qγ h inf φ h qγ h 17 dong van roy zhou algorithm 1 discounted q learning input initial situational state f situational state update function r reward function τ eﬀective planning horizon duration operation 1 γ 2 β 4 p log 2 3 4 q 5 1 2 6 unif q 7 execute action register observation 8 n 1 9 α n 10 11 q α r γ q β n 12 q q τ 13 14 end γ 1 distortion introduced predicting optimal value eﬀective horizon τ based situational state instead history following regret bound theorem 3 τ π regretτ π p sat log 2 h sa 5 2 log algorithm 1 requires eﬀective planning horizon τ duration operation input establish section regret bound sophisticated agent doe not require τ input agent relaxes need parameter operating eﬀective planning horizon increase time growing horizon rather targeting ﬁxing duration operation eﬀective planning horizon done algorithm 1 discounted q learning design agent operates eﬀectively any duration planning growing horizon study primary agent executes algorithm 2 growing horizon q learning accomplish agent instantiated only three input initial situational state situational state update function reward function worth noting agent interacts environment single stream experience no reset reinitialization situational state update line 14 look identical 18 simple agent complex environment algorithm 1 eﬀective horizon τ thus discount factor γ optimism coeﬃcient β change time algorithm 2 growing horizon q learning input initial situational state f situational state update function r reward function 1 2 q n 3 1 2 4 τ 5 β 6 q 7 n 8 unif q 9 execute action register observation 10 n 1 11 α n 12 13 γ 14 q α r γ q β n 15 q q τ 16 17 end algorithm call subroutine govern evolution eﬀective planning horizon τ optimism coeﬃcient β suitably adjust action value q visitation count n tandem change τ particular prescribes eﬀective planning horizon prescribes optimism coeﬃcient increase action value remain optimistic eﬀective planning horizon increase deemphasizes le recent temporal diﬀerences based tially diﬀerent discount factor sequence change point beginning 1 continuing tk 20 k 1 2 3 underlie function specify function helpful deﬁne notation recent change point time particular index recent change point given kt max k tk 0 0 recent change point tkt ﬁrst function provides eﬀective planning horizon τ kt 18 19 dong van roy zhou optimism coeﬃcient β similarly updated changed point according kt q log 2 kt 19 note kt 1 optimism coeﬃcient scale eﬀective planning horizon raised power time logarithmic term ensure action value remain optimistic incremented amount eﬀective horizon accomplished kt 20 finally simplify analysis reset count change point multiplying 1 tkt 21 count becomes one upon next visit any pair resulting step size α 1 replaces action value temporal diﬀerence eﬀectively forcing agent forget experience preceding change point important note choice designed facilitate analysis rather produce eﬀective agent discus alternative choice next section may improve performance denote b π policy executed algorithm 2 subroutine speciﬁed recall regretπ regret experienced b π relative reference policy π maximum distortion eﬀective horizon equal exceeding reward averaging time τπ following theorem main theoretical result paper theorem 4 π regretπ 120 p sa log 2 18 log 5 algorithm 2 viewed operating sequence episode delineated change point eﬀective planning horizon optimism coeﬃcient ﬁxed thought instantiating applying 1 episode despite theorem 4 not follow directly theorem reason latter applies agent begin empty history whereas agent instantiated some change point doe not theorem 3 appendix bridge gap oﬀering generalization theorem 3 applies agent starting arbitrary history h long situational state initialized φ h two corollary theorem 4 facilitate interpretation implication ﬁrst terizes regret relative reference class pτ consists policy reward averaging time not exceed since regretpτ regretπ following corollary 20 simple agent complex environment corollary 5 τ regretpτ 120 p sa log 2 18 log 5 22 follows corollary ϵ 0 τ some polynomial poly agent attains average reward within ϵ λπ within τ 5 poly timesteps hence within time scale τ 5 agent attains average reward competitive any policy reward averaging time also implicit observation time agent becomes competitive policy require longer time evaluate second corollary bound regret relative optimal average reward follows theorem 4 next lemma consequence corollary van roy 2006 lemma 6 τ π applying lemma 6 taking π π arrive following corollary theorem corollary 7 regretp 120 p sa log 2 π πt 18 log 5 π 23 corollary conveys another intriguing property result agent approach asymptotic performance time scale τ 5 particular time doe not depend reward averaging time optimal policy complex environment could intractably large even inﬁnite dependence instead reward averaging time π determined situational rather environment state dynamic scheduling scheme function prescribe schedule adjusting eﬀective planning horizon optimism coeﬃcient value count function particular choice speciﬁed previous section part algorithm 2 designed facilitate regret analysis indeed irregular structure abrupt adjustment occurring particular change point wa introduced solely simplify analysis partitioning stream episode natural choice involving smooth schedule may substantially improve realized performance satisfying similar improved regret bound rate eﬀective horizon grows time play important role rate may onerously slow requiring long time develop plan span reasonable horizon illustrate importance schedule let u revisit service rate control example section simulation result reported section demonstrated capability optimistic improve performance time made use particular smooth 21 dong van roy zhou schedule log note may beneﬁcial modify rate planning horizon grows purpose current study retain rate only tune aspect schedule figure 6 compare result reported section schedule algorithm plot represents average two hundred simulated trajectory latter agent eventually improves performance requires long time due impractical schedule figure 6 performance algorithm 2 original schedule versus improved smooth schedule may surprising algorithm 2 performs poorly despite satisfying regret bound previous section indeed common mathematical result eﬃcient reinforcement learning regret bound tend weak typically not oﬀer accurate prediction realized performance given level inaccuracy not oﬀer precise guidance agent design however bound analysis lead useful developing qualitative understanding insight discussed earlier section closing remark presented studied simple agent general interface interacts single stream experience result bound regret realized agent bound bear implication asymptotic performance rate agent 22 simple agent complex environment approach level performance importantly bound not depend number environment state mixing time one interesting insight emerges involves relation agent eﬀective planning horizon duration past experience agent plan eﬀectively horizon grows number direction result work strengthened extended discus section agent us particularly simple representation action value function comprised ﬁxed prespeciﬁed situational state update function lookup table situational state agent adapt situational state update function based agent experience generalize situational state typically using neural network instead lookup table extension allow much larger situational state space greatly improve performance agent analyze discard previous experience whenever eﬀective planning horizon increased impractical done only facilitate analysis ought possible analyze variation gradually phase inﬂuence past data along line discussed section maximize average reward may natural agent learn diﬀerential value function directly studied wan et al 2020 rather discounted value function doe agent however open issue whether agent explore environment eﬃciently believe problem worth investigation finally suspect term regret bound reﬂects rate agent approach asymptotic performance not fundamental improved better agent design nuanced analysis note dependence stem subroutine 18 21 agent us adjust planning horizon mentioned section setting induce eﬀective planning horizon grows suggesting take time τ 5 plan eﬀectively horizon length conjecture exists environment any agent requires time τ 3 would translate instead term regret lower bound lower bound could shed light limit learning could oﬀer useful insight agent designer long agent ought plan given duration past experience acknowledgement financial support army research oﬃce aro grant national science foundation grant digital twin research grant bain company gratefully acknowledged shi dong wa partly supported herb jane dwight stanford graduate fellowship zhengyuan zhou acknowledges generous support new york university spring 2022 center global economy business faculty research grant thank michael harrison satinder singh john tsitsiklis xiuyuan lu stimulating discussion helpful feedback also thank alex cloud pointing mistake previous version paper 23 dong van roy zhou appendix probabilistic framework appendix deﬁne probabilistic framework notation deﬁne random quantity respect probability space ω f p probability event f denoted p f event f g p g 0 probability f conditioned g denoted p random variable function set outcome ωas domain random variable z p z denotes probability event z lie within set probability p z event f conditioned event z z take value rk ha density pz though p z z 0 z conditional probability p z whenever pz z 0 denoted p z ﬁxed f function denote value evaluated z z p random variable even p z some z p problematic event occur zero probability possible realization z probability p z z z z function denote value function evaluated z p z note p z random variable depends random variable z possible realization z probability p z conditioned z z function z evaluating function z yield random variable denote p particular random variable appear routinely throughout paper one environment e ρ deterministic set deﬁne interface observation probability function ρ random variable randomness reﬂects agent designer epistemic uncertainty environment often consider probability p event f conditioned environment main result involve upper bounding random variable form e function establish e probability one suﬃcient argument environment e distribution e ha positive mass e e e appendix c reader encounter notation like e e may helpful think accompanying quantiﬁer distribution e e enjoys positive policy π assigns probability π action history policy π random variable aπ 0 oπ 1 aπ 1 oπ 2 represent sequence interaction erated selecting action according particular hπ aπ 0 oπ 1 oπ denoting history interaction time p aπ π aπ p oπ aπ e ρ oπ aπ shorthand generally suppress superscript π instead indicate policy subscript example pπ p aπ π aπ pπ e p oπ aπ e ρ oπ aπ dependence π extends algorithmic state zπ situational state sπ epistemic state p π use convention suppress superscript appropriate 24 simple agent complex environment figure 7 customer arrival probability decreasing function maximum service time experienced among recent dozen customer served expressing expectation use subscripting notation probability example expectation reward rπ r sπ aπ oπ conditioned ment e state sπ action aπ written e rπ sπ aπ eπ st much paper study property interaction speciﬁc policy b clear context suppress superscript subscript indicate example ht hb π ab π ob π p pb π b π appendix service rate control example appendix supplement discussion section particular provide precise characterization environment dynamic establish two baseline agent described section not deviate slow mode service also present third sophisticated baseline agent establish even doe not learn deviate slow mode environment dynamic service station initially vacant customer may arrive starting end ﬁrst timestep time arrival probability depends maximum service time experienced among recent 12 customer served denote statistic wt initialize customer arrival probability decrease wt increase illustrated figure particular conditioned wt probability customer arrives time service station vacant pt choice service mode impact service time fast mode service always completed single timestep slow mode service completed 25 dong van roy zhou next timestep probability observation probability conditioned st 0 given departure arrival 0 pt 0 1 conditioned st 1 fast departure arrival pt 0 1 0 slow departure arrival 0 1 easy verify average reward maximized agent applies fast mode service every timestep policy minimizes service time customer waiting precisely one timestep consequently policy wt converges 1 doe arrival probability average reward therefore 1 analysis baseline agent study performance two baseline agent introduced section well sophisticated variant recall agent time apply policy πϵ selects fast mode some probability ϵ vary time agent begin knowledge service completion probability 1 slow fast mode respectively throughout discussion let random variable sampled distribution wt pt respectively let cϵ service completion probability any timestep customer served policy πϵ particular cϵ ϵ 1 2 1 2 average reward λπϵ given λπϵ proﬁt per customer mean interarrival time 1 ϵ cϵ 1 cϵ 1 eπϵ 24 ﬁrst agent applies only deviate warranted observing data long duration assuming arrival probability ﬁxed whether decides increase ϵ depends arrival probability estimate policy customer service time 1 probability otherwise least since wt largest among 12 service time 1 e e consequently e h e keep thing simple suppose agent estimate arrival probability exactly e agent estimate average reward πϵ ˆ λπϵ proﬁt per customer mean interarrival time 1 1 cϵ 1 cϵ 1 1 1 25 26 simple agent complex environment note diﬀerence equation 25 equation 24 due ﬁrst agent use arrival probability result rather πϵ since e 1 ˆ λπϵ strictly decreasing ϵ 0 1 agent doe not deviate slow mode second agent additionally try small value ϵ 0 some duration order estimate derivative average reward ϵ derivative positive increase show derivative negative hence second agent doe not deviate slow mode one easily check positive integer w ϵ 0 1 pπϵ e 1 2 26 recalling cϵ 2 following equation 24 λπϵ 1 1 cϵ 1 cϵ 1 eπϵ g ϵ 1 g ϵ 1 ϵ 27 g ϵ eπϵ e pπϵ w e consequently taking derivative respect ϵ evaluating ϵ 0 yield dλπϵ dϵ dϵg ϵ 0 g 0 2 g 0 1 28 second agent not deviate finally even consider third agent similar second agent except additionally estimate second derivative chooses ϵ maximize taylor expansion λπϵ around ϵ 0 subject constraint 0 agent end always selecting slow mode service even exploiting information suggests staying best thing see evaluate second derivative λπϵ ϵ 0 yielding 1 2 using polynomial extrapolation one would get λπϵ dϵ 1 2 0 thereby yielding strictly smaller value ϵ 0 1 summary ﬁrst baseline agent represents might produced conservative designer demand see empirical evidence justifying fast service ever trying second agent representative approach used policy gradient literature discussed sutton barto 2018 reference therein third agent pursues sophisticated approach entailing estimation use second derivative addition gradient per analysis given three agent end choosing policy hence perform poorly relative optimistic agent adapts action value qt st predict future return select action 27 dong van roy zhou appendix proof proof lemma 2 lemma restated lemma 8 π h γ 0 1 v γ π h λπ prove lemma recall ﬁxed policy π discount factor γ 0 1 history h v γ π h x γt πrπ h 29 simplicity let ℓ πrπ h v γ π h x γℓrℓ 30 deﬁnition τπ τπ ℓ x 31 hence v γ π h λπ 1 x x 1 ℓ x x 1 ℓ x x 1 τπ τπ desired result property learning rate subsection generalize useful lemma jin et 2018 property learning rate let αi k αi k 1 1 k 32 28 simple agent complex environment αℓ 1 2 learning rate sequence 1 k 1 k 0 33 naturally pk αi k also following lemma 9 k 1 k αi k 2 k b k k αi k k pk αi k 2 k c αi k 1 1 proof proof lemma jin et 2018 cover case h positive integer note proof part b also applies h thus left u showing part c hold real number h end ﬁrst establish positive real number b b x j b j 34 fact show induction positive integer ℓthat b ℓ x j b j b ℓ j b j 35 1 b b 1 b 1 b 1 b 1 b 1 36 hence 35 hold suppose 35 hold b x j b j b ℓ x j b j j b j b ℓ j b j j b j 37 b ℓ j b j 1 b b 1 b j b j 38 29 dong van roy zhou 37 follows induction hypothesis thus 35 also hold 1 concluding induction following 35 b x j b j lim b x j b j lim b ℓ j b j 39 however log ℓ j b j ℓ x log 1 b j ℓ x b b j 40 side go implying lim ℓ j b j 0 41 thus 34 follows 39 x αi k h 1 h x j h j 1 h 1 h x k j h j h 1 h 1 h h 1 h 1 1 42 claimed part c regret analysis discounted agent section focus discounted variant agent section throughout section assume discount factor γ 1 0 1 ﬁxed also consider hypothetical setting history start arbitrary h agent start situational state φ h since every time agent change discount factor environment history not reset main goal demonstrate result hold regardless initial history long agent start corresponding agent state order simplify notation section always condition environment generic ﬁxed environment e ρ p e e e shortened p e respectively let h ﬁxed possibly history φ h also omit superscript γ value 30 simple agent complex environment algorithm 3 discounted subroutine 1 input f r γ β 2 initialize history h 3 0 h 4 q n 5 v q 6 true 7 unif arg q 8 n 1 9 α 10 execute action register observation 11 12 q 1 q α h r γ v β n 13 q n q 1 14 v q 15 1 16 end function v γ qγ v γ π qγ reader keep mind value function section respect discount factor speciﬁcally consider algorithm 3 identical algorithm 1 except initial history arbitrary use initialize q let history trajectory algorithm 3 ht ot 1 2 43 let st φ ht rt r ot 1 2 44 also let vt value situational state timestep immediately update q 1 q α rt γ v st β p n 45 q q 1 1 46 v q 47 31 dong van roy zhou let vt h shorthand vt φ h similarly qt qt h deﬁned note since action selected greedily qt ht max qt ht vt ht 48 finally let p transition operator function g h pair h pg h x g 49 let π policy corresponding algorithm recall distortion respect eﬀective planning horizon τ deﬁned max sup φ h h inf φ h h 50 avoid cluttering γ 0 1 simply use represent τ 1 aim show following result theorem 10 algorithm 3 executed γ 0 1 β 4 1 p log 2 51 some ι h ι 1 h 1 1 52 initial history h π x ht π ht 24 1 5 2 p sat log 2 3 1 sa 3 1 2 53 max ι regret decomposition subsection prove following lemma decomposes side 53 pave way analysis lemma 11 π x ht π ht 1 1 π x ht ht 1 1 2 54 32 simple agent complex environment proof π x ht π ht π x vt ht π ht π x vt ht ht 55 taking closer look ﬁrst term side since π greedy respect vt π x vt ht π ht π x vt ht π ht π x vt ht ht π x ht π ht π x vt ht ht γ π x π 56 combining 55 56 1 π x ht π ht π x ht ht ht π ht 57 dividing side 1 considering ht π ht 1 arrive 54 simplicity let χk vk hk hk 1 58 ξk qk hk ak hk ak 59 k using notation 54 written equivalently π x ht π ht 1 1 π x 1 2 1 1 2 60 33 dong van roy zhou establishing subsection show timestep value function vt almost optimistic uniformly across history following result lemma 12 algorithm 3 executed γ 0 1 βδ 4 1 r log δ 61 some ι qγ h ι 1 h 1 1 62 probability least 1 h 0 vt h h 1 qt h h 1 max ι proof moment let u ﬁx h let ˆ qk h ha updated k time ˆ 1 initial value k 1 2 let tk timestep h updated note φ htk φ h atk 63 update rule algorithm 3 n ˆ qn n x αi n γ vti βδ 64 34 simple agent complex environment thus n ˆ qn h n x αi n γ vti βδ h n x αi n γ vti βδ n x αi n hti ati 65 n x αi n γ vti hti ati n x αi n βδ 66 n x αi n vti n x αi n hti ati βδ 67 65 follows 63 φ hti φ h h hti 68 66 follows fact hti ati γ hti ati 69 consider following sequence gk k x αi n hti ati k 1 n 70 k e e αk n htk atk e e αk n htk atk htk atk 0 71 implying gk k 0 n martingale result follows hoeﬀding inequality probability least 1 gn n x αi n hti ati 4 1 1 r log 2 δ 72 35 dong van roy zhou used assertion b lemma 9 fact hti ati 1 1 73 scaling δ applying union bound probability least 1 simultaneously h n long h updated not n time timesteps 1 2 n x αi n hti ati 4 1 1 r log δ 74 denote event recall choose βδ 4 1 r log δ 75 result following 67 conditioned event e ˆ qn n x αi n 76 show desired result induction assume event e occurs 0 requirement obviously h h 1 h suppose result hold h long h updated n time timesteps 1 2 76 h h γ n x αi n γ 1 1 77 otherwise h not updated timesteps 1 2 h h h 1 lead h h min max h 1 1 h 1 h therefore result hold 0 note one direct implication lemma 12 χt 0 1 36 simple agent complex environment bound subsection prove following lemma lemma 13 algorithm 3 executed γ βδ speciﬁed lemma 12 probability least 1 x 1 sa 1 1 24 1 r sat log δ ξt χt deﬁned 59 58 respectively proof moment let u ﬁx 0 1 consider situational pair ht ha updated n time including timestep let 1 tn timestep ht updated recall φ hti φ ht ati 1 78 conditioned event e qt ht ht n x αi n γ vti βδ ht n x αi n h γ vti ati n x αi n βδ 79 γ n x αi n h vti ati 80 γ n x αi n h vti γ n x αi n h ati γ n x αi n h vti 81 79 follows fact φ hti φ ht ht hti 82 37 dong van roy zhou inequality 80 follows assertion lemma 9 81 follows conditioned event e n x αi n h ati 83 combining 48 81 vt ht ht vt ht ht qt ht ht γ n x αi n h vti 84 1 n φ φ hti vti vti vti 85 otherwise φ φ hti φ not updated timestep ti 1 leading vti 86 combining two case claim exists 1 wn 1 vt ht ht qt ht ht γ n x αi n h vwi 2 87 note 87 only applies time ha updated least suppose ht ﬁrst updated time 1 meaning ha not updated even prior timestep naturally vt ht ht 1 1 88 recall n number time value ht updated including timestep let take value 0 1 replace n wi nt wt respectively reﬂect dependence summing side 87 0 considering 88 x vt ht ht x qt ht ht γ x nt x αi nt h vwt 2 sa 1 x 89 38 simple agent complex environment note sa many ﬁrst want determine whether vt ht ht appears summation side 89 based 85 86 appears must nt 90 meaning situational pair ht ha never visited 0 1 lead vt ht ht 1 therefore claim x vt ht ht γ x nt x αi nt h vwt 2 sa 1 1 x 91 wt shown conditioned event e χk inequality 89 implies x ξt γ x nt x αi nt χwt 1 2 sa 1 1 x γ x nt x αi nt χwt 2 1 sa 1 1 x 92 examining ﬁrst term side 92 lemma 9 c x nt x αi nt χwt 2 x χt 93 term last term 92 letting nt number time situational pair updated time 1 x 1 x x nt x 1 x x 2 p nt 2 sa x x nt 2 sat 94 ﬁnal step used fact x x nt 39 dong van roy zhou revisit 92 starting x ξt γ x nt x αi nt χwt 2 1 sa 1 1 x γ 3 2 x χt 2 1 sa 1 1 sat 95 equivalently x 3 2 x χt 2 1 sa 1 1 sat 96 recall conditioned event e χt γ 3 2 0 0 1 thus drop ﬁrst term 96 deduce probability 1 x 1 sa 1 1 24 1 r sat log δ 97 finishing proof theorem 10 remains show bound lemma 13 also implies bound expected sum ξt δ 0 let eδ denote event 74 ξt ht ht 1 1 1 98 e x 1 ec 1 eδ δt 1 99 therefore e x e x 1 e e x 1 ec 24 1 r sat log δ 2 1 sa 1 δt 1 100 letting δ β 4 1 p log 2 101 40 simple agent complex environment e x 24 1 p sat log 2 2 1 sa 2 1 102 plugging inequality 60 arrive π x ht π ht 24 1 5 2 p sat log 2 2 1 2 1 1 2 sa 2 1 2 1 1 2 24 1 5 2 p sat log 2 3 1 sa 3 1 2 concludes proof theorem 10 discounted return average reward section still continue study discounted subroutine algorithm 3 shift focus learning performance respect average reward goal show following stronger version theorem theorem 14 τ τ log algorithm 3 executed γ 1 β log 2 103 some ι qγ h h 104 initial history h π x p sat log 2 3 h h sa 5 2 log τ 105 max ι proof first notice π h v γ π ht e x 106 41 dong van roy zhou thus e x v γ ht γ π ht e x 1 x x 107 e x x e x 1 1 e x 1 1 108 107 result lemma notice since e x 1 1 x 1 1 1 γ 1 1 1 2 109 let γ 0 1 γ 0 γt log e 1 110 therefore e x γ 0 1 1 e x γ 0 1 1 x γ 0 1 1 1 e x γ 0 x γ 0 1 1 1 e x γ 0 1 1 1 1 e x γ 0 1 1 111 42 simple agent complex environment hand e γ 0 x 1 1 1 1 γ 0 1 2 112 theorem 10 also e x v γ ht γ π ht e x v γ ht γ π ht p sat log 2 3 τ h sa 3 τ 2 113 note since γ 1 combining 108 113 τ e x γ 0 p sat log 2 3 τ h sa 3 τ 2 h log 1 τ 2 τ 114 τ log γ 0 e x p sat log 2 3 h sa 3 τ h log 1 τ 1 γ 0 p sat log 2 3 h sa 5 τ log τ 115 claim theorem 14 proof lemma 6 lemma restated lemma 15 τ π fact ﬁxed τ let γ 1 γ 1 long φ φ v 116 theorem van roy 2006 ϵ 0 γ 1 exists policy ϵ p distribution µϵ h h µϵ h 0 117 43 dong van roy zhou 1 x µϵ h v h ϵ h ϵ 118 taking limit noticing π h lim 1 v π h λπ 119 arrive lim sup h ϵ h ϵ 120 mean ι 0 exists γι γ 1 λπγι h πγι ϵ h ϵ ι 121 since πγι ϵ π inequality implies ϵ ι 0 λπγι h π h ϵ ι 122 notice following result blackwell 1962 γι λπγι h h hence take ϵ ι give u h π h 123 desire concluding proof theorem 4 section complete ﬁnal step towards theorem 4 restate theorem 4 π regretπ 120 p sa log 2 18 log 5 set subroutine speciﬁed 18 21 interaction agent environment viewed epoch k 1 2 epoch corresponding executing algorithm 3 ﬁxed ﬁxed discount factor γk 1 k equivalent ﬁxed eﬀective planning horizon τk k tk change point 1 tk 20 k previous subsection analysed regret algorithm 3 discount factor ﬁxed subsection allow discount factor change use doubling trick bound total regret b throughout section assume total number timesteps ﬁxed start following useful lemma 44 simple agent complex environment lemma 16 let 0 b integer ζ 0 1 f f x consider sequence tj j 0 1 let k minimum index tk k f f tk sζ 124 proof first notice tk 125 based deﬁnition 1 126 hence pk tk tk contradicting minimality since pk tk k 127 therefore f f tk k x tζ k 2 ζ sζ 128 desired light requirement algorithm 3 also need following result lemma 17 0 1 h 1 1 h 1 1 129 proof 1 2 let πi h πi uniform distribution arg qγi h proposition 1 πi 1 2 exists qγi h rh x x πirπi 1 2 h 130 result h h h h x x 2 1 x x 2 1 1 1 1 1 1 1 1 131 45 dong van roy zhou desire lemma together lemma 12 subroutine deﬁned 20 ensures beginning epoch k q h h τk 0 1 h 132 let tk denote timesteps epoch k 0 1 l l index epoch contains timestep last epoch concerned 0 1 19 20 21 39 40 41 79 tl 20 20 1 letting length epoch tk 1 20 ﬁxed reference policy π let rπ tk e e 133 k let γk 1 1 1 1 134 discount factor used epoch 0 discount factor epoch theorem 14 either rπ tk k p log h h sa 2 log 5 τk 24 p sa log 2 τπ h 2 log 1 5 135 τk rπ tk 136 let ik 1 τk τπ 0 otherwise 137 46 simple agent complex environment let g 24 p sa log 2 4 5 2 log 1 5 138 arrive regretπ l x rπ tk l x 1 ik 0 g 1 ik 1 l x 1 τk g 1 τk τπ l x 1 5 π g 1 τ 5 π 5 π l x g 1 τ 5 π 139 lemma 16 l x g 1 τ 5 π 120 p sa log 2 4 5 18 log 1 5 therefore regretπ 120 p sa log 2 4 5 18 log 1 5 5 π 140 justiﬁes claim theorem 4 appendix result approximate dynamic programming theorem 18 exists environment ρ set situational state situational state update function f reward function r conditioned e π 141 proof consider environment two action 1 2 two observation 0 1 observation probability given ρ 1 h 0 otherwise 142 47 dong van roy zhou ρ 1 143 word observation deterministically 1 only agent take diﬀerent action one took previous timestep observation always 0 ﬁrst timestep only one situational state 1 situational state update function f 1 144 reward equal observation r 145 notice 1 environment attained alternating two action timestep let environment claim conditioned e 1 γ 0 1 since only one situational state only verify history pair action 1 2 qγ 146 indeed history h action qγ h 1 h γ otherwise 147 since qγ h only take two value 146 obviously hold thus 1 148 however only two policy p one always taking action 1 always taking action either policy result reward sequence implying π thus π 1 149 desire next show intuitive learning algorithm may generate policy whose performance much worse π happens even environment dynamic known learning algorithm proceeds follows iteration k algorithm independently sample k 1 k according uniform distribution value function updated via v k min v x v k n rao k γ p k 150 48 simple agent complex environment γ 0 1 ﬁxed discount factor note environment dynamic function ρ known algorithm able compute r p otherwise ha use ﬁtted value r p 150 algorithm version ﬁtted value iteration algorithm loss function studied munos ari 2008 worth mentioning general ﬁtted value iteration need not converge however since update rule 150 amonts ﬁtted value iteration aggregated state sequence k k 0 1 2 always converges probability v per munos ari 2008 using example adapted van roy 2006 following result theorem 19 value function updated via 150 γ 0 1 ϵ 0 exists integer environment e conditioned e e τ π greedy respect v γ 151 proof integer n consider mdp state 1 2 two action 1 2 note mdp viewed environment en ρ 1 2 1 2 observation corresponding mdp state simply call state hereafter ρ depends history only recent observation observation state induce equivalence class set history h h only last observation henceforth use observation denote equivalence class h induced also not distinguish observation every timestep probability system reset next state drawn uniformly 1 2 conditioned system doe not reset state 3 5 system transition deterministically state 1 state 4 6 system transition deterministically state 2 regardless action state 1 system transition state 2 probability action state 2 system transition state 1 deterministically action 1 stay state 2 deterministically action environment dynamic ρ thus written ρ 1 1 1 1 3 5 2 1 0 otherwise ρ 1 1 1 4 6 2 2 0 otherwise ρ 3 4 5 49 dong van roy zhou let δ κ every state 4 6 agent receives reward δ take action 1 reward take action every state 3 5 agent receives reward regardless action additionally agent also receives reward take action 2 state scenario agent receives zero reward situational state space 1 2 φ 1 whenever 1 3 φ 2 whenever 2 4 conditioned e en ﬁxed γ 0 1 verify qγ 1 0 1 2 qγ k 3 5 1 2 also qγ 2 1 0 qγ 2 2 qγ k 1 δ qγ k 2 4 6 since state 1 3 mapped situational state 1 state 2 4 mapped situational state 2 δ τ optimal average reward environment en 0 attained applying action 1 every state consider p chooses action 1 situational state 1 action 2 situational state apparently addition van roy 2006 established whenever κ 1 1 exists n π thus 152 since environment 1 follows τπ γ γ γδ 1 153 therefore ϵ 0 choose κ 1 154 allows 151 finally would like provide brief discussion dependence distortion factor van roy 2006 shown environment mdp 50 simple agent complex environment consider solution version approximate value iteration take account invariant distribution appropriate policy greedy policy respect solution always attains optimal dependence distortion speciﬁcally projection approximate value iteration ha incorporate invariant distribution greedy policy respect current value however approximate value iteration planning algorithm value pair mdp updated simultaneously come learning algorithm value update depend pair visited timestep learning constitutes core agent implicitly carry projection reader interested connection refer section 8 van roy 2006 set lecture slide value iteration aggregated state russo 2020 reference abbas abdolmaleki jost tobias springenberg yuval tassa remi munos nicolas heess martin riedmiller maximum posteriori policy optimisation international conference learning representation alekh agarwal sham kakade akshay krishnamurthy wen sun flambe tural complexity representation learning low rank mdps arxiv preprint mohammad gheshlaghi azar ian osband emi munos minimax regret bound reinforcement learning international conference machine learning page pmlr peter l bartlett ambuj tewari regal regularization based algorithm forcement learning weakly communicating mdps arxiv preprint dimitri p bertsekas abstract dynamic programming athena scientiﬁc david blackwell discrete dynamic programming annals mathematical statistic page mayank daswani peter sunehag marcus hutter forcement learning asian conference machine learning page pmlr mayank daswani peter sunehag marcus hutter et al feature reinforcement learning state art sequential big data paper workshop association advancement artiﬁcial intelligence daniela pucci de farias benjamin van roy linear program cost approximate dynamic programming performance guarantee mathematics operation research 31 3 2006 51 dong van roy zhou geoﬀrey j gordon stable function approximation dynamic programming machine learning proceeding 1995 page elsevier marcus hutter universal artiﬁcial intelligence sequential decision based algorithmic probability springer science business medium tommi jaakkola michael jordan satinder p singh convergence stochastic iterative dynamic programming algorithm neural computation 6 6 mehdi rahul jain ashutosh nayyar online learning unknown partially observable mdps arxiv preprint thomas jaksch ronald ortner peter auer regret bound ment learning journal machine learning research 11 4 nan jiang alex kulesza satinder singh richard lewis dependence eﬀective planning horizon model accuracy proceeding 2015 international conference autonomous agent multiagent system page citeseer nan jiang akshay krishnamurthy alekh agarwal john langford robert e schapire contextual decision process low bellman rank international conference machine learning page pmlr chi jin zeyuan sebastien bubeck michael jordan provably eﬃcient arxiv preprint chi jin zhuoran yang zhaoran wang michael jordan provably eﬃcient reinforcement learning linear function approximation conference learning theory page pmlr kyung jo lagrangian algorithm computing optimal service rate jackson queuing network computer operation research 16 5 ali devran kara serdar yuksel near optimality ﬁnite memory feedback policy partially observed markov decision process arxiv preprint michael kearns satinder singh reinforcement learning polynomial time machine learning 49 2 lihong li unifying framework computational reinforcement learning theory rutgers state university new brunswick lihong li thomas j walsh michael l littman towards uniﬁed theory state abstraction mdps isaim 4 5 xiuyuan lu benjamin van roy vikranth dwaracherla morteza ibrahimi ian osband zheng wen reinforcement learning bit bit arxiv preprint r andrew mccallum utile distinction reinforcement learning hidden state machine learning proceeding 1995 page elsevier 1995 52 simple agent complex environment volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc g bellemare alex graf martin riedmiller andreas k fidjeland georg ostrovski et al control deep reinforcement learning nature 518 7540 ciamac c moallemi sunil kumar benjamin van roy approximate dynamic programming queueing network unpublished manuscript emi munos csaba ari bound ﬁtted value iteration journal machine learning research 9 5 oﬁr nachum shixiang gu honglak lee sergey levine representation learning hierarchical reinforcement learning arxiv preprint ian osband daniel russo benjamin van roy eﬃcient reinforcement learning via posterior sampling arxiv preprint ian osband benjamin van roy daniel j russo zheng wen et al deep exploration via randomized value function journal machine learning research 20 124 yi ouyang mukul gagrani ashutosh nayyar rahul jain learning unknown markov decision process thompson sampling approach arxiv preprint majid raeis ali tizghadam alberto reinforcement learning approach providing quality service arxiv preprint daniel russo online variant value iteration approximation via state gation pdf lecture slide julian schrittwieser ioannis antonoglou thomas hubert karen simonyan laurent sifre simon schmitt arthur guez edward lockhart demis hassabis thore graepel et al mastering atari go chess shogi planning learned model nature 588 7839 linn sennott stochastic dynamic programming control queueing system volume john wiley son francis song abbas abdolmaleki jost tobias springenberg aidan clark hubert soyer jack rae seb noury arun ahuja siqi liu dhruva tirumala nicolas heess dan belov martin riedmiller matthew botvinick maximum posteriori policy optimization discrete continuous control international conference learning representation url shaler stidham jr richard r weber monotonic insensitive optimal policy control queue undiscounted cost operation research 37 4 jayakumar subramanian amit sinha raihan seraj aditya mahajan approximate information state approximate planning reinforcement learning partially observed system journal machine learning research 2022 53 dong van roy zhou richard sutton andrew g barto reinforcement learning introduction john n tsitsiklis asynchronous stochastic approximation machine learning 16 3 john n tsitsiklis benjamin van roy method large scale dynamic programming machine learning 22 1 benjamin van roy performance loss bound approximate value iteration state aggregation mathematics operation research 31 2 yi wan abhishek naik richard sutton learning planning markov decision process arxiv preprint ruosong wang rus r salakhutdinov lin yang reinforcement learning general value function approximation provably eﬃcient approach via bounded eluder dimension advance neural information processing system christopher watkins learning delayed reward phd thesis christopher watkins peter dayan machine learning 8 richard r weber shaler stidham optimal control service rate network queue advance applied probability page wei mehdi jafarnia jahromi haipeng luo hiteshi sharma rahul jain reinforcement learning markov decision process international conference machine learning page pmlr ward whitt approximation dynamic program mathematics operation research 3 3 andrea zanette alessandro lazaric mykel kochenderfer emma brunskill learning near optimal policy low inherent bellman error international conference machine learning page pmlr zihan zhang xiangyang ji simon du reinforcement learning diﬃcult bandit algorithm escaping curse horizon arxiv preprint dongruo zhou quanquan gu csaba szepesvari nearly minimax optimal reinforcement learning linear mixture markov decision process conference learning theory page pmlr 2021 54