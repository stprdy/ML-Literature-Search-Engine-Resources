review conference paper iclr 2017 learning navigate complex environment piotr razvan fabio viola hubert soyer andrew ballard andrea banino misha denil ross goroshin laurent sifre koray kavukcuoglu dharshan kumaran raia hadsell deepmind london uk piotrmirowski razp fviola soyer aybd abanino mdenil goroshin sifre korayk dkumaran raia abstract learning navigate complex environment dynamic element tant milestone developing ai agent work formulate navigation question reinforcement learning problem show data efﬁciency task performance dramatically improved relying additional auxiliary task leveraging multimodal sensory input particular consider jointly learning reinforcement learning problem auxiliary depth prediction loop closure classiﬁcation task approach learn navigate raw sensory input complicated maze approaching performance even condition goal location change frequently provide detailed analysis agent ability localise network activity dynamic showing agent implicitly learns key navigation ability 1 introduction ability navigate efﬁciently within environment fundamental intelligent behavior whilst conventional robotics method simultaneous localisation mapping slam tackle navigation explicit focus position inference mapping dissanayake et 2001 follow recent work deep reinforcement learning mnih et 2015 2016 propose navigational ability could emerge agent learning policy maximizes reward one advantage intrinsic approach action not divorced representation rather learnt together thus ensuring feature present representation learning navigate reinforcement learning partially observable environment however pose several challenge first reward often sparsely distributed environment may only one goal location second environment often comprise dynamic element requiring agent use memory different timescales rapid memory goal location together short term memory subserving temporal integration velocity signal visual observation longer term memory constant aspect environment boundary cue improve statistical efﬁciency bootstrap reinforcement learning procedure augmenting loss auxiliary task provide denser training signal support representation learning consider two additional loss ﬁrst one involves reconstruction depth map time step predicting one input modality depth channel others colour channel auxiliary task concern geometry environment aimed encourage learning representation aid obstacle avoidance trajectory planning second task directly invokes loop closure slam agent trained predict current location ha previously visited within local trajectory equal contribution video illustrating navigation agent available 1 13 jan 2017 review conference paper iclr 2017 figure 1 view small 5 10 maze large 9 15 maze corresponding maze layout sample agent trajectory maze made public different texture visual cue well exploration reward goal shown right address memory requirement task rely stacked lstm architecture graf et 2013 pascanu et 2013 evaluate approach using ﬁve maze environment demonstrate accelerated learning increased performance proposed agent architecture environment feature complex geometry random start position orientation dynamic goal location long episode require thousand agent step see figure 1 also provide detailed analysis trained agent show critical navigation skill acquired important neither position inference mapping directly part loss therefore raw performance goal ﬁnding task not necessarily good indication skill acquired particular show proposed agent resolve ambiguous observation quickly localizes complex maze localization capability correlated higher task reward 2 approach rely learning framework incorporates multiple objective firstly try maximize cumulative reward using approach secondly minimizes auxiliary loss inferring depth map rgb observation finally agent trained detect loop closure additional auxiliary task encourages implicit velocity integration reinforcement learning problem addressed asynchronous advantage algorithm mnih et 2016 relies learning policy π θ value function v st θv given state observation policy value function share intermediate representation computed using separate linear layer topmost layer model agent setup closely follows work mnih et 2016 refer work detail use convolutional encoder followed either mlp lstm use action repetition entropy regularization prevent policy saturation detail also found appendix baseline consider work agent mnih et 2016 receives only rgb input environment using either recurrent purely model see figure b encoder rgb input used considered architecture 3 layer convolutional network support navigation capability approach also rely nav agent figure employ stacked lstm convolutional encoder expand observation agent include velocity action sampled stochastic policy immediate reward previous time step opt feed velocity previously selected action directly second recurrent layer ﬁrst layer only receiving reward postulate ﬁrst layer might able make association reward visual observation provided context second layer policy computed thus observation st may include image xt w h width 2 review conference paper iclr 2017 xt vt enc ᵨ ᬭ xt enc ᵨ ᬭ enc ᵨ ᬭloop l depth ff nav nav xt vt enc ᵨ ᬭ xt lstm depth figure 2 different architecture convolutional encoder followed feedforward layer policy π value function output b ha lstm layer c us additional input velocity reward action well stacked lstm ha additional output predict depth loop closure height image lateral rotational velocity vt previous action previous reward figure show augmentation nav different possible auxiliary loss particular consider predicting depth convolutional layer refer choice top lstm layer predicting loop closure l auxiliary loss computed current frame via single layer mlp agent trained applying weighted sum gradient coming gradient depth prediction multiplied gradient loop closure scaled βl detail online learning algorithm given appendix b depth prediction primary input agent form rgb image however depth information covering central ﬁeld view agent might supply valuable information structure environment depth could directly used input argue presented additional loss actually valuable learning process particular prediction loss share representation policy could help build useful feature rl much faster bootstrapping learning since know eigen et 2014 single frame enough predict depth know auxiliary task learnt comparison depth input versus additional loss given appendix c show signiﬁcant gain depth loss since role auxiliary loss build representation model not necessarily care speciﬁc performance obtained nature prediction care data efﬁciency aspect problem also computational complexity loss useful main task converge faster compared solving rl problem using le data sample additional computational cost minimal achieve use low resolution variant depth map reducing screen resolution explore two different variant loss ﬁrst choice phrase regression task natural choice formulation combined higher depth resolution extract information mean square error imposes unimodal distribution van den oord et 2016 address possible issue also consider classiﬁcation loss depth position discretised 8 different band band distributed pay attention object detail appendix b motivation classiﬁcation formulation greatly reduces resolution depth ﬂexible learning perspective result faster convergence hence faster bootstrapping image cropped subsampled lessen ﬂoor ceiling little relevant depth information 3 review conference paper iclr 2017 loop closure prediction loop closure like depth valuable navigating agent since used efﬁcient exploration spatial reasoning produce training target detect loop closure based similarity local position information episode obtained integrating velocity time speciﬁcally trajectory noted pt pt position agent time deﬁne loop closure label lt equal 1 position pt agent close position earlier time order avoid trivial loop closure consecutive point trajectory add extra condition intermediary position far pt threshold provide two limit learning predict binary loop label done minimizing bernoulli loss lt output output hidden representation ht last hidden layer model followed sigmoid activation 3 related work rich literature navigation primarily robotics literature however focus related work deep rl deep dqn breakthrough extremely challenging domain atari mnih et 2015 recent work ha developed rl method advantage use asynchronous training multiple agent parallel mnih et 2016 recurrent network also successfully incorporated enable state disambiguation partially observable environment koutnik et 2013 hausknecht stone 2015 mnih et 2016 narasimhan et 2015 deep rl ha recently used navigation domain kulkarni et al 2016 used feedforward architecture learn deep successor representation enabled behavioral ﬂexibility reward change mazebase gridworld provided mean detect bottleneck vizdoom zhu et al 2016 used feedforward siamese architecture incorporating pretrained resnet support navigation target discretised environment oh et al 2016 investigated performance variety network external memory weston et 2014 simple navigation task minecraft block world environment tessler et al 2016 also used minecraft domain show beneﬁt combining feedforward network learning resuable skill module cf option sutton et 1999 transfer navigation task tai liu 2016 trained convnet agent using depth channel input obstacle avoidance environment barron et al 2016 investigated well convnet predict depth channel rgb minecraft environment not use depth training agent auxiliary task often used facilitate representation learning suddarth kergosien 1990 recently incorporation additional objective designed augment representation learning auxiliary reconstructive decoding pathway zhang et 2016 rasmus et 2015 zhao et 2015 mirowski et 2010 ha yielded beneﬁts large scale classiﬁcation task deep rl setting however only two previous paper examined beneﬁt auxiliary task speciﬁcally li et al 2016 consider supervised loss ﬁtting recurrent model hidden representation predict next observed state context imitation learning sequence provided expert lample chaplot 2016 show performance dqn agent shooter game vizdoom environment substantially enhanced addition supervised auxiliary task whereby convolutional network wa trained task information presence enemy weapon provided game engine contrast contribution address fundamental question learn intrinsic sentation space geometry movement simultaneously maximising reward reinforcement learning method validated challenging maze domain random start goal location 4 experiment consider set maze deepmind lab environment beattie et 2016 see fig 1 visually rich additional observation available agent inertial 4 review conference paper iclr 2017 static maze small b static maze large c random goal random goal maze small e random goal maze large f random goal maze large different lation depth prediction figure 3 reward achieved agent 5 different task two static maze small large ﬁxed goal two static maze comparable layout dynamic goal result averaged top 5 random hyperparameters conﬁguration star label indicates use reward clipping please see text detail information local depth action space discrete yet allows ﬁnegrained control comprising 8 action agent rotate small increment accelerate forward backward sideways induce rotational acceleration moving reward achieved environment reaching goal random start location orientation goal reached agent respawned new start location must return goal episode terminates ﬁxed amount time expires affording agent enough time ﬁnd goal several time sparse fruit reward serve encourage exploration apple worth 1 point strawberry 2 point goal 10 point video agent solving maze linked appendix static variant maze goal fruit location ﬁxed only agent start location change dynamic random goal variant goal fruit randomly placed every episode within episode goal apple location stay ﬁxed episode end encourages strategy agent initially explore maze retain goal location quickly reﬁnd respawn variant static random goal consider small large map small maze 5 10 episode last 3600 timesteps large maze 9 15 10800 step see figure 1 rgb observation 84 environment see figure 1 right inspired classic used investigate navigation rodent olton et 1979 layout remains ﬁxed throughout agent spawn central corridor apple reward ha locate goal placed alcove one four arm goal hidden alcove optimal agent behaviour must rely memory goal location order return goal using direct route goal location constant within episode varies randomly across episode different agent architecture described section 2 evaluated training ﬁve maze figure 3 show learning curve averaged 5 top performing agent agent feedforward model ff recurrent model lstm stacked lstm version velocity previous action reward input nav nav depth prediction convolution layer nav nav depth prediction last lstm layer nav nav loop closure prediction nav well nav environment used paper publicly available 5 review conference paper iclr 2017 figure 4 left example depth prediction pair ground truth predicted depth sampled every 40 step right example loop closure prediction agent start gray square trajectory plotted gray blue dot correspond true positive output loop closure detector red cross correspond false positive green cross false negative note false positive occur agent actually square away actual loop closure auxiliary loss considered together nav case ran 64 experiment randomly sampled range detail please see appendix mean top 5 run well top 5 curve plotted expert human score established professional game player compared result nav agent reach performance static 1 2 attain 91 59 human score random goal 1 mnih et al 2015 reward clipping used stabilize learning technique employed work well unfortunately particular task yield slightly suboptimal policy agent doe not distinguish apple 1 point goal 10 point removing reward clipping result unstable behaviour base agent see appendix c however seems auxiliary signal depth prediction mediates problem some extent resulting stable learning dynamic figure nav v nav clearly indicate whether reward clipping used adding asterisk agent name figure also explores difference two formulation depth prediction regression task classiﬁcation task see regression agent nav mse performs worse one doe classiﬁcation nav result extends map therefore only use classiﬁcation formulation also see predicting depth last lstm layer hence providing structure recurrent layer not convolutional one performs better note some particular result learning curve figure 3 b consider feedforward model red curve versus lstm version pink curve even though navigation seems intrinsically require memory single observation could often ambiguous forward model achieves competitive performance static maze suggest might good strategy not involve temporal memory give good result namely reactive policy held weight encoder learning strategy motivates dynamic environment encourage use memory general navigation strategy figure 3 also show advantage adding velocity reward action input well impact using two layer lstm orange curve v red pink though agent nav better simple architecture still relatively slow train maze believe mainly due slower data inefﬁcient learning generally seen pure rl approach supporting see adding auxiliary prediction target depth loop closure nav black curve speed learning dramatically maze see table 1 auc metric ha strongest effect static maze accelerated learning also give substantial lasting performance increase random goal maze although place value task performance auxiliary loss report result loop closure prediction task 100 test episode 2250 step within large maze random goal 2 nav agent demonstrated successful loop detection reaching score sample trajectory seen figure 4 right exception nav agent figure us depth regression reward clipping doe worse include some analysis based agent 6 review conference paper iclr 2017 mean top 5 agent highest reward agent maze agent auc score human goal position acc latency 1 1 score ff 98 102 lstm 244 203 nav 266 252 nav 268 269 nav 258 251 static 1 ff 79 83 84 lstm 98 103 110 nav 119 125 122 nav 116 122 123 static 2 ff 81 47 111 lstm 153 91 155 nav 200 116 202 nav 192 112 192 random goal 1 ff 61 64 lstm 65 66 nav 96 91 91 nav 81 76 random goal 2 ff 69 77 lstm 57 nav 90 106 nav 103 59 109 nav 91 53 102 table 1 comparison four agent architecture ﬁve maze conﬁgurations including random static goal auc area learning curve score human averaged best 5 hyperparameters evaluation single best performing agent done analysis 100 test episode goal give number episode goal wa reached one time position accuracy classiﬁcation accuracy position decoder latency 1 1 average time ﬁrst goal acquisition average time subsequent goal acquisition score mean score 100 test episode 5 analysis position decoding order evaluate internal representation location within agent either hidden unit ht last lstm case ff agent feature ft last layer train position decoder take representation input consisting linear classiﬁer multinomial probability distribution discretized maze location small maze 5 10 50 location large maze 9 15 135 location ha 77 location note not backpropagate gradient position decoder rest network position decoder only see representation exposed model not change example position decoding nav agent shown figure 6 initial uncertainty position improved near perfect position prediction observation acquired agent observe position entropy spike respawn decrease agent acquires certainty location additionally video agent position decoding linked appendix complex maze localization important purpose reaching goal seems position accuracy ﬁnal score correlated shown table pure architecture still achieves accuracy static maze static goal suggesting encoder memorizes position weight small maze solvable agent sufﬁcient training time random goal 1 nav achieves best position decoding performance accuracy whereas ff lstm architecture approximately 50 opposite branch maze nearly identical exception sparse visual cue observe goal ﬁrst found nav agent capable directly returning correct branch order achieve maximal score however linear position decoder agent only accurate whereas plain lstm agent hypothesize symmetry induce symmetric policy need not sensitive exact position agent see analysis 7 review conference paper iclr 2017 figure 5 trajectory nav agent left nav random goal maze 1 right course one episode beginning episode gray curve map agent explores environment ﬁnds goal some unknown location red box subsequent respawns blue path agent consistently return goal value function plotted episode rise agent approach goal goal plotted vertical red line figure 6 trajectory nav agent random goal maze 1 overlaid position probability prediction predicted decoder trained lstm hidden activation taken 4 step episode initial uncertainty give way accurate position prediction agent navigates desired property navigation agent random goal task able ﬁrst ﬁnd goal reliably return goal via efﬁcient route subsequent latency column table 1 show nav agent achieve lowest latency goal goal ha discovered ﬁrst number show time second ﬁnd goal ﬁrst time second number average time subsequent ﬁnds figure 5 show clearly agent ﬁnds goal directly return goal rest episode random goal 2 none agent achieve lower latency initial goal acquisition presumably due larger challenging environment stacked lstm goal analysis figure 7 show show trajectory traversed agent four goal location initial exploratory phase ﬁnd goal agent consistently return goal location visualize agent policy applying tsne dimension reduction maaten hinton 2008 cell activation step agent four goal location whilst cluster corresponding four goal location clearly distinct lstm agent 2 main cluster nav agent trajectory diagonally opposite arm maze represented similarly given action sequence opposite arm equivalent straight turn left twice top left bottom right goal location suggests nav lstm maintains efﬁcient representation 2 rather 4 independent policy critical information currently relevant goal provided additional lstm investigating different combination auxiliary task result suggest depth prediction policy lstm yield optimal result however several auxiliary task concurrently introduced jaderberg et 2017 thus provide comparison reward prediction depth prediction following paper implemented two additional agent architecture one performing reward prediction convnet using replay buffer called nav one combining reward prediction convnet depth prediction lstm nav table 2 suggests reward prediction nav improves upon plain stacked lstm architecture nav not much depth prediction policy lstm nav combining reward prediction depth prediction nav yield comparable result depth prediction alone nav normalised average auc value respectively future work explore auxiliary task 8 review conference paper iclr 2017 agent trajectory episode different goal location b lstm activation agent c lstm activation nav agent figure 7 lstm cell activation lstm nav agent collected multiple episode reduced 2 dimension using tsne coloured represent goal location lstm nav agent shown navigation agent architecture maze nav nav nav nav nav nav static 1 static 2 random goal 1 random goal 2 table 2 comparison ﬁve navigation agent architecture ﬁve maze conﬁgurations random static goal including agent performing reward prediction nav nav reward prediction implemented following jaderberg et 2017 report auc area learning curve averaged best 5 hyperparameters 6 conclusion proposed deep rl method augmented memory auxiliary learning target training agent navigate within large visually rich environment include frequently changing start goal location result analysis highlight utility auxiliary objective namely depth prediction loop closure providing richer training signal bootstrap learning enhance data efﬁciency examine behavior trained agent ability localise network activity dynamic order analyse navigational ability approach augmenting deep rl auxiliary objective allows learning may encourage development general navigation strategy notably work auxiliary loss related jaderberg et 2017 independently look data efﬁciency exploiting auxiliary loss one difference two work auxiliary loss online current frame not rely any form replay also explored loss different nature finally focus navigation domain understanding navigation emerges solving rl problem jaderberg et al 2017 concerned data efﬁciency any whilst best performing agent relatively successful navigation ability would stretched larger demand placed rapid memory procedurally generated maze due limited capacity stacked lstm regard important future combine visually complex environment architecture make use external memory graf et 2016 weston et 2014 olton et 1979 enhance navigational ability agent whilst work ha focused investigating beneﬁts auxiliary task developing ability navigate deep reinforcement learning would interesting future work compare technique approach acknowledgement 9 review conference paper iclr 2017 would like thank alexander pritzel thomas degris joseph modayil useful discussion charles beattie julian schrittwieser marcus wainwright stig petersen environment design development amir sadik sarah york expert human game testing reference trevor barron matthew whitehead alan yeung deep reinforcement learning world environment deep reinforcement learning frontier challenge ijcai charles beattie joel leibo denis teplyashin tom ward marcus wainwright heinrich kãijttler andrew lefrancq simon green victor valdes amir sadik julian schrittwieser keith anderson sarah york max cant adam cain adrian bolton stephen gaffney helen king demis hassabis shane legg stig petersen deepmind lab arxiv url mwm gamini dissanayake paul newman steve clark hugh michael csorba solution simultaneous localization map building slam problem ieee transaction robotics automation 17 3 david eigen christian puhrsch rob fergus depth map prediction single image using deep network proc neural information processing system nip alex graf mohamed abdelrahman geoffrey hinton speech recognition deep recurrent neural network proceeding international conference acoustic speech signal processing icassp alex graf greg wayne malcolm reynolds tim harley ivo danihelka agnieszka nska sergio gómez colmenarejo edward grefenstette tiago ramalho john agapiou et al hybrid computing using neural network dynamic external memory nature matthew hausknecht peter stone deep recurrent partially observable mdps proc conf artiﬁcial intelligence aaai max jaderberg volodymir mnih wojciech czarnecki tom schaul joel leibo david silver koray kavukcuoglu reinforcement learning unsupervised auxiliary task submitted int l conference learning representation iclr jan koutnik giuseppe cuccu jãijrgen schmidhuber faustino gomez evolving neural network reinforcement learning proceeding annual conference genetic evolutionary computation gecco tejas kulkarni ardavan saeedi simanta gautam samuel gershman deep successor reinforcement learning corr url guillaume lample devendra singh chaplot playing fps game deep reinforcement learning corr url xiujun li lihong li jianfeng gao xiaodong jianshu chen li deng ji recurrent reinforcement learning hybrid approach proceeding international conference learning representation iclr url laurens van der maaten geoffrey hinton visualizing data using journal machine learning research 9 nov piotr mirowski marc aurelio ranzato yann lecun dynamic semantic indexing nip deep learning unsupervised learning workshop volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness et al control deep reinforcement learning nature volodymyr mnih lnech badia mehdi mirza alex graf timothy lillicrap tim harley david silver koray kavukcuoglu asynchronous method deep reinforcement learning proc int l conf machine learning icml 2016 10 review conference paper iclr 2017 arun nair praveen srinivasan sam blackwell cagdas alcicek rory fearon et al massively parallel method deep reinforcement learning proceeding international conference machine learning deep learning workshop icml karthik narasimhan tejas kulkarni regina barzilay language understanding game using deep reinforcement learning proc empirical method natural language processing emnlp junhyuk oh valliappa chockalingam satinder singh honglak lee control memory active perception action minecraft proc international conference machine learning icml david olton james becker gail e handelmann hippocampus space memory behavioral brain science 2 03 razvan pascanu caglar gulcehre kyunghyun cho yoshua bengio construct deep recurrent neural network arxiv preprint antti rasmus mathias berglund mikko honkala harri valpola tapani raiko learning ladder network advance neural information processing system nip steven c suddarth yl kergosien hint mean improving network performance learning time neural network pp springer richard sutton doina precup satinder singh mdps framework temporal abstraction reinforcement learning artiﬁcial intelligence 112 1 lei tai ming liu towards cognitive exploration deep reinforcement learning mobile robot arxiv url chen tessler shahar givony tom zahavy daniel mankowitz shie mannor deep hierarchical approach lifelong learning minecraft corr url tijmen tieleman geoffrey hinton lecture rmsprop divide gradient running average recent magnitude coursera neural network machine learning volume 4 van den oord kalchbrenner kavukcuoglu pixel recurrent neural network jason weston sumit chopra antoine bordes memory network arxiv preprint yuting zhang kibok lee honglak lee augmenting supervised neural network pervised objective image classiﬁcation proc international conference machine learning icml junbo zhao michaël mathieu ross goroshin yann lecun stacked int l conf learning representation workshop iclr url yuke zhu roozbeh mottaghi eric kolve joseph lim abhinav gupta li ali farhadi visual navigation indoor scene using deep reinforcement learning corr url 11 review conference paper iclr 2017 supplementary material video trained navigation agent show behaviour nav agent 5 video corresponding 5 navigation environment small static large static small random goal large random goal video show video actual input agent rgb image value function time fruit reward goal acquisition layout maze consecutive trajectory agent marked different colour output trained position decoder overlayed top maze layout b network architecture training online algorithm learning introduce class neural agent modular structure trained multiple task input coming different modality vision depth past reward past action implementing agent architecture simpliﬁed modular nature essentially construct multiple network one per task using shared building block optimise network jointly some module used perceiving visual input lstms used learning navigation policy shared among multiple task module depth predictor gd loop closure predictor gl navigation network output policy value function trained using reinforcement learning depth prediction loop closure prediction network trained using learning within thread asynchronous training environment agent play episode game environment therefore see observation reward pair st rt take action different experienced agent parallel thread within thread multiple task navigation depth loop closure prediction trained schedule add gradient shared parameter vector arrive within thread use system subordinate gradient update reinforcement learning procedure network training detail experiment use encoder model 2 convolutional layer followed fully connected layer recurrent layer predict policy value function architecture similar one mnih et 2016 convolutional layer follows ﬁrst convolutional layer ha kernel size stride 16 feature map second layer ha kernel size stride 32 feature map fully connected layer ff architecture figure ha 256 hidden unit output visual feature ft lstm lstm architecture ha 256 hidden unit output lstm hidden activation ht lstms figure fed extra input past reward previous action expressed vector dimension 8 lateral rotational velocity vt encoded vector concatenated vector nav architecture figure ﬁrst lstm 64 128 hiddens second lstm 256 hiddens depth predictor module gd loop closure detection module gl mlps 128 hidden unit depth mlps followed 64 independent softmax output one per depth pixel loop closure mlp followed softmax output illustrate figure 8 architecture nav agent depth taken labyrinth environment value 0 255 divided 255 taken power 10 spread value interval 0 1 empirically decided use following quantization 0 1 ensure uniform nav agent nav agent static maze 1 nav agent static maze 2 nav agent random goal maze 1 nav agent random goal maze 2 1 review conference paper iclr 2017 16 3 32 256 128 128 2 64 256 1 6 8 8 1 xt vt v ft ht gl ht gd ft 128 gd ft gl ht figure 8 detail architecture nav agent taking rgb visual input xt past reward previous action well velocity vt producing policy π value function v depth prediction gd ft ht well loop closure detection gl ht binning across 8 class previous version agent single depth prediction mlp gd regressing 8 16 128 depth pixel convnet output parameter module point subset common vector parameter optimise parameter using asynchronous version rmsprop tieleman hinton 2012 nair et 2015 wa recent example asynchronous parallel gradient update deep reinforcement learning case focus speciﬁc asynchronous advantage actor critic reinforcement learning procedure mnih et 2016 learning follows closely paradigm described mnih et 2016 use 16 worker rmsprop algorithm without momentum centering variance gradient computed chunk episode score point training curve average episode model get ﬁnish environment step whole experiment run maximum environment step agent ha action repeat 4 mnih et 2016 mean 4 consecutive step agent use action picked beginning series reason paper actually report result term agent perceived step rather environment step maximal number agent perceived step any particular run grid sample categorical distribution learning rate wa sampled 5 strength entropy regularization reward not scaled not clipped new set experiment previous set experiment reward scaled factor clipped 1 prior advantage algorithm gradient computed chunk 50 75 step episode previous set experiment used chunk 100 step auxiliary task used hyperparameters sampled coefﬁcient βd depth prediction loss convnet feature ld sampled 10 33 coefﬁcient depth prediction loss lstm hiddens sampled 1 10 coefﬁcient βl loop closure prediction loss sampled 1 10 loop closure us following threshold maximum distance position similarity 1 square minimum distance removing trivial 2 square 2 review conference paper iclr 2017 random goal maze small comparison reward clipping b random goal maze small comparison depth prediction figure 9 result averaged top 5 random hyperparameters conﬁguration star label indicates use reward clipping please see text detail c additional result reward clipping figure 9 show additional learning curve particular left plot show baseline ff lstm well nav agent without auxiliary loss perform worse without reward clipping reward clipping seems removing reward clipping make learning unstable absence auxiliary task particular reason chose show baseline reward clipping main result depth prediction regression classification task right subplot figure 9 compare depth input versus target note using rgbd input nav agent performs even worse predicting depth regression task general worse predicting depth classiﬁcation task task maze environment evaluated behaviour agent introduced paper well agent reward prediction introduced jaderberg et 2017 nav combination reward prediction convnet depth prediction policy lstm nav different maze environment speciﬁc task ﬁrst environment arena apple yielding 1 point lemon yielding point disposed arena agent need pick apple respawning episode last 20 second second environment stairway melon thin square corridor one direction lemon followed stairway melon 10 point reset level direction 7 apple dead end melon visible not reachable agent spawn lemon apple random orientation environment released deepmind lab beattie et 2016 environment not require navigation skill shortest path planning simple reward identiﬁcation lemon apple melon persistent exploration figure 10 show no major difference auxiliary task related depth prediction reward prediction depth prediction boost performance agent beyond stacked lstm architecture hinting general applicability depth prediction beyond navigation task sensitivity towards sampling experiment paper 64 replica run hyperparameters learning rate entropy cost sampled interval figure 11 show nav architecture 3 review conference paper iclr 2017 learning curve b stairway melon learning curve c layout stairway melon layout figure 10 comparison agent architecture maze conﬁgurations arena stairway melon described detail beattie et 2016 image credit c jaderberg et 2017 static maze small b random goal maze large c random goal figure 11 plot area curve auc reward achieved agent across different experiment 3 different task large static maze ﬁxed goal large static maze comparable layout dynamic goal reward auc value computed replica 64 replica run per experiment reward auc value sorted decreasing value auxiliary task achieve higher result comparatively larger number replica hinting fact auxiliary task make learning robust choice hyperparameters asymptotic performance agent finally compared asymptotic performance agent term navigation ﬁnal reward obtained end episode term representation policy lstm rather visualising convolutional ﬁlters quantify change representation 4 review conference paper iclr 2017 agent architecture frame performance lstm nav score mean top 5 57 103 position acc score mean top 5 90 114 position acc table 3 asymptotic performance analysis two agent random goal 2 maze comparing training labyrinth frame frame without auxiliary task term position decoding following approach explained section speciﬁcally compare baseline agent lstm navigation agent one auxiliary task depth prediction get twice many gradient update number frame seen environment rl task auxiliary depth prediction task table 3 show performance baseline agent well position decoding accuracy signiﬁcantly increase twice number training step going 57 point 90 point not reach performance position decoding accuracy nav agent half number training frame reason believe auxiliary task simply accelerate training 5