experimental support categorical compositional distributional model meaning edward grefenstette university oxford department computer science wolfson building park road oxford uk mehrnoosh sadrzadeh university oxford department computer science wolfson building park road oxford uk mehrs abstract modelling compositional meaning tences using empirical distributional method ha challenge computational guists implement abstract categorical model coecke et al 2010 using data bnc evaluate implementation based unsupervised learning matrix relational word applying vector argument evaluation based word disambiguation task oped mitchell lapata 2008 sitive sentence similar new ment designed transitive sentence model match result competitor ﬁrst experiment better second general improvement result increase syntactic complexity case compositional power model 1 introduction competent language speaker human trivially make sense sentence never seen heard naturally good derstanding ambiguous word given context forming meaning sentence ing part human seem comfortable machine fail deliver search engine google either fall back bag word syntax lexical exploit superﬁcial model lexical semantics retrieve page term related query manning et 2008 however model fail shine come processing semantics phrase tences discovering process meaning signment natural language among challenging foundational question tic computer science ﬁndings thereof increase understanding cognition gence shall assist application automating task document search compositional approach tague 1974 lambek 2008 distributional el lexical semantics schutze 1998 firth 1957 provided two partial orthogonal solution question compositional formal semantic model stem classical idea mathematical logic mainly frege principle meaning tence function meaning part frege 1892 distributional model recent related wittgenstein later philosophy meaning use whereby meaning word determined context wittgenstein 1953 logical model relate well known robust logical formalism hence offering scalable theory meaning used reason tially distributional model found way real world application thesaurus extraction grefenstette 1994 curran 2004 tomated essay marking landauer 1997 connection semantically motivated information retrieval manning et 2008 deﬁning property meaning logical form versus contextual use ha left quest foundational structure meaning even challenge recently coecke et al 2010 used high level technique logic category 20 jun 2011 theory physic bring two proaches together developed uniﬁed matical framework whereby sentence vector deﬁnition function kronecker product word vector concrete instantiation ory wa exempliﬁed toy hand crafted corpus grefenstette et al 2011 paper ment training model entire bnc highlight implementation word relational type verb adjective adverb matrix act argument provide general algorithm building indeed learning matrix corpus implementation evaluated task provided mitchell lapata 2008 biguating intransitive verb well similar new experiment transitive verb model improves best method evaluated mitchell lapata 2008 offer promising result transitive case demonstrating scalability comparison model still feel need different class experiment showcase compositionality statistically signiﬁcant manner work show categorical positional distributional model meaning permit practical implementation open way production large scale compositional model 2 two orthogonal semantic model formal semantics compute meaning sentence consisting n word meaning word must interact one another formal mantics interaction represented function derived grammatical structure sentence meaning word amorphous object domain no distinction made tween word type model consist pairing syntactic interpretation rule form grammar semantic tion rule exempliﬁed simple model sented figure parse sentence cat like milk typically produce semantic interpretation substituting semantic representation matical constituent applying needed derivation shown figure syntactic analysis semantic interpretation vp np milk etc vp np vt hug etc x figure 1 simple model formal semantics x x figure 2 parse tree showing semantic derivation methodology used translate sentence natural language logical formula use automation tool reason alshawi 1992 one major drawback result analysis only deal truth falsity meaning sentence say nothing closeness meaning topic expression beyond model satisfy hence not perform well language task search furthermore derlying domain object valuation function must provided any logic leaving open question might learn meaning language using model rather use distributional model distributional model semantics hand dismiss interaction syntactically linked word solely concerned lexical semantics word meaning obtained empirically examining word appears equating meaning word distribution context share intuition context use peal learning meaning word word frequently sort context common likely semantically related instance beer sherry drink coholic often cause hangover expect fact reﬂected sufﬁciently large pu word beer sherry occur within word appear sentence window word hold particular grammatical dency relation word learned context identifying word drink holic hangover frequently cur content word context distribution encoded tor high dimensional space context basis vector any word vector word scalar weight cword associated context basis tor ni function number time word ha appeared context semantic vector cword 1 cword 2 cword n also denoted sum vector pair word x cword ni learning semantic vector learning si weight corpus setting offer ometric mean reason semantic similarity via cosine measure clustering discussed widdows 2005 principal drawback model nature ignore grammatical structure logical word hence not pute meaning phrase sentence efﬁcient way word mon operation discussed mitchell lapata 2008 vector addition wise multiplication cf detail mutative hence vw v w v w vw wv leading unwelcome equality dog bit man man bit dog operation kronecker product cf deﬁnition take account smolensky 1990 even some complex syntactic relation described clark pulman 2007 however dimensionality sentence vector produced manner differs sentence different length barring sentence compared vector space growing exponentially sentence length hence quickly becoming computationally intractable 3 hybrid model whereas semantic compositional mechanism construction well understood no obvious corresponding method tor space solve problem coecke et al 2010 use abstract setting category theory turn grammatical structure sentence morphism compatible higher level logical structure vector space one pragmatic consequence abstract idea follows distributional model meaning vector word cat like milk logical recipe tell u apply ing verb meaning subject object vector apply vector lution proposed implies one need different level meaning word different type similar logical model verb relation noun atomic set verb tor built differently noun vector instance matrix general information word matrix word atomic vector fact encoded representation grammatical structure sentence linear map word vector input sentence vector output hence least theoretically one able build sentence vector pare synonymity exactly way one measure word synonymity pregroup grammar aforementioned linear map turn grammatical reduction called lambek pregroup mar lambek 2008 pregroups vector space share high level mathematical structure ferred compact closed category proof detail claim see coecke et al 2010 friendly introduction category theory see ecke paquette 2011 one consequence parity grammatical reduction group grammar directly transformed ear map act vector nutshell pregroup type either atomic compound atomic type simple n noun phrase statement adjoint type nr nl example compound type verb nrsnl superscripted type express verb relation two argument type n usage pregroup type not essential type any logic instance ccg used translated language pregroups occur right left output argument type transitive sentence ha type shown figure type n cancel right adjoint nr right left adjoint nl left mathematically speaking nln nnr 1 unit concatenation corresponding grammatical reduction transitive sentence nnrsnl reduction depicted wire diagram diagram transitive sentence shown figure cat n like nrs nl milk n figure 3 pregroup type reduction diagram transitive sentence semantic composition ing coecke et al 2010 based general completeness theorem compact category wire diagram vector space meaning sentence canonically reduced linear braic formula following meaning vector transitive sentence cat like milk f cat like milk f linear map encodes cal structure categorical morphism ing denoted tensor product 3 nents ϵv v w subject object space sentence space ϵ cup straight line diagram cup stand taking inner product done basis vector imitate substitution straight line stand identity map doe nothing rule category equation duce following linear algebraic formula relation partial order pregroup sponds implication logical reading thereof inequality replaced equality nln 1 nnr pregroup collapse group nl nr lower dimension hence dimensional explosion problem kronecker product avoided x itj ii vi wj basis vector v inner product cat weight cat ﬁrst argument place verb similarly object second argument place st basis vector sentence space meaning sentence live regardless grammatical ture degree synonymity sentence tained taking cosine measure vector abstract space need instantiated provide concrete meaning synonymity sures instance model tained taking sentence space dimensional space basis vector true false 4 building matrix relational word section present general scheme build matrix relational word recall given vector space basis ni kronecker product two vector v p ca ni w p cb ni deﬁned follows v w x ij ca cb j ni nj ni nj pairing basis ni nj kronecker product vector belong tensor product hence ha dimension r dimensionality multiplication vector deﬁned follows v w x ca cb ni intuition behind matrix tional word any relation r set x r represented matrix namely one ha x weight cxy 1 x 0 otherwise distributional ting weight natural real number represent extent according x related determined ferent way suppose x set animal chase relation chase take x dog cat glass obvious choice would take cxy ber time dog ha chased cat number time sentence dog chase cat ha appeared corpus distributional ting method syntactic dismissive actual meaning cat dog instead corpus contains sentence hound hunted wild cat cxy 0 restricting u only assign meaning sentence directly peared corpus propose instead use level abstraction taking word verb distribution semantic information vector context word rather context word start vector space n basis n meaning vector atomic word noun live basis vector n principle word corpus ever practice following mitchell lapata 2008 restrict subset occurring word basis vector not restricted noun well verb tives adverb deﬁne ing noun possible usual not only text noun note basis word lational type treated pure lexical item rather semantic object represented matrix short count many time noun ha occurred close word syntactic type elect scientiﬁc rather count many time ha occurred close corresponding matrix lexical token form context not meaning relational word p grammatical type π adjoint type αm encoded r r matrix dimension since vector space n ha ﬁxed basis trix represented vector form follows p x ij ζ z n n j n ζ z vector life tensor space n z computed according procedure described figure 4 1 consider sequence word containing lational word p argument wm occurring order described p grammatical type refer sequence p suppose k 2 retrieve vector w l argument wl 3 suppose ha weight basis vector n ha weight j basis vector n j wm ha weight cm ζ basis vector n multiply weight j cm ζ 4 repeat step k p relation suma corresponding weight x k j cm ζ k awe also experimented multiplication sity noun vector resulted verb matrix empty figure 4 procedure learning weight matrix word p relational type π argument linear algebraically procedure corresponds computing following p x k w 1 w 2 w k example relational word verb adjective adverb transitive verb represented 2 dimensional matrix since type nrsnl two adjoint type nr nl responding vector matrix verb x ij cij n n j weight cij corresponding basis vector n n j extent according word n subject verb word n j object verb example computation demonstrated figure 5 1 consider phrase containing verb subject object suppose k 2 retrieve vector w 1 w 2 3 suppose w 1 ha weight n w 2 ha j n multiply weight j 4 repeat step k verb relation sum corresponding weight p k j k figure 5 procedure learning weight matrix transitive verb linear algebraically computing verb x k w 1 w 2 k example consider verb show pose two show corpus table show result map show location vector show show table result map location consider n space four basis vector far room scientiﬁc elect weighted value vector four noun built bnc shown table ni table map result location 1 far 7 2 room 27 3 scientiﬁc 0 13 4 elect 0 0 0 table 1 sample weight selected noun vector part matrix show presented table sample computation weight vector 1 1 far far computed ing weight table result far far room scientiﬁc elect far room scientiﬁc 0 elect 0 0 0 0 table 2 sample semantic matrix show multiplying weight map location far adding obtaining total weight method applied build matrix transitive verb 3 dimension adjective adverb 1 dimension 5 computing sentence vector meaning sentence vector computed ing variable categorical prescription meaning linear map f obtained matical reduction sentence determined matrix relational word instance meaning transitive sentence sub verb obj sub verb obj x itj sub v w j take v w n n p itj determined matrix verb substitute p ij cij n n j hence sub verb obj becomes x ij sub n n j n n j x ij csub cobj j cij n n j decomposed tion two vector follows x ij csub cobj j n n j x ij cij n n j also reducing verb space n n n since construction only need tuples form n n n j n j isomorphic pair n n j left argument kronecker product ject object vector right argument vector verb obtain sub obj verb since commutative provides u tributional version meaning sentence multiplication meaning verb kronecker product subject object sub verb obj verb sub obj mathematical operation informally scribed structured mixing information subject object followed tered information verb applied order produce information sentence transitive case n hence n n generally vector space responding abstract sentence space concrete tensor space n mension matrix verb seen practice not need build tensor space computation thereof reduce multiplication summation similar computation yield meaning sentence adjective adverb instance ing transitive sentence modiﬁed subject modiﬁed verb adj sub verb obj adv adv verb adj sub obj building vector sentence pare meaning measure degree onymy taking cosine measure 6 evaluation evaluating framework no easy task evaluate depends heavily sort tion practical instantiation model geared towards grefenstette et 2011 gested simpliﬁed model presented expanded could evaluated way lexical semantic model measuring compositionally built sentence vector benchmark dataset provided mitchell lapata 2008 section brieﬂy describe evaluation model dataset following present new evaluation task extending mental methodology mitchell lapata 2008 transitive sentence compare model discussed mitchell lapata 2008 within new experiment first dataset description ﬁrst experiment described detail mitchell lapata 2008 evaluates well compositional model biguate ambiguous word given context tentially disambiguating noun entry dataset provides noun target verb landmark verb intransitive noun must posed verb produce short phrase tor similarity measured didate also provided entry cation high low indicating whether not verb indeed semantically close within context noun well larity score 1 7 along evaluator identiﬁer 1 low similarity 7 high evaluation methodology candidate model vide similarity score entry score high similarity entry low similarity entry averaged produce mean high score mean low score model correlation model similarity judgement human judgement also calculated using spearman ρ metric deemed scrupulous ultimately model ranked mitchell lapata 2008 mean model 0 1 scale except upperbound 1 7 scale annotator used ρ score 1 scale assumed agreement provides ical maximum ρ any model experiment cosine measure verb vector ignoring noun taken baseline no composition model model compare evaluated mitchell ata 2008 provide selection result paper worst add tiply performing model well previous performing model kintsch ditive multiplicative model simply tions vector addition plication invite reader consult mitchell lapata 2008 description kintsch additive model parametric choice model parameter provide accurate comparison existing multiplicative model exploiting aforementioned feature categorical model built top existing lexical distributional model used ters described mitchell lapata 2008 produce vector evaluated original iment noun vector vector built lemmatised version bnc noun basis wa 2000 common context word basis weight probability context word given target word divided overall bility context word intransitive verb vector trained using procedure presented since dataset only contains intransitive verb noun used cosine sure vector wa used similarity metric first experiment result table 3 present comparison selected model ical model performs signiﬁcantly better isting kintsch obtains ρ identical multiplicative model indicating niﬁcant correlation annotator score not large difference mean high score mean low score bution figure 6 show model make distinction high similarity phrase low similarity phrase despite solute score not different percentile multiplicative model presented ﬁed best mitchell lapata 2008 however also present slightly better performing ρ model combination multiplicative model weighted additive model difference ρ qualiﬁed not tistically signiﬁcant original paper furthermore mixed model requires parametric optimisation hence wa not evaluated entire test set reason chose not include comparison model high low ρ baseline add kintsch multiply categorical upperbound table 3 selected model mean high low ity item correlation coefﬁcients human ments ﬁrst experiment mitchell lapata 2008 p high low figure 6 distribution predicted similarity egorical distributional model high low similarity item second dataset description second developed author follows format mitchell lapata 2008 dataset used ﬁrst experiment exception target landmark verb transitive object noun provided addition subject noun hence forming small transitive sentence dataset comprises 200 entry consisting sentence pair hence total 400 sentence constructed lowing procedure outlined mitchell lapata 2008 using transitive verb example sentence see table dataset wa split four section 100 entry guaranteed 50 exclusive overlap 6 7 exactly two datasets section wa given group evaluator total 25 asked form simple transitive sentence pair verb subject object provided entry instance table showed result table show result evaluator asked rate semantic similarity verb pair within context sentence offer score 1 7 entry entry wa given trary classiﬁcation high low author purpose calculating mean score model example ﬁrst two pair table 4 classiﬁed high whereas second two pair low sentence 1 sentence 2 table show result table express result map show location map picture location table show result table picture result map show location map express location table 4 example entry transitive dataset annotator score second experiment evaluation methodology evaluation methodology second experiment wa identical ﬁrst scale mean score also spearman ρ deemed rigorous way determining well model track difference meaning imprecise nature cation verb pair high low since objective similarity score produced model distinguishes sentence different meaning similar meaning renormalised practice therefore delta high mean low mean not serve deﬁnite indication practical applicability lack thereof semantic model mean provided aid comparison result ﬁrst experiment model parameter ﬁrst experiment lexical vector mitchell lapata 2008 used model evaluated additive multiplicative baseline 8 noun wa not evaluated required optimising model parameter segment test set could not replicate methodology mitchell lapata tor categorical model transitive verb tor trained described second experiment result result model evaluated second dataset sented table model high low ρ baseline add multiply categorical upperbound table 5 selected model mean high low ity item correlation coefﬁcients human ments second experiment p observe signiﬁcant according p improvement alignment categorical model human judgement additive model continues make tle distinction sens verb composition multiplicative model ment doe not change becomes statistically distinguishable baseline model note mean not indicative model performance ference high mean low mean categorical model much smaller baseline model multiplicative model despite better alignment annotator judgement 7 discussion paper described implementation categorical model meaning coecke et 2010 combine formal logical cal distributional framework uniﬁed tic model implementation based ing matrix word relational type jectives verb vector word atomic type noun based data bnc show apply verb order compute meaning intransitive transitive sentence 2008 full conﬁdence work us matrix model meaning roni zamparelli 2010 guevara 2010 only phrase approach easily ply composition well sentence containing combination adjective noun verb adverb key difference learn matrix fashion gression composite context vector whereas model learns meaning compositionally vector compartment composite nally similar function example verb argument alternation break break x break not treated unrelated matrix intransitive break us observed information subject break cluding similarly matrix sitive break us information subject object including x leave thorough study phenomenon fall der providing modular representation active similarity future work evaluated model two way ﬁrst word disambiguation task mitchell ata 2008 intransitive verb similar new experiment transitive verb developed ﬁndings ﬁrst experiment show categorical method performs par leading existing approach not prise u given context small method becomes similar multiplicative model mitchell lapata 2008 however proach sensitive grammatical structure ing u develop second experiment taking account differentiating model commutative composition operation second experiment result deliver pected qualitative difference model categorical model outperforming others showing increase alignment human ments correlation increase sentence complexity use second evaluation pally show strong case opment complex experiment measuring not only disambiguating quality compositional model also syntactic sensitivity not directly measured existing experiment result show high level cal distributional model uniting empirical data logical form implemented like any concrete model furthermore show better result experiment involving higher syntactic ity tip iceberg matics underlying implementation ensures uniformly scale larger complicated tences enables compare synonymity tences different grammatical structure 8 future work treatment function word well logical word quantiﬁers junctives left future work build alongside general guideline coecke et al 2010 concrete insight work dows 2005 not yet entirely clear isting approach example discourse representation generalised quantiﬁers apply setting preliminary work integration two ha presented preller 2007 recently also preller sadrzadeh 2009 mentioned one reviewer group approach grammar ﬂattens sentence representation verb applied ject object time whereas approach ccg ﬁrst applied object produce verb phrase applied subject produce sentence advantage disadvantage method comparison system particular ccg constitutes ing work 9 acknowledgement wish thank blunsom clark coecke pulman anonymous emnlp er discussion comment support epsrc grant gratefully edged sadrzadeh reference alshawi ed core language engine mit press baroni zamparelli noun vector adjective matrix proceeding conference empirical method natural language processing emnlp clark pulman combining symbolic distributional model meaning proceeding aaai spring symposium quantum interaction aaai press coecke paquette category practicing physicist new structure physic coecke lecture note physic springer coecke sadrzadeh clark ical foundation distributed compositional model meaning lambek festschrift linguistic analysis 36 van benthem moortgat buszkowski curran distributional semantic larity phd thesis university edinburgh erk structured vector space model word meaning context proceeding conference empirical method natural guage processing emnlp frege 1892 uber sinn und bedeutung zeitschrift ur philosophie und philosophische kritik firth synopsis linguistic theory study linguistic analysis grefenstette sadrzadeh clark coecke pulman concrete compositional sentence space compositional distributional model meaning international conference computational semantics iwcs 11 oxford grefenstette exploration automatic saurus discovery kluwer guevara regression model noun compositionality distributional semantics proceeding acl gem workshop harris cycling sentence international tation centre bulletin 5 hudson word grammar blackwell lambek word sentence polimetrica milan landauer dumais solution plato problem latent semantic analysis theory quisition induction representation knowledge psychological review manning raghavan utze troduction information retrieval cambridge versity press mitchell lapata el semantic composition proceeding annual meeting association computational linguistics montague english formal language mal philosophy nivre efﬁcient algorithm projective dependency parsing proceeding tional workshop parsing technology iwpt preller towards discourse representation via group grammar journal logic language mation 16 preller sadrzadeh semantic vector el functional model pregroup grammar journal logic language information doi appear saffron newport asling word tion role distributional cue journal ory language 35 schuetze automatic word sense tion computational linguistics 24 smolensky tensor product variable binding representation symbolic structure nectionist system computational linguistics 46 2 steedman syntactic process mit press widdows geometry meaning university chicago press wittgenstein philosophical investigation blackwell