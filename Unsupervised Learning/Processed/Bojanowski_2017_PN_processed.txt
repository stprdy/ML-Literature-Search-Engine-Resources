unsupervised learning predicting noise piotr bojanowski 1 armand joulin 1 abstract convolutional neural network provide visual feature perform well many computer sion application however training work requires large amount supervision paper introduces generic framework train network no supervision propose ﬁx set target representation called noise target nat constrain deep feature align domain agnostic approach avoids standard vised learning issue trivial solution lapsing feature thanks stochastic batch reassignment strategy separable square loss function scale million image proposed approach produce representation perform par unsupervised method imagenet pascal voc introduction recent year convolutional neural network vnets fukushima 1980 lecun et 1989 pushed limit computer vision krizhevsky et 2012 et 2016 leading important progress variety task like object detection girshick 2015 image mentation pinheiro et 2015 key success ability produce feature easily transfer new domain trained massive database labeled age razavian et 2014 oquab et 2014 supervised data joulin et 2016 however human notation may introduce unforeseen bias could limit potential learned feature capture subtle tion hidden vast collection image several strategy exist learn deep convolutional feature no annotation donahue et 2016 either try capture signal source form supervision doersch et 2015 wang gupta 2015 ai research correspondence piotr bojanowski bojanowski proceeding 34 th international conference machine learning sydney australia pmlr 70 copyright 2017 author learn underlying distribution image vincent et 2010 goodfellow et 2014 some proaches obtain promising performance transfer ing donahue et 2016 wang gupta 2015 not explicitly aim learn discriminative feature some tempts made retrieval based approach vitskiy et 2014 clustering yang et 2016 liao et 2016 hard scale only tested small datasets unfortunately supervised case lot data required learn good representation work propose discriminative framework signed learn deep architecture large datasets approach general focus convnets since require million image produce good feature ilar map kohonen 1982 martinetz schulten 1991 map deep feature set ﬁned representation low dimensional space posed approach aim learn feature fashion traditionally suffers ture collapsing problem approach deal sue ﬁxing target representation aligning feature representation sampled informative distribution use noise target nat approach also share some similarity standard clustering approches like lloyd 1982 discriminative clustering bach harchaoui 2007 addition propose online algorithm able scale massive image database like imagenet deng et 2009 importantly approach barely le efﬁcient train standard supervised approach use any optimization procedure designed achieved using quadratic loss tygert et 2017 fast approximation hungarian rithm show potential approach training imagenet standard architecture namely alexnet krizhevsky et 2012 no supervision test quality feature several image ﬁcation problem following setting donahue et al 2016 par unsupervised learning approach much simpler train scale paper organized follows brief review related work section 2 present approach unsupervised learning predicting noise section validate solution several periments comparison standard unsupervised approach section 4 related work several approach recently proposed tackle problem deep unsupervised learning coates ng 2012 mairal et 2014 dosovitskiy et 2014 some based clustering loss xie et 2016 yang et 2016 liao et 2016 not tested scale comparable supervised convnet ing coates ng 2012 us vnets learning layer sequentially fashion work train convnet loss share similarity closer work dosovitskiy et al 2014 proposes train convnets solving retrieval problem assign class per age transformation contrast work approach hardly scale hundred thousand image requires tecture use standard alexnet another traditional approach learning visual tions unsupervised manner deﬁne parametrized mapping predeﬁned random variable set image traditional example approach tional autoencoders kingma welling 2013 generative adversarial network goodfellow et 2014 lesser extent noisy autoencoders vincent et 2010 work opposite map image predeﬁned random variable allows u standard convolutional network greatly simpliﬁes training generative adversarial network among proaches generative adversarial network gans fellow et 2014 denton et 2015 donahue et 2016 share another similarity approach namely explicitly minimizing discriminative loss learn feature model not learn inverse mapping donahue et al 2016 recently proposed add encoder extract visual feature gans like encoder any standard convolutional work however loss aim differentiating real generated image aiming directly entiating image make approach much simpler faster train since not need learn generator discriminator recently lot work ha explored leveraging supervision contained input signal doersch et 2015 noroozi favaro 2016 pathak et 2016 vein mikolov et 2013 doersch et al 2015 show spatial context strong signal learn visual feature noroozi favaro 2016 extended work others shown temporal coherence video also provides signal used learn erful visual feature agrawal et 2015 jayaraman grauman 2015 wang gupta 2015 particular wang gupta 2015 show feature provide ing performance imagenet contrast work approach domain dependent since require explicit derivation weak supervision directly input autoencoders many also used autoencoders reconstruction loss bengio et 2007 ranzato et 2007 masci et 2011 idea encode code image minimizing loss coded original image trained encoder duce image feature decoder used erate image code decoder often fully nected network ranzato et 2007 deconvolutional network masci et 2011 zhao et 2016 sophisticated like pixelcnn network van den oord et 2016 map family unsupervised od aim learning low dimensional representation data preserve certain topological property honen 1982 vesanto alhoniemi 2000 particular neural gas martinetz schulten 1991 aligns feature vector input data input datum assigned one vector manner feature vector spirit similar target tions use similar assignment strategy contrast work target vector not ﬁxed aligned input vector since primarly aim learning input feature opposite discriminative clustering many method proposed use discriminative loss clustering xu et 2004 bach harchaoui 2007 krause et 2010 joulin bach 2012 particular bach harchaoui 2007 show ridge regression loss could use learn discriminative cluster ha successfully applied several computer vision application like ject discovery joulin et 2010 tang et 2014 alignment bojanowski et 2013 2014 manathan et 2014 work show similar framework designed neural network posed xu et al 2004 address empty assignment problem restricting set possible reassignment permutation rather using global linear constrains assignment assignment updated online lowing approach scale large datasets unsupervised learning predicting noise target space feature assignment image cj p f x cnn figure approach take set image computes deep feature convolutional network match set predeﬁned target low dimensional space parameter network learned aligning feature target method section present model discus lations several clustering approach including mean figure 1 show overview approach also show trained massive datasets using online procedure finally provide tation detail unsupervised learning interested learning visual feature no pervision feature produced applying parametrized mapping fθ image presence supervision parameter θ learned ing loss function feature produced mapping some given target label absence supervision no clear target representation thus need learn well precisely given set n image xi jointly learn parameter θ mapping fθ some target vector yi min θ 1 n n x min ℓ fθ xi yi 1 dimension target vector rest paper use matrix notation denote matrix whose row target representation yi x matrix whose row image xi slight abuse notation denote fθ x n matrix feature whose row obtained applying function fθ image independently choosing loss function supervised setting popular choice loss ℓis softmax function ever computing loss linear number target making impractical large output space goodman 2001 workarounds scale loss large output space tygert et al 2017 ha recently shown using squared distance work well many pervised setting long ﬁnal activation unit normalized loss only requires access single get per sample making computation independent number target lead following problem min θ min 1 x f 2 still denote fθ x unit normalized tures using ﬁxed target representation directly solving problem deﬁned eq 2 would lead representation collapsing problem image would assigned representation xu et 2004 avoid issue ﬁxing set k predeﬁned target representation matching visual feature precisely matrix deﬁned product matrix c taining k representation assignment matrix p 0 1 pc 3 note assume k greater n no loss generality duplicating representation erwise image assigned different target target only assigned lead set p constraint assignment matrix p p 0 1 p 4 formulation force visual feature diversiﬁed avoiding collapsing issue cost ﬁxing target representation predeﬁning target issue number k small interested case k least large number n image choosing target representation not discussed set target representation stored simple choice target would take k element canonical basis rd larger n formulation would similar framework dosovitskiy et al 2014 impractical large hand smaller n tion equivalent discriminative clustering approach bach harchaoui 2007 choosing target make strong assumption nature underlying problem indeed assumes image belongs unique class class orthogonal assumption might true some classiﬁcation datasets unsupervised learning predicting noise doe not generalize large image collection capture subtle similarity image belonging different class since feature unit normalized another natural choice uniformly sample target vector unit sphere note dimension directly ence level correlation representation correlation inversely proportional square root using noise target nat eq 2 equivalent max θ max p tr x 5 problem interpreted mapping deep feature uniform distribution manifold namely dimension sphere using k predeﬁned representation discrete approximation manifold justiﬁes restriction mapping matrix set p assignment matrix some sense optimizing crude approximation earth mover distance distribution deep feature given target tion rubner et 1998 relation clustering approach using tations eq 5 several clustering approach share similarity method linear case spherical minimizes loss function p c max c max p tr main difference set q assignment matrix q p 0 1 set only guarantee data point assigned single target representation jointly learn feature assignment set doe not prevent collapsing data point single target tion another similar clustering approach diffrac bach harchaoui 2007 loss equivalent case unit normalized feature set r assignment matrix however different r p 0 1 p c 0 some ﬁxed parameter restricting assignment matrix set prevents collapsing issue introduces global constraint not suited online optimization make approach hard scale large datasets optimization section describe efﬁciently optimize cost function described eq 5 particular explore algorithm 1 stochastic optimization eq 5 require batch image 0 1 obtain batch b representation r compute fθ xb compute p minimizing eq 2 p compute θ eq 2 p update θ θ end approximated update assignment matrix compatible online optimization scheme like tic gradient descent sgd updating assignment matrix directly solving optimal assignment requires evaluate tances n feature k representation order efﬁciently solve problem ﬁrst reduce number k representation limit set p set permutation matrix p p 0 1 p 6 restricting problem deﬁned eq 5 set linear assignment problem p solved exactly hungarian algorithm kuhn 1955 hibitive cost instead perform stochastic update matrix given batch sample optimize assignment matrix p restriction batch given subset b b tinct image only update b b square sub matrix pb obtained restricting p b image corresponding target word image only target wa previously assigned another image batch procedure ha ity per batch leading overall complexity linear number data point perform update updating parameter θ feature manner note simple dure would not possible k n would also consider k unassigned representation stochastic gradient descent apart update assignment matrix p use optimization scheme standard supervised approach sgd batch normalization ioffe szegedy 2015 noted tygert et al 2017 batch normalization play cial role optimizing loss avoids exploding gradient batch b image ﬁrst perform forward pas compute distance image corresponding subset target representation hungarian algorithm used distance obtain optimal reassignment within batch unsupervised learning predicting noise softmax square loss imagenet table comparison softmax square loss supervised object classiﬁcation imagenet architecture alexnet feature unit normalized square loss tygert et 2017 report accuracy validation set assignment updated use chain rule order compute gradient parameter mization algorithm summarized algorithm 1 implementation detail experiment solely focus learning visual feature convnets detail required train tures approach described standard trick used supervised setting deep feature ensure fair empirical comparison previous work follow wang gupta 2015 use alexnet architecture train end end using unsupervised loss function subsequently test quality learned visual feature ﬁer top transfer learning consider output last convolutional layer feature vian et al 2014 use perceptron mlp krizhevsky et al 2012 classiﬁer observe practice processing image greatly help quality learned feature ranzato et al 2007 use age gradient instead image avoid trivial tions like clustering according color using processing not surprising since feature like sift hog based image gradient lowe 1999 dalal triggs 2005 addition processing also perform standard image formation commonly applied supervised setting krizhevsky et 2012 random cropping ﬂipping image optimization detail project output work sphere tygert et al 2017 work trained sgd batch size ing ﬁrst batch use constant step size ter batch use linear decay step size lt unless mentioned otherwise permute assignment within batch every 3 epoch transfer learning experiment follow guideline scribed donahue et al 2016 experiment perform several experiment validate different design choice nat evaluate quality tures comparing unsupervised approach several auxiliary supervised task namely object classiﬁcation imagenet object classiﬁcation detection pascal voc 2007 everingham et 2010 transfering feature order measure quality feature measure performance transfer learning freeze parameter convolutional layer overwrite parameter mlp classiﬁer random gaussian weight precisely follow training testing procedure speciﬁc datasets following donahue et al 2016 datasets baseline use training set agenet learn convolutional network deng et 2009 dataset composed 1 281 167 image belong 1 000 object category transfer ing experiment also consider pascal voc addition fully supervised approach krizhevsky et 2012 compare method several unsupervised approach autoencoder gan bigan ported donahue et al 2016 also compare supervised approach agrawal et al 2015 doersch et al 2015 pathak et al 2016 wang gupta 2015 zhang et al 2016 finally compare feature sift fisher vector anchez et 2013 reduce fisher vector 4 096 dimensional vector pca apply 8 192 unit mlp top detailed analysis section validate some design choice like loss function representation inﬂuences some parameter quality feature periments run imagenet softmax versus square loss table 1 compare formance alexnet trained softmax square loss report accuracy validation set square loss requires feature unit ized avoid exploding gradient previously observed tygert et al 2017 performance similar hence validating choice loss function effect image preprocessing supervised ﬁcation image not frequently used transformation remove information usually avoided unsupervised case however observe preferable work simpler input unsupervised learning predicting noise clean sobel acc 1 table performance supervised model various image applied train alexnet imagenet report classiﬁcation accuracy avoids learning trivial feature particular serve using grayscale image gradient greatly help method mentioned sec order verify preprocessing doe not destroy crucial information propose evaluate effect supervised classiﬁcation also compare ﬁltering table 2 show impact preprocessing method accuracy alexnet validation set imagenet none degrade perform signiﬁcantly meaning information related gradient ﬁcient object classiﬁcation experiment conﬁrms doe not lead signiﬁcant drop upper bound performance model continuous versus discrete representation pare choice target vector commonly used clustering element canonical basis k dimensional space representation make strong assumption structure problem linearly separated k different class hold agenet giving fair advantage discrete tion test representation k 103 104 105 range 1 000 class agenet matrix c contains replication k ments canonical basis assumes cluster balanced veriﬁed imagenet compare representation tinuous target vector transfer task imagenet ing discrete target achieves accuracy 22 signiﬁcantly worse best performance possible explanation binary vector induce sharp discontinuous distance representation tances hard optimize may result early convergence poorer local minimum evolution feature experiment terested understanding quality feature evolves optimization cost function ing unsupervised training freeze network every 20 epoch learn mlp classiﬁer top report accuracy validation set imagenet figure 2 show evolution performance transfer task optimize unsupervised approach ing performance improves monotonically epoch unsupervised training suggests optimizing 50 100 150 200 epoch unsupervised training 20 30 40 50 60 accuracy transfer train acc transfer test acc 5 10 permutation period 20 30 40 50 60 accuracy figure left measure accuracy imagenet training feature unsupervised approach function number epoch performance improves longer unsupervised training right measure accuracy imagenet training feature different permutation rate clear optimum permutation performed every 3 epoch objective function correlate learning transferable feature feature not destroy useful information hand test accuracy seems saturate hundred epoch suggests mlp overﬁtting rapidly feature effect permutation assigning image target representation main feature approach experiment want understand frequently update assignment updating assignment even partially costly may not required achieve good performance figure 2 show transfer accuracy imagenet function frequency date model quite robust choice frequency test accuracy always 30 interestingly accuracy actually degrades slightly high frequency possible explanation network overﬁts rapidly output leading relatively worse feature tice observe updating assignment matrix every 3 epoch offer good performance accuracy training network keeping p ﬁxed obtain baseline test accuracy visualizing ﬁlters figure 4 show comparison tween ﬁrst convolutional layer alexnet trained without supervision take grayscale gradient image input visualization obtained posing sobel ﬁltering ﬁlters ﬁrst layer alexnet unsupervised ﬁlters slightly le sharp supervised counterpart still maintain edge orientation information nearest neighbor query loss optimizes distance feature ﬁxed vector mean ing distance feature provide some information type structure model unsupervised learning predicting noise figure image 3 nearest neighbor imagenet according model using distance query image shown top row nearest neighbor sorted closer feature seem capture global distinctive structure figure filter form ﬁrst layer alexnet trained agenet supervision left nat right ﬁlters grayscale since use grayscale gradient image input visualization show composition gradient ﬁrst layer tures given query image x compute feature fθ x search nearest neighbor according tance figure 3 show image nearest neighbor feature capture relatively complex structure age object distinctive structure like trunk fruit well captured approach however information not always related image label ample image bird sea matched image related sea sky rather bird comparison state art report result transfer task imagenet pascal voc model trained imagenet imagenet classiﬁcation experiment evaluate quality feature object classiﬁcation task imagenet setup build unsupervised tures image correspond predeﬁned image gories even though not access label data biased towards class order uate feature freeze layer last lutional layer train classiﬁer supervision experimental setting follows noroozi favaro 2016 compare model several wang gupta 2015 doersch et 2015 zhang et 2016 one unsupervised approach donahue et al 2016 note approach use loss speciﬁcally designed visual feature like bigans donahue et 2016 nat doe not make any assumption main structure feature table 3 compare nat approach among unsupervised approach nat compare ably bigan donahue et 2016 interestingly performance nat slightly better method even though not explicitly use speciﬁc clue image video guide learning model provide performance 30 range not clear learn feature nally unsupervised deep feature outperformed unsupervised learning predicting noise method acc 1 random noroozi favaro 2016 anchez et 2013 wang gupta 2015 doersch et al 2015 zhang et al 2016 favaro 2016 bigan donahue et 2016 nat table comparison proposed approach unsupervised feature learning imagenet full perceptron retrained top feature compare eral approach unsupervised approach bigan donahue et 2016 favaro 2016 us signiﬁcantly larger amount feature original alexnet report classiﬁcation accuracy feature particular fisher vector sift descriptor baseline us slightly bigger mlp classiﬁer performance improved bagging 8 model difference 20 accuracy show unsupervised deep feature still quite far among unsupervised feature transferring pascal voc carry second transfer experiment pascal voc dataset classiﬁcation detection task model trained imagenet depending task ﬁnetune layer network solely classiﬁer ing donahue et al 2016 experiment ters convolutional layer initialized one obtained unsupervised approach ters classiﬁcation layer initialized gaussian weight get rid batch normalization layer use rescaling parameter uhl et 2015 table 4 show comparison model unsupervised approach result method taken donahue et al 2016 except zhang et al 2016 imagenet classiﬁcation task performance par approach tection classiﬁcation among purely unsupervised approach outperform standard approach like toencoders gans large margin model also performs slightly better best performing bigan model donahue et 2016 experiment conﬁrm ﬁndings imagenet experiment despite simplicity nat learns feature good tained sophisticated model classiﬁcation detection trained layer imagenet label agrawal et al 2015 pathak et al 2016 wang gupta 2015 doersch et al 2015 zhang et al 2016 autoencoder gan bigan donahue et 2016 nat table comparison proposed approach unsupervised feature learning voc 2007 classiﬁcation tection either ﬁx feature whole model compare several supervised approach gan autoencoder baseline donahue et al 2016 report mean average prevision customary pascal voc conclusion paper present simple unsupervised framework learn discriminative feature aligning output neural network noise obtain tures par unsupervised learning proaches approach explicitly aim learning inative feature unsupervised approach target surrogate problem like image denoising image tion opposed approach make assumption input space make appproach simple fast train interestingly also share some similarity traditional clustering proaches well retrieval method show potential approach visual data ing try domain finally work only considers simple noise distribution alignment method sible direction research explore target distribution alignment informative also would strengthen relation nat method based distribution matching like earth mover distance acknowledgement greatly thank e egou help throughout development project also thank allan jabri edouard grave iasonas kokkinos eon bottou matthijs douze rest fair support helpful discussion finally thank richard zhang jeff donahue florent perronnin help unsupervised learning predicting noise reference agrawal carreira malik learning see moving iccv bach harchaoui diffrac discriminative ﬂexible framework clustering nip bengio lamblin popovici larochelle greedy training deep network nip bojanowski bach laptev ponce schmid sivic finding actor action movie iccv bojanowski lajugie bach laptev ponce schmid sivic weakly supervised action labeling video ordering constraint eccv coates ng learning feature representation neural network trick trade springer dalal triggs histogram oriented gradient human detection cvpr deng dong socher li li imagenet hierarchical image database cvpr denton chintala fergus deep generative image model using laplacian pyramid adversarial network nip doersch gupta efros unsupervised visual representation learning context prediction cvpr donahue uhl darrell ial feature learning arxiv preprint dosovitskiy springenberg riedmiller brox discriminative unsupervised feature learning convolutional neural network nip everingham van gool williams winn zisserman pascal visual object class voc challenge ijcv fukushima neocognitron neural network model mechanism pattern recognition unaffected shift position biological cybernetics girshick fast cvpr goodfellow mirza xu ozair courville bengio generative adversarial net nip goodman class fast maximum entropy training icassp zhang ren sun deep residual ing image recognition cvpr ioffe szegedy batch normalization ing deep network training reducing internal covariate shift arxiv preprint jayaraman grauman learning image tations tied iccv joulin bach convex relaxation weakly supervised classiﬁers icml joulin bach ponce discriminative clustering image cvpr joulin van der maaten jabri vasilache learning visual feature large weakly supervised data eccv kingma welling variational bayes arxiv preprint kohonen formation topologically rect feature map biological cybernetics uhl philipp doersch carl donahue jeff darrell trevor initialization convolutional neural network arxiv preprint krause perona gomes discriminative clustering regularized information maximization nip krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural network nip kuhn hungarian method assignment problem naval research logistics quarterly 2 97 lecun boser denker henderson howard hubbard jackel ten digit recognition network nip liao schwing zemel urtasun learning deep parsimonious representation nip lloyd least square quantization pcm transaction information theory 28 2 unsupervised learning predicting noise lowe object recognition local tures iccv mairal koniusz harchaoui schmid volutional kernel network nip martinetz schulten network learns topology masci meier cires schmidhuber stacked convolutional hierarchical feature extraction icann mikolov chen corrado dean efﬁcient estimation word representation vector space arxiv preprint noroozi favaro unsupervised learning visual representation solving jigsaw puzzle arxiv preprint oquab bottou laptev sivic learning transferring image representation using convolutional neural network cvpr pathak krahenbuhl donahue darrell efros context encoders feature learning ing cvpr pinheiro collobert dollar learning segment object candidate nip ramanathan joulin liang ing people video name using coreference resolution eccv ranzato huang boureau lecun unsupervised learning invariant feature hierarchy application object recognition cvpr razavian sharif azizpour sullivan son cnn feature astounding line recognition arxiv rubner tomasi guibas metric distribution application image database iccv anchez perronnin mensink verbeek image classiﬁcation ﬁsher vector theory practice ijcv 105 3 tang joulin li localization image cvpr tygert chintala szlam tian zaremba learning convolutional network applied computational harmonic analysis 42 1 van den oord kalchbrenner espeholt vinyals graf conditional image generation pixelcnn decoder nip vesanto alhoniemi clustering organizing map transaction neural network vincent larochelle lajoie bengio zagol stacked denoising autoencoders learning useful representation deep network local noising criterion jmlr 11 dec wang gupta unsupervised learning visual representation using video iccv xie girshick farhadi unsupervised deep embedding clustering analysis icml xu neufeld larson schuurmans mum margin clustering nip yang parikh batra joint unsupervised ing deep representation image cluster cvpr zhang isola efros colorful image tion eccv zhao mathieu goroshin lecun stacked workshop iclr 2016