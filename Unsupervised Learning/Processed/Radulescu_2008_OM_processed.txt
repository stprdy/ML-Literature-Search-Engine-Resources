13 jan 2008 hebbian inspeciﬁcity oja model november 23 2018 abstract recent work long term potentiation brain slice show hebb rule not completely probably due intersynapse diﬀusion calcium factor extend classical oja unsupervised model learning single linear neuron include hebbian inspeciﬁcity introducing error matrix e express possible crosstalk updating diﬀerent connection show modiﬁed algorithm converges leading eigenvector matrix ec c input covariance matrix no inspeciﬁcity e identity matrix give classical result convergence ﬁrst principal component input distribution study outcome learning using diﬀerent version biologically plausible case arising no intrinsically privileged connection e ha diagonal element q diagonal element 1 n q quality expected decrease number input reasonable assumption biophysics long term potentiation take q 1 n discrete model q bn 1 continuous model b single synapse inaccuracy parameter reﬂects synapse density calcium diﬀusion etc analyze case detail uncorrelated correlated input study dependence angle θ leading eigenvector ec b n amount input activity correlation analytically using matlab calculation ﬁnd θ increase learning becomes gradually le useful increase b particularly intermediate correlation strength although some useful learning always occurs trivial limit q discus relation result hebbian unsupervised learning brain 1 introduction various brain structure neocortex believed use unsupervised synaptic learning form neural representation capture exploit statistical regularity animal world neural model unsupervised learning use some form hebb rule update synaptic connection typically rule implemented updating connection according product input output ﬁring rate form update rule sometimes used still typically local activity dependent often hebbian sense depend input output activity biological network may also use dependent rule also hebbian sense depend relative timing postsynaptic spiking key element hebbian learning update depend extent input appears take part ﬁring output 1 added appears emphasize 1 single neural connection know nothing actual causation merely responds statistical coincidence postsynaptic spike interested possibility hebb rule not completely local sense might some possibly weak dependence local update activity connection unsupervised learning might fail catastrophically not only preventing new learning wiping previous learning proposed basic task neocortex avoid hypothetical learning catastrophe 2 3 paper modify classical model unsupervised learning oja single neuron principal component analyzer 4 include hebbian inaccuracy inspeciﬁcity inaccuracy mean part local update calculated using hebb rule example proportional product input output ﬁring rate assigned connection one product wa calculated also refer postulated nonlocality leakage crosstalk simply error some paper topic appeared 5 discussion try clarify relationship various study conclude modiﬁed oja model perhaps others only sensitive statistic not show true error catastrophe ﬁnite network size behaviour give important clue understanding diﬃculties brain might encounter learning statistic recent experimental work ha shown long term potentiation ltp biological manifestation hebb rule indeed not completely synapse speciﬁc 6 7 8 9 example engert bonhoeﬀer shown ltp induced local set connection pyramidal cell spill induce ltp nearby set inactive connection earlier work using le reﬁned method concluded ltp wa speciﬁc 10 11 even engert experiment 6 likely pairing method used induce ltp wa rather crude inspeciﬁcity wa far greater would ever actually seen awake brain recent work ha shown least one type hebbian inspeciﬁcity induced theta burst stimulation retinotectal connection reﬂects dendritic spread calcium 9 even recent ltp experiment single synapsis shown ltp only expressed locally 12 threshold ltp induced neighboring synapsis reduced 13 thus some degree hebbian inspeciﬁcity probably inevitable eﬀects learning need evaluated 2 overview brieﬂy review classical oja model 4 14 15 16 deﬁne term background new analysis model network consists single output neuron ceiving n signal xn set n input neuron via connection corresponding strength ωn see figure 1 resulting output deﬁned weighted sum input n x xiωi input column vector x xn randomly drawn probability distribution p x x denotes transposition vector accordance hebb postulate learning synaptic weight ωi strengthen proportionally product xi ωi 1 ωi γy xi 2 γ time independent learning rate argument represents dependence time input draw relation formulation neural process ltp considered discussion oja 4 modiﬁed normalizing weight vector ω respect euclidean metric rn ωi 1 ωi γy xi γy x expanding taylor series respect γ ignoring term γ suﬃciently small result ω 1 ω γy x ω henceforth omit variable whenever no ambiguity equation rewritten ω 1 ω γ xxt ω xxt ω ω consider covariance matrix distribution p x deﬁned c xt clearly c symmetric semipositive deﬁnite following additional assumption learning process slow enough ω treated stationary x ω statistically independent take conditional expectation p x rewrite learning rule 1 w γ cw cw oja concluded ω converges limit expected one two opposite normalized eigenvectors corresponding maximal eigenvalue c principal component matrix c 4 14 general form rule use throughout paper allows element x ω negative however biologically interesting special case component x corresponding ﬁring rate ω corresponding synaptic strength always positive ﬁrst term corresponds ltp second term corresponds long term depression ltd see discussion operation oja rule understood intuitively following way new input vector twist current weight vector constrained lie close unit hypersphere surface towards twist magnitude proportional corresponding input vector ﬁnal weight vector would lie direction mean input distribution however oja rule magnitude twist also depends output inﬂuence input vector closer direction current weight vector magniﬁed since dot product weight vector larger thus weight vector direction get closer ﬁrst principal component direction arriving input pattern closer produce larger twist away thus diﬀerent pc not contribute ﬁnal outcome instead only largest pc win see eﬀect error moderate behavior inaccurate learning lead incomplete victory learned weight vector linear combination eigenvectors however long learning show some degree speciﬁcity learned weight vector always closer ﬁrst pc any others sense learning continues useful least gaussian input since allows useful scalar representation merely randomly selecting 3 one input pattern element introduce inspeciﬁcity learning equation assume average only fraction q intended update reach appropriate connection remaining fraction distributed amongst connection according deﬁned biologically plausible rule actual update given connection thus includes contribution erroneous innacurate update connection erroneous updating process formally described possibly error matrix e e independent input whose element depend average q reﬂect time step fractional contribution activity across weight ωi make update ωj hebb rule change normalized version ωi 1 ωi γy ex normalization linearization respect γ becomes ωi 1 ωi γy ex taking conditional expectation side rewriting equation matrix form lead 1 w γ ecw cw w deﬁned w e e symmetric circulant matrix zero error case q 1 e would become identity matrix 3 method prelude analyzing dynamic inspeciﬁc learning revisit oja inal model zero error method used establish asymptotic ior 5 4 14 16 word size n n want know whether not vector w stabilizes iteration function f rn f w w γ cw wt cw w vector w ﬁxed point f f w w γ cw wt cw w w wt cw w equivalent set condition cw λww λw wt cw cw λww λw λwwt w condition translate w eigenvector c case c invertible eigenvalue nonzero w unit eigenvector c euclidean norm consider orthonormal basis b eigenvectors c respect euclidean norm rn eigenvector w eigenvalue λw hyperbolic attractor f eigenvalue n n jacobian matrix dfw ij w le one absolute value calculate dfw ﬁnd b also basis eigenvectors dfw see appendix b corresponding eigenvalue 1 eigenvector w 4 figure 1 three spine short dendritic segment shown hebbian adjustment occurs middle synapse factor red dot calcium diﬀuses nearby synapsis aﬀects hebbian adjustment input neuron activity xi converge output neuron output via weight ωi coincident activity synapsis comprising weight lead modiﬁcation weight weight left diagram show case only immediate neighboring connection made one synapse aﬀected right diagram show case connection equal neighbor either ha many synapsis dispersed randomly dendrite synapsis move around curved red arrow ωn show periodic boundary condition assumed aﬀects ωn equally 1 λw any eigenvector v v w therefore set equivalent condition w hyperbolic attractor f λw 1 v v w 1 w hyperbolic ﬁxed point f only λw λv v w λw maximal eigenvalue ii γ 1 λw particular γ 2 v w condition always satisﬁed provided c ha maximal eigenvalue tiplicity one ii γ small enough γ 1 λw conclusion condition ii network learns ﬁrst principal ponent distribution p x learning principal component requires relationship rate learning γ input distribution p x maximal eigenvalue correlation matrix c large variance input pattern 5 projection high network ha learn slowly order achieve gence moreover convergence time along eigendirection given inverse magnitude corresponding eigenvalue dfw see simulation figure 2 formalize learning inspeciﬁcity introduced error matrix e r ha positive entry symmetric equal identity matrix r error zero studied asymptotic behavior new system using approach outlined section 2 also see appendix inspeciﬁc learning iteration function becomes f e w w γ ecw wt cw w also w ﬁxed point f e only eigenvector ec eigenvalue λw wt cw w furthermore w hyperbolic attractor f e only λw principal eigenvalue ec γ 1 λw rule maximizes variance output neuron λw therefore gaussian input also maximizes mutual information input output see discussion altough erroneous rule no longer maximizes output variance tolerates faster learning rate conversely ﬁxed γ learning slowed error error matrix one way incorrect strengthening silent synapse occur diﬀusion messenger calcium one spine head another illustrated figure assume output neuron connected least potentially input neuron 17 amount error depends number synapsis input neuron make output neuron relative dendritic length l well factor space constant dendritic calcium diﬀusion λc 18 hill coeﬃcient calcium action h 58 63 amount calcium attenuation deﬁne per synapse error factor b 0 1 b l 1 equivalently synaptic quality q 0 1 q 1 see text deﬁnitions detail formula say per synapse error b proportional two factor ratio length constant calcium spread λc dendritic length l eﬀective calcium coupling constant ah two adjoining spine assumes extra input added dendritic length remains see discussion probability q correct synapse strengthened depends b network size text analyze plausible model develop two approximation q q n b continuous model weight adjust continuously q 1 discrete model weight adjust discretely q 1 error spread consider diﬀerent possibility way part hebbian update xiy could spread diﬀerent connection presumably result inttracellular diﬀusion 6 messenger calcium general reﬂect particular anatomical lationships synapsis expressed e could change learning proceeds examine two extreme case first connection made single ﬁxed synapse parallel cell connection 76 second case connection equivalent tabula rasa 19 20 nearest neighbor model connection consists single ﬁxed synapse calcium only spread two nearest neighbor synapsis e ha diagonal element q oﬀdiagonal element 2 e q ǫ 0 ǫ ǫ q ǫ 0 0 0 ǫ q ǫ 0 0 ǫ q ǫ ǫ 0 ǫ q appearance ǫ top right bottom right corner reﬂects periodic boundary condition deﬁne trivial error rate ǫ hebbian adjustment lack speciﬁcity marked figure red asterisk curve model connection equally distant no privileged connection oﬀdiagonal element e equal e q ǫ ǫ ǫ ǫ q ǫ ǫ ǫ ǫ ǫ q ǫ ǫ ǫ ǫ q ǫ ǫ ǫ ǫ q important notice error matrix case becomes singular q ǫ update leak erroneous connection large update right connection call value trivial error value n corresponds n 1 n 1 n discrete model n 1 n continuous model biological purpose need only consider error smaller trivial value arrangement could arise two nonexclusive diﬀerent way connection composed large number α ﬁxed synapsis possible conﬁgurations synapsis occur synapsis not ﬁxed location appear disappear randomly sible location touchpoints 19 17 axon approach dendrite close enough new spine create synapse case assuming dendrite axonal geometry ﬁxed 21 22 23 postsynaptic neuron ha reservoir potential synapsis 17 composed 2 shifting subset anatomically existing synapsis voir incipient 3 synapsis spine could form see text order maintain constant weight absence learning synapse would replaced another synapse equal strength possibly connectivity could done ply only silent synapsis appear disappear since connectivity would not conserved evidence case 40 7 synapsis appear disappear one ha problem synapsis equally plastic learning rate γ stochastic change overall number synapsis comprising connection change overall learning rate connection simplest case number new silent plastic synapsis happen appear connection overall weight unchanged learning rate increased one way prevent would ensure only one synapsis comprising connection plastic 3 nonlocal rule another way would average number potential synapsis comprising connection reasonably high perhaps ﬂuctuations relatively small several case average number actual synapsis connection around 5 62 26 since may only form 10 total potential synapsis learning rate diﬀerent connection wold fairly similar course identical course ready solution work axon dendrite would intersect suﬃciently often implying high degree branching although some claim weak synapsis plastic 12 evidence suggests synapsis equally plastic 77 could achieved strengthening added new plastic unit synapse previously added unit rendered implastic 3 78 also see discussion related issue synapsis comprising connection diﬀerent electrotonic distance along dendrite therefore inﬂuence spiking diﬀerently diﬀerent eﬀective learning rate rumsey et al 83 proposed separate antistdp used equalize eﬃcacy weight strong evidence way achieve complete connectivity 19 40 think biologically plausible assumption principal target analysis stabilized weight vector modiﬁed inaccurate oja model diﬀers principal component analysis section 3 text show inspeciﬁc learning algorithm still converge principal eigenvector wec ec rather principal component wc input distribution since output oja neuron allows optimal input reconstruction least least square sense see discussion hebbian inﬁdelity lead suboptimal performance quantiﬁed eﬀect inﬁdelity cosine angle θ principal component wc c principal eigenvector wec ec stabilized weight vector absence respectively presence error co θ wct wec examined measure error depends parameter size n input error ǫ given analysis arbitrary input distribution rather intractable detail only simple case uncorrelated section correlated input section illustrating result matlab plot simulation 4 result start example simulation behavior erroneous rule case using either uncorrelated figure correlated figure c input case network initialised random weight uncorrelated case absence error correct principal component learned rapidly accurately small ﬂuctuations away reﬂect nonzero learning rate obvious 8 figure 2 eﬀect error performance oja network n 10 neuron plot represent cosine angle θ weight vector principal nent updating process left input uncorrelated gaussian vector one source variance 2 compared others right correlated input generated mixing source equal variance dom mixing matrix element distributed uniformly 0 case total error n ǫ wa initially set zero weight vector converged quickly ﬁrst pc co θ 1 remained minor ﬂuctuations error wa increased zero step 4 104 epoch producing approximately stepwise decrease performance equilibration time increased ror increased step height associated ﬂuctuation increased decreased note correlated case error produce only small decrease performance since principal component already point approximately direction 1 1 1 error rate dependence performance control parameter steepest see figure figure show eﬀect error performance depends degree correlation present case performance measured co θ gradually deteriorates progressive increase error although magnitude decrease depends error correlation remainder result explore eﬀects detail using calculation implicit analysis figure 2 also show learning somewhat slowed inaccuracy expected however not analyze learning kinetics uncorrelated input section show network performance depends quality factor q 0 n alternatively error factor b 1 case uncorrelated input illustrate dependence combination matlab plot analytical result uncorrelated input case consider diagonal c higher variance ﬁrst component c λ 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 2 9 λ 1 wc 1 0 0 case co θ wec measure system performance studied co θ change error either ǫ b n numerically calculated co θ function error two case error apportioned two neighbor model figure connection error onto model figure figure 3 dependence co θ error factor b case uncorrelated input λ 2 continuous error model curve corresponds diﬀerent n shown legend left continuous error nearest neighbor model note increasing n value value total error ǫ increase any ﬁxed per synapse error b reducing performance co θ curve shown solid line trivial error value q ǫ b learning inspeciﬁc red dot beyond point curve unbiological shown dotted right distribution weight asymptotically stable weight vector principal eigenvector ec ﬁxed network size n 51 ﬁxed variance λ diﬀerent value per synapse error lower quality similar weight become b value shown le trivial except curve marked diamond almost update transferred neighbor exception weight neuron labeled 26 weight decay approximately exponentially function distance neuron illustrated black dashed curve shifted exponential space constant one unit neuron space constant calculated equation 7 3 corresponding value b n λ neuron curve model figure understood following way first consider curve given network size outlined method absence error high variance connection grows rapidly low variance connection eventually completely winning ﬁnal weight vector point rection however presence error immediate neighbor strengthen would absence error result leakage preferred connection see figure mean future pattern produce extra strengthening neighboring connection stronger produce larger output tra strengthening neighbor lead increased strengthening neighbor 10 neighbor line since weight vector normalised wrong strengthening combine reduce preferred weight although long learning show some speciﬁcity preferred ﬁnal weight always strongest see figure figure show distribution equilibrium weight function distance preferred neuron error nearest neighbor case corresponds ﬁtness model simulated previous work analysed large n limit 3 ﬁtnesses input variance model weight distribution nonpreferred connection followed double exponential function distance suﬃciently small error large λ distribution weight nonpreferred low variance connection close single exponential distribution found limit ﬁtness model see dashed curve figure also found space constant weight distribution varied expected manner hebbian error figure variance figure 4 discrete update model left dependence co θ error factor b case uncorrelated input λ performance measured co θ θ angle principal eigenvector ec principal component curve corresponds diﬀerent network size n shown legend curve plotted solid line b zero trivial value n n dotted line error b larger trivial value range not biological point steepest downward slope inﬂection point marked graph red asterisk consistently calculation inﬂection point always situated zero trivial value n getting arbitrarily close zero large enough note n 10 curve agrees almost exactly result figure b value converted corresponding ǫ value right ﬁxed network size n 20 diﬀerent value input variance λ show dependence output performance co θ synaptic error b b 0 n curve corresponds diﬀerent λ value λ λ variance close λ stable weight vector approach w 1 1 1 independently error co θ value b also co θ λ b n performance improves larger variance agrees analytical result dotted line show perturbation approximation λ 5 see text work well only low error remainder paper focus model quality network q 0 error ǫ present detail only discrete update model since seems biologically realistic 28 29 30 continuous 11 case rather similar treated text numerical calculation performance diﬀerent per synapse error value various network size uncorrelated case shown figure curve n 10 plotted trivial error value b 10 10 learning completely inspeciﬁc smoothly increasing degradation performance error drop much lower value inspeciﬁc learning seen previous case since error aﬀects nonpreferred input equally b weight vector parallel 1 1 1 limiting cosine case n 10 10 remaining plot figure unbiological point curve beyond trivial value shown dotted figure show plot performance b diﬀerent value variance λ case n large change performance small increase λ one especially low error value λ 1 eigenvalue equal corresponding eigenvectors only tiny bit error stabilises behavior only preferred weight selected although never perfectly obtained approximation co θ small ǫ value using perturbation theory 81 co θ λ λ 2 n 1 2 figure show formula agrees well exact result suﬃciently small proceed analytic treatment numerical result characteristic polynomial error matrix p e x det e q 1 note e invertible except q ǫ q ǫ depend biological parameter see text 2 maximal eigenvalue µ ec given case larger solution quadratic equation λ 1 ǫ λ λ 0 maximal eigenvector direction 1 1 ǫ n λ selectivity value express strongly one weight favored one input active outcome reﬂects fact no weight except corresponding input preferred no privileged neighbor relation behavior boil competition preferred weight set nonpreferred weight leading quadratic equation usually estimate output performance co θ simplest calculate tan θ related co θ co θ p θ 1 θ 0 h q tan θ q λ q µ q 1 µ 2 µ 2 µ 3 derivative respect 0 0 consistent simulation performance decay quality factor decrease 12 discrete continuous update model show similar feature section text angle θ θ q measured tangent h q tan θ q decrease q go 0 case h 1 0 corresponds perfect performance perfect quality also h 0 n show output degrades severely error larger value network size synaptic crowding moreover 1 n show rate angle decay q 1 get steep large since slope always ﬁnite ﬁnite epsilon no error catastrophe see discussion le obvious observation concern inﬂection point graph decay rate error sensitivity performance steepest see red asterisk figure 5 although exact estimate intractable obtained using expression derivative tan θ lower bound inﬂection point always situated interval n 1 equivalently 0 n reporting synaptic error see text figure suggests inﬂection point always move left step leftward shift trivial error value n get larger summary uncorrelated case high quality ensures excellent formance except input numerous high n almost indistinguishable low λ conversely since performance only improves slightly error reduced initially low value would diﬃcult evolution attain low error rate next asked feature remain true correlated input correlated input study equilibrium behavior network response two simple case correlated input model following covariance matrix c 1 λ ξ ξ λ 1 ξ ξ ξ ξ 1 ξ ξ ξ ξ 1 3 1 λ ξ 0 c ha higher covariance one pair c λ ξ ξ ξ ξ 1 ξ ξ ξ ξ 1 ξ ξ ξ ξ 1 4 λ 1 ξ 0 c ha small uniform background covariance one high variance input figure 5 illustrates dependence performance error various background correlation ξ value network 20 input two case left plot higher covariance pair right plot high variance neuron uniform background correlation since slope along curve corresponding diﬀerent ξ value not simply scaling follows way performance degrades error depends background correlation intermediate background correlation ξ output show highest sensitivity small error weak strong background correlation maximum sensitivity appears larger value error result 13 figure 5 dependence co θ error factor b discrete update model size n 20 λ 4 ﬁxed curve illustrates diﬀerent background covariance ξ shown legend inﬂection point curve wa marked red asterisk inﬂection point closest zero intermediate value ξ agrees result figure left high covariance pair input distribution right uniform covariance input inﬂection point ﬁrst moving left ξ increase moving back right see figure rightward movement only visible lower ξ value shown figure figure 6 show dependence performance error various network size using ﬁxed λ ξ value type background correlation model case initial eﬀect error strong large network size synaptic crowding performance reach rather constant level fairly close level high background correlation tend equalize weight even absence error analyze numerical result model 1 high covariance one pair principal component ec unit vector pointing direction 1 1 output selectivity n ǫ λ ξ given 1 1 1 λ ec ec 0 smaller root quadratic deﬁned text competition set preferred nonpreferred weight model selectivity used interpret feature output mance various degree error see discussion explicit formula rather complicated calculated upper bound lower bound r ǫ simpler yet still suggest some main feature r ǫ 1 1 λ 1 n ǫ λ n ǫ ξ 14 lim r ǫ 1 lim r ǫ 1 λ λ n ξ compared measure output performance cosine angle θ θ n ǫ λ ξ principal eigenvector 1 1 ec principal component input 1 1 n 0 λ ξ selectivity absence error measure performance compared discussion co θ n n p 0 n figure 6 dependence co θ error factor b discrete update model variance λ ξ ﬁxed λ 4 ξ curve corresponds diﬀerent network size n inﬂection point curve marked red asterisk infection point approach zero n get arbitrarily large left pair input distribution right uniform variance input red dot show trivial error value curve shown dotted beyond point nonbiological range model 2 uniform pairwise covariance compute eigenvector w ec corresponding µec expected get w direction 1 1 output selectivity given 1 1 1 λ ec ha upper lower bound r ǫ 1 1 λ n ξ ξ also lim r ǫ 1 lim r ǫ 1 λ n ξ relation co θ given co θ n n p 0 n 15 figure 7 dependence output sensitivity δ co θ covariance ξ n 20 λ 4 ﬁve error value ǫ 0 ǫ ǫ ǫ ǫ ǫ curve corresponds diﬀerent error shown legend output show sensitivity ǫ intermediate error value ǫ low covariance value 0 ξ maple software wa used compute δ generate picture selectivity zero error thus model co θ ha similar dependence b n see figure 6 error sensitivity deﬁne quantity δ error sensitivity performance δ θ figure 7 show maple plot dependence δ background covariance measured diﬀerent error rate two correlated case δ always negative error degrades performance uncorrelated case except course ξ 1 erroneous equilibrium eigenvectors already form also δ small near zero error uncorrelated case low error rate adding background correlation increase error sensitivity maximum error sensitivity greatest intermediate error rate eﬀects reﬂect two opposing process background correlation increase rate growth connection connection point view look though pressure driving selective growth one figure two figure connection ha reduced equivalent reduction λ figure increase background correlation tend make weight equal synergistic increase error second eﬀect dominates high error value looked maple calculation sensitivity performance change ground covariance co θ various value ǫ used understand dependence size ﬂuctuations visible figure 2 parameter pret ﬂuctuations small deviation input statistic average value small spontaneous transient perturbation parameter ξ amplitude 16 therefore follow co θ found co θ increased error increased seen figure figure independent equal variance source linearly mixed generate correlated random vector used input erroneous oja rule correlation act background tends equalize weight even absence error adding error ha relatively little eﬀect model extension consider input distribution variance higher uneven two component covariance uniform possibly zero correlation matrix form c ξ ξ ξ ξ ξ ξ ξ ξ 1 ξ ξ ξ ξ 1 5 1 modiﬁed correlation matrix ec ha eigenvalue q multiplicity three eigenvalue distinct lie respectively within interval q 1 q q q max q nξ 1 ǫ n ξ ǫ ǫ clearly µ always unique maximal eigenvalue ec case uncorrelated input example ǫ 0 corresponds 1 0 maximal eigenvalue µ corresponding eigenvector ﬁrst element 1 0 0 standard orthonormal basis rn error increase ǫ 0 ǫ eigenvector 0 0 see figure 8 evolves ratio decay dramatically ǫ 0 ﬁnite value ǫ trivial value weight equalize thus figure thus situation 2 highly inevitably unequally active input selectively wired hebbian learning presence error actually promotes desired outcome least large n case input correlated dependence µ parameter cated eigenspace µ direction 1 1 selectivity depend via eigenvalue µ system parameter 1 q µ q easy observe ǫ trivial error value hence uncorrelated case weight tend equalize error get close trivial value see figure 9 however slope decay diﬀerent uncorrelated case since always ﬁnite input correlated ξ 0 even zero although result not general seem apply various situation increasing degree background correlation figure 2 similar behavior 17 figure 8 discrete update model n 20 cell receiving uncorrelated input left dependence co θ synaptic error b shown b increase zero trivial value n 1 n 1 20 20 trivial error n equalizes weight make co θ 20 independently distribution variance right evolution normalized weight respect b increase zero n weight equalize ratio drop figure 9 discrete update model n 20 cell receiving correlated input variance 1 small uniform covariance ξ left dependence co θ synaptic error b shown b increase zero trivial value n 1 n 1 20 20 trivial error n equalizes weight co θ varies n since principal component c varies parameter angle θ trivial error value right evolution normalized weight respect b increase zero n weight equalize ratio drop initial ﬁnite value observed instance oja network learning correlated input obtained rotation vector one see lations increase inﬂexion point plot shift left right figure 10 compare figure 5 figure conﬁrming low error 18 figure 10 oja network learning distribution correlated input obtained rotation λ 10 n amount rotation alpha wa varied show inset introducing correlation increase error sensitivity 5 discussion overview present study form part ongoing eﬀort 3 31 evaluate novel sweeping ultimately prosaic hypothesis neural basis mind sophisticated brain machine learning structure world 32 33 34 propose key issue becomes accuracy synaptic learning key issue underlying darwinian evolution form molecular intelligence 35 replication accuracy 36 37 38 case physical limit biological accuracy 79 would set amount compressed information stored view neocortical microcircuit would device allowing highly accurate synapse adjustment thus learning weak order correlation 31 thus core machinery cell devoted accurate replication 39 even though diﬀerent type cell perform additional complex function core neocortical circuitry would allow accurate learning remaining variable circuitry traditionally studied would merely specialized though interesting result provide background exploration simple apparently powerful idea reviewing detail brieﬂy consider idea ha completely overlooked seem several interacting factor apart inadequacy previous explanatory attempt first widespread assertion hebbian adjustment completely curate 40 41 underlying calcium signal doe not spread beyond spine head 42 assertion assisted misleading colorizations ambiguous data 43 run counter necessity spine neck resistance electrical diﬀusional must relatively low powerful synapsis well experimental evidence 9 6 13 second almost relevant neural network theory 34 44 16 ha developed using speciﬁc learning rule partly analytical simplicity partly widespread 19 currency synapse isolation hypothesis partly technical work naturally focus proving algorithm work partly physical error not occur serial computer implementation algorithm likely however analog massively parallel implementation may required scale neural net algorithm problem 45 also suﬀer learning accuracy problem second biology usually safely sloppy one glaring exception dna replication almost miraculously accurate exception only relevant parallel since darwinian evolution neural learning adaptive process store information environment based repeated interaction 46 47 35 third view neocortical circuitry may not devoted specialized task not intrinsically absurd diﬃcult swallow focus elucidating current paper extends initial attempt 2 3 exploring eﬀects hebbian inaccuracy neural network learning selected unsupervised learning two reason first likely vast majority learning unsupervised simply labeled example relatively rare second many study supervised learning use data doe not conform statistical model course whole point supervised learning present paper explore classic oja model neuron principal component analyzer 4 15 next section justify choice oja model biologically realistic since focus biological realism may seem odd study model apparently almost formal unbiological one imagine however goal not construct model learning realistic neuron better understand one speciﬁc aspect realism ha hitherto neglected possible inspeciﬁcity learning rule making aspect realistic would unnecessarily complicate task might prevent analytical treatment section argue oja rule not biophysically unrealistic ﬁrst appears however not address issue whether brain actually pca seems likely least visual system locally decorrelating representation developed 48 49 50 34 51 insofar learned crosstalk would probably similar eﬀects ﬁrst obvious simpliﬁcation oja rule pattern element weight allowed negative however only positive pattern allowed hebbian part rule would always positive rule only requires ltp part conversely normalizing part rule would always negative would only require biological mechanism ltp ltp considered furthermore seems brain negative positive part signal represented using diﬀerent neuron oﬀcells retina thalamus mean even though two half oja rule would operate biologically ﬁxed opposite polarity ltp ltd overall eﬀect biological implementation would original oja rule allows either polarity part rule next simpliﬁcation oja rule temporal relation incoming tern ignored time often great biological importance music movie sometimes le painting sculpture extent brain system specialize spatial temporal resolution varies crucial point likely adding learning rule stdp would unlikely make easier implement physical synapse level basic problem plasticity ha triggered calcium narrow time window calcium signal 20 must larger given biological binding process diﬀusion controlled place even greater demand pump buﬀers viewed diﬀerent angle creased speed only achieved decreased aﬃnity hence worsened selectivity regime also diﬃculty mathematics dependent learning rule complicated especially spread error synapsis ha represented error matrix simplest case multiplication would replaced convolution closely related implicit oja model however seems justiﬁed temporal order unimportant mean rate thought ﬁring probability some suitable small time window membrane time constant behavior synapse response repeated interleaved stochastic spike tern meanﬁeld approximation identical deterministic response input lead implementation learning rule standard though view hebbian multiplication biologically implemented making tiny increase strength whenever spike fall within small time window presumably comparable membrane time constant produce stochastic multiplication 45 ﬁring ities input output ﬁre independently implied underlying assumption ﬁring occurs poisson fashion whether tiny reﬂects small change produced every coincidence bigger change generated stochastically only some coincidence discussed seems u within constraint atemporal viewpoint match tween hebbian multiplication oja rule actual machinery detection pretty good direct experimental evidence spike arrives near peak activation nmdars doe lead extra 24 also increased ity strengthening 52 53 widely thought reﬂects nmdar 54 though no adequate quantitative model ha constructed certainty calcium current voltage relationship 55 furthermore extra inﬂux smaller timing action potential suboptimal since time course nmdar activation similar unitary epsps 56 seems rough matching increased calcium entry time course increased ﬁring probability following unitary activation unitary ﬁring probability ha rapid time course corresponding unitary epsps could compensated complication blocking detail 57 54 model us weight discrete update model outlined text although update agreement ments 28 29 30 continuous weight still obtained limit larger number synapsis smaller number synapsis approximate crete model mostly used result becomes exact low error limit conversely continuous model sometimes also use figure 2 approach exact discrete model high error large number synapsis one diﬃculty surprisingly linear form hebb part oja rule model inspeciﬁcity presented text introduce parameter h reﬂects hill coeﬃcient activation camkinase 58 measure number calcium ion required activate enzyme camkinase come two diﬀerent gene lead several diﬀerentially spliced variant somewhat like bk channel protein mediate wide kinetic range hair cell electrical turning 59 80 likely parameter well ca aﬃnity tailored 21 diﬀerent type synapse diﬀerent function nevertheless h likely always greater one meaning change strength synapse result spine head calcium increase whether generated locally secondary increase synapsis likely nonlinear function ca conversely simplest case size calcium signal would linear function postysynaptic ﬁring would lead nonlinear hebb rule would not general solely driven input covariance matrix representation would therefore approximately gaussian input statistic suboptimal possibility nonlinearities exploited reduce discussed one way neuron could handle problem cancel intrinsic nonlinearity calcium transduction machinery suitable inverse nonlinearity calcium induction machinery overall strength change roughly linearly related product ﬁring rate however strict cancellation would probably diﬃcult achieve needed demonstration approximately linear hebb rule learns approximately ﬁrst pc general case may elusive outcome nonlinear learning depends detail input statistic unlike linear case only sensitive pairwise statistic simplest case input statistic exactly gaussian nonlinear learning behaves linearly since higher order statistic cancel 61 extent input brain encounter gaussian probably good approximation earliest stage sensory processing strict cancellation might not needed furthermore adding learning inspeciﬁcity ha eﬀect linearising learning rule indeed completely prevent learning high order correlation cox adam unpublished conclusion though important nonlinearities mechanism translate spike strength change overall rule linear eﬀect combination low h cancellation gaussian statistic inspeciﬁcity problem likely not early linear learning later nonlinear learning ﬁnal issue whether learning done every pattern many pattern accumulated present work used recipe presumably batch mode would work almost well provided number pattern batched small compared total number pattern however biological weight adjustment stochastic 28 distinction may not important probability weight change following spike coincidence small must learning slow follows only way reliably produce weight change result series coincidence small probability accumulate thus synapse contain register past coincidence coincidence would increment register one step count register exceed threshold weight would increased could also stochastic version coincidence increment register variable amount equal one average model register content also slowly decay else decremented anticoincidence learning rate would set threshold decay rate experimental study ltp single synapsis strongly suggest batch model since many repeated pairing correctly timed postsynaptic spike some case depolarisation required reliably induce ltp occurs ner 62 28 however averaged many synapsis comprising connection overall outcome would multiplicative hebbian rule simple mechanism batching would calcium increase synapse activated binding calmodulin some fraction camkinase molecule calcium pulse calmodulin would dissociate leave some kinase molecule phosphorylated 22 successive pulse eventually enough would activated entire set camkinases would autophosphorylate triggering strengthening 27 63 58 biological implementation normalizing ltd part oja rule le clear part rule elegant since basic normalization step division euclidean norm weight vector lead approximation purely local online rule however two nontrivial biophysical requirement 1 calculation 2 multiplication recent work neocortex 64 65 suggests ltd occurs following way backpropagating spike lead calcium signal trigger endocannabinoid release local dendrite perhaps spine endocannabinoid diﬀuses back presynaptic specialization activates endocannabonoid receptor near neous activation presynaptic nmdars glutamate transmitter release depressed seems likely previously favored model level spine calcium achieved ltp ltd inducing stimulus produce determines sign strength change 63 66 wrong 25 new picture ltd seems well suited meet two biophysical requirement normalizing part oja rule sense rule would formal description endocannabinoid enzyme triggered calcium entering channel activated backpropagating spike would implement multiplication would achieved requirement simultaneous activation nmdar dependence ω could achieved two way endocannabinoid signal might proportional postsynaptic strength synapse extent activation presynaptic nmdar could depend amount glutamate released would depend extent active zone known long term adjust match psd area hence presumably synaptic strength thus synaptic strength would slowly adjust combination matched distinct adjustment reﬂect arriving spike way required oja rule background necessary discus important issue accuracy normalizing part oja rule clearly ltd triggered presynaptically retrograde messenger one must consider possibility extracellular ltd crosstalk ltd part rule implemented described error diﬀusioin retrograde messenger diﬀerent synapsis neuron would not matter although diﬀusion synapsis located neuron would matter problem avoided weight requirement presynaptic nmdar activation simultaneously released glutamate dependent occurence presynaptic spike instead apparently unbiologically weight read postsynaptically combined signal retrogradely correct presynaptic structure diﬀusion retrograde signal would cause normalization error nutshell could modeled adding new error matrix f averaged rule would become f wtxxtw w ﬁrst glance appears normalization error could cancel hebbian error f appropriately matched e adjustment quality cancelation would correspond weight erroneously forgetting exactly erroneously learns pattern problem averaged value e f simple closely related instantaneous value e f least locally quite diﬀerent one involves intracellular diﬀusion extracellular diﬀusion furthermore stability algorithm also aﬀected 23 performance pca mutual information paper used cosine angle learned correct vector simple measure performance natural measure would mutual information input vector output scalar requires knowledge distribution component input vector simplest case distribution gaussian known case pc optimal representation input single scalar mutual information given log plus constant since output variance maximal weight vector parallel pc mi erroneous case variance le mi found curve decline mi function parameter error n sian input figure 11 similar curve co θ shown preferred use co θ performance measure doe not depend input statistic figure 11 mutual information mi diﬀerent error value network size sian input left plot show mi function error diﬀerent network size mi depends output variance since input contribute output variance mi increase network size error increase relative contribution poorly correlated input output variance mi decrease error right mi dotted line diﬀerent error rate expressed fraction mi zero error compared co θ diﬀerent error solid line various network size case error decrease mi co θ small closely track mi biophysics error early experiment wa found ltp wa connection located far apart dendrite 10 recent work showed ltp induced localized set synapsis spread unstimulated synapsis le 50 micron away no spread wa seen beyond 70 micron 6 uisng somewhat diﬀerent protocol spread wa not seen even 10 micron separation 67 even recent work examined possible spread ltp induced single synapsis no change detectable even closely neighboring synapsis 12 leading conclusion hebb postulate wa ﬁrst time directly conﬁrmed however background noise small change 1 would not detected single synapse test place upper limit per synapse error b expected biological error rate seems quite low however even b value 1 signiﬁcant 5 decrease performance occur 24 presence background correlation figure 5 previously noted error sensitivity approach zero low error case extremely recent result examined whether stimulus one synapse modify threshold inducing ltp neighboring synapsis 13 wa argued ltp doe not spread eﬀect doe seems u distinction not valid observed ltp none model updating process stochastic discrete weight change quasi continuously cally any change threshold discrete model would indistinguishable spread ltp continuous model equivalence implicitly recognized harvey svoboda argument result match vious result however harvey svoboda not explicitly show character ltp presumably obtained experiment therefore conclude result fact support basic premise ltp not completely seems likely spread ltp discrete updating spread lowering mediated intracellular diﬀusion mobile factor optic tectum tao et al obtained evidence factor calcium since ltp spread retracts maturation parallel restriction calcium diﬀusion 9 harvey svoboda argued ltp crosstalk experiment not due calcium diﬀusion issue related previous suggestion calcium conﬁned spine head action calcium pump located narrow spine neck 41 crucial interpretation result not discussed detail view calcium localization good perhaps good 99 complete could only perfect head isolated dendrite not aﬀect electrically rendering synapse useless also note since protocol rapidly shut diﬀusional coupling spine head shaft 68 presumably process seems unlikely any factor calcium release could travel faster calcium furthermore any avoidable spread ltp would probably degrade utility hebbian learning shown current study only circumstance ltp leakage might actually beneﬁcial rather unusual scenario envisaged mel 69 mentioned harvey svoboda basic computational unit brain not neuron dendritic branch thus branch act element one might want selected subset input target branch selection could driven initial phase inspeciﬁc hebbian learning subsequently learning would restricted single synapsis branch rather neuron would learn pc however successful implementation strategy would require host undocumented biophysical specialization local backpropagating spike calcium perhaps electrical constriction branchpoints etc etc fact feature neuron already known furthermore number possible input single branch would much le number input neuron even fatally putative advantage branch neuron would impossible reap could no corresponding increase density synapsis work suggests main factor limit learning hence neural information processing model unlikely brain doe pca analysis strict sense uncorrelated tation correlated activity set input neuron projection complete 25 set principal component even high variance subset would not eﬃcient since ignores fact neuron ha similar channel capacity would feasible since error would inevitable output neuron connected every input would intolerable however likely brain doe develop related uncorrelated representation using hebbian learning example ters seen retina thalamus viewed local form decorrelation 49 50 48 output neuron learns only subset input lower error rate since synapsis better separated oja model normalization tive keeping weight vector near hypersphere surface vertical adjustment 70 subtractive normalization linear hebb rule ha also much studied 70 34 weight adjusted amount weight vector length maintained subtractive normalization requires limit weight learning point weight vector corner weight space hypercube typically closest second eigenvector c 70 simulated eﬀect error rule cox adam unpublished ﬁnite learning rate rule stochastic even absence error weight vector sometimes point corner close correct corner found error increase tendency graceful manner error rate correct corner occupied only observed sharp collapse performance error catastrophe using nonlinear hebbian rule cox adam unpublished only condition performance drop chance level linear hebbian model trivial condition learning becomes completely inspeciﬁc example high n value synapsis crowded experimental test any evidence crosstalk type consider actually doe degrade precision wiring brain main problem no case know exactly wiring supposed unclear whether any observed deviation ideal wiring bug biology inaccurate feature biology cleverer focus particularly case wiring retina thalamus make some simplifying assumption main assumption 1 activity diﬀerent retinal ganglion cell approximately equal uncorrelated ensemble natural image 48 49 50 2 although thalamus simple relay 72 approximately 3 hebbian mechanism control least ﬁnal reﬁnement thalamocortical wiring recognize none assumption proven exact form useful initial framework discussion role error previous author also suggested some aspect detailed retinothalamic wiring might result inevitable wiring inaccuracy rather initial information processing justify assumption current model suggest organization ganglion cell rf reﬂects decorrelating strategy response limited number limited bandwidth output channel statistic natural image 48 49 50 doe not mean ganglion cell completely uncorrelated merely uncorrelated possible still transmitting high level visual information particular assume intercalation thalamic relay doe alter spatiotemporal rf ganglion cell doe only response descending ences resulting initial cortical processing thus default transformation would identity spike would transmitted clearly considerable divergence convergence retinothalamic wiring 72 view 26 provides coding ﬂexibility only exploited cortex ha made preliminary analysis since retinal representation some sense mized sent colliculus any additional optimization done thalamus only response cortical feedback much evidence many aspect retinothalamic wiring achieved 60 nmdar based plasticity many lateral geniculate relay cell get only one retinal input thus act simple relay however although almost cell get 1 dominant input many also get 1 subsidiary input hebbian mechanism explain case input uncorrelated possible failure eliminate subsidiary input 84 reﬂects hebbian inaccuracy particular principal subsidiary input rf tend close together could show some low level correlation show background correlation low level error synergistic alonso et al 82 argued convergence might useful provide receptive ﬁeld diversity whether spatial mixing useful would depend detail spatiotemporal noise signal 6 conclusion although widely appreciated physic set ultimate limit biology little attention ha paid physical limit process interest human ing oja rule simplest unsupervised learning rule capture key point hebbian learning driven pairwise correlation form input covariance matrix not surprisingly rule inaccurate fails accurately learn expected typically useful result although failure graceful severe patterned activity driving growth particular weight rather weak propose even though chemical change driving hebbian learning largely conﬁned synapsis learning induced high density synapsis along dendrite mean signiﬁcant crosstalk therefore somewhat degraded learning inevitable future work hope show inevitable crosstalk completely prevent hebbian learning correlation unless additional interesting machinery roughly corresponding basic neocortical microcircuit employed 27 text stability calculate jacobian matrix dfw ﬁxed vector w lemma dfw γ c cw wt cw proof call g w wt cw w f w w γ cw w gi w wt cw wi j w x k l cklwkwl wi 2 x k ckjwk wi 2 cw jwi j w x k l cklwkwl wi x k l cklwkwl 2 x k ckiwk cw 2 cw iwi wt cw dgw cw wt cw take orthonormal basis b eigenvectors c respect euclidean norm rn fix vector w pick any v v call λw λv corresponding eigenvalue dfw v v γ cv cw v wt cw v v γ cv cv wt cw w v γ λvv λvv 1 λw v dfw w w γ cw cw w wt cw w γ λww λww w γ 1 w b also basis eigenvectors dfw next goal generalize argument iteration function includes error new model introduces error matrix e r ha positive entry symmetric equal identity matrix r case error zero moreover assume ec ha strictly positive maximal eigenvalue multiplicity one f e w w γ ecw wt cw w note symmetric positive deﬁnite matrix c r deﬁnes dot product rn 28 vt cw v w eigenvectors ec corresponding eigenvalue λv λw orthogonal respect dot product indeed ecv λvv ecw λww hence λv λw follows 0 hence v w orthogonal respect given dot product ﬁxed point f e vector w wn ecw wt cw word w ﬁxed f e only eigenvector ec corresponding eigenvalue λw normalized λw clearly possible only λw ecw λww λw multiplicity λw one w orthogonal eigenvectors ec recall df e w γ ec cw wt cw take w ﬁxed point f w hence eigenvector ec eigenvalue λw wt cw w calculate df e ww w γ ecw cw w wt cw w w γ cw 1 w df e wv v γ ecv cv v γ λv v 1 λw v any eigenvector v ec eigenvalue λv λw error free case df e w ha eigenvalue le one absolute value only λw principal eigenvalue ec γ 1 λw 29 text synaptic dynamic appendix consider several type synapse formed dendrite cell made spine potential synapse refers any site axon within spine length dendrite 17 note original deﬁnition potential synapse ﬁxed spine length wa considered deﬁnition could generalised include distribution spine length suitable redeﬁnition parameter 73 axon dendrite assumed ﬁxed geometry divide set potential synapsis set existing synapsis form fraction f potential synapsis called ﬁlling factor stepanyants et al 17 set incipient synapsis empty site spine could spontaneously form generate existing synapse existing synapsis either silent active silent synapse plastic ha zero strength promoted active synapse result suﬃcient past conjoint activity across increment strength zero one unit stochastic manner depending recent history transynaptic activity repeated spike pairing coincidence leading ltp similarly active synapsis strengthen either continuous unitary etc manner result conjoint activity history activity expressed either accumulation small change stochastic insertion single ampa receptor result calcium increase accumulation small calcium signal register camkinase lead larger change insertion packet ampa receptor threshold reached mechanism would show similar behavior active synapsis also discretely decrement ltd see discussion perhaps eventually resilencing silent synapsis also disappear recreating incipient synapse assume process forming removing synapsis doe not change connectional strength go steady rate similar connection though may inﬂuenced overall level activity particulalrly neuron balance unsilencing silencing formation removal process set ﬁlling fraction typically around 10 17 stepanyants et al pointed f 1 allows anatomical plasticity however since come expense physiological plasticity strengthening weakening existing synapsis no net gain instead argue f 1 lower eﬀective error rate well eﬀective learning rate connection given presynaptic neuron postsynaptic neuron made average number α synapsis incipient existing since feedforward connection cortex typically made 10 actual synapsis comprised potential synapsis however learning proceeds fraction potential synapsis comprise existing connection gradually change input may become anatomically disconnected extended period ltd exceeds ltp synapsis become silent removed however anatomical disconnection doe not remove functional connectivity continued background generation silent synapsis 74 75 reinstate anatomical connection trial period number existing synapsis comprise connection average f hence total number existing synapsis along postsynaptic dendrite n f α learning model assumed rate learning approximately connection simple way ensure would existing synapse ha plasticity responsiveness transsynaptic activity α n approximately input since weak connection would tend fewer existing synapsis would slower learning 30 figure 12 two route part left show dendritic segment receives potential synapsis spine not shown large set input neuron some represented schematically circle top row connection three input neuron colored circle shown detail comprised three existing synapsis plus additional incipient synapsis not shown dendrite beneath middle red synapse colored red indicate neighborhood calcium diﬀusing synapse act action decline exponentially distance decreasing red shading set synapsis within neighborhood ordering random case green synapse purple synapse equally close distance along dendritic width irrelevent neither synapse preferred however case eg bottom red synapse either green red connection favored possible conﬁguration average spatially especially calcium action spread average distance synapsis shown connection approximately equally close black synapsis originate neuron shown black top row neuron not currently make synapsis dendrite shown dotted circle part corresponds mature network many input disconnected result previous learning remaining input quite strong rather stable synapsis part b right show earlier situation connection made fewer synapsis input connected top row bottom row show three snapshot state dendritic segment case colored input make only 1 synapse shift around dendrite perhaps ﬁrst disappearing result normalisation reappearing new location new spine contact branch axon pass nearby arrow show connection ha unique conﬁguration neighbor any one time detail get temporally averaged combination mechanism b mostly b immature network mature network could ensure average error matrix throughout protracted learning consider neighborhood relation diﬀerent connection argue connection approximately equivalent despite happenstance particular axodendritic geometry based three factor figure 1 part synapsis ﬁxed connection made many synapsis high n provided n not large possible neighborhood relation occur resulting spatial averaging figure synapsis described any given synapse eventually 31 sample possible neighbor figure case averaging spatial temporal eﬀective greater extent neighborhood shown red shading figure none factor alone would suﬃce make connection equivalent suggest some biologically reasonable combination would plausibly equivalence would apply electrical eﬀects connection factor 3 ing electrical spread rather powerful 41 crosstalk pattern factor 3 representing chemical spread le powerful context consider cortical volume v containing tightly packed neuronal cell process see 17 subset organized learning network calculate number np potential synapsis dendrite length l output postsynaptic neuron approximate cylinder diameter equal spine length divide axon little piece oriented solid angle θi axis distribution angle θi axonal piece isotropic dent particular axon number potential synapsis np given calculation similar one performed 17 np π 2 slla v n αn 6 spine length l dendritic length la average per neuron axonal length v volume contains network n network size note np proportional size n proportionality constant intrinsic network consequence number n existing synapsis fraction f np n f π 2 slla v n fαn 7 may reinterprete equation 7 density ρ existing synapsis along dendritic segment length l estimated ρ f π 2 sla v n depends linearly size n network goal estimate quality factor q learning network suppose output cell receives signal strengthen connection prescribed presynaptic axon target site activation signal trigger release messenger ideally appear only target site instead messenger not completely locally contained diﬀuses dendritic spine small portion reach dendritic cable leak along direction concentration distance x target spine follows function g x messenger could spread along dendrite some arbitrary site b represents existing either active silent synaptic site some proportion travel spine arriving spine head could update synaptic mechanism either activate silent synapse strengthen already active synapse 32 compute probability messenger get target site site b situated distance x along dendrite probability update activated site b depends power h messenger concentration px assume distribution existing synapsis along dendrite neous probability arbitrary synapse activated anywhere along length l dendrite b 1 l z l 0 pxdx 1 l z l 0 ahe λ dx ahλ hl 1 λ suppose knew position xn n existing synapsis along postsynaptic dendrite probability messenger diﬀusion site aﬀects exactly synapsis position xjk x p p xjk 1 1 xjn hence probability possible placement existing synapsis along drite update exactly k synapsis 1 ln z 0 l n x p p xjk 1 1 xjn dxn 1 ln x z 0 l n p p xjk 1 1 xjn dxn 1 ln x z 0 l p z 0 l p xjk dxjk z 0 l 1 z 0 l 1 xjn dxjn k l 0 p x dx 1 z l 0 p x dx k bk 1 k n k number possible combination choose k element set k site additionally activated mean k 1 synapsis overall dated including target site order obtain desired degree strengthening required learning rule normalization decides random one k 1 change applied discard one normalization probability synapse win update correct target synapse equal 1 k 1 n k n bk 1 calculate probability system turn keep desired target synapse no matter many wrong synapsis ﬁckered oﬀin malization process discrete model quality q information transfer 33 q n x 1 k 1 n k n bk 1 1 b n 1 n x n 1 k 1 n 1 1 b n 1 x n 1 j n 1 bj 1 1 b n 1 8 calculation q approximated small value b q ha linear expansion around b 0 original expression one approximation discrete model use simplify analysis applicable context since learning operates small error value another useful approximation given average treatment problem follows expected number large number epoch existing site updated along dendrite probability b site turn anywhere multiplied number site total error ǫ ratio number site expected erroneously update expected number aﬀected site n b nb nb 1 hence q 1 nb average estimate call continuous model oﬀers good approximation 8 small large value lim 1 1 1 nb b n 1 1 b 0 1 two expression agree limit n since case number synapsis becomes unlimited figure 13 show relationship exact expression q given discrete model two approximation use since n α f n may choose simplicity unit continuous model α f 1 q depend network size n q 1 nb approximate discrete model make α f 2 dependence n becomes q 1 n qn examine maximum error sensitivity depends q 1 n discrete continuous case continuous model q 1 n 1 1 calculate second derivative notice discussion applies value q 1 0 1 except q q ǫ ec becomes singular 1 n λ 2 n 0 34 figure 13 comparison two synaptic error model n 20 existing synapsis show dependence overall network learning quality respect error factor b three case continuous model q 1 approximate discrete model q 1 exact discrete model q b 1 n λ n λ nλ n n λ 1 0 inﬂection point h interval 1 n 1 discrete model q qn following applies only q value make e singular case n 1 n λ 2 n 0 1 n µ 2 2 0 inﬂection point h trapped interval 1 n 1 35 text output performance correlated input model 1 high covariance one pair matrix form e ǫm 1 c ξm λ 1 matrix mij mij 1 j n aij 1 aij 0 otherwise therefore ec ǫ ξ n ǫξ 1 λ ǫ λ 1 1 characteristic polynomial ec 1 1 1 1 x p x 1 1 1 1 n ǫ ξ 2 1 1 ǫ ξ ǫ λ eigenvalue ec x 1 1 multiplicity one x 1 1 multiplicity two obtained solution quadratic equation 1 n ǫ λ n ǫ ξ z 1 λ n ǫ ξ 0 9 substituted z 1 1 notice ǫ 0 equation becomes λ n ξ z n ξ λ 0 10 maximal eigenvalue ec computed µec 1 1 ec maximal eigenvalue c µc 1 c ec 0 smaller root 5 c 0 smaller root 6 moreover upper lower bound ec give u corresponding estimate µec µec 1 1 1 n ǫ λ µec 1 1 n ǫ ξ 1 n ξ µec 1 1 1 n ǫ λ n ǫ ξ notice µec go n bound respect parameter quite strict λ small model 2 uniform pairwise covariance compact matrix notation c characteristic polynomial ec 1 1 x 36 p x 1 λ 1 1 n ξ ǫ 1 1 1 ξ ǫ λ hence ec ha eigenvalue x 1 1 multiplicity n two others given remaining quadratic equation assume q ǫ remaining equation ha two distinct real root simplifying notation call z 1 1 consider two root ec ec new equation z λ n ǫ n ξ ǫ n λ ξ ǫ 0 11 unique largest eigenvalue ec µec 1 λ ec upper lower bound µec get µec 1 λ µec n ξ ǫ λ µec 1 λ 1 n ξ ǫ λ reference 1 hebb organization behavior wiley son new york 1949 2 cox kja adam pr implication synaptic digitisation error neocortical function neurocomputing 3 adam p cox kja new view thalamocortical function phil trans soc lond b 357 4 oja e simpliﬁed neuron model principal component analyzer mathematical biology 15 5 botelho f jamison j qualitative behavior diﬀerential equation sociated artiﬁcial neural network journal dynamic diﬀerential equation 16 6 engert f bonhoeﬀer synapse speciﬁcity potentiation break short distance nature 388 7 schuman em madison dv locally distributed synaptic potentiation hippocampus science 263 8 bi spatiotemporal speciﬁcity synaptic plasticity cellular rule mechanism biol cybern 87 9 tao hw zhang li engert f poo mm emergence input speciﬁcity ltp development retinotectal connection vivo neuron 31 10 andersen p sundberg sh sveen wigstrom h speciﬁc potentiation synaptic transmission hippocampal slice nature 266 37 11 levy wb steward synapsis associative memory element hippocampal formation brain 175 12 matsuzaki honkura n gc kasai h structural basis potentiation single dendritic spine nature 429 13 harvey cd svoboda k locally dynamic synaptic learning rule midal neuron dendrite nature 450 14 oja e karhunen j stochastic approximation eigenvectors eigenvalue expectation random matrix j math anal appl 106 15 diamantaras ki kung principal component neural network theory application john wiley 1996 16 herz j krogh palmer rg introduction theory neural tion lecture note volume santa fe insitutue study science complexity perseus book publishing 1991 17 stepanyants hof p chklovskii geometry structural plasticity synaptic connectivity neuron 34 2 18 zador koch c linearized model calcium dynamic formal lence cable equation j neurosci 14 19 le jv markram h spontaneous evoked synaptic rewiring neonatal neocortex proc natl acad sci usa 103 20 elman j bates ea johnson mh parisi plunkett rethinking innateness connectionist perspective development mit press 1996 21 holtmaat aj et transient persistent dendritic spine cortex vivo neuron 45 22 grutzendler j kasthuri n gan wb dendritic spine stability adult cortex nature 420 23 lendvai b stern ea chen b svoboda k plasticity dendritic spine developing rat barrel cortex vivo nature 404 881 24 nevian sakmann b single spine signal evoked coincident epsps backpropagating action potential spiny stellate cell layer 4 juvenile rat somatosensory barrel cortex j neurosci 24 25 nevian sakmann b spine signaling plasticity j neurosci 26 43 26 barbour b brunel n hakim v nadal jp learn synaptic weight distribution trend neuroscience 30 12 27 lisman j mechanism hebb process lying learning memory proc natl acad sci usa 86 38 28 petersen cch malenka rc nicoll ra hopﬁeld jj tion synapsis proc natl acad sci usa 95 29 connor dh wittenberg gm wang graded bidirectional synaptic plasticity composed unitary event proc natl acad sci usa 102 27 30 bagal aa kao jpy tang thompson sm potentiation exogenous glutamate response single dendritic spine proc natl acad sci usa 102 31 adam pr cox kja synaptic darwinism neocortical function rocomputing 42 32 barlow hb sensory mechanism reduction redundancy symposium mechanization thought process 10 33 marr theory cerebral neocortex proceeding royal society london b 176 34 dayan p abbott le theoretical neuroscience computational ical modeling neural system mit press 2001 35 adami introduction artiﬁcial life springer verlag ny 1998 36 tarazona p error threshold molecular quasispecies phase transition simple landscape model phys rev 45 37 swetina p schuster p error model cleotide replication biophys chem 16 38 eigen mccaskill j schuster p molecular quasispecies adv chem phys 75 39 watson jd baker ta bell sp gann levine losick molecular biology gene edition new york benjamin cummings 2003 40 alvarez va sabatini bl anatomical physiological plasticity dendritic spine annual review neuroscience 30 41 koch c zador function dendritic spine device subserving biochemical rather electrical compartmentalization neurosci 13 422 42 sabatini b oertner svoboda k ion dendritic spine neuron 33 43 purves fitzpatrick katz lamantia mcnamara williams neuroscience auer associate 2004 44 haykin neural network comprehensive foundation macmillan new york ny 1994 45 likharev kk strukov db cmol device circuit architecture cuniberti et al ed introduction molecular electronics springer berlin 2005 39 46 baum thought mit press 2004 47 volkenstein mv physical approach biological evolution springer varlag 1994 48 srinivasan mv laughlin sb dub predictive coding fresh view inhibition retina proc r soc lond b biol sci 216 1205 49 atick j redlich n towards theory early visual processing neural comp 2 50 atick jj redlich doe retina know natural scene neural computation 4 51 bell aj sejnowski tj information maximization approach blind separation blind deconvolution neural computation 7 52 kalisma n silberberg g markram h neocortical microcircuit tabula rasa pnas 18 102 53 markram h luebke j frotscher sakmann b regulation synaptic eﬃcacy coincidence postsynaptic aps epsps science 275 54 letzkus jj kampa bm stuart gj learning rule spike dependent plasticity depend dendritic synapse location journal science 26 41 55 wollmuth lp sakmann b diﬀerent mechanism transport nmda ampa glutamate receptor channel physiol 112 56 koch biophysics computation oxford univ press 2004 57 kampa bm clements j jonas p stuart gj kinetics unblock nmda receptor implication dependent synaptic plasticity j physiol 556 58 de koninck p schulman h sensitivity cam kinase ii frequency oscillation science 279 59 gaertner tr kolodziej sj wang kobayashi r koomen jm stoop jk waxham mn comparative analysis structure enzymatic property alpha beta gamma delta isoforms protein kinase ii j biol chem 279 60 shatz cj emergence order visual system development proc natl acad sci usa 93 61 hyvarinen karhunen j oja independentcomponent analysis wiley science 2001 62 markram h luebke j frotscher roth sakmann b physiology anatomy synaptic connection thick tufted pyramidal neurones developing rat neocortex j physiol 500 40 63 lisman j cam kinase ii hypothesis storage synaptic memory trend neurosci 17 10 64 sjostrom pj turrigiano gg nelson sb neocortical ltd via coincident activation presynaptic nmda cannabinoid receptor neuron 39 65 sjostrom pj turrigiano gg nelson sb cortical ltd absence postsynaptic spiking neurophysiol 92 66 shouval hz bear mf cooper ln uniﬁed model nmda dependent bidirectional synaptic plasticity proc natl acad sci usa 99 10836 67 chevaleyre v castillo pe metaplasticity hippocampus neuron 43 68 bloodgood bl sabatini b neuronal activity regulates diﬀusion across neck dendritic spine science 310 69 hausser mel b dendrite bug feature curr opin neurobiol 13 70 miller kd mackay role constraint hebbian learning neural comput 6 71 sherman sm thalamus relay curr opin neurobiol 17 4 72 sherman sm guillery rw exploring thalamus san diego academic press 2001 73 stepanyants hirsch ja martinez lm kisvarday zf ferecsko chklovskii db local potential connectivity cat primary visual cortex cereb cortex 18 1 74 holtmaat aj trachtenberg jt wilbrecht l shepherd gm zhang x knott gw svoboda k transient persistent dendritic spine neocortex vivo neuron 45 279291 75 lendvai b stern ea chen b svoboda k plasticity dendritic spine developing rat barrel cortex vivo nature 404 881 76 llinas rr walton kd cerebellum pp synaptic organization brain shepherd gm oxford univ press 1998 77 kopec cd li b wei w boehm j malinow r glutamate receptor cytosis spine enlargement chemically induced potentiation j neurosci 26 7 78 lisman j raghavachari l uniﬁed model presynaptic synaptic change ltp synapsis science stke 79 bialek w physical limit sensation perceptioin ann rev biophys chem 16 41 80 jones emc fettiplace r role activated channel splice variant tonotopic organization turtle cochlea j physiol 518 81 kahn pb mathematical method scientist engineer wiley interscience 1990 82 alonso jm yeh ci weng c stoelzel c retinogeniculate connection balancing act connection speciﬁcity receptive ﬁeld diversity progress brain research 154 chap 1 83 rumsey cc abbott lf equalization synaptic eﬃcacy synaptic plasticity j neurophysiol 91 5 84 chen c regehr wg developmental remodeling retinogeniculate synapse neuron 28 955966 42