received april 19 2021 accepted may 18 2021 date publication may 27 2021 date current version june 14 digital object identifier survey unsupervised learning image classification lars schmarje monty santarossa schröder reinhard koch member ieee multimedia information processing group kiel university 24118 kiel germany corresponding author lars schmarje la work wa supported land open access publikationsfonds funding program abstract deep learning strategy achieve outstanding result computer vision task one issue remains current strategy rely heavily huge amount labeled data many problem not feasible create amount labeled training data therefore common incorporate unlabeled data training process reach equal result fewer label due lot concurrent research difﬁcult keep track recent development survey provide overview often used idea method image classiﬁcation fewer label compare 34 method detail based performance commonly used idea rather taxonomy analysis identify three major trend lead future research opportunity method scalable application theory issue like class imbalance robustness fuzzy label not considered degree supervision needed achieve comparable result usage label decreasing therefore method need extended setting variable number class method share some common idea identify cluster method not share many idea show combining idea different cluster lead better performance index term unsupervised image classiﬁcation deep learning survey introduction deep learning strategy achieve outstanding success computer vision task reach best performance diverse range task image classiﬁcation 1 3 object detection 4 5 semantic segmentation 6 7 quality deep neural network strongly enced number image 8 genet 1 huge labeled dataset one million image allows training network sive performance recent research show even larger datasets imagenet improve result 9 ever many application not possible create labeled datasets million image common strategy dealing problem transfer learning strategy improves result even small specialized datasets like medical imaging 10 might practical workaround some application fundamental issue associate editor coordinating review manuscript approving publication wa varuna de silva remains unlike human supervised learning need mous amount labeled data given problem often access large dataset unlabeled data unsupervised data could used neural network ha research interest many year 11 xie et al among ﬁrst 2016 gate unsupervised deep learning image clustering strategy leverage data 12 since usage beled data ha researched numerous way ha created research ﬁelds like unsupervised metric learning 13 generally speaking unsupervised learning us no labeled data learning us unlabeled labeled learning generates labeled data research direction even different learning us only partial mation label metric learning aim learning good distance metric idea uniﬁes approach using unlabeled data beneﬁcial training process see figure 1 illustration either make 82146 work licensed creative common attribution license information see volume 9 2021 schmarje et al survey unsupervised learning image classification figure image illustrates simplifies benefit using unlabeled data deep learning training red dark blue circle represent labeled data point different class light grey circle represent unlabeled data point only small number labeled sample available only make assumption dotted line underlying true distribution solid line true distribution only determined also consider unlabeled data point clarify decision boundary training fewer label robust some rare case even surpasses supervised case 14 due beneﬁt many researcher company work ﬁeld unsupervised learning main goal close gap supervised learning even surpass result considering presented method like 15 16 believe research breaking point achieving goal hence lot research ongoing ﬁeld survey provides overview keep track major recent development unsupervised learning investigated research topic share variety mon idea differing goal application context implementation detail survey give overview wide range research topic focus survey describing similarity difference method whereas look broad range learning strategy compare method only based image cation task addressed audience survey consists deep learning researcher interested people ble preliminary knowledge want keep track recent development ﬁeld unsupervised learning related work subsection give quick overview previous work reference topic not address maintain focus survey research unsupervised technique puter vision ha long history variety research veys book ha published topic 17 21 unsupervised cluster algorithm researched breakthrough deep learning still widely used 22 already extensive survey describe vised strategy without deep ing 18 23 focus only technique including deep neural network many newer survey focus only vised learning 19 20 24 min et al wrote overview unsupervised deep learning strategy 24 sented beginning ﬁeld research work architecture perspective author looked broad range architecture focus only one architecture min et al refer clustering deep neural network cdnn deep clustering 24 even though work wa published 2018 already miss recent major development deep learning last year look recent development show connection research ﬁelds min et al not include van engelen hoos give broad overview general recent method 20 cover some recent development deep learning strategy 14 25 28 not covered furthermore author not explicitly compare presented method based structure performance jing tian concentrated survey recent ments learning 19 like u author provide performance comparison taxonomy taxonomy distinguishes different kind pretext task look pretext task one common idea compare method based underlying idea jing tian look different task apart classiﬁcation not include unsupervised method without pretext task qi luo one look unsupervised learning one survey 29 however look different learning strategy separately give comparison only inside respective learning strategy show bridging gap lead new insight improved performance future research approach some survey focus not general overview unsupervised learning special detail survey cheplygina et al present variety method context medical image analysis 30 include deep learning older machine learning approach look different strategy medical perspective mey loog focused underlying theoretical assumption learning 31 keep survey limited general image classiﬁcation task focus practical application survey focus deep learning approach image classiﬁcation investigate different learning strategy spotlight loss function concentrate recent method older one already adequately addressed previous literature 17 21 ing limitation mind topic unsupervised learning still includes broad volume 9 2021 82147 schmarje et al survey unsupervised learning image classification figure overview structure survey learning strategy unsupervised supervised commonly used literature learning incorporating many method defined training strategy subdivides learning detail training learning strategy including learning see subsection method belongs one training strategy us several common idea common idea concept pretext task loss definition method common idea given section ii detail common idea defined subsection method survey shortly described categorized section iii method compared based information concerning used common idea performance subsection result comparison three resulting trend discussed subsection range research ﬁelds exclude some related topic survey keep focus work example research different aim evaluated different datasets therefore topic like ric learning 13 meta learning 32 excluded speciﬁc network like general adversarial network 33 graph network 34 excluded also application like pose estimation 35 segmentation 36 image source like video sketch 37 excluded topic like learning method 38 excluded survey however see subsection topic like learning learn future like 39 outline rest paper structured following way deﬁne explain term used survey method training strategy common idea section ii visual representation term dependency seen analysis part figure method presented short description training strategy common idea section iii section iv compare method based used idea performance across four common image tion datasets section also includes description datasets evaluation metric finally discus result comparison subsection identify three trend research opportunity figure 2 complete overview structure survey seen ii underlying concept throughout survey use term training strategy common idea method speciﬁc meaning training strategy general using vised data training training strategy similar term vised learning provide deﬁnition corner case term not explain difference similarity detail subsection paper discus detail survey propose different element like algorithm general idea extension previous work consistent survey call main algorithm idea extension paper method method brieﬂy described section iii method follows ing strategy based several common idea use term common idea short idea concept approach shared different method roughly sort method based training strategy compare detail based used common idea see subsection information common idea rest chapter use shared deﬁnition following variable arbitrary set image x deﬁne xl xu x xl labeled unlabeled image respectively image x corresponding label deﬁned zx image 82148 volume 9 2021 schmarje et al survey unsupervised learning image classification figure illustration supervised learning three presented reduced training strategy red dark blue circle represent labeled data point different class light grey circle represent unlabeled data point black line define underlying decision boundary class striped circle represent data point not use label information first stage access information second stage detail stage different learning strategy see subsection x ha no label otherwise would belong xl distinction xu xl only usage label information training important example image x might label used evaluation long label not used training deﬁne x learning strategy lsx dataset x either unsupervised x xu supervised x xl xu different phase training different image datasets xn n could used two consecutive datasets xi different long different image xi different label xli used learning strategy lsi dataset xi training calculated based xu xl consecutive phase training grouped stage stage change consecutive datasets xi iff learning strategy different lsxi overall learning strategy change lsi due deﬁnition only two stage occur training seven possible combination visualized figure detail see subsection let c number class label given neural network f input x output neural network f x formulation f arbitrary network arbitrary weight parameter training strategy term like vised learning often used literature lapping deﬁnitions certain method summarize general understanding deﬁnition term highlight borderline case difﬁcult classify due borderline case deﬁne new taxonomy based stage training precise distinction method subsection see onomy lead clear clustering method regarding common idea justiﬁes taxonomy visual comparison unsupervised learning training strategy found figure unsupervised learning describes training without any label however goal clustering 14 27 good representation 25 40 data some method combine several unsupervised step achieve ﬁrstly good representation clustering 41 case unsupervised training achieved generating label therefore method called counterexample unsupervised method without would 22 often achieved pretext task different dataset pretrained network downstream task 19 many method follow paradigm say method form tation learning 25 40 42 44 survey focus image classiﬁcation therefore representation learning method need labeled data combination pretraining neither called unsupervised nal labeled information used learning describes method use labeled unlabeled data ever method like 16 26 45 49 use labeled unlabeled data beginning parison representation learning method like 25 40 42 44 use different stage training some method combine idea learning learning unsupervised learning 15 27 even difﬁcult classify explanation see method either unsupervised context image classiﬁcation usage labeled unlabeled data method varies clear tion common taxonomy not obvious nevertheless need structure method some way keep overview allow comparison acknowledge difference research focus decided providing taxonomy previous literature 29 believe future research come new combination not thought separate method only based rough distinction labeled unlabeled data used training detailed comparison distinct volume 9 2021 82149 schmarje et al survey unsupervised learning image classification method based common idea deﬁned described detail subsection call unsupervised learning strategy together reduced supervised learning strategy deﬁned stage see section ii ent interval training ent learning strategy supervised x xl unsupervised x xu xu used example method us pretraining xu image label ha two stage method us different algorithm loss datasets training only us unsupervised data xu ha one stage 41 method us xu xl complete training ha one stage 26 based deﬁnition stage training sify reduced supervised method training gy overview stage combination corresponding training strategy given figure concentrate reduced supervised ing survey not discus any method completely supervised due deﬁnition stage ﬁfth combination data usage stage exists combination would use only labeled data ﬁrst stage unlabeled data second stage rest survey exclude training strategy following reason case stage complete supervision followed stage partial no supervision unusual training strategy due unusual usage only know weight initialization followed reduced supervised training step combination could occur see initialization network pretrained weight supervised training different dataset imagenet 1 architectural decision not part reduced supervised training process used mainly sophisticated weight initialization exclude weight initialization reason know no method belongs stage following paragraph describe training strategy detail illustrated figure 3 1 supervised learning supervised learning common strategy image classiﬁcation deep neural network method only use labeled data xl corresponding label goal minimize loss function output network f x expected label zx x 2 training method follow training strategy trained one stage usage xl xu main difference supervised learning strategy usage additional unlabeled data xu common way integrate unlabeled data add one unsupervised loss supervised loss figure illustration different training strategy row stand different combination data usage first second stage defined section ii first column state common learning strategy name literature usage whereas last column state training strategy name used survey second column represents used data overall third fourth column represent used data stage one two blue grey circle represent usage labeled data xl unlabeled data xu respectively stage overall minus mean no stage used dashed half circle last row represents dashed part data used 3 training method follow ing strategy trained one stage usage only unlabeled sample xu therefore many author training strategy call method unsupervised variety loss function exist unsupervised learning 12 14 50 case problem rephrased way input loss generated reconstruction loss autoencoders 12 due some call also method want point one major difference many method ing training strategy method give image classiﬁcations without any usage labeled data 4 training method follow training strategy trained two stage usage xu ﬁrst stage xl maybe xu second stage many method called author fall strategy commonly pretext task used learn representation unlabeled data xu second stage representation image classiﬁcation xl important difference method method return useable classiﬁcations only additional training stage common idea different common idea used train model unsupervised learning section present selection idea used across multiple method literature important notice usage common idea fuzzy incomplete deﬁnition common idea not identical implementation approximation underlying motivation fuzziness needed two 82150 volume 9 2021 schmarje et al survey unsupervised learning image classification reason firstly comparison would not possible due many small difference exact implementation secondly allow u abstract some core element method therefore similarity detected also not detail concept motivation captured common idea limit common idea described since believe enough acterize recent method time know idea need extended future new common idea arise old one disappear focus shift idea contrast detailed taxonomy new idea easily integrated new tag sorted idea alphabetical order distinguish loss function general concept since idea might ence may jump corresponding entry would like know loss function ce common loss function image classiﬁcation 51 commonly used measure ence f x corresponding label zx given x loss deﬁned equation 1 goal minimize difference ce zx f x c x p log p x c x p log p c x p log p x p h p kl p p x 1 p probability distribution class imated output neural network f x given label zx h entropy probability tribution kl divergence important note sum entropy zx divergence f x zx general entropy h p zero due encoded label zx loss function ce could also used different probability distribution p based label distribution could example based target pretext task abbreviate used common idea ce not label used highlight specialty contrastive loss cl contrastive loss try distinguish positive negative pair positive pair could different view image negative pair could pairwise combination batch 25 hadsell et al proposed learn representation based contrasting 53 recent year idea ha extended visual representation learning method 25 54 57 example contrastive loss function 25 infonce 55 based loss computed across positive pair xi xj ﬁxed subset x n element batch training inition loss positive pair given equation similarity sim output measured normalized dot product τ temperature parameter batch consists n image pair lxi xj exp sim f xi f xj sim f xi f xk 2 chen li generalize loss broader family loss function alignment distribution part 58 alignment part encourages representation positive pair similar whereas distribution part encourages representation match prior bution 58 loss infonce motivated like contrastive loss maximizing agreement mutual information different view van der oord et al showed infonce lower bound mutual mation view 55 detail different bound loss found 59 however tschannen et al show evidence lower bound might not main reason success method 60 due fact count loss like infonce mixture common idea contrastive loss mutual information entropy minimization em grandvalet bengio noticed distribution diction learning tend distributed many class instead sharp one class 61 proposed sharpen output prediction word force network make conﬁdent prediction minimizing entropy 61 minimized entropy h p x probability bution p x based certain neural output f x image x minimization lead sharper conﬁdent prediction loss used only loss would degenerate trivial minimization divergence kl divergence also commonly used image classiﬁcation since interpreted part general kl measure difference two given distribution 62 therefore often used deﬁne auxiliary loss output f x image x given secondary discrete ability distribution q class deﬁnition given equation second distribution could another network output distribution prior known tion distribution depending goal volume 9 2021 82151 schmarje et al survey unsupervised learning image classification figure illustration four selected common idea blue red circle represent two different class line decision boundary class ϵ sphere around circle define area possible transformation arrow represent adversarial change vector r push decision boundary away any data point b image cat dog combined parametrized blending label also combined parameterization shown image taken dataset 52 c circle represents data point coloring circle label example image middle fuzzy label classification only draw one arbitrary decision boundary dashed line datapoints whereas overclustering create multiple subregions method could also applied outlier rather fuzzy label loop represents one version neural network predicts output distribution distribution cast hard used training neural network minimization kl q p x c x q c log p x q c 3 mean squared error mse mse measure euclidean distance two vector two neural network output f x f image x contrast loss ce kl mse not probability measure therefore vector arbitrary euclidean feature space see equation 4 minimization mse pull two vector example network output together similar minimization entropy would lead degeneration network loss used only loss network output mse f x f x 2 4 mutual information mi mi deﬁned two probability distribution p q kullback leiber kl divergence joint bution marginal distribution 63 many reduced supervised method goal maximize mutual mation distribution distribution could based input output intermediate step neural network case conditional distribution p q therefore joint distribution not known example could use output neural network f x f two augmented view x image distribution p general distribution could dependent x could identical similar distribution could independent x crop distinct class background sky ground object therefore mutual information need approximated used approximation varies depending method deﬁnition distribution p theoretical insight several approximation see 59 64 show deﬁnition mutual information two network output f x f image x x example equation equation also show alternative representation mutual information aration entropy h p x conditional entropy h p x p ji et al argue tion illustrates beneﬁts using mi ce pervised case 14 degeneration avoided mi balance effect maximizing entropy form distribution p x minimizing tional entropy equalizing p x p case lead degeneration neural network p x p kl p x f p x c x p c x f log p c x f p x h p x h p x p 5 virtual adversarial training vat vat 65 try make prediction invariant small mations minimizing distance image transformed version image miyato et al showed transformation chosen approximated adversarial way adversarial transformation maximizes distance image transformed version possible transformation loss deﬁned equation 6 image x output given 82152 volume 9 2021 schmarje et al survey unsupervised learning image classification figure illustration 8 selected pretext task example image pretext task b example image dataset batch c jigsaw pretext task consists solving simple jigsaw puzzle generated main image augments jigsaw puzzle adding part different image e exemplar pretext task distribution weakly augmented image upper right corner several strongly augmented image aligned f image rotated around fixed set rotation 0 90 180 270 degree network predict rotation ha applied g central patch adjacent patch image given task predict one 8 possible relative position second patch first one example correct answer upper center h network receives list pair predict positive pair example positive pair consists augmented view image some illustration inspired 40 42 44 neural network f x vat f x p x p x radv radv argmax r p x p x r 6 p probability distribution output neural network function measure distance illustrated figure r vector ϵ maximum length vector two example used tance measure 65 divergence 15 concept mixup mu mixup creates convex combination image blending illustration concept given figure prediction convex combination corresponding label turned beneﬁcial network need create consistent prediction ate interpolation image approach ha ﬁcial supervised learning general 66 therefore also used several learning algorithm 26 45 46 overclustering oc normally k class supervised case also use k cluster unsupervised case research showed beneﬁcial use cluster actual class k exist 14 27 67 call idea tering overclustering beneﬁcial unsupervised case due effect neural work decide split data separation helpful data intermediate class sorted adjacent class randomly 27 illustration idea presented figure pretext task pt pretext task description training neural network different task target task task example predicting rotation image 40 solving jigsaw puzzle 43 using contrastive loss 25 55 maximizing mutual tion 14 27 overview pretext task survey given figure 6 complete overview given table case text task used learn representation image classiﬁcation 25 40 42 44 volume 9 2021 82153 schmarje et al survey unsupervised learning image classification figure illustration four selected method used method given image input including label information given blue box left side right side illustration method provided general process organized top bottom first input image preprocessed none two different random transformation special augmentation technique like autoaugment 69 represented red box following neural network us preprocessed image x input calculation loss dotted line different method share common part method use ce label predicted distribution p x labeled example detail method found corresponding entry section iii whereas abbreviation common method defined subsection ema stand exponential moving average 55 68 context some method use pretext task deﬁne additional loss training 45 pl simple approach estimating label unknown data using 47 lee proposed classify unseen data neural network use prediction label process illustrated figure sound ﬁrst like assumption work reasonably well image classiﬁcation task important notice network need additional information prevent total random prediction additional tion could some known label weight tion supervised data unsupervised text task several modern method based core idea creating label predicting 46 48 iii method section shorty summarizes method survey roughly chronological order separated ing strategy summary state used common idea explains usage highlight special case viations common idea deﬁned subsection include large number recent method not claim list complete 47 describes common idea deep learning learning method description common idea see subsection contrast many method doe not use combination unsupervised supervised loss approach us prediction neural network label unknown data described common idea therefore labeled unlabeled data used parallel minimize ce loss common idea ce ce pl temporal ensembling laine aila present two similar learning method name temporal ensembling 49 method use combination supervised ce loss unsupervised consistency loss mse ﬁrst input consistency loss case output network randomly augmented input image ond input different method augmentation image used ral ensembling exponential moving average previous prediction evaluated laine aila show ral ensembling two time faster ble comparison 49 illustration method given figure common idea ce mse mean teacher mean teacher tarvainen valpola present learning 48 develop approach based ral ensembling 49 therefore also use mse consistency loss two prediction create prediction differently argue temporal bling incorporates new information slowly tions reason exponential moving average ema only updated per epoch fore propose use teacher based average weight student update step tarvainen valpola show model inferior consistency loss mse illustration method given figure common idea ce mse 82154 volume 9 2021 schmarje et al survey unsupervised learning image classification virtual adversarial training vat vat 65 not name common idea also method miyato et al used combination vat unlabeled data ce labeled data 65 showed adversarial transformation lead lower error image classiﬁcation random transformation furthermore showed adding min 61 loss increased accuracy even common idea ce em vat interpolation consistency training ict ict 70 us linear interpolation unlabeled data point regularize consistency image verma et al use combination supervised loss ce unsupervised loss mse unsupervised loss measured prediction interpolation two image polation interpolation ated mixup 66 algorithm two unlabeled data point unlabeled data point predicted mean teacher 48 network common idea ce mse mu pl weight averaging contrast method waratkun et al not change loss optimization algorithm 71 analyzed learning process based idea concept swa 72 49 mean teacher 48 athiwaratkun et al show averaging cycling learning rate beneﬁcial learning stabilizing training call improved version swa due faster convergence lower performance variance 71 architecture loss either copied 49 mean teacher 48 common idea ce mse mixmatch mixmatch 46 us combination supervised unsupervised loss berthelot et al use ce supervised loss mse prediction erated unsupervised loss created previous prediction mented image propose novel sharping method multiple prediction improve quality label sharpening also enforces implicitly tion entropy unlabeled data furthermore extend algorithm mixup 66 learning incorporating generated label common idea ce em mse mu pl ensemple autoendocing transformation enaet enaet 73 combine pretext task autoencoding transformation 74 mixmatch 46 wang et al apply spatial transformation tions rotation transformation color distortion input image pretext task transformation estimated original mented image given difference pretext task estimation often based augmented image only 40 loss used together loss mixmatch extended kullback leiber divergence prediction original augmented image common idea ce em kl mse mu pl pt unsupervised data augmentation uda xie et al present uda learning algorithm concentrate usage augmentation 16 use supervised vised loss supervised loss ce whereas pervised loss kullback leiber divergence output prediction output prediction based image augmented version image image classiﬁcation propose use augmentation scheme generated autoaugment 69 combination cutout 75 autoaugment us reinforcement learning create useful augmentation automatically cutout augmentation scheme randomly selected region image masked xie et al show bined augmentation method achieves higher performance comparison previous method like cutout cropping flipping addition different tation propose use variety regularization method proposed training signal annealing restricts inﬂuence labeled example training process prevent overﬁtting use entmin 61 kind 47 use term kind not use prediction label use ﬁlter unsupervised data outlier illustration method given figure common idea ce em kl pl spamco et al propose general framework across multiple view 76 context image classiﬁcation different neural network used different view main idea different view similar using main difference spamco not used sample inﬂuence across view unlabeled image ha weight value view based age parameter unlabeled image considered iteration ﬁrst only conﬁdent used time also le conﬁdent one allowed proposed hard soft also inﬂuence weighting beled image regularizers encourage select unlabeled image training across view without regularization training would degenerate independent training different ce used loss label additional regularization et al show application including text tion object detection common idea ce ce mse pl volume 9 2021 82155 schmarje et al survey unsupervised learning image classification figure illustration four selected method used method given image input including label information given blue box left side right side illustration method provided foc second stage represented general process organized top bottom first input image preprocessed none two different random transformation special augmentation technique like ctaugment 45 represented red box following neural network us preprocessed image x input calculation loss dotted line different method share common part method use ce label predicted distribution p x labeled example detail method found corresponding entry section iii whereas abbreviation common method defined subsection remixmatch remixmatch 45 extension mixmatch tion alignment augmentation anchoring berthelot et al motivate distribution alignment analysis mutual information use entropy minimization via ing not use any prediction equalization like mutual information argue equal distribution also not desirable since distribution unlabeled data could skewed therefore align prediction unlabeled data marginal class distribution seen example berthelot et al exchange augmentation scheme mixmatch augmentation anchoring instead averaging prediction different slight tions image only use stronger augmentation regularization augmented prediction image encouraged result distribution ce instead mse furthermore loss based rotation pretext task 40 wa added common idea ce ce em mi mu pl pt fixmatch fixmatch 26 building idea remixmatch dropping several idea make framework simple achieving better performance fixmatch using loss supervised unsupervised data image unlabeled data one one version ated version used conﬁdence threshold surpassed network calculated network output version compared hard label via implicitly encourages prediction unlabeled data 26 sohn et al not use idea like mixup vat distribution alignment state used provide ablation some extension common idea ce ce em pl exemplar dosovitskiy et al proposed pretext task additional 68 randomly sample patch different image augment patch heavily augmentation example rotation lations color change contrast adjustment cation task map augmented version patch correct original patch using loss common idea ce ce pt context doersch et al propose use context prediction pretext task visual representation learning 42 central patch adjacent patch image used input task predict one 8 possible relative position second patch ﬁrst one using loss illustration pretext task given figure doersch et al argue task becomes easier recognize content patch author representation task show superiority comparison random initialization aside tuning doersch et al show method could used visual data mining common idea ce ce pt jigsaw noroozi favaro propose solve jigsaw puzzle pretext task 43 idea network ha understand concept presented object solve puzzle using classiﬁcation loss prevent simple solution only look edge corner including small random margin puzzle patch supervised data image classiﬁcation task noroozi et al extended jigsaw task adding image part different image 44 call extension 82156 volume 9 2021 schmarje et al survey unsupervised learning image classification figure illustration four selected method used method given image input given red box left side right side illustration method provided part excluded only first task represented general process organized top bottom first input image either preprocessed one two random transformation split following neural network us preprocessed image x input calculation loss dotted line different method amdim cpc use internal element network calculate loss deepcluster iic use predicted output distribution p x p calculate loss detail method found corresponding entry section iii whereas abbreviation common method defined subsection example jigsaw puzzle given figure common idea ce ce pt deepcluster deepcluster 67 method generates label clustering caron et al iterate clustering predicted label generate training label show beneﬁcial use overclustering pretext task pretext task network label illustration method given figure common idea ce oc pl pt rotation gidaris et al use pretext task based image rotation prediction 40 propose randomly rotate input image 0 90 180 270 degree let network predict chosen rotation degree train network classiﬁcation task work also evaluate different number rotation four rotation score best result image classiﬁcation labeled data common idea ce ce pt contrastive predictive coding cpc cpc 55 56 method predicts representation local image region based previous image region author determine quality prediction contrastive loss identiﬁes correct prediction randomly sampled negative one call loss infonce prediction positive example 55 van den oord et al showed minimizing infonce maximizes lower bound mi previous image region predicted image region 55 illustration method given figure representation pretext task common idea ce ce cl mi pt constrastive multiview coding cmc cmc 54 generalizes cpc 55 arbitrary collection view tian et al try learn embedding different contrastive sample equal similar image like oord et al train network identifying rect prediction multiple negative one 55 however tian et al take different view image color channel depth segmentation similar image common image classiﬁcation datasets like use similarity pretext task resentations desired dataset common idea ce ce cl mi pt deep infomax dim dim 77 maximizes mi local input region output representation hjelm et al show maximizing local input region rather complete image beneﬁcial image classiﬁcation also use inator match output representation given prior distribution end network additional small neural network common idea ce mi pt augmented multiscale deep infomax amdim amdim 78 maximizes mi input output network extension method dim 77 dim usually maximizes mi local region image representation image amdim extends idea dim several way firstly author sample local region representation different augmentation source image secondly maximize mi multiple scale local region representation use powerful encoder deﬁne representation achieve higher accuracy bachman et al representation labeled data measure quality illustration method given figure common idea ce mi pt volume 9 2021 82157 schmarje et al survey unsupervised learning image classification deep metric transfer dmt dmt 79 learns metric pretext task gate label onto unlabeled data metric liu et al use image colorization 80 unsupervised instance discrimination 81 calculate metric ond stage propagate label unlabeled data tral clustering network new additionally show approach complementary previous method use ﬁdent method mean teacher 48 vat 65 improve accuracy label 30 common idea ce ce pl pt invariant information clustering iic iic 14 maximizes mi augmented view image idea image belong class regardless augmentation tion ha transformation neural network invariant author not maximize directly output distribution class distribution approximated every batch ji et al use auxiliary overclustering different output head increase performance unsupervised case idea allows network learn subclass handle noisy data ji et al use sobel ﬁltered image input instead original rgb image additionally show extend iic image segmentation point method completely unsupervised comparable method model subset available label illustration method given figure ﬁrst unsupervised stage seen pretext task contrast pretext task task already predicts representation seen classiﬁcations common idea ce mi oc pt learning 15 name suggests combination method zhai et al split loss supervised unsupervised part supervised loss ce whereas unsupervised loss based technique using rotation exemplar prediction 40 68 author show method form better technique 40 47 61 65 68 mix model moam combine rotation prediction vat entropy minimization single model multiple training step since discus result moam identify method common idea ce ce em pl pt vat simple framework contrastive learning visual representation simclr simclr 25 maximizes agreement two ent augmentation image method similar cpc 55 iic 14 comparison cpc chen et al not use different inner representation contrary iic use normalized loss based cosine similarity prediction measure whether positive pair similar negative pair dissimilar augmented version image treated positive pair pair any image negative pair system trained large batch size 8192 instead memory bank create enough negative example common idea ce ce cl pt fuzzy overclustering foc fuzzy overclustering 27 extension iic 14 foc focus using overclustering subdivide fuzzy label datasets therefore uniﬁes used data loss proposed iic different stage extends new idea novel loss inverse loss inspired used overclustering result network no ground truth label known foc not achieving result common image classiﬁcation dataset however plankton dataset fuzzy label surpasses fixmatch show consistent prediction achieved like iic foc viewed method general foc trained one unsupervised one stage seen method like iic produce classiﬁcations already pervised stage therefore also seen method common idea ce ce mi oc pt momentum contrast moco et al propose use momentum encoder contrastive learning 82 method 25 55 57 negative example contrastive loss sampled positive pair large batch size needed ensure great variety negative example et al sample negative example queue encoded another network whose weight updated nential moving average main network solve pretext task proposed 81 negative example sample queue second stage labeled data chen et al provide ablation baseline moco framework using mlp head 83 common idea ce cl pt bootstrap latent byol grill et al use online target network posed pretext task online network predicts image representation target network image 28 difference prediction measured mse normally approach would lead degeneration network constant prediction image would also 82158 volume 9 2021 schmarje et al survey unsupervised learning image classification figure illustration four selected method used method given image input given red not using label blue using label box left side right side illustration method provided part excluded only first task represented second stage distillation step illustrated general process organized top bottom first input image either preprocessed one two random transformation split following neural network us preprocessed image x input detail method found corresponding entry section iii whereas abbreviation common method defined subsection ema stand exponential moving average achieve goal contrastive learning degeneration avoided selecting positive pair example multiple negative one 25 55 57 82 83 using average weight online target network grill et al show empirically degeneration constant prediction avoided approach ha positive effect byol performance depending le hyperparameters like augmentation batch size 28 work richemond et al show byol even work no batch normalization might introduced kind contrastive learning effect batch used 84 common idea mse pt simple framework contrastive learning visual representation chen et al extend framework simclr using larger deeper network incorporating memory anism moco 57 moreover propose use framework three step ﬁrst training contrastive learning pretext task deep neural network method second step large network small amount labeled data third step distillation large pretrained network used predict complete unlabeled data soft used train smaller neural network ce distillation step could also performed network pretext task chen et al show even lead performance improvement 57 common idea ce ce cl pl pt deep adaptive image clustering dac dac 50 reformulates unsupervised clustering wise classiﬁcation similar idea chang et al predict cluster use retrain network twist calculate cosine tance cluster prediction distance used determine whether input image similar similar given certainty network trained binary ce certain similar dissimilar input image one interpret similarity ities similarity classiﬁcation task training process lower needed certainty include image input chang et al use combination rgb extracted hog feature common idea pl information maximizing training imsat imsat 85 maximizes mi input output model consistency regularization hu et al use ce image prediction augmented image prediction show best augmentation diction calculated vat 65 maximization mi directly image input lead problem datasets like 86 52 color information dominant comparison actual content shape workaround hu et al use feature generated pretrained cnn imagenet 1 input common idea mi vat invariant information clustering iic iic 14 described method comparison presented method iic ate usable classiﬁcations without model labeled data reason pretext task structed way label prediction extracted directly model lead conclusion iic also interpreted unsupervised learning method common idea mi oc fuzzy overclustering foc foc 27 described method like iic foc also seen volume 9 2021 82159 schmarje et al survey unsupervised learning image classification figure example four random cat different datasets illustrate difference quality method ﬁrst stage yield cluster prediction common idea mi oc semantic clustering adopting nearest neighbor scan gansbeke et al calculate clustering assignment building pretext task mining nearest neighbor using propose use simclr 25 pretext task show pretext task 40 81 could also used step sample k nearest neighbor selected gained feature space novel semantic clustering loss encourages sample cluster gansbeke et al noticed wrong nearest neighbor lower conﬁdence propose create only conﬁdent example tuning also show overclustering fully used number cluster not known common idea oc pl pt iv analysis chapter analyze common idea shared differ method compare formance method common deep learning datasets datasets survey compare presented method ety datasets selected four datasets used multiple paper allow fair comparison overview example image given figure large datasets tiny color image size 32 32 86 datasets contain image belonging 10 100 class respectively 100 class 100 combined 20 superclass set provide training example validation example image label presented result only trained label label represent case method us label marked independently dataset designed unsupervised learning 52 dataset inspired 86 provides fewer label only consists training label validation label however unlabeled example image also provided unlabeled example belong training class some different class image 96 96 color image acquired combination label imagenet 1 subset imagenet 1 training set consists million image whereas validation test set include image image belong 1000 object category due large number category mon report accuracy accuracy classical accuracy one prediction compared one label accuracy check ground truth label set ﬁve prediction ther detail accuracy see subsection presented result only trained 10 label represent case method us label marked independently evaluation metric compare performance method based classiﬁcation score score deﬁned differently pervised setting follow standard protocol use classiﬁcation accuracy case pervised learning use cluster accuracy need handle missing label training need ﬁnd best permutation σ network 82160 volume 9 2021 schmarje et al survey unsupervised learning image classification cluster prediction class n image xn label zxi prediction f xi accuracy deﬁned equation 7 whereas cluster accuracy deﬁned equation acc xn pn f xi j n 7 acc xn max σ pn f xi j n 8 comparison method subsection compare method concerning used common idea performance rize presented result discus underlying trend next subsection comparison concerning used common idea table 1 present method used mon idea following deﬁnition common idea subsection evaluate only idea used frequently different paper special detail different optimizer used tion mi excluded please see section iii detail one might expect common idea used equally method training strategy rather see tendency common idea differ training gy step common idea based signiﬁcance differentiating training strategy major separation training strategy based ce pretext task method use loss training whereas only two use additional loss based pretext task method use pretext task use ce method use no ce often use pretext task due deﬁnition training strategy grouping expected however cluster common idea ble notice some common idea almost solely used one two strategy common idea em kl mse mu method cl mi oc method hypothesize shared different usage idea exists due ferent usage unlabeled data example method use unlabeled labeled data stage therefore might need regularize training mse compare training notice mi oc pt often used three not often used training stated hypothesize similarity arises method unsupervised stage followed supervised stage method iic author even proposed unsupervised method surpass purely supervised result ce pl vat used several different method due simple complementary idea used variety different method uda example us pl ﬁlter unlabeled data useful image ce seems often used method parenthesis table 1 indicate often also motivate another idea like 27 cl loss 25 55 see deﬁned training strategy share common idea inside strategy differ usage idea conclude deﬁnition training strategy not only logical also supported usage common idea comparison concerning performance compare performance different method based respective reported result paper better comparability would liked recreate every method uniﬁed setup wa not feasible whereas using reported value might only possible approach lead drawback analysis kolesnikov et al showed change tecture lead signiﬁcant performance boost drop 89 state neither ranking architecture consistent across different method ranking method consistent across tures 89 method try achieve comparability previous one similar setup time small difference still aggregate lead variety used architecture some method use only early convolutional network alexnet 1 others use modern architecture like wide 90 91 oliver et al proposed guideline ensure parable evaluation learning 92 showed not following guideline may lead change performance 92 whereas some method try follow guideline not guarantee od impact comparability considering limitation not focus small difference look general trend specialty instead table 2 show collected result presented od also provide result respective supervised baseline reported author keep fair comparability not add baseline complex architecture table 3 show result even fewer label normally deﬁned subsection general used architecture become complex accuracy rise time behavior expected new result often improvement earlier work change architecture may led improvement however many paper include ablation study parisons only supervised method show impact method believe combination volume 9 2021 82161 schmarje et al survey unsupervised learning image classification table overview method used common idea side reviewed method section iii sorted training strategy top row list common idea detail idea abbreviation given subsection last column some row sum usage idea per method training strategy legend x idea only used indirectly individual explanation given section iii modern architecture advanced method lead improvement dataset almost method reach 90 accuracy best method mixmatch fixmatch reach accuracy 95 roughly three percent worse fully supervised baseline dataset fewer result reported fixmatch 77 dataset best method comparison fully supervised baseline 80 newer method also provide result 1000 even 250 label instead 4000 label especially enaet remixmatch fixmatch stick since achieve only worse result 250 label instead 4000 label dataset method report better result supervised baseline result ble due unlabeled part dataset unlabeled data only utilized unsupervised method enaet achieves best result 95 fixmatch report accuracy nearly 95 only 1000 label method achieve 5000 label 82162 volume 9 2021 schmarje et al survey unsupervised learning image classification table overview reported accuracy first column state used method supervised baseline used result considered baseline referenced paper original paper given bracket score architecture given second column last four column report accuracy score respective dataset see subsection detail result not reported original paper reference given result blank entry represents fact no result wa reported aware different architecture framework used might impact result please see subsection detailed explanation legend 100 label used instead default value defined subsection multilayer perceptron used instead one fully connected layer remark special architecture evaluation 1 architecture includes regularization 2 network us wider hidden layer 3 method us ten random class default 1000 class 4 network only predicts 20 superclass instead default 100 class 5 input pretrained imagenet feature 6 method us different copy network input 7 network us selective kernel 87 dataset difﬁcult dataset based reported accuracy method only achieve accuracy roughly 20 worse reported supervised baseline around 86 only method simclr byol achieve accuracy le 10 worse baseline achieves best accuracy accuracy accuracy around 96 fewer label also simclr byol achieve best result unsupervised method separated vised baseline clear margin 10 scan achieves best result comparison method build strong pretext task simclr also illustrates reason including unsupervised method ison method unsupervised method not use labeled example therefore expected worse however data show gap 10 not large unsupervised method beneﬁt idea learning some paper report result volume 9 2021 82163 schmarje et al survey unsupervised learning image classification table overview reported accuracy fewer label first column state used method last seven column report accuracy score respective dataset amount label number either given absolute number percent blank entry represents fact no result wa reported even fewer label shown table 3 close gap unsupervised learning imsat report racy 94 since imsat us pretrained imagenet feature superset result not directly comparable discussion subsection discus presented result vious subsection divide discussion three major trend identiﬁed trend lead possible future research opportunity 1 trend real world application previous method not scalable image application used workarounds extracted tures 85 process image many method report result 90 simple dataset only ﬁve method achieve 5 accuracy 90 dataset conclude method not scalable complex image classiﬁcation lem however method like fixmatch seem surpassed point only tiﬁc usage could applied classiﬁcation task conclusion applies image classiﬁcation task balanced clearly separated class clusion also implicates issue need solved future research class imbalance 93 94 noisy label 27 95 not treated presented method datasets also unlabeled data point not sidered see good performance datasets doe not always transfer completely datasets 27 assume issue arise due assumption not hold datasets like clear distinction datapoints 27 hyperparameters like augmentation batch size 28 future research ha address issue reduced supervised learning method applied any datasets 2 trend much supervision needed see gap reduced supervised vised method shrinking gap le 5 left total supervised reduced supervised learning reduced supervised method even surpass total supervised case 20 due additional set unlabeled data conclude reduced supervised learning reach comparable result using only roughly 10 label general considered reduction 100 10 label however see method like fixmatch achieve comparable result even fewer label usage 1 label 2012 equivalent 13 image per class fixmatch even achieves median accuracy around 65 one label per class dataset 26 trend result improve overtime expected result indicate near point learning need almost no label per class 10 label practice ing cost unsupervised almost common classiﬁcation datasets unsupervised method would need bridge performance gap classiﬁcation datasets useful anymore questionable unsupervised method achieve would need guess human want classiﬁed even competing feature available already see datasets like imagenet additional data used improve vised training 96 98 large amount data only collected without any weak label lection process ha automated interesting investigate discussed method survey also scale datasets using only label per class 82164 volume 9 2021 schmarje et al survey unsupervised learning image classification conclude datasets ﬁxed number class method important unsupervised method however lot class new class detected like learning 38 94 99 100 unsupervised od still lower labeling cost high importance mean future research ha investigate idea transferred pervised method 14 41 setting many unknown rising amount class like 39 96 3 trend combination common idea comparison identiﬁed common idea shared method believe only little overlap method due different aim respective author many paper focus creating good representation result only comparable paper aim best accuracy score label possible look method like enaet match see beneﬁcial combine ferent idea mindset method used broad range idea also idea uncommon respective training strategy call combined approach even mix model 15 state method strong learner 57 assume combination one reason superior performance assumption supported included comparison original paper example showed impact method separately well combination 15 method like fixmatch illustrate doe not need lot common idea achieve performance rather selection correct idea combining meaningful important identiﬁed some common idea not often combined tion broad range unusual idea beneﬁcial believe combination different common idea promising future research ﬁeld many reasonable combination yet not explored conclusion paper provided overview unsupervised method analyzed difference ities combination based 34 different method analysis led identiﬁcation several trend possible research ﬁelds based analysis deﬁnition different training strategy common idea strategy showed method work general idea use provide simple classiﬁcation despite difﬁcult comparison method performance due different architecture implementation identiﬁed three major trend result 90 accuracy 2012 only 10 label indicate method could applied problem however issue like class imbalance noisy fuzzy label not considered robust method need researched learning applied issue performance gap supervised method closing number label get comparable result fully supervised learning ing future unsupervised method almost no labeling cost beneﬁt comparison method due development conclude combination fact method beneﬁt using label guidance unsupervised method lose importance however large number class increasing number class idea unsupervised still high importance idea learning need transferred setting concluded training mainly use different set common idea strategy use combination different idea overlap technique identiﬁed trend combination different technique beneﬁcial overall performance combination small overlap idea identiﬁed possible future research opportunity acknowledgment work wa supported land open access publikationsfonds funding program reference 1 krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural network proc adv neural inf process vol 60 no new york ny usa association computing machinery 2012 pp 2 zhang ren sun deep residual learning image recognition proc ieee conf comput vi pattern recognit cvpr jun 2016 pp 3 brünger dippel koch veit tailception using neural network assessing tail lesion picture pig carcass animal vol 13 no 5 pp 2019 4 redmon farhadi incremental ment 2018 online available 5 clausen zelenka schwede koch parcel ing detection large camera network pattern recognition brox bruhn fritz ed cham switzerland springer 2019 pp 6 long shelhamer darrell fully convolutional network semantic segmentation proc ieee conf comput vi pattern recognit cvpr jun 2015 pp 7 gkioxari dollár girshick mask proc ieee int conf comput 2017 pp 8 bishop pattern recognition machine learning new york ny usa 2006 9 mahajan girshick ramanathan paluri li bharambe van der maaten exploring limit weakly supervised pretraining proc eur conf comput vi eccv 2018 pp volume 9 2021 82165 schmarje et al survey unsupervised learning image classification 10 schmarje zelenka geisen glüer koch segmentation uncertain local collagen ﬁber orientation shg microscopy proc dagm german conf pattern lecture note computer science vol 11824 2019 pp 11 hinton sejnowski unsupervised learning foundation neural computation cambridge usa mit press 1999 12 xie girshick farhadi unsupervised deep embedding clustering analysis proc int conf mach vol 1 2016 pp 13 kaya bilge deep metric learning survey symmetry vol 11 no 9 1066 2019 14 ji vedaldi henriques invariant information clustering unsupervised image classiﬁcation segmentation proc int conf comput vi iccv 2019 pp 15 beyer zhai oliver kolesnikov learning proc int conf comput vi iccv 2019 pp 16 xie dai hovy luong le unsupervised data augmentation consistency training proc adv neural inf process syst nip 2020 pp 17 chapelle schölkopf zien learning ieee trans neural vol 20 no 3 542 mar 2009 18 xu wunsch ii survey clustering algorithm ieee trans neural vol 16 no 3 pp may 2005 19 jing tian visual feature learning deep neural network survey ieee trans pattern anal mach early access may 4 2020 doi 20 van engelen hoos survey ing mach vol 109 no 2 pp 2020 21 ciocca cusano santini schettini use vised feature unsupervised image categorization evaluation comput vi image vol 122 pp may 2014 22 macqueen some method classiﬁcation analysis variate observation proc berkeley symp math statist oakland ca usa vol 1 1967 pp 23 zhu learning literature survey dept put univ madison wi usa tech 2008 vol 2 online available 24 min guo liu zhang cui long survey tering deep learning perspective network architecture ieee access vol 6 pp 2018 25 chen kornblith norouzi hinton simple framework contrastive learning visual representation proc int conf mach 2020 pp 26 sohn berthelot li zhang carlini cubuk kurakin zhang raffel fixmatch simplifying supervised learning consistency conﬁdence proc adv neural inf process syst nip 2020 pp 27 schmarje brünger santarossa schröder kiko koch beyond cat dog classiﬁcation fuzzy label overclustering 2020 online able 28 grill strub altché tallec richemond buchatskaya doersch pires guo azar piot kavukcuoglu munos valko bootstrap latent new approach learning proc adv neural inf process syst nip 2020 pp 29 qi luo small data challenge big data era survey recent progress unsupervised method 2019 online available 30 cheplygina de bruijne pluim survey transfer learning medical image analysis med image vol 54 pp may 2019 31 mey loog improvability ing survey theoretical result 2019 pp online available 32 finn abbeel levine fast adaptation deep network proc int conf mach learn icml vol 3 mar 2017 pp 33 goodfellow mirza xu ozair courville bengio generative adversarial work proc int conf neural inf process jun 2014 pp 34 liu zhou long jiang yao zhang prototype propagation network ppn learning category graph proc int joint conf artif 2019 pp 35 ukita uematsu human pose estimation comput vi image vol 170 pp may 2018 36 mahapatra combining multiple expert annotation using supervised learning graph cut medical image segmentation comput vi image vol 151 pp 2016 37 xu song yin song wang deep representation learning sketch ieee trans circuit syst video vol 31 no 4 pp apr 2021 38 liu zhou long jiang dong zhang isometric propagation network generalized learning proc int conf learn 2021 pp 39 yu chen cheng luo transmatch scheme learning proc conf comput vi pattern recognit cvpr jun 2020 pp 40 gidaris singh komodakis unsupervised representation learning predicting image rotation proc int conf learn 2018 pp 41 van gansbeke vandenhende georgoulis proesmans vangool scan learning classify image without label proc eur conf comput 2020 pp 42 doersch gupta efros unsupervised visual tation learning context prediction proc ieee int conf comput vi iccv 2015 pp 43 noroozi favaro unsupervised learning visual tions solving jigsaw puzzle proc eur conf comput 2016 pp 44 noroozi vinjimoor favaro pirsiavash boosting supervised learning via knowledge transfer proc conf comput vi pattern jun 2018 pp 45 berthelot carlini cubuk kurakin sohn zhang raffel remixmatch learning tion alignment augmentation anchoring proc int conf learn 2020 pp 46 berthelot carlini goodfellow papernot oliver raffel mixmatch holistic approach ing proc adv neural inf process 2019 pp 47 lee simple efﬁcient learning method deep neural network proc workshop lenges represent learn icml vol 3 2013 2 48 tarvainen valpola mean teacher better role el consistency target improve deep learning result proc int conf learn 2017 pp 49 laine aila temporal ensembling ing proc int conf learn 2017 pp 50 chang wang meng xiang pan deep adaptive image clustering proc ieee int conf comput vi iccv 2017 pp 51 goodfellow bengio courville deep learning cambridge usa mit press 2016 52 coates ng lee analysis network unsupervised feature learning proc int conf artif intell 2011 pp 53 hadsell chopra lecun dimensionality reduction learning invariant mapping proc ieee comput soc conf put vi pattern recognit cvpr vol 2 jun 2006 pp 54 tian krishnan isola contrastive multiview coding proc eur conf comput 2019 pp 55 van den oord li vinyals representation learning contrastive predictive coding 2018 online able 56 hénaff srinivas de fauw razavi doersch eslami van den oord image recognition contrastive predictive coding proc int conf mach 2020 pp 57 chen kornblith swersky norouzi hinton big supervised model strong learner proc adv neural inf process syst nip 2020 pp 58 chen li intriguing property contrastive loss 2020 online available 82166 volume 9 2021 schmarje et al survey unsupervised learning image classification 59 poole ozair van den oord alemi tucker variational bound mutual information proc int conf mach 2019 pp 60 tschannen djolonga rubenstein gelly lucic mutual information maximization representation learning proc int conf learn 2020 pp 61 grandvalet bengio learning entropy imization proc adv neural inf process 2005 pp 62 kullback leibler information sufﬁciency ann math vol 22 no 1 pp 1951 63 cover thomas element information theory hoboken nj usa wiley 1991 64 belghazi baratin rajeswar ozair bengio courville hjelm mutual information neural estimation proc int conf mach 2018 pp 65 learning miyato maeda koyama ishii koyama virtual adversarial training regularization method supervised learning ieee trans pattern anal mach vol 41 no 8 pp 2019 66 zhang cisse dauphin mixup beyond empirical risk minimization proc int conf learn 2018 pp 67 caron bojanowski joulin douze deep clustering unsupervised learning visual feature proc eur conf comput vi eccv 2018 pp 68 dosovitskiy fischer springenberg riedmiller brox discriminative unsupervised feature learning exemplar tional neural network ieee trans pattern anal mach vol 38 no 9 pp 2016 69 cubuk zoph mane vasudevan le ment learning augmentation strategy data proc conf comput vi pattern recognit cvpr jun 2019 pp 70 verma lamb kannala bengio lation consistency training learning proc int joint conf artif 2019 pp 71 athiwaratkun finzi izmailov wilson podoprikhin garipov vetrov wilson many consistent explanation unlabeled data average proc int conf learn 2019 pp 72 izmailov podoprikhin garipov vetrov wilson averaging weight lead wider optimum better generalization proc conf uncertainty artif 2018 pp 73 wang kihara luo qi enaet framework supervised learning ble transformation 2019 online available 74 zhang qi wang luo aet aed unsupervised representation learning transformation rather data proc conf comput vi pattern recognit cvpr jun 2019 pp 75 devries taylor improved regularization convolutional neural network cutout 2017 online able 76 meng dong yang training mach learn vol 21 no 57 pp 2020 77 hjelm fedorov grewal bachman trischler bengio learning deep representation mutual information estimation maximization proc int conf learn 2019 pp 78 bachman hjelm buchwalter learning representation maximizing mutual information across view proc adv neural inf process 2019 pp 79 liu wu hu lin deep metric transfer label propagation limited annotated data proc int conf comput vi workshop iccvw 2019 pp 80 zhang isola efros colorful image colorization proc eur conf comput 2016 pp 81 wu xiong yu lin unsupervised feature learning via instance discrimination proc conf comput vi pattern jun 2018 pp 82 fan wu xie girshick momentum contrast unsupervised visual representation learning proc conf comput vi pattern recognit cvpr jun 2020 pp 83 chen fan girshick improved baseline momentum contrastive learning 2020 online available 84 richemond grill altché tallec strub brock smith de pascanu piot valko byol work even without batch statistic 2020 online available 85 hu miyato tokui matsumoto sugiyama ing discrete representation via information maximizing training proc int conf mach vol 70 2017 pp 86 krizhevsky hinton learning multiple layer feature tiny image thesis univ toronto toronto canada 2009 online available 87 li wang hu yang selective kernel network proc conf comput vi pattern recognit cvpr jun 2019 pp 88 touvron vedaldi douze jégou fixing test resolution discrepancy fixefﬁcientnet 2020 online available 89 kolesnikov zhai beyer revisiting visual representation learning proc conf comput vi pattern recognit cvpr jun 2019 pp 90 zagoruyko komodakis wide residual network proc brit mach vi 2016 pp 91 gastaldi regularization 2017 online available 92 oliver odena raffel cubuk goodfellow realistic evaluation deep learning algorithm proc adv neural inf process syst nip 2018 pp 93 lin goyal girshick dollár focal loss dense object detection proc ieee int conf comput vi iccv 2017 pp 94 schröder kiko koch morphocluster efﬁcient tation plankton image clustering sensor vol 20 no 11 3060 may 2020 95 li peng cao du xing qiao peng product image recognition guidance learning noisy supervision put vi image vol 196 jul 2020 art no 102963 96 pham dai xie luong le meta pseudo label 2020 online available 97 kolesnikov beyer zhai puigcerver yung gelly houlsby big transfer bit general visual representation learning proc eur conf comput lecture note computer science 2020 pp 98 xie luong hovy le noisy student improves imagenet classiﬁcation proc conf comput vi pattern recognit cvpr jun 2020 pp 99 wang yao kwok ni generalizing example survey learning acm comput vol 53 no 3 pp jul 2020 100 wang zheng yu miao survey learning setting method application acm trans intell syst vol 10 no 2 pp lars schmarje received degree computer science kiel sity 2016 2018 respectively currently pursuing degree computer science 2016 worked project planner direkt gruppe gmbh hamburg 2017 2018 worked vater solution gmbh kiel security engineer since 2019 ha research assistant multimedia information processing group kiel university also part project build autonomous racing car current research interest includes learning focus fuzzy label volume 9 2021 82167 schmarje et al survey unsupervised learning image classification monty santarossa received degree computer science kiel versity 2017 2019 respectively currently pursuing degree computer science 2019 connection master si spent six month daimler research development researching learned ment perception autonomous car rent research interest kiel university include diagnosis prognosis eye disease image understanding task including image registration siﬁcation segmentation santarossa werner petersen preis der technik 2019 category best master thesis schröder received degree computer science kiel university kiel germany 2013 2016 respectively currently pursuing degree computer science primary area expertise deep learning image recognition working recognition plankton image research est include supervised unsupervised deep learning application machine learning method natural science reinhard koch member ieee received diploma degree electrical ing university hannover germany 1985 1996 respectively postdoctoral research ku leuven belgium joined department computer science kiel university germany professor 1999 currently director department computer science vice dean faculty engineering author coauthor 250 publication research interest include computer vision object tracking analysis processing computer graphic augmented reality cation deep learning approach scene understanding cation received numerous award including olympus award pattern recognition 1997 david marr price iccv also serf president german association pattern recognition dagm german delegate governing board international association pattern recognition iapr 82168 volume 9 2021