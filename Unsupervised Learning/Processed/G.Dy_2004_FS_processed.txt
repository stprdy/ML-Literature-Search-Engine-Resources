journal machine learning research 5 2004 submitted published feature selection unsupervised learning jennifer dy jdy department electrical computer engineering northeastern university boston 02115 usa carla brodley brodley school electrical computer engineering purdue university west lafayette 47907 usa editor stefan wrobel abstract paper identify two issue involved developing automated feature subset tion algorithm unlabeled data need ﬁnding number cluster conjunction feature selection need normalizing bias feature selection criterion respect dimension explore feature selection problem issue fssem ture subset selection using em clustering two different performance criterion evaluating candidate feature subset scatter separability maximum likelihood present proof dimensionality bias feature criterion present normalization scheme applied any criterion ameliorate experiment show need feature selection need addressing two issue effectiveness proposed solution keywords clustering feature selection unsupervised learning introduction paper explore issue involved developing automated feature subset selection rithms unsupervised learning unsupervised learning mean unsupervised classiﬁcation clustering cluster analysis process ﬁnding natural grouping grouping similar based some similarity measure object together many learning domain human deﬁnes feature potentially useful however not feature may relevant case choosing subset original feature often lead better performance feature selection popular supervised learning naga 1990 almuallim dietterich 1991 cardie 1993 kohavi john 1997 supervised learning feature selection algorithm maximize some function predictive accuracy given class label natural want keep only feature related lead class unsupervised learning not given class label feature keep not use information problem not feature important some feature may redundant some may irrelevant some even misguide clustering result addition reducing number feature increase ity ameliorates problem some unsupervised learning algorithm break high dimensional data c jennifer dy carla brodley dy brodley x x x x xx xx x xx x x xx x x x x xx x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x xx x x x xx xxxxxxxxxxxxxx xxxxxx xx x x x xx x x xx x xx x xx x x x x x x x xx x x x xx x x xxx x x x x x x x x x x xx x xxx xxx xx x x xx x xx x x x x x x x x x x x figure 1 example feature x redundant feature x provides information feature regard discriminating two cluster x x xx xx x xx x x xx x x x x xx x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x xx x x xx xxx xx x x xx x x xx xxx x xx x x xx x xx x xxxxxxxxxxxxxx xxxxx x x xx x x x x x x x xx x x x xx x x xxx x x x x x x x x x figure 2 example consider feature irrelevant omit x only one cluster uninteresting figure 1 show example feature redundancy unsupervised learning note data grouped way using only either feature x feature therefore consider feature x redundant figure 2 show example irrelevant feature observe feature doe not contribute cluster discrimination used feature lead single cluster structure uninteresting note irrelevant feature misguide clustering result especially irrelevant feature relevant one addition situation unsupervised learning complex depict figure 1 example figure b show cluster obtained using feature subset b c respectively different feature subset lead varying cluster structure feature set pick unsupervised learning difﬁcult problem difﬁcult simultaneously ﬁnd relevant feature well key element solution any problem able precisely deﬁne problem paper deﬁne task 846 feature selection unsupervised learning x x b x x x x x xx x x x x x x x x x x x x x x x x x x xx x x xxx x x x x x x x x x x xx x xx x x x x x xxx x xx x x x x x x xx x xx xx x x x x x x xx xxx x xx x x xx x xx x x x x x x x c x x x x x x x xx x x x x x x x x x x xx x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x xx x x xx x x x x xx x x x x x x x xx x xx xx x x x xx x xx x xx xx x xx x x x x x x x xx x xx x x x x xx x x xx x xx x x x x x x x x x xx x xx x x x x xx xx x x x xx x x x x x x x x x xx x xx x x x x xx x x x xx xx x x x x x x x x x xx x xx x x x x xx x xx x xxx x xx xx x b figure 3 complex example figure scatterplot data feature figure b scatterplot data feature c goal feature selection unsupervised learning ﬁnd smallest feature subset best uncovers interesting natural grouping cluster data ing chosen criterion may exist multiple redundant feature subset solution satisﬁed ﬁnding any one solution unlike supervised learning ha class label guide feature search unsupervised learning need deﬁne interesting natural mean usually represented form criterion function present example different criterion section since research feature selection unsupervised learning relatively recent hope paper serve guide future researcher aim explore wrapper framework unsupervised learning identify issue involved developing feature selection algorithm unsupervised learning within framework suggest way tackle issue point lesson learned endeavor suggest avenue future research idea behind wrapper approach cluster data best candidate feature subspace according natural mean select interesting subspace minimum number feature framework inspired supervised wrapper proach kohavi john 1997 rather wrap search best feature subset around supervised induction algorithm wrap search around clustering algorithm 847 dy brodley search feature final subset feature criterion value cluster cluster algorithm clustering evaluation criterion feature feature subset figure 4 wrapper approach unsupervised learning particular paper investigates wrapper framework fssem feature subset lection using em clustering introduced dy brodley term em clustering refers em algorithm dempster et 1977 mclachlan ishnan 1997 moon 1996 wolfe 1970 wu 1983 applied estimating maximum likelihood parameter ﬁnite gaussian mixture although apply wrapper approach em clustering framework presented paper applied any clustering method fssem serf example present paper applying different clustering algorithm feature selection criterion would only require replacing corresponding clustering feature criterion section 2 describe fssem particular present search method clustering method two different criterion selected guide feature subset search scatter bility maximum likelihood exploring problem wrapper framework encounter tackle two issue different feature subset different number cluster feature selection criterion bias respect feature subset dimensionality section 3 discus complication ﬁnding number cluster brings neous feature problem present one solution section 4 present theoretical explanation feature selection criterion bias occur section 5 provides general normalization scheme ameliorate bias any feature criterion toward dimension section 6 present empirical result synthetic data set designed swer following question 1 feature selection unsupervised learning algorithm better clustering feature 2 using ﬁxed number cluster k better using variable k feature search 3 doe normalization scheme work 4 feature selection criterion better section 7 provides survey existing feature selection algorithm section 8 provides summary lesson learned endeavor finally section 9 suggest avenue future research feature subset selection em clustering fssem feature selection algorithm categorized either ﬁlter wrapper john et 1994 proaches ﬁlter approach basically feature applies selected feature subset clustering algorithm whereas wrapper approach incorporates clustering rithm feature search selection choose explore problem wrapper 848 feature selection unsupervised learning work interested understanding interaction clustering algorithm feature subset search figure 4 illustrates wrapper approach input set feature output selected feature cluster found feature subspace basic idea search feature subset space evaluating candidate subset ft ﬁrst clustering space ft using clustering algorithm evaluating resulting cluster feature subset using chosen feature selection criterion repeat process ﬁnd best feature subset corresponding cluster based feature evaluation criterion wrapper approach divide task three component 1 feature search 2 clustering algorithm 3 feature subset evaluation feature search exhaustive search possible feature subset number available feature subset maximizes selection criterion computationally intractable therefore greedy search sequential forward backward elimination fukunaga 1990 kohavi john 1997 typically used sequential search result worst case search experiment reported applied sequential forward search sequential forward search sfs start zero feature sequentially add one feature time feature added one provides largest criterion value used combination feature chosen search stop adding feature doe not improve chosen feature criterion sfs not best search method doe guarantee optimal solution however sfs popular simple fast provides reasonable solution purpose investigation paper sfs would sufﬁce one may wish explore search method wrapper approach example kim et al 2002 applied evolutionary method kittler 1978 russell norvig 1995 provide good overview different search strategy clustering algorithm choose em clustering clustering algorithm clustering method also used framework recall cluster data need make assumption deﬁne natural grouping mean apply standard assumption natural group gaussian assumption not limiting allow number cluster adjust data aside ﬁnding cluster also ﬁnd number gaussian cluster section 3 discus present solution ﬁnding number cluster conjunction feature selection provide brief description em clustering application em imate maximum likelihood estimate ﬁnite mixture multivariate gaussians appendix one obtain detailed description em clustering fraley raftery 2000 lan krishnan 1997 gaussian mixture assumption limit data continuous valued attribute however wrapper framework extended mixture probability butions mclachlan basford 1988 titterington et 1985 clustering method including graph theoretic approach duda et 2001 fukunaga 1990 jain dubes 1988 849 dy brodley feature subset selection criterion section investigate feature subset evaluation criterion deﬁne estingness mean two general view issue one criterion deﬁning interestingness feature subset selection criterion criterion used clustering two criterion need not using criterion clustering feature selection provides consistent theoretical optimization formulation using two different criterion hand present natural way combining two criterion check ances proof view better outside scope paper interesting topic future research paper look two feature selection criterion one similar clustering criterion different bias recall goal ﬁnd feature subset best discovers interesting grouping data select optimal feature subset need measure ass cluster quality choice performance criterion best made considering goal domain study performance criterion common conclusion different classiﬁcations clustering right different purpose not say any one classiﬁcation hartigan 1985 paper not attempt determine best criterion one refer milligan 1981 comparative study different clustering criterion investigate two measure scatter separability maximum likelihood section describe criterion ing assumption made scatter separability criterion property typically desired among grouping cluster ration investigate scatter matrix separability criterion used discriminant analysis fukunaga 1990 feature selection criterion choose explore scatter separability criterion used any clustering criterion used discriminant ysis assume feature interested feature group data cluster unimodal separable sw scatter matrix sb class scatter matrix deﬁned follows sw k πje x j x j j k πjς j 1 sb k πj µ j µ j 2 mo e x k πjµ j 3 π j probability instance belongs cluster ω j x random feature vector representing data k number cluster µ j sample mean vector cluster ω j mo total sample mean σ j sample covariance matrix cluster ω j e expected value operator one choose use version criterion measure fukunaga 1990 clustering algorithm 850 feature selection unsupervised learning sw measure scattered sample cluster mean sb measure tered cluster mean total mean would like distance pair sample particular cluster small possible cluster mean far apart possible respect chosen similarity metric euclidean case among many possible separability criterion choose trace w sb criterion invariant any nonsingular linear transformation fukunaga 1990 transformation invariance mean feature chosen any nonsingular linear transformation feature doe not change criterion value implies apply weight feature apply any nonsingular linear transformation projection feature still obtain criterion value make trace w sb criterion robust variant w sb sb normalized average cluster covariance hence larger value trace w sb larger normalized distance cluster result better cluster discrimination maximum likelihood ml criterion choosing em clustering assume ing cluster gaussian maximize likelihood data given parameter model thus maximum likelihood ml tell u well model gaussian mixture ﬁts data clustering criterion ml natural criterion feature selection also ml case interesting grouping natural grouping grouping gaussian need finding number cluster searching best subset feature run new problem number cluster k depends feature subset figure 5 illustrates point two dimension shown left three cluster whereas shown right only two cluster using ﬁxed number cluster feature set doe not model data respective subspace correctly x x x x xx x x x x x x xx x x xx xxx xx x x xx x xx x xx x xx x x xx x x x x xx x xx x x x x x x x x xx x x x xx x x x x x xx x x x xx x x x x x x x xxxxxxxxxxx xxxxxx xxxxxxxxxxxxxx xxx xxxxx xx project x x figure 5 number cluster component varies dimension unsupervised clustering made difﬁcult not know number cluster search k given feature subset currently applies bouman et method 1998 merging cluster add bayesian information criterion bic schwarz 1978 penalty term criterion penalty term needed maximum likelihood estimate increase cluster used not want end trivial result wherein data point considered individual cluster new objective function becomes f k φ log f n n number data point l number free 851 dy brodley parameter φ log f observed data x given parameter note l φ vary using bouman et method 1998 begin search k large number cluster kmax sequentially decrement number one only one cluster remains merge method method start k 1 add cluster needed split method perform split merge operation ueda et 1999 initialize parameter th model two cluster kth model merged choose two cluster among pair cluster k merged give minimum difference f k φ f k φ parameter value not merged retain value initialization th model parameter merged cluster l initialized follows 0 j πl 0 j 0 j πl 0 j 0 j 0 j 0 j superscript k indicates k cluster model superscript 0 indicates ﬁrst iteration reduced order model candidate k iterate em change f k φ le ε default n default 500 iteration algorithm output number cluster k parameter clustering assignment maximize f k φ criterion modiﬁed ml criterion myriad way ﬁnd optimal number cluster k em clustering method generally grouped three category hypothesis testing method mclachlan basford 1988 penalty method like aic akaike 1974 bic schwarz 1978 mdl rissanen 1983 bayesian method like autoclass cheeseman stutz 1996 smyth 1996 introduced new method called monte carlo mccv possible k value average likelihood run computed k value highest likelihood selected experimental evaluation smyth showed mccv autoclass found k value closer number class k value found bic data set chose bouman et method bic mccv computationally expensive mccv ha complexity number run kmax maximum number cluster considered number feature n number sample e average number em iteration complexity bouman et approach furthermore k kmax not need initialize em merged two cluster resulting note fssem run em candidate feature subset thus feature selection total complexity complexity complete em run time feature search space recently figueiredo jain 2002 presented efﬁcient algorithm integrates estimation model selection ﬁnding number cluster using minimum message length penalty method would interest future work examine way ﬁnding k coupled feature selection bias criterion value dimension feature subset selection criterion bias respect dimension need analyze bias feature subset selection compare criterion value subset 852 feature selection unsupervised learning ferent cardinality corresponding different dimensionality section 5 present solution problem bias scatter separability criterion separability criterion prefers higher dimensionality criterion value monotonically crease feature added assuming identical clustering assignment fukunaga 1990 narendra fukunaga 1977 however separability criterion may not monotonically increasing respect dimension clustering assignment change scatter separability trace criterion prefers higher dimension intuitively data scattered higher dimension mathematically feature mean adding term trace function observe figure 6 feature doe not provide additional discrimination data set yet trace criterion prefers feature subset x feature subset x ideally would like criterion value remain discrimination information x x x x x xx xx xx xx x x x x x x x x x x x x x x x xx x x x x x x x x xxx x x x x x x x x x x x x x x x x x x x xx x xx xxx xx x xx xx xx x x x xx x x x x xxxxxxxxxxx xxxxxxxxxxxxxxx figure 6 illustration scatter separability bias dimension following simple example provides u intuitive understanding bias assume feature subset feature subset produce identical clustering assignment 1 feature respectively assume also feature uncorrelated within cluster let swd sbd scatter scatter sion respectively compute trace 1 dimension simply add positive term trace wd sbd value dimension 1 dimensional space computed swd 0 0 sbd 0 0 853 dy brodley since wd 0 0 1 trace would trace wd sbd since 0 trace 1 clustering always greater equal trace clustering stated assumption separability criterion monotonically increase dimension even feature correlated long clustering assignment remain narendra fukunaga 1977 proved criterion form x xd xd vector sd positive deﬁnite matrix monotonically increase dimension showed xt xt xd b ct b xd 2 4 xd xd c ct b c column vector xd b scalar matrix symbol mean matrix augmentation show trace wd sbd expressed criterion form xt xjd sbd expressed z jbdzt jbd z jbd column vector trace wd sbd trace wd k z jbdzt jbd trace k wd z jbdzt jbd k trace wd z jbdzt jbd k trace zt wd z jbd since trace trace any rectangular matrix zt wd z jbd scalar k trace zt wd z jbd k zt wd z jbd since term monotonically increase dimension summation also monotonically crease dimension thus scatter separability criterion increase dimension assuming clustering assignment remain mean even new feature doe not itate ﬁnding new cluster criterion function increase 854 feature selection unsupervised learning bias maximum likelihood ml criterion contrary ﬁnding number cluster problem wherein ml increase number model parameter k increased feature subset selection ml prefers lower dimension ﬁnding number cluster try ﬁt best gaussian mixture data data ﬁxed try ﬁt model best feature selection given different feature space select feature subset best modeled gaussian mixture bias problem occurs deﬁne likelihood likelihood data ing candidate feature subset see equation 10 appendix b avoid bias parison two complete relevant irrelevant feature included model data case likelihood deﬁned candidate relevant feature modeled dent cluster irrelevant feature modeled no dependence cluster variable problem approach need deﬁne model irrelevant feature vaithyanathan dom us document clustering vaithyanathan dom 1999 multinomial distribution relevant irrelevant feature appropriate model text tures document clustering domain deﬁning model irrelevant feature may difﬁcult moreover modeling irrelevant feature mean parameter predict implies still work feature mentioned earlier algorithm may break high dimension may not enough data predict model parameter one may avoid problem adding assumption independence among irrelevant feature may not true irrelevant feature distribution may cause algorithm select many feature throughout paper use maximum likelihood deﬁnition only relevant feature ﬁxed number sample ml prefers lower dimension problem occurs compare feature set feature set b wherein set subset set b joint probability single point x le equal marginal probability x sequential search lead trivial result selecting only single feature ml prefers lower dimension discrete random feature joint probability mass function discrete random vector x p x p p x since 0 p x p p x x thus p x always greater equal p x any deal continuous random variable paper deﬁnition f x f f x still hold f probability density function f always greater equal zero however f greater one marginal density f x greater equal joint probability f x iff f theorem ﬁnite multivariate gaussian mixture assuming identical clustering assignment feature subset b dimension db ml φa φb iff k j j j 1 φa represents parameter σa j covariance matrix modelling cluster j feature subset π j mixture proportion cluster j k number cluster 855 dy brodley corollary ﬁnite multivariate gaussian mixture assuming identical clustering ments feature subset x x x disjoint ml φx φxy iff k j j 1 dy covariance matrix feature subset x σxx σxy σyx σyy dy dimension prove theorem corollary appendix theorem corollary reveal dependency comparing ml criterion different dimension note jth ponent left hand side term corollary determinant conditional covariance f covariance term covariance eliminating effect conditioning variable x conditional covariance doe not depend right hand side imately equal dy mean ml criterion increase feature feature subset added ha generalized variance determinant covariance matrix smaller dy ideally would like criterion measure remain subset veal cluster even feature subset reveal cluster corollary informs u ml decrease increase depending whether not generalized variance new feature greater le constant respectively normalizing criterion value method argument previous section illustrate apply ml trace criterion feature selection need normalize value respect dimension typical approach normalization divide penalty factor example scatter criterion could divide dimension similarly ml criterion could divide 1 1 would not remove covariance term due increase dimension could also divide log ml divide only portion criterion affected problem dividing penalty requires speciﬁcation different magic function criterion approach take project cluster subspace comparing given two feature subset different dimension clustering data using subset produce cluster way obtain clustering using feature subset feature subset enables u discover better cluster let crit si cj feature selection criterion value using feature subset si represent data cj clustering ment crit represents either criterion presented section normalize criterion value normalizedvalue crit criterion value normalizedvalue crit normalizedvalue si ci normalizedvalue j cj choose feature subset si malized criterion value equal si j favor lower dimensional feature subset 856 feature selection unsupervised learning choice product sum operation arbitrary taking product similar obtaining geometric mean sum arithmetic mean general one perform normalization based semantics criterion function example geometric mean would appropriate likelihood function arithmetic mean clustering assignment resulting different feature subset identical normalizedvalue would equal normalizedvalue want formally proposition 1 given equal clustering assignment two different feature subset normalizedvalue normalizedvalue proof deﬁnition normalizedvalue normalizedvalue crit substituting normalizedvalue crit normalizedvalue understand normalization remove some bias introduced difference dimension focus normalizedvalue common factor cluster found using feature subset measure criterion value feature subset evaluate cluster since cluster projected feature subset bias due data representation dimension diminished normalized value focus quality cluster obtained example figure 7 would like see whether subset lead better cluster subset crit crit give criterion value cluster found feature subspace see figure project clustering ure apply criterion obtain crit similarly project feature space obtain result shown figure measure result crit example ml maximum likelihood cluster found subset using equation 10 pendix b compute ml use cluster assignment e zij membership probability data point xi remain compute ml apply em clustering update equation equation appendix compute model parameter increased feature space since project data subset essentially comparing criterion number dimension comparing crit figure crit figure crit figure crit figure example normalized trace chooses subset exists better cluster separation subspace using rather normalized ml also chooses subset ha better gaussian mixture ﬁt smaller variance cluster subspace figure figure c note underlying one compute maximum log ml efﬁciently q φ φ h φ φ applying lemma equation 16 appendix lemma express q term only parameter estimate equation 16 h φ φ cluster entropy requires only e zij value practice work log ml avoid precision problem product normalizedvalue function becomes log normalizedvalue si ci log ml si ci ml j ci 857 dy brodley b c 2 2 2 2 f x 3 f f x x xx x x x x xxxxxxxxxxxxxx xxxxx x xxxxxxxxxxxxxx xxxxx x x x x x x x x xx x x xx xxx xx x x xx x x 2 2 3 f f f 1 2 x x x x x xx x x x xx x x x xx x x f f x x x x x x xx x xx x x xx x x x x xx x xx x x x x x x x xx x x x x x xx x x x x x x xx x x xx xxx xx x x xx x x xx x xx x xx x x xx x x x x xx x xx x x x x x x x xx x x x x x xx x x x xx x x x x x x x figure 7 illustration normalizing criterion value compare subset project clustering result call feature space shown c also project clustering result b onto feature space shown tr ml logml b tr ml logml c tr ml logml tr ml logml evaluate subset normalized tr subset normalized tr way using ml normalized value subset subset log ml normalized value subset respectively assumption behind normalization scheme cluster found new feature space consistent structure data previous feature subset ml criterion mean ci model well trace criterion mean cluster ci well separated 858 feature selection unsupervised learning experimental evaluation experiment 1 investigate whether feature selection lead better cluster using feature 2 examine result feature selection without criterion normalization 3 check whether not ﬁnding number cluster help feature selection 4 compare ml trace criterion ﬁrst present experiment synthetic data detailed analysis fssem variant using four data set section ﬁrst describe synthetic gaussian data evaluation method synthetic data em clustering implementation detail present result experiment synthetic data finally section present discus experiment three benchmark machine learning data set one new real world data set synthetic gaussian mixture data 0 1 2 3 0 1 2 3 4 5 6 feature 1 feature 2 problem 0 1 2 3 4 0 1 2 3 4 feature 1 feature 2 problem b 0 2 4 6 8 0 2 4 6 8 feature 1 feature 2 problem 0 2 4 6 8 0 2 4 6 feature 1 feature 18 problem 0 2 4 6 8 0 5 feature 1 feature 2 15 relevant feature problem c e figure 8 synthetic gaussian data understand performance algorithm experiment ﬁve set synthetic sian mixture data data set relevant irrelevant feature relevant mean created k component mixture model using feature irrelevant feature generated gaussian normal random variable ﬁve synthetic data set generated n 500 data point generated cluster equal proportion 859 dy brodley 2 relevant feature 3 noise feature ﬁrst data set shown figure consists two gaussian cluster covariance matrix mean similar data set used smyth 1996 considerable overlap two cluster three additional noise feature increase difﬁculty problem 2 relevant feature 3 noise feature second data set consists three gaussian cluster shown figure two cluster mean covariance matrix orthogonal third cluster overlap tail right side two cluster add three irrelevant feature data set used smyth 1996 2 relevant feature 3 noise feature third data set figure ha four cluster mean covariance equal add three gaussian normal random noise feature 5 relevant feature 15 noise feature fourth data set twenty tures only ﬁve relevant feature 1 10 18 19 20 true mean µ sampled uniform distribution element diagonal covariance matrix σ sampled uniform distribution fayyad et 1998 figure show scatter plot data two relevant feature 15 relevant feature 5 noise feature ﬁfth data set figure shown two relevant feature ha twenty feature ﬁfteen relevant feature 1 2 3 5 8 9 10 11 12 13 14 16 17 18 20 true mean µ sampled uniform distribution element diagonal covariance matrix σ sampled uniform distribution fayyad et 1998 evaluation measure would like measure algorithm ability select relevant feature correctly identify k ﬁnd structure data cluster no standard measure evaluating cluster clustering literature jain dubes 1988 moreover no single clustering assignment class label explains every application hartigan 1985 nevertheless need some measure mance fisher 1996 provides discus different internal external criterion measuring clustering performance since generated synthetic data know true cluster instance longs true cluster component generates instance refer true cluster known class label although used class label measure performance fssem not use information training selecting feature discovering cluster class error deﬁne class error number instance misclassiﬁed vided total number instance assign data point likely cluster assign cluster class based examining class label training data signed cluster choosing majority class since true cluster label compute classiﬁcation error one careful comparing clustering 860 feature selection unsupervised learning different number cluster using training error class error based training decrease increase number cluster k trivial result 0 error data point cluster ameliorate problem use error fold randomly partition data set ten mutually exclusive subset consider partition fold test set rest training set perform feature selection clustering training set compute class error test set fssem variant reported error average standard deviation value run bayes error since know true probability distribution synthetic data provide bayes error duda et 2001 value give u lowest average class error rate able data set instead full integration error possibly discontinuous decision region multivariate space compute bayes error experimentally using relevant feature true distribution classify generated data optimal bayes classiﬁer calculate error evaluate algorithm ability select relevant feature report average number feature selected average feature recall precision recall precision concept text retrieval salton mcgill 1983 deﬁned recall number relevant feature selected subset divided total number relevant feature precision number relevant feature selected subset divided total number feature selected measure give u indication quality feature selected high value sion recall desired feature precision also serf measure well dimension normalization scheme stopping criterion work finally evaluate clustering gorithm ability ﬁnd correct number cluster report average number cluster found initializing em implementation detail em algorithm start initial estimate parameter φ 0 iterate using update equation convergence note em initialized new feature subset em algorithm get stuck local maximum hence initialization value tant used initialization algorithm proposed fayyad et al 1998 10 j 10 iteration si 1 j randomly initialized run duda et 2001 not permitting empty cluster empty cluster exists end reset empty cluster mean equal data furthest cluster centroid result set cluster centroid cmi cluster combined set cm cmi using initialized cmi resulting new centroid fmi select fmi 1 j maximizes likelihood cm initial cluster initializing parameter em clustering iterates convergence likelihood doe not change n default 500 iteration whichever come ﬁrst limit 861 dy brodley number iteration em converges slowly near maximum avoid problem handling singular matrix adding scalar δ average variance unclustered data multiplied identity matrix δi component covariance matrix σ make ﬁnal matrix positive deﬁnite eigenvalue greater zero hence nonsingular constrain solution away spurious cluster deleting cluster any diagonal element equal le δ experiment gaussian mixture data investigate bias compare performance different feature selection criterion refer fssem using separability criterion using ml aside evaluating performance algorithm also report performance em clustering using feature see whether not feature selection helped ﬁnding interesting structure structure reveal class label fssem em assume ﬁxed number cluster k equal number class refer em clustering fssem ﬁnding number cluster respectively due clarity purpose space constraint only present relevant table report result evaluation measure presented section dy brodley 2003 ml versus trace compare performance different feature selection criterion synthetic data use rather fssem section show feature selection ﬁnding k better feature selection ﬁxed k fssem table 1 show cv error average number cluster result trace ml ﬁve data set percent cv error method average number cluster method table 1 error average number cluster versus ml applied simulated gaussian mixture data performed better term cv error trace performed better ml selected feature high cluster separation ml preferred feature low variance variance cluster ml prefers feature subset fewer cluster happens noise feature bias reﬂected average feature recall hand wa biased toward separable cluster identiﬁed deﬁned relevant feature reﬂected average feature recall 862 feature selection unsupervised learning raw data versus standardized data previous subsection ml performed worse trace synthetic data ml prefers feature low variance fewer cluster noise feature lower variance relevant feature subsection investigate whether standardizing data dimension normalizing dimension yield variance equal one would eliminate bias standardizing data sometimes done step data analysis algorithm equalize weight contributed feature would also like know standardization affect performance fssem variant let x random data vector xf f 1 element vector number feature standardize x dividing element corresponding feature standard deviation xf f σ f standard deviation feature f table 2 report cv error additional experimental result found dy brodley 2003 aside fssem variant examine effect standardizing data clustering ﬁnding number cluster using feature represent corresponding variant standardized data sufﬁx result show only affected standardizing data trace criterion computes scatter normalized average scatter invariant any linear transformation since standardizing data linear transformation trace criterion result remain unchanged standardizing data improves ml performance eliminates ml bias lower overall ance feature assuming equal variance cluster ml prefers single gaussian cluster two gaussian cluster standardization two gaussian cluster become favorable two cluster ha lower variance higher probability single cluster noise feature observe compare performance similar data set result show scale invariance important property feature evaluation criterion criterion not scale invariant ml case standardizing data dimension necessary scale invariance incorporated ml criterion ifying function presented dy brodley 2003 throughout rest paper standardize data feature selection clustering percent cv error method table 2 percent cv error fssem variant standardized raw data feature search fixed k versus search k section 3 illustrated different feature subset different number cluster model cluster feature search correctly need incorporate ﬁnding number 863 dy brodley cluster k approach section investigate whether ﬁnding k yield better formance using ﬁxed number cluster represent fssem em variant using ﬁxed number cluster equal known class fssem em stand fssem em searching table 3 4 summarize cv error average number cluster feature precision recall result different algorithm ﬁve synthetic data set percent cv error method bayes average number cluster method ﬁxed 2 ﬁxed 3 ﬁxed 4 ﬁxed 5 ﬁxed 5 ﬁxed 2 ﬁxed 3 ﬁxed 4 ﬁxed 5 ﬁxed 5 ﬁxed 2 ﬁxed 3 ﬁxed 4 ﬁxed 5 ﬁxed 5 table 3 percent cv error average number cluster result fssem em ﬁxed number cluster versus ﬁnding number cluster looking ﬁrst compared see including order identiﬁcation feature selection result lower cv error trace criterion data set except data signiﬁcantly lower cv error adding search k within feature subset selection search allows algorithm ﬁnd relevant feature average feature recall versus best number cluster depends chosen feature subset example closer examination noted problem k ﬁxed three cluster formed feature 1 better separated cluster formed feature 1 2 together consequence not select feature k made variable feature search ﬁnds two cluster feature feature 2 considered feature 1 three cluster found resulting higher separability way wa better ﬁxing k data set term cv error except data performed slightly better data set term feature precision recall note recall value low data some relevant feature redundant reﬂected cv error obtained feature selection algorithm 864 feature selection unsupervised learning average feature precision method average feature recall method table 4 average feature precision recall obtained fssem ﬁxed number cluster versus fssem ﬁnding number cluster show incorporating ﬁnding k help selecting relevant feature lower cv error due prior knowledge correct number cluster std poorer performance retained noisy feature feature criterion normalization versus without normalization percent cv error method bayes average number feature selected method table 5 percent cv error average number feature selected fssem criterion malization versus without 865 dy brodley table 5 present cv error average number feature selected feature selection criterion normalization versus without sufﬁx notnorm throughout paper refer normalization feature normalization scheme projection method described section trace criterion without normalization not affect cv error however normalization achieved similar cv error performance using fewer feature without normalization ml criterion criterion normalization deﬁnitely needed note without selected only single feature data set resulting worse cv error performance normalization except data ha only one relevant feature feature selection versus without feature selection case feature selection fssem obtained better result without feature lection em reported table note data set noise feature misled leading fewer cluster true observe wa able ﬁnd approximately true number cluster different data set subsection experiment sensitivity fssem variant number noise feature figure plot error average number cluster average number noise feature feature precision recall respectively feature selection without feature selection noise feature added data note cv error performance average number cluster average number selected feature feature recall feature selection algorithm le constant throughout approximately equal clustering no noise feature precision recall plot reveal cv error performance feature selection wa not affected noise variant able select relevant feature recall 1 discard noisy feature high precision figure 9 demonstrates need feature selection irrelevant feature mislead clustering result reﬂected performance noise feature added conclusion experiment synthetic data experiment simulated gaussian mixture data reveal standardizing data feature subset selection conjunction ml criterion needed remove ml preference low variance feature order identiﬁcation led better result ﬁxing k different feature subset different number cluster illustrated section 3 criterion normalization scheme introduced section 5 removed bias trace ml respect dimension normalization scheme enabled feature selection trace remove redundant feature prevented feature selection ml selecting only single feature trivial result ml trace feature selection performed equally well ﬁve data set criterion able ﬁnd relevant feature feature selection obtained better result without feature selection 866 feature selection unsupervised learning 0 2 4 6 8 10 12 14 16 18 0 10 20 30 40 50 60 70 80 number noise feature percent 0 2 4 6 8 10 12 14 16 18 1 2 3 4 number noise feature average number cluster b 0 2 4 6 8 10 12 14 16 18 2 4 6 8 10 12 14 16 18 20 number noise feature average number feature 0 2 4 6 8 10 12 14 16 18 1 number noise feature average feature precision c 0 2 4 6 8 10 12 14 16 18 0 1 2 number noise feature average feature recall e figure 9 feature selection versus without feature selection data experiment real data examine fssem variant iris wine ionosphere data set uci learning repository blake merz 1998 high resolution computed tomography hrct lung 867 dy brodley image data collected iupui medical center dy et 2003 dy et 1999 though data set class information known remove class label training unlike synthetic data not know true number gaussian cluster data set class may composed many gaussian cluster moreover cluster may not even gaussian distribution see whether clustering algorithm found cluster correspond class wherein class compute class error way synthetic gaussian data real data set not know relevant feature hence not compute precision recall therefore report only average number feature selected average number cluster found although use class error measure cluster performance not let misguide u interpretation cluster quality interestingness difﬁcult measure depends particular application major distinction unsupervised clustering pervised learning class error one interpretation data also measure cluster performance term trace criterion ml criterion naturally performed best term trace best term maximum likelihood choosing either tr ml depends application goal interested ﬁnding feature best separate data use interested ﬁnding feature model gaussian cluster best use illustrate generality ease applying clustering method wrapper work also show result different variant feature selection wrapped around clustering algorithm forgy 1965 duda et 2001 coupled tr ml criterion use sequential forward search feature search ﬁnd number cluster apply bic penalty criterion pelleg moore 2000 use following acronym throughout rest paper kmeans stand algorithm stand feature selection wrapped around tr represents trace criterion feature evaluation ml represents ml criterion evaluating feature represents variant ﬁnds number cluster show data wa standardized feature ha variance equal one since cluster quality depends initialization method used clustering performed em clustering using three different initialization method initialize using ten start initialized random seed pick ﬁnal clustering corresponding highest likelihood ten random fayyad et method described earlier section fayyad et 1998 item one two similar clustering hence initialize item two three item three performed using fayyad et method bradley fayyad 1998 applies data instead em distortion pick best clustering instead ml criterion discussion section follows show result fssem variant using initialization provides consistently good across method present result using initialization method fssem variant dy brodley 2003 appendix table represent initialization method 1 2 3 respectively 868 feature selection unsupervised learning iris data fssem variant method cv error no cluster no feature ﬁxed 3 ﬁxed 3 ﬁxed 3 ﬁxed 4 ﬁxed 4 iris data variant method cv error no cluster no feature ﬁxed 3 ﬁxed 3 ﬁxed 3 ﬁxed 4 ﬁxed 4 table 6 result different variant iris data iris data ﬁrst look simplest case iris data data ha three class four feature 150 instance fayyad et al method initialization work best large data set since iris data only ha number instance class ten start provided consistently best result initializing em clustering across different method table 6 summarizes result different variant fssem compared em clustering without feature selection iris data set kmax equal six fssem ﬁxed k three equal number labeled class cv error much better mean not know true number cluster feature selection help ﬁnd good cluster even found correct number cluster em clustering true number cluster gave good result feature selection case not improve however produced similar error rate fewer feature fssem different variant consistently chose feature 3 feature 4 fact learned experiment only two feature needed correctly cluster iris data three group corresponding figure 10 b show clustering result scatterplot ﬁrst two feature chosen respectively result feature selection wrapped around also shown table infer similar conclusion result variant fssem variant data set wine data wine data ha three class thirteen feature 178 instance data set kmax equal six fssem ﬁxed k three equal number labeled class table 7 summarizes result fssem variant initialized ten start ten random respectively initialization method led best performance em without feature selection k 869 dy brodley 1 2 3 4 0 1 2 3 feature 3 feature 4 1 2 3 4 0 1 2 3 feature 3 feature 4 b figure 10 scatter plot iris data using ﬁrst two feature chosen b different class assignment cluster mean ellipsis covariance corresponding cluster discovered wine data fssem variant method cv error no cluster no feature ﬁxed 3 ﬁxed 3 ﬁxed 3 ﬁxed 13 ﬁxed 13 wine data variant method cv error no cluster no feature ﬁxed 3 ﬁxed 3 ﬁxed 3 ﬁxed 13 ﬁxed 13 table 7 result different variant wine data set known wa able ﬁnd cluster corresponding true class correctly em clustering also performed well k given em clustering performed poorly term cv error k unknown situation feature selection helped base clustering method ﬁnd good grouping interestingly wine data performed better better example using different criterion feature selection clustering improved result interaction figure 11 b show scatterplots cluster discovered projected ﬁrst two feature chosen tr respectively picked feature 870 feature selection unsupervised learning 2 3 4 5 6 1 2 3 4 5 feature 12 feature 13 1 2 3 4 5 1 2 3 4 5 feature 2 feature 13 b figure 11 scatter plot wine data using ﬁrst two feature chosen b different class assignment cluster mean ellipsis covariance corresponding cluster discovered selected feature feature 12 13 stand diluted wine ionosphere data radar data collected phased array sixteen antenna target free electron atmosphere class label data either good radar return showing structure ionosphere bad return 351 instance 34 continuous attribute measuring time pulse pulse number feature 1 2 discarded value constant discrete instance constant feature value produce inﬁnite likelihood value gaussian mixture model discrete feature value discrete level le equal number cluster also produce inﬁnite likelihood value ﬁnite gaussian mixture model table 8 report error number cluster found ent em fssem algorithm ionosphere data set kmax equal ten ﬁxed k two equal number labeled class fssem em clustering k known performed better term cv error compared rest em variant note gave comparable performance em using fewer feature no knowledge true number cluster table 8 also show result different variant obtains best cv error followed closely std interestingly two method chose feature 5 3 based original 34 feature ﬁrst two feature figure b present scatterplots ionosphere data ﬁrst two feature chosen together corresponding mean covariance ellipsis discovered observe favored cluster feature figure cluster well separated hand favored cluster figure small generalized variance since ml criterion match ionosphere feature subset feature provided best performance among run 871 dy brodley ionosphere data fssem variant method cv error no cluster no feature ﬁxed 2 ﬁxed 2 ﬁxed 2 ﬁxed 32 ﬁxed 32 ionosphere data variant method cv error no cluster no feature ﬁxed 2 ﬁxed 2 ﬁxed 2 ﬁxed 32 ﬁxed 32 table 8 result different variant ionosphere data set class label closely performed better respect cv error ml obtained better cv error also performed better kmeans term cv error feature selection variant performed better using fewer feature compared 32 feature used kmeans interesting note data random initialization obtained signiﬁcantly better cv error em clustering compared initialization method ten start fayyad et method respectively two true class highly overlapped ten start tend cluster 0 1 2 0 1 2 feature 5 feature 14 0 1 2 0 1 2 feature 5 feature 3 b figure 12 scatter plot ionosphere data using ﬁrst two feature chosen b different class assignment cluster mean ellipsis covariance corresponding cluster discovered 872 feature selection unsupervised learning data centrilobular emphysema b paraseptal emphysema c ipf figure 13 image consists 500 instance instance represented 110 level continuous feature measuring geometric gray level texture feature dy et 1999 actually used only 108 feature two feature constant discrete also data make feature mostly positive number gaussian feature negative value like feature local mean minus global mean add offset making minimum value equal zero assign log 0 log data classiﬁed ﬁve disease class centrilobular emphysema paraseptal emphysema eg ipf panacinar figure 13 show three image three disease class white marking pathology bearing region pbr marked radiologist instance represents pbr image may contain one pbr one disease classiﬁcation note centrilobular emphysema ce characterized large number low intensity darker region may occupy entire lung figure paraseptal sema pe also characterized low intensity region see figure unlike ce region occur near boundary near ﬁssures dark region usually separated thin wall adjacent boundary ﬁssure ce pe grouped according disease severity characterized intensity region lower intensity indicate severe case lung image ipf characterized high intensity forming structure shown figure feature selection important data set em clustering using feature result one cluster table 9 present result data set hrct lung data performed better respectively term cv error figure 14 b present scatterplots data ﬁrst two feature chosen observe cluster found well separated match class label well hand selects feature result cluster figure 14 b demonstrates clearly note also true number cluster data ﬁve number labeled class helped obtained better result variant 873 dy brodley data fssem variant method cv error no cluster no feature ﬁxed 5 ﬁxed 5 ﬁxed 5 ﬁxed 108 ﬁxed 108 data variant method cv error no cluster no feature ﬁxed 5 ﬁxed 5 ﬁxed 5 ﬁxed 108 ﬁxed 108 table 9 result image data set different variant feature 7 feature 9 0 0 feature 27 feature 36 b figure 14 scatter plot data using ﬁrst two feature chosen b different class ments cluster mean ellipsis covariance corresponding cluster discovered difﬁcult data set due skewed class distribution approximately data disease centrilobular emphysema even though discovered approximately only one cluster class error equal error using majority ﬁcation rule close value obtained method high dimension obscure class result ﬁnding only one cluster even difﬁcult problem feature selection obtained better without feature selection using much fewer feature average compared original 108 feature picked feature chose feature feature 9 gray level histogram value lung region feature 59 histogram value local pathology bearing region feature make sense discriminating 874 feature selection unsupervised learning centrilobular emphysema largest class rest class characterized low gray level value conclusion experiment real data result real data show feature selection improved performance clustering rithms ﬁnding interesting pattern measure interestingness performance well discovered cluster match labeled class tained better using fewer feature moreover experiment reveal no one feature selection criterion ml tr better ent bias ml selects feature result cluster performed better tr ionosphere data scatter separability tr prefers feature reveal cluster performed better ml data well iris wine data related work review feature selection algorithm unsupervised learning three different way select feature unsupervised data 1 clustering 2 clustering 3 clustering example algorithm performs feature selection clustering mirkin 1999 method ﬁrst applies new version clustering computes contribution weight variable proportion squared deviation variable mean total mean represents cluster conjunctive concept starting variable highest weight adding variable conceptual description doe not improve cluster precision error feature selection clustering important conceptual learning describing summarizing structure data type selecting feature remove redundancy not feature irrelevance initial clustering performed using feature pointed earlier existence irrelevant feature misguide clustering result using feature clustering also assumes clustering algorithm doe not break high dimensional data paper only examine feature selection algorithm affect change clustering outcome clustering signiﬁcant body research exists method feature subset selection supervised data method grouped ﬁlter marill green 1963 narendra fukunaga 1977 almuallim dietterich 1991 kira rendell 1992 kononenko 1994 liu setiono 1996 cardie 1993 singh provan 1995 wrapper john et 1994 doak 1992 caruana freitag 1994 aha bankert 1994 langley sage 1994 pazzani 1995 approach maintain model distinction used supervised learning deﬁne ﬁlter method unsupervised learning using some intrinsic property data select feature without utilizing clustering algorithm ultimately applied wrapper approach hand apply unsupervised learning algorithm candidate feature subset evaluate feature subset criterion function utilize clustering result ﬁrst started research not much work ha done feature subset selection unsupervised learning context machine learning although research form cipal component analysis pca chang 1983 factor analysis johnson wichern 1998 projection pursuit friedman 1987 huber 1985 existed early work data reduction unsupervised data thought ﬁlter method select feature prior 875 dy brodley x x x xx x x x x x x x x x xx x x xx x x x x x x x x x x x x xxx x xx x x x xx x x x x x x x xx x x x xx x x x x x x x x x x x x x x x x xx x x x xx x x x x x x x xx x x x xx x x x xx x x x x x x x xx x x x xx x x x xx x x x x xx x x x x x xx x x x x xx x x x b figure 15 illustration pca poor discriminator ing clustering rather selecting subset feature involve some type feature transformation pca factor analysis aim reduce dimension representation faithful possible original data note data reduction technique based tion like pca better suited compression application rather classiﬁcation fukunaga 1990 provides illustrative example figure 15 recreates example pca chooses projection highest variance projecting two dimension one dimension example pca would project data axis b clearly inferior axis discriminating two cluster contrary pca factor analysis projection pursuit aim ﬁnd interesting tions data visualizing structure data recent method ﬁnding transformation called independent component analysis ica arinen 1999 ha gained spread attention signal processing ica try ﬁnd transformation transformed variable statistically independent ﬁlter method described previous paragraph involve transformation original variable space paper interested subset original space some main prefer original variable order maintain physical interpretation feature moreover transformation variable space require computation collection feature dimension reduction achieved whereas subset original space require tation collection only selected feature subset feature selection determined some feature cost others one consider cost selecting feature paper assume feature ha equal cost interesting current direction feature selection involving feature transformation mixture principal component analyzer kambhatla leen 1997 tipping bishop 1999 mixture factor analyzer ghahramani beal 2000 ghahramani hinton 1996 ueda et 1999 consider mixture algorithm wrapper approach recent year attention ha paid unsupervised feature subset selection method wrapper approach gennari 1991 incorporates feature selection call attention classit incremental concept formation hierarchical clustering algorithm troduced gennari et 1989 attention algorithm inspects feature starting salient contribution category utility attribute least salient attribute 876 feature selection unsupervised learning stop inspecting feature remaining feature not change current clustering decision purpose attention mechanism increase efﬁciency without loss prediction racy devaney ram 1997 applied sequential forward backward search evaluate candidate subset measured category utility cluster found applying cobweb fisher 1987 conjunction feature subset talavera 1999 applied blind similar ﬁlter feedback analogous wrapper approach cobweb used feature dependence measure select feature vaithyanathan dom 1999 formulated objective function choosing feature subset ﬁnding optimal number cluster document clustering problem using bayesian statistical estimation framework modeled cluster multinomial extended concept create hierarchical cluster vaithyanathan dom 2000 agrawal et al 1998 introduced clustering algorithm clique proceeds one feature highest dimension no feature subspace cluster region high density point generated clique density based clustering algorithm doe not assume any density model however clique need specify eters τ density threshold ν equal length interval partitioning dimension contrast method make assumption distribution avoid specifying parameter kim street menczer 2002 apply evolutionary local selection algorithm elsa search feature subset number cluster two clustering algorithm em clustering diagonal covariance pareto front combine multiple objective evaluation function law figueiredo jain 2002 estimate feature saliency using em modeling relevant feature conditionally independent given component label irrelevant feature probability density identical component also developed wrapper approach selects feature using divergence entropy friedman meulman 2003 designed distance measure data clustering subset attribute allow feature subset cluster different summary paper introduced wrapper framework performing feature subset selection pervised learning explored issue involved developing algorithm framework identiﬁed need ﬁnding number cluster feature search provided proof bias ml scatter separability respect dimension presented method ameliorate problem experimental result showed incorporating ﬁnding number cluster k feature subset selection process led better result ﬁxing k true number class two reason 1 number class not necessarily equal number gaussian cluster 2 different feature subset different number cluster supporting theory experiment simulated data showed ml scatter separability some way biased respect dimension thus normalization scheme needed chosen feature selection criterion proposed criterion normalization scheme wa able eliminate bias although examined wrapper framework using fssem search method feature lection criterion especially trace criterion feature normalization scheme easily applied any clustering method issue encountered solution presented cable any feature subset wrapper approach fssem serf example depending one 877 dy brodley application one may choose apply appropriate search method clustering feature selection criterion future direction research feature subset selection unsupervised learning quite young even though addressed some issue paper open question need answered hartigan 1985 pointed no single criterion best application reiterated result hrct ionosphere data led u work visualization user interaction guide feature search dy brodley another interesting direction look feature selection hierarchical clustering gennari 1991 fisher 1996 devaney ram 1997 talavera 1999 vaithyanathan dom 2000 since hierarchical clustering provides grouping various perceptual level addition cluster may modeled better different feature subset cluster one may wish develop algorithm select different feature subset cluster component explored unsupervised feature selection wrapper framework would teresting rigorous investigation ﬁlter versus wrapper approach unsupervised learning one may also wish venture transformation original variable space particular vestigate mixture principal component analyzer kambhatla leen 1997 tipping bishop 1999 mixture factor analyzer ghahramani beal 2000 ghahramani hinton 1996 ueda et 1999 mixture independent component analyzer arinen 1999 difﬁculty unsupervised learning absence labeled example guide search breiman breiman 2002 suggests transforming clustering problem classiﬁcation problem assigning unlabeled data class one adding amount random vector another class two second set generated independent sampling marginal distribution class one understanding developing trick uncover structure unlabled data remains topic need investigation another avenue future work explore labeled example large amount unlabeled data method feature selection finally feature selection unsupervised learning several fundamental question still unanswered deﬁne interestingness mean criterion interestingness feature selection criterion terion natural grouping clustering criterion literature us terion feature selection clustering lead clean optimization formulation however deﬁning interestingness mathematical criterion difﬁcult problem lowing different criterion interact may provide better model experimental result wine data suggest direction experiment synthetic data indicate need standardize feature mirkin 1999 also standardized feature feature always standardized feature lection standardize data containing different feature type nominal discrete 878 feature selection unsupervised learning best way evaluate result paper evaluate performance using external criterion class error standard measure used paper feature selection unsupervised learning literature class error task speciﬁc measure performance one labeling solution best way compare different clustering algorithm acknowledgment author wish thank craig codrington discussion criterion normalization lunch group purdue university helpful comment reviewer constructive remark research supported nsf grant no nih grant no 1 appendix em clustering clustering using ﬁnite mixture model method ha used long time pattern recognition duda hart 1973 fukunaga 1990 jain dubes 1988 statistic mclachlan basford 1988 titterington et al 1985 fraley raftery 2000 model one assumes data generated mixture component density function component density function represents cluster probability distribution function data ha following form f k πj f j j 5 f j j probability density function class j π j mixing proportion class j prior probability class j k number cluster xi random data vector θ j set parameter class j φ π θ set parameter f probability density function observed data point xi given parameter since π j prior probability subject following constraint π j πj xi 1 n data vector trying cluster n number sample cluster xi need estimate parameter one method estimating φ ﬁnd φ maximizes log f log f compute f need know cluster missing data xi observed data belongs apply em algorithm provides u information data point xi belong one cluster weighted probability belong cluster em algorithm introduced some generality dempster laird rubin 1977 iterative approximation algorithm computing maximum likelihood ml estimate missing data problem going derivation applying em gaussian mixture model obtain following em update equation wolfe 1970 e zij p zij φ f j j π j f π 6 879 dy brodley π j 1 n n e zij 7 µ j 1 nπ j n e zij xi 8 σ j 1 nπ j n e zij xi j xi j 9 e zij probability xi belongs cluster j given current parameter xi e zij estimated number data point class j superscript refers iteration appendix additional proof ml bias dimension appendix prove theorem corollary state condition need satisﬁed maximum likelihood feature subset ml φa greater equal maximum likelihood feature subset b ml φb prove result ﬁrst deﬁne maximum likelihood criterion mixture gaussians prove lemma derives simpliﬁed form exp q φ φ ﬁnite gaussian mixture lemma state condition need satisﬁed complete expected data q function given observed data parameter estimate feature subset q φa φa greater equal q function feature subset b q φb φb maximum likelihood data x ml max φ f max φ n k πj f j j 10 f j j probability density function class j π j mixing proportion class j prior probability class j n number data point k number cluster xi random data vector θ j set parameter class j φ π θ set parameter f probability density function observed data x xn given parameter choose feature subset maximizes criterion lemma ﬁnite mixture gaussians exp q φ φ k πnπ j j 1 dnπ j 2 nπ j 2 2 dnπ j xi 1 n n observed data point zij missing variable equal one xi belongs cluster j zero otherwise π j mixture proportion µ j mean σ j covariance matrix gaussian cluster respectively φ π µ σ set estimated parameter 880 feature selection unsupervised learning proof q φ φ log f x φ log f φ φ log f φ n k p zij φ log f j j n k p zij φ logπ j n k p zij φ log π j f j j 11 n k e zij log π j f j j exp q φ φ n k π j f j j e zi j 12 substituting parameter estimate equation 12 sample data xi exp q φ φ k e zi j j n 1 2 1 2 2 j j j e zi j k πnπ j j 1 dnπ j 2 nπ j 2 2 e zi j j j j 13 simplifying exponent e obtain 2 n e zij xi j j xi j 2 n e zij tr xi j j xi j 2 n e zij tr j xi j xi j j n e zij xi j xi j adding subtracting x j x j 1 nπ j e zij xi last expression becomes j n e zij xi j j j xi j j j cancelling term yield j n e zij xi j xi j n e zij x j j x j j 881 dy brodley ﬁnally substituting parameter estimate equation give expression j σ jnπ j j 14 thus exp q φ φ expressed exp q φ φ k πnπ j j 1 dnπ j 2 nπ j 2 2 dnπ j lemma assuming identical clustering assignment feature subset b sion db q φa φa φb φb iff k j j j 1 proof applying lemma assuming subset b equal clustering assignment exp q φa φa exp q φb φb 1 πnπ j j 1 danπ j 2 1 nπ j 2 j πnπ j j 1 db nπ j 2 1 nπ j 2 j 1 15 given db without loss generality cancelling common term k j j nπ j 2 nπ j 2 1 k j j j π j 1 π j k j j j 1 k j j j 1 theorem theorem restated ﬁnite multivariate gaussian mixture assuming cal clustering assignment feature subset b dimension db ml φa φb iff k j j j 1 882 feature selection unsupervised learning proof logl φ log f logl φ log f x φ log f φ φ q φ φ h φ φ h φ φ log f φ φ n k p zij φ j log p zij φ j n k e zij loge zij 16 since identical clustering assignment assumption mean e zij feature set equal e zij feature set b h φa φa h φb φb thus ml φa ml φb exp q φa φa exp q φb φb ﬁnite gaussian mixture lemma ml φa ml φb iff k j j j 1 corollary corollary restated ﬁnite multivariate gaussian mixture assuming tical clustering assignment feature subset x x x andy disjoint ml φx ml φxy iff k j j 1 dy proof applying theorem let marginal feature vector x dimension dx b joint feature vector x dimension dx dy subset x disjoint maximum likelihood x greater equal maximum likelihood x iff ml φx ml φxy 1 dy k σxx σxy σyx σyy j j π j exercise johnson wichern 1998 show any square matrix 883 dy brodley 22 0 11 thus dy k 1 j j j 1 k j j 1 dy result mean one compute maximum log ml efﬁciently q φ φ φ φ applying lemma equation lemma show ml criterion prefers low covariance cluster equation 16 show ml criterion penalizes increase cluster entropy theorem corollary reveal dependency comparing ml criterion different dimension note left hand side term corollary determinant covariance f covariance minus correlation criterion measure unbiased respect dimension criterion value different subset cluster assignment equal not dependent dimension case ml increase additional feature ha small variance decrease additional feature ha large variance reference agrawal gehrke gunopulos raghavan automatic subspace clustering high dimensional data data mining application proceeding acm sigmod international conference management data page seattle wa june acm press aha bankert feature selection classiﬁcation cloud type empirical comparison proceeding 1994 aaai workshop reasoning page seattle wa aaai press akaike new look statistical model identiﬁcation ieee transaction automatic control 6 december almuallim dietterich learning many irrelevant feature proceeding ninth national conference artiﬁcial intelligence page anaheim ca aaai press blake merz uci repository machine learning database 1998 884 feature selection unsupervised learning bouman shapiro cook atkins cheng cluster unsupervised algorithm modeling gaussian mixture cluster october bradley fayyad reﬁning initial point clustering proceeding fifteenth international conference machine learning page san francisco ca morgan kaufmann breiman wald lecture ii looking inside black box 2002 ing institute mathematical statistic banff alberta canada cardie using decision tree improve learning machine learning proceeding tenth international conference page amherst morgan kaufmann caruana freitag greedy attribute selection proceeding eleventh international conference machine learning page new brunswick nj morgan kaufmann chang using principal component separating mixture two multivariate normal distribution applied statistic cheeseman stutz bayesian classiﬁcation autoclass theory result advance knowledge discovery data mining page cambridge press dempster laird rubin maximum likelihood incomplete data via em algorithm journal royal statistical society series b 39 1 devaney ram efﬁcient feature selection conceptual clustering proceeding fourteenth international conference machine learning page nashville tn morgan kaufmann doak evaluation feature selection method application computer security technical report university california davis duda hart pattern classiﬁcation scene analysis wiley son ny duda hart stork pattern classiﬁcation second edition wiley son ny dy brodley feature subset selection order identiﬁcation unsupervised learning proceeding seventeenth international conference machine learning page stanford university morgan kaufmann dy brodley interactive visualization feature selection unsupervised data proceeding sixth acm sigkdd international conference knowledge discovery data mining page boston august acm press dy brodley feature selection unsupervised learning technical report northeastern university december 2003 885 dy brodley dy brodley kak broderick aisen unsupervised feature selection applied retrieval lung image ieee transaction pattern analysis machine intelligence 25 3 march dy brodley kak shyu broderick approach cbir using em proceeding ieee conference computer vision pattern recognition volume 2 page fort collins co june ieee computer society press fayyad reina bradley initialization iterative reﬁnement clustering algorithm proceeding fourth international conference knowledge discovery data mining page new york august aaai press figueiredo jain unsupervised learning ﬁnite mixture model ieee transaction pattern analysis machine intelligence fisher iterative optimization simpliﬁcation hierarchical clustering journal artiﬁcial intelligence research fisher knowledge acquisition via incremental conceptual clustering machine learning 2 2 forgy cluster analysis multivariate data efﬁciency interpretability classiﬁcations biometrics fraley raftery clustering discriminant analysis density estimation technical report technical report no 380 university washington seattle wa friedman exploratory projection pursuit journal american statistical association 266 friedman meulman clustering object subset attribute appear journal royal statistical society fukunaga statistical pattern recognition second edition academic press san diego ca gennari concept formation attention proceeding thirteenth annual conference cognitive science society page chicago il lawrence erlbaum gennari langley fisher model incremental concept formation artiﬁcial intelligence ghahramani beal variational inference bayesian mixture factor analyser solla leen muller editor advance neural information processing system volume 12 page mit press ghahramani hinton em algorithm mixture factor analyzer technical report technical report university toronto toronto canada hartigan statistical theory clustering journal classiﬁcation 1985 886 feature selection unsupervised learning huber projection pursuit annals statistic 13 2 arinen survey independent component analysis neural computing survey jain dubes algorithm clustering data prentice hall englewood cliff nj john kohavi pﬂeger irrelevant feature subset selection problem machine learning proceeding eleventh international conference page new brunswick nj morgan kaufmann johnson wichern applied multivariate statistical analysis 4 edition kambhatla leen dimension reduction local principal component analysis neural computation kim street menczer evolutionary model selection unsupervised learning intelligent data analysis kira rendell practical approach feature selection proceeding ninth ternational workshop machine learning page aberdeen scotland morgan kaufmann kittler feature set search algorithm pattern recognition signal processing page kohavi john wrapper feature subset selection artiﬁcial intelligence 97 kononenko estimating attribute analysis extension relief proceeding pean conference machine learning langley sage oblivious decision tree abstract case working note 94 workshop reasoning page seattle aaai press law figueiredo jain feature selection clustering advance neural information processing system 15 vancouver december liu setiono dimensionality reduction via discretization system 9 1 february marill green effectiveness receptor recognition system ieee action information theory mclachlan krishnan em algorithm extension john wiley new york mclachlan basford mixture model inference application clustering marcel dekker new york 1988 887 dy brodley milligan monte carlo study thirty internal criterion measure cluster analysis chometrika 46 2 june mirkin concept learning feature selection based clustering machine ing moon algorithm ieee signal processing magazine page november narendra fukunaga branch bound algorithm feature subset selection ieee transaction computer 9 september pazzani searching dependency bayesian classiﬁers proceeding ﬁfth national workshop artiﬁcial intelligence statistic lauderdale florida pelleg moore extending efﬁcient estimation number cluster proceeding seventeenth international conference machine learning page morgan kaufmann san francisco ca rissanen universal prior integer estimation minimum description length annals statistic 11 2 russell norvig artiﬁcial intelligence modern approach prentice hall saddle river nj salton mcgill introduction modern information retrieval book company schwarz estimating dimension model annals statistic 6 2 singh provan comparison induction algorithm selective bayesian classiﬁers machine learning proceeding twelfth international conference page smyth clustering using monte carlo simoudis han fayyad editor proceeding second international conference knowledge discovery data mining page portland aaai press talavera feature selection preprocessing step hierarchical clustering proceeding sixteenth international conference machine learning page bled slovenia morgan kaufmann tipping bishop mixture probabilistic principal component analyser neural computation 11 2 titterington smith makov statistical analysis finite mixture butions john wiley son chichester uk ueda nakano ghahramani hinton smem algorithm mixture model neural computation appear 888 feature selection unsupervised learning vaithyanathan dom model selection unsupervised learning application ment clustering proceeding sixteenth international conference machine learning page bled slovenia morgan kaufmann vaithyanathan dom hierarchical unsupervised learning proceeding teenth international conference machine learning page palo alto ca morgan kaufmann wolfe pattern clustering multivariate mixture analysis multivariate behavioral research 5 3 wu convergence property em algorithm annals statistic 11 1 1983 889