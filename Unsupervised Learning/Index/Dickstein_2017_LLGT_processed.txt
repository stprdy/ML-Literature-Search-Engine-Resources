1 unsupervised algorithm learning lie group transformation jascha 4 5 ching ming 2 6 bruno 2 3 center theoretical neuroscience optometry neuroscience institute university california berkeley applied physic stanford university academy keywords lie group video coding transformation natural scene statistic namical system abstract present several theoretical contribution allow lie group ﬁt high dimensional datasets transformation operator represented ducing computational complexity parameter estimation training linear transformation model transformation speciﬁc blurring operator introduced allows inference escape local minimum via smoothing transformation space penalty traversed manifold distance added encourages discovery sparse minimal distance transformation state learning inference demonstrated using method full set afﬁne transformation natural image patch transformation operator trained natural video sequence 7 jun 2017 shown learned video transformation provide better description frame difference standard motion model based rigid translation 1 introduction past several decade research natural scene statistic community ha shown possible learn efﬁcient representation sensory data age sound intrinsic statistical structure representation exhibit higher coding efﬁciency compared standard fourier wavelet base lewicki olshausen 1999 also match neural representation found visual auditory system olshausen field 1996 van hateren van der schaaf 1998 smith lewicki 2006 explore whether approach may used learn transformation data observing pattern change time apply problem coding image sequence video central problem video coding ﬁnd representation image sequence efﬁciently capture image content change time current approach problem largely based assumption local image region undergo rigid spatial translation one frame next encode local motion along resulting prediction error wiegand et 2003 however spatiotemporal structure occurring natural image sequence quite complex due occlusion motion object lighting shading change projection motion onto image plane attempting capture complexity simple translation model lead larger prediction error thus higher bit rate right approach would seem require complex transformation model adapted statistic image actually change time approach propose based learning lie continuous tion group representation dynamic occur visual world rao ruderman 1999 miao rao 2007 culpepper olshausen 2010 cohen welling 2014 lie group built ﬁrst describing inﬁnitesimal formation image may undergo full group generated possible composition inﬁnitesimal transformation allows formation applied smoothly continuously large class visual 2 mations including afﬁne transformation intensity change due change lighting contrast change described simply using lie group operator tially localized version preceding transformation also captured miao rao 2007 rao ruderman 1999 lie group operator trained image sequence containing subset afﬁne transformation memisevic hinton 2010 trained second order restricted boltzmann machine pair frame alternative technique also show promise capturing temporal structure video despite simplicity power lie group representation training model difﬁcult part due high computational cost evaluating gating learning gradient matrix exponential previous work rao erman 1999 miao rao 2007 olshausen et 2007 ha approximated full model using ﬁrst order taylor expansion reducing exponential model linear one computationally efﬁcient linear model approximates full exponential model only small range transformation hinderance dealing real video data contain large change pair frame note miao rao 2007 full lie group model used inferring tion coefﬁcients only linear approximation used learning culpepper olshausen 2010 work full exponential model technique requires forming costly eigendecomposition effective transformation operator sample every learning inference step another hurdle one encounter using lie group model describe tions inference process computes transformation pair image highly many local minimum problem ha tensively studied image registration stereo matching computation optic ﬂow speciﬁc set transformation translation rotation isotropic scaling kokiopoulou frossard 2009 showed one could ﬁnd global minimum formulating problem using overcomplete image representation composed gabor gaussian basis function arbitrary transformation one solution initialize inference many different coefﬁcient value miao rao 2007 drawback number initial guess grows exponentially number transformation alternatively lucas kanade 1981 vasconcelos lippman 1997 black jepson 1996 perform matching image pyramid 3 using solution lower resolution level seed search algorithm higher resolution level culpepper olshausen 2010 used technique perform learning lie group operator natural movie piecewise scheme avoid local minimum searching smooth part transformation space proceeding le smooth part space constitutes indirect method coarsening transformation spatial blurring show also possible smooth transformation space directly resulting robust method estimating transformation coefﬁcients arbitrary transformation work propose method directly learning lie group operator mediate continuous transformation natural movie demonstrate ability robustly infer transformation frame video using learned operator computational complexity learning operator reduced term eigenvectors eigenvalue resulting complexity equivalent linear approximation inference made robust tractable smoothing transformation space directly allows continuous search transformation coefﬁcients learning inference demonstrated test data containing full set known afﬁne transformation technique used learn set lie operator describing change frame natural movie optimal solution not known unlike previous lie group tations demonstrate ability work simultaneously multiple transformation large difference inference learning another paper additionally show utility approach video compression wang et 2011 2 model rao ruderman 1999 consider class continuous transformation described ﬁrst order differential equation µ x µ 1 x µ represents pixel value n n image patch inﬁnitesimal transformation operator generator lie group 4 x µ image x 0 transformed amount µ differential equation ha solution x µ eaµx 0 µ x 0 2 µ eaµ matrix exponential deﬁned taylor expansion goal use transformation form model change cent frame x x occur natural video seek ﬁnd model parameter adapted ensemble video image sequence cients µ inferred pair frame minimize reconstruction error e x x x 2 2 3 extended multiple transformation derive learning rule necessary compute gradient naive application chain rule requires n 6 operation ortiz et 2001 n 2 operation per element n 4 element making computationally intractable many problem interest however computation reduces n complexity matrix inversion rewritten term learning instead performed directly term u u complex matrix consisting eigenvectors λ complex diagonal matrix holding eigenvalue inverse trice must complex order facilitate periodic transformation rotation note u need not orthonormal beneﬁt representation 1 4 matrix exponential diagonal matrix simply nential diagonal entry representation therefore replaces full matrix exponential two matrix multiplication matrix inversion ponential change form enforces restriction diagonalizable not expect restriction onerous since set matrix 5 measure zero set matrix however some speciﬁc desirable transformation may lost instance since nilpotent matrix translation without periodic boundary condition translation content lost move frame not captured approach adaptive smoothing general reconstruction error described equation 3 highly µ contains many local minimum illustrate red solid line figure 1 plot reconstruction error image patch shifted three pixel right function transformation coefﬁcient µ generator corresponding translation clear performing gradient inference µ error function would problematic overcome problem propose search adaptively smooth operator range transformation coefﬁcients speciﬁcally operator averaged range transformation using gaussian weighting coefﬁcient value µ σ z 1 2 5 u 2 1 6 ueµλe 1 2 7 µ σ replaces µ equation error minimized respect µ instead ﬁnding single best value µ minimizes e µ σ allows gaussian distribution µ effectively blurring signal along transformation direction given case translation instance averaging alternatively seen introducing additional transformation operator one smoothing operator asmooth 1 coefﬁcient µsmooth asmooth smooth along transformation direction given 6 range transformation blur image along direction translation higher value σ larger blur blurring speciﬁc operator corresponded rotation one quadrant image patch averaging would lead rotational blur single quadrant simultaneous inference µ σ image matched ﬁrst coarse scale match reﬁnes blurring image decrease approach spirit arathorn circuit arathorn 2002 also spirit transformation speciﬁc blurring kernel presented mobahi et 2012 mobahi et 2012 differs ﬁx blurring coefﬁcient σ rather inferring smoothing speciﬁc afﬁne transformation homography applies ﬁrst order differential ators smooth full objective function rather transformation kernel illustrate way proposed transformation lead better inference dotted line figure 1 show reconstruction error function µ different value note allowing σ vary steepest descent path open local minimum detouring coarser scale multiple transformation single transformation inadequate describe change observed visual world model presented extended multiple transformation concatenating transformation following way tmulti µ σ k tk µk σk 8 tk µk σk ukeµkλke 1 2 k 9 k index transformation note transformation tk µk σk not general commute thus ordering term product must maintained ﬁxed ordering transformation due lack ity multiple transformation case no longer constitutes lie group choice transformation generator describing group structure new model goal future work present note many obvious video tions afﬁne transformation brightness scaling contrast scaling fully 7 0 1 0 µ objective function value reconstruction error function µ different value figure 1 local minimum error function landscape escaped increasing smoothing coefﬁcient plot show reconstruction error equation 3 function transformation coefﬁcient µ several value smoothing coefﬁcient case target pattern x ha translated one dimension relative initial white noise pattern x operator translation operator captured model form equation 8 though choice coefﬁcient value µk transformation may depend order term product regularization via manifold distance order encourage learned operator act independently learn transform patch direct way possible penalize distance transformation move image pixel space since penalty consists sum distance traversed operator act similarly penalty sparse coding encourages travel two point occur via path described single transformation rather longer path described multiple transformation distance traversed transformation expressed dmulti x k dk µk yk 0 10 yk 0 q k tm µm σm x 0 image patch prior application 8 formation assuming euclidean metric neglecting effect adaptive ring distance dk µk yk 0 traversed single transformation chain dk µk yk 0 z µk 0 yk τ 11 z µk 0 τ 12 z µk 0 0 13 finding closed form solution integral difﬁcult mated using linearization around τ µk 2 dk µk yk 0 µk 2 yk 0 2 14 complete model putting together component full model objective function e µ σ u λ x ηn x x σ x 2 2 ηd x x k µ k µ k 2 akx k 2 ησ x x k σ k 2 15 small regularization term σ k included wa found speed convergence learning used ηn 1 ηd ησ inference learning ﬁnd u λ use map approximation expectation step iterates following two step set video patch x ﬁnd optimal estimate latent variable ˆ µ ˆ σ pair frame holding estimate model parameter ˆ u ˆ λ ﬁxed ˆ µ ˆ σ argmin µ σ e µ σ ˆ u ˆ λ x 16 9 optimize estimated model parameter ˆ u ˆ λ holding latent ables ﬁxed ˆ u ˆ λ argmin u λ e ˆ µ ˆ σ u λ x 17 optimization wa performed using implementation minfunc schmidt 2009 note degeneracy uk due fact column ing eigenvectors ak rescaled arbitrarily ak remain unchanged inverse scaling occur row k not dealt uk k random walk column row length many learning step one ill conditioned described detail appendix effect compensated rescaling column uk identical power corresponding row k similarly degeneracy relative scaling µ order prevent ˆ µ ˆ λ random walking large value every iteration ˆ µk multiplied scalar αk ˆ λk divided αk αk chosen ˆ µk ha unit variance training set 3 experimental result inference afﬁne transforms verify correctness proposed inference algorithm test dataset containing set known transformation wa constructed applying afﬁne transformation natural image patch transformation coefﬁcients inferred using set operator matching used construct dataset purpose pool 1000 natural image patch cropped set short video clip bbc animal world series image patch wa transformed full set afﬁne transformation simultaneously transformation coefﬁcients drawn uniformly range listed 2 skew left since constructed using combination afﬁne operator 10 transformation type range horizontal translation 5 pixel vertical translation 5 pixel rotation 180 degree horizontal scaling 50 vertical scaling 50 horizontal skew 50 eigenvector matrix uk initialized isotropic unit norm zero mean gaussian noise followed transformation described appendix value λk initialized λkjj nii nr λkjj jth nal entry eigenvalue matrix λk ni nr unit norm zero mean sian finally sample µk σk initialized µk n 0 σk 0 proposed inference algorithm equation 16 used recover mation coefﬁcients figure 2 show fraction recovered coefﬁcients differed le 1 true coefﬁcients distribution psnr reconstruction also shown inference algorithm recovers coefﬁcients high degree accuracy psnr reconstructed image patch wa higher 85 transformed image patch addition found adaptive blurring signiﬁcantly improved inference evident figure learning afﬁne transformation demonstrate ability learn transformation trained algorithm image sequence transformed single afﬁne transformation operator translation rotation scaling skew training data used single image patch bbc clip section transformed open source matlab package shen 2008 transformation range used section initialization wa section afﬁne transformation operator spatial derivative operator direction motion example horizontal translation operator derivative operator horizontal direction rotation operator computes derivative circularly 11 hor trans vert trans rotate hor scale vert scale hor skew blur no blur fraction μ correctly recovered 1 error blur no blur blur no blur blur no blur blur no blur blur no blur b 250 200 150 100 50 00 20 40 60 80 histogram count psnr db reconstruction psnr figure 2 performance inferring transformation coefﬁcients fraction recovered coefﬁcients differed le 1 true coefﬁcient value image patch transformed using set hand coded afﬁne transformation transformation simultaneously recovery wa performed via gradient descent equation inference blur without no blur adaptive smoothing compared b distribution psnr value image patch reconstructed ing coefﬁcients inferred adaptive smoothing majority transformed image patch reconstructed high psnr 12 learned operator exhibit property figure 3 show two learned tion operator block corresponds one column block position ﬁgure corresponds pixel location original image patch viewed array basis function one showing intensity given pixel location inﬂuences instantaneous change pixel intensity pixel location see equation 1 ﬁgure basis ha become spatial differentiator tom two row figure 3 show operator applied image patch animation full set learned afﬁne operator applied image patch found supplemental material learning transformation natural movie explore transformation statistic natural image trained algorithm pair 17 17 pixel image patch cropped consecutive frame video dataset section order allow learned transformation capture image feature moving patch surround allow direct comparison motion compensation algorithm error function inference learning wa only applied central 9 9 region 17 17 patch patch therefore viewed 9 9 patch surrounded 4 pixel wide buffer region 15 transformation case 2 pixel wide buffer region wa used computational reason 15 transformation case act 13 13 pixel patch reconstruction penalty applied central region initialization wa section training wa performed variety model different number mations several model two operator patch horizontal vertical translation wa done since expect translation predominant mode transformation natural video allows algorithm focus learning le obvious transformation contained video remaining operator belief supported observation several operator converge full ﬁeld translation learning unconstrained illustrated operator figure 15 transformation case prespecifying translation also 13 horizontal translation b rotation c figure 3 learned transformation operator corresponds horizontal lation rotation b 11 11 block corresponds one column block position ﬁgure corresponds pixel location original image patch block therefore show intensity one pixel location contributes instantaneous change intensity pixel location note block correspond spatial derivative direction motion panel c show translation rotation operator respectively applied image patch 14 b c figure 4 sample transformation operator set 15 transformation trained unsupervised fashion pixel patch including 2 pixel wide buffer region natural video block corresponds one column block position ﬁgure corresponds pixel location original image patch block therefore illustrates inﬂuence single pixel ha entire image patch transformation applied full ﬁeld translation operator b full ﬁeld intensity scaling c full ﬁeld contrast scaling unknown difﬁcult interpret 15 15 17 19 21 23 25 27 29 31 33 average psnr different model 15 learned cont trans 4 learned cont trans 3 learned cont trans 2 learned cont trans 1 learned cont trans cont trans smoothing quarter pixel trans full pixel trans no trans model psnr db figure 5 complex model allow accurate representation difference bar show psnr reconstruction second frame via mation ﬁrst frame averaged pair frame natural video variety model conﬁgurations provides useful basis comparing existing motion compensation algorithm used video compression based translation model case greatest variety transformation operator consisted 15 unconstrained transformation selection learned ak shown figure learned transformation operator performed full ﬁeld translation intensity scaling contrast scaling spatially localized mixture preceding 3 transformation type number transformation no clear interpretation demonstrate effectiveness learned transformation capturing interframe change natural video psnr image reconstruction pair 17 17 image patch extracted consecutive frame wa compared learned transformation model well standard motion compensation reconstruction model compared follows no transformation frame x compared frame x without any mation full pixel motion compensation central 9 9 region x compared 16 best matching 9 9 region x full pixel resolution quarter pixel motion compensation bilinear interpolation central 9 9 region x compared best matching region x quarter pixel resolution continuous translation without smoothing only vertical horizontal tion operator used model allowed perform subpixel translation continuous translation vertical horizontal translation operator used model addition adaptive smoothing used continuous translation plus learned operator additional transformation tor randomly initialized learned unsupervised fashion 7 15 learned transformation operator fifteen operator randomly initialized learned unsupervised fashion no operator hard coded tion shown figure 5 steady increase psnr transformation el become complex suggests operator added model learning transformation matched progressively complex change occurring natural movie note also continuous translation adaptive smoothing yield substantial improvement quarter pixel tion suggests addition smoothing operator could useful even employed standard motion compensation model 4 discussion described tested method learning lie group operator high mensional time series data speciﬁcally image sequence extracted natural movie build previous work using lie group operator represent image tions making four key contribution 1 lie group operator allows computationally tractable learning 2 adaptive ing operator reduces local minimum inference 3 mechanism combining 17 multiple transformation operator 4 method regularizing coefﬁcients manifold distance traversed result obtained inferring transformation coefﬁcients learning operator natural movie demonstrate effectiveness method representing transformation complex dimensional data contrast traditional video coding us motion pensation algorithm method attempt learn optimal representation image transformation statistic image sequence resulting model er additional transformation beyond simple translation code natural video ing substantial improvement frame prediction reconstruction tions learned natural video include intensity scaling contrast scaling spatially localized afﬁne transformation well full ﬁeld translation improved coding difference natural video point tential utility algorithm video compression purpose additional cost encoding transformation coefﬁcients µ σ need accounted weighed gain psnr predicted frame tradeoff encoding reconstruction cost explored analysis performed separate paper wang et 2011 beyond video coding lie group method could also used represent plex motion natural movie purpose neurophysiological psychophysical experiment example neural response could correlated value inferred coefﬁcients transformation operator alternatively synthetic video sequence could generated using learned transformation sensitivity neural response observer could measured respect change formation coefﬁcients way learned transformation operator could provide natural set feature probing representation transformation brain another extension method would learn transformation latent variable representation image rather directly pixel example one might ﬁrst learn dictionary describe image using sparse coding olshausen field 1996 model transformation among sparse coefﬁcients response natural movie alternatively one might infer model image 18 learn operator underlying transformation object ment following approach bregler malik 1998 reference arathorn circuit visual cognition computational mechanism biological machine vision black jepson eigentracking robust matching tracking ticulated object using representation proc fourth european conference computer vision page christoph bregler jitendra malik tracking people twist exponential map computer vision pattern recognition proceeding 1998 ieee computer society conference page ieee taco cohen max welling learning irreducible representation commutative lie group international conference machine learning culpepper olshausen learning transport operator image manifold proceeding neural information processing system 22 kokiopoulou frossard minimum distance pattern transformation manifold algorithm application ieee transaction pattern analysis machine intelligence 31 7 lewicki olshausen probabilistic framework adaptation comparison image code journal optical society america 16 7 1601 lucas kanade iterative image registration technique application stereo vision proceeding imaging understanding workshop page r memisevic g hinton learning represent spatial transformation factored boltzmann machine neural computation jan 19 url miao rao learning lie group visual invariance neural tation 19 10 h mobahi cl zitnick seeing blur computer vision tern recognition url ba olshausen david j field emergence receptive ﬁeld property learning sparse code natural image ture url olshausen cadieu culpepper warland bilinear model natural image spie proceeding vol 6492 human vision electronic imaging xii rogowitz pappa daly ed ortiz radovitzky repetto computation exponential logarithmic mapping ﬁrst second linearizations international journal numerical method engineering rao ruderman learning lie group invariant visual perception advance neural information processing system 11 page mark schmidt minfunc technical report url shen resampling volume image afﬁne matrix evan c smith michael lewicki efﬁcient auditory coding nature 439 7079 2006 20 j h van hateren van der schaaf independent component filter natural age compared simple cell primary visual cortex proceeding biological science 265 1394 march vasconcelos lippman multiresolution tangent distance afﬁne invariant classiﬁcation proceeding neural information processing system 10 wang tosic olshausen lie group transformation model predictive video coding data compression conference page wiegand sullivan bjontegaard luthra overview video coding standard ieee transaction circuit system video nology 13 7 appendix degeneracy u decompose transformation generator v λv λ diagonal introduce another diagonal matrix diagonal r populated any complex number following equation still hold v λv v v v r λ v r set u v r uλu 21 r represents degeneracy remove degeneracy u choosing r minimize joint power every learning step r argmin r x x j v 2 jj x x j jj ji argmin r x x j v 2 jj x x j ji 1 jj setting derivative 0 2 x v 2 ijrjj x ji 1 jj 0 rjj x v 2 ij 1 jj x ji jj p v 2 ji p v 2 ij rjj p v 2 ji p v 2 ij 1 4 practically mean every learning step set rjj p u 2 ji p u 2 ij 1 4 set unew ur b appendix derivative let ε x n yn 1 2 2 derivative inference one operator learning gradient respect µ x n n u e 1 2 x n n uλeµλe 1 2 22 err n reconstruction error nth sample err n yn 1 2 similarly learning gradient respect σ x n n 1 2 derivative learning one operator learning gradient respect λ x n n u h eµλe 1 2 u x n n u µeµλ 1 2 u learning gradient respect u x n n eµλe 1 2 x n n ueµλe 1 2 xn recall du u du u learning gradient therefore x n n eµλe 1 2 x n n ueµλe 1 2 derivative complex variable accommodate complex variable u λ rewrite objective function ε x n err n terr n 23 err n denotes complex conjugate derivative error function respect any complex variable broken derivative respect real imaginary part result λ x n err n n λ x n err n n similarly u derivative manifold penalty model ai solution want ﬁnd minimize distance traveled image patch transformation operator total distance z give following z z z p dt z q 0 eat know solve analytically instead make approximation z derivative b b 0 ea 2 tataea 24