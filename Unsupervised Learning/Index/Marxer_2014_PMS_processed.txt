1 unsupervised incremental learning prediction music signal ricard marxer hendrik purwins system presented segment cluster predicts musical audio unsupervised manner adjusting number timbre cluster instantaneously audio input sequence learning algorithm adapts structure dynamically changing clustering tree ﬂow system follows 1 segmentation onset detection 2 timbre representation segment mel frequency cepstrum coefﬁcients 3 discretization incremental clustering yielding tree different sound class instrument grow shrink ﬂy driven instantaneous sound event resulting discrete symbol sequence 4 extraction statistical regularity symbol sequence using hierarchical newly introduced conceptual boltzmann machine 5 prediction next sound event sequence system robustness assessed respect complexity noisiness signal clustering isolation yield adjusted rand index ari data set singing voice drum onset detection jointly clustering achieve ari prediction entire system yield ari index information retrieval unsupervised ing adaptive algorithm prediction algorithm introduction human music listening adapts novel acoustic stimulus largely based unsupervised learning contrast traditional music analysis system music transcription 9 prediction 8 36 representation 22 32 automatic niment 2 34 traditional system usually based either symbolic data instead audio input classiﬁers labelled data base 9 system based classiﬁers need cope new musical concept instrument harmony pitch motif ha not designed may cease work reasonably system would retrained labeled data every time new instrument pitch harmony etc appears present severe lack ﬂexibility system contrast human cognition processing new instrument harmony ease even one ha not heard human mind grasp novel motif listening piece improvisation unsupervised marxer speech hearing group department computer ence university shefﬁeld regent court 211 portobello street shefﬁeld uk audio analysis lab sound music ing group aalborg universitet københavn meyers vænge 15 2450 copenhagen sv hpu marxer purwins music technology group universitat pompeu fabra roc boronat 138 08018 barcelona spain purwins wa also neurotechnology group berlin institute technology sekr mar marchstr 23 10587 berlin germany work wa partially funded emcap european commission contract 013123 european commission project thanks daniel bartz berlin institute technology advice statistic proof reading learning clustering instead supervised classiﬁcation one paradigm algorithm model cognition novel concept 17 24 26 based discrete representation input derived clustering sufﬁx tree used statistical representation structure input sequence 8 36 paper extend system equipping capability deal varying number cluster number cluster increase new instrument appears cluster number decrease two instrument become sound similar implement feature using unsupervised online learning requires sufﬁx tree must coupled clustering order able merge split symbol count cluster number change introduce system prototype learns unsupervised adaptive manner generates prediction audio sequence ﬁrst note begin generate reasonable prediction without using previous knowledge many previous approach predicting musical sequence based symbolic representation 2 8 22 32 34 36 paiement et al 35 present model capable predicting generating melody using combination bayesian network clustering rhythmic special representation melody distance rhythmical pattern clustered continuation melody predicted conditioned chord root chord type narmour group recent melodic note hazan et al 17 build system generation musical expectation operates music audio data format auditory segment musical stream extract timbre timing description initial bootstrap phase unsupervised clustering process build maintains set different sound class resulting sequence symbol processed technique based model selection performed bootstrap phase via akaike information criterion marchini purwins 24 present system learns rhythmic pattern drum audio recording synthesizes music variation learned sequence procedure us fuzzy level representation moreover tempo estimation procedure used guarantee metrical structure preserved generated sequence online clustering ha proposed zhang et al 46 document clustering et al 5 used online cluster pattern hierarchical dirichlet process hidden markov model 43 ha used segmentation conjunction clustering fox et al 13 ren et al 40 proposed sticky version troduce explicit modelling state occupancy duration 23 oct 2015 2 model applied segmentation beethoven sonata musical section 40 speaker diarization 13 stepleton et al 42 used block diagonal inﬁnite hidden markov model musical theme labelling however method not perform incremental online learning whereas propose online incremental clustering method us separate segmentation method onset detection switch relatively rapidly state bargi et al 3 adapted hmm online setting employing initial supervised learning phase bootstrap whereas approach entirely unsupervised part work covered paper application hierarchical voice data ha presented previously 26 compare method conceptual boltzmann machine extended data set using advanced evaluation measure adjusted rand index providing amples adaptive clustering give overview system introduce component namely segmentation timbre representation clustering prediction introduce adjusted rand index test performance sequence analysis algorithm noisy condition system module separately conjunction finally give some demonstration example data example available supporting website 27 ii system overview system present paper cf fig 1 consists four main stage segmentation onset detection ture extraction resulting timbre representation incremental clustering giving symbol sequence sequence analysis yielding prediction next symbol particular clustering tree generated incremental clustering grows shrink online driven recent sound turn sequence model adapts changing number symbol segmentation representation interpreted model perception whereas discretization prediction considered cognitive model segmentation audio feature extraction incremental clustering sequence analysis next symbol ioi segment timbre representation symbol sequence clustering tree structural adaptation fig system architecture audio sound ﬁle segmented using onset detection segment represented timbre feature vector clustered symbol symbol added removed clustering tree ﬂy symbol sequence statistically analysed adapting varying number symbol allowing prediction next symbol next interval ioi segmentation onset detection section explain segment audio stream event using onset detection order generally applicable employed complex domain based onset detector 10 since subsumes onset detection algorithm based energy spectral difference phase special case onset detection function capture onset due abrupt energy change well soft onset induced pitch change little energy variation frame l fourier transform yield complex spectrum xk l rk l eiφk l magnitude rk phase φk bin frame length k 0 build onset detection function euclidean distance actual complex spectrum xk l bin k estimated complex spectrum 10 ˆ xk l ˆ rk l ei ˆ φk l 1 estimated amplitude ˆ rk l set equal nitude previous frame l estimated phase ˆ φk l calculated linear extrapolation unwrapped phase two preceding frame ˆ φk l princarg ϕk l ϕk l ϕk l ϕ denotes unwrapped phase princarg operator map unwrapped value back π range calculate euclidean distance actual estimated complex spectrum quantifying stationarity bin l l xk l summing across k bin across 1 consecutive frame centered around frame l smoothing yield onset detection function η l 1 2 x 2 x l j 2 similarly previous approach 4 adaptive threshold θ l used threshold calculated scaled median across window length p 1 θ p c p η n 3 0 predeﬁned parameter controlling sensitivity onset detector order eliminate multiple occurrence onset shortly one another smoothing applied via another window length w centered sample l µ l 2 x 2 max η l l 0 4 silence threshold θs applied µs l max µ l 0 5 finally local maximum µs l deﬁne predicted onset time feature extraction timbre representation onset short window length l subsequent onset time analyzed frame within window ﬁrst 13 cepstrum coefﬁcients mfcc 31 calculated model coefﬁcient temporal behaviour right onset coefﬁcient another discrete cosine transform dct calculated sequence 3 coefﬁcients across frame taking ﬁrst 4 dct efﬁcients mfcc yield vector representing timbral feature sound event spectral characteristic initial temporal development incremental clustering symbol sequence generation clustering stage receives multivariate feature vector preprocessing stage convert symbol important state system event clustered online manner order arrival since symbolic representation used immediately create prediction future event reference benchmark compare online clustering cobweb art batch clustering method exploiting sequential information 1 cobweb purpose marxer et al 28 used cobweb 12 cobweb incremental clustering model continuously build knowledge tree hierarchical partitioning object space assigns instance partition created level object reach leaf tree node tree represents concept concept modelled univariate gaussian feature dimension edge structure represent taxonomic relation work 29 45 proposed technique create unsupervised manner concept tree based sequence data presented use heuristic function maximized heuristic function used paper numerical version standard category utility function used fisher introduced gluck corter 16 version cobweb use wa presented 29 later extended 45 algorithm cluster feature vector x xd extracted previous section consider particular cluster containing feature vector let σd standard deviation component input feature vector assigned cluster pd 1 σd speciﬁcity cluster across feature dimension consider utility u quantify gain speciﬁcity splitting cluster k child cluster potential child cluster 1 ik instance input feature dimension deﬁne σdk inner cluster standard deviation dimension pd 1 σdk speciﬁcity cluster k pk ik pd 1 σdk speciﬁcity child cluster altogether cluster utility hold u k k x ik x 1 max σdk x 1 σd 6 acuity parameter upper limit maximal speciﬁcity minimal standard deviation cluster thereby controlling maximal resolution clustering nation incorporation object process clustering object descending tree along appropriate path updating count along way possibly performing one several operation level operator creating new node removing child node pruning combining two cluster single node splitting node several node operation applied single object set partition set sibling tree composition primitive operation transform single clustering tree search strategy use space clustering tree thereby input converted sequence not only symbol also meta symbol partition according parent node grandparent node cobweb tree symbol meta symbol provide alphabet expectation generated hierarchical modify set possible cobweb operation see order achieve persistent partitioning reduced set operation perform any cobweb original operation reformulate second cobweb operation see order control clustering only new incoming event partition past event not considered reduces operation creating new partition inside container partition removing partition reparenting child ha any 2 hierarchical dirichlet process hidden markov model feature vector sequence may also elled emission bayesian metric model hidden state considered cluster given observed feature vector sequence likely hidden state sequence interpreted sequence symbol hidden state assumed drawn countably inﬁnite state space used jointly estimate number cluster cluster assignment feature vector transition probability cluster inference hmm performed using weak limit approximation 13 implemented pyhsmm 19 however inference doe not work online manner requires entire feature vector sequence input method ofﬂine batch mode only used reference benchmark since doe not fulﬁl constraint clustering feature vector arrive perform immediate prediction beginning sequence analysis next prediction choose two method hierarchical ceptual boltzmann machine 37 require relatively little storage deducing frequency count longer sequence frequency count shorter subsequence algorithm iteratively predict next symbol onset ioi respectively based previous symbol iois ct previously generated incremental clustering thereby derive sound expect prediction symbol iois performed independently predicting ioi determine onset time symbol 1 4 1 hierarchical hn used analysis genome sequence language modeling 48 exhaustive count instance possible symbol ioi sequence length memory ment exponential sequence length n problem arises account pattern not occurred zero frequency problem use estimate forward conditional distribution online prediction next symbol ioi hierarchical hn 37 need le memory exhaustive hn combination sparse model hierarchical structure allows compositional learning compositional learning consists learning long pattern already learned sparse count frequent pattern separate total count pattern kept technique separate estimate pattern whose statistic reliable estimate infrequent pattern whose statistic biased hand exhaustive approach consists keeping count possible pattern length model able represent any distribution pattern width let set cluster index renumbered reﬂect order ﬁrst ance symbol sequence c ct achieved clustering process previous section form alphabet cn set possible gram length n composed alphabet exploit sparsity only consider pattern actually occurred subsequence c far time set pattern length n occurred far denoted cn subpatterns ordered according ﬁrst appearance c deﬁned position c consider hn maximal length let cn n frequency count pattern length n let tn total count pattern length n since pattern occurred ﬁrst time algorithm 1 use count tn cn iteratively estimate joint probability pn pattern seen far deﬁne 1 simple empirical frequency cn could used estimate probability pattern length hn method eq 12 probability p n pattern length n joint distribution width n estimated pattern ci ci 1 ci n statistical estimate eq pﬂeger p n p ci 1 ci p ci 2 ci n p p ci 2 ci 7 calculated using ith pattern length estimate probability n eq 11 pattern length n ith pattern length n not subpattern ﬁrst pattern length eq 12 weighted conﬁdence conﬁdence value depend number occurrence pattern therefore pattern length n ha appeared rarely data stream probability occurrence estimated small number count not reliable cobweb h b c b c b b b b b c c b b b b b b b b b c b c b c b b b e b b b e e b b b e b e b e b b sequence b b b b c b b b b c b b e b b e b b e merge c e c b e b e fig effect concept merge hierarchical node c merged new symbol inherits count pattern including c pattern including b c b c b c ba ac ba ac bac b c ba ac bac fig illustration continuous composition symbol atom boltzmann machine longer pattern chunk case probability appearance better estimated n length p n word information pattern large length integrated information model small length pﬂeger show probability given pattern calculated linear sweep updating probability order pattern ﬁrst occurrence length order adapt pﬂeger hn 37 architecture link operation clustering model operation fig 2 two cluster merged clustering model remove superﬂuous cluster set cluster index eq 8 sum count merged cluster eq 9 example track pattern bbc bbd suddenly clustering model merges symbol c new symbol e must sum count bbc bbd substitute count bbe 2 conceptual boltzmann machine cb boltzmann machine 1 stochastic neural work used represent joint distribution 5 algorithm 1 hierarchical merged cluster initialization cn 1 incoming event ct 1 ct add new pattern cn cn ct tn 0 cn 0 end ck merged cobweb min ck 8 cn k x cn ck 9 cn ck 10 update index end update count cn ct cn ct 1 update total count tn tn 1 1 end calculate joint probability p 0 1 1 1 1 1 qm n 1 x p n k n n 11 calculate p n according p n n 1 x tn j qn n j p n n j 12 end end end random variable complete pattern particular case predict continuation time series formally boltzmann machine consists vector binary unit si 0 1 symmetric weight wij pair unit si sj update rule unit learning rule weight applied categorical data 38 v symbol cu encoded binary unit suv suv 1 only cu connect two v variable cu ci v 2 weight wij uv needed connect binary unit representing two variable initially architecture particular boltzmann machine implementation consists set binary variable consecutive symbol binary node variable initially only connected binary node previous next symbol depending unit weight stochastic softmax update rule symbol p ci j 1 1 p pv suv wij uv 13 temperature decreasing 50 100 step example gibbs sampling update rule applied iteratively general simulated annealing temperature state converge particular state vector 1 training boltzmann machine weight wij learned case restricted boltzmann machine 41 case not pair si sj connected zero weight wij unit representing symbol not connected among binary previous symbol sequence update rule 13 iteratively applied ﬁnal state reached denoted addition update rule applied no unit ﬁxed another vector ﬁnal state reached stochastic learning step weight performed learning rate µ single training instance yielding j µ j j 14 learning step aim minimizing difference j j µ used 6 weight wij rise threshold θw new hidden unit created representing concatenation symbol connected strong weight cf fig 3 38 addition weight wij removed iteratively hidden unit pattern length n 1 created node representing pattern length n new set binary node representing pattern length n appended set θw respectively depending length pattern unit represents length variant boltzmann machine called categorical boltzmann machine 38 predicting next symbol trained boltzmann machine respective unit ﬁxed previous symbol sequence running unit update rule 13 convergence predicted next symbol sequence retrieved corresponding binary unit boltzmann machine system implemented new method called conceptual boltzmann machine cb pﬂeger mann machine act static set category tended architecture operates dynamically changing taxonomy category therefore model adjusts tree structure generated cobweb mean boltzmann machine change architecture ﬂy guided creation removal splitting merging operation suggested cobweb accordingly boltzmann machine unit update rule must adjusted new structure run sequence atom cause creation chunk represent pattern newly created chunk represent pattern chunked node represent pattern longer length longest pattern represented node ﬁxed value iii performance analysis system measure clustering evaluation unlike supervised learning accuracy sured annotated label label predicted classiﬁer number cluster predicted analysis different number annotated label category addition mapping annotated predicted bel unclear creates need particular clustering evaluation measure following measure evaluating agreement annotation predicted label suggested purity 47 21 pearson chi squared coefﬁcient 44 rand index 44 choose latter measure evaluation since natural extension classifying element pair element partition clustering c set x deﬁned set c cj subset cj x cj disjoint j let p set partition x let number element partition c let partition generated annotation let c predicted partition derived algorithm let p x x set pair distinct event let l set event pair x share provided let k set event pair x lie cluster provided c number point pair lie cluster time share annotated label 1 rand index 44 deﬁned r c 2 15 since r depends number cluster adjust rand index comparing expected value r baseline random clustering er expected value r partition combination p calculated 14 er 1 x r c 16 er get maximal c rmax 1 x r c c 17 adjusted rand index ari hold ari c r c rmax 18 ari ha value 0 random partitioning 1 c ari assumes annotation clustering drawn randomly ﬁxed number cluster ﬁxed number element per cluster 44 although assumption not always true evaluation use ari since established measure alternative one index mirkin metric jaccard index 30 measure 44 47 normalized mutual information variation information evaluating system use ari two way evaluation 1 clustering feature vector event table iv v 2 prediction entire symbol sequence explained sequel according fig 1 segmentation feature extraction clustering input sound wave transformed sequence c ct event one represented one j symbol occurrence symbol j included cluster cj contains index event ct equal symbol c cj partition x 1 2 evaluate prediction c annotate one ground truth label 1 2 segment input yielding annotated sequence partition ai generated way partition c number annotated label not necessarily number symbol j determined clustering stage system ari used compare c done table fig data set two set test data employed repetitive symbol sequence generate sequence consist pattern length nl 2 5 made distinct symbol pattern repeated 20 7 0 5 10 15 20 25 30 35 40 event adjusted rand index cb nl cb nl cb nl cb nl hn nl hn nl hn nl hn nl fig learning rate two sequence learning algorithm cb hn depending number pattern repetition ari eq 18 given increasing number repetition pattern various length nl 2 3 4 5 hn reach perfect ari quickly contrast cb time pattern length nl partition ai 1 2 nl one sequence generated way element partition set ai symbol position sequence nl 5 partition 1 3 5 2 4 symbol 1 occurs position 1 3 5 symbol 2 occurs position 2 4 yielding symbol sequence 1 2 1 2 1 audio recording voice informal low quality short voice recording simpliﬁed beat boxing consisting 3 different sound category different degree tonality simple changing rhythm sequence repetitive pattern ritardando altogether 5 recording duration order demonstrate unsupervised character system choose sound not belong predeﬁned category acoustical instrument enst drum formal high quality automatically annotated recorded drum sequence 5 segment scribed term style complexity tempo disco simple slow complex medium rock simple fast country simple slow complex medium 15 audio recording annotated evaluated audio data available website 27 result system architecture consists processing chain 1 onset detection feature extraction 2 clustering 3 expectation evaluate stage 1 2 3 isolation 1 2 together referred transcription entire chain 1 2 3 together referred prediction use repetitive symbol sequence order ass expectation learning rate noise robustness sequence analysis audio recording used test processing stage entire system separately skip probability adjusted rand index cb nl cb nl cb nl cb nl hn nl hn nl hn nl hn nl fig robustness two sequence learning algorithm cb hn cf fig 4 respect skipping noise ari given sequence 20 repetition pattern different length nl increasing probability psk randomly skipping event psk hn performs better cb 1 learning rate noise robustness repetitive symbol sequence ass learning rate noise robustness sequence analysis stage cb hn sequence learning algorithm receives initial chunk repetitive symbol sequence section input algorithm determines probable expected next symbol algorithm output expected next symbol given annotated symbol prediction tition c generated compared partition corresponding based annotation explained tion annotation far used diction only last 12 annotation used prediction stochastic bm ari averaged 100 run partition given length nl trivial sequence consists constant repetition symbol not considered first ass learning rate scale pattern length number pattern repetition fig 4 show averaged ari across partition length nl 2 test hn set maximum length n hn reach perfect prediction event 2 pattern repetition cb seems converge much slowly hn nl 2 reaching ari higher 8 event increasing much slowly higher nl cb seems converge towards perfect prediction even slowly different type noise used transform sequence order ass robustness sequence learning technique skipping noise original sequence symbol skipped given probability 0 switching noise original sequence given ability 0 symbol selected randomly uniform distribution across nl alternative bols average ari calculated 100 run nl 2 3 50 run nl 4 20 run nl 5 8 switch probability adjusted rand index cb nl cb nl cb nl cb nl hn nl hn nl hn nl hn nl fig robustness cb hn cf fig 4 respect switching noise ari given increasing probability psw randomly switching symbol hn performs better cb psw 0 reaching random guess level psw cb hn fig 5 show prediction performance ari affected skipping symbol deﬁned psk repetitive symbol sequence simulates failure onset extraction algorithm detect event prediction performed given sequence 20 repetition basic pattern hn cb performance degrades psk random guess level reached psk hn appears robust towards skipping noise cb cb worse ari higher nl fig 6 effect clustering error sequence learning process simulated increasing switching ability psw symbol replaced any nl symbol uniform distribution graph show prediction performance using ari cb hn different pattern length result similar skipping noise fig 5 hn robust wrt noise bm reaching random guess level psw summarized relatively small noise hn appears robust skipping switching noise especially longer pattern length 2 testing processing stage audio recording test voice recording section serve proof concept clustering dynamically varying number cluster enst recording used comprehensive quantitative evaluation system test process stage separately audio sample rate f 44100 hz segmentation feature extraction hop size 128 sample window size 1024 sample onset detection evaluation onset tection section employ widely used procedure 9 23 onset time manually annotated subject serve reference onset estimated onset detection algorithm compared manually annotated onset annotated estimated onset considered match difference time smaller given threshold evaluation use onset match threshold since data assumed monophonic evaluation only permit mapping estimated annotated onset using following onset detection parameter smoothing length 33 eq 2 sensitivity c ahead window length p 10 eq 3 threshold window length w 11 eq 4 silence threshold θs eq 5 onset detection yield voice data set therefore focus clustering prediction stage also notice smoothing length 33 system doe not improve signiﬁcantly large smoothing length reduce temporal precision onset important good feature extraction since information event located attack b clustering compare performance incremental online cobweb clustering benchmark ofﬂine batch clustering constant yet inferred cluster number benchmark order ass clustering process isolation assume onset detection previous stage order achieve use annotated onset input order ass stability system tested performing grid search two sensible parameter involved task algorithm cobweb explore analysis window length l section acuity eq 6 parameter grid window pair maximal ari determined extending parameter grid maximum lie grid border empirically set constant grid step size voice cobweb performance peak l parameter grid l 50 75 175 15 enst cobweb form best l parameter grid l 25 50 100 13 15 cf bles iv v supplementary material 27 mean timbre model clustering process successfully classify audio event also notice voice need much longer analysis window enst test explained wa performed using annotated onset result could change onset estimated effect evaluated transcription test section ﬁrst reduce feature vector input dimension mean pca full sequence observation distribution used gaussian parameter sampled normal inverse wishart prior 19 parameter 0 maximum number state weak limit approximation inference set 10 number gibbs sampling iteration voice mance peak γ α 2 parameter grid γ α 2 enst performs best hdp concentration parameter γ α 43 2 parameter grid γ α 2 benchmark performs better voice cobweb whereas enst cobweb performs murphy 33 section 20 meaning parameter 9 table expectation voice left enst right ari different maximum length n row n cb hn 2 3 4 5 6 7 n cb hn 2 3 4 5 6 7 better comparing result one ha keep mind clustering ha learned ofﬂine jointly stable cluster number transition probability exploiting sequential information whereas cobweb ha trained online adaptive cluster number c transcription transcription test evaluates subsystem composed onset detection feature extraction clustering contrast expectation test entire symbol interval sequence ct extracted clustering stage always used beginning predict next symbol interval annotation not used prediction only evaluation partition generated detected event compared partition generated annotated label using ari online learning cobweb dynamically changing clustering number ofﬂine learning constant cluster number compared cobweb voice performs ari l 150 enst data set cobweb yield ari l 50 using parameter grid clustering comparison result clustering ari degrades bit particular enst due wrongly estimated set cf table vi vii supplementary material 27 transcription performance voice peak γ α parameter grid γ α enst transcription performance peak γ α parameter grid voice enst coweb almost equal although voice ari much higher benchmark cobweb keep mind cobweb learns online changing cluster number time whereas trained ofﬂine constant number cluster expectation expectation test evaluates formance sequence learning module data set predict cluster label event 1 based annotation start result table show prediction sequence voice enst data set hn ari n 4 work lot better cb yield ari slightly better random 0 cb low performance attributed various factor general many traditional recurrent network known slow learning rate 18 particular observed slow learning rate fig 4 low noise robustness fig 5 fig 6 whereas hn update frequency count getting smaller relative count far 1 2 higher step relative 1 100 101 relative 100 cb weight table ii full prediction voice data set using hn ari different temporal acuity eq 6 row timbral acuity eq 6 column 17 18 19 20 table iii full prediction enst data set using hn ari different temporal acuity eq 6 row timbral acuity eq 6 column 18 19 20 21 updated constant learning rate µ eq 14 weight update performed stochastic gradient descent instance only used ha occurred although cognitively plausible assume only limited number instance stored cognitive system come price diminished learning speed compared system update performed using batch instance addition architecture cb may suboptimal hidden node also network new hidden node only generated one time limiting learning speed furthermore parameter θk creating new hidden node chosen heuristically may suboptimal short sequence like one presented also see maximum length n 5 voice n 3 enst result doe not improve linguistic data slower convergence worse performance cb relative hn also observed pﬂeger 37 pp 80 133 sequel only use hn e prediction prediction task consists running full system including hn sequence analyzer table ii iii transcription event ct system predicts next symbol timing next ioi evaluating match predicted annotated onset set tolerance threshold 150 best conﬁguration full prediction yield ari voice ari enst performance limited weakest performance component case sequence analysis example section present example audio website 27 transcription prediction using hn order demonstrate performance evolution shortcoming system hazan et al 17 adopt procedure optimally map annotated symbol cluster found clustering algorithm calculate matching matrix annotation score cluster event matching matrix iteratively yield maximal 10 entry thereby establishing connection row notation column cluster eliminating row column maximal entry determine maximal entry matrix vanishes 0 2 4 6 8 10 12 time ta tschi bum annotation full prediction bumtatschi fig system hn quickly capture simple pattern time horizontal axis mapped versus event label one line ta tschi bum annotated label indicated black line horizontal line ﬁnd event correctly estimated matched wrong cluster unmatched due wrongly estimated onset fig 7 8 display sequence annotation cluster line linked mapping fig 7 simple pattern quickly captured see ﬁrst three event annotated ta tschi bum matched wrong cluster bum initial blue triangle top line ta tschi red square ﬁrst three cluster mismatch expected since system ha no previous knowledge symbol space sequence therefore not predict symbol pattern not yet occurred around event annotated tschi matched ta cluster timing last bum misestimated last tschi timing cluster matching wrong time deviation error due fact recorded voice doe not follow temporally regular pattern fig 8 observe system adapts pattern change within sequence ﬁrst two event ter matching wrong processed enough sound system performs correct prediction middle around repetition pattern ta introduced three ta event onset wrongly estimated two event well mismatched wrong cluster one additional event only mismatched wrong cluster error middle sequence due pattern change able update statistic 0 5 10 15 time ta bong annotation full prediction bongtabongtata fig system hn adapts pattern change cf fig 7 pca component pca component 10 event 2 1 20 event 2 1 38 event root 1 2 10 event 1 2 20 event root 38 event fig cluster merging 38 event two cluster merge one cluster projection mfcc vector timbre tion onto ﬁrst two principal component incremental clustering tree shown pca component pca component 8 event root 20 event 1 2 80 event 1 root 8 event 1 2 20 event 1 80 event fig creation new cluster 20 80 sound event new cluster emerge ﬂy cf fig perform correct prediction three occurrence new pattern fig 9 sound video website 27 sequence alternating bass drum sample played sequence gradually mixed linear fashion increasing amount bass drum vice versa end bass drum mixed together balanced way yielding repetitive sequence similar sound system recognizes two sound cluster beginning ﬁnally merges two cluster one single cluster fig 10 sound video website 27 sequence sound event analyzed start one sound later joined second third sound system able split initial cluster gradually 2 3 cluster iv conclusion perspective presented full system predicts next sound event previous event operating audio 11 data taking account no previous knowledge neither used sound instrument timing rhythmical structure audio segment system start tabula rasa performing prediction ﬁrst sound event system adapts pattern change sequence well appearance new sound struments any time currently system limited lack metrical analysis making especially sensitive missed onset considering metrical context could signiﬁcantly improve quality prediction goal metrical alignment procedure 24 25 could combined incremental learning alternative cb hn variable length markov model 6 24 25 deep learning architecture used thereby overcoming context length limitation hn cb slow learning cb long memory lstm 18 recurrent neural network developed capture dependency disconnected distant chunk within time series crucial speeding learning lstm special memory cell used access latter opened closed special gating unit successfully applied protein homology detection automatic composition 11 handwriting spoken language recognition lstm could used replace cb hn improve learning speed application 43 could adapted online learning 3 incremental cluster comprising also segmentation thereby replacing onset detection presented system also modiﬁed learn melody chord progression learning melody feature extraction stage section mfccs need replaced pitch detection method used marxer et al 28 learning song mbenzele pygmy cherla et al 7 learning guitar riff analysing piano chord progression mfccs replaced constant q proﬁles 20 39 future work includes development better representation pitch harmony using larger training set processing complex music inspired idea imagine musical tional dialogue human machine human may spontaneously articulate novel idea new sound motif rhythm harmony dumb ignorant machine would dampen ﬁnally stop musical ﬂow machine could take novel idea reply varying suggestion human partner could develop enhanced musical conversation reference 1 david h ackley geoffrey e hinton terrence j jnowski learning algorithm boltzmann machine cognitive science 9 1 1985 2 erard assayag shlomo dubnov using factor oracle machine improvisation soft computing 8 9 2004 3 bargi xu piccardi online hmm joint action segmentation classiﬁcation motion capture data computer vision pattern recognition workshop ieee computer society conf page june 2012 4 juan pablo bello mark sandler note onset detection music signal proc ieee int conf acoustic speech signal processing 5 2 2003 5 thierry ron j wei daniel pw elli clustering pattern large music database proc int society music information trieval page 2010 6 peter buhlmann abraham wyner variable length markov chain annals statistic 1999 7 srikanth cherla hendrik purwins marco marchini automatic phrase continuation guitar bass guitar melody computer music journal 37 3 2013 8 darrell conklin ian h witten multiple viewpoint system music prediction journal new music research 24 1 1995 9 stephen downie kris west andreas ehmann emmanuel vincent 2005 music information retrieval evaluation exchange mirex 2005 nary overview proc int society music information retrieval page 2005 10 duxbury bello davy sandler plex domain onset detection musical signal proc digital audio effect workshop london uk 2003 11 douglas eck urgen schmidhuber finding temporal structure music blue improvisation lstm current network ieee workshop neural network signal processing page ieee 2002 12 douglas fisher knowledge acquisition via tal conceptual clustering machine learning 2 2 172 1987 13 emily b fox erik b sudderth michael jordan alan willsky sticky application speaker diarization annals applied statistic page 2011 14 ana ln fred anil k jain combining multiple terings using evidence accumulation pattern analysis machine intelligence ieee transaction 27 6 2005 15 gillet richard automatic transcription drum loop proc ieee int conf acoustic speech signal processing 4 2004 16 gluck corter information uncertainty utility category proc annual conf cognitive science society page 1985 17 hazan marxer brossier purwins herrera serra causal expectation modelling applied audio signal connection science 143 2009 18 sepp hochreiter urgen schmidhuber long term memory neural computation 9 8 1997 19 matthew johnson alan willsky bayesian nonparametric hidden model journal machine learning research february 12 2013 20 katerina kosta marco marchini hendrik purwins unsupervised generation audio example ismir page 2012 21 larsen aone fast effective text mining using document clustering proc acm sigkdd int conf knowledge discovery data mining page 1999 22 olivier lartillot shlomo dubnov erard assayag gill bejerano automatic modeling musical style proc int computer music conference 2001 23 pierre leveau laurent daudet methodology tool evaluation automatic onset detection rithms music proc int society music information retrieval page 2004 24 marco marchini hendrik purwins unsupervised generation percussion sound sequence sound example sound music computing conference 2010 25 marco marchini hendrik purwins unsupervised analysis generation audio percussion sequence exploring music content page springer 2011 26 ricard marxer hendrik purwins unsupervised incremental learning prediction audio signal proc int symposium music acoustic 2010 27 ricard marxer hendrik purwins unsupervised incremental learning prediction audio signal supplementary material december url http 28 ricard marxer piotr holonowicz hendrik purwins amaury hazan dynamical hierarchical harmonic motivic pitch category music brain cognition workshop held neural tion processing conference 2007 29 mckusick thompson cobweb 3 portable implementation technical report no 1990 30 marina meila comparing clustering information based distance journal multivariate analysis 98 5 873 895 2007 31 mermelstein distance measure speech tion psychological instrumental pattern nition artiﬁcial intelligence page demic new york 1976 32 mozer neural network music composition prediction exploring beneﬁts psychophysical straints multiscale processing connection science 1994 33 kevin murphy conjugate bayesian analysis gaussian distribution technical report university british columbia 2007 34 francois pachet continuator musical interaction style journal new music research 32 3 341 2003 35 paiement grandvalet bengio predictive model music connection science 21 2 2009 36 marcus pearce geraint wiggins improved method statistical modelling monophonic music journal new music research 33 4 2004 37 karl pﬂeger learning predictive sitional hierarchy phd thesis stanford university 2002 38 karl pﬂeger learning predictive compositional hierarchy hebbian chunking technical report proceeding workshop new research problem machine learning 2002 39 purwins blankertz obermayer new method tracking modulation tonal music audio data format int joint conf neural network ijcnn 00 volume 6 page 2000 40 lu ren david b dunson lawrence carin dynamic hierarchical dirichlet process proc int conf machine learning page acm 2008 41 paul smolensky information processing dynamical system foundation harmony theory land rumelhart editor parallel distributed ing volume 1 page mit press cambridge 1986 42 thomas stepleton zoubin ghahramani geoffrey j gordon tai lee block diagonal inﬁnite den markov model int conf artiﬁcial intelligence statistic page 2009 43 yee whye teh michael jordan hierarchical bayesian nonparametric model application nal american statistical association 1 2010 44 silke wagner dorothea wagner comparing terings overview karlsruhe ur informatik karlsruhe 2007 45 jungsoon yoo sung yoo concept formation numeric domain proc acm annual conf computer science page 1995 46 jian zhang zoubin ghahramani yiming yang probabilistic model online document clustering application novelty detection advance neural information processing system 17 page mit press cambridge 2005 47 zhao g karypis hierarchical clustering rithms document datasets data mining edge discovery 10 2 2005 48 imed zitouni olivier siohan jeff kuo lee backoff hierarchical class language modelling automatic speech recognition system proc interspeech 2002 13 supplement result grid search parameter table iv cobweb clustering voice data ari different timbral acuity eq 6 row analysis window length l section column 15 16 17 18 19 50 75 100 125 150 175 table v cobweb clustering enst data ari different timbral acuity eq 6 row analysis window length column 13 14 15 25 50 75 100 table vi onset cobweb transcription ari acuity eq 6 timbre clustering row versus analysis window length l column measured voice data set 15 16 17 18 19 50 75 100 125 150 175 table vii onset cobweb transcription ari acuity eq 6 timbre clustering row versus analysis window length l column measured enst data set 13 14 15 25 50 75 100