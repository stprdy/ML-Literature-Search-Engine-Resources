8 nov 2007 pattern recognition random tree associated functionality family protein ana georgina flesia ricardo fraiman florencia leonardi abstract paper address problem identifying protein functionality using information contained aminoacid sequence propose method deﬁne sequence similarity relationship used input classiﬁcation clustering via well known metric based statistical method obtain measure sequence similarity ﬁrst ﬁt variable length markov model sequence database generating estimated context tree compute bffs distance tree space pair tree bffs distance take account structure tree directly related relevant motif sequence indirectly probability occurrence motif approach motivated idea protein functionality could modelled vlmc estimated context tree observation random variable close together tree space example speciﬁcally address two problem supervised unsupervised learning structural genomics via simple metric based technique space tree 1 unsupervised detection functionality family via k mean clustering space tree 2 classiﬁcation new protein known family via k nearest neighbor tree found evidence similarity measure induced approach concentrate information nation classiﬁcation ha high performance others vlmc approach clustering harder task though approach clustering alignment free automatic may lead many interesting variation choosing clustering classiﬁcation procedure based similarity information one performs clustering using ﬂow simulation see yona et al 2000 enright et al 2003 introduction central problem functional genomics determine function protein using only information contained aminoacid chain karp 2002 well known protein functionality family formed protein perform function diﬀerent organism protein come organism derived genetic duplication rearrangement dayhoﬀ 1976 hegyi 1999 well characterized protein within family may help enhance process classiﬁcation family member whose function not well known not well understood eisenberg 2000 also feature characterizing functionality family may give information common evolutionary history sasson et al 2003 used method proposing hypothesis protein functionality based sequence alignment smith 1981 exact sequence alignment ha quadratic computational complexity make unfeasible large database heuristic method like blast altschul et al 1997 fasta pearson et al 2000 common choice comparing sequence large data set recently problem ha addressed also non alignment method look family model parameter characteristic determine functionality example body work ﬁtting diﬀerent markovian model like hidden markov model rabiner et al 1986 hidden markov transducer variable length markov chain bejerano 2001 apostolico et al 2000 hidden markov model powerful tool task disadvantage key word phrase random tree protein functionality agf wa partially supported pict agf corresponding author fgl supported fapesp grant would like thank pablo ferrari antonio galves many interesting discussion metric space tree modelling protein functionality 1 2 ana georgina flesia ricardo fraiman florencia leonardi many parameter ﬁt even though practice not guarantee optimal choice model recently bejerano et al 2001 proposed apply variable length markov chain problem classiﬁcation protein family some advantage model following doe not depend alignment ha not many parameter hmm algorithm ﬁt model linear time apostolico et al 2000 going address two speciﬁc problem 1 detection functionality family via sequence clustering 2 classiﬁcation new protein known family problem directly related problem detecting protein functionality addressed method great variation performance mathematical point view clustering ill posed problem deﬁnition functionality family quite ambiguous diﬃcult quantify mathematically obtain unique objective function optimize result computational clustering approach diﬀer representation protein clustered deﬁnition optimization goal also resulting partition known protein space stability heterogeneity resulting cluster known problem shared method still help build big picture going experimental structure represents goal fully automated clustering method becomes give partial answer respect global organization protein sequence sequence classiﬁcation family simpler task unsupervised learning still ha delicate problem one introduced protein accuracy labelling training set related problem classify sequence belonging family subfamily case deﬁned evolutionary history tailored method rely multiple alignment family sequence well phylogeny tree inferred indeed resulting evolutionary tree reconstructed accurately functional subtypes often identiﬁed subtrees within many clustering technique rely pairwise similarity measure protein sequence protomap yona et al 2000 protonet sasson et al 2003 biospace yona levitt 2000 use combination three common measure pairwise similarity fasta blast followed construction weighted graph ha resulting cluster strong connected component evolution graph diﬀers algorithm enright et us also blast build dissimilarity matrix converting probability matrix used simulate ﬂow lead ﬁnal graph algorithm ha evolved complicated learning machine avoid multi domain protein problem generate hierarchical view protein space paper work simple clustering machine also relies pairwaise similarity k mean algorithm well suited detect generative cluster underlying distribution concentrated case non alignment method rely modelling protein functionality accurate ﬁt model better result clustering classiﬁcation paper explore hybrid approach protein classiﬁcation clustering ﬁt variable length markov model protein sequence use architecture associated context tree perform classiﬁcation clustering considering metric space tree bffs distance balding et al 2007 take account structure tree directly related relevant motif sequence indirectly transition probability associated motif approach motivated idea protein functionality could modelled vlmc consequence estimated context tree observation random element close together tree space combining model based technique classical similarity based statistical learning pattern recognition random tree associated functionality family protein 3 material method data handling fasta ﬁle containing sequence clustered classiﬁed family assembled label only visible evaluation training purpose ﬁle transformed via pst algorithm bejerano et al 2001 tree ﬁxed depth algorithm ﬁle containing tree compared using bffs distance tree sequence similarity generated analysis stored square matrix label reserved evaluation training purpose 1 unsupervised clustering chosen simple clustering technique rely distance optimizes within sum distance cluster distance two sequence fact distance two tree estimated via pst algorithm obtain partition c alternating optimization procedure ﬁrst compute cluster mean centroid tree given partition reassign observation closest centroid tree objective function no longer decreased done notion average tree mean centroid tree share property average euclidean space adapted matlab code k mean order handle tree bfs format computing bffs distance mean centroid tree needed 2 supervised learning approached classiﬁcation also one simple scheme rely distance given new sequence classify family ha member k closest member database standard code matlab availability original pst algorithm variable length markov chain modelling adapted bejerano 2003 additional module computing distance mean tree necessary protein sequence clustering classiﬁcation obtained author upon request variable length markov chain modeling protein functionality starting point approach supervised unsupervised learning learn structure variable length markov chain model family claim paper estimator context tree characterize vlmc family random tree random object produce tree following distribution also characterize family classiﬁcation clustering carried metric space tree well known success depends strongly concentration distribution family tree space compute estimate vlmc context tree family using probabilistic suﬃx tree algorithm vlmc bffs space tree variable length markov chain stochastic process introduced rissanen 1983 information theory see also bhlmann wyner 1999 model probability occurrence symbol given time depends ﬁnite number precedent symbol number relevant precedent symbol may variable depends speciﬁc precisely vlmc stochastic process xn value ﬁnite alphabet 1 p xn p xn xr represents sequence x xr k stopping time depends sequence process homogeneous relevant past sequence not depend n denoted relevant past called context set context τ represented rooted tree complete path leaf root represents context calling p transition probability associated context τ given 1 pair τ p called probabilistic context tree ha information relevant model see rissanen 1983 uhlmann et al 1999 4 ana georgina flesia ricardo fraiman florencia leonardi λ 2 1 21 11 b λ 2 1 12 22 figure example two probabilistic context tree alphabet 1 2 tree represents pair τ p τ 11 21 2 set context p transition probability given 2 b tree represents pair η q η 12 22 1 set context q transition probability given 3 example take binary alphabet 1 2 transition probability 2 p xn xn p xn 1 1 1 p xn 1 2 1 p xn 1 2 2 stopping time k 1 xn 1 probability otherwise stopping time k 2 xn 1 probability 1 probability 1 set context τ 11 21 2 set active node rooted tree e vt 11 21 2 1 since 1 internal node path context 11 another example alphabet given transition probability 3 p yn yn p yn 1 1 p yn 1 2 2 p yn 1 1 2 set context η 1 12 22 set active node rooted tree e vy 1 12 2 22 since 2 internal node path context 12 corresponding probabilistic context tree represented figure going embed set possible context tree constructed given ﬁxed alphabet compact metric space rooted tree deﬁned balding et al 2004 relationship simple since rooted tree thought subset node satisfying condition son present implies father present kind space natural sigma algebra b minimal one containing cylinder set tree deﬁned ﬁnite number node natural topology one generated cylinder open set associate topology family distance take value depending matched node tree not only leaf also internal one let deﬁne e v set possible sequence alphabet possible stopping time k deﬁne tree function e v 0 1 only give value one set context pattern recognition random tree associated functionality family protein 5 tree set internal node context deﬁnition easy deﬁne distance x v φ v v v 2 4 φ e v strictly positive function p v φ v particular use function 5 φ v zgen v z gen v stand generation node number symbol reach root let compute distance two tree given preceding example x v φ v v v 2 φ λ λ λ 2 φ 1 1 1 2 φ 2 2 2 2 φ 11 11 11 2 12 12 12 2 φ 21 21 21 2 φ 22 22 22 2 0 0 0 φ 11 φ 12 φ 21 φ 22 4 distance becomes compact metric space see detail balding et al 2004 computation distance not fast computation euclidean distance may devised code search coincidence context mean going root leaf random tree random tree distribution ν measurable function 6 p z ν dt any borel set ω f p probability space ν probability b given sample independent random tree tn identical distribution ν compact metric space measure central tendency sample centroid deﬁned tree set tree tn satisfying 7 tn arg min 1 n n x ti formula may show problem diﬃcult since calling search whole set tree grows exponentially number node easy prove sample centroid mean tree set tree built majority vote node mean least one sample centroid doe not need unique deﬁned tree whose node present only present least half sample supervised unsupervised learning space tree cluster analysis goal ﬁnd optimal partition observation object within cluster similar cluster dissimilar diﬀers fundamentally classiﬁcation analysis observation allocated known number predeﬁned group population many technique based certain measure similarity pair observation 6 ana georgina flesia ricardo fraiman florencia leonardi k mean clustering concerned particular cluster technique called k mean clustering procedure generates class label trough minimization within cluster point scatter dissimilarity based loss function deﬁned w c 1 2 k x x c x c rule characterizes extent observation assigned cluster tend close wa initially intended real valued quantitative variable squared euclidean distance wa chosen measure dissimilarity case object belonging space tree choose distance dissimilarity measure scatter may redeﬁned w c k x x c ti tk tk sample centroid associated kth cluster case euclidean space iterative descent algorithm solving arg min c k x x c ti tk may obtained noting any set observed tree arg min x ti deﬁnition hence obtain alternating optimization procedure ﬁrst compute cluster mean centroid tree reassign observation closest centroid tree objective function no longer decreased one popular iterative descent algorithm go name k mean one use example not diﬃcult prove following pollard 1981 par example case locally compact metric space sample centroid tree converge population centroid tree sample increase great result ensures partition population k cluster enough data proper initialization avoid local minimum algorithm give accurate outline cluster ﬁnal note state k mean algorithm could used classiﬁcation purpose following next step apply k mean clustering training data k class separately using r prototype per class assign class label k r prototype classify new feature class closest prototype example prototype method classiﬁcation also adapted work space tree diﬀerence method neighbor fact prototype not part training sample center partition training sample class neighbor classiﬁcation given family protein f new sequence amino acid goal determine belongs f not answer question bejerano 2003 leonardi 2007 estimate ﬁrst model family f using sequence classiﬁed inside family determine label new protein search family whose model ha higher probability produced sequence model constructed family f variable length markov chain obtained estimating probabilistic tree match chain mean pst algorithm bejerano 2003 paper ﬁt simplest model classiﬁcation consider protein family clustered tightly measuring bffs pattern recognition random tree associated functionality family protein 7 distance context tree score new protein rule neighbour label protein belonging family ha neighbour neighbourhood neighbour rule simple distance based method pattern recognition data classiﬁcation method relies intuitive concept data point class neighbour class distance high probability result given data point unknown class simply compute distance point training data assign class determined using majority vote among k neighbour data point algorithm straightforward given new observation ﬁnd k training point 1 k closest distance classiﬁcation made using majority vote among k neighbor tie broken random simplicity rule extreme success depend only ability measure cluster family ability vlmc modelling successfully detect diﬀerences protein chain family resume context tree computational example section would like ass capability vlmc method capture essential structure family would help discrimination problem traditional pst classiﬁcation method choose training set sequence given family estimate context tree family concatenating sequence training set classiﬁcation performed computing probability given sequence would produced context tree motivation approach related biological understanding evolution composition protein family suppose group evolutionary related protein sequence exhibit many identical short segment either preserved selection not diverged long enough common single ancestral sequence variable memory model well equipped pick locally conserved segment showing architecture context tree bejerano 2003 approach diverges classical approach vlmc method since not use classiﬁcation empirical probability associated context architecture context tree computed protein sequence family also not collect sample generate estimation model compute estimate per sample sequence look cluster together tree space context tree built collected sequence show segment consistently repeated sequence context tree built sequence show pattern inside particular sequence family bond emerge relationship tree space deﬁnition distance thus fundamental approach illustrated idea some small example following methodology bejerano 2003 since want compare approach traditionally pst approach indirectly classiﬁcation method reference pfam database based hidden markov model trained smaller database manually curated well known protein k mean clustering hard problem since no help ground truth shape family database constructed clustering sequence statistical method help core manually curated well known protein not aim show method would accurately cluster whole pfam database second discus potential model could checking coherence relationship among family notice k mean algorithm considers similarity pairwise fashion method consider global similarity could help cluster assignment order ass ability k mean procedure cluster protein tree space using bffs distance started transforming 5 family protein selected pfam database labelled overall performance figure 5 plot two matrix one count percentage well classiﬁed misplaced protein notice two family correctly 8 ana georgina flesia ricardo fraiman florencia leonardi 74 0 0 4 0 0 39 0 1 10 1 0 91 11 5 0 0 0 56 0 0 0 0 0 45 conf mat data count 0 0 0 0 78 0 2 20 0 0 0 0 100 0 0 0 0 0 100 conf mat recog rate overall recognition rate figure confusion matrix data count percentage well classiﬁed misplaced protein 5 family protein clustered showing high degree coherence also attract member family shown figure second example added 6 family actin ank efhand overall recognition rate drop 64 confusion matrix plot figure 5 show u protein not scattered around misplaced speciﬁc family feature could interesting time determine coherence family relationship diﬀerent family example ten protein beta lactamase family incorrectly assigned family 9 reassigned ank family 50 ank protein assigned family showing three family close tree space 349 0 0 1 47 77 6 0 0 34 0 0 120 0 0 15 0 1 0 4 1 0 0 0 43 1 13 0 110 0 0 12 0 0 0 0 107 8 0 13 0 0 0 0 0 0 0 0 16 0 12 2 6 45 1 0 0 0 0 2 74 0 0 2 0 0 0 0 0 1 9 0 39 0 0 1 0 0 0 0 0 3 3 3 90 9 0 0 0 0 0 0 0 0 1 0 55 0 0 0 0 0 0 16 0 0 0 0 29 0 0 0 0 1 15 3 15 0 64 26 195 actin ank efhand actin ank efhand conf mat data count 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 18 0 78 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 actin ank efhand actin ank efhand conf mat recog rate overall recognition rate figure confusion matrix data count percentage well classiﬁed misplaced protein 11 family protein pattern recognition random tree associated functionality family protein 9 1 2 3 4 5 6 7 8 9 10 88 90 92 94 96 98 100 recognition rate data data using neighbor recognition rate figure blue plot overall recognition rate k nearest neighbor rule function k considering training set 80 total protein classifying 100 red plot overall recognition rate computed classifying only 20 set aside training set k nearest neighbor consider 11 family previous example actin short ank beta lactamase efhand easy diﬃculty clustering problem allowing help training set selected randomly 80 whole set protein set aside training set could reﬁne later example choosing randomly 80 family training set classify whole set 1700 protein using training set generate neighborhood diﬀerent size plot blue percentage true positive detected function number neighbor considered secondly compute percentage true positive detected considering only 20 protein not included training set plot red function number neighbor considered see figure last rate computed better indicator performance rule since any rule well train set fact usually one nearest neighbor achieves 100 detection train set even though want compare pst classiﬁcation rule ﬁrst rate one reported bejerano 2003 report start stating 98 whole set counting training set ha well classiﬁed using one nearest neighbour rule 60 true positive given k mean not count training set 92 test set well classiﬁed one nearest neighbour rule interesting notice allow three point neighbourhood le accuracy classiﬁcation giving evidence idea diﬀerences tree subtle written table 5 percentage true positive per family reported bejerano 2003 point using short tree still achieve rate classiﬁcation ﬁrst three column percentage good classiﬁcation training 1 2 3 neighbour computed using training set 80 random protein scoring 100 last three column show variable computed only 20 sample not part training set percentage reduced some case not much interpretability result credibility experiment ha reinforced only compute ﬁrst three column compare pst value computed fashion final remark pattern recognition active ﬁeld research within engineering computer science community main goal develop automatic method recognizing pattern data encloses sub discipline like discriminant analysis feature extraction error estimation cluster analysis among others two speciﬁc method well known literature statistical pattern recognition refereed 10 ana georgina flesia ricardo fraiman florencia leonardi 1 k mean clustering 2 k nearest neighbor classiﬁcation addressed two method not conventional setting euclidean space database classify cluster consist ﬁnite tree considered compact metric space tree natural space database lie shown extension two aforementioned procedure apply new space interesting example database tree one obtained general set codiﬁed string modelled variable length markov chain vlmc represented context tree estimated string using algorithm like pst bejerano 2003 context rissanen 1983 leading ﬁnal database estimated tree codiﬁcation correct claim context tree chain information needed discrimination metric based method functional genomics protein codiﬁed string amino acid vlmc model naturally ﬁtted functional family string amino acid chain natural candidate type modelling any suitable codiﬁcation object ﬁnite alphabet make model arise problem besides functional genomics could make proﬁt type approach classiﬁcation written report codiﬁcation report usually made order reduce dimensionality extract feature leading database string see jeske liu 2004 jeske liu 2006 paper codiﬁcation derived carefully ensure discrimination bad report good report also codiﬁcation could represent conjecture made style prosody speech written text ha done veilleux et al 1990 general setting dorea et al 1997 frota et al 2001 case detecting diﬀerences brazilian european portuguese case successful discrimination may give evidence support linguistic conjecture believe context great importance addressing several problem computational linguistics natural language modelling speech processing reference 1 altschul madden schaeﬀer zhang j zhang z miller w lipman 1997 gapped blast psi blast new generation protein database search program nucleic acid research 25 2 apostolico bejerano 2000 optimal amnesic probabilistic automaton learn classify protein linear time space proc int l conf computational molecular biology 4 3 bejerano g 2003 automaton learning stochastic modelling bio sequence analysis phd thesis hebrew university 4 bejerano g yona g 2001 variation probabilistic suﬃx tree statistical modelling prediction protein family bioinformatics 2001 5 balding ferrari fraiman sued limit theorem sequence random tree arxiv 6 bateman coin durbin finn hollich khanna marshall moxon sonnhammer studholme yeats eddy 2004 pfam protein family database nucl acid 32 90001 7 dayhoﬀ 1976 origen evolution protein superfamily fed proc 35 8 dorea galves kira alencar 1997 markovian modeling stress contour brazilian european portuguese brazilian journal probability statistic vol 9 einsenberg marcotte xenarios yeates 2000 protein function post genomic era nature 405 10 frota vigario 2001 correlate rhythmic distinction portuguese case probus vol 11 hefyi gerstein 1999 relationship protein structure function comprehensive survey application yeast genome mol biol 288 12 jeske liu 2004 mining massive text data developing tracking statistic classiﬁcation clustering data mining application ed bank et springer pp 13 jeske liu 2006 mining tracking useful feature massive text data application risk management appear technometrics 14 karp 2002 mathematical challenge genomics molecular biology notice amer math 49 5 krause stoye vingron 2000 systers protein sequence cluster setr nucleic acid 28 1 pattern recognition random tree associated functionality family protein 11 15 leonardi 2007 sparse stochastic chain application classiﬁcation phylogeny protein sequence phd thesis bioinformatics program universidade de sao paulo 16 leonardi 2006 generalization pst algorithm modelling sparse nature protein sequence bioinformatics vol 17 enright van dongen ouzonis 2002 eﬃcient algorithm large scale detection protein family nucleic acid 18 pearson 2000 flexible sequence similarity searching fasta 3 program package method mol biol 19 rabiner tutorial hidden markov model selected application speech recognition proc ieee 20 rissanen j 1983 universal data compression system ieee trans inform theory vol 29 5 21 pollard 1981 strong consistency clustering annals statistic vol 9 no 1 22 1981 strong law large number measure central tendency dispersion random variable compact metric space annals statistic vol 9 no 23 sasson vaakin fleischer h portugaly bilu lineal n linial 2003 protonet hierachical classiﬁcation protein space nucleic acid 31 1 24 smith annau chandrasegaran 1990 finding sequence motif group functionally related protein 2 25 van dongen 2000 graph clustering flow simulation phd thesis university utrecht 26 yona lineal n lineal 2000 protomap automatic classiﬁcation protein sequence hierarchy protein family nucleic acid 28 1 27 yona levitt 2000 towards complete map protein space based uniﬁed sequence structure analysis known protein proc int conf biol vol 8 ana georgina flesia ing medina allende ciudad universitaria cp 5000 ordoba argentina address flesia url ricardo fraiman universidad de san andrs argentina universidad de la repblica uruguay address rfraiman florencia leonardi instituto de atica e ıstica universidade de ao ao paulo brazil address leonardi 12 ana georgina flesia ricardo fraiman florencia leonardi family size coverage pst 515 36 100 100 100 12 100 100 100 100 aaa 66 269 actin 142 100 180 129 aldedh 69 114 aminotran 63 ank 83 arf 43 100 100 asp 72 79 100 180 100 62 100 51 98 92 90 bzip 95 78 cadherin 31 100 100 cellulase 40 42 100 coesterase 60 connexin 40 100 100 100 61 100 100 80 100 100 109 57 100 84 100 100 100 crystall 53 100 100 100 cyclin 80 61 95 91 cystatin 53 130 170 175 68 48 46 dsrm 14 102 efhand 320 egf 169 enolase 40 100 88 152 49 100 100 100 100 fgf 39 100 100 100 100 100 table family name size number protein percentage correct classiﬁcation pst method percentage classiﬁcation using 80 sample training test 100 test set 20 aside training test set