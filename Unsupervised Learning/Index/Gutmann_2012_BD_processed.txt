bregman divergence general framework estimate unnormalized statistical model michael gutmann dept computer science hiit dept mathematics statistic university helsinki hirayama graduate school informatics kyoto university hirayama abstract show bregman divergence vides rich framework estimate malized statistical model continuous discrete random variable model not integrate sum one tively prove recent estimation od estimation tio matching score matching belong proposed framework explain interconnection based supervised learning discus role boosting supervised learning 1 introduction denote x xtd sample td dent observation random variable x unknown probability distribution pd discus method ﬁnd sample estimate pd solving unconstrained optimization problem probability distribution random variable refer probability density function pdf continuous random variable probability mass function pmf case discrete random variable property nonnegative normalized integral pdf domain one pmf sum one summation done possible state dom variable any valid estimate pd must satisfy nonnegativity normalization condition seek estimate ˆ pd form ˆ pd x pm 1 j x pm cost function pm family model distribution look pd member pm family satisﬁes nonnegativity condition doe not integrate sum one another number call pm unnormalized model diﬀerent choice j pm assessed two viewpoint statistical perspective ˆ pd converge pd sample size td increase consistency computational perspective choice j pm not lead tion problem only solved via expensive computation classical choice j negative jll x pm ln pm xt family pm must choice j large enough set bility distribution order consistency requirement member pm normalized ten lead computational diﬃculties tion consider instance case pm pdf speciﬁed vector parameter model speciﬁcation must pm u θ pm u θ du 1 choice analytical integration not possible dimension n high numerical integration method applicable second condition not fulﬁlled similarly discrete random variable number possible state grows exponentially dimension n summing state come quickly computationally prohibitive markov random ﬁelds widely used computer sion li 2009 face example problem normalization derive large class cost function j avoid aforementioned diﬃculties not require family pm consist normalized probability distribution particular presented cost function unconstrained tion lead estimate pd cost function based bregman divergence two able function show framework cludes recently proposed estimation method estimation extension mann arinen 2010 pihlaja et 2010 score matching arinen 2005 lyu 2009 well tio matching arinen 2007 framework also allow u make interconnection explicit basic idea estimation perform unsupervised learning estimation pd mean supervised learning logistic regression framework outlined show many existing timation method similarly interpreted along line supervised learning also show important concept supervised learning ing applied unsupervised learning work boosting unsupervised learning see welling et 2003 rosset segal 2003 rest paper structured follows tion 2 brieﬂy review bregman divergence use estimate unnormalized model topic section section contains main result paper section 4 present simulation result section 5 concludes paper 2 bregman divergence brieﬂy review bregman divergence background advanced treatment refer example bregman 1967 unwald dawid 2004 frigyik et al 2008 bregman divergence dψ b two vector b deﬁned dψ b ψ b b 2 distance positive b only ψ u diﬀerentiable function derivative exists point domain ψ strictly convex domain ψ convex see example boyd vandenberghe 2004 section following assume case note adding dψ b dψ b positivity bregman divergence show ψ b scalar case mean derivative strictly monotonically increasing measure distance two vector valued function f g computing dψ value f g domain summing possibly weighted nondecreasing function µ f g dψ f g dµ 3 quantity known separable bregman gence unwald dawid 2004 section bregman divergence special case sion avoids using underlying dψ frigyik et 2008 proposition open tion whether general version also used estimate unnormalized model gral sign interpreted tegral µ example cumulative bution function cdf associated distribution p obtain f g dψ f u g u p u du p pdf continuous random variable f g u dψ f u g u p u p pmf discrete random variable given property dψ f g f g 0 mean f equal g almost everywhere f g used approximate f g assume f ﬁxed minimization dψ f g respect g equivalent minimization lψ g g g g g f dµ 4 respect note optimization formed without any constraint tral equation paper since estimation method propose essentially originate derive alternative expression lψ g f not vector valued scalar function case not use bold face letter sion useful order relate work estimation method unnormalized model notation g g g g g g 5 eq 4 written l g g g f dµ 6 changed subscript ψ highlight dependency cost function function given deﬁnitions property ψ function satisfy 0 g 1 g g 1 g 0 7 3 estimation unnormalized model minimization bregman divergence cost function lψ depends convex tiable function ψ nondecreasing weighting tion µ function discus possible choice unconstrained minimization lψ lead together sample x mate pd choice f one issue discussion center around second issue computation integral deﬁnition lψ mean sample average matching data distribution pd straightforward choice f pd lψ come lψ g g g gdµ g pddµ 8 second term evaluated sample age ﬁrst term however problematic since integral not evaluated closed form possible solution would choose ψ v v v constant lution diﬀerential equation ψ v however not strictly convex function hence not applicable however introduce auxiliary distribution pn write lψ lψ g pn g pn g g pn g pddµ equal lψ g e g pn g g pn e g x 9 µ either identity function ous random variable staircase function discrete one expectation taken random variable x using sults section 2 minimization cost equivalent minimization l l g e g pn g x 10 assuming sample ytn dom variable available ical expression pn hand cost function evaluated sample version l j g 1 tn tn g yt pn yt td td g xt 11 denote j highlight connection eq 1 introduction given choice f pd function minimizes j g vides estimate ˆ pd fact lψ g minimized g pd proven certain technical condition estimator ﬁned consistent see example wasserman 2004 ch 9 also hold cost function derive based bregman vergence although not always mention possibility could choose µ cdf associated matching ratio data noise distribution continuing assumption sample random variable known distribution pn available consider case f νpn some positive constant choose µ cdf associated pn multiplied factor cost lψ becomes lψ g ν e g g g e g x 12 compact form l l g ν e g g x 13 previous section sample version l deﬁnes cost function j whose minimization lead consistent estimator pd since f wa deﬁned νpn estimate ˆ pd obtained via ˆ pd minimizer alternatively one may plug cost function g optimize directly respect pm parameter deﬁning pm matching ratio distribution provides link supervised learning especially classiﬁcation let random variable x correspond class c 1 random variable class c 0 let pd u p 1 pn u p 0 ν p c 0 c 1 ratio f νpn equal f u pd u νpn u p u c 1 p u c 0 p c p c 14 serve discriminant function classify tween two class minimal error rate see example wasserman 2004 section hence learning ratio f learning optimal ﬁer note inverse doe not strictly hold reason classiﬁcation minimal error rate only need know decision boundary f u unsupervised learning however need learn complete function relation estimation comparison eq 13 equation 7 pihlaja et 2010 show estimator family cited paper exactly one dence follows ﬂipping sign consider minimization pihlaja et 2010 maximization performed plugging g setting ν 1 function corresponds nonlinearity used pihlaja et al 2010 respectively condition choice f µ cdf pd another possibility eq 7 occurs also cited paper note correspondence also show mation gutmann arinen 2010 special case framework considered fact contrastive estimation follows ψ u u ln u u ln 1 u equivalently u ln 1 u u ln u 1 u conceptual side correspondence allows extend connection unsupervised ing supervised learning gutmann arinen 2010 pihlaja et 2010 latter work ha formulated without reference vised learning however see method learn function f eq 14 optimally inate data x noise cal side correspondence show estimation method proposed gutmann arinen 2010 pihlaja et 2010 not limited tion probability density function exactly method also applied estimation ability mass function discrete random variable section 4 explore ﬁnding mean ulations relation boosting continuing tion unsupervised supervised learning make explicit connection jective function used boosting see example collins et 2002 murata et 2004 eq 13 purpose denote log discriminant function ln f u ln p u c 1 u c 0 g u let ν p c 0 c 1 framework boosting following cost function lboost estimation g ha proposed murata et al 2004 lboost g ν e g e x 15 u satisﬁes u exp u turn cost function special case eq 13 let u exp u u exp eq 13 take form ν e g e x satisfy 0 u 1 exp u hence case cost function eq 13 becomes lboost example given u ln u ln u used estimation corresponds objective function used logitboost see example friedman et 2000 tion boosting log discriminant function g u sically sought form additive model g u gi u component gi found greedy manner see example friedman et 2000 section 3 framework since pn ν known g u ln pm u νpn u hence ln pm factorizes model g u additive model mean stepwise estimation mean estimation extension equivalent boosting related tic work unnormalized model boosting see welling et 2003 investigate iterative estimation unnormalized model section 4 matching ratio data data dependent noise distribution consider case noise tion pn dependent distribution data pn u αpd bu v βpd u α β sum one matrix b mal orthonormality assumption made plicity z bt x ha distribution pd bz continuous discrete random variable noise thus mixture original perturbed data let f u pd u u g u pm u αpm bu v βpm u choice make sense must β µ cdf pn l g e g bt x v g x g x 16 expectation taken x l evaluated taking sample average x previous section furthermore one may best plug deﬁnition g l g order obtain cost function l pm b v depends directly pm cost depends particular perturbation chosen b order avoid dependency subjective choice one average l pm b v possible value b v minimize average respect pm show ratio matching arinen 2007 score matching arinen 2005 emerge some particular choice perturbation relation ratio matching ratio matching tribution pd binary random variable x 1 n estimated minimizing cost function lrm pm e n pm pm x pm 17 see arinen 2007 expectation taken x term denotes x bit ha ﬂipped sign let bi diagonal matrix one diagonal slot bi bix matrix bi orthonormal bt bi bibix appendix show α β v 0 b bi u u u minimizing cost function l eq 16 mean minimizing l pm bi 0 2 e pm bix pm x pm bix 18 comparison eq 18 eq 17 show lrm pm l pm bi 0 const since noise density obtained data ing ith bit x probability one half ratio matching interpreted estimate pm ing detect corruption x any single bit relation score matching consider case b identity matrix v zero mean random variable covariance matrix ther assume x continuous random variable consider cost l pm v averaged appendix show minimizing average mean minimizing ev l pm v c 1 1 e ln pm x 1 ln pm x 19 mean term order higher c constant x ev mean taking expectation furthermore laplace del operator term bracket right hand side exactly objective minimized score matching arinen 2005 since 1 1 positive see eq 7 minimizing average ev l pm v small level noise sponds score matching score matching thus interpreted estimate pm learning discriminate classify original slightly noisy data see also arinen 2008 lyu 2009 matching score function score function derivative log pdf respect argument score function used arinen 2005 estimate unnormalized model generalize result based eq 4 let f g score function f u ln pd u g u ln pm u respectively note not trying ﬁnd pd data only transformed version shown however f g implies pd pm lyu 2009 u f g vector valued function hence letter bold face choose µ cdf associated pd cost function lψ eq 4 becomes choice lψ g g u g u g u pd u du g u u du 20 ﬁrst term evaluated sample average data second term transformed tractable form mean partial integration done arinen 2005 score matching note element vector g u pd u ui reach boundary domain pd lψ g equal lψ g e g x g x g x n g x 21 expectation taken random able x lψ computed taking sample average implicitly assumed involved function smooth enough derivative formula exist relation score matching particular choice ψ g x gi x 2 g x gi x gi x element vector g x ln pm x cost function lψ g e 1 n x 22 one used score ing arinen 2005 lyu 2009 ha generalized score matching along another direction learned data not approximation f f linear operator l ha property implies pm pd generalization score matching combined one presented 4 simulation illustrate selected piece theory previous section focus connection tween unsupervised supervised learning see tion estimation discrete random variable section shown estimation applied continuous crete random variable illustrate date result mean estimation fully visible boltzmann machine assume nary random variable x follows pmf pd log distribution ln pd x 1 x z 23 matrix symmetric ha zero diagonal element z partition function normalizes distribution every possible order easily sample exactly bution set dimension n x ﬁve model estimate ha log distribution ln pm x θ 1 mx bt x c 24 c parameter negative log partition function parameter vector b upper triangular part symmetric matrix diagonal element set zero pound vector parameter denoted model unnormalized since doe not sum one choice parameter pn use bernoulli distribution equal success tie mixture bernoulli distribution wa ﬁrst ﬁtted data factor ν ten validate consistency estimator puting diﬀerent sample size td estimation ror sum squared error b figure 1 show estimation error decay linearly scale sample size increase illustrates consistency since gence quadratic mean implies convergence ability result hold case trastive noise single bernoulli distribution red curve square marker mixture bernoulli distribution black curve asterisk er reference also show estimation result boltzmann machine estimated dolikelihood besag 1975 precision mate similar diﬀerent method boosting unsupervised learning section pointed greedy stepwise estimation model mean estimation corresponds boosting logitboost stepwise estimation tationally lighter estimation expert time since estimation performed ing optimization problem stepwise estimation may suboptimal investigate stepwise estimation optimization aﬀects estimation accuracy purpose generated td 10000 sample random variable x log pdf ln pd x 4 k ln ln 2 25 pdf four expert x follows ica model term parenthesis malizes pdf assume normalizing constant not known furthermore also assume number expert unknown estimate model ln pm x k θ k k c 26 parameter θ bk bk c contrastive estimation use noise density gaussian density sample covariance matrix data covariance matrix set factor ν two test boosting estimate manner diﬀerent bk k 1 set k 8 larger number expert data pdf ass estimation performance compute learning bk k 4 matrix r bt column b given bk fect b matrix r contains two block upper 4 4 permutation matrix possible sign ﬂips lower k 4 block zero use frobenius norm r subtraction tity matrix upper block measure estimation error accounted possible permutation sign ﬂips figure 2 show distribution error 100 random timation problem comparison also plot error measure two four bk learned time scenario still learned k 8 tures total learning four vector time corresponds estimating model correct number expert ﬁgure show estimating model iterative way lead le accurate timates hence computation estimation accuracy clearly visible computationally lighter optimization performed boosting come statistical expense le curate estimate interestingly scenario norm vector bk much smaller k 4 k show some kind model selection wa performed result not shown 5 conclusion shown bregman divergence serf rich framework estimation ized statistical model continuous 3 4 samplesize estimation error mpl nce bernoulli nce bernoulli mixture figure 1 estimation fully visible boltzmann chine estimation used bernoulli distribution equal success probability red curve square marker mixture bernoulli distribution black curve asterisk auxiliary noise distribution pn also show estimation result blue curve dot horizontal axis sample size vertical axis estimation error every sample size result erage 100 randomly created estimation problem parameter vector upper triangular part matrix drawn normal distribution mean zero standard viation crete random variable found contrastive estimation generalization mann arinen 2010 pihlaja et 2010 score matching generalization arinen 2005 lyu 2009 well ratio matching arinen 2007 situated within proposed framework related data dependent noise distribution framework highlight tion supervised unsupervised learning show also boosting used estimate unnormalized distribution manner outlined framework proposes many estimator choose open question whether general version bregman divergence frigyik et al 2008 also used estimate ized model would provide even tor important open question thus best one possible choice measure formance estimation error ﬁnite large sample size computational statistical performance performance bilistic inference task answer depend measure used may also data future work 0 1 1 2 4 number jointly learned bi error measure figure 2 boosted estimation ized model data followed ica model laplacian source data pdf ha four expert see eq 25 model eq 26 ha eight tal axis number jointly learned expert bk vertical axis distribution estimation error 100 mation problem estimating model iterative way instead learning expert together tationally lighter lead le accurate estimate acknowledgement wa partially supported jsps research ship young scientist author wish thank aapo arinen provided research vironment collaboration appendix section relation ratio matching derivation eq 18 simplify notation drop index bi furthermore denote ratio pm x pm x pm bx r x g x x α β u u u cost function l eq 16 l e r bx 2 r x 2 x 27 using r bx 1 x obtain l 2 e 1 x 2 28 using 1 x r bx equal pm bx pm x pm bx give eq 18 section relation score matching derivation eq 19 please note save space need leave many line calculation b function g section equal g x v pm x αpm x v βpm x 29 use notation g x v make dency v explicit derive eq 19 ﬁrst expand g x v g x v around v result g x v 1 α pm x x v hpv x pm x 2 x v 02 g x v 1 α pm x x v αvt hpv x αβ pm x 2 x v 02 element matrix hp x mean partial derivation respect xi next expansion used develop cost eq 16 around v taking expectation v expectation x using 0 z 1 z give ev l pm v c 1 1 e x pm x 1 2 x pm x 2 30 c constant using x pm x ln pm x x pm x 2 31 see lyu 2009 give eq 19 reference besag statistical analysis data statistician boyd vandenberghe convex optimization cambridge university press bregman relaxation method ﬁnding common point convex set tion solution problem convex ming ussr comp math math 217 collins schapire singer logistic regression adaboost bregman distance chine learning 48 1 issn friedman hastie tibshirani additive logistic regression statistical view boosting annals statistic 28 2 frigyik srivastava gupta tional bregman divergence bayesian estimation distribution ieee trans information ory 54 11 unwald dawid game theory imum entropy minimum discrepancy robust bayesian decision theory annals statistic 32 gutmann arinen timation new estimation principle ized statistical model proc int conf ﬁcial intelligence statistic arinen estimation cal model using score matching journal chine learning research arinen some extension score matching computational statistic data analysis 2512 arinen optimal approximation signal prior neural computation li markov random field modeling image analysis springer lyu interpretation generalization score matching proc conf uncertainty ﬁcial intelligence murata takenouchi kanamori eguchi information geometry bregman divergence neural computation 16 7 pihlaja gutmann arinen family computationally eﬃcient simple estimator unnormalized statistical model proc conf uncertainty artiﬁcial intelligence rosset segal boosting density estimation advance neural information processing system 15 page wasserman statistic springer welling zemel hinton self vised boosting advance neural information processing system 15 page 2003