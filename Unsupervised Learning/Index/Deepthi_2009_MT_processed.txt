ijcsis international journal computer science information security vol 6 no 1 2009 mirroring theorem application new method unsupervised hierarchical pattern classification dasika ratna deepthi department computer science aurora engineering college bhongir nalgonda india eswaran department computer science srinidhi institute science technology yamnampet ghatkesar hyderabad india paper prove crucial theorem called mirroring theorem affirms given collection sample enough information classified class exists mapping classifies classifies sample ii exists hierarchical classifier constructed using mirroring neural network mnns combination clustering algorithm approximate mapping thus proof mirroring theorem provides theoretical basis existence practical feasibility constructing hierarchical classifier given map proposed mirroring theorem also considered extension kolmogrov theorem providing realistic solution unsupervised classification technique develop general nature led construction learning machine tree like structure ii modular iii module running common algorithm tandem algorithm iv actually built architecture developed tandem algorithm hierarchical classifier demonstrated example problem unsupervised pattern recognition mirroring theorem classifier mirroring neural network feature extraction tandem algorithm learning introduction various way field artificial intelligence machine learning furthered starting experimentation 1 abstraction 2 3 study locomotion 4 many technique developed learn pattern 5 6 well reduce large dimensional data 7 8 relevant information used classification pattern 9 10 investigator tackled varying degree success pattern recognition problem like face detection 11 gender classification 12 human expression recognition 13 object learning 14 15 unsupervised learning new task 16 also studied complex neuronal property higher cortical area 17 naming however technique not require automatic feature extraction step pattern classification approach developed machine based proposed mirroring theorem performs feature extraction pattern learning simultaneously pattern unsupervised mode automatic feature extraction step prior unsupervised classification fulfills one additional crucial requirement called dimensional reduction furthermore proving stated mirroring theorem actually demonstrate unsupervised hierarchical classifier mathematically exist also proved hierarchical classifier perform unsupervised classification approximated network node forming architecture term node architecture actually abstraction entity executes two procedure mirroring neural network mnn algorithm coupled clustering algorithm mnn performs automatic data reduction feature extraction see 18 detail mnn clustering doe subsequent step called unsupervised classification extracted feature mnn two step performed tandem hence term tandem algorithm mirroring theorem provides proof technique always work provided sufficient information contained ensemble sample classified certain continuity condition mapping satisfied mirroring theorem prove paper may considered extension kolmogrov theorem 19 providing practical method unsupervised classification detail theorem used developing unsupervised hierarchical classifier discussed next section main contribution paper propose prove theorem called mirroring theorem provides mathematical base constructing new kind architecture performs unsupervised hierarchical classification input implementing single common algorithm call tandem algorithm demonstrated example set image pattern proposed hierarchical classifier mathematically proved exist develop new common algorithm doe two machine step namely automatic feature extraction clustering execute unsupervised 16 issn ijcsis international journal computer science information security vol 6 no 1 2009 classification given input hence say paper proposes new method build hierarchical classifier mathematical basis new kind common algorithm implemented throughout hierarchy classifier demonstration example problem find necessary discus point mnn moving detail proposed theorem tandem algorithm mnn nothing neural network nn architecture trained produce output equal input closely possible mirror input output layer training process proceeds repeated presentation input sample stop mnn could mirror least 95 input sample mnn said successfully trained given input sample best possible extracted feature input automatically obtained mnn least dimensional hidden layer feature used unsupervised input classification clustering algorithm see figure 1 illustration mnn architecture wherein input given x dimension n reduced dimension much le n since capable mirroring x output contains much information x even though ha lesser dimension component thought feature contains pattern x hence used classification detail mnn architecture referred 20 21 proceeding proving main theorem presentation actual computer simulation perhaps appropriate write line idea motivated paper presently well known neural architecture human neocortex hierarchical 22 23 24 25 constituted neuron different level information exchanged level via neuron 26 27 28 29 initiated receipt data coming sensory receptor eye ear etc organization various region within level system not completely understood much evidence region neuron one level connected region neuron another level thus forming many tree like structure 25 30 also see 31 various intelligent task example image recognition performed numerous neuron firing sending signal back forth level 32 many researcher working field artificial intelligence sought imitate human brain attempt build learning machine 33 34 employed tree like architecture different level performing recognition task 35 described attempt demonstrate hierarchical classifier address task feature extraction reduction recognition constructed architecture perform intelligent recognition task unsupervised manner plan paper follows next section prove proposed mirroring theorem pattern recognition section 3 based proof mirroring theorem show build pattern classifier posse ability automatically extract feature architecture used develop proposed architecture unsupervised pattern learning classification including proposed tandem algorithm section 4 report result demonstration classifier applied unsupervised pattern recognition problem wherein real image face flower furniture automatically classified unsupervised manner section 5 discus future possibility kind architecture ii mirroring theorem prove term mirroring theorem pattern recognition statement theorem hierarchical invertible map exists map set dimensional data n fall j distinct class ii j class turn map exist map class subclass map approximated set j 1 node mnns associated clustering algorithm forming treelike structure proof fact invertible map exist indicate exist j vector function map point falling some different region rd vector function may denoted fj clarify notation cautioning fj not treated vectoral component mean map carry point set point contained hence thought collection ray ray denotes map starting some point culminating point belonging similarly collection ray start some point culminates point belonging thus define map f f u fj argue follows since first map take space image say set point similarly take image say set point set sj since assumption target image region contains j distinct class could conclude set point sj distinct non overlapping otherwise assumption j distinct class violated region separable distinct one another also exist map distinct renumber region rd manner union first set belong union next set belong similarly till sj also implies since 17 issn ijcsis international journal computer science information security vol 6 no 1 2009 image set reclassified k pattern assumption exists vector function set 1 2 3 take distinct region distinct set denoted 1 2 3 thought collection ray denoting map point point set thus 1 collection ray leading set 2 collection ray set hence similar definition f denote map 1 u 2 u 3 u u order not clutter diagram possible within set not drawn figure 2 assume without prejudicing theorem generality number subset map similarly 1 2 3 take distinct set function set gj 1 gj 2 gj 3 gj map sj respective cjt existence function map fj map point set n dimensional space j distinct class implies set point separable j distinct class dimension strictly speaking necessary assume function fi 1 2 j property invertible bijective order exclude map also property necessary prove function f approximated mnn along clustering algorithm also implicitly assumed map considered theorem least continuous first order point close one another expected image close one another proceed proof first show possible approximate set map take space single mnn show mnn explicitly constructed trained perform map assume sufficient sample available training consider mnn converging set layer adalines pe converging set consists n input first layer end layer adalines shown figure 1 set n thought simple nn trained x input output weight network obtained using back propagation algorithm impose following condition output fk k class input vector belongs obviously k known hand f known thus train converging part nn similarly assume exists diverging nn depicted figure 1 b exist starting adalines ending n adalines second neural network assume input set output would original point x dimension space whose image imposing latter condition second diverging neural network trained sufficient sample weight obtained stage diverging neural network take input output corresponding combining two converging diverging first lead second without changing weight nothing mnn pictorially represented figure 1 c mnn mirror input vector x output middle layer adalines thus proved existence mnn map point x space dimensional space back original point dimensional x space point space classified j class using suitable clustering algorithm thus proved node hierarchical classifier approximated combination mnn clustering algorithm proof second set map space r space exists us similar argument immediately see j map j class space hence j node class constructed using similar procedure see set map assumed approximated j 1 node whose existence proved forming treelike structure shown figure 3 qed may noted node architecture depicted figure illustrate use mirroring theorem develop hierarchical classifier performs task unsupervised pattern recognition iii unsupervised hierarchical pattern recognition section describes architecture engine next section report application example problem wherein set input image automatically classified intent build learning engine ha following characteristic hierarchical ii modular iii unsupervised iv run single common algorithm mnn associated clustering advantage developing recognition system 4 characteristic learning method doe not depend problem size learning network simply extended recognition task becomes complex ha surmised investigator architecture human doe loosely speaking posse 4 characteristic except instead iv some kind analog classification process procedure performed set neuron seemingly behave similar manner also reinforced conviction since architecture imitates neural architecture though admittedly crude manner reasonable expect would meet greater success make improvement technique deal problem larger size using increasingly powerful computer fact prospect ha prime motive force behind work 18 issn ijcsis international journal computer science information security vol 6 no 1 2009 figure 1 convergning nn b diverging nn c mirroring neural network combining b figure illustration hierarchical invertible map adaline pe input x output input output x b x x c 19 issn ijcsis international journal computer science information security vol 6 no 1 2009 x lev el lev el ii z z z cjt lev el iii figure organized collection node block containing mnn corresponding forgy algorithm forming treelike hierarchical structure node x mnn x x forgy figure node block hierarchical classifier constructed mnn forgy clustering tandem algorithm devised paper pattern recognition task using hierarchical architecture depicted figure 3 may noted block hierarchical architecture trained implementation single common algorithm tandem algorithm tandem process done node two step step data reduction feature extraction mnn step classification extracted feature output mnn using clustering algorithm mnn first level train extract feature repeated presentation data sample extracted feature data sent clustering procedure classification module second level undergo tandem process feature extraction classification single common algorithm implemented throughout hierarchy module resulting unsupervised classification section 4 show method actually work apply classifier collage image flower face furniture collection automatically classified sub classified develop tandem algorithm actually implement writing computer program learning engine used classify pattern report result technique used development algorithm based upon application two procedure mirroring automatic feature extraction ii unsupervised classification tandem manner described following algorithm module block hierarchy computer program used two level hierarchy continuing discussion tandem algorithm consider figure 3 pictorial representation hierarchical architecture detail block node shown figure 4 structure mnn figure tandem algorithm proceeds block node block node level starting level see figure 3 tandem algorithm hierarchical classifier train mnn topmost block hierarchy see figure 3 ensemble sample input given mnn mirrored output minimum loss information suitable threshold fixed mark topmost node present node iterative step stop output figure almost equal input able reconstruct input due training mnn present node mnn could accurately reconstruct 95 input within specified threshold limit collection output mnn least dimensional hidden layer extracted number feature equal dimension mnn see figure considered classifying input present node feature extracted step 2 given input data set forgy clustering algorithm subroutine present node unsupervised classification explained step 4 step involved clustering procedure select initial seed point many number no class input divided input data set 20 issn ijcsis international journal computer science information security vol 6 no 1 2009 data sample input dataset repeat step b calculate distance sample input data set seed point representing cluster ii place input data sample group associated seed point closest least distance step 4 b sample clustered centroid cluster considered new seed point repeat step 4 b 4 c long data set leave one cluster join another step 4 b ii proceed repeat step 6 node next level hierarchy mark one node present node train mnn present node sample extracted feature immediate level belong particular cluster step 4 sample given mnn mirrored output minimum loss information threshold fixed trail error repeat step 2 3 4 repeat step 5 6 till no enough data present sample immediate level tandem algorithm feature extraction concurrent data reduction step 1 2 3 automatic data classification based reduced unit data step tandem process data reduction classification extended next lower level hierarchy ensemble step 5 6 till stated condition met step detail mnn architecture mnn training given 20 21 illustrate concept hierarchical architecture unsupervised classification using figure assume purpose illustration only 4 category image say face flower furniture tree j 4 broadest level trained 4 category image successful training reduce amount input data based reduced unit data categorizes pattern one class using forgy algorithm reduced unit represent input data pattern present node fed one next level level ii node alternatively input vector could fed appropriate mnn next level level ii instead reduced vector case large amount data reduction done present level level expected loss information required finer classification level ii selection node module next level depends upon classification input pattern present level example selected classifies input face else selected classifies input flower furniture tree respective node module level ii reduces input doe classification denote level ii classification based only reduced unit level ii gender classification distinguishes male face female face typical level ii classification pictorial representation level ii classification contains subcategories j category assuming some lower level identical level level ii containing node classify pattern instance reduced unit level ii given input one appropriate module level iii detailed classification example case k different person face group tandem procedure mirroring followed ii classification performed level extended lower level say level iv v proposed architecture doe unsupervised pattern learning recognition explained earlier hierarchical architecture implement common algorithm data reduction extracted feature classification node data reduction precedes classification accuracy classification dependent efficiency data reduction algorithm need evaluate performance mnn data reduction fact mnn dimensional reduction technique efficient method reduce irrelevant part data wa amply demonstrated extensive trial detail 20 21 used mnn along clustering algorithm data reduction feature extraction tool hierarchical pattern classification demonstration use forgy algorithm clustering reduced unit input module wherein number cluster provided user instead without prejudice generality technique one could use sophisticated clustering algorithm wherein number class cluster determined algorithm leave work part future enhancement would result completely automated unsupervised classification algorithm iv demonstration result show explicit construction hierarchical architecture actually built used classification image giving example case example took collection 360 image training equal no face see database feret 36 manchester 37 jaffe 38 reference table flower build two level classifier constructed mnns associated forgy clustering first level automatically classifies 360 image training set three class one would face class two belong table class flower class automatic procedure doe follows 4 layer mnn consisting input layer 676 input representing 26 x 26 image 60 processing element first layer 47 676 processing element 21 issn ijcsis international journal computer science information security vol 6 no 1 2009 two layer used train mnn mirror input data training done automatically stop output vector 676 dimension closely match corresponding input vector image point mnn said satisfactorily mirror 360 input image output layer least number processing element case 47 taken reduced feature vector corresponding input image would set 360 vector 47 dimension representing input data 360 image set 360 vector reduced dimension classified three class using forgy algorithm see 39 40 actual classification varies somewhat choice initial seed point randomly chosen program chooses three distinct initial random seed point us forgy algorithm cluster reduced vector input image clustering done many iteration till convergence class frozen data clustered second time starting second set seed point using forgy algorithm till another set three class obtained average resulting two nearest set cluster centroid considered new cluster centroid based reduced feature vector classified obtain three distinct class class treated final three class everything work well one would face class remaining two would table class flower class first level classification program proceeds next level three class identified level procedure reduction classification level ii similar carried level except three mnns trained one receiving input form face class another table class flower class mnns level ii use architecture two mnns suitably trained mirror respective input acceptable accuracy program proceeds classify input sub category mnns separately course time feature vector reduced vector ha 30 dimension forgy algorithm used following similar procedure described level except time classification done reduced vector would render sub category male face female face classification reduced vector obtaining subcategories centrally supported table four legged table classification reduced vector obtaining subcategories flower bud open flower mnns initiated random weight chosen initially choosing random seed point executing forgys algorithm intention demonstrate classification not overly dependent random choice ran program time starting ab initio taken 10 trial meaning 10 different training classification session level followed level ii average 10 trail considering training test set error level 7 average error three node level ii subcategorizing face male female table centrally supported flower flower bud open flower additional 7 actually not bad whole exercise unsupervised error made level classification remain undiscovered actually uncorrected classifier indiscriminately feed data second level input see sample illustration example figure summary result example given table various parameter used mnn training classification given table ii brute force obvious procedure training mnn node hierarchical classifier using raphston beyond capability pc available u wa not tried instead adopted approximate procedure trained mnns using back propagation algorithm 41 42 actually try determine best mnn changing weight presentation image ideally best mnn obtained entire ensemble input image reduced unit image mnn node would involve wa avoided technique used reported efficient term time space taken execution performed pc significance work conclusion paper proved crucial theorem called mirroring theorem based mathematical proof theorem developed architecture hierarchical classifier implement proposed tandem algorithm perform unsupervised pattern recognition also specifically written computer code demonstrate technique building classifier applied example classifier characteristic hierarchical modular unsupervised run single common algorithm therefore mimic admittedly crude manner collective behavior neuron expected expanded analyze much complex data super classifier could employ many structure type shown figure 3 working parallel experimentation within available resource found not possible many class first level figure 3 j not large value best j 4 therefore large problem involving many class need network structure type shown figure 3 j limited 2 3 4 working parallel structure trained recognize set class eg face class alphabet class thus binary tertiary tree node structure type shown figure 3 envisaged construction super classifier 22 issn ijcsis international journal computer science information security vol 6 no 1 2009 using forgy algorithm 676 input variable using forgy 47 reduced dimensional unit input using forgy 47 reduced dimensional unit input using forgy 47 reduced dimensional unit input figure 5 pictorial representation hierarchical classifier implemented using example image face flower table classification level based 47 reduced dimensional vector input image male face female face flower bud open flower centrally supported table table level ii based 30 reduced dimensional vector image table result hierarchical classifier example image success rate averaged 10 trail clustering reduced unit input type dimension input dimension reduced unit no sample training no sample testing no category training sample average training test set image 676 26 x 26 47 360 150 3 face table flower efficiency level node efficiency level node reduced unit image 47 30 120 category 50 category 2 category average efficiency level ii node age efficiency level ii node table ii various parameter used mnn forgy algorithm type mnn architecture distance input output seed point forgy algorithm learning rate parameter weight bias term level mnn threshold random seed point random selection level ii mnns threshold any two random seed point random selection 23 issn ijcsis international journal computer science information security vol 6 no 1 2009 expected technique developed presented paper implemented many future researcher building advanced highly sophisticated pattern classifier also hoped procedure also used building model associative memory 43 say voice signal eg mary spoken word associated picture image mary development could near future lead versatile machine learning system possibly ape human brain least elemental function acknowledgment thank management srinidhi group aurora educational institution encouragement altech imaging computing providing facility research reference 1 buchanan role experimentation phil trans soc vol 349 pp 1994 2 zucker grounded theory abstraction artificial intelligence phil trans soc b vol 358 pp 2003 3 holte choueiry abstraction reformulation phil trans roy vol 358 pp 2003 4 cruze durr schmitz insect walking based deetralized architecture revealing simple robust controller phil tran soc 365 pp 2007 5 freund schapire experiment new boosting algorithm proc international conference machine learning pp 1996 6 viola jones rapid object detection using boosted cascade simple feature proc ieee computer society conference computer vision pattern recognition vol 1 pp 2001 7 hinton salakhutdinov reducing dimensionality data neural network science vol 313 pp 2006 8 law clustering dimensionality reduction side information ph thesis michigan state university 2006 9 joachim text categorization support vector machine learning many relevant feature proc european conference machine learning pp 1998 10 craven dipasquo freitag mccallum mitchell learning construct knowledge base world wide artificial intelligence vol 118 pp 2000 11 garcia convolutional face finder neural architecture fast robust face detection ieee trans pattern anal mach intell vol 26 pp 2004 12 phung bouzerdoum pyramidal neural network visual pattern recognition ieee transaction neural network vol 18 pp 2007 13 rosenblum yacoob davis human expression recognition motion using radial basis function network architecture ieee trans neural network vol 7 pp 1996 14 baldi harnik neural network principal component analysis learning example without local minimum neural network vol 2 pp 1989 15 demers cottrell dimensionality reduction advance neural information processing system vol 5 morgan kaufmann pp 1993 16 hopfield brody learning rule network repair computation network proc natl acad sci vol 101 pp 2004 17 lau stanley dan computational subunit visual cortical neuron revealed artificial neural network proc nat acad sci usa vol 99 pp 2002 18 eswaran system method identifying pattern patent filed indian patent office also patent trade mark office vide no 20080019595 20080232682 respectively 2006 19 kolmogorov representation continuous function several variable superposition continuous function one variable addition doklady akademia nauk sssr 114 5 pp 956 1957 20 deepthi kuchibhotla eswaran dimensionality reduction reconstruction using mirroring neural network object recognition based reduced dimension characteristic vector ieee international conference advance computer vision information technology ieee pp 2007 21 deepthi automatic pattern recognition application image processing robotics ph thesis osmania university hyderabad india 2009 22 creutzfeldt generality functional structure neocortex naturwissenschaften vol 64 1977 23 mountcastle organizing principle cerebral function unit model distributed system mindful brain edelman mountcastle cambridge mass mit press 1978 24 felleman van essen distributed hierarchical processing primate cerebral cortex cerebral cortex vol 1 pp 1991 25 rao ballard predictive coding visual cortex functional interpretation some effect nature neuroscience vol 2 pp 1999 26 sherman guillery role thalamus ow information cortex phil trans roy soc london vol 357 pp 2002 27 kaiser brain architecture design natural computation phil trans roy soc vol 365 pp 2007 28 buzsaki geisler henze wang interneuron diversity series circuit complexity axon wiring economy cortical interneurons trend neurosci vol 27 pp 2004 29 johnsen santhakumar morgan huerta tsimring soltesz topological determinant epileptogenesis scale structural functional model dentate gyrus derived experimental data vol 97 2007 30 van essen anderson felleman information processing primate visual system integrated system perspective science vol 255 1992 31 hawkins intelligence owl book henry holt new york pp 2005 32 bell level loop future artificial intelligence neuroscience phil trans soc b vol 354 1994 24 issn ijcsis international journal computer science information security vol 6 no 1 2009 33 hawkins george hierarchical temporal memory concept theory terminology numenta numenta inc pp 2007 34 george brain might work hierarchical temporal model learning recognition ph thesis stanford university 2008 35 herrero valencia dopazo hierarchical unsupervised growing neural network clustering gene expression pattern bioinformatics vol 17 pp 2001 36 feret database 37 manchester database 38 jaffe database 39 gose johnsonbaugh jost pattern recognition image analysis prentice hall india new delhi pp 2000 40 deepthi krishna eswaran automatic pattern classification unsupervised learning using dimensionality reduction data mirroring neural network ieee international conference advance computer vision information technology ieee 2007 41 rumelhart hinton williams learning representation error nature vol 323 1986 42 widrow lehr 30 year adaptive neural network perceptron madaline backpropagation proceeding ieee 78 9 1990 43 deepthi eswaran pattern recognition memory mapping using mirroring neural network ieee international conference emerging trend computing ieee icetic 2009 india author profile author 1 working associate professor cse dept aurora engineering college submitted ph cse thesis automatic pattern recognition application image processing robotics osmania university hyderabad tech software engineering university hyderabad author 2 working professor cse dept srinidhi institute science technology ph mathematical physic phase coherence quantum system university madras 1973 36 year research experience application computer area industrial image processing pattern recognition neural network electromagnetics fluid mechanic structural mechanic artificial intelligence ha 40 paper international journal international conference subject 25 issn