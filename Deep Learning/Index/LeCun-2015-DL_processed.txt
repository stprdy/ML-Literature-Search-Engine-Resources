ai research 770 broadway new york new york 10003 usa york university 715 broadway new york new york 10003 usa computer science operation research université de montréal pavillon po box 6128 stn montréal quebec canada 1600 amphitheatre parkway mountain view california 94043 usa computer science university toronto 6 king college road toronto ontario canada technology power many aspect modern society web search content filtering social work recommendation website increasingly present consumer product camera smartphones system used identify object image transcribe speech text match news item post product user interest select relevant result search increasingly application make use class technique called deep learning conventional technique limited ability process natural data raw form decade structing system required careful engineering considerable domain expertise design ture extractor transformed raw data pixel value image suitable internal representation feature vector learning subsystem often classifier could detect classify pattern input representation learning set method allows machine fed raw data automatically discover representation needed detection classification method method multiple level tion obtained composing simple module transform representation one level starting raw input representation higher slightly abstract level composition enough transformation complex function learned classification task higher layer representation amplify aspect input important discrimination suppress irrelevant variation image example come form array pixel value learned feature first layer representation typically represent presence absence edge particular orientation location image second layer typically detects motif spotting particular arrangement edge regardless small variation edge position third layer may assemble motif larger combination correspond part familiar object subsequent layer would detect object combination part key aspect deep learning layer feature not designed human engineer learned data using learning procedure deep learning making major advance solving problem resisted best attempt artificial intelligence nity many year ha turned good discovering intricate structure data therefore ble many domain science business government addition beating record image speech ha beaten technique predicting ity potential drug analysing particle accelerator reconstructing brain predicting effect mutation dna gene expression perhaps surprisingly deep learning ha produced extremely promising result various task natural language particularly topic classification sentiment analysis question guage think deep learning many success near future requires little engineering hand easily take advantage increase amount available putation data new learning algorithm architecture currently developed deep neural network only ate progress supervised learning common form machine learning deep not vised learning imagine want build system classify image containing say house car person pet first collect large data set image house car people pet labelled category training machine shown image produce output form vector score one category want desired category highest score category unlikely happen training compute objective function measure error tance output score desired pattern score machine modifies internal adjustable parameter reduce error adjustable parameter often called weight real number seen knob define tion machine typical system may hundred million adjustable weight hundred million labelled example train machine properly adjust weight vector learning algorithm putes gradient vector weight indicates amount error would increase decrease weight increased tiny amount weight vector adjusted opposite tion gradient vector objective function averaged training example deep learning allows computational model composed multiple processing layer learn representation data multiple level abstraction method dramatically improved speech ognition visual object recognition object detection many domain drug discovery genomics deep learning discovers intricate structure large data set using backpropagation algorithm indicate machine change internal parameter used compute representation layer representation previous layer deep convolutional net brought breakthrough processing image video speech audio whereas recurrent net shone light sequential data text speech deep learning yann yoshua geoffrey 4 3 6 n u r e v l 5 2 1 2 8 2 0 1 5 review 2015 macmillan publisher limited right reserved seen kind hilly landscape space weight value negative gradient vector indicates direction steepest descent landscape taking closer minimum output error low average practice practitioner use procedure called stochastic gradient descent sgd consists showing input vector example computing output error computing average gradient example adjusting weight accordingly process repeated many small set example training set average objective function stop decreasing called stochastic small set example give noisy estimate average gradient example simple procedure usually find good set weight surprisingly quickly compared far elaborate optimization training performance system measured different set example called test set serf test generalization ability machine ability produce sensible answer new input ha never seen training many current practical application machine learning use linear classifier top feature linear classifier computes weighted sum feature vector component weighted sum threshold input classified belonging particular category since known linear classifier only carve input space simple region namely rated problem image speech nition require function insensitive irrelevant variation input variation position orientation illumination object variation pitch accent speech sensitive particular minute variation example difference white wolf breed white dog called samoyed pixel level image two samoyed different pose different environment may different whereas two image samoyed wolf position similar background may similar linear classifier any shallow classifier operating figure 1 multilayer neural network backpropagation layer neural network shown connected dot distort input space make class data example red blue line linearly separable note regular grid shown left input space also transformed shown middle panel hidden unit illustrative example only two input unit two hidden unit one output unit network used object recognition natural language processing contain ten hundred thousand unit reproduced permission olah b chain rule derivative tell u two small effect small change x z composed small change δx x get transformed first small change δy getting multiplied definition partial derivative similarly change δy creates change δz substituting one equation give chain rule derivative δx get turned δz multiplication product also work x z vector derivative jacobian matrix c equation used computing forward pas neural net two hidden layer one output layer constituting module one backpropagate gradient layer first compute total input z unit weighted sum output unit layer function f applied z get output unit simplicity omitted bias term function used neural network include rectified linear unit relu f z max 0 z commonly used recent year well conventional sigmoids hyberbolic tangent f z exp z exp exp z exp logistic function logistic f z 1 exp equation used computing backward pas hidden layer compute error derivative respect output unit weighted sum error derivative respect total input unit layer convert error derivative respect output error derivative respect input multiplying gradient f z output layer error derivative respect output unit computed differentiating cost function give yl tl cost function unit l yl tl 2 tl target value known weight wjk connection unit j layer yj input 2 output 1 sigmoid hidden 2 sigmoid b c x x z x z z z δ δ δ δ δ δ z z x x x z z x x compare output correct answer get error derivative j k e yl tl e zl e yl yl zl l e yj wjk e zk e zj e yj yj zj e yk wkl e zl e zk e yk yk zk wkl wjk wij j k yl f zl zl wkl yk l yj f zj zj wij xi yk f zk zk wjk yj output unit input unit hidden unit hidden unit wkl wjk wij k k j input 2 8 2 0 1 5 v l 5 2 1 n u r e 4 3 7 review insight 2015 macmillan publisher limited right reserved raw pixel could not possibly distinguish latter two putting former two category shallow classifier require good feature extractor solves dilemma one produce representation selective aspect image important discrimination invariant irrelevant aspect pose animal make classifier powerful one use generic feature kernel generic feature arising gaussian kernel not allow learner ize well far training conventional option hand design good feature extractor requires able amount engineering skill domain expertise avoided good feature learned automatically using learning procedure key advantage deep learning architecture multilayer stack simple ules subject learning many compute mapping module stack transforms input increase selectivity invariance representation multiple layer say depth 5 20 system implement extremely intricate tions input simultaneously sensitive minute detail distinguishing samoyed white wolf insensitive large irrelevant variation background pose lighting surrounding object backpropagation train multilayer architecture earliest day pattern aim er ha replace feature trainable multilayer network despite simplicity solution wa not widely understood mid turn multilayer architecture trained simple stochastic gradient descent long module relatively smooth function input internal weight one compute gradient using backpropagation procedure idea could done worked wa discovered independently several different group backpropagation procedure compute gradient objective function respect weight multilayer stack module nothing practical application chain rule derivative key insight derivative ent objective respect input module computed working backwards gradient respect output module input subsequent module 1 backpropagation equation applied repeatedly propagate gradient module starting output top network produce prediction way bottom external input fed gradient computed straightforward compute gradient respect weight module many application deep learning use feedforward neural work architecture fig 1 learn map input example image output example ability several category go one layer next set unit compute weighted sum input previous layer pas result function present popular function rectified linear unit relu simply rectifier f z max z 0 past decade neural net used smoother tanh z 1 exp relu typically learns much faster network many layer allowing training deep supervised network without unsupervised unit not input output layer conventionally called hidden unit hidden layer seen distorting input way category become linearly separable last layer 1 late neural net backpropagation largely forsaken community ignored community wa widely thought learning useful multistage feature extractor tle prior knowledge wa infeasible particular wa commonly thought simple gradient descent would get trapped poor local minimum weight configuration no small change would reduce average error practice poor local minimum rarely problem large work regardless initial condition system nearly always reach solution similar quality recent theoretical empirical result strongly suggest local minimum not serious issue general instead landscape packed rially large number saddle point gradient zero surface curve dimension curve figure 2 inside convolutional network output not filter layer horizontally typical convolutional network architecture applied image samoyed dog bottom left rgb red green blue input bottom right rectangular image feature map corresponding output one learned feature detected image position information flow bottom feature acting oriented edge detector score computed image class output relu rectified linear unit red green blue samoyed 16 papillon pomeranian arctic fox eskimo dog white wolf siberian husky convolution relu max pooling max pooling convolution relu convolution relu 4 3 8 n u r e v l 5 2 1 2 8 2 0 1 5 review insight 2015 macmillan publisher limited right reserved analysis seems show saddle point only downward curving direction present large number almost similar value tive function hence doe not much matter saddle point algorithm get stuck interest deep feedforward network wa revived around 2006 ref group researcher brought together dian institute advanced research cifar researcher duced unsupervised learning procedure could create layer feature detector without requiring labelled data objective learning layer feature detector wa able reconstruct model activity feature detector raw input layer several layer progressively complex feature detector using reconstruction objective weight deep network could initialized sensible value final layer output unit could added top network whole deep system could using standard worked remarkably well recognizing handwritten digit detecting pedestrian especially amount labelled data wa first major application approach wa speech recognition wa made possible advent fast graphic processing unit gpus convenient allowed researcher train network 10 20 time faster 2009 approach wa used map short temporal window ficients extracted sound wave set probability various fragment speech might represented frame centre window achieved result standard speech recognition benchmark used small wa quickly developed give result large vocabulary 2012 version deep net 2009 developed many major speech already deployed android phone smaller data set unsupervised help prevent leading significantly better generalization number labelled ples small transfer setting lot example some source task some target task deep learning rehabilitated turned stage wa only needed small data set wa however one particular type deep feedforward work wa much easier train generalized much better network full connectivity adjacent layer wa convolutional neural network convnet achieved many practical success period neural network favour ha recently widely adopted vision community convolutional neural network convnets designed process data come form multiple array example colour image composed three array containing pixel intensity three colour channel many data modality form multiple array signal sequence including language image audio spectrogram video volumetric image four key idea behind convnets take advantage property natural signal local connection shared weight pooling use many layer architecture typical convnet fig 2 structured series stage first stage composed two type layer convolutional layer pooling layer unit tional layer organized feature map within unit connected local patch feature map previous layer set weight called filter bank result local weighted sum passed relu unit feature map share filter bank ent feature map layer use different filter bank reason architecture twofold first array data image local group value often highly correlated forming distinctive local motif easily detected second local statistic image signal invariant location word motif appear one part image could appear anywhere hence idea unit different location sharing weight detecting pattern different part array cally filtering operation performed feature map discrete convolution hence name although role convolutional layer detect local junction feature previous layer role pooling layer merge semantically similar feature one relative position feature forming motif vary somewhat reliably detecting motif done tion feature typical pooling unit computes maximum local patch unit one feature map feature map neighbouring pooling unit take input patch shifted one row column thereby reducing dimension representation creating invariance small shift tortions two three stage convolution ing stacked followed convolutional layer backpropagating gradient convnet simple regular deep network allowing weight filter bank trained deep neural network exploit property many natural nals compositional hierarchy feature obtained composing one image local nation edge form motif motif assemble part part form object similar hierarchy exist speech text sound phone phoneme syllable word sentence pooling allows representation vary little element ous layer vary position appearance convolutional pooling layer convnets directly inspired classic notion simple cell complex cell visual overall architecture reminiscent hierarchy visual cortex ventral convnet model monkey shown ture activation unit convnet explains half variance random set 160 neuron monkey otemporal convnets root architecture wa somewhat similar not algorithm backpropagation primitive convnet called neural net wa used recognition phoneme simple numerous application convolutional work going back early starting ral network speech document document reading system used convnet trained jointly probabilistic model implemented language constraint late system wa reading 10 cheque united state number optical character nition handwriting recognition system later deployed convnets also experimented early object detection natural image including face face image understanding deep convolutional network since early convnets applied great success detection segmentation recognition object region image task labelled data wa relatively dant traffic sign segmentation biological particularly detection face text pedestrian human body natural major recent practical success convnets face importantly image labelled pixel level application technology including autonomous mobile robot 2 8 2 0 1 5 v l 5 2 1 n u r e 4 3 9 review insight 2015 macmillan publisher limited right reserved company mobileye nvidia using method upcoming vision tems car application gaining importance involve natural language speech despite success convnets largely forsaken mainstream community imagenet competition deep convolutional network applied data set million image web contained different class achieved tacular result almost halving error rate best ing success came efficient use gpus relus new regularization technique called niques generate training example deforming existing one success ha brought revolution computer vision convnets dominant approach almost recognition detection approach human performance some task recent stunning demonstration combine convnets recurrent net module generation image caption 3 recent convnet architecture 10 20 layer relus dreds million weight billion connection unit whereas training large network could taken week only two year ago progress hardware software algorithm parallelization reduced training time hour performance vision system ha caused major technology company including google facebook microsoft ibm yahoo twitter adobe well quickly growing number initiate research development project deploy image understanding product service convnets easily amenable efficient hardware tations chip gate number company nvidia mobileye intel qualcomm samsung developing convnet chip enable vision application smartphones camera robot car distributed representation language processing theory show deep net two different nential advantage classic learning algorithm not use distributed advantage arise power composition depend underlying distribution appropriate componential first learning distributed representation enable generalization new combination value learned feature beyond seen training example combination possible n binary feature second composing layer representation deep net brings potential another exponential exponential depth hidden layer multilayer neural network learn sent network input way make easy predict target output nicely demonstrated training multilayer neural network predict next word sequence local figure 3 image text caption generated recurrent neural network rnn taking extra input representation extracted deep convolution neural network cnn test image rnn trained translate representation image caption top reproduced permission ref rnn given ability focus attention different location input image middle bottom lighter patch given attention generates word bold exploit achieve better translation image caption vision deep cnn language generating rnn group people shopping outdoor market many vegetable fruit stand woman throwing frisbee park little girl sitting bed teddy bear group people sitting boat water girafe standing forest tree background dog standing hardwood foor stop sign road mountain background 4 4 0 n u r e v l 5 2 1 2 8 2 0 1 5 review insight 2015 macmillan publisher limited right reserved context earlier word context presented network vector one component ha value 1 rest first layer word creates different pattern activation word vector 4 language model layer network learn convert input word tor output word vector predicted next word used predict probability any word vocabulary appear next word network learns word vector contain many active component interpreted separate feature word wa first context learning distributed representation symbol semantic feature not explicitly present input discovered learning procedure good way factorizing structured relationship input output symbol multiple learning word vector turned also work well word sequence come large corpus real text individual trained predict next word news story example learned word vector tuesday wednesday similar word vector sweden norway representation called distributed representation element feature not mutually exclusive many configuration correspond variation seen observed data word vector composed learned feature not determined ahead time expert automatically discovered neural network vector representation word learned text widely used natural language issue representation lie heart debate paradigm cognition paradigm instance symbol something only property either identical symbol instance ha no internal structure relevant use reason symbol must bound variable judiciously chosen rule inference contrast neural network use big activity vector big weight matrix scalar perform type fast tive inference underpins effortless commonsense reasoning introduction neural language standard approach statistical modelling language not exploit uted representation wa based counting frequency rences short symbol sequence length n called number possible order vn v vocabulary size taking account context handful word would require large training corpus treat word atomic unit not generalize across semantically related sequence word whereas neural language model associate word vector real valued feature semantically related word end close vector space 4 recurrent neural network backpropagation wa first introduced exciting use wa training recurrent neural network rnns task involve sequential input speech language often better use rnns fig 5 rnns process input sequence one element time maintaining hidden unit state vector implicitly contains information history past element sequence consider output hidden unit different discrete time step output different neuron deep multilayer network fig 5 right becomes clear apply backpropagation train rnns rnns powerful dynamic system training ha proved problematic backpropagated gradient either grow shrink time step many time step typically explode thanks advance way training rnns found good predicting next character next word also used complex task example reading english sentence one word time english encoder network trained final state vector hidden unit good representation thought expressed sentence thought vector used initial hidden state extra input jointly trained french decoder network output ability distribution first word french translation particular first word chosen distribution provided input decoder network output probability tribution second word translation full stop overall process generates sequence french word according probability distribution depends english sentence rather naive way performing machine translation ha quickly become competitive raise serious doubt whether understanding tence requires anything like internal symbolic expression manipulated using inference rule compatible view everyday reasoning involves many simultaneous analogy figure 4 visualizing learned word vector left illustration word representation learned modelling language projected visualization using right representation phrase learned recurrent neural one observe semantically similar word sequence word mapped nearby representation distributed representation word obtained using backpropagation jointly learn representation word function predicts target quantity next word sequence language modelling whole sequence translated word machine translation 9 10 11 12 13 14 community organization institution society industry company organization school company community ofce agency community association body school agency past month day last day past day month coming month month ago quot two group two group last month dispute two last two decade next six month two month nearly two month last two decade within month 2 8 2 0 1 5 v l 5 2 1 n u r e 4 4 1 review insight 2015 macmillan publisher limited right reserved contribute plausibility instead translating meaning french sentence english sentence one learn translate meaning image english sentence fig 3 encoder deep vnet convert pixel activity vector last hidden layer decoder rnn similar one used machine translation neural language modelling ha surge interest system recently see example mentioned ref 86 rnns unfolded time fig 5 seen deep feedforward network layer share weight although main purpose learn dependency theoretical empirical evidence show difficult learn store information correct one idea augment network explicit memory first proposal kind long memory lstm network use special hidden unit natural behaviour remember input long special unit called memory cell act like accumulator gated leaky neuron ha connection next time step ha weight one copy state accumulates external signal multiplicatively gated another unit learns decide clear content memory lstm network subsequently proved effective conventional rnns especially several layer time enabling entire speech recognition system go way acoustic sequence character transcription lstm network related form gated unit also currently used encoder decoder network perform well machine past year several author made different proposal augment rnns memory module proposal include neural turing machine network augmented memory rnn choose read write memory network regular network augmented kind associative memory network yielded lent performance standard benchmark memory used remember story network later asked answer question beyond simple memorization neural turing machine ory network used task would normally require reasoning symbol manipulation neural turing machine taught algorithm among thing learn output sorted list symbol input consists unsorted sequence symbol accompanied real value indicates priority memory network trained keep track state world setting similar text adventure game reading story answer question require complex one test example network shown version lord ring correctly answer question frodo 89 future deep learning unsupervised catalytic effect reviving interest deep learning ha since overshadowed success purely supervised learning although not focused review expect unsupervised learning become far important longer term human animal learning largely unsupervised discover structure world observing not told name every object human vision active process sequentially sample optic array intelligent way using small fovea large surround expect much future progress vision come system trained end combine convnets rnns use reinforcement learning decide look system combining deep learning forcement learning infancy already outperform passive vision classification task produce impressive result learning play many different video natural language understanding another area deep ing poised make large impact next year expect system use rnns understand sentence whole document become much better learn strategy selectively attending one part ultimately major progress artificial intelligence come system combine representation learning complex reasoning although deep learning simple reasoning used speech handwriting recognition long time new paradigm needed replace manipulation symbolic expression operation large received 25 february accepted 1 may 2015 krizhevsky sutskever hinton imagenet classification deep convolutional neural network proc advance neural information processing system 25 2012 report wa breakthrough used convolutional net almost halve error rate object recognition precipitated rapid adoption deep learning computer vision community farabet couprie najman lecun learning hierarchical feature scene labeling ieee trans pattern anal mach intell 35 2013 tompson jain lecun bregler joint training convolutional network graphical model human pose estimation proc advance neural information processing system 27 2014 szegedy et al going deeper convolution preprint 2014 mikolov deoras povey burget cernocky strategy training large scale neural network language model proc automatic speech recognition understanding 2011 hinton et al deep neural network acoustic modeling speech recognition ieee signal processing magazine 29 2012 joint paper major speech recognition laboratory summarizing breakthrough achieved deep learning task phonetic classification automatic speech recognition wa first major industrial application deep learning sainath mohamed kingsbury b ramabhadran deep convolutional neural network lvcsr proc acoustic speech signal processing 2013 sheridan liaw dahl svetnik deep neural net method quantitative relationship chem inf model 55 2015 ciodaro deva de seixas j damazio online particle detection neural network based topological calorimetry information phys conf series 368 012030 2012 kaggle higgs boson machine learning challenge kaggle 2014 helmstaedter et al connectomic reconstruction inner plexiform layer mouse retina nature 500 2013 xt x unfold v w w w w w v v v u u u u ot st figure 5 recurrent neural network unfolding time computation involved forward computation artificial neuron example hidden unit grouped node value st time get input neuron previous time step represented black square representing delay one time step left way recurrent neural network map input sequence element xt output sequence element ot ot depending previous xtʹ tʹ parameter matrix u v w used time step many architecture possible including variant network generate sequence output example word used input next time step backpropagation algorithm fig 1 directly applied computational graph unfolded network right compute derivative total error example generating right sequence output respect state st parameter 4 4 2 n u r e v l 5 2 1 2 8 2 0 1 5 review insight 2015 macmillan publisher limited right reserved leung xiong lee j frey deep learning regulated splicing code bioinformatics 30 2014 xiong et al human splicing code reveals new insight genetic determinant disease science 347 6218 2015 collobert et al natural language processing almost scratch mach learn 12 2011 bordes chopra weston question answering subgraph embeddings proc empirical method natural language processing http 2014 jean cho memisevic bengio using large target vocabulary neural machine translation proc 2015 sutskever vinyals le sequence sequence learning neural network proc advance neural information processing system 27 2014 paper showed machine translation result architecture introduced ref 72 recurrent network trained read sentence one language produce semantic representation meaning generate translation another language bottou bousquet tradeoff large scale learning proc advance neural information processing system 20 2007 duda hart pattern classiﬁcation scene analysis wiley 1973 schölkopf b smola learning kernel mit press 2002 bengio delalleau le roux curse highly variable function local kernel machine proc advance neural information processing system 18 2005 selfridge pandemonium paradigm learning mechanisation thought process proc symposium mechanisation thought process 1958 rosenblatt perceptron perceiving recognizing automaton tech cornell aeronautical laboratory 1957 werbos beyond regression new tool prediction analysis behavioral science phd thesis harvard univ 1974 parker learning logic report mit press 1985 lecun une procédure apprentissage pour réseau à seuil assymétrique cognitiva 85 la frontière de l intelligence artiﬁcielle de science de la connaissance et de neuroscience french 1985 rumelhart hinton williams learning representation error nature 323 1986 glorot bordes bengio deep sparse rectiﬁer neural network proc international conference artificial intelligence statistic 2011 paper showed supervised training deep neural network much faster hidden layer composed relu dauphin et al identifying attacking saddle point problem dimensional optimization proc advance neural information processing system 27 2014 choromanska henaff mathieu arous b lecun loss surface multilayer network proc conference ai statistic http 2014 hinton kind graphical model brain proc international joint conference artificial intelligence 2005 hinton osindero teh fast learning algorithm deep belief net neural comp 18 2006 paper introduced novel effective way training deep neural network one hidden layer time using unsupervised learning procedure restricted boltzmann machine bengio lamblin popovici larochelle greedy training deep network proc advance neural information processing system 19 2006 report demonstrated unsupervised method introduced ref 32 significantly improves performance test data generalizes method unsupervised technique ranzato poultney chopra lecun efﬁcient learning sparse representation model proc advance neural information processing system 19 2006 hinton salakhutdinov reducing dimensionality data neural network science 313 2006 sermanet kavukcuoglu chintala lecun pedestrian detection unsupervised feature learning proc international conference computer vision pattern recognition 2013 raina madhavan ng deep unsupervised learning using graphic processor proc annual international conference machine learning 2009 mohamed dahl hinton acoustic modeling using deep belief network ieee trans audio speech lang process 20 2012 dahl yu deng acero deep neural network large vocabulary speech recognition ieee trans audio speech lang process 20 2012 bengio courville vincent representation learning review new perspective ieee trans pattern anal machine intell 35 2013 lecun et al handwritten digit recognition network proc advance neural information processing system 1990 first paper convolutional network trained backpropagation task classifying image handwritten digit lecun bottou bengio haffner learning applied document recognition proc ieee 86 1998 overview paper principle training modular system deep neural network using optimization showed neural network particular convolutional net combined search inference mechanism model complex output interdependent sequence character associated content document hubel wiesel receptive ﬁelds binocular interaction functional architecture cat visual cortex physiol 160 1962 felleman j essen distributed hierarchical processing primate cerebral cortex cereb cortex 1 1991 cadieu et al deep neural network rival representation primate cortex core visual object recognition plo comp biol 10 2014 fukushima miyake neocognitron new algorithm pattern recognition tolerant deformation shift position pattern recognition 15 1982 waibel hanazawa hinton shikano lang phoneme recognition using neural network ieee trans acoustic speech signal process 37 1989 bottou blanchet lienard experiment time delay network dynamic time warping speaker independent isolated digit recognition proc eurospeech 89 1989 simard steinkraus platt best practice convolutional neural network proc document analysis recognition 2003 vaillant monrocq lecun original approach localisation object image proc vision image signal processing 141 1994 nowlan platt neural information processing system 1995 lawrence giles tsoi back face recognition convolutional approach ieee trans neural network 8 1997 ciresan meier masci j schmidhuber deep neural network trafﬁc sign classiﬁcation neural network 32 2012 ning et al toward automatic phenotyping developing embryo video ieee trans image process 14 2005 turaga et al convolutional network learn generate afﬁnity graph image segmentation neural comput 22 2010 garcia delakis convolutional face ﬁnder neural architecture fast robust face detection ieee trans pattern anal machine intell 26 2004 osadchy lecun miller synergistic face detection pose estimation model mach learn 8 2007 tompson goroshin jain lecun bregler efﬁcient object localization using convolutional network proc conference computer vision pattern recognition 2014 taigman yang ranzato wolf deepface closing gap performance face veriﬁcation proc conference computer vision pattern recognition 2014 hadsell et al learning vision autonomous driving field robot 26 2009 farabet couprie najman lecun scene parsing multiscale feature learning purity tree optimal cover proc international conference machine learning 2012 srivastava hinton krizhevsky sutskever salakhutdinov dropout simple way prevent neural network overﬁtting machine learning 15 2014 sermanet et al overfeat integrated recognition localization detection using convolutional network proc international conference learning representation 2014 girshick donahue darrell malik rich feature hierarchy accurate object detection semantic segmentation proc conference computer vision pattern recognition 2014 simonyan zisserman deep convolutional network image recognition proc international conference learning representation 2014 boser sackinger bromley lecun jackel analog neural network processor programmable topology solid state circuit 26 1991 farabet et al convolutional network scaling machine learning parallel distributed approach ed bekkerman bilenko langford j cambridge univ press 2011 bengio learning deep architecture ai 2009 montufar morton doe mixture product contain product mixture discrete math 29 2014 montufar pascanu cho bengio number linear region deep neural network proc advance neural information processing system 27 2014 bengio ducharme vincent neural probabilistic language model proc advance neural information processing system 13 2001 paper introduced neural language model learn convert word symbol word vector word embedding composed learned semantic feature order predict next word sequence cho et al learning phrase representation using rnn 2 8 2 0 1 5 v l 5 2 1 n u r e 4 4 3 review insight 2015 macmillan publisher limited right reserved statistical machine translation proc conference empirical method natural language processing 2014 schwenk continuous space language model computer speech lang 21 2007 socher lin c manning ng parsing natural scene natural language recursive neural network proc international conference machine learning 2011 mikolov sutskever chen corrado dean distributed representation word phrase compositionality proc advance neural information processing system 26 2013 bahdanau cho bengio neural machine translation jointly learning align translate proc international conference learning representation 2015 hochreiter untersuchungen zu dynamischen neuronalen netzen german diploma thesis münich 1991 bengio simard frasconi learning dependency gradient descent difﬁcult ieee trans neural network 5 1994 hochreiter schmidhuber long memory neural comput 9 1997 paper introduced lstm recurrent network become crucial ingredient recent advance recurrent network good learning dependency elhihi bengio hierarchical recurrent neural network dependency proc advance neural information processing system 8 1995 sutskever training recurrent neural network phd thesis univ toronto 2012 pascanu mikolov bengio difﬁculty training recurrent neural network proc international conference machine learning 1318 2013 sutskever marten j hinton generating text recurrent neural network proc international conference machine learning 1024 2011 lakoff johnson metaphor live univ chicago press 2008 rogers mcclelland semantic cognition parallel distributed processing approach mit press 2004 xu et al show attend tell neural image caption generation visual attention proc international conference learning representation http 2015 graf mohamed hinton speech recognition deep recurrent neural network proc international conference acoustic speech signal processing 2013 graf wayne danihelka neural turing machine 2014 weston chopra bordes memory network 2014 weston bordes chopra mikolov towards question answering set prerequisite toy task 2015 hinton dayan frey j neal algorithm unsupervised neural network science 268 1995 salakhutdinov hinton deep boltzmann machine proc international conference artificial intelligence statistic 2009 vincent larochelle bengio manzagol extracting composing robust feature denoising autoencoders proc international conference machine learning 2008 kavukcuoglu et al learning convolutional feature hierarchy visual recognition proc advance neural information processing system 23 2010 gregor lecun learning fast approximation sparse coding proc international conference machine learning 2010 ranzato mnih susskind hinton modeling natural image using gated mrfs ieee trans pattern anal machine intell 35 2013 bengio alain yosinski deep generative stochastic network trainable backprop proc international conference machine learning 2014 kingma rezende mohamed welling learning deep generative model proc advance neural information processing system 27 2014 ba mnih kavukcuoglu multiple object recognition visual attention proc international conference learning representation http 2014 mnih et al control deep reinforcement learning nature 518 2015 bottou machine learning machine reasoning mach learn 94 2014 vinyals toshev bengio erhan show tell neural image caption generator proc international conference machine learning http 2014 van der maaten hinton visualizing data using mach learn research 9 2008 acknowledgement author would like thank natural science engineering research council canada canadian institute advanced research cifar national science foundation office naval research support cifar fellow author information reprint permission information available author declare no competing financial interest reader welcome comment online version paper correspondence addressed yann 4 4 4 n u r e v l 5 2 1 2 8 2 0 1 5 review insight 2015 macmillan publisher limited right reserved