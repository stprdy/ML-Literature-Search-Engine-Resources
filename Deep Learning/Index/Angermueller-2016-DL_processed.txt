review deep learning computational biology christof tanel leopold oliver abstract technological advance genomics imaging led explosion molecular cellular profiling data large number sample rapid increase biological data sion acquisition rate challenging conventional analysis strategy modern machine learning method deep ing promise leverage large data set finding hidden structure within making accurate prediction review discus application new breed analysis approach regulatory genomics cellular imaging provide background deep learning setting successfully applied derive biological insight addition presenting specific application providing tip practical use also highlight possible pitfall limitation guide computational biologist make use new technology keywords cellular imaging computational biology deep learning machine learning regulatory genomics doi received 11 april 2016 revised 2 june 2016 accepted 6 june 2016 mol syst biol 2016 12 878 introduction machine learning method approach learn functional relationship data without need define priori hastie et al 2005 murphy 2012 michalski et al 2013 computational biology appeal ability derive predictive model without need strong assumption underlying mechanism frequently unknown insufficiently defined case point accurate prediction gene expression level currently made broad set epigenetic feature using sparse linear model karlic et al 2010 cheng et al 2011 random forest li et al 2015 selected feature determine transcript level remains active research topic prediction genomics libbrecht noble 2015 et al 2016 proteomics swan et al 2013 metabolomics kell 2005 sensitivity compound eduati et al 2015 rely machine learning approach key ingredient application described within canonical machine learning workflow involves four step data ing feature extraction model fitting tion fig customary denote one data sample including covariates feature input x usually vector number label response variable output value usually single number available supervised machine learning model aim learn function f x list training pair data recorded fig one typical application biology predict viability cancer cell line exposed chosen drug menden et al 2013 eduati et al 2015 input feature x would capture somatic sequence variant cell line chemical drug concentration together measured viability output label used train support vector machine random forest classifier related method functional relationship f given new cell line unlabelled data sample x future learnt function predicts survival output label calculating f x even f resembles black box inner working particular mutation combination influence cell growth not easily interpreted regression real number classification categorical class label viewed way counterpart unsupervised machine learning approach aim discover pattern data sample x self without need output label method ing principal component analysis outlier detection typical example unsupervised model applied biological data input x calculated raw data represent model see world choice highly specific fig deriving informative feature essential performance process requires domain knowledge bottleneck especially limiting dimensional data even computational feature selection method not scale ass utility vast number possible input combination major recent advance machine learning automating critical step learning suitable representation data deep artificial neural network bengio et al 2013 lecun et al 2015 schmidhuber 2015 fig briefly deep neural network take raw data lowest input layer transforms increasingly abstract feature representation successively combining output preceding layer driven manner encapsulating highly complicated function 1 european molecular biology laboratory european bioinformatics institute wellcome trust genome campus hinxton cambridge uk 2 department computer science university tartu tartu estonia 3 wellcome trust sanger institute wellcome trust genome campus hinxton cambridge uk corresponding author tel 1223 834 244 corresponding author tel 1223 494 101 author contributed equally work ª 2016 author published term cc license molecular system biology 12 878 2016 1 downloaded march 28 2024 ip process box 1 deep learning one active field machine learning ha shown improve performance image speech recognition hinton et al 2012 krizhevsky et al 2012 graf et al 2013 zeiler fergus 2014 deng togneri 2015 natural language understanding bahdanau et al 2014 sutskever et al 2014 lipton 2015 xiong et al 2016 recently computational biology eickholt cheng 2013 dahl et al 2014 leung et al 2014 sønderby winther 2014 alipanahi et al 2015 wang et al 2015 zhou troyanskaya 2015 kelley et al 2016 potential deep learning biology clear principle allows better exploit availability increasingly large data set dna sequencing rna measurement flow cytometry automated microscopy training complex network multiple layer capture internal structure fig learned network discover feature improve performance traditional model increase interpretability provide additional understanding structure biological data review discus recent forthcoming application deep learning focus application regulatory genomics biological image analysis goal review wa not provide comprehensive background technical detail found specialized literature bengio 2012 bengio et al 2013 deng 2014 schmidhuber 2015 goodfellow et al 2016 instead aimed provide practical pointer necessary background get started deep architecture review current software solution give recommendation applying data application cover deliberately broad illustrate difference commonality approach review focusing specific domain found elsewhere park kellis 2015 gawehn et al 2016 leung et al 2016 mamoshina et al 2016 finally discus potential possible pitfall deep learning contrast method traditional machine learning classical statistical analysis approach deep learning regulatory genomics conventional approach regulatory genomics relate sequence variation change molecular trait one approach leverage variation genetically diverse individual map quantitative trait locus qtl principle ha applied identify regulatory variant affect gene expression level montgomery et al 2010 pickrell et al 2010 dna methylation gibbs et al 2010 bell et al 2011 histone mark grubert et al 2015 waszak et al 2015 proteome variation vincent et al 2010 albert et al 2014 part et al 2014 battle et al 2015 fig better statistical method helped increase power detect regulatory qtl kang et al 2008 stegle et al 2010 part et al 2011 rakitsch stegle 2016 however any mapping approach intrinsically limited variation present training population thus studying effect rare mutation particular requires data set large sample size alternative train model use variation region within genome fig splitting sequence window centred trait interest give rise ten sand training example molecular trait even using single individual even large data set predicting ular trait dna sequence challenging due multiple layer x feature model result clean data feature extraction discriminative feature raw data label c intron exon feature extraction training evaluation supervised unsupervised x linear regression logistic regression random forest svm pca factor analysis clustering outlier detection b c g c g c g g c c g g c g g g g g c g c c g g c c g g c g c g c g c c g tc g g c c g ag ca c c acc g g tg g c g c c g c g c g c g c tg c ga c c g g c g c g g c c g cgg ct c g g c c tg c c c g g c g c cca g cc g ct c g c g c c g c g g c c gg c g c gg c g c c g c g c g g c c g c g g g c g c g c g c g c g g c g g c g c c g ac c g tg c g tc c g layer 1 g c g g c c g cgg ct c g g c c tg c c g g c g c c g ac c g tg c g tc c g raw data processing raw data layer 2 intron exon tss figure machine learning representation learning classical machine learning workflow broken four step data feature extraction model learning model evaluation b supervised machine learning method relate input feature x output label whereas unsupervised method learns factor x without observed label c raw input data often related corresponding label complicated way challenging many classical machine learning algorithm left plot alternatively feature extracted using deep model may able better discriminate class right plot deep network use hierarchical structure learn increasingly abstract feature representation raw data molecular system biology 12 878 2016 ª 2016 author molecular system biology deep learning computational biology christof angermueller et al 2 downloaded march 28 2024 ip abstraction effect individual dna variant trait interest well dependence molecular trait broad sequence context interaction distal regulatory element value deep neural network context twofold first classical machine learning method not operate sequence directly thus require feature extracted sequence based prior knowledge presence absence variant snvs frequency motif occurrence conservation known regulatory variant structural element deep neural network help circumventing manual extraction feature learning data second representational richness capture nonlinear dependency sequence interaction effect span wider sequence context multiple genomic scale attesting utility deep neural network fully applied predict splicing activity leung et al 2014 xiong et al 2015 specificity protein box 1 artificial neural network artificial neural network initially inspired neural network brain mcculloch pitt 1943 farley clark 1954 rosenblatt 1958 consists layer interconnected compute unit neuron depth neural network corresponds number hidden layer width maximum number neuron one layer became possible train network larger number hidden layer artificial neural network rebranded deep network canonical configuration network receives data input layer transformed nonlinear way multiple hidden layer final output computed output layer panel neuron hidden output layer connected neuron previous layer neuron computes weighted sum input applies nonlinear activation function calculate output f x panel b popular activation function rectified linear unit relu panel b threshold negative signal 0 pass positive signal type activation function allows faster learning compared alternative sigmoid tanh unit glorot et al 2011 weight w neuron free parameter capture model representation data learned sample learning minimizes loss function l w measure fit model output true label sample panel bottom minimization challenging since loss function similar landscape many hill valley panel c took several decade backward propagation algorithm wa first applied compute loss function gradient via chain rule derivative rumelhart et al 1988 ultimately enabling efficient training neural network using stochastic gradient descent learning predicted label compared true label compute loss current set model weight loss backward propagated network compute ents loss function update panel loss function l w typically optimized using descent step current weight vector red dot moved along direction steepest descent dw direction arrow learning rate g length vector decaying learning rate time allows explore different domain loss function jumping valley beginning training left side parameter smaller learning rate later stage model training learning deep neural network remains active area research existing software package table 1 already applied without knowledge mathematical detail involved alternative architecture fully connected feedforward network developed specific application differ way neuron arranged include convolutional neural network widely used modelling image box 2 recurrent neural network sequential data sutskever 2013 lipton 2015 restricted boltzmann machine salakhutdinov larochelle 2010 hinton 2012 autoencoders hinton salakhutdinov 2006 alain et al 2012 kingma welling 2013 unsupervised learning choice network architecture parameter made objective way assessing model performance validation data set input layer hidden layer output layer 1 max 0 weighted sum activation function output input relu predicted label true label forward propagation backward propagation loss 0 1 local optimum global optimum b c w 1 w 2 1 0 1 0 f x l w 2 l w σ w w ηδw η δw ª 2016 author molecular system biology 12 878 2016 christof angermueller et al deep learning computational biology molecular system biology 3 downloaded march 28 2024 ip alipanahi et al 2015 epigenetic mark study effect dna sequence alteration zhou troyanskaya 2015 kelley et al 2016 early application neural network regulatory genomics first successful application neural network regulatory genomics replaced classical machine learning approach deep model without changing input feature example xiong et al 2015 considered fully connected feedforward neural network predict splicing activity individual exon model wa trained using feature extracted candidate exon adjacent intron despite relatively low number training sample combination model complexity method achieved substantially higher prediction accuracy splicing activity compared simpler approach particular wa able identify rare mutation implicated splicing misregulation convolutional design recent work using convolutional neural network cnns allowed direct training dna sequence without need define feature alipanahi et al 2015 zhou troyanskaya 2015 angermueller et al 2016 kelley et al 2016 cnn architecture allows greatly reduce number model parameter compared fully connected network applying convolutional operation only small region input space sharing parameter region key advantage resulting approach ability directly train model larger sequence window box 2 fig alipanahi et al 2015 considered convolutional network tectures predict specificity protein g c c g g ca c c acc g g tg g c g c g g c g c c g ac c g tg c g tc c g pooling convolution convolution individual 1 c g c c g c c g g c c g c c g g c g c g g c c g c c g g c c g c c g g c g c c g c c g c c g g c c g c c g g c g c c g c c g c c g individual variation anova eqtl variation locus 1 locus 2 locus 3 fully connected layer output layer convolution layer convolution layer input sequence variant score g c g g c c g cgg ct c g g c c tg c active normal deleterious b c g c g g c c g cgg ct c g g c c tg c e wild type individual 2 individual 3 mutant wild type response g g c g c g c g c c c g c g c c c c c g c c g g g g g g g g sequence alignment motif mutant response g c g g c c g c c g g c c g c c g c c g peak g c g g c c g cgg ct c g g c c tg c g c c g ag ca c c acc g g tg g c g c g g c g c c g ac c g tg c g tc c g g c g g c c g cgg ct c g g c c tg c g c c g ag ca c c acc g g tg g c g c g g c g c c g ac c g tg c g tc c g cgc g c c g c c g g c c c g c c cgc cgc wt cgc c g figure principle using neural network predicting molecular trait dna sequence dna sequence molecular response variable along genome three individual conventional approach regulatory genomics consider variation individual whereas deep learning allows exploiting variation tiling genome sequence dna window centred individual trait resulting large training data set single sample b convolutional neural network predicting molecular trait raw dna sequence window filter first convolutional layer example shown edge scan motif input sequence subsequent pooling reduces input dimension additional convolutional layer model interaction motif previous layer c response variable predicted neural network shown b mutant sequence used input additional neural network predicts variant score allows discriminate normal deleterious variant visualization convolutional filter aligning genetic sequence maximally activate filter creating sequence motif e mutation map sequence window row correspond four possible base pair substitution column sequence position predicted impact any sequence change letter top denote sequence height nucleotide denoting maximum effect across mutation figure panel adapted alipanahi et al 2015 molecular system biology 12 878 2016 ª 2016 author molecular system biology deep learning computational biology christof angermueller et al 4 downloaded march 28 2024 ip deepbind model outperformed existing method wa able recover known novel sequence motif could quantify effect sequence alteration identify functional snvs key innovation enabled training model directly raw dna sequence wa application tional layer intuitively neuron convolutional layer scan motif sequence combination thereof similar conventional position weight matrix stormo et al 1982 learning signal deeper layer informs convolutional layer motif relevant motif recovered model visualized heatmaps sequence logo fig box 2 convolutional neural network convolutional neural network cnns originally inspired cognitive neuroscience hubel wiesel seminal work cat visual cortex wa found simple neuron respond small motif visual field complex neuron respond larger one hubel wiesel 1963 1970 cnns designed model input data form multidimensional array image three colour channel lecun et al 1989 jarrett et al 2009 krizhevsky et al 2012 zeiler fergus 2014 et al 2015 szegedy et al genomic sequence one channel per nucleotide alipanahi et al 2015 wang et al 2015 zhou troyanskaya 2015 angermueller et al 2016 kelley et al 2016 high dimensionality data million pixel image render training fully connected neural network challenging number parameter model would typically exceed number training data fit circumvent cnns make additional assumption structure network thereby reducing effective number parameter learn convolutional layer consists multiple map neuron feature map filter size equal dimension input image panel two concept allow reducing number model parameter local connectivity parameter sharing first unlike fully connected network neuron within feature map only connected local patch neuron previous layer receptive field second neuron within given feature map share parameter hence neuron within feature map scan feature previous layer however different location different feature map might example detect edge different orientation image sequence motif genomic sequence activity neuron obtained computing discrete convolution receptive field computing weighted sum input neuron applying activation function panel b application exact position frequency feature irrelevant final prediction recognizing object image using assumption pooling layer summarizes adjacent neuron computing example maximum average activity resulting smoother representation feature activity panel c applying pooling operation small image patch shifted one pixel input image effectively thereby reducing number model parameter cnn typically consists multiple convolutional pooling layer allows learning abstract feature increasing scale small edge object part finally entire object one fully connected layer follow last pooling layer panel model meter number convolutional layer number feature map size receptive field strictly selected validation data set cytoplasm cell periphery vacuole convolutional layer pooling layer input image max 2 discrete convolution max pooling fully connected layer output layer max pooling discrete convolution n receptive field feature map b c ª 2016 author molecular system biology 12 878 2016 christof angermueller et al deep learning computational biology molecular system biology 5 downloaded march 28 2024 ip silico prediction mutation effect important application deep neural network trained raw dna sequence predict effect mutation silico assessment effect sequence change complement method based qtl mapping particular help uncover regulatory effect rare snvs likely causal gene intuitive approach visualizing predicted regulatory effect mutation map alipanahi et al 2015 whereby effect possible mutation given input sequence represented matrix view fig author could reliably identify deleterious snvs training additional neural network predicted binding score mutant sequence fig joint prediction multiple trait extension following initial success convolutional architecture extended applied range task regulatory mics example zhou troyanskaya 2015 considered architecture predict chromatin mark dna sequence author observed size input sequence window major determinant model performance larger window 1 kb coupled multiple convolutional layer enabled capturing sequence feature different genomic length scale second innovation wa use neural network architecture multiple output variable multitask neural network predict multiple chromatin state parallel multitask tures allow learning shared feature output thereby improving generalization performance markedly reducing computational cost model training compared learning dent model trait dahl et al 2014 similar vein kelley et al 2016 developed deep learning framework basset predict dnase hypersensitivity across multiple cell type quantify effect snvs matin accessibility model improved prediction mance compared conventional method wa able retrieve known novel sequence motif associated dnase hypersensitivity related architecture ha also considered angermueller et al predict dna methylation state bisulphite sequencing study angermueller et al 2016 approach combined convolutional architecture detect informative dna sequence motif additional feature derived neighbouring cpg site thereby accounting lation context recently koh pierson kundaje applied cnns genomewide chromatin immunoprecipitation followed sequencing data order obtain accurate prevalence estimate different chromatin mark koh et al 2016 present cnns among widely used architecture extract feature dna sequence window however alternative architecture could also considered example recurrent neural network rnns suited model sequential data lipton 2015 applied modelling natural language speech hinton et al 2012 graf et al 2013 sutskever et al 2014 che et al 2015 deng togneri 2015 xiong et al 2016 protein sequence agathocleous et al 2010 sønderby winther 2014 clinical medical data che et al 2015 lipton et al 2015 limited extent dna sequence xu et al 2007 lee et al 2015 rnns appealing application regulatory mics allow modelling sequence variable length capture interaction within sequence across multiple output however present rnns difficult train cnns additional work needed better understand setting one preferred complementary supervised method unsupervised deep ing architecture learn feature representation unlabelled data similarly classical principal component analysis factor analysis using nonlinear model example approach stacked autoencoders vincent et al 2010 restricted boltzmann machine deep belief network hinton et al 2006 learnt feature used visualize data input classical supervised learning task example sparse autoencoders applied classify cancer case using gene expression profile fakoor et al 2013 predict protein backbone lyon et al 2014 restricted boltzmann machine also used unsupervised deep network quently train supervised model protein secondary structure spencer et al 2015 disordered protein region eickholt cheng 2013 amino acid contact eickholt cheng 2012 neural network applied learn sentations protein sequence improve protein classification asgari mofrad 2015 general unsupervised model powerful approach large quantity unlabelled data available complex model trained model help improve performance classification task smaller number labelled example typically available deep learning biological image analysis historically perhaps important success deep neural network image analysis deep architecture trained million photograph famously detect object picture better human et al 2015 current model image classification object detection image retrieval semantic segmentation make use neural network convolutional neural network box 2 common network architecture image analysis briefly cnn performs pattern matching convolution aggregation pooling tions box 2 pixel level convolution operation scan image given pattern calculates strength match every position pooling determines presence pattern region example calculating maximum pattern match smaller patch thereby aggregating region tion single number successive application convolution pooling operation core network architecture used image analysis box 2 first application computational level classification early application deep network biological image focused task additional model building network output example ning et al 2005 applied molecular system biology 12 878 2016 ª 2016 author molecular system biology deep learning computational biology christof angermueller et al 6 downloaded march 28 2024 ip convolutional neural network study predicted abnormal development elegans embryo image trained cnn 40 40 pixel patch classify centre pixel cell wall plasm nucleus membrane nucleus outside medium using three convolutional pooling layer followed fully connected output layer model prediction fed based model analysis cnns outperformed standard method example markov random field conditional random field li 2009 raw data analysis task ple restoring noisy neural circuitry image jain et al 2007 adding layer allows moving clearing pixel noise modelling abstract image feature ciresan et al 2013 used five convolutional pooling layer followed two fully connected layer find mitosis breast histology image model mitosis detection challenge international conference pattern recognition 2012 outperforming competitor substantial margin approach wa also used segment neuronal structure electron microscopy image fying pixel membrane ciresan et al 2012 application cnns trained manner additional wa required obtain class probability output new image successive pooling operation lose information localization only summary retained larger larger region avoid skip link added carry information early layer forward deeper one currently performing classification method neuronal structure ronneberger et al 2015 employ architecture neuron take input lower layer localize feature well overcome arbitrary choice context size analysis whole cell cell population tissue many case prediction not required example xu et al directly classified colon histopathology image ous finding supervised feature learning deep network wa superior using handcrafted feature xu et al 2014 pa part used cnns classify segmented image patch individual yeast cell carrying cent protein different subcellular localization pattern pa part 2016 deep network outperformed method based traditional feature kraus et al combined tion classification task single architecture learned applied model full resolution yeast microscopy image kraus et al 2015 approach allowed sifying entire image without performing segmentation processing step cnns even applied count bacterial colony agar plate ferrari et al 2015 since early noising application pixel level field ha moving towards image analysis pipeline make use large bioimage data set representational power cnns reusing trained model training convolutional neural network requires large data set biological data acquisition expensive doe not mean deep neural network not used million image not available regardless image source lower level network tend capture similar signal edge blob not specific training data application instead recur perceptual task general thus convolutional neural network reuse picture similar domain help learning even data thereby requiring fewer image model task interest indeed donahue et al 2013 razavian et al 2014 showed feature learned million image classify object successfully used image retrieval detection classification new domain only hundred image labelled effectiveness approach depends similarity training data new domain yosinski et al 2014 concept transferring model parameter ha also successful bioimage analysis example zhang et al 2015 showed feature learned natural image transferred biological data improving prediction drosophila ster developmental stage situ hybridization image model wa first data imagenet russakovsky et al 2015 open corpus one million diverse image extract rich feature different scale xie et al 2015 used synthetic image train cnn automatic cell counting microscopy image expect network tory host model emerge biological image analysis effort already exist general image processing task see learning section trained model could downloaded used feature extractor fig 3 tuned adapted particular task data interpreting visualizing convolutional network convolutional neural network successful across many domain interpreting performance useful stand feature capture visualizing input weight one way understand particular neuron represents look input maximally activate some mathematical constraint pattern proportional incoming weight see also box 1 krizhevsky et al visualized weight first convolutional layer krizhevsky et al 2012 found maximally activating pattern correspond colour blob edge different orientation filter fig 4 gabor filter widely used feature image analysis neural network rediscover way useful component image model higher layer weight visualized well input not pixel weight difficult interpret finding image maximize neuron activity understand deeper layer term input pixel girshick et al 2014 retrieved simonyan et al 2013 generated image maximize output individual neuron fig 4 approach yield no explicit representation provide view type feature differentiate image large neuron activity others visualization tend show feature combine edge first layer ª 2016 author molecular system biology 12 878 2016 christof angermueller et al deep learning computational biology molecular system biology 7 downloaded march 28 2024 ip thereby detecting corner angle deeper layer neuron activate specific object part nose eye deepest layer detect whole object face car complicated engineer feature look specifically nose eye face neural network learn feature solely example hiding important image part understand image part important determining value feature zeiler fergus 2014 occluded image smaller grey box part influential drastically change feature value occluded similar vein simonyan et al 2013 springenberg et al 2014 ized individual pixel make difference feature bach binder colleague developed pixel relevance vidual classification decision general framework bach et al 2015 information also used object tion segmentation sensitive image pixel usually correctly correspond true object kraus et al 2015 used idea effectively localize cell large microscopy image visualizing similar input two dimension visualizing cnn representation help gauge input get mapped similar feature vector hence understand model ha learned donahue et al 2013 projected cnn feature two dimension show subsequent layer transforms data separable linear classifier general different cnn visualization method show higher layer feature specific learning task feature tend capture general aspect image edge corner tool practical consideration deep learning framework deep learning framework developed easily build neural network existing module high level popular one caffe jia et al 2014 theano bastien et al 2012 collobert et al 2011 tensorflow abadi et al 2016 rampasek goldenberg 2016 table 1 differ ity ease use way model defined trained caffe jia et al 2014 developed berkeley vision learning center written network architecture specified configuration file model trained used via command line without writing code additionally python matlab interface available caffe offer one efficient implementation cnns provides multiple trained model image recognition making well suited computer vision task downside custom model need implemented difficult additionally caffe not optimized recurrent architecture theano bastien et al 2012 team et al 2016 developed maintained university montreal written python model definition follow declarative instead imperative programing paradigm mean user specifies need done not order neural network declared computational graph compiled native code executed design allows theano optimize computational step automatically derive main strength consequently theano well suited building custom model offer particularly efficient implementation rnns software wrapper kera kera lasagne provide additional abstraction allow building network existing component reusing network major back theano frequently long compile time building larger model collobert et al 2011 wa initially developed university new york based scripting language luajit network easily built stacking existing module not compiled hence making suited fast ing theano offer efficient cnn implementation access range model possible downside need user familiar luajit scripting language also luajit le suited building custom recurrent network tensorflow abadi et al 2016 recent deep learning framework developed google software written offer interface python similar theano neural network declared computational graph optimized compilation however shorter compile time make suited prototyping key strength tensorflow native support parallelization across different device including cpu gpus using multiple compute node cluster accompanying tool tensorboard allows conveniently visualize network web browser monitor training progress example learning curve parameter update present fully connected pool conv pool conv pool conv vacuole cytoplasm cell periphery figure convolution pooling operator stacked thereby creating deep network image analysis standard application convolution layer followed pooling layer box 2 example lowest level convolutional unit operate 3 3 patch deeper one use capture information larger region convolutional layer followed one multiple fully connected layer learn feature informative classification layer learnable weight three example image maximize some neuron output shown molecular system biology 12 878 2016 ª 2016 author molecular system biology deep learning computational biology christof angermueller et al 8 downloaded march 28 2024 ip tensorflow provides efficient implementation rnns software recent active development hence only model currently available data preparation training data key every machine learning application since data informative feature usually result better mance effort spent collecting labelling cleaning normalizing data required data set size successful application deep learning supervised learning setting sufficient labelled training sample available fit complex model rule thumb number training sample least high number model parameter although special architecture model larization help avoid overfitting training data scarce bengio 2012 central problem regulatory genomics example predicting molecular trait genotype limited number training instance hundred ten thousand training ples typical strategy considering sequence window centred trait interest splice site transcription factor binding site epigenetic mark see fig widely used approach help increasing number pair single individual image analysis data abundant manually curated labelled training example typically difficult obtain instance training set augmented scaling ing cropping existing image approach also enhances robustness krizhevsky et al 2012 another strategy reuse network wa large data set image tion alexnet krizhevsky et al 2012 vgg simonyan zisserman 2014 googlenet szegedy et al resnet et al 2015 parameter data set interest microscopy image particular segmentation task approach exploit fact different data set share important characteristic feature edge curve transferred caffe lasagne torch limited extend tensorflow provide repository model partitioning data training validation test set machine learning model need trained selected tested independent data set avoid overfitting assure model generalize unseen data holdout validation partitioning data training validation test set standard deep neural network fig training set used learn model different assessed validation set model best performance example prediction accuracy error selected evaluated test set quantify performance unseen data comparison method typical data set proportion 60 training 10 validation 30 model testing data set small bootstrapping used instead hastie et al 2005 normalization raw data appropriate choice data normalization help accelerate training identification good local minimum categorical feature dna nucleotide first need encoded numerically typically represented binary vector one entry set zero indicates gory coding example dna nucleotide category table overview existing deep learning framework comparing four widely used software solution caffe theano tensorflow core language python luajit interface python matlab python c python wrapper lasagne kera kera pretty tensor scikit flow programming paradigm imperative declarative imperative declarative well suited cnns reusing existing model computer vision custom model rnns custom model cnns reusing existing model custom model parallelization rnns first layer feature third layer feature top left top right bottom right left right bottom figure network used generic feature extractor feeding input first layer left give feature representation term pattern left right present smaller patch every cell top bottom neuron activation extracted deeper layer right give rise abstract feature capture information larger segment image ª 2016 author molecular system biology 12 878 2016 christof angermueller et al deep learning computational biology molecular system biology 9 downloaded march 28 2024 ip commonly encoded 1 0 0 0 g 0 1 0 0 c 0 0 1 0 0 0 0 1 fig dna sequence sented binary string concatenating encoding nucleotide treating nucleotide independent input feature feedforward neural network cnn four bit encoded base commonly considered analogously colour nels image preserve entity nucleotide numerical feature typically subtracting mean value image pixel usually not individually jointly subtracting mean pixel intensity per colour nel additional common normalization step standardize feature unit variance whiting used decorrelate feature fig computationally involved since requires computing feature covariance matrix hastie et al 2005 distribution feature skewed due extreme value log transformation similar processing step may appropriate validation test data need normalized tently training data example feature validation data need subtracting mean computed training data not validation data model building choice model architecture preparing data design choice model tures need made default architecture feedforward neural network fully connected hidden layer appropriate starting point many problem convolutional tectures well suited data image abundant genomic data recurrent neural network capture dependency sequential data varying length text protein dna sequence sophisticated model built combining different architecture describe content image example cnn combined rnn cnn encodes image rnn generates corresponding image description vinyals et al 2015 xu et al 2015 deep learning work provide module different architecture combination determining number neuron network optimal number hidden layer hidden unit dependent optimized validation set one common heuristic maximize number layer unit without overfitting data layer unit increase number representable function local optimum empirical evidence show make finding good local optimum le sensitive weight initialization dauphin et al 2014 model training goal model training find parameter w minimize objective function l w measure fit tions model parameterized w actual observation scaled whitened loss epoch low high good performance epoch point early stopping overfitting training set validation set g c g c 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 b c e f learning rate test 30 validation 10 training 60 train model evaluate model test final performance repeat selected model figure data normalization deep neural network dna sequence encoded binary vector using code 1 0 0 0 g 0 1 0 0 c 0 0 1 0 0 0 0 1 b continuous data green orange scaling unit variance blue whiting purple c holdout validation partition full data set randomly training validation test set model trained different training set model highest performance validation set selected generalization performance model assessed compared machine learning method test set shape learning curve indicates learning rate low red shallow decay high orange steep decay followed saturation appropriate particular learning task green gradual decay e large difference model performance training set blue validation set green indicate overfitting stopping training soon validation set performance start drop early stopping prevent overfitting f illustration dropout regularization shown feedforward neural network randomly dropping neuron crossed reduces sensitivity neuron neuron previous layer due input greyed edge molecular system biology 12 878 2016 ª 2016 author molecular system biology deep learning computational biology christof angermueller et al 10 downloaded march 28 2024 ip common objective function sification error regression minimizing l w challenging since fig see also box 1 fig stochastic gradient descent stochastic gradient descent widely used train deep model starting initial set parameter gradient dw l respect w computed random batch only example 128 training sample dw point direction steepest descent towards w updated step size eta learning rate fig step parameter updated direction steepest descent minimum reached gously ball running hill valley bengio 2012 training performance strongly depends parameter initialization learning rate batch size parameter initialization general model parameter initialized randomly avoid local optimum determined fixed initialization starting point model parameter sampled independently normal distribution small variance commonly normal distribution variance scaled inversely number hidden unit input layer glorot bengio 2010 et al 2015 learning rate batch size learning rate batch size stochastic gradient descent need chosen care since strongly impact training speed model performance different learning rate usually explored logarithmic scale recommended default value bengio 2012 batch size 128 training sample suitable application batch size increased speed training decreased reduce memory usage important training complex model gpus optimum learning rate batch size connected larger batch size typically requiring smaller learning rate learning rate decay learning rate gradually reduced training based idea larger step may helpful early training stage order overcome possible local optimum whereas smaller step size allow exploring narrow parameter region loss function advanced stage training common approach include linearly reduce learning rate constant factor validation loss stop improving exponentially every training iteration epoch bengio 2012 gawehn et al 2016 momentum vanilla stochastic gradient descent extended tum usually improves training sutskever et al 2013 instead updating current parameter vector wt time gradient vector directly fraction previous update added current one momentum rate v weight updated momentum vector mt 2 approach help take larger step direction gradient point consistently therefore speed convergence momentum rate v set 0 1 typical value nesterov momentum nesterov 1983 2013 special form concept sometimes provides additional advantage adaptive learning rate method reduce sensitivity specific choice learning rate adaptive learning rate method rmsprop adagrad srivastava et al 2014 adam kingma ba 2014 developed order appropriately adapt learning rate per parameter training recent method adam ne strength previous method rmsprop adagrad generally recommended many application batch normalization batch normalization ioffe szegedy 2015 recently described approach reduce dependency training parameter initialization speed training reduce overfitting easy implement ha marginal additional compute cost ha hence become common practice batch normalization zero centre normalizes data not only input layer also hidden layer activation function approach allows using higher learning rate hence also accelerates training analysing learning curve validate learning process loss monitored function number training epoch number time full training set ha traversed fig ing curve decrease slowly learning rate may small increased loss decrease steeply beginning saturates quickly learning rate may high extreme learning rate result increasing fluctuating learning curve bengio 2012 monitoring training validation performance parallel training loss recommended monitor target performance accuracy training validation set training fig low decreasing tion performance relative training performance indicates fitting bengio 2012 avoiding overfitting deep neural network notoriously difficult train ting data major challenge since nonlinear many parameter overfitting result complex model relative size training set thus reduced decreasing model complexity example number hidden layer unit increasing size training set example via data augmentation following training guideline help avoid overfitting dropout srivastava et al 2014 common tion technique often one key ingredient train deep model activation some neuron randomly set zero dropped training forward pas intuitively result ensemble different network whose ª 2016 author molecular system biology 12 878 2016 christof angermueller et al deep learning computational biology molecular system biology 11 downloaded march 28 2024 ip prediction averaged fig dropout rate corresponds probability neuron dropped sensible default value addition dropping hidden unit input unit dropped however usually lower rate dropout often combined regularizing magnitude parameter value norm le commonly norm another popular regularization method early stopping training stopped soon validation performance start saturate deteriorate parameter best mance validation set chosen layerwise bengio et al 2007 salakhutdinov hinton 2012 considered model overfits despite mentioned regularization technique instead training entire network layer first unsupervised using autoencoders restricted boltzmann machine afterwards entire network using actual supervised learning objective optimization table 2 summarizes recommendation starting point common excluding size number filter cnn since best configuration model different configuration trained performance evaluated tion set number configuration grows exponentially number trying sible practice bengio 2012 therefore recommended optimize important ing rate batch size length convolutional filter dently via line search exploring different value keeping constant refined parameter space explored random sampling setting best performance validation set chosen framework spearmint snoek et al 2012 hyperopt bergstra cox 2013 smac hutter et al 2011 allow automatically explore space using bayesian optimization however although ally powerful present difficult apply parallelize random sampling training gpus training neural network compared shallow model take hour day even week depending size training set model architecture training gpus considerably reduce training time commonly tenfold therefore crucial evaluating multiple model ciently reason speedup learning deep network requires large number matrix multiplication parallelized efficiently gpus deep learning framework provide support train model either cpu gpus without requiring any knowledge gpu programming desktop machine local gpu card often used framework support specific brand alternatively commercial provider provide gpu cloud compute cluster pitfall no single method universally applicable choice whether use deep learning approach specific conventional analysis approach remain valid advantage data scarce aim ass statistical significance currently difficult using deep ing method another limitation deep learning increased training complexity applies model design required compute environment conclusion deep learning method powerful complement classical machine learning tool analysis strategy already approach found use number application tional biology including regulatory genomics image analysis first publicly available software framework helped reduce overhead model development provided rich accessible toolbox practitioner expect continued improvement software infrastructure make deep learning applicable growing range biological problem acknowledgement ca funded european molecular biology laboratory tp wa supported european regional development fund biomedit project estonian research council lp wa supported wellcome trust estonian research council wa supported european research council agreement conflict interest author declare no conflict interest table central parameter neural network recommended setting name range default value learning rate batch size 64 128 256 128 momentum rate weight initialization normal uniform glorot uniform glorot uniform adaptive learning rate method rmsprop adagrad adadelta adam adam batch normalization yes no yes learning rate decay none linear exponential linear rate activation function sigmoid tanh relu softmax relu dropout rate regularization 0 molecular system biology 12 878 2016 ª 2016 author molecular system biology deep learning computational biology christof angermueller et al 12 downloaded march 28 2024 ip reference abadi agarwal barham p brevdo e chen z citro c corrado g davis dean j devin ghemawat goodfellow harp irving g isard jia josofowicz r kaiser l kudlur levenberg j et al 2016 tensorflow machine learning heterogeneous distributed system agathocleous christodoulou g promponas v christodoulou c vassiliades v antoniou 2010 protein secondary structure prediction bidirectional recurrent neural net weight updating residue enhance performance artificial intelligence application innovation papadopoulos h andreou bramer ed vol 339 pp 128 berlin heidelberg springer alain g bengio rifai 2012 regularized estimate local statistic proc corr pp 1 17 albert fw treusch shockley ah bloom j kruglyak l 2014 genetics protein abundance variation large yeast population nature 506 494 497 alipanahi b delong weirauch mt frey bj 2015 predicting sequence specificity protein deep learning nat biotechnol 33 831 838 angermueller c lee h reik w stegle 2016 accurate prediction cell dna methylation state using deep learning biorxiv doi 055715 asgari e mofrad mrk 2015 protvec continuous distributed representation biological sequence plo one 10 bach binder montavon g klauschen f muller kr samek w 2015 explanation classifier decision relevance propagation plo one 10 bahdanau cho k bengio 2014 neural machine translation jointly learning align translate bastien f lamblin p pascanu r bergstra j goodfellow bergeron bouchard n bengio 2012 theano new feature speed improvement battle khan z wang sh mitrano ford mj pritchard jk gilad 2015 genomic variation impact regulatory variation rna protein science 347 664 667 bell jt pai aa pickrell jk gaffney dj r degner jf gilad pritchard jk 2011 dna methylation pattern associate genetic gene expression variation hapmap cell line genome biol 12 bengio lamblin p popovici larochelle h 2007 greedy training deep network advance neural information processing system schölkopf b platt b hofmann ed vol 19 pp 153 cambridge mit press bengio 2012 practical recommendation training deep architecture neural network trick trade montavon g orr g müller ed pp 437 berlin heidelberg springer bengio courville vincent p 2013 representation learning review new perspective pattern anal mach intell ieee trans 35 1798 1828 bergstra j cox dd 2013 hyperparameter optimization boosting classifying facial expression good null model che z purushotham khemani r liu 2015 distilling knowledge deep network application healthcare domain cheng c yan kk yip ky rozowsky j alexander r shou c gerstein 2011 statistical framework modeling gene expression using chromatin feature application modencode datasets genome biol 12 ciresan giusti gambardella lm schmidhuber j 2012 deep neural network segment neuronal membrane electron microscopy image advance neural information processing system pp 2843 cambridge mit press ciresan dc giusti gambardella lm schmidhuber j 2013 mitosis detection breast cancer histology image deep neural network medical image computing 2013 pp 411 berlin heidelberg springer collobert r kavukcuoglu k farabet c 2011 environment machine learning biglearn nip workshop dahl ge jaitly n salakhutdinov r 2014 neural network qsar prediction dauphin yn pascanu r gulcehre c cho k ganguli bengio 2014 identifying attacking saddle point problem optimization advance neural information processing system pp 2933 cambridge mit press deng l 2014 deep learning method application found signal process 7 197 387 deng l togneri r 2015 deep dynamic model learning hidden representation speech feature speech audio processing coding enhancement recognition ogunfunmi togneri r narasimha ed pp 153 new york springer donahue j jia vinyals hoffman j zhang n tzeng e darrell 2013 decaf deep convolutional activation feature generic visual recognition eduati f mangravite lm wang tang h bare jc huang r norman kellen menden mp yang j zhan x zhong r xiao g xia abdo n kosyk collaboration friend dearry simeonov et al 2015 prediction human population response toxic compound collaborative competition nat biotechnol 33 933 940 eickholt j cheng j 2012 predicting protein contact using deep network boosting bioinformatics 28 3066 3072 eickholt j cheng j 2013 dndisorder predicting protein disorder using boosting deep network bmc bioinformatics 14 88 fakoor r ladhak f nazi huber 2013 using deep learning enhance cancer diagnosis classification proceeding icml workshop role machine learning transforming healthcare atlanta ga jmlr w cp farley b clark w 1954 simulation system digital computer trans ire profess group inf theory 4 76 84 ferrari lombardi signoroni 2015 bacterial colony counting convolutional neural network engineering medicine biology society embc 2015 annual international conference ieee pp 7458 new york ieee gawehn e hiss ja schneider g 2016 deep learning drug discovery mol informatics 35 3 14 gibbs jr van der brug mp hernandez dg traynor bj nalls lai arepalli dillman rafferty ip troncoso j 2010 abundant quantitative trait locus exist dna methylation gene expression human brain plo genet 6 girshick r donahue j darrell malik j 2014 rich feature hierarchy accurate object detection semantic segmentation proceeding ieee conference computer vision pattern recognition pp 580 new york ieee glorot x bengio 2010 understanding difficulty training deep feedforward neural network international conference artificial intelligence statistic pp 249 jmlr conference proceeding ª 2016 author molecular system biology 12 878 2016 christof angermueller et al deep learning computational biology molecular system biology 13 downloaded march 28 2024 ip glorot x bordes bengio 2011 deep sparse rectifier neural network international conference artificial intelligence statistic pp 315 jmlr conference proceeding goodfellow bengio courville 2016 deep learning cambridge mit press preparation graf mohamed hinton g 2013 speech recognition deep recurrent neural network 2013 ieee international conference acoustic speech signal processing icassp pp 6645 new york ieee grubert f zaugg jb kasowski ursu spacek dv martin ar greenside p srivas r phanstiel dh pekowska heidari n euskirchen g huber w pritchard jk bustamante cd steinmetz lm kundaje snyder 2015 genetic control chromatin state human involves local distal chromosomal interaction cell 162 1051 1065 hastie tibshirani r friedman j franklin j 2005 element statistical learning data mining inference prediction math intell 27 83 85 k zhang x ren sun j 2015 deep residual learning image recognition hinton ge salakhutdinov rr 2006 reducing dimensionality data neural network science 313 504 507 hinton ge osindero teh 2006 fast learning algorithm deep belief net neural comput 18 1527 1554 hinton ge 2012 practical guide training restricted boltzmann machine neural network trick trade montavon g orr g müller ed pp 599 heidelberg berlin springer hinton g deng l yu dahl ge mohamed jaitly n senior vanhoucke v nguyen p sainath tn 2012 deep neural network acoustic modeling speech recognition shared view four research group signal process mag ieee 29 82 97 hubel wiesel 1963 shape arrangement column cat striate cortex j physiol 165 559 hubel dh wiesel tn 1970 period susceptibility physiological effect unilateral eye closure kitten j physiol 206 419 hutter f hoos hh k 2011 sequential optimization general algorithm configuration learning intelligent optimization coello coello ca ed vol 6683 pp 507 heidelberg berlin springer ioffe szegedy c 2015 batch normalization accelerating deep network training reducing internal covariate shift jain v murray jf roth f turaga zhigulin v briggman kl helmstaedter mn denk w seung h 2007 supervised learning image restoration convolutional network 2007 ieee international conference computer vision pp 1 new york ieee jarrett k kavukcuoglu k ranzato lecun 2009 best architecture object recognition 2009 ieee international conference computer vision pp 2146 new york ieee jia shelhamer e donahue j karayev long j girshick r guadarrama darrell 2014 caffe convolutional architecture fast feature embedding proceeding acm international conference multimedia pp 675 new york acm kang hm ye c eskin e 2008 accurate discovery expression quantitative trait locus confounding spurious genuine regulatory hotspot genetics 180 1909 1925 karlic r chung hr lasserre j vlahovicek k vingron 2010 histone modification level predictive gene expression proc natl acad sci usa 107 2926 2931 kell db 2005 metabolomics machine learning modelling towards understanding language cell biochem soc trans 33 520 524 kelley dr snoek j rinn j 2016 basset learning regulatory code accessible genome deep convolutional neural network genome kingma dp welling 2013 variational bayes kingma ba j 2014 adam method stochastic optimization koh pw pierson e kundaje 2016 denoising histone convolutional neural network biorxiv kraus oz ba lj frey b 2015 classifying segmenting microscopy image using convolutional multiple instance learning krizhevsky sutskever hinton ge 2012 imagenet classification deep convolutional neural network advance neural information processing system pp 1097 cambridge mit press lecun boser b denker j henderson howard hubbard w jackel ld 1989 backpropagation applied handwritten zip code recognition neural comput 1 541 551 lecun bengio hinton g 2015 deep learning nature 521 436 444 lee b lee na b yoon 2015 splice junction prediction using deep recurrent neural network leung mkk xiong hy lee lj frey bj 2014 deep learning regulated splicing code bioinformatics 30 leung mkk delong alipanahi b frey bj 2016 machine learning genomic medicine review computational problem data set proceeding ieee vol 104 pp 176 new york ieee li sz 2009 markov random field modeling image analysis berlin heidelberg springer science business medium li j ching huang garmire lx 2015 using epigenomics data predict gene expression lung cancer bmc bioinformatics 16 suppl 5 libbrecht mw noble w 2015 machine learning application genetics genomics nat rev genet 16 321 332 lipton zc 2015 critical review recurrent neural network sequence learning lipton zc kale dc elkan c wetzell r 2015 learning diagnose lstm recurrent neural network lyon j dehzangi heffernan r sharma paliwal k sattar zhou yang 2014 predicting backbone ca angle dihedrals protein sequence stacked sparse deep neural network j comput chem 35 2040 2046 mamoshina p vieira putin e zhavoronkov 2016 application deep learning biomedicine mol pharm 13 1445 1454 märtens k hallin j warringer j liti g part l 2016 predicting quantitative trait genome phenome near perfect accuracy nat commun 7 11512 mcculloch w pitt w 1943 logical calculus idea immanent nervous activity bull math biophys 5 115 133 menden mp iorio f garnett mcdermott u benes ch ballester pj rodriguez j 2013 machine learning prediction cancer cell sensitivity drug based genomic chemical property plo one 8 michalski r carbonell jg mitchell tm 2013 machine learning artificial intelligence approach berlin heidelberg springer science business medium montgomery sb sammeth lach rp ingle c nisbett j guigo r dermitzakis et 2010 transcriptome genetics using second generation sequencing caucasian population nature 464 773 777 molecular system biology 12 878 2016 ª 2016 author molecular system biology deep learning computational biology christof angermueller et al 14 downloaded march 28 2024 ip murphy kp 2012 machine learning probabilistic perspective cambridge mit press nesterov 1983 method solving convex programming problem convergence rate soviet math doklady 27 372 376 nesterov 2013 introductory lecture convex optimization basic course vol 87 berlin heidelberg springer science business medium ning f delhomme lecun piano f bottou l barbano pe 2005 toward automatic phenotyping developing embryo video image process ieee trans 14 1360 1371 park kellis 2015 deep learning regulatory genomics nat biotechnol 33 825 826 pärnamaa part l 2016 accurate classification protein subcellular localization high throughput microscopy image using deep learning biorxiv part l stegle winn j durbin r 2011 joint genetic analysis gene expression data inferred cellular phenotype plo genet 7 part l liu yc tekkedil mm steinmetz lm caudy aa fraser ag boone c andrew bj rosebrock ap 2014 heritability genetic basis protein level variation outbred population genome 24 1363 1370 pickrell jk marioni jc pai aa degner jf engelhardt nkadori e veyrieras jb stephen gilad pritchard jk 2010 understanding mechanism underlying human gene expression variation rna sequencing nature 464 768 772 rakitsch b stegle 2016 modelling local gene network increase power detect genetic effect gene expression genome biol 17 33 rampasek l goldenberg 2016 tensorflow biology gateway deep learning cell syst 2 12 14 razavian azizpour h sullivan j carlsson 2014 cnn feature shelf astounding baseline recognition proceeding ieee conference computer vision pattern recognition workshop pp 806 new york ieee ronneberger fischer p brox 2015 convolutional network biomedical image segmentation medical image computing 2015 pp 234 heidelberg berlin springer rosenblatt f 1958 perceptron probabilistic model information storage organization brain psychol rev 65 386 rumelhart de hinton ge williams rj 1988 learning representation error cogn model 5 1 russakovsky deng j su h krause j satheesh huang z karpathy khosla bernstein 2015 imagenet large scale visual recognition challenge int j comput vi 115 211 252 salakhutdinov r larochelle h 2010 efficient learning deep boltzmann machine international conference artificial intelligence statistic pp 693 jmlr conference proceeding salakhutdinov r hinton g 2012 efficient learning procedure deep boltzmann machine neural comput 24 1967 2006 schmidhuber j 2015 deep learning neural network overview neural netw 61 85 117 simonyan k vedaldi zisserman 2013 deep inside convolutional network visualising image classification model saliency map simonyan k zisserman 2014 deep convolutional network scale image recognition snoek j larochelle h adam rp 2012 practical bayesian optimization machine learning algorithm advance neural information processing system pp 2951 cambridge mit press sønderby sk winther 2014 protein secondary structure prediction long short term memory network spencer eickholt j cheng j 2015 deep learning network approach ab initio protein secondary structure prediction trans comput biol bioinformatics 12 103 112 springenberg jt dosovitskiy brox riedmiller 2014 striving simplicity convolutional net srivastava n hinton g krizhevsky sutskever salakhutdinov r 2014 dropout simple way prevent neural network overfitting j mac learn 15 1929 1958 stegle part l durbin r winn j 2010 bayesian framework account complex factor gene expression level greatly increase power eqtl study plo comput biol 6 stormo gd schneider td gold l ehrenfeucht 1982 use perceptron algorithm distinguish translational initiation site coli nucleic acid 10 2997 3011 sutskever 2013 training recurrent neural network phd thesis graduate department computer science university toronto toronto canada sutskever marten j dahl g hinton g 2013 importance initialization momentum deep learning proceeding international conference machine learning pp 1139 jmlr conference proceeding sutskever vinyals le qv 2014 sequence sequence learning neural network advance neural information processing system pp 3104 cambridge mit press swan al mobasheri allaway liddell bacardit j 2013 application machine learning proteomics data classification biomarker identification postgenomics biology omics 17 595 610 szegedy c liu w jia sermanet p reed anguelov erhan vanhoucke v rabinovich going deeper convolution proceeding ieee conference computer vision pattern recognition pp 1 new york ieee szegedy c vanhoucke v ioffe shlens j wojna z rethinking inception architecture computer vision team ttd r alain g almahairi angermueller c bahdanau ballas n bastien f bayer j belikov 2016 theano python framework fast computation mathematical expression vincent p larochelle h lajoie bengio manzagol 2010 stacked denoising autoencoders learning useful representation deep network local denoising criterion j mac learn 11 3371 3408 vinyals toshev bengio erhan 2015 show tell neural image caption generator proceeding ieee conference computer vision pattern recognition pp 3156 new york ieee wang k cao k hannenhalli 2015 chromatin genomic determinant alternative splicing proceeding acm conference bioinformatics computational biology health informatics pp 345 new york acm waszak sm delaneau gschwind ar kilpinen h raghav sk witwicki rm orioli wiederkehr panousis ni yurovsky l planchon bielser padioleau udin g thurnheer hacker hernandez n reymond deplancke b et al 2015 population variation genetic control modular chromatin architecture human cell 162 1039 1050 xie xing f kong x su h yang l 2015 beyond classification structured regression robust cell detection using convolutional neural network medical image computing 2015 pp 358 heidelberg berlin springer ª 2016 author molecular system biology 12 878 2016 christof angermueller et al deep learning computational biology molecular system biology 15 downloaded march 28 2024 ip xiong hy alipanahi b lee lj bretschneider h merico yuen rkc hua gueroussov najafabadi h hughes tr morris q barash krainer ar jojic n scherer sw blencowe bj frey bj 2015 human splicing code reveals new insight genetic determinant disease science 347 1254806 xiong c merity socher r 2016 dynamic memory network visual textual question answering xu r wunsch ii frank r 2007 inference genetic regulatory network recurrent neural network model using particle swarm optimization trans comput biol bioinformatics 4 681 692 xu mo feng q zhong p lai chang ei 2014 deep learning feature representation multiple instance learning medical image analysis 2014 ieee international conference acoustic speech signal processing icassp pp 1626 new york ieee xu k ba j kiros r cho k courville salakhutdinov r zemel r bengio 2015 show attend tell neural image caption generation visual attention yosinski j clune j bengio lipson h 2014 transferable feature deep neural network advance neural information processing system 27 ghahramani z welling cortes c lawrence nd weinberger kq ed pp 3320 cambridge mit press zeiler md fergus r 2014 visualizing understanding convolutional network computer 2014 pp 818 heidelberg berlin springer zhang w li r zeng sun q kumar ye j ji 2015 deep model based transfer learning biological image analysis proceeding acm sigkdd international conference knowledge discovery data mining pp 1475 new york acm zhou j troyanskaya og 2015 predicting effect noncoding variant deep sequence model nat method 12 931 934 license open access article term creative common attribution license permit use distribution tion any medium provided original work properly cited molecular system biology 12 878 2016 ª 2016 author molecular system biology deep learning computational biology christof angermueller et al 16 downloaded march 28 2024 ip