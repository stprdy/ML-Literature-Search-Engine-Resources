sip 2014 vol 3 page 1 29 author online version article published within open access environment subject condition creative common attribution licence overview paper tutorial survey architecture algorithm application deep learning li deng invited paper overview material topic presented plenary overview session tutorial material presented conference 1 expanded updated include recent development deep learning previous updated material cover theory application analyze future direction goal tutorial survey introduce emerging area deep learning hierarchical learning apsipa community deep learning refers class machine learning technique developed largely since 2006 many stage information processing hierarchical architecture exploited pattern classification feature learning recent literature also connected representation learning involves hierarchy feature concept level concept defined one concept help define one tutorial survey brief history deep learning research discussed first classificatory scheme developed analyze summarize major work reported recent deep learning literature using scheme provide survey existing deep architecture algorithm literature categorize three class generative discriminative hybrid three representative deep architecture deep autoencoders deep stacking network generalization temporal domain recurrent network deep neural network pretrained deep belief network one three class presented detail next selected application deep learning reviewed broad area signal information processing including multimodality language modeling natural language processing information retrieval finally future direction deep learning discussed analyzed keywords deep learning algorithm information processing received 3 february 2012 revised 2 december 2013 n r u c n research nowadays ha significantly widened scope compared year ago ha encompassed many broad area information ing signal semantic information 2 since 2006 deep learning recently referred representation learning ha emerged new area machine learning research within past year technique developed deep learning research already impacting wide range work within traditional new widened scope including machine learning artificial intelligence 1 see recent new york time medium coverage progress 9 series workshop tutorial special issue conference special session devoted sively deep learning application various sical expanded area include microsoft research redmond wa 98052 usa phone corresponding author deng email deng 2013 international conference learning tations 2013 icassp special session new type deep neural network learning speech recognition related application 2013 icml workshop audio speech language processing 2013 2012 2011 2010 nip workshop deep learning unsupervised feature learning 2013 icml workshop tion learning challenge 2013 intern conf learning representation 2012 icml workshop representation learning 2011 icml workshop learning architecture representation optimization speech visual information processing 2009 icml workshop ing feature hierarchy 2009 nip workshop deep learning speech recognition related application 2012 icassp deep learning tutorial special section deep learning speech language processing ieee trans audio speech language processing january 2012 special issue learning deep architecture ieee trans pattern analysis machine intelligence 2013 author ha directly involved research organizing several event editorial ha seen emerging nature field hence need providing tutorial survey article 1 published online cambridge university press 2 li deng deep learning refers class machine learning technique many layer stage hierarchical architecture exploited tern classification feature representation ing intersection among research area neural network graphical modeling optimization pattern recognition signal processing three important son popularity deep learning today drastically increased chip processing ability gpu unit nificantly lowered cost computing hardware recent advance machine learning processing research active researcher area include university toronto new york university versity montreal microsoft research google ibm research baidu facebook stanford university sity michigan mit university washington numerous place researcher strated success deep learning diverse application computer vision phonetic recognition voice search conversational speech recognition speech image ture coding semantic utterance classification recognition audio processing visual object recognition information retrieval even analysis molecule may lead discovering new drug reported recently 9 paper expands recent overview material topic presented plenary overview session well tutorial material presented conference 1 aimed introduce apsipa transaction reader emerging technology enabled deep learning attempt provide tutorial review research work conducted exciting area since birth deep learning 2006 ha direct vance signal information processing future research direction discussed attract interest apsipa researcher student practitioner ing signal technology core mission apsipa community remainder paper organized follows section ii brief historical account deep learning provided perspective signal information processing section iii classification scheme large body work deep learning developed growing number deep architecture classified 1 generative 2 discriminative 3 hybrid egories description provided category section three category rial example chosen provide detailed ment example chosen 1 deep autoencoders generative category section iv 2 dnns trained dbn hybrid category section v 3 deep stacking network dsns related cial version recurrent neural network rnns discriminative category section vi section vii set typical successful application deep learning diverse area signal information processing reviewed section viii summary future direction given b r e f h r c l c c u n f e e p l e r n n g recently machine learning technique exploited architecture architecture typically contain single layer linear feature transformation lack multiple layer adaptive feature example low architecture conventional commonly used sian mixture model gmms hidden markov model hmms linear dynamical system tional random field crfs maximum entropy maxent model support vector machine svms logistic sion kernel regression perceptron mlp neural network single hidden layer including extreme learning machine property common shallow learning model relatively simple architecture consists only one layer responsible transforming raw input signal feature feature space may unobservable take example svm conventional kernel method use shallow linear pattern separation model one zero feature transformation layer kernel trick used otherwise notable exception recent kernel od inspired integrated deep learning shallow architecture shown effective solving many simple problem limited modeling representational power cause difficulty dealing plicated application involving natural signal human speech natural sound language natural image visual scene human mechanism vision speech however suggest need deep architecture extracting complex structure ing internal representation rich sensory input example human speech production perception tems equipped clearly layered hierarchical structure transforming information form level linguistic level similar vein human visual system also hierarchical nature perception side interestingly also generative side natural believe art advanced processing type natural signal efficient effective deep learning algorithm developed learning tems deep architecture composed many layer processing stage lower layer output fed immediate higher layer input successful deep learning technique developed far share two additional key property generative nature model typically requires adding additional published online cambridge university press tutorial survey architecture algorithm application deep learning 3 top layer perform discriminative task vised pretraining step make effective use large amount unlabeled training data extracting structure regularity input feature historically concept deep learning wa originated artificial neural network research hence one may occasionally hear discussion ral network neural network mlps many hidden layer indeed good example el deep architecture backpropagation popularized ha algorithm learning weight network unfortunately backpropagation alone not work well practice learning network small number hidden layer see review analysis 4 20 pervasive presence local optimum objective function deep network main source difficulty ing backpropagation based local gradient descent start usually some random initial point often get trapped poor local optimum severity increase significantly depth network increase ficulty partially responsible steering away machine learning research ral network shallow model convex loss tions svms crfs maxent model global optimum efficiently obtained cost le powerful model optimization difficulty associated deep model wa empirically alleviated reasonably cient unsupervised learning algorithm wa introduced two paper 3 21 paper class deep generative model wa introduced called deep belief work dbn composed stack restricted boltzmann machine rbms core component dbn greedy learning algorithm optimizes dbn weight time complexity linear size depth network separately some surprise initializing weight mlp spondingly configured dbn often produce much better result random weight mlps many hidden layer deep neural network dnns learned unsupervised dbn pretraining lowed backpropagation sometimes also called dbns literature recently researcher careful distinguishing dnn dbn 6 25 dbn used ize training dnn resulting network called 6 addition supply good initialization point dbn come additional attractive feature first learning algorithm make effective use unlabeled data second interpreted bayesian probabilistic erative model third value hidden variable deepest layer efficient compute fourth overfitting problem often observed model million parameter dbns fitting problem occurs often deep network effectively addressed generative pretraining step insightful analysis speech information dbns capture provided 26 procedure not only one make effective training dnns possible since lication seminal work 3 21 number researcher improving applying deep learning technique success example one alternatively pretrain dnns layer layer considering pair layer denoising autoencoder regularized setting subset input zero 4 27 also tractive autoencoders used purpose regularizing via penalizing gradient activity hidden unit respect input 28 ranzato et al 29 developed sparse encoding ric machine sesm ha similar architecture rbms building block dbn principle sesm may also used effectively initialize dnn training historically use generative model dbn facilitate training dnns play important role igniting interest deep learning speech feature ing speech recognition 6 22 25 30 effectiveness wa demonstrated research showed many alternative simpler way pretraining large amount training data know learn dnn starting shallow neural network one hidden layer shallow network ha trained discriminatively new hidden layer inserted previous hidden layer softmax output layer full network discriminatively trained one continue process desired number hidden layer reached dnn finally full backpropagation carried complete dnn training training data ful weight initialization process discriminative pretraining removed also effective dnn training next section overview provided iou architecture deep learning including beyond original dbn published 3 h r e e b r c l e f e e p r c h e c u r e n v e r v e w described earlier deep learning refers rather wide class machine learning technique tures hallmark using many layer linear stage hierarchical nature depending architecture technique intended use one broadly categorize work area three main class 1 generative deep architecture intended characterize correlation property observed visible data pattern analysis synthesis purpose characterize joint statistical butions visible data associated class published online cambridge university press 4 li deng latter case use bayes rule turn type architecture discriminative one 2 discriminative deep architecture intended directly provide discriminative power pattern sification often characterizing posterior tions class conditioned visible data 3 hybrid deep architecture goal nation assisted often significant way outcome generative architecture via better mization regularization discriminative criterion used learn parameter any deep generative model category 1 note use hybrid 3 different used sometimes literature refers hybrid pipeline system speech recognition feeding output probability neural network hmm machine learning tradition 34 may ural use classification scheme according discriminative learning neural network versus deep probabilistic generative learning dbn dbm classification scheme however miss key insight gained deep learning research generative model greatly improve learning dnns deep discriminative model via better optimization ization also deep generative model may not necessarily need probabilistic deep autoencoder ertheless classification point important difference dnns deep probabilistic model former usually efficient training ing flexible construction le constrained no normalization difficult partition function replaced sparsity suitable learning complex system no approximate inference learning latter hand easier interpret embed domain knowledge ier compose handle uncertainty typically intractable inference learning complex system distinction however retained also proposed classification adopted throughout paper briefly review representative work three class several basic definition used summarized table application deep architecture deferred section vii generative architecture associated generative category often see unsupervised feature learning since label data not concern applying generative architecture pattern recognition supervised learning key cept unsupervised pretraining concept arises need learn deep network learning lower level network difficult especially training data limited therefore desirable learn lower layer without relying layer learn layer greedy manner bottom gist pretraining subsequent learning layer together among various subclass generative deep tecture deep model including coder common 4 original form deep autoencoder 21 30 give detail section iv typical example generative model category form deep autoencoders also generative nature quite different property implementation example transforming autoencoders 39 predictive sparse coder stacked version denoising autoencoders stacked version 27 specifically denoising autoencoders input tor first corrupted randomizing percentage input setting zero one design hidden encoding node reconstruct original rupted input data using criterion kl distance original input reconstructed input rupted encoded representation used input next level stacked denoising autoencoder another prominent type generative model deep boltzmann machine dbm dbm contains many layer hidden variable ha no connection variable within layer cial case general boltzmann machine bm network symmetrically connected unit make stochastic decision whether simple learning algorithm general bm complex study slow compute learning dbm layer capture complicated correlation activity hidden feature layer dbms potential learning internal representation become increasingly complex highly desirable solving object speech recognition lem furthermore representation built large supply unlabeled sensory input limited labeled data used only slightly model specific task hand number hidden layer dbm reduced one rbm like dbm no no connection main virtue rbm via composing many rbms many hidden layer learned efficiently using feature activation one rbm training data next composition lead dbn describe detail together rbms section standard dbn ha extended factored bm bottom layer strong result phone recognition obtained 43 model called rbm mcrbm recognizes tion standard rbm ability represent covariance structure data however cult train mcrbm use higher level deep architecture furthermore strong result lished not easy reproduce architecture 43 mcrbm parameter full dbn not easy using discriminative information published online cambridge university press tutorial survey architecture algorithm application deep learning 5 table some basic deep learning terminology deep learning class machine learning technique many layer processing stage hierarchical architecture exploited unsupervised feature learning pattern essence deep learning compute hierarchical feature representation observational data feature factor defined one deep belief network dbn probabilistic generative model composed multiple layer stochastic hidden variable top two layer undirected symmetric connection lower layer receive directed connection layer boltzmann machine bm network symmetrically connected unit make stochastic decision whether restricted boltzmann machine rbm special bm consisting layer visible unit layer hidden unit no connection deep boltzmann machine dbm special bm hidden unit organized deep layered manner only adjacent layer connected no hidden connection within layer deep neural network dnn multilayer network many hidden layer whose weight fully connected often initialized pretrained using stacked rbms dbn literature dbn sometimes used mean dnn deep dnn whose output target data input often pretrained dbn using distorted training data regularize learning distributed representation representation observed data way eled generated interaction many hidden factor particular factor learned configuration factor often generalize well distributed representation form basis deep learning regular rbms higher layer however recent work showed better feature used cepstral speech feature subject linear discriminant analysis fmllr transformation mcrbm not needed covariance transformed data already modeled 26 another representative deep generative architecture network spn 44 45 spn directed acyclic graph data leaf sum product operation internal node deep architecture sum node give mixture model product node build feature hierarchy erties completeness consistency constrain spn desirable way learning spn carried using em algorithm together tion learning procedure start dense spn find spn structure learning weight zero weight remove connection main ficulty learning found common one learning signal gradient quickly dilutes propagates deep layer empirical solution found mitigate difficulty reported 44 wa pointed despite many desirable tive property spn difficult fine tune weight using discriminative information limiting effectiveness classification task difficulty ha overcome subsequent work reported 45 efficient discriminative training algorithm spn wa presented wa pointed standard gradient descent computed derivative conditional likelihood suffers dient diffusion problem well known regular deep network marginal inference replaced ring probable state hidden variable hard gradient descent reliably estimate deep spns weight excellent result image tion task reported rnns regarded class deep generative tectures used model generate tial data 46 depth rnn large length input data sequence rnns erful modeling sequence data speech text recently not widely used partly extremely difficult train properly due known vanishing gradient problem recent advance optimization 47 partially overcome difficulty using information stochastic vature estimate recent work 48 rnns trained optimization used tive deep architecture language eling lm task gated connection introduced allow current input character predict tion one latent state vector next generative rnn model demonstrated well capable ating sequential text character recently bengio et al 49 sutskever 50 explored new optimization method training generative rnns modify tic gradient descent show modification perform optimization method mikolov et al 51 reported excellent result using rnns lm recently mesnil et al 52 reported success rnns spoken language understanding example different type generative deep el ha long history speech recognition research human speech production mechanism exploited construct dynamic deep structure abilistic generative model comprehensive review see book 53 specifically early work described generalized extended conventional shallow published online cambridge university press 6 li deng conditionally independent hmm structure imposing dynamic constraint form polynomial trajectory hmm parameter variant approach ha recently developed using different learning niques hmm parameter application extended speech recognition robustness 60 61 similar trajectory hmms also form basis metric speech synthesis subsequent work added new hidden layer dynamic model itly account erties human speech generation 15 16 efficient implementation deep architecture den dynamic achieved fir filter recent study generative model speech shown special case general dynamic bayesian network model even general dynamic graphical model 77 78 graphical model comprise many hidden layer acterize complex relationship variable speech generation armed powerful graphical ing tool deep architecture speech ha recently successfully applied solve difficult problem speech recognition mixed speech visible variable mixed speech becomes represented new hidden layer deep generative architecture 79 80 deep tive graphical model indeed powerful tool many application due capability embedding domain knowledge however addition weakness using representation classification gories also often implemented ate approximation inference learning prediction topology design arising inherent intractability task application problem ha partly addressed recent work 81 provides interesting direction making deep tive graphical model potentially useful practice future standard statistical method used speech recognition understanding combine shallow hmms speech acoustic higher layer structure representing different level natural language hierarchy combined hierarchical model suitably regarded deep generative architecture whose motivation some technical detail may found chapter 7 recent book 82 hierarchical hmm hhmm related model greater technical depth ical treatment found 83 hhmm 84 layered hmm early deep model formulated directed graphical model missing key aspect tributed representation embodied recent deep generative architecture dbn dbm discussed earlier section finally temporally recursive deep generative el found 85 human motion modeling 86 natural language natural scene parsing latter model particularly interesting ing algorithm capable automatically determining optimal model structure contrast deep architecture dbn only parameter learned architecture need predefined specifically reported 86 recursive structure monly found natural scene image natural guage sentence discovered using structure prediction architecture not only unit tained image sentence identified way unit interact form whole b discriminative architecture many discriminative technique signal mation processing apply shallow architecture hmms crfs since crf defined conditional probability input data well output label intrinsically shallow discriminative architecture interesting lence crf discriminatively trained gaussian model hmms found 101 recently crfs developed stacking output lower layer crf together original input data onto higher layer 96 ous version crfs usefully applied phone recognition 102 spoken language tion 103 natural language processing 96 however least phone recognition task performance crfs purely discriminative ha not able match hybrid approach involving dbn take shortly recent article 33 give excellent review major existing discriminative model speech nition based mainly traditional neural network mlp architecture using backpropagation learning dom initialization argues importance increased width layer neural network increased depth particular class dnn model form basis popular tandem approach discriminatively learned neural network developed context computing discriminant emission ities hmms some representative recent work area see 104 105 tandem approach generates discriminative feature hmm using tie one hidden layer neural network various way information combination regarded form discriminative deep architecture 33 106 recent work new deep learning architecture sometimes called dsn together tensor variant 111 112 kernel version 11 developed focus discrimination scalable parallelizable learning relying little no generative component describe type discriminative deep architecture detail section rnns successfully used generative model output taken predicted input data published online cambridge university press tutorial survey architecture algorithm application deep learning 7 future discussed preceding subsection see also neural predictive model 113 mechanism also used discriminative model output explicit label sequence associated input data sequence note discriminative rnns applied speech long time ago limited success 114 training rnns discrimination presegmented training data typically required also needed transform output label sequence highly desirable remove requirement especially costly presegmentation training data often separate hmm used automatically segment sequence ing training transform rnn classification result label sequence 114 however use hmm purpose doe not take advantage full potential rnns interesting method wa proposed enables rnns perform sequence fication removing need presegmenting training data output underlying method idea interpreting rnn output conditional distribution possible label sequence given input sequence differentiable tive function derived optimize conditional distribution correct label sequence no segmentation data required another type discriminative deep architecture volutional neural network cnn module sisting convolutional layer pooling layer module often stacked one top another dnn top form deep model convolutional layer share many weight pooling layer subsamples output convolutional layer reduces data rate layer weight sharing convolutional layer together ately chosen pooling scheme endows cnn some invariance property translation invariance ha argued limited invariance not adequate complex pattern recognition task principled way handling wider range ance needed 39 nevertheless cnn ha found highly effective commonly used computer vision image recognition 154 recently appropriate change cnn designed image analysis taking account tie cnn also found effective speech recognition discus application detail section vii useful point neural network tdnn 127 129 developed early speech recognition special case cnn weight sharing limited one two dimension time dimension wa not recently researcher discovered time wrong dimension impose invariance frequency dimension effective sharing weight ing output 122 123 126 analysis underlying reason provided 126 together new egy designing cnn pooling layer demonstrated effective nearly previous cnns phone recognition also useful point model chical temporal memory htm 17 128 130 another variant extension cnn extension includes following aspect 1 time temporal dimension introduced serve supervision information crimination even static image 2 information flow used instead cnn 3 bayesian probabilistic formalism used fusing information decision making finally learning architecture developed speech recognition proposed 131 developed since 2004 notably using technique also categorized discriminative deep architecture category no intent mechanism architecture ize joint probability data recognition target speech attribute phone word current implementation approach based multiple layer neural network using tion learning 135 one intermediate neural network layer implementation framework explicitly represents speech attribute plified entity atomic unit speech developed early work 136 137 simplification lie removal temporally overlapping property speech attribute feature ding realistic property future work expected improve accuracy speech recognition c hybrid architecture term hybrid third category refers deep architecture either comprises make use generative discriminative model component many existing hybrid architecture published ature 21 23 25 138 generative component exploited help discrimination final goal hybrid architecture generative ing help discrimination examined two viewpoint 1 optimization viewpoint generative model provide excellent initialization point highly linear parameter estimation problem commonly used term pretraining deep learning ha introduced reason 2 regularization perspective generative el effectively control complexity overall model study reported 139 provided insightful analysis experimental evidence supporting point published online cambridge university press 8 li deng generative deep architecture dbn discussed section subject discriminative training using backprop commonly called erature obtain equivalent architecture dnn weight dnn pretrained stacked rbms dbn instead usual random tion see 24 detailed explanation equivalence relationship use often confusing ogy review detail dnn context pretraining well interface commonly used shallow generative architecture hmm section iv another example hybrid deep architecture developed 23 generative dbn used initialize dnn weight fine tuning carried not using discriminative mation error criterion level one combination static dnn shallow discriminative architecture crf overall architecture learned using discriminative criterion conditional probability full label sequence given input sequence data shown equivalent hybrid deep architecture dnn hmm whose eters learned jointly using imum mutual information mmi entire label sequence input vector sequence closely related training method carried success shallow neural network 140 deep one 141 useful point connection hybrid discriminative training highly lar minimum phone error mpe training technique hmm 89 iterative mpe training procedure using extended initial hmm parameter not arbitrary one commonly used initial eter set trained generatively using algorithm maximum likelihood furthermore polation term taking value generatively trained hmm parameter needed extended updating formula may analogous fine tuning dnn training discussed earlier 89 ha similar spirit dbn pretraining hybrid dnn learning along line using discriminative criterion train parameter generative model hmm training example briefly discus method applied learning generative architecture 142 generative model rbm learned using native criterion posterior probability label vector concatenated input data tor form overall visible layer rbm way rbm considered solution classification problem author derived native learning algorithm rbm shallow generative model recent work 146 deep erative model dbn gated mrf lowest level learned feature extraction nition difficult image class including occlusion generative ability dbn model facilitates ery information captured lost level representation deep model demonstrated 146 related work using discriminative rion empirical risk train deep graphical model found 81 example hybrid deep architecture use generative model dbn train deep convolutional neural network deep dnn 123 144 145 like dnn cussed earlier dbn pretraining also shown improve discrimination deep cnn random initialization final example given hybrid deep tecture based idea work 147 148 one task discrimination speech recognition produce output text serf input second task discrimination machine translation overall system giving functionality speech translation translating speech one language text another language deep architecture consisting generative discriminative element model speech nition hmm machine translation phrasal mapping alignment generative nature parameter learned tion framework described 148 enables performance optimization overall deep architecture using unified learning framework initially published 90 hybrid deep learning approach applied not only speech translation also possibly task speech information retrieval speech understanding lingual understanding retrieval etc 11 109 briefly surveying wide range work three class deep architecture following three section elaborate three prominent el deep learning one three class ideally represent influential architecture giving state art performance chosen three familiar responsible development may serve tutorial purpose well simplicity tural mathematical description three tures described following three section may not interpreted representative influential work three class example category generative architecture highly complex deep tecture generative training method developed described 154 beyond scope rial performs quite well image recognition likewise category discriminative architecture even complex architecture learning described kingsbury et al 141 seide et al 155 yan et al 156 gave state art performance speech recognition published online cambridge university press tutorial survey architecture algorithm application deep learning 9 v g e n e r v e r c h e c u r e e e p u e n c e r introduction deep autoencoder special type dnn whose output data input used learning efficient encoding dimensionality reduction set data specifically feature extraction method involving no class label hence generative autoencoder us three layer neural network input layer data efficiently coded pixel image spectrum speech one considerably smaller hidden layer form encoding output layer neuron ha meaning input layer number hidden layer greater one autoencoder considered deep autoencoder often trained using one many backpropagation variant conjugate gradient method steepest descent etc though often reasonably effective fundamental problem using gation train network many hidden layer error get backpropagated first layer become minuscule quite ineffective cause network almost always learn reconstruct average training data though advanced agation method conjugate gradient method help some degree still result slow learning poor solution problem remedied using tial weight approximate final solution process find initial weight often called pretraining successful pretraining technique developed 3 training deep autoencoders involves treating boring set two layer rbm pretraining approximate good solution using tion technique minimize coding error training technique applied construct deep autoencoder map image short binary code fast image retrieval also applied ing document called semantic hashing coding speech feature review b use deep autoencoder extract speech feature review recent work 30 developing similar type autoencoder extracting bottleneck speech instead image feature discovery efficient binary code related feature also used speech mation retrieval importantly potential benefit using discrete representation speech constructed type deep autoencoder derived almost ited supply unlabeled data speech recognition retrieval system fig architecture deep autoencoder used 30 extracting speech feature spectrogram deep generative model patch spectrogram contain 256 frequency bin 1 3 9 13 frame trated fig undirected graphical model called rbm built ha one visible layer linear variable gaussian noise one hidden layer binary latent variable learning rbm activation probability hidden unit treated data training another rbm two rbms posed form dbn easy infer state second layer binary hidden unit input single forward pas dbn used work illustrated left side fig 1 two rbms shown separate box see detailed discussion rbm dbn next section deep autoencoder three hidden layer formed unrolling dbn using weight matrix lower layer deep autoencoder use matrix encode input upper layer use matrix reverse order decode input deep autoencoder using backpropagation make output similar possible input shown right side fig learning complete any spectrogram encoded structed follows first overlapping frame log power spectrum normalized mean provide input deep autoencoder first hidden layer us logistic function compute activation real ues fed next coding layer compute code activation hidden unit coding layer quantized either zero one old binary code used reconstruct original spectrogram individual patch published online cambridge university press 10 li deng fig top bottom original spectrogram reconstruction using input window size n 1 3 9 13 forcing coding unit zero one binary code value indicate fft bin number fft used constructing spectrogram reconstructed first using two upper layer work weight finally technique used reconstruct speech spectrogram output produced applying deep autoencoder every possible window n consecutive frame show some illustrative encoding reconstruction example c illustrative example top fig 2 original speech followed reconstructed speech utterance forced binary value zero one 312 unit code layer encoding window length n 1 3 9 13 respectively lower coding error n 9 13 clearly seen encoding accuracy deep autoencoder tively examined compare traditional code via vector quantization vq figure various aspect encoding accuracy top original speech utterance spectrogram next two spectrogram blurry reconstruction vq much faithful reconstruction deep coder coding error coder plotted function time shown spectrogram ing autoencoder red curve producing lower error vq coder blue curve throughout entire span utterance final two spectrogram show detailed coding error distribution time frequency bin transforming autoencoder deep autoencoder described extract pact code feature vector due many layer extracted code would change dictably input feature vector transformed desirable able code change predictably reflects underlying transformation invariant ceived content goal transforming autoencoder proposed image recognition 39 building block transforming autoencoder capsule independent subnetwork extract single parameterized feature representing single entity visual audio transforming autoencoder receives input vector target output vector related input vector simple global tion translation whole image frequency shift due vocal tract length difference speech explicit representation global transformation known also bottleneck coding layer transforming coder consists output several capsule training phase different capsule learn extract different entity order minimize error final output target addition deep autoencoder architecture described section many type erative architecture literature characterized use data alone free classification label automatically derive feature although complex architecture produced state published online cambridge university press tutorial survey architecture algorithm application deep learning 11 fig top bottom original spectrogram test set reconstruction vq coder reconstruction autoencoder coding error function time vq coder blue autoencoder red spectrogram vq coder residual spectrogram deep autoencoder residual art result 154 complexity doe not permit detailed treatment tutorial paper rather brief vey broader range generative deep architecture wa included section v h b r r c h e c u r e n n p r e r n e w h b n basic section present widely studied hybrid deep architecture dnns consisting pretraining using generative dbn stage eter learning part review based recent publication 6 7 25 generative component dbn abilistic model composed multiple layer stochastic latent variable unobserved variable binary value often called hidden unit feature detector top two layer undirected symmetric tions form associative memory lower layer receive directed connection layer state unit lowest layer visible unit represent input data vector efficient procedure ing generative weight determine variable one layer depend variable layer learning value latent variable every layer inferred single pas start observed data vector bottom layer us generative weight reverse direction dbns learned one layer time treating ues latent variable one layer inferred data data training next layer efficient greedy learning followed bined learning procedure weight improve generative discriminative formance full network latter learning procedure constitutes discriminative component dbn hybrid architecture discriminative performed adding final layer variable represent desired put backpropagating error derivative network many hidden layer applied highly structured input data speech image backpropagation work much better feature detector hidden er initialized learning dbn model structure input data originally proposed 21 dbn viewed composition simple ing module via stacking simple learning ule called rbms introduce next b restricted bm rbm special type markov random field ha one layer typically bernoulli stochastic hidden unit one layer typically bernoulli gaussian tic visible observable unit rbms represented bipartite graph visible unit connected published online cambridge university press 12 li deng hidden unit no hidden connection rbm joint distribution p v h θ ble unit v hidden unit h given model parameter θ defined term energy function e v h θ p v h θ exp v h θ z z v h exp v h θ normalization factor partition function marginal probability model assigns visible vector v p v θ h exp v h θ z bernoulli visible hidden rbm energy function defined e v h θ j wi jvihj bivi j jhj wi j represents symmetric interaction term visible unit vi hidden unit hj bi j bias term j number visible den unit conditional probability efficiently calculated p hj θ σ wijvi j p vi θ σ j wijhj bi σ x 1 exp x similarly gaussian visible hidden rbm energy e v h θ j wi jvihj 2 vi 2 j jhj corresponding conditional probability become p hj θ σ wi jvi j p θ n j wi jhj bi 1 vi take real value follows gaussian tribution mean wi jhj bi variance one rbms used convert valued stochastic variable binary stochastic variable fig pictorial view sampling rbm negative learning phase rbm courtesy hinton processed using bernoulli rbms discussion used two common tional distribution visible data rbm sian data binomial binary data general type distribution rbm also used see 157 use general family distribution purpose taking gradient log likelihood log p v θ derive update rule rbm weight j edata vihj vihj edata vihj expectation observed ing set emodel vihj expectation distribution defined model unfortunately emodel vihj intractable compute contrastive divergence cd approximation gradient used emodel vihj replaced running gibbs pler initialized data one full step step approximating emodel vihj follows initialize data sample sample sample sample model rough estimate emodel vihj true sample model use approximate emodel vihj give rise algorithm sampling process pictorially depicted fig 4 careful training rbms essential success applying rbm related deep learning technique solve practical problem see technical report 158 useful practical guide training rbms rbm discussed generative model characterizes input data distribution using hidden ables no label information involved however label information available used together data form joint data set cd learning applied optimize imate generative objective function related data lihood interestingly discriminative objective function defined term conditional likelihood label discriminative rbm used fine tune rbm classification task 142 note sesm architecture ranzato et al 29 veyed section iii quite similar rbm described symmetric encoder published online cambridge university press tutorial survey architecture algorithm application deep learning 13 fig illustration architecture decoder logistic top encoder main difference rbm trained using approximate maximum likelihood sesm trained simply minimizing average energy plus additional code sparsity term sesm relies sparsity term vent flat energy surface rbm relies explicit contrastive term loss approximation log tition function another difference coding strategy code unit noisy binary rbm sparse sesm c stacking rbms form architecture stacking number rbms learned layer layer bottom give rise dbn example shown fig stacking procedure follows learning rbm application continuous feature speech rbm application nominal binary feature image coded text treat tion probability hidden unit data training rbm one layer activation probability rbm used visible data input rbm some theoretical tifications efficient greedy learning strategy given 3 shown stacking procedure improves variational lower bound likelihood training data composite model greedy procedure achieves approximate learning note learning cedure unsupervised requires no class label applied classification task generative training followed combined cally discriminative learning procedure weight jointly improve performance network discriminative performed adding final layer variable represent desired output label provided training data backpropagation algorithm used adjust tune dbn weight use final set weight way standard feedforward neural work go top label layer dnn depends application speech recognition application top layer denoted j fig 5 resent either syllable phone subphones phone state speech unit used speech nition system generative pretraining described ha duced excellent phone speech recognition result wide variety task surveyed section vii research ha also shown effectiveness pretraining strategy example greedy training may carried additional native term generative cost function level without generative pretraining purely discriminative training dnns random initial weight using ditional stochastic gradient decent method ha shown work well scale initial weight set carefully size trade noisy gradient convergence speed used stochastic ent decent adapted prudently increasing size training epoch also randomization order creating need judiciously determined importantly wa found effective learn dnn ing shallow neural net single hidden layer ha trained discriminatively using early stop avoid overfitting second hidden layer inserted first hidden layer labeled softmax put unit expanded deeper network trained discriminatively continued desired number hidden layer reached full propagation fine tuning applied discriminative pretraining procedure found work well practice 155 type discriminative pretraining procedure closely related learning algorithm developed deep architecture called deep network described section vi interleaving linear layer used building deep tures modular manner original input vector concatenated output vector module consisting shallow neural net discriminative ing used positioning subset weight module reasonable space using parallelizable convex optimization followed fine tuning cedure also parallelizable due constraint two subset weight module purely discriminative training full dnn random initial weight known work much published online cambridge university press 14 li deng fig interface hmm form architecture ha successfully used speech recognition experiment reported 25 better thought early day provided scale initial weight set carefully large amount labeled training data available size training epoch set appropriately le generative pretraining still improves test performance sometimes significant amount especially small task generative pretraining wa originally done using rbms various type autoencoder one hidden layer also used interfacing dnn hmm discussed static classifier input vector fixed dimensionality however many tical pattern recognition lem including speech recognition machine translation natural language understanding video processing information processing require sequence recognition sequence recognition sometimes called classification structured dimensionality input output variable hmm based dynamic programming operation convenient tool help port strength static sifier handle dynamic sequential pattern thus natural combine hmm bridge gap static sequence pattern recognition tecture show interface dnn hmm provided fig architecture ha successfully used speech recognition experiment reported 25 important note unique elasticity poral dynamic speech elaborated 53 would require model better hmm published online cambridge university press tutorial survey architecture algorithm application deep learning 15 ultimate success speech recognition integrating dynamic model realistic property dnn possibly deep learning model form coherent dynamic deep architecture ing new research v c r n v e r c h e c u r e n n r e c u r r e n n e w r k introduction dnn reviewed ha shown extremely powerful connection performing nition classification task including speech recognition image classification training dbn ha proven difficult computationally particular tional technique training dnn fine tuning phase involve utilization stochastic gradient descent ing algorithm extremely difficult parallelize make learning large scale cally impossible example ha possible use one single powerful gpu machine train speech recognizers dozen hundred hour speech training data remarkable result difficult however scale success thousand hour training data describe new deep learning architecture dsn attack learning scalability problem section based part recent publication 11 107 111 112 expanded discussion central idea dsn design relates concept stacking proposed originally 159 simple ules function classifier composed first stacked top order learn complex function classifier various way menting stacking operation developed past typically making use supervised information simple module new feature stacked classifier higher level stacking architecture often come concatenation classifier output lower module raw input feature 160 simple module used stacking wa crf type deep architecture wa developed hidden state added successful natural language speech recognition application segmentation information unknown training data 96 convolutional neural network 161 also considered stacking architecture supervision information typically not used final stacking module dsn architecture wa originally presented 107 also used name deep convex network dcn emphasize convex nature main learning algorithm used learning network dsn discussed section make use supervision information ing basic module take simplified form perceptron basic module fig dsn architecture stacking only four module illustrated distinct color dashed line denote copying layer output unit linear hidden unit sigmoidal linearity output unit permit highly efficient parallelizable estimation result convex optimization output network weight given hidden unit activity owing form constraint input output weight input weight also elegantly estimated efficient parallelizable manner name convex used 107 accentuates role convex optimization learning output network weight given hidden unit activity basic ule also point importance constraint derived convexity input output weight constraint make learning remaining network parameter input network weight much easier otherwise enabling learning dsn distributed cpu cluster recent publication dsn wa used key operation stacking emphasized b architectural overview dsn dsn shown fig 7 includes variable number layered module wherein module specialized ral network consisting single hidden layer two trainable set weight fig 7 only four module published online cambridge university press 16 li deng illustrated module shown rate color practice hundred module efficiently trained used image speech classification experiment lowest module dsn comprises first linear layer set linear input unit layer set hidden unit second linear layer set linear output unit hidden layer lowest module dsn prises set unit mapped input unit way first weight matrix denote instance weight matrix may comprise plurality randomly generated value zero one weight rbm trained separately linear unit may sigmoidal unit configured perform operation weighted output input unit weighted accordance first weight matrix w second linear layer any module dsn includes set output unit representative get classification unit module dsn may mapped set linear put unit way second weight matrix denote second weight matrix learned way batch learning process ing undertaken parallel convex optimization employed connection learning instance u learned based least part upon first weight matrix w value coded classification target value input unit indicated dsn includes set serially connected overlapping layered module wherein module includes aforementioned three layer first linear layer includes set linear input unit whose number equal dimensionality input feature hidden layer comprises set unit whose number tunable second linear layer comprises plurality linear output unit whose number equal target classification class module referred herein layered output unit lower module subset input unit adjacent higher module dsn specifically second module directly lowest ule dsn input unit include output unit hidden unit lower module input unit additionally include raw training data word output unit lowest module appended input unit second module input unit second module also include output unit lowest module pattern discussed including output unit lower module portion input unit cent higher module dbn thereafter learning weight matrix describes connection weight hidden unit linear output unit via convex tion continue many module resultant learned dsn may deployed connection matic classification task speech phone state classification connecting dsns output hmm any dynamic programming device enables continuous speech recognition form sequential pattern recognition c learning dsn weight some technical detail provided use linear output unit dsn facilitates learning dsn weight single module used illustrate advantage simplicity reason first clear upper layer weight matrix u efficiently learned activity matrix h training sample den layer known let u denote training vector x xi xn vector denoted xi x ji xdi dimension input vector function block n total number training sample denote l number hidden unit c dimension put vector output dsn block yi uthi hi σ wtxi vector sample u l c weight matrix upper layer block w l weight matrix lower layer block σ sigmoid function bias term implicitly sented formulation xi hi augmented one given target vector full training set total n sample ti tn vector ti tji tci parameter u w learned minimize average total square error e 1 2 n 1 output network yn uthn utσ wtxn g n u w depends weight matrix standard neural net assuming h hi hn known equivalently w known setting error tive respective u zero give u hht f w hn σ wtxn provides explicit constraint u w treated independently popular backprop algorithm given equality constraint u f w let u use lagrangian multiplier method solve optimization problem learning optimizing lagrangian e 1 2 n n u w w derive gradient descent learning algorithm gradient take following form 108 ht 1 htt published online cambridge university press tutorial survey architecture algorithm application deep learning 17 fig comparison one single module dsn left tdsn two equivalent form tdsn module shown right ht hht h bol multiplication compared backprop method ha le noise gradient computation due exploitation explicit constraint u f w wa found imentally unlike backprop batch training effective aid parallel learning dsn tensorized dsn dsn architecture discussed far ha recently generalized tensorized version call tdsn 111 112 ha scalability dsn term allelizability learning generalizes dsn providing feature interaction missing dsn architecture tdsn similar dsn way stacking operation carried module tdsn stacking similar way form deep architecture difference tdsn dsn lie mainly module constructed dsn one set hidden unit forming hidden layer denoted left panel fig contrast module tdsd tains two independent hidden layer denoted hidden 1 hidden 2 middle right panel fig result different weight denoted u fig 8 change matrix array dsn tensor array tdsn shown cube labeled u middle panel tensor u ha connection one prediction layer remaining two separate den layer equivalent form tdsn module shown right panel fig 8 implicit hidden layer formed expanding two separate hidden layer outer product resulting large vector contains possible product two set layer vector turn tensor u matrix whose dimension 1 size prediction layer 2 uct two hidden layer size equivalence enables convex optimization learning u developed dsn applied learning tensor importantly order hidden feature interaction enabled tdsn via outer product construction large implicit hidden layer stacking tdsn module form deep architecture pursues similar way dsn concatenating various fig stacking tdsn module concatenating prediction vector input vector vector two example shown fig 9 note stacking concatenating hidden layer input fig 10 would difficult dsn since hidden layer tends large practical purpose e recurrent neural network consider increasingly higher module dsn version shallow neural network turn dsn stacking instead stacking discrete time index corresponds depth dsn constraint dsn among weight matrix similarly applied type rnn learning published online cambridge university press 18 li deng fig stacking tdsn module concatenating two tor input vector weight parameter provided output unit linear fact concept rnn dsn bined form recurrent version dsn lently stacked version simple rnn not discussed paper one way learning rnns linear output adopt approach shown effective dsn learning outlined section would capture short memory one time step increase memory length apply traditional method backprop time bptt exploit relationship among various weight matrix turn recursive procedure simpler analytical form however difficult formulate derive dsn case discussed section use general bptt 162 ha advantage dling output unit shown speed learning substantially compared use linear output unit rnn commonly discussed problem vanishing exploding gradient bptt mitigated applying constraint regarding rnn recurrent matrix optimization process remainder section let u formulate rnn term state space model monly used signal processing compare formulation dynamic tems used generative model speech acoustic contrast discriminative rnn use mathematical model generative mode allows u shed light onto one approach work better another rnn state dynamic noise free expressed ht f wxhxt observation predicted label target vector lt vector coded class label observation equation formulation becomes 116 yt whyht yt g whyht define error function sum squared difference yt lt time bptt unfolds rnn time ing gradient respect wxh whh stochastic gradient descent applied update weight matrix using similar formulation rnn model generative mode known den dynamic model briefly discussed section speech recognition researcher built many type speech recognizers past 20 some year see vey section 34 particular corresponding state observation equation erative ht g state noise xt h ht obsnoise rewritten equation 13 14 34 sistent rnn variable system matrix driving state dynamic dependent label lt time hence model also called switching dynamic system system matrix parameter analogous whh rnn parameter set governs mapping hidden state speech production acoustic feature speech one implementation took form shallow mlp weight 69 163 164 another implementation took form set matrix mixture linear expert 73 state equation many existing implementation hidden dynamic model speech doe not take linear form rather following linear form wa used 163 ht whh lt lt lt state noise exhibit property dynamic parameter whh function phonetic label lt particular time lt mapping symbolic quantity lt target vector surface based mathematical tion striking similarity tive rnn generative hidden dynamic model however essence well representational substance two model different summarize first rnn adopts strategy using distributed representation supervision information label whereas hidden dynamic model label locally represented used index separate set parameter leading switching dynamic considerably complicates decoding computation published online cambridge university press tutorial survey architecture algorithm application deep learning 19 second rnn run directly producing posterior probability class contrast hidden dynamic model run top generating likelihood ues class individually difference clear comparing two observation equation one give label prediction another give input feature prediction state equation rnn model ha input drive system dynamic whereas generative model ha label index drive dynamic via intermediate representation articulatory vocal tract resonance get third learning algorithm bptt rnn directly minimize label prediction error contrast kalman filtering e step em algorithm used learning generative model doe not ination explicitly given known difficulty bptt rnn 162 one obvious direction adopt hybrid deep architecture using generative hidden dynamic model pretrain discriminative rnn analogous using generative dbn pretrain dnn discussed preceding subsection v p p l c n f e e p l e r n n g g n l n n f r n p r c e n g expanded technical scope signal processing signal endowed not only traditional type audio speech image video also text language document convey semantic tion human consumption addition scope processing ha extended conventional ing enhancement analysis recognition include task interpretation understanding retrieval mining user interface 2 signal processing researcher working one signal processing area defined matrix constructed two ax signal processing discussed deep learning technique discussed paper recently applied large number traditional extended area some recent interesting cation predicting protein structure 110 not cover provide brief survey body work four main category pertaining closely signal information processing speech audio traditional neural network mlp ha use speech recognition many year used alone performance typically lower hmm system observation probability approximated gmms recently deep learning technique wa successfully applied phone recognition 23 24 116 126 165 166 large vocabulary speech recognition task 22 25 135 155 156 integrating powerful discriminative training ability dnns sequential modeling ability hmms speech recognition ha long dominated method underlying shallow ative model 129 170 171 neural network popular approach not competitive 32 33 113 129 generative model deep hidden dynamic likewise not competitive either 69 74 deep learning dnn started making impact speech recognition 2010 close collaboration academic industrial researcher see review 6 169 collaborative work started small ulary task 23 24 30 126 165 demonstrating power hybrid deep architecture work also showed importance raw speech feature spectrogram back mfcc feature not yet reaching raw level 173 174 collaboration continued large vocabulary task ing highly positive result 22 25 103 success large part attributed use large dnn put layer structured way speech unit senones motivated initially speech industry desire keep change already highly efficient decoder software infrastructure minimum meantime body work also demonstrated possibility reduce need ing effective learning dnns large amount labeled data available combination three tor quickly spread success deep learning speech recognition entire speech industry academia 1 minimally required decoder change new based speech recognizer deployment condition enabled use senones dnn output 2 significantly ered error compared hmm system 3 training simplicity empowered big data training timeframe least 15 major speech recognition group worldwide confirmed experimental success dnns large task use raw speech spectral feature away mfccs notable group include major trial speech lab worldwide microsoft 155 156 168 169 ibm 125 141 175 google 120 176 baidu result represent new speech tion widely deployed company voice product service extensive medium coverage discussed section concept convolution time wa originated tdnn shallow neural net 127 129 developed early speech recognition only recently deep architecture deep cnn used ha found weight sharing effective phone recognition time domain previous tdnn 126 study also show designing pooling deep cnn properly invariance vocal tract length discrimination speech sound together regularization technique dropout 177 lead even better phone recognition performance set work also point direction trajectory discrimination invariance expressed whole dynamic pattern speech defined mixed time published online cambridge university press 20 li deng frequency domain using convolution pooling recent work 125 show cnns also useful large vocabulary continuous speech tion demonstrates multiple convolutional layer provide even improvement tional layer use large number convolution kernel feature map addition rbm dbn cnn dsn deep model also developed reported literature speech audio processing related application example crf stack many layer crfs successfully used task language identification 103 phone tion 102 sequential labeling natural language ing 96 confidence calibration speech recognition 178 179 furthermore rnn ha early success phone recognition 114 wa not easy duplicate due intricacy training let alone scale larger speech recognition task learning algorithm rnns dramatically improved since better result obtained recently using rnns 115 180 especially structure long memory lstm embedded rnn several layer trained 116 rnns also recently applied processing application 49 use rectified linear hidden unit instead logistic tanh explored rnn rectified linear unit relu compute max x 0 lead sparser gradient le diffusion credit blame rnn faster training addition speech recognition impact deep learning ha recently spread speech synthesis aimed overcome limitation conventional approach statistical parametric synthesis based hmm model clustering ference icassp may 2013 four different deep learning approach reported improve traditional based speech synthesis system ling et al 64 181 rbm dbn generative model used replace traditional gaussian model achieving significant quality improvement subjective objective measure synthesized voice approach developed 182 dbn generative model used represent joint distribution linguistic acoustic feature decision tree gaussian model replaced dbn hand study reported 183 make use discriminative model dnn represent conditional distribution acoustic feature given linguistic feature no joint distribution modeled finally 184 discriminative model dnn used feature extractor summarizes structure raw acoustic feature dnn feature used input second stage system prediction prosodic contour target contextual tures fill speech synthesis system application deep learning speech synthesis infancy much work expected community near future likewise audio music processing deep ing ha also become intense interest only recently impacted area include mainly music signal processing music information retrieval 49 deep learning present unique set challenge area music audio signal time series event organized musical time rather real time change function rhythm expression measured signal typically combine multiple voice synchronized time overlapping frequency mixing temporal dependency influencing tor include musical tradition style composer pretation high complexity variety give rise signal representation problem high level abstraction afforded perceptually biologically motivated processing technique deep learning much work expected music audio signal processing community near future b image video multimodality original dbn deep autoencoder oped demonstrated success simple image recognition dimensionality reduction coding task mnist 21 interesting note gain coding efficiency using autoencoder image data conventional method principal ponent analysis demonstrated 21 similar gain reported 30 speech data traditional technique vq 188 modified dbn developed layer model us bm type dbn applied norb database object recognition task error rate close best published result task reported particular shown dbn substantially outperforms shallow model svms deep architecture convolution structure found highly effective commonly used puter vision image recognition 154 161 189 190 notable advance wa recently achieved 2012 imagenet lsvrc contest 1000 different image class target million image training set test set consisting 150 000 image deep cnn approach described 121 achieved error rate considerably lower ous large deep cnns used sisting 60 million weight 650 000 neuron five convolutional layer together er additional three layer dnn described previously used top deep cnn layer although structure developed separately earlier work best combination accounted part success additional factor contributing final success 1 powerful regularization technique called dropout see detail 177 2 use saturating neuron relu compute max x 0 significantly speeding training process especially published online cambridge university press tutorial survey architecture algorithm application deep learning 21 efficient gpu implementation recently similar deep cnn approach stochastic pooling also reported excellent result four image datasets 191 deep network shown powerful computer vision image recognition task extract appropriate feature jointly performing discrimination 192 another type deep architecture ha created stantial impact image recognition le et al 154 reported excellent result using generative model based sparse autoencoders largely framework type extremely large network 11 billion parameter wa trained using thousand cpu core recent work along direction reported size network alternatively trained using cluster only 16 gpu server machine 193 use temporally conditional dbn video sequence human motion synthesis reported 85 conditional dbn make dbn weight associated fixed time window conditioned data previous time step computational tool offered type temporal dbn related recurrent network may provide opportunity improve toward efficient integration human speech production mechanism speech production model interesting study appeared 37 38 author propose evaluate novel application deep network learn feature audio video modality similar deep autoencoder architecture described section iv 30 used considered generalization single modality two modality feature learning ha demonstrated better feature video learned audio video information source able feature learning time author show learn shared audio video representation evaluate fixed task classifier trained data tested data vice versa work concludes deep ing architecture generally effective learning modal feature unlabeled data improving gle modality feature learning one exception setting using cuave dataset result presented 37 38 show improvement learning video feature video audio compared learning feature only video data however paper also show model 194 sophisticated signal ing technique extracting visual feature together method developed nally robust speech recognition 195 give best classification accuracy learning task beating feature derived generative deep tecture designed task deep generative architecture multimodal learning described 37 38 based autoencoder neural net probabilistic version based dbm ha appeared recently multimodal application 42 dbm used extract unified representation integrating separate modality useful classification information retrieval task rather using bottleneck layer deep autoencoder represent multimodal input probability density defined joint space multimodal input state suitably defined latent variable used representation advantage probabilistic lation lacking deep autoencoder missing modality information filled naturally pling conditional distribution bimodal data consisting image text multimodal dbm shown outperform deep multimodal autoencoder well multimodal dbn classification information retrieval task c language modeling research language document text processing ha seen increasing popularity recently signal ing community ha designated one main focus area society audio speech guage processing technical committee ha long history 196 197 using shallow neural work lm important component speech nition machine translation text information retrieval natural language processing recently dnns attracting attention statistical lm lm function capture salient statistical characteristic distribution sequence word natural language allows one make probabilistic diction next word given preceding one neural network lm one exploit neural network ability learn distributed representation reduce impact curse dimensionality distributed representation symbol vector feature characterize meaning bol neural network lm one relies learning algorithm discover meaningful tures basic idea learn associate word dictionary vector tion word corresponds point feature space one imagine dimension space corresponds semantic grammatical characteristic word hope functionally similar word get closer space least along some tions sequence word thus transformed sequence learned feature vector neural work learns map sequence feature vector probability distribution next word sequence distributed representation approach lm ha advantage allows model generalize well sequence not set training word sequence similar term feature distributed representation neural network tend map nearby input nearby output prediction corresponding word sequence similar feature mapped similar prediction published online cambridge university press 22 li deng idea neural network lm mented various study some involving deep tecture 198 temporally factored rbm wa used lm unlike traditional model factored rbm us distributed representation not only text word also word predicted approach generalized deeper structure reported 199 recent work neural network lm deep architecture found 51 particular work described 202 203 make use rnns build large scale language model achieves stability fast convergence training helped capping growing dient training rnns also develops adaptation scheme lm sorting training data respect relevance training model ing processing test data empirical comparison lm show much better performance rnn especially perplexity measure separate work applying rnn lm unit ters instead word found 46 interesting property predicting dependency making open closing quote paragraph strated usefulness practical application ha not clear word powerful representation natural language changing word character lm limit practical application scenario furthermore use hierarchical bayesian prior building deep recursive structure lm appeared 204 specifically process exploited bayesian prior deep four layer tic generative model built offer principled approach lm smoothing incorporating tion natural language discussed section iii type prior knowledge embedding readily able probabilistic modeling setup neural network one natural language processing sometimes debatable work ral language processing collobert weston 205 oped employed convolutional dbn common model simultaneously solve number classic lem including tagging chunking named entity tagging semantic role identification similar word identification recent work reported 206 developed fast purely discriminative approach parsing based deep recurrent convolutional tecture called graph transformer network collobert et al 207 provides comprehensive review line work specifically way applying unified neural work architecture related deep learning algorithm solve natural language processing problem scratch theme line work avoid feature engineering providing versatility unified feature constructed automatically deep learning applicable natural task system described 207 automatically learns nal representation vast amount mostly unlabeled training data one important aspect work described 205 207 transformation raw word tions term sparse vector high sion vocabulary size square even cubic vector processing sequent neural network layer known word embedding widely used natural language processing lm nowadays unsupervised learning used text word used learning signal neural network excellent tutorial wa recently given 208 explains neural network trained form word embedding originally proposed 205 recent work proposes new way word embedding better capture semantics word incorporating local global document context better account homonymy polysemy learning multiple ding per word 209 also evidence use rnn also provide empirically good performance word embedding 203 concept word embedding wa recently extended single language two producing gual word embeddings machine translation application 210 211 good performance wa shown zou et al 210 chinese semantic similarity bilingual trained embeddings use embeddings compute semantic similarity phrase pair wa shown improve bleu score slightly machine translation hand gao et al 211 made use word embeddings source target language raw input tures dnns extract semantic feature translation score computed measuring distance semantic feature new feature space dnn weight learned directly mize quality bleu score machine translation another area applying deep learning natural language processing appeared 86 recursive neural network used build deep tecture network shown capable successful merging natural language word based learned semantic transformation original feature deep learning approach provides excellent performance natural language parsing approach also demonstrated author successful parsing natural scene image related study similar recursive deep architecture used paraphrase tion 212 predicting sentiment distribution text 213 recent work socher et al 214 extended recursive neural network tensor similar way dnn wa extended tensor sion 215 applied semantic compositionality recursive neural tensor network resulted semantic word space capable expressing meaning longer phrase drastically improved prediction accuracy sentiment label published online cambridge university press tutorial survey architecture algorithm application deep learning 23 e information retrieval discus application dbn related deep autoencoder advanced deep learning method developed recently document indexing mation retrieval salakhutdinov hinton 216 217 showed hidden variable final layer dbn not only easy infer also give better representation ument based feature widely used latent semantic analysis traditional approach information retrieval use pact code produced deep autoencoder document mapped memory address way cally similar text document located nearby address facilitate rapid document retrieval mapping vector compact code highly efficient requiring only matrix multiplication subsequent sigmoid function evaluation hidden layer encoder part network briefly lowest layer dbn represents count vector document top layer represents leaned binary code document top two layer dbn form undirected associative memory remaining layer form bayesian also called belief network directed connection dbn composed set stacked rbms reviewed section v produce feedforward encoder network convert vector compact code ing rbms opposite order decoder network constructed map compact code vector structed vector combining encoder decoder one obtains deep autoencoder subject ther discussed section iv document coding subsequent retrieval deep model trained retrieval process start mapping query document binary code performing forward pas model thresholding similarity ming distance query binary code document binary code computed efficiently semantic hashing method described intended extract hierarchical semantic structure ded query document nevertheless adopts unsupervised learning approach dbn deep autoencoder parameter optimized struction document rather real goal information retrieval differentiate relevant document irrelevant one given query result fails significantly outperform line retrieval model based keyword matching semantic hashing model also face scalability challenge regarding matrix multiplication problem recently addressed huang et al 218 weakly supervised approach taken specifically series deep structured semantic model dssm developed work deterministic word ing constructed document query first produce vector relatively low ity feed dnns extracting semantic feature pair learning dnns instead using optimization criterion dssm construct novel objective function directly target goal document ranking enabled ability data supervision tion objective function defined basis cosine similarity measure semantic feature pair extracted dnns lent result based ndcg performance measure reported web search task using semantic feature produced dssm discriminative manner instead using deep net produce semantic feature aid information retrieval deng et al 152 applies dsn described section directly perform task information retrieval based rich set traditional feature query length text match translation probability query ment application deep learning information retrieval infancy expect work area emerge coming year including open constrained ad document search aimed predict document relevant input query v u r n c u n paper present brief history deep learning develops categorization scheme analyze existing deep architecture literature generative inative hybrid class deep autoencoder dsn including generalization rnn architecture one three class discussed analyzed detail appear popular promising approach author personal research experience application deep learning five broad area information processing reviewed literature deep learning vast mostly ing machine learning community processing community embraced deep learning only within past 4 year momentum growing fast overview paper written mainly processing perspective beyond surveying existing deep learning work classificatory scheme based ture nature learning algorithm developed analysis concrete example conducted hopefully provide insight reader better stand capability various deep learning system discussed paper connection among different similar deep learning method way design proper deep learning algorithm different circumstance throughout review important message veyed deep architecture archies feature highly desirable discussed difficulty learning parameter layer published online cambridge university press 24 li deng due pervasive local optimum diminishing exploding gradient generative pretraining method hybrid architecture reviewed detail section v appears offered useful albeit cal solution poor local optimum optimization especially labeled training data limited deep learning emerging technology despite empirical promising result reported far much need developed importantly ha not experience deep learning researcher single deep learning technique successful classification task example popular learning strategy generative pretraining followed discriminative seems work well empirically many task failed work some task reviewed success deep learning number perceptual task speech language vision task require trivial internal representation tion retrieval natural language processing task artificial intelligence causality inference decision making would likely benefit deep learning approach deep learning branch machine learning graphical model kernel method enhance issue remain explored recent published work showed vast room improve current optimization technique learning deep architecture 47 48 50 120 219 extent training important learning full set parameter deep architecture ha currently investigation especially large amount labeled training data available reduces even obliterates need model regularization some experimental result discussed paper 6 effective scalable parallel algorithm critical training deep model large data many mon application speech recognition machine translation information retrieval web scale popular stochastic ent technique known parallelization computer recent advance developing chronous stochastic gradient learning showed promise using cpu cluster 120 219 gpu cluster 193 make deep learning technique scalable large training data theoretically sound parallel ing algorithm effective architecture ing one need developed 49 50 112 120 220 one major barrier application dnns related deep model currently requires erable skill experience choose sensible value learning rate schedule strength regularizer number layer number unit per layer etc sensible value one parameter may depend value chosen tuning dnns especially expensive some interesting method ing problem developed recently including random sampling 221 bayesian optimization cedure 222 research needed tant area finally solid theoretical foundation deep learning need established myriad aspect example success deep learning vised mode ha not demonstrated much supervised learning yet essence major tion deep learning lie right unsupervised learning aimed automatic discovery data representation appropriate objective learning effective sentations may deep learning architecture algorithm use distributed representation effectively entangle hidden explanatory factor variation data computational neuroscience model hierarchical brain structure learning style help improve engineering deep learning architecture rithms important question need intensive research order push frontier deep learning r e f e r e n c e 1 deng overview learning information processing proc signal information processing annu summit conf october 2011 2 deng expanding scope signal processing ieee signal process 25 3 2008 3 hinton osindero teh fast learning algorithm deep belief net neural 18 2006 4 bengio learning deep architecture ai found trend mach 2 1 2009 5 bengio courville vincent representation learning review new perspective ieee trans pattern anal mach 35 2013 6 hinton et al deep neural network acoustic modeling speech recognition ieee signal process 29 6 2012 7 yu deng deep learning application signal information processing ieee signal process 28 2011 154 8 arel rose karnowski deep machine learning new frontier artificial intelligence ieee computational intelligence 5 2010 9 markoff scientist see promise program new york time november 24 2012 10 cho saul kernel method deep learning nip 2009 11 deng tur use kernel deep vex network learning spoken language standing proc ieee workshop spoken language technology december 2012 12 vinyals jia deng darrell learning recursive perceptual representation proc nip 2012 13 baker et al research development direction speech recognition understanding ieee signal process 26 3 2009 14 baker et al updated min report speech recognition understanding ieee signal process 26 4 2009 published online cambridge university press tutorial survey architecture algorithm application deep learning 25 15 deng computational model speech production putational model speech pattern processing verlag 1999 berlin heidelberg 16 deng switching dynamic system model speech tion acoustic mathematical foundation speech language processing springer new york 2003 17 george brain might work hierarchical poral model learning recognition thesis stanford university 2008 18 bouvrie hierarchical learning theory application speech vision thesis mit 2009 19 poggio brain might work role information learning understanding replicating intelligence tion science technology new century jacovitt pettorossi consolo senni ed lateran university press 2007 amsterdam netherlands 20 glorot bengio understanding difficulty training deep feedforward neural network proc aistat 2010 21 hinton salakhutdinov reducing dimensionality data neural network science 313 5786 2006 22 dahl yu deng acero hmms large vocabulary continuous speech recognition proc icassp 2011 23 mohamed yu deng investigation ing deep belief network speech recognition proc speech september 2010 24 mohamed dahl hinton acoustic modeling using deep belief network ieee trans audio speech lang 20 1 2012 25 dahl yu deng acero hmms large vocabulary continuous speech recognition ieee trans audio speech lang 20 1 2012 26 mohamed hinton penn understanding deep belief network perform acoustic modelling proc icassp 2012 27 vincent larochelle lajoie bengio manzagol stacked denoising autoencoders leaning useful representation deep network local denoising criterion mach learn 11 2010 28 rifai vincent muller glorot bengio contractive autoencoders explicit invariance feature extraction proc icml 2011 29 ranzato boureau lecun sparse feature learning deep belief network proc nip 2007 30 deng seltzer yu acero mohamed hinton binary coding speech spectrogram using deep proc interspeech 2010 31 bengio de mori flammia kompe global tion neural network hidden markov model hybrid proc proc eurospeech 1991 32 bourlard morgan connectionist speech recognition hybrid approach kluwer norwell 1993 33 morgan deep wide multiple layer automatic speech recognition ieee trans audio speech lang 20 1 2012 34 deng li machine learning paradigm speech recognition overview ieee trans audio speech 21 2013 35 lecun chopra ranzato huang model document recognition computer vision proc int conf document analysis recognition icdar 2007 36 ranzato poultney chopra lecun efficient learning sparse representation model proc nip 2006 37 ngiam khosla kim nam lee ng multimodal deep learning proc icml 2011 38 ngiam chen koh ng learning deep energy model proc icml 2011 39 hinton krizhevsky wang transforming proc int conf artificial neural network 2011 40 salakhutdinov hinton deep boltzmann machine proc aistats 2009 41 salakhutdinov hinton better way pretrain deep mann machine proc nip 2012 42 srivastava salakhutdinov multimodal learning deep boltzmann machine proc nip 2012 43 dahl ranzato mohamed hinton phone tion restricted boltzmann machine proc nip 23 2010 44 poon domingo network new deep tecture proc conf uncertainty artificial ligence barcelona spain 2011 45 gen domingo discriminative learning network proc nip 2012 46 sutskever marten hinton generating text recurrent neural network proc icml 2011 47 marten deep learning optimization proc icml 2010 48 marten sutskever learning recurrent neural network optimization proc icml 2011 49 bengio boulanger pascanu advance optimizing recurrent network proc icassp 2013 50 sutskever training recurrent neural network thesis university toronto 2013 51 mikolov karafiat burget cernocky khudanpur recurrent neural network based language model proc icassp 2010 52 mesnil deng bengio investigation architecture learning method spoken guage understanding proc interspeech 2013 53 deng dynamic speech model theory algorithm application morgan claypool december 2006 54 deng generalized hidden markov model conditioned trend function time speech signal signal 27 1 1992 55 deng stochastic model speech incorporating hierarchical nonstationarity ieee trans speech audio 1 4 1993 56 deng aksmanovic sun wu speech recognition using hidden markov model polynomial regression function nonstationary state ieee trans speech audio 2 4 1994 57 ostendorf digalakis kimball hmm segment model unified view stochastic modeling speech recognition ieee trans speech audio 4 5 1996 58 deng sameti transitional speech unit tion regressive markov state application speech recognition ieee trans speech audio 4 4 1996 published online cambridge university press 26 li deng 59 deng aksmanovic phonetic sification using hidden markov model mixture trend function ieee trans speech audio 5 1997 60 yu deng solving nonlinear estimation problem using spline ieee signal process 26 4 2009 61 yu deng gong acero novel framework training algorithm hidden markov el ieee trans audio speech lang 17 7 2009 1360 62 zen nankaku tokuda continuous stochastic feature mapping based trajectory hmms ieee trans audio speech lang 19 2 2011 63 zen gale nankaku tokuda product expert statistical parametric speech synthesis ieee trans audio speech lang 20 3 2012 64 ling richmond yamagishi articulatory control based parametric speech synthesis using multiple regression ieee trans audio speech lang 21 2013 65 ling deng yu modeling spectral envelope using restricted boltzmann machine statistical parametric speech synthesis icassp 2013 66 shannon zen byrne autoregressive model tical parametric speech synthesis ieee trans audio speech lang 21 3 2013 67 deng ramsay sun production model structural basis automatic speech recognition speech 33 1997 68 bridle et al investigation segmental hidden dynamic el speech coarticulation automatic speech recognition final report 1998 workshop language engineering clsp john hopkins 1998 69 picone et al initial evaluation hidden dynamic model conversational speech proc icassp 1999 70 minami mcdermott nakamura katagiri nition method parametric trajectory synthesized using direct relation static dynamic feature vector time series proc icassp 2002 71 deng huang challenge adopting speech recognition commun acm 47 1 2004 72 deng efficient decoding strategy tional speech recognition using constrained nonlinear space model ieee trans speech audio 11 6 2003 73 deng mixture dynamic model taneous speech recognition ieee trans speech audio 12 1 2004 74 deng yu acero structured speech modeling ieee trans audio speech lang 14 5 2006 75 deng yu acero bidirectional target filtering model speech coarticulation implementation phonetic recognition ieee trans audio speech 14 1 76 deng yu use differential cepstra acoustic feature hidden trajectory modeling phonetic recognition proc icassp april 2007 77 bilmes bartels graphical model architecture speech recognition ieee signal process 22 2005 78 bilmes dynamic graphical model ieee signal process 33 2010 79 rennie hershey olsen multitalker speech recognition graphical modeling approach ieee signal process 33 2010 80 wohlmayr stark pernkopf probabilistic interaction model multipitch tracking factorial hidden markov model ieee trans audio speech lang 19 4 2011 81 stoyanov ropson eisner empirical risk minimization graphical model parameter given approximate inference decoding model structure proc aistat 2011 82 kurzweil create mind viking book december 2012 83 fine singer tishby hierarchical hidden markov model analysis application mach 32 1998 84 oliver garg horvitz layered representation ing inferring office activity multiple sensory channel comput vi image 96 2004 85 taylor hinton roweis modeling human motion using binary latent variable proc nip 2007 86 socher lin ng manning learning continuous phrase representation syntactic parsing recursive neural network proc icml 2011 87 juang chou lee minimum classification error rate method speech recognition ieee trans speech audio 5 1997 88 chengalvarayan deng speech trajectory discrimination using minimum classification error learning ieee trans speech audio 6 6 1998 89 povey woodland minimum phone error improved discriminative training proc icassp 2002 90 deng chou discriminative learning sequential pattern recognition unifying review speech recognition ieee signal process 25 2008 91 jiang li parameter estimation statistical model using convex optimization advanced method discriminative ing speech language processing ieee signal process 27 3 2010 92 yu deng acero minimum sification error training speech recognition task proc icassp 2007 93 xiao deng geometric perspective training gaussian model ieee signal process 27 6 2010 123 94 gibson hain error approximation minimum phone error acoustic model estimation ieee trans audio speech lang 18 6 2010 95 yang furui combining crf model joint source channel model machine transliteration proc acl uppsala sweden 2010 96 yu wang deng sequential labeling using conditional random field sel top signal 4 2010 973 97 hifny renals speech recognition using augmented tional random field ieee trans audio speech lang 17 2 2009 98 heintz brew discriminative input stream combination conditional random field phone recognition ieee trans audio speech lang 17 8 2009 published online cambridge university press tutorial survey architecture algorithm application deep learning 27 99 zweig nguyen segmental crf approach large lary continuous speech recognition proc asru 2009 100 peng bo xu conditional neural field proc nip 2009 101 heigold ney lehnen gas schluter equivalence generative model ieee trans audio speech lang 19 5 2011 102 yu deng hidden conditional random field phonetic recognition proc interspeech september 2010 103 yu wang karam deng language recognition using conditional random field proc icassp 2010 104 pinto garimella hermansky bourlard analysis hierarchical phone posterior probability estimator ieee trans audio speech lang 19 2 2011 105 ketabdar bourlard enhanced phone posterior ing speech recognition system ieee trans audio speech lang 18 6 2010 106 morgan et al pushing envelope aside speech recognition ieee signal process 22 5 2005 107 deng yu deep convex network scalable architecture speech pattern classification proc interspeech 2011 108 deng yu platt scalable stacking learning building deep architecture proc icassp 2012 109 tur deng towards deep standing deep convex network semantic utterance tion proc icassp 2012 110 lena nagata baldi deep spatiotemporal architecture learning protein structure prediction proc nip 2012 111 hutchinson deng yu deep architecture ear modeling hidden representation application phonetic recognition proc icassp 2012 112 hutchinson deng yu tensor deep stacking work ieee trans pattern anal mach 35 2013 1957 113 deng hassanein elmasry analysis correlation ture neural predictive model application speech nition neural 7 2 114 robinson application recurrent net phone probability estimation ieee trans neural 5 1994 115 graf fernandez gomez schmidhuber ist temporal classification labeling unsegmented sequence data recurrent neural network proc icml 2006 116 graf mahamed hinton speech recognition deep recurrent neural network proc icassp 2013 117 graf sequence transduction recurrent neural network representation learning worksop icml 2012 118 lecun bottou bengio haffner ing applied document recognition proc ieee 86 1998 2324 119 ciresan giusti gambardella schidhuber deep ral network segment neuronal membrane electron microscopy image proc nip 2012 120 dean et al large scale distributed deep network proc nip 2012 121 krizhevsky sutskever hinton imagenet classification deep convolutional neural network proc nip 2012 122 mohamed jiang penn applying volutional neural network concept hybrid model speech recognition icassp 2012 123 deng yu exploring convolutional neural network structure optimization speech recognition proc interspeech 2013 124 deng yu jiang deep segmental neural network speech recognition proc interspeech 125 sainath mohamed kingsbury ramabhadran lutional neural network lvcsr proc icassp 2013 126 deng yu deep convolutional neural work using heterogeneous pooling trading acoustic invariance phonetic confusion proc icassp 2013 127 lang waibel hinton neural network tecture isolated word recognition neural 3 1 1990 128 hawkins blakeslee intelligence new standing brain lead creation truly intelligent machine time book new york 2004 129 waibel hanazawa hinton shikano lang phoneme recognition using neural network ieee trans assp 37 3 1989 130 hawkins ahmad dubinsky hierarchical temporal memory including htm cortical learning algorithm numenta technical report december 10 2010 131 lee ing new speech research paradigm automatic speech recognition proc icslp 2004 132 yu siniscalchi deng lee boosting attribute phone estimation accuracy deep neural network based speech recognition proc icassp 2012 133 siniscalchi yu deng lee exploiting deep neural network speech recognition neurocomputing 106 2013 134 siniscalchi svendsen lee modular search approach large vocabulary continuous speech recognition ieee trans audio speech lang 21 2013 135 yu seide li deng exploiting sparseness deep neural network large vocabulary speech recognition proc icassp 2012 136 deng sun statistical approach automatic speech nition using atomic speech unit constructed overlapping articulatory feature acoust soc 85 5 1994 137 sun deng based phonological model incorporating linguistic constraint application speech tion acoust soc 111 2 2002 138 sainath kingsbury ramabhadran improving training time deep belief network hybrid larger batch size proc nip workshop model december 2012 139 erhan bengio courvelle manzagol vencent bengio doe unsupervised help deep learning mach learn 11 2010 140 kingsbury optimization sequence tion criterion acoustic modeling proc icassp 2009 141 kingsbury sainath soltau scalable minimum bayes risk training deep neural network acoustic model using distributed optimization proc interspeech published online cambridge university press 28 li deng 142 larochelle bengio classification using discriminative restricted boltzmann machine proc icml 2008 143 lee grosse ranganath ng unsupervised ing hierarchical representation convolutional deep belief network communication acm vol 54 no 10 october 2011 pp 144 lee grosse ranganath ng convolutional deep belief network scalable unsupervised learning hierarchical representation proc icml 2009 145 lee largman pham ng unsupervised feature ing audio classification using convolutional deep belief network proc nip 2010 146 ranzato susskind mnih hinton deep generative model application recognition proc cvpr 2011 147 ney speech translation coupling recognition translation proc icassp 1999 148 deng speech recognition machine translation speech translation unifying discriminative framework ieee signal process 28 2011 149 yamin deng wang acero integrative inative technique spoken utterance classification ieee trans audio speech lang 16 2008 150 deng optimization information cessing criterion technique proc icassp 2012 151 deng information processing approach proc ieee 2013 152 deng gao deep stacking network information retrieval proc icassp 153 deng tur adaptive training robust spoken language understanding proc icassp 2013 154 le ranzato monga devin corrado chen dean ng building feature using large scale unsupervised learning proc icml 2012 155 seide li yu conversational speech transcription using deep neural network proc interspeech 2011 156 yan huo xu scalable approach using feature based acoustic modeling lvcsr proc interspeech 2013 157 welling hinton exponential family niums application information retrieval proc nip vol 20 2005 158 hinton practical guide training restricted boltzmann machine utml technical report university toronto august 2010 159 wolpert stacked generalization neural 5 2 1992 259 160 cohen de carvalho stacked sequential learning proc ijcai 2005 161 jarrett kavukcuoglu lecun best multistage architecture object recognition proc int conf computer vision 2009 162 pascanu mikolov bengio difficulty training recurrent neural network proc icml 2013 163 deng spontaneous speech recognition using cal coarticulatory model vocal tract resonance dynamic acoust soc 108 2000 164 togneri deng joint state parameter estimation nonlinear dynamic system model ieee trans nal 51 12 2003 165 mohamed dahl hinton deep belief network phone recognition proc nip workshop deep learning speech recognition related application 2009 166 sivaram hermansky sparse multilayer perceptron phoneme recognition ieee trans audio speech lang 20 1 2012 167 kubo hori nakamura integrating deep neural network structural classification approach based weight transducer proc interspeech 2012 168 deng et al recent advance deep learning speech research microsoft proc icassp 2013 169 deng hinton kingsbury new type deep neural work learning speech recognition related application overview proc icassp 2013 170 juang levinson sondhi maximum likelihood estimation multivariate mixture observation markov chain ieee trans inf theory 32 1986 171 deng lennig seitz mermelstein large lary word recognition using allophonic den markov model comput speech 4 4 1990 172 deng kenny lennig gupta seitz mermelstein phonemic hidden markov model continuous mixture output density large vocabulary word recognition ieee trans signal process 39 7 1991 173 sheikhzadeh deng speech recognition using hidden filter model parameter selection sensitivity power normalization ieee trans speech audio 2 1994 174 jaitly hinton learning better representation speech sound wave using restricted boltzmann machine proc icassp 2011 175 sainath kingbury ramabhadran novak mohamed making deep belief network effective large vocabulary tinuous speech recognition proc ieee asru 2011 176 jaitly nguyen vanhoucke application deep neural network large vocabulary speech recognition proc interspeech 2012 177 hinton srivastava krizhevsky sutskever nov improving neural network preventing feature detector arxiv 2012 178 yu deng dahl role speech recognition proc nip workshop 2010 179 yu li deng calibration confidence measure speech recognition ieee trans audio speech 19 2010 180 maas le neil vinyals nguyen ng rent neural network noise reduction robust asr proc interspeech 2012 181 ling deng yu modeling spectral envelope using restricted boltzmann machine deep belief network tical parametric speech synthesis ieee trans audio speech lang 21 10 2013 182 kang qian meng deep belief network speech synthesis proc icassp 2013 published online cambridge university press tutorial survey architecture algorithm application deep learning 29 183 zen senior schuster statistical parametric speech thesis using deep neural network proc icassp 2013 7966 184 fernandez rendel ramabhadran hoory tour prediction deep belief process hybrid model proc icassp 2013 185 humphrey bello lecun moving beyond feature design deep architecture automatic feature learning music matics proc ismir 2012 186 batternberg wessel analyzing drum pattern using tional deep belief network proc ismir 2012 187 schmidt kim learning acoustic feature deep belief network proc ieee application signal processing audio acoustic 2011 188 nair hinton object recognition deep belief net proc nip 2009 189 lecun bengio convolutional network image speech time series handbook brain theory neural network arbib ed mit press cambridge sachusetts 1995 190 kavukcuoglu sermanet boureau gregor mathieu lecun learning convolutional feature hierarchy visual recognition proc nip 2010 191 zeiler fergus stochastic pooling regularization deep convolutional neural network proc iclr 2013 192 lecun learning invariant feature hierarchy proc eccv 2012 193 coates huval wang wu ng catanzaro deep learning cot hpc proc icml 2013 194 papandreou katsamanis pitsikalis maragos adaptive multimodal fusion uncertainty compensation application audiovisual speech recognition ieee trans audio speech lang 17 3 2009 195 deng wu droppo acero dynamic compensation hmm variance using feature enhancement uncertainty puted parametric model speech distortion ieee trans speech audio 13 3 2005 196 bengio ducharme vincent jauvin neural bilistic language model proc nip 2000 197 fast evaluation connectionist language model int conf artificial neural network 2009 198 mnih hinton three new graphical model statistical language modeling proc icml 2007 199 mnih hinton scalable hierarchical distributed language model proc nip 2008 200 le allauzen wisniewski yvon training continuous space language model some practical issue 2010 201 le oparin allauzen gauvain yvon structured output layer neural network language model proc icassp 2011 202 mikolov deoras povey burget cernocky gy training large scale neural network language model proc ieee asru 2011 203 mikolov statistical language model based neural network thesis brno university technology 2012 204 huang renals hierarchical bayesian language model conversational speech recognition ieee trans audio speech lang 18 8 2010 205 collobert weston unified architecture natural language processing deep neural network multitask learning proc icml 2008 206 collobert deep learning efficient discriminative parsing proc nip workshop deep learning unsupervised feature learning 2010 207 collobert weston bottou karlen kavukcuoglu kuksa natural language processing almost scratch mach learn 12 2011 208 socher bengio manning deep learning nlp rial acl 2012 209 huang socher manning ng improving word sentations via global context multiple word prototype proc acl 2012 210 zou socher cer manning bilingual word ding machine translation proc emnlp 2013 211 gao yih deng learning semantic tions phrase translation model september 2013 212 socher pennington huang ng manning supervised recursive autoencoders predicting sentiment butions proc emnlp 2011 213 socher pennington huang ng manning dynamic pooling unfolding recursive autoencoders paraphrase tion proc nip 2011 214 socher perelygin wu chuang manning ng potts recursive deep model semantic compositionality sentiment treebank proc emnlp 2013 215 yu deng seide deep tensor neural network application large vocabulary speech recognition ieee trans audio speech lang 21 2013 216 salakhutdinov hinton semantic hashing proc sigir workshop information retrieval application graphical model 2007 217 hinton salakhutdinov discovering binary code ments learning deep generative model top cognit 2010 218 huang gao deng acero heck learning deep structured semantic model web search using clickthrough data acm int conf information knowledge management cikm 2013 219 le ngiam coates lahiri prochnow ng optimization method deep learning proc icml 2011 220 bottou lecun large scale online learning proc nip 2004 221 bergstra bengio random search mization mach learn 3 2012 222 snoek larochelle adam practical bayesian optimization machine learning algorithm proc nip published online cambridge university press