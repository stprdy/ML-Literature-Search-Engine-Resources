lu et al bmc medical research methodology 2022 research comparative study deep learning model text classification unstructured medical note various level class imbalance hongxia louis cyril abstract background discharge medical note written physician contain important information health tion patient many deep learning algorithm successfully applied extract important information unstructured medical note data entail subsequent actionable result medical domain study aim explore model performance various deep learning algorithm text classification task medical note respect different disease class imbalance scenario method study employed seven artificial intelligence model cnn convolutional neural network transformer encoder pretrained bert bidirectional encoder representation transformer four typical sequence neural network model namely rnn recurrent neural network gru gated recurrent unit lstm long memory long memory classify presence absence 16 disease condition patient discharge summary note analyzed question composition 16 binary separate classification problem model performance seven model 16 datasets ous level imbalance class compared term area curve receiver operating characteristic area curve precision recall score balanced accuracy well training time model performance also compared combination different word ding approach glove biowordvec no word embeddings result analysis 16 binary classification problem showed transformer encoder model form best nearly scenario addition disease prevalence close greater 50 convolutional neural network model achieved comparable performance transformer encoder ing time wa shorter second fastest model shorter transformer encoder shorter model biowordvec embeddings slightly improved performance model disease prevalence scenario cnn model performed better without word embeddings addition training time wa significantly reduced glove embeddings model author open access article licensed creative common attribution international license permit use sharing adaptation distribution reproduction any medium format long give appropriate credit original author source provide link creative common licence indicate change made image third party material article included article creative common licence unless indicated otherwise credit line material material not included article creative common licence intended use not permitted statutory regulation exceeds permitted use need obtain permission directly copyright holder view copy licence visit creative common public domain dedication waiver applies data made available article unless otherwise stated credit line data open access correspondence rakovski 1 schmid college science technology chapman university 1 university dr orange ca 92866 usa full list author information available end article page 2 12 lu et al bmc medical research methodology 2022 background unstructured medical note discharge ries valuable health record contain rich clinical information patient health condition some disease detail may not reflected structured data field many study carried extract additional information unstructured medical note make mortality prediction based data alone study aim explore model performance various deep learning algorithm text classification task medical note help point attention research community potential text tion behavior various nlp natural language processing algorithm medical note data different class imbalance scenario algorithm compared study include traditional recurrence network rnn gru lstm well cnn attention algorithm transformer encoder model performance evaluated term score balanced accuracy deep learning algorithm rnn gru lstm model typically used nlp task achieved promising result designed work sequence data allowing vious output used input allows flow information previous element posterior ments sequence grus lstms advanced variant vanilla rnns additional gate mathematical operation involving additional weight trained added rnn unit overcome vanishing exploding gradient problem rnns often suffer long sequence lstms two additional gate pared grus entail better performance long sequence allowing information back carried current unit additional layer top lstm unit go backwards information posterior element passed previous unit feature work particularly well text data since context information previous element terior element important interpretation word extra gate layer however consequently result complexity longer training time recently cnns attracted attention nlp task due superior performance especially lengthy text cnns widely used computer vision image classification image recognition 12 13 cnns computer vision feature sional convolutional layer extract information neighboring pixel thus nizes pattern across space cnns nlp task employ convolutional layer extract mation adjacent word not quite clear exactly cnn outperforms traditional nlp algorithm rnn gru lstm many case widely accepted number kernel convolutional layer cnn serf n adjacent word treated one technique nlp traditional nlp algorithm anything would cumbersome cnn rithms however easily adopt even higher gram technique depending length text without increasing computational cost transformer successfully applied many nlp task since introduction transformer vaswani et al 17 transformer model novel network architecture based solely attention mechanism dispensing recurrence tions entirely 17 transformer task lation question answering encoders decoder transformer text classification task typically only encoders encoder ha two layer layer feedforward layer unlike recurrent network process word sequentially taking information ous word input processing current word transformer process input sequence whole another novel design transformer duce positional embedding capture information order word positional embeddings added word embeddings fed encoder one major disadvantage transformer high computational cost especially text sequence long longer sequence tionately expensive attention quadratic sequence length due word every word sequence 18 bert encoder representation transformer language sentation model wa designed deep bidirectional representation unlabeled text conclusion classification task medical note transformer encoders best choice computation resource not issue otherwise class relatively balanced cnns leading candidate competitive performance computational efficiency keywords medical note text classification bert cnn deep learning embedding transformer encoder page 3 12 lu et al bmc medical research methodology 2022 bookscorpus english wikipedia 18 two tures bert model introduced original paper model ha 12 transformer encoders 12 head encoder hidden size 768 total 110 parameter model ha 24 former encoders 16 head encoder hidden size 1024 total 340 parameter bert model achieved performance number natural language understanding task wa published ha successfully applied many nlp task since one major drawback bert costly computational resource needed train model due large number parameter 22 data study used discharge summary data made available harvard university 2008 challenge classify obesity comorbidities multiple class presence absence questionable disease annotated textual ments intuitive judgment respectively 23 data consist unique discharge summary partner healthcare research patient data tory annotated list 16 disease tions three expert massachusetts general hospital 24 literature classification task using dataset focused optimizing score classification task primarily ing method method bined traditional machine learning algorithm svm involved heavy text preprocessing tailored specific discharge summary association 16 disease 24 example ware et al employed apelon terminology engine provide synonym set drug name used domain specific language dsl frame rule identify presence disease 25 yang et al built ary disease symptom treatment medication synonym 26 solt et al also developed regular expression driven string replacement dictionary occurrence relevant abbreviation synonym plain english equivalent spelling variant frequent typo suffixed form etc 27 goal study compare behavior 7 deep learning algorithm term performance datasets training efficiency ability handle imbalanced class well effect two type word embedding approach therefore simplified task problem intuitive label only applied eral text preprocessing converted data 16 datasets binary classification discharge summary different binary come variable denoting presence absence ticular disease disease prevalence 16 disease condition datasets listed table ease prevalence range 5 73 eridemia least prevalent hypertension prevalent disease prevalence also reflects class imbalance level positive disease ence negative disease absence class binary classification problem discharge summary note dataset include content description current illness medical history information physical tion laboratory examination treatment service provided applicable discharge medication unstructured medical note require special treatment fed deep learning algorithm first converted word lower case word disease disease treated word removed number tions not carry significant information diagnosis standard stop word common word any natural language not add much value nlp modeling removed well template word discharge admission date word only one two acters mg detailed descriptive statistic variable denoting number word character discharge summary cleaning shown table particular average number word cleaning 1170 557 minimum 146 50 maximum 4280 2098 respectively similarly average number acters cleaning 6870 4429 minimum 903 410 maximum respectively table 1 disease prevalence n osa obstructive sleep apnea pvd peripheral vascular disease oa osteo arthritis gerd gastroesophageal reflux disease chf congestive heart failure cad coronary artery disease disease disease prevalence disease disease prevalence hypertriglyceridemia 5 gerd 20 venous insufficiency 7 depression 20 asthma 13 obesity 40 gout 13 chf 43 osa 14 hypercholesterolemia 47 pvd 15 cad 55 gallstone 15 diabetes 66 oa 18 hypertension 73 page 4 12 lu et al bmc medical research methodology 2022 dataset wa randomly split stratified according disease label training test set containing 75 25 data respectively 10 iteration average metric 10 iteration used comparison model formance iteration dataset wa randomly split stratification model trained training set tested test set regular tokenizing kera tokenizer wa formed convert text number model except bert us different tokenization technique wordpiece tokenization 18 28 lar tokenizing procedure take following two step first creates dictionary based word frequency training set every unique word assigned integer value index integer 1 maximum number unique word text 0 reserved padding form text sequence integer taking word discharge summary note looking dictionary replacing responding index next medical note test set converted sequence integer looking word dictionary previously constructed training set reason dictionary built based training set only avoid information leaking test set test set supposed contain new data model ha never seen point cal note converted number different length discharge note ha different length arbitrarily chose maximum sequence length 557 average length sequence dataset forced sequence length truncating longer sequence padding shorter sequence 0 preprocessing step original discharge summary note transformed sequence integer length ready fed deep learning model model maximum sequence length allowed method cnn model eight filter nel size eight rnn model eight unit gru model eight unit lstm model eight unit model eight unit transformer encoder one encoder two head fit 16 datasets batch size 32 20 epoch model layer classification output layer wa also fit batch size 32 3 epoch el word embedding layer word representation capture similarity word input length 557 output dimension base model word embeddings positional embeddings dimension 512 since focus study wa compare formance different model instead optimizing specific model tried use default hyperparameters model many study debate choice hyperparameters batch size any number 1 number sample training set many factor could affect optimal choice batch size available computational resource size data choice optimizer ing rate generally speaking larger batch size likely algorithm converge global minimum memory required ing process batch size small model prone noisiness thus requires smaller learning rate stability result training step thus longer training time sweet spot dataset model specific requires trial error search commonly used batch size 16 32 64 small datasets chose use 32 dataset wa relatively small table 2 descriptive statistic descriptive statistic number word number character cleaning cleaning cleaning cleaning minimum 146 50 903 410 25 percentile 819 391 4798 3089 median 1084 517 6391 4098 mean 1170 557 6870 4429 75 percentile 1425 687 8404 5420 maximum 4280 2098 standard deviation 506 242 2960 1931 page 5 12 lu et al bmc medical research methodology 2022 input sequence long similar choice batch size choice number epoch also dataset model specific larger number epoch requires longer training time could result overfitting small number epoch could result underfitting chose use 20 epoch model except bert 3 epoch author bert mended epoch bert 18 dropout wa used mitigate overfitting problem commonly used dropout rate chose use model hyperparameters learning rate activation function optimizer chose use default value model table 3 show architecture information seven model model evaluated term roc area curve receiver ing characteristic area curve precision recall score balanced accuracy detailed report metric including precision recall specificity shown table appendix word embeddings replaced traditional bow representation count vectorization become essential nlp task projection tokenized word vector onto embedding matrix learned data training large datasets one advantage word embeddings bow significantly lower dimension equal maximum length input sequence versus imum number unique word dataset bow another advantage dense feature le zero word embeddings compared sparse feature bow 32 word embeddings widely used nlp task small datasets better ture semantic syntactic meaning word since trained large datasets many different model creating word embeddings glove fasttext biowordvec among others developed study implemented glove embeddings dimension 200 biowordvec embeddings dimension 200 model except transformer encoder no word embeddings model word embeddings dimension 200 learned data training trained word embeddings not used former encoder study not priority design transformer bert additionally requires special token input sequence cl sep glove wa trained five corpus varying size including wikipedia gigaword web data common 33 wordvec embeddings trained biomedical text biomedical controlled vocabulary called medical subject heading mesh accommodate nlp need biomedical domain 34 result figure 1 b c show score balanced accuracy seven model 16 datasets ordered lowest highest ing disease prevalence transformer encoder produced highest 13 datasets highest 13 diabetes highest 14 datasets highest 14 diabetes highest score 15 datasets highest 15 diabetes highest balanced accuracy 14 datasets highest 14 diabetes cnn exceeded transformer encoder 3 datasets term hypertriglyceridemia chf cad 2 datasets term triglyceridemia cad 1 dataset term score cad 2 datasets term table 3 model architecture model number embedding dimension max sequence length dropout activation function optimizer total parameter cnn 8 200 557 relu adam rnn 8 200 557 relu adam 8 200 557 relu adam lstm 8 200 557 relu adam 8 200 557 relu adam transformer encoder 1 encoder 2 head 200 557 relu adam 12 encoders 12 head 768 512 layer relu layer adam layer 110 page 6 12 lu et al bmc medical research methodology 2022 balanced accuracy cad hypertension value score balanced accuracy unavailable some model due zero value true positive actual disease presence correctly predicted wa encountered disease prevalence wa le 20 zero value true positive occur rithm predicts positive case negative similarly good portion positive case small number small sample classified score balanced accuracy would low overall accuracy still high not surprising class highly imbalanced sample size small not many minority case algorithm learn distinct characteristic minority class cost misclassifying minority case small even minority case misclassified sordo et al reported sample size increase machine learning algorithm naïve bayes svm decision tree show substantial improvement formance predicting smoking status patient text excerpt extracted narrative medical report 35 reasonable algorithm need learn latent information data exposed large enough amount data class given small sample size total lengthy medical note fact deep learning algorithm data hungry performance transformer encoder cnn algorithm quite promising relatively balanced datasets datasets transformer encoder cnn algorithm formed poorly highly imbalanced five model performed even worse low performance imbalanced datasets result sample minority class rithms learn number minority case fig 1 model performance score balanced accuracy some point graph missing due na value resulted zero value true positive highly imbalanced datasets page 7 12 lu et al bmc medical research methodology 2022 training set not big enough represent minority class test set table 4 show small sample size minority class training set especially class highly imbalanced datasets disease prevalence le 10 number ples minority class training set no 62 challenging any algorithm perform well shown fig 1 b c prevalence number sample minority class increase performance model improved substantially term score balanced racy therefore used big enough datasets large enough number sample class algorithm could render excellent performance figure 2 display training time seven rithms averaged 16 classification task computer intel r core tm cpu ghz 16 gb ram cnn model ran consistently faster model task average cnn ran faster much rnn wa second fastest algorithm faster table 4 number sample class training test set disease prevalence training set test set disease presence presence disease absence disease presence disease absence hypertriglyceridemia 5 50 878 17 292 venous insufficiency 7 62 865 21 289 asthma 13 123 805 41 268 gout 13 120 808 40 269 osa 14 129 799 43 266 pvd 15 135 793 45 264 gallstone 15 141 787 47 262 oa 18 168 760 56 253 gerd 20 184 743 62 248 depression 20 187 741 62 247 obesity 40 374 554 125 184 chf 43 402 526 134 175 hypercholesterolemia 47 432 496 144 165 cad 55 512 416 170 139 diabetes 66 616 312 205 104 hypertension 73 677 250 226 84 fig 2 average training time page 8 12 lu et al bmc medical research methodology 2022 transformer encoder faster base model time ing due large number parameter fed top layer even though wa only using 3 epoch figure 3 show model performance word embeddings wa slight improvement term score balanced racy rnn gru lstm glove biowordvec embeddings disease prevalence greater 50 biowordvec embeddings performed slightly better glove embeddings case improvement significant model cnn model performance better without word embeddings figure 4 show training time model without word embeddings averaged iteration datasets word embeddings shortened training time significantly model model glove embeddings ran fastest training time wa shortened much 61 glove embeddings without embeddings discussion medical note often lengthy thus constitute data data relatively small sample size challenging due inevitable overfitting problem longest discharge summary data word cleaning sample size wa only issue wa gated following treatment data first tokenizing discharge note different length forced arbitrary length 557 study truncating longer note padding shorter note 0 wa no formula choice length ying wen et al reported length close average length text training fig 3 model performance without word embeddings score balanced accuracy some point graph missing due nan value resulted zero value true positive highly imbalanced datasets page 9 12 lu et al bmc medical research methodology 2022 set generally produce better result length small result great loss information length large lead sparse data shorter note include noise longer note 32 second relatively small number epoch 20 epoch wa used train model avoid model memorizing training data another source overfitting data study transformer encoder stood among model nearly class imbalance scenario strength lie feature word sequence considered encoding specific word effectively resolve issue forgetting information previous word long sequence recurrent network often ter cnn also outperformed five model achieved comparable performance transformer encoder disease prevalence close greater 50 somewhat surprisingly powerful nlp model performed poorly scenario likely due fact wa trained general text not medical text thus failed capture information relationship medical word addition maximum sequence length allowed 512 average length charge summary note dataset 557 could lead much loss information result five model different word embeddings show biowordvec embeddings slightly improved performance model some datasets general model biowordvec embeddings performed slightly better glove embeddings reasonable since wordvec embeddings trained biomedical text glove embeddings trained corpus not biomedical domain reason improvement biowordvec wa not quite noticeable may due fact many medical word cially medication name zanflex fondapurinox diurhesis word dopthromycin fig 4 average training time without word embeddings page 10 12 lu et al bmc medical research methodology 2022 anestheteic amoxicil dataset still not ognized biowordvec expected even word not recognized glove presumably uted faster training time class highly imbalanced disease prevalence lower 30 higher 70 el performed poorly low low score balanced accuracy even no able score balanced accuracy mainly due unequal misclassification cost ing minority class doe not result much cost especially true dataset small sample minority class matter misclassified dataset large majority class minority class combination two used balance class small datasets collecting data minority class not feasible using data augmentation smote random swap random deletion etc increase number sample minority class may help improve performance model 19 addition lengthy text data length text sequence fed model may also affect model performance since short sequence may lose much information long sequence may result sparse data introduce noise well longer training time overfitting problem medical expert consultation available applying text preprocessing method implemented some top 10 challenge solution also help improve model performance 24 example abbreviation common medical note expanding help improve result since otherwise often treated unknown word thus not contributing any information addition drug name also not recognized tagging drug name strongly indicative certain disease also help improve prediction accuracy conclusion binary text classification task studied former encoder stood among algorithm studied term model performance metric roc score balanced accuracy class balanced cnn model performed equally well markedly shorter training time dataset wa highly imbalanced positive class disease presence minority roc may inflated may able metric evaluate model performance turn dataset highly imbalanced negative class disease absence minority may accurate measure model performance addition domain specific word embeddings biobert 39 clinicalbert 40 41 may help yield better result since word embeddings trained medical text using powerful bert model summary classification task medical note transformer encoders best choice computation resource not issue otherwise class relatively balanced cnns leading candidate comparable performance computational efficiency abbreviation ai artificial intelligence nlp natural language processing cnn tional neural network rnn recurrent neural network gated recurrent unit lstm long memory long memory bert encoder representation transformer bow area curve receiver ing characteristic area curve precision recall osa obstructive sleep apnea pvd peripheral vascular disease oa osteo arthritis gerd gastroesophageal reflux disease chf congestive heart failure cad coronary artery disease smote synthetic minority oversampling technique supplementary information online version contains supplementary material available additional file 1 appendix table model performance evaluation metric average 10 iteration table model performance evaluation metric average 10 iteration cnn word embeddings table model performance evaluation metric average 10 iteration rnn word embeddings table model performance evaluation metric average 10 iteration gru word embeddings table model mance evaluation metric average 10 iteration lstm word embeddings table model performance evaluation metric average 10 iteration word embeddings acknowledgement not applicable author contribution hl aggregated managed quality checked medical discharge mary note carried ai model fitting summarized result wrote section manuscript le reviewed analyzed result cr analyzed data interpreted result wrote section manuscript provided guidance insight study author read approved final manuscript funding study received no specific grant any funding agency public commercial sector availability data material data support finding study derived datasets publicly available simple step approval code study available git page 11 12 lu et al bmc medical research methodology 2022 declaration ethic approval consent participate study used deidentified data harvard website followed ethical consideration instituted provider data no participant involved study consent publication not applicable competing interest author declare no competing interest regarding article author detail 1 schmid college science technology chapman university 1 university dr orange ca 92866 usa 2 child health orange county choc orange ca 92868 usa received 21 august 2021 accepted 23 may 2022 reference feder vainstein rosenfeld r hartman hassid matias active deep learning detect demographic trait clinical note j biomed inform 2020 107 103436 miotto r percha bl glicksberg b lee hc cruz l dudley jt nabeel identifying acute low back pain episode primary care practice clinical note observational study jmir med inform 2020 8 2 gunjal h patel p thaker k nagrecha mohammed marchawala text summarization classification clinical discharge summary using deep learning 2020 ye j yao l shen j janarthanam r luo predicting mortality cally ill patient diabetes using machine learning clinical note bmc med inform decis mak 2020 20 11 yang yu x zhou lstm gru neural network performance comparison study taking yelp review dataset example 2020 international workshop electronic communication artificial intelligence iwecai girgis amer e gadallah deep learning algorithm detecting fake news online text 2018 international conference computer engineering system icces onan sentiment analysis product review based weighted word embeddings deep neural network concurrency putation practice experience 2020 kim h jeong sentiment classification using convolutional neural network appl sci 2019 9 11 hughes li kotoulas suzumura medical text classification using convolutional neural network informatics health connected wellness population health io press widiastuti ni convolution neural network text mining natural language processing iop conference series material science engineering 52010 banerjee ling chen mc hasan sa langlotz cp moradzadeh n chapman b amrhein mong rubin dl et al comparative ness convolutional neural network cnn recurrent neural network rnn architecture radiology text report classification artif intell med 2019 hijazi kumar r rowen c et al using convolutional neural network image recognition san jose cadence design system li q cai w wang x zhou feng dd chen medical image cation convolutional neural network 2014 international conference control automation robotics vision icarcv liu z huang h lu c lyu multichannel cnn attention text classification arxiv preprint 2020 zhao w joshi nair vn sudjianto shap value explaining based text classification model arxiv preprint 2020 cheng h yang x li z xiao lin interpretable text classification using cnn arxiv preprint 2019 vaswani shazeer n parmar n uszkoreit j jones l gomez kaiser ł polosukhin attention need advance neural mation processing system devlin j chang mw lee k toutanova bert deep bidirectional transformer language understanding arxiv preprint 2018 samghabadi n patwa p srinivas p mukherjee p da solorio aggression misogyny detection using bert approach proceeding second workshop trolling aggression cyberbullying gao z feng song x wu sentiment tion bert ieee access 2019 geng z yan h qiu x huang fasthan toolkit chinese nlp arxiv preprint 2020 zhang j chang w cheng yu h fu dhillon fast former extreme text classification advance neural information processing system 2021 34 harvard university obesity challenge 2008 data internet cited 2022 apr 28 available uzuner recognizing obesity comorbidities sparse data j med inform assoc 2009 16 4 ware h mullett cj jagannathan natural language processing framework ass clinical condition j med inform assoc 2009 16 4 yang h spasic keane ja nenadic text mining approach prediction disease status clinical discharge summary j med inform assoc 2009 16 4 solt tikk gál v kardkovács zt semantic classification disease discharge summary using classifier j med inform assoc 2009 16 4 schuster nakajima japanese korean voice search 2012 ieee international conference acoustic speech signal cessing icassp jastrzebski kenton z arpit ballas n fischer bengio storkey three factor influencing minimum sgd arxiv preprint 2017 kandel castelli effect batch size generalizability convolutional neural network histopathology dataset ict express 2020 6 4 smith sl kindermans pj ying c le q decay learning rate increase batch size arxiv preprint 2017 almeida f xexéo word embeddings survey arxiv preprint 2019 pennington j socher r manning cd glove global vector word representation proceeding 2014 conference empirical method natural language processing emnlp zhang chen q yang z lin h lu biowordvec improving cal word embeddings subword information mesh sci data 2019 6 1 sordo zeng sample size classification accuracy formance comparison international symposium biological medical data analysis wen zhang w luo r wang learning text representation using recurrent convolutional neural network highway layer arxiv preprint 2016 ibrahim torki imbalanced toxic comment sification using data augmentation deep learning 2018 ieee international conference machine learning application icmla lauren p qu g watta convolutional neural network clinical tive categorization 2017 ieee international conference big data big data lee j yoon w kim kim kim ch kang biobert trained biomedical language representation model biomedical text mining bioinformatics 2020 36 4 alsentzer e murphy jr boag w weng wh jin naumann mott publicly available clinical bert embeddings arxiv preprint 2019 page 12 12 lu et al bmc medical research methodology 2022 fast convenient online submission thorough peer review experienced researcher ﬁeld rapid publication acceptance support research data including large complex data type gold open access foster wider collaboration increased citation maximum visibility research website view per year bmc research always progress learn ready submit research ready submit research choose bmc benefit choose bmc benefit huang k altosaar j ranganath clinicalbert modeling clinical note predicting hospital readmission arxiv preprint 2019 publisher note springer nature remains neutral regard jurisdictional claim lished map institutional affiliation