survey deep learn algorithm techniqu applic samira pouyanfar florida intern univers saad sadiq yilin yan univers miami haiman tian florida intern univers yudong tao univers miami maria presa rey florida intern univers shyu univers miami chen iyengar florida intern univers field machin learn wit golden era deep learn slowli becom leader thi domain deep learn use multipl layer repres abstract data build comput model key enabl deep learn algorithm gener adversari network convolut neural network model transfer complet chang percept inform process howev exist apertur understand behind thi tremend domain becaus wa never previous repres multiscop perspect lack core understand render power method machin inhibit develop fundament level moreov deep learn ha repeatedli perceiv silver bullet stumbl block machin learn far truth thi articl present comprehens review histor recent art approach visual audio text process social network analysi natur languag process follow analysi pivot groundbreak advanc deep learn applic wa also undertaken review issu face deep learn unsupervis learn model onlin learn illustr challeng transform prolif futur research avenu cc concept comput methodolog network machin learn algorithm allel algorithm distribut algorithm theori comput learn theori addit key word phrase deep learn neural network machin learn distribut process big data survey acm refer format samira pouyanfar saad sadiq yilin yan haiman tian yudong tao maria presa rey shyu ching chen iyengar survey deep learn algorithm techniqu applic acm comput surv articl septemb page http author address pouyanfar tian rey chen iyengar school comput inform enc florida intern univers miami fl email chen iyengar sadiq yan tao shyu depart electr comput engin univers miami coral gabl fl email shyu permiss make digit hard copi part thi work person classroom use grant without fee provid copi made distribut profit commerci advantag copi bear thi notic full citat first page copyright compon thi work acm must honor abstract credit permit copi otherwis republish post server redistribut list requir prior specif permiss fee request permiss permiss associ comput machineri http acm comput survey vol articl public date septemb pouyanfar et al introduct recent year machin learn ha becom popular research ha incorpor larg number applic includ multimedia concept retriev imag sific video recommend social network analysi text mine forth among iou algorithm deep learn also known represent learn wide use applic explos growth avail data remark advanc hardwar technolog led emerg new studi distribut deep learn deep learn ha root convent neural network significantli outperform predecessor util graph technolog transform among neuron develop learn model mani latest deep learn techniqu present demonstr promis result across differ kind applic natur languag process nlp visual data process speech audio process mani applic tradit effici algorithm highli reli good represent input data bad data represent often lead lower perform compar good data represent therefor featur engin ha import research direct machin learn long time focus build featur raw data ha led lot research studi furthermor featur engin often veri domain specif requir signific human effort exampl comput vision differ kind featur propos compar includ histogram orient gradient hog scale invari featur transform sift bag word bow onc new featur propos perform well becom trend year similar situat happen domain includ speech recognit nlp compar deep learn algorithm perform featur extract autom way allow research extract discrimin featur minim domain knowledg human effort algorithm includ layer architectur data represent featur extract last layer network featur extract lower layer kind architectur origin inspir artifici intellig ai simul process key sensori area human brain brain automat extract data represent differ scene input scene inform receiv eye output classifi object thi highlight major advantag deep mimic human brain work great success mani field deep learn one hottest research direct societi thi survey give overview deep learn differ perspect includ histori challeng opportun algorithm framework applic parallel distribut comput techniqu histori build machin simul human brain dream sage centuri veri begin deep learn trace back aristotl propos ation start histori human ambit tri understand brain sinc idea requir scientist understand mechan human recognit system modern histori deep learn start mcp model wa introduc becam known prototyp artifici neural model creat comput model base neural network function mimick neocortex human brain combin algorithm mathemat call threshold logic wa use model mimic human thought process learn sinc deep learn ha evolv steadili signific mileston develop acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic mcp model hebbian theori origin use biolog system ral environ wa implement first electron devic call perceptron within context cognit system wa introduc though differ typic perceptron nowaday perceptron highli resembl modern one power substanti association end first ai winter emerg propagandist becam anoth mileston werbo introduc backpropag use error train deep learn model open gate modern neural network neocogitron inspir convolut neural network wa introduc current neural network rnn propos next lenet made deep neural network dnn work practic howev get highli recogn due hardwar limit structur lenet quit naiv appli larg dataset around deep belief network dbn along pretrain framework develop main idea wa train simpl unsupervis model like restrict boltzmann machin rbm freez paramet stick new layer top train paramet new layer research abl train neural network much deeper previou attempt use techniqu prompt rebrand neural network deep learn origin artifici neural network ann decad develop deep learn one effici tool compar learn algorithm great perform seen deep learn method root initi ann includ dbn rbm rnn convolut neural network cnn graphic process unit gpu well known perform comput matric network architectur singl machin number distribut deep learn framework develop speed train deep learn model becaus vast amount data come without label noisi label search studi focu improv nois robust train modul use unsupervis semisupervis deep learn techniqu sinc current deep learn model onli focu singl modal thi lead limit represent data research pay attent structur may yield huge step forward deep learn one recent inspir applic deep learn googl alphago complet shock world start year pseudonym name master onlin game row human profession go player includ three victori ke jie decemb januari alphago abl defeat world champion go player becaus use modern deep learn algorithm suffici hardwar resourc research object outlin deep learn consid huge research field thi articl aim draw big pictur share research experi peer previou survey paper onli focus certain scope deep learn novelti thi articl focus differ aspect deep learn present review paper author experi breakthrough research applic deep neural network topmost challeng deep learn face today train massiv dataset abl hand dataset becom bigger divers complex deep learn ha path critic tool cater big data analysi survey challeng opportun key area deep learn rais requir attent ing parallel scalabl power optim solv aforement issu differ acm comput survey vol articl public date septemb pouyanfar et al tabl summari deep learn dl network dl network descript key point paper rvnn use structur goller et al prefer nlp socher et al rnn good sequenti inform cho et al prefer nlp speech process li et al cnn origin imag recognit extend nlp speech process comput vision lecunn et al krizhevski et al kim et al dbn unsupervis learn hinton direct connect hinton et al dbm unsupervis learn composit model rbm undirect connect salakhutdinov et al salakhutdinov et al gan unsupervis learn goodfellow et al framework radford et al vae unsupervis learn probabilist graphic model kingma et al kind deep network introduc differ domain rnn nlp cnn imag process articl also introduc compar popular deep learn tool ing caff tensorflow theano torch optim techniqu deep learn tool addit variou deep learn applic review help research expand view deep learn rest thi articl organ follow section popular deep learn network briefli present section discuss sever algorithm techniqu framework deep learn deep learn ha use nlp speech imag recognit well applic number deep learn applic provid section section point challeng potenti research direct futur final section conclud thi articl deep learn network thi section sever popular deep learn network recurs neural network rvnn rnn cnn deep gener model discuss howev sinc deep learn ha grow veri fast mani new network new architectur appear everi month scope thi articl tabl contain summari deep learn network duce thi section major key point repres paper recurs neural network rvnn rvnn make predict hierarch structur well classifi output use composit vector develop rvnn wa mainli inspir recurs soci memori raam architectur creat process object structur arbitrari shape tree graph approach wa take recurs data structur variabl size gener distribut represent backpropag acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic fig rvnn rnn cnn architectur structur bt learn scheme wa introduc train network bt follow proach similar standard backpropag algorithm also abl support structur network train autoassoci reproduc pattern input layer output layer rvnn ha especi success nlp socher et al propos rvnn architectur handl input differ modal show two exampl use rvnn classifi natur imag natur languag sentenc imag separ differ segment interest sentenc divid word rvnn calcul score possibl pair merg build syntact tree pair unit rvnn comput score plausibl merg pair highest score combin composit vector merg rvnn gener larger region multipl unit composit vector repres region class label unit two noun word class label new region would noun phrase root rvnn tree structur composit vector represent entir region figur c show exampl rvnn tree recurr neural network rnn anoth wide use popular algorithm deep learn especi nlp speech cess rnn unlik tradit neural network rnn util sequenti inform network thi properti essenti mani applic embed structur data sequenc convey use knowledg exampl understand word sentenc necessari know context therefor rnn seen memori unit includ input layer x hidden state layer output layer figur b depict typic unfold rnn diagram input sequenc three deep rnn approach includ deep introduc base three solut deep rnn propos onli take tage deeper rnn also reduc difficult learn deep network acm comput survey vol articl public date septemb pouyanfar et al one main issu rnn sensit vanish explod gradient word gradient might decay explod exponenti due multipl lot small big deriv dure train thi sensit reduc time mean network forget initi input entranc new one therefor long memori lstm util handl thi issu provid memori block recurr connect memori block includ memori cell store network tempor state moreov includ gate unit control inform flow furthermor residu tion veri deep network allevi vanish gradient issu significantli discuss section convolut neural network cnn cnn also popular wide use algorithm deep learn ha extens pli differ applic nlp speech process comput vision name similar tradit neural network structur inspir neuron anim human brain specif simul visual cortex cat brain contain complex sequenc cell describ cnn ha three main advantag name paramet share spars interact equival represent fulli util dimension structur input data imag signal local connect share weight network util instead tradit fulli connect network thi process result veri fewer paramet make network faster easier train thi oper similar one visual cortex cell cell sensit small section scene rather whole scene word cell oper local filter input extract spatial local correl exist data typic cnn number convolut layer follow pool subsampl layer final stage layer fulli connect layer ident multilay perceptron mlp usual use figur c show exampl cnn architectur imag classif layer cnn input x arrang three dimens r refer height width input r refer depth channel number r rgb imag convolut layer sever filter kernel k size n n n smaller input imag q either smaller size mention earlier filter base local connect convolv input share paramet weight w k bia bk gener k featur map hk size similar mlp convolut layer comput dot product weight input illustr equat input small region origin input volum activ function f nonlinear appli output convolut layer hk f w k bk thereaft subsampl layer featur map downsampl decreas eter network speed train process henc control overfit pool oper averag max done p p p filter size contigu region featur map final final stage layer usual fulli connect seen lar neural network layer take previou midlevel featur gener abstract data last layer softmax svm use erat classif score score probabl certain class given instanc acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic fig structur gener model deep gener network four deep gener network dbn deep boltzmann machin dbm gener adversari network gan variat autoencod vae discuss dbn brid probabilist gener model typic rbm undirect connect form top two layer lower layer use direct connect receiv input layer abov lowest layer visibl layer repres state input unit data vector dbn learn probabilist reconstruct input unsupervis approach layer act featur detector input moreov train process supervis way give dbn capac perform classif task dbn resembl composit sever rbm subnetwork hidden layer view visibl layer next subnetwork figur illustr structur dbn rbm gener stochast artifici neural network output probabl distribut learn input energi configur defin equat calcul probabl distribut base connect weight unit bias take state vector v visibl layer e v h h binari configur hidden layer unit b refer bias visibl hidden unit respect matrix w repres connect weight layer thi energi function provid probabl possibl visibl hidden vector pair use equat p v h v h partit function defin sum v h possibl configur gener normal constant guarante probabl distribut aggreg dbn includ greedi algorithm improv gener model allow network sequenti receiv differ represent data sinc rbm abl model origin data ideal onc initi weight learn data map transpos weigh matrix wt creat data next layer shown log probabl input data vector bound approxim distribut furthermor time ad new layer dbn variat bound deeper layer improv compar previou one initi new rbm block right direct like dbn dbm learn complex intern represent consid robust deep learn model speech object recognit task hand unlik dbn approxim reason procedur allow dbm handl ambigu input robustli figur b present architectur dbm composit model rbm also clearli acm comput survey vol articl public date septemb pouyanfar et al show dbm differ dbn lower layer dbn build direct belief network instead undirect rbm dbm greedi train algorithm dbm easili calcul modifi procedur dbn factori approxim posterior take either result first rbm probabl second layer take geometr averag two distribut would better idea balanc approxim posterior use second layer weight gan consist gener model g discrimin model g captur distribut pд real data local tri differenti sampl come model data rather pд repres pm everi iter backpropag gener discrimin like game cat mous compet gener tri gener realist data fool confus discrimin latter tri identifi real data fake one gener minimax game establish valu function v g min g max v g loдd loд g repres probabl came data rather pд pdata distribut data model consid stabl reach point none improv pд pdata discrimin longer identifi two distribut figur c show gan architectur anoth famou gener model vae exampl vae architectur given figur util data leverag strategi deriv lower bound estim direct graphic model continu latent variabl tive paramet θ gener model assist learn process variat paramet ϕ variat approxim model variat bay aevb algorithm optim paramet ϕ θ probabl encod qϕ neural network approxim gener model pθ x z z latent variabl simpl distribut n ident matrix aim maxim probabl x train set entir gener process pθ x pθ z pθ dz deep learn techniqu framework differ deep learn algorithm help improv learn perform broaden scope applic simplifi calcul process howev extrem long train time deep learn model remain major problem research furthermor classif accuraci drastic enhanc increas size train data model paramet order acceler deep learn process sever advanc techniqu propos literatur deep learn framework combin implement modular deep ing algorithm optim techniqu distribut techniqu support infrastructur develop simplifi implement process boost ment research thi section repres techniqu framework introduc unsupervis transfer learn contrari vast amount work done supervis deep learn veri studi address unsupervis learn problem deep learn howev recent year acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic benefit learn reusabl featur use unsupervis techniqu ha shown promis result differ applic last decad idea learn framework ha wide discuss literatur recent year gener model gan vae becom domin niqu unsupervis deep learn instanc gan train reus fix ture extractor supervis task thi network base cnn ha shown supremaci unsupervis learn visual data analysi anoth work deep spars toencod train veri imag dataset learn featur thi network ate featur extractor unlabel data use face detect unsupervis manner gener featur also discrimin enough detect level object like anim face human bodi bengio et al propos gener stochast network unsupervis learn altern maximum likelihood base transit oper markov chain mont carlo practic veri peopl luxuri access veri gpu power hardwar train veri deep network scratch reason time therefor pretrain deep network cnn dataset imagenet veri common thi techniqu also known transfer learn done use pretrain network fix featur extractor especi small new dataset weight pretrain model especi larg new dataset similar origin one latter model continu learn weight level part deep network thi approach consid semisupervis learn label data insuffici train whole deep network onlin learn usual network topolog architectur deep learn time static predefin befor learn start also time invari thi restrict time complex pose seriou challeng data stream onlin onlin learn ousli came mainstream research onli modest advanc ha observ onlin deep learn convent dnn built upon stochast gradient descent sgd approach train sampl use individu updat model paramet known label need rather sequenti process sampl updat appli batch process one approach wa present sampl batch treat independ ident distribut iid batch process approach proport balanc comput resourc execut time anoth challeng stack issu onlin learn data vari distribut thi challeng repres retail bank data pipelin hold tremend busi valu current premis data larg close time safe assum piecewis stationar thu similar distribut thi assumpt character data certain degre correl develop model accordingli discuss unfortun nonstationari data stream iid often longitudin data stream moreov onlin learn often memori delimit harder parallel requir linear learn rate input sampl develop method capabl onlin learn data would big leap forward big data deep learn optim techniqu deep learn train dnn optim process find paramet network imiz loss function practic sgd method fundament algorithm appli deep learn iter adjust paramet base gradient train acm comput survey vol articl public date septemb pouyanfar et al sampl comput complex sgd lower origin gradient descent method whole dataset consid everi time paramet updat learn process updat speed control hyperparamet learn rate lower learn rate eventu lead optim state long time higher learn rate decay loss faster may caus fluctuat dure train order control oscil sgd idea use momentum introduc inspir newton first law motion thi techniqu get faster converg proper momentum improv optim result sgd hand sever techniqu propos determin proper learn rate itiv weight decay learn rate decay introduc adjust learn rate erat converg weight decay work penalti coeffici cost function avoid overfit learn rate decay reduc learn rate dynam improv perform moreov adapt learn rate respect gradient previou stage found help avoid fluctuat adagrad first adapt algorithm cess use deep learn amplifi learn rate infrequ updat paramet suppress learn rate frequent updat paramet record mulat squar gradient sinc squar gradient alway posit learn rate adagrad becom extrem small doe optim model anymor solv thi sue adadelta propos decay fraction introduc limit accumul squar gradient follow e e accumul squar gradient stage squar gradient stage later adadelta improv introduc anoth decay fraction record accumul gradient shown adam perform better practic algorithm adapt learn rate adamax also propos paper extens adam l norm use adam replac l norm achiev stabl algorithm adam also incorpor nesterov acceler gradient nag call nadam show better converg speed case deep learn distribut system effici model train limit system distribut deep learn techniqu develop acceler train process two main approach train model distribut system name data parallel model parallel data parallel model replic comput node model train assign subset data certain period time weight updat need synchron among node compar model parallel data process one model node respons partial estim paramet model among approach straightforward algorithm combin result slave node paramet averag let wt paramet neural network node time n slave node use train time weight master node wt copi current paramet distribut slave node updat paramet sent back master node weight time master node n n acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic paramet averag would ident train paramet averag ter minibatch worker process number data copi howev network commun synchron cost nullifi benefit extra machin therefor averag process usual appli certain number minibatch fed slave node frequenc train model perform need balanc requir popular approach data parallel use sgd known data parallel updat learn rate decay momentum fer howev synchron weight updat scalabl larger cluster overhead commun increas exponenti respect number node therefor ramet server framework propos googl process train asynchron instead wait paramet updat master node asynchron updat allow node spend time comput meanwhil network commun cost significantli reduc decentr transmit updat mode instead mode hand model parallel approach split train step across multipl gpu straightforward strategi gpu comput onli subset model exampl model two lstm layer system two gpu use calcul one lstm layer advantag strategi make train predict massiv deep neural network possibl instanc cot hpc system train neural network billion paramet requir memori imposs fit larg model one machin therefor need partit use strategi howev sinc model partit across node one drawback model parallel node onli comput subset result synchron thu need get full result synchron loss commun overhead strategi strategi sinc node former must synchron gradient paramet valu everi updat step word scalabl model parallel inferior handl thi issu googl ha propos autom devic placement framework base deep reinforc learn find best scheme model partit placement framework take embed represent oper place group oper differ devic show perform improv compar human expert strategi limit one hand data parallel ha mani train modul ha decreas learn rate make ing procedur smooth hand model parallel ha mani segment output node increas sharpli reduc effici accordingli gener speak larger dataset benefici data parallel larger deep learn model suitabl model parallel besid compar data parallel hard hide commun need synchron model parallel becaus onli partial inform includ node whole batch though vanc framework like tensorflow support asynchron kernel save commun cost thu necessari wait till synchron step finish befor move forward next layer sinc activ unabl process onli partial inform two kind strategi also fuse hybrid model discuss deep learn framework tabl list smatter popular deep learn framework architectur design caff torch neon theano mxnet tensorflow microsoft cognit toolkit cntk tabl licens core acm comput survey vol articl public date septemb pouyanfar et al tabl comparison differ deep learn framework framework licens core languag interfac support cnn rnn support dbn support caff bsd python matlab ye apach java java scala python ye ye torch bsd c lua lua python ye ye neon apach python python ye ye theano bsd python python ye ye mxnet apach python r scala perl julia etc ye ye tensorflow apach python python java go ye ye cntk mit python brainscript ye languag support interfac languag framework support cnn rnn dbn also list observ tabl usual use implement deep learn framework becaus acceler speed train sinc gpu significantli help speed matrix comput aforement framework also support gpu via terfac provid cudnn meanwhil python ha becom common languag deep learn architectur design sinc make program effici easier simplifi program process also distribut calcul becom common centli releas framework tensorflow mxnet cntk goal improv calcul effici deep learn moreov tensorflow also includ support custom deep learn integr circuit asic call tensor process unit tpu help increas effici decreas power consumpt caff implement berkeley vision learn center one wide use framework support commonli use layer cnn rnn doe directli enabl use dbn user caff design architectur declar structur comput graph convolut layer pretrain model avail wide rang neural network alexnet googlenet resnet thermor caff framework word doe support multinod execut calcul support extern offer like caffeonspark yahoo integr caff big data engin like spark popular framework implement java develop maintain mind sinc cooper hadoop spark capabl distribut putat well howev thi framework report longer train time similar architectur benchmark framework torch wa first releas extend deep learn featur combin facebook deep learn cuda librari fbcunn torch oper acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic fig popular deep learn applic parallel comput unlik framework torch built base dynam graph represent instead static graph dynam graph allow user updat comput graph chang model structur dure runtim static graph use certain function defin graph advanc recent torch releas python interfac pytorch usag thi framework ha greatli increas due flexibl neon theano two framework develop python intel siti montreal respect perform code optim system kernel level therefor train speed usual outperform framework howev although onli parallel support multinod calcul design framework mxnet support sever interfac includ python r scala perl matlab javascript go julia support comput graph declar imper comput declar architectur design mxnet onli support data model parallel also follow paramet server scheme support distribut calcul well mxnet ha comprehens function perform optim much art framework tensorflow implement googl provid seri intern function help ment ani deep neural network base static comput graph recent kera start support tensorflow via interfac allow user design architectur worri intern design framework provid differ level parallel distribut oper fatal toler robust design attract lot user ha becom one popular deep learn framework sinc releas cntk design microsoft ha specif script languag brainscript neural network implement cntk model neural network direct graph node graph repres oper filter edg refer data flow instead paramet server model messag pass interfac appli distribut calcul support variou applic deep learn nowaday applic deep learn includ limit nlp sentenc ficat translat etc visual data process comput vision multimedia data analysi etc speech audio process enhanc recognit etc social network analysi healthcar thi section provid detail differ techniqu use applic main deep learn applic also visual figur acm comput survey vol articl public date septemb pouyanfar et al tabl popular deep learn method nlp paper nlp task architectur dataset socher et al sentiment analysi rntn sst kim sentiment analysi gener classif cnn sst wehrmann et al sentiment analysi mtd bahdanau et al translat bidir rnn decod cho et al translat rnn decod wu et al translat gnmt socher et al paraphras identif unfold rae msrp yin et al paraphras identif question answer abcnn wikiqa msrp kågebäck et al summar unfold rae od dong et al question answer mccnn wq feng et al question answer cnn iqa natur languag process nlp seri algorithm techniqu mainli focu teach comput stand human languag nlp task includ document classif translat phrase identif text similar summar question answer nlp develop challeng due complex ambigu structur human languag moreov natur languag highli context specif liter mean chang base form word sarcasm domain specif deep learn method recent abl strate sever success attempt achiev high accuraci nlp task tabl contain mari lead deep learn nlp solut architectur dataset nlp model follow similar preprocess step input text broken word token word reproduc form vector repres word low dimens import creat accur percept tie differ variou word challeng arriv need decid length word contain thi procedur context specif requir prior domain knowledg highli impact approach solv nlp task present sentiment analysi thi branch nlp deal examin text classifi feel opinion writer dataset sentiment analysi label either posit neg neutral phrase remov subject classif method one popular exampl standford sentiment treebank sst dataset movi review label five categori rang veri neg veri posit along introduct sst socher et al propos recurs neural tensor network rntn util word vector pars tree repres phrase captur interact element composit function thi recurs approach advantag come classif sinc grammar often display structur acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic kim improv accuraci sst follow differ approach even though cnn model first creat imag recognit classif mind implement nlp ha proven success achiev excel result kim present simpl cnn model use one convolut layer top train vector bow architectur model kept rel simpl small number hyperparamet tune combin low tune pretrain paramet manag achiev high accuraci sever benchmark social media popular sourc data studi sentiment multilingu twitter dataset mtd one largest public dataset contain million manual annot tweet languag appli sentiment analysi tweet challeng due short natur text address issu multilingu dataset small amount text propos architectur exempt depend languag although approach wa capabl outperform embed architectur author argu simplic predict power consumpt good tradeoff machin translat deep learn ha play import role improv tradit automat translat method cho et al introduc novel code decod architectur train word neural machin translat nmt rnn framework use two rnn one map input sequenc length vector rnn decod vector target symbol downsid rnn perform deterior input sequenc symbol becom larger bahdanau et al address thi issu introduc vector jointli learn align translat procedur approach perform binari search look part speech predict translat nonetheless recent propos translat system known comput expens ineffici handl sentenc contain rare word thu googl neural machin translat gnmt system propos introduc balanc flexibl provid level model effici model gnmt deep lstm network make use eight encod eight decod layer connect use mechan method wa first introduc improv nmt gener model achiev score wmt benchmark paraphras identif paraphras identif process analyz two tenc project similar base underli hidden semant key featur benefici sever nlp job plagiar detect answer question context detect summar domain identif socher et al propos use unfold recurs autoencod rae measur similar two sentenc use syntact tree develop featur space measur tie even though veri similar rvnn rae use unsupervis classif unlik rvnn rae comput reconstruct error instead supervis score dure merg two vector composit vector paper also introduc dynam pool layer compar classifi two sentenc differ size either paraphras sever notabl method investig monolingu semant similar detect also enlist notabl dataset paraphras identif microsoft research paraphras corpu msrp topic cluster news articl dataset cnn abcnn recent propos deep learn architectur goal determin interdepend two sentenc paraphras detect ha also appli answer select textual entail acm comput survey vol articl public date septemb pouyanfar et al summar automat summar extract signific relev inform larg text document summari effect reduc size text without lose import inform thi consider decreas time comput requir analyz larg dataset kågebäck et al propos continu vector model sentenc model evalu multipl combin composit meaning represent new vector represent test use rae compar simpl vector addit paper make use roug benchmark metric evalu effect summar framework ganesan et al util model produc brief summari opinion dataset known opinosi dataset od model target user opinion term feedback product review custom satisfact report without lose ani educ materi question answer automat system abl terpret natur languag question use reason return appropri repli modern knowledg base famou freebas dataset allow thi field flourish leap time featur rule set specif domain dong et al came multicolumn cnn approach analyz question sever aspect context choos underli semant mean answer form answer use multitask approach rank pair also neousli learn correl affili word semant gener deep learn architectur limit ani one languag propos tion answer qa framework propos paper base cnn use approach answer question insur domain test mani differ setup base architectur compar result berant et al propos highli scalabl version model solut problem larg dataset avoid logic form text learn model sole tupl abcnn prove biliti nlp task rank candid answer base close interdepend question dataset use paper mention thi section web question wq insur question answer iqa wikiqa visual data process deep learn techniqu becom main part variou multimedia tem comput vision specif cnn shown signific result differ task includ imag process object detect video process thi section discuss detail recent deep learn framework algorithm propos past year visual data process imag classif lecun et al present first version convent cnn includ two convolut layer along subsampl layer final end full connect last layer although sinc earli cnn techniqu greatli leverag differ problem includ segment detect classif imag almost forsaken data mine research group one decad later cnn algorithm ha start prosper comput vision commun specif alexnet consid first cnn model substanti improv imag classif result veri larg dataset imagenet wa winner ilsvrc improv best result previou year almost regard top five test error improv effici speed train gpu implement cnn util thi network data augment dropout techniqu also use substanti reduc overfit problem acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic fig network top five error layer imagenet classif time sinc varieti cnn method develop submit ilsvrc tition two influenti differ model present mostli focus depth neural network first one known vggnet includ veri simpl cnn thi network layer spatial size input reduc depth network increas achiev effect effici model although vggnet wa winner ilsvrc still show signific improv top five error previou top model came two major specif simplic depth contrast vggnet googlenet winner thi competit error propos new complex modul name incept allow sever oper pool convolut etc work parallel microsoft deep residu network known resnet took lead tition includ ilsvrc coco detect segment task introduc residu connect cnn design learn model layer thi model achiev incred perform top five error mean first time comput model could beat human brain error imag classif contrari extrem deep represent resnet handl vanish gradient well degrad problem satur accuraci deep network util residu block last year sever variat resnet propos first group method ha tri increas number layer current cnn model may includ layer final resnext wa propos extens resnet vggnet thi simpl model includ sever branch residu block perform transform final aggreg summat oper thi gener model reshap techniqu alexnet resnext outperform origin version resnet use half layer also improv well network imagenet dataset figur demonstr revolut depth perform imag classif imagenet time problem supervis imag classif regard solv imagenet classif challeng conclud object detect semant segment deep learn techniqu play major role advanc object detect recent year befor best object detect perform came complex system sever featur sift hog etc context howev advent new deep learn techniqu object detect ha also reach new stage advanc advanc driven success method region propos cnn bridg acm comput survey vol articl public date septemb pouyanfar et al gap object detect imag classif introduc object local method use deep network addit transfer learn pertain larg dataset imagenet util sinc small object detect dataset pascal includ insuffici label data train larg cnn network howev train comput time memori veri expens especi new network vggnet moreov object detect step veri slow later thi techniqu extend overcom aforement issu introduc two success techniqu fast faster former leverag share comput speed origin train veri deep vggnet latter propos region propos network rpn enabl almost object detect object detect call yolo onli look onc contain singl cnn convolut network perform detect class probabl tion box simultan benefit yolo includ fast train test frame per second reason perform compar previou system unlik recent method call fulli convolut network util fulli convolut network share almost comput imag thi method use resnet classifi object detector achiev speed faster faster method final multibox detector ssd propos faster yolo perform accur techniqu faster model base singl cnn gener set bound box fix size well correspond object score box semant segment process understand imag pixel level necessari applic autonom drive robot vision medic system question convert imag classif semant segment recent year mani research studi appli deep learn techniqu classifi imag lution network instanc includ deconvolut unpool modul detect classifi segment region anoth work fulli convolut network fcn propos util network alexnet vggnet googlenet recent mask wa propos facebook ai research fair object instanc segment extend faster ad new branch gener segment mask predict region interest time bound box class label gener thi simpl flexibl model ha shown great perform result coco instanc segment object detect video process video analyt ha attract consider attent comput vision commun consid challeng task sinc includ spatial poral inform earli work youtub video contain sport class use train cnn model model includ multiresolut architectur util local motion inform video includ context stream imag ing fovea stream imag process modul classifi video event detect sport video use deep learn present work spatial tempor inform encod use cnn featur fusion via regular coder recent year new techniqu call recurr convolut network rcn wa introduc video process appli cnn video frame visual understand feed frame rnn analyz tempor inform video new rcn model propos use rnn intermedi layer cnn addit gate recurr unit use leverag sparsiti local rnn modul thi model valid dataset acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic tabl popular visual dataset deep learn dataset data type num instanc num class ground truth applic imagenet imag ye imag classif object local object detect etc imag ye imag classif pascal voc imag ye imag classif object detect semant segment microsoft coco imag ye object detect semant segment mnist imag ye handwritten digit classif imag video partial video imag understand video automat video classif trecvid video vari vari partial video search event detect local etc video ye human action detect kinet video ye human action detect cnn ha demonstr better perform video analysi task tradit cnn automat learn spatiotempor featur video input model appear motion time network anoth set video analysi techniqu model spatial rgb frame tempor inform optic flow separ averag predict last layer network thi network extend recent work call inflat convnet util idea also pretrain kinet dataset propos approach could significantli enhanc perform action recognit dataset visual dataset signific advanc imag video process onli reli develop new learn algorithm util power hardwar also crucial depend veri public dataset sever visual dataset use train deep learn algorithm list tabl imagenet consid import influenti dataset deep learn use train popular network alexnet googlenet vggnet resnet due label imag collect imag dataset util mani research studi thi dataset also use evalu mani dnn imag classif task mention earlier pascal voc microsoft coco use variou object detect semant segment task final rel new dataset gener googl play role imagenet video process util benchmark dataset variou video analys includ event detect understand classif speech audio process audio process process oper directli electr analog audio signal essari speech recognit speech transcript speech enhanc phone classif acm comput survey vol articl public date septemb pouyanfar et al music classif speech process activ research area becaus import perfect interact till centuri automat speech recognit asr technolog ha risen unpreced level howev still far mimick behavior commun human asr system made mani compon includ speech signal preprocess featur extract acoust ing phonet unit recognit languag model tradit asr system integr hidden markov model hmm gaussian mixtur model gmm hmm use deal variat speech relat time space gmm repres acoust characterist sound unit model process requir veri larg train dataset order reach high accuraci ann introduc dure compos mani nonlinear comput element oper parallel howev deeper architectur multipl layer need settl limit gmm suffici repres hmm dbn one commonli use deep learn model thi area significantli improv perform acoust model model spectral variat speech rbm build block seid et al use pretrain dbn demonstr strength model publicli avail benchmark switchboard transcript task introduc weight spars relat learn strategi reduc recognit error model size follow wide studi dbn pretrain method dahl et al propos novel acoust model lvsr model integr pretrain dnn use dbn pretrain algorithm depend cd hidden markov model name use unsupervis dbn pretrain algorithm activ train process instead phonem benchmark evalu wa perform lvsr wa first applic appli larg vocabulari dataset pretrain dnn model mani research studi follow thi direct investig improv evalu effici differ investig strength dbn grave et al focu explor deep rnn achiev test error timit phonem dataset deep lstm perform better recogn context use memori cell store inform recent year interest speech recognit restrict improv acoust model within asr system larg rnn includ bidirect layer multipl convolut layer wa train end end use connectionist tempor classif ctc loss function propos deep rnn architectur call deep speech take advantag capac provid deep learn system keep robust overal network noisi environ besid approach ha shown capabl quickli appli new languag recogn scalabl model deploy gpu server also evalu model achiev higher effici transcript hybrid model rcnn introduc work lvsr origin cnn introduc asr allevi comput problem howev tend veri challeng train slow converg core modul insid rcnn recurr convolut layer rcl whose state evolv discret time step comparison made lstm timit phonem dataset besid speech recognit task mani research studi focu speech emot tion ser speech enhanc se seaker separ ss current proach summar tabl speech emot recognit ser emot influenc voic characterist linguist content speech ser reli heavili effect speech featur use acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic tabl popular deep learn approach audio process paper task architectur dataset abel tim se dnn timit sdc ntt han et al ser elm iemocap huang et al se dnn rnn timit kolbæk et al se dnn neumann vu ser attent cnn iemocap pascual et al se gan voic bank corpu weng et al ss dnn brir yu et al ss dnn cnn zhang wang ss dnn speech separ challeng data classif classifi two type global model statist function hsf mean varianc median linear regress coeffici etc dynam model approach dynam descriptor lld like mel frequenc cepstral coeffici mfcc voic probabl ratio forth newli develop ann one hidden layer call extrem learn machin elm propos classif use dnn method evalu use audio track interact emot dyadic motion captur iemocap databas contain audiovisu data actor experiment result demonstr elm perform enhanc compar approach besid show strength attent cnn model featur learn cnn also util speech emot recognit work achiev perform result improvis iemocap data speech enhanc se recent speech enhanc ha aim improv speech qualiti use deep learn algorithm dnn artifici speech bandwidth extens abe framework propos deal speech enhanc task narrowband speech signal input timit databas us sdc base use train dnn model pretrain sigmoid unit improv achiev ntt databas upper band cepstral distanc mo point improv compar baselin huang et al studi monaur sourc separ deep learn studi joint optim dnn rnn extra mask layer propos manc evalu compar nonneg matrix factor nmf model use timit speech corpu pascual et al also propos segan leverag gan speech enhanc speech separ ss view subtask speech enhanc aim rate reverber target speech spatial diffus background interfer differ environ speaker separ focus reconstruct speech speaker mix speech one speaker talk simultan earli stage sever method includ soft mask modul frequenc analysi spars sition propos address issu input approach propos attack multitalk speech recognit problem pose approach show remark nois robust outperform ibm superhuman tem author implement speech separ model train techniqu model includ dnn three hidden layer one cnn acm comput survey vol articl public date septemb pouyanfar et al model gener separ view instead multiclass regress segment wa previous popular applic aforement applic deep learn algorithm also appli format retriev robot transport predict autonom drive biomedicin disast manag forth pleas note deep learn ha shown capabl leverag variou applic onli select applic introduc thi section social network analysi popular mani social network like facebook ter ha enabl user share larg amount inform includ pictur thought opinion due fact deep learn ha shown promis perform visual data nlp differ deep learn approach adopt social network analysi includ semant evalu link predict crisi respons semant evalu import field social network analysi aim help chine understand semant mean post social network although varieti niqu propos analyz text nlp approach may fail address sever main challeng social network analysi spell error abbrevi special ter inform languag twitter consid commonli use sourc sentiment classif cial network analysi gener sentiment analysi aim determin attitud review thi purpos semev ha provid benchmark dataset base twitter run timent classif task sinc anoth similar exampl amazon start onlin bookstor world largest onlin retail abund purchas transact vast amount review creat custom make amazon dataset great sourc sentiment classif field social network link predict also commonli use mani scenario recommend network complet social tie predict approach appli improv perform predict tackl problem iti nonlinear sinc data social network highli dynam convent deep learn algorithm ha modifi adapt thi characterist deep learn approach use rbm perform link predict sinc unknown link user directli model hidden layer rbm thu predict liu et al propos pervis dbn approach base pretrain rbm link predict approach process separ three step pretrain dbn construct part two layer rmb contain dbn first step unsupervis link predict encod link use input featur gener predict link unsupervis manner next represent origin link gener base output unsupervis link predict featur represent step final step link predict step perform link represent gener predict link supervis way differ task semant classif link predict crisi respons social network requir immedi detect natur disast main goal crisi respons identifi inform piec post classifi ing topic class like flood earthquak wildfir forth address thi topic nguyen et al propos deep learn framework combin onlin learn featur automat detect possibl disast tweet sentenc level identifi type detect disast first goal perform binari classif network use acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic inform noninform piec post label inform post classifi specif type inform retriev deep learn ha great impact inform retriev structur semant model dssm propos document retriev web search latent semant analysi conduct dnn queri along data use determin result retriev encod queri data map word hash featur space gener multilay nonlinear project propos dnn train bridg given queri semant mean help data howev thi pose model treat word separ ignor connect word resent improv version thi method convolut dssm word sequenc map featur space convolut structur grate gener sever featur space subset word sequenc end layer addit project layer use gener final output gener inform retriev task deep stack network dsn propos atom modul dsn compos simpl classifi nonlinear function step previou output modul stack origin input gener new result use thi method origin input featur repres abstract featur thu retriev result improv transport predict transport predict anoth applic deep ing et al propos deep learn framework base architectur dict transport network congest evolut due congest one locat congest statu encod binari represent histor data transport congest use visibl unit input sequenc model propos method show least improv accuraci approach take around runtim howev reason accuraci effici reach cost lose sensit specif model instead traffic internet traffic complex due properti analyz deep learn approach traffic matrix predict estim method data center network propos base dbn predict modul logist regress model contain output layer gener predict traffic matrix valu base model train histor data estim modul dbm model train prior link count input traffic matrix time output therefor current traffic matrix estim use propos model link count cost less comput time resourc deep learn approach show least improv predict timat comparison approach autonom drive larg number big compani unicorn startup includ googl tesla aurora uber work automot technolog back hadsel et al use rel simpl dbn two convolut layer one max sampl layer extract deep featur use learn techniqu vision terrain train classifi discrimin featur vector recent autonom drive system categor robot approach recogn object behavior clone approach learn direct map sensori input drive action tradit robot approach involv recognit object combin sensor fusion object detect imag classif path plan control theori geiger et al built rectifi autonom drive dataset acm comput survey vol articl public date septemb pouyanfar et al captur wide rang interest scenario includ car pedestrian traffic lane road sign traffic light behavior clone approach often base deep learn involv train dnn take sensor input produc steer throttl brake output koutník et al train fulli connect rnn use reinforc learn proach also use compress network encod reduc dimension search space util inher regular environ keep car track network map imag directli steer angl recent paper take advantag approach construct map imag sever possibl afford dicat road situat distanc lane mark angl car rel road distanc car current adjac lane compact afford represent percept output build autom work learn deep learn featur imag afford estim make drive decis autonom drive technolog matur still ha long way go handl unpredict complex situat biomedicin deep learn highli progress research field reach domain histopatholog open opportun one earli attempt includ sens mitot figur cell propos anoth method employ autoencod section basal cell carcinoma breast cancer framework appli cnn sentinel lymph node tri accur detect clump isol tumor cell howev method lag gener larg dataset make harder evalu relev moreov sever studi method use cnn train model singl patient care center lab treatment stroke prostat cancer breast cancer surviv risk predict dure highli relev huge gap deep learn method thi domain onli notabl paper concentr deep surviv analysi surviv analysi found structur attribut like patient age marit statu bmi recent advanc medic imag provid unstructur imag also predict surviv probabl tional featur obtain human design howev research challeng featur provid limit insight depict highli conceptu data deep learn el cnn perfectli suit repres conceptu attribut surviv analysi success outperform exist cox framework nonetheless still limit challeng requir attent research commun newest research progress machin learn complic biomedicin task accomplish deep learn techniqu even fascin news machin learn reveal thing undetect human recent research team googl stanford use deep learn discov new knowledg retin fundu imag predict cardiovascular risk factor previous thought quantifi present retin imag beyond current human knowledg disast manag system anoth applic disast manag system attract great attent commun disast affect muniti human live economi structur disast inform system help gener public personnel emerg oper center eoc awar rent hazard situat assist disast relief process current major challeng appli deep learn method disast inform system system need deal data provid accur assist acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic nearli manner accid natur catastroph suddenli happen great amount data need collect analyz tian et al appli tradit neural work build prototyp disast inform system mlp integr featur translat algorithm perform multilay learn structur though research studi util deep learn disast inform manag still earli stage ha great potenti deep learn deep learn challeng futur direct acut develop deep learn research venu limelight deep learn ha gain extraordinari momentum speech languag visual detect system howev sever domain practic still untouch dnn due either challeng natur lack data avail gener public thi creat signific tie fertil ground reward futur research avenu thi section domain key insight challeng like futur direct major deep learn method discuss linger percept dnn mean deep learn model assess base final output without understand get decis thi weak statist interpret ha also identifi especi applic data produc ani type physic manifest et al explain neural network use cell biolog molecular scale map layer neural network compon yeast cell start microscop nucleotid make dna move upward larger structur ribosom take instruct dna make protein final move organel like mitochondrion nucleu run cell oper sinc visibl neural network could easili observ chang cellular mechan dna wa alter one uniqu techniqu googl brain peer synthet brain dnn method call inception isol specif part data neuron estim see certainti neuron thi process coupl deep dream techniqu map network respons ani given imag instanc imag cat dog relev neuron almost alway pretti sure dog floppi ear cat pointi ear help dissect dataset interpret part network man et al also talk similar method understand semant behind given dataset peek variou network path activ part data howev lack attent thi problem larg attribut differ way statistician profession use deep learn plausibl way forward relat neural network exist physic biolog phenomenon thi aid develop metaphys relationship help demystifi dnn brain moreov consensu literatur deep learn research need simplifi interfac low process overhead model analyz better understand thi lead us next challeng relev futur problem suffici train sampl label apart zettabyt current avail data petabyt data ad everi day thi exponenti growth pile data never label human assist current sentiment favor supervis learn mostli becaus readili avail label small size current dataset howev rapid increas size complex data unsupervis learn domin solut futur current deep learn model also need adapt rise issu data sparsiti miss data messi data order captur approxim inform observ rather train furthermor incomplet acm comput survey vol articl public date septemb pouyanfar et al heterogen unlabel dataset open venu deep learn method thi veri excit becaus inher agnost natur dnn give uniqu abil work unsupervis data advanc deep learn model built handl noisi messi data author attempt tackl challeng databas million tini imag contain rgb photo queri use novel robust cost function reduc noisi label data moreov increas number applic involv huge amount data stream live format includ time seri sequenc xml file social network data store suffer incomplet heterogen unlabel data deep learn model learn domain ha discuss relev problem thi time anoth landmark challeng face deep learn method reduct dimension without lose critic inform need classif medic applic like cancer rna sequenc analysi common number sampl label far less number featur current deep learn model thi caus sever overfit problem inhibit proper classif untrain case method tri empir deduc variabl predict reduc featur set supervis manner thi often result loss resolut detail similar challeng face analyz medic imag becaus train data tremend costli obtain foundat paper attempt build model requir minim number sampl dure learn stand pioneer public appli cnn breast prostat cancer detect strong way forward known deep reinforc learn idea infer behavior psycholog agent take action minim aggreg cost method use game theori control theori multiag system forth learn perform action given data limit multimedia data start feed imag network say give thi gener feedback loop cloud look similar rabbit neural network reinforc look like rabbit sever iter process consequ make network predict rabbit distinctli till elabor bunni appear result fascin even rel small network train tumor cell use overinterpret imag detect minut detail current undetect deep learn one grow pain deep learn relat issu comput effici achiev maximum throughput consum least amount resourc rent deep learn framework requir consider amount comput resourc proach perform one method attempt overcom thi challeng use reservoir comput anoth altern use increment approach exploit medium larg dataset offlin train current year mani research shift focu build parallel scalabl deep learn framework late focu ha shift migrat learn process gpu howev gpu notori leakag current thi abstract ani plausibl realiz deep learn el portabl devic one solut use gate array fpga deep learn acceler order optim data access pipelin achiev significantli better result wang et al use deep learn acceler unit dlau abl architectur use three pipelin process unit use tile method local techniqu attain time increas speed compar cpu power consumpt anoth approach target architectur base fpga arc loss leakag forth still manag achiev detect rate abl acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic achiev time faster process speed softwar implement although gpu provid peak perform fpga requir less power similar perform throughput mount motherboard uniqu approach propos ment cnn use rooflin model sinc memori bandwidth fpga design critic evalu requir memori bandwidth use loop tile implement achiev gigaflop significantli reduc power consumpt gigaflop unit measur perform unit processor unfortun deep learn fpga test bed avail thi time limit explor thi area onli well vers fpga design summari deep learn new hot topic machin learn defin cascad layer perform nonlinear process learn multipl level data represent decad research tri discov pattern data represent raw data thi method call represent learn unlik convent data mine techniqu deep learn abl gener veri data represent massiv volum raw data therefor ha provid solut mani plicat thi articl survey algorithm techniqu deep learn start histori artifici neural network sinc move recent deep learn algorithm major breakthrough differ applic key algorithm framework thi area well popular techniqu deep learn present first briefli introduc tradit neural network sever supervis deep learn algorithm includ recurr recurs convolut neural network well deep belief network boltzmann machin thereaft advanc deep learn approach unsupervis onlin learn discuss moreov sever optim techniqu also provid ular framework thi area includ tensorflow caff theano addit handl big data challeng distribut techniqu deep learn briefli discuss thereaft thi articl review success deep learn method variou applic includ nlp visual data process speech audio process social network analysi thi articl discuss challeng provid sever exist solut challeng howev still sever issu need address futur deep learn sever find thi articl possibl futur work summar although deep learn memor massiv amount data inform weak reason understand data make solut mani tion interpret deep learn investig futur deep learn still ha difficulti model multipl complex data modal time multimod deep learn anoth popular direct recent deep learn search unlik human brain deep learn need extens dataset prefer label data train machin predict unseen data thi problem becom daunt avail dataset small healthcar data data need process real time learn learn studi recent year allevi thi problem major exist deep learn implement supervis algorithm machin learn gradual shift unsupervis semisupervis learn handl data without manual human label acm comput survey vol articl public date septemb pouyanfar et al spite deep learn advanc recent year mani applic still untouch deep learn earli stage leverag deep learn niqu disast inform manag financ medic data analyt deep learn new method provid numer challeng well opportun solut varieti applic importantli transfer machin learn new stage name smarter refer martín abadi ashish agarw paul barham eugen brevdo zhifeng chen craig citro greg corrado andi davi jeffrey dean matthieu devin sanjay ghemawat ian goodfellow andrew harp geoffrey irv michael isard yangq jia rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg dan mané rajat monga sherri moor derek murray chri olah mike schuster jonathon shlen benoit steiner ilya sutskev kunal talwar paul tucker vincent vanhouck vijay vasudevan fernanda viéga oriol vinyal pete warden martin wattenberg martin wick yuan yu xiaoqiang zheng tensorflow machin learn heterogen distribut system corr retriev http ossama moham hui jiang li deng gerald penn dong yu convolut neural network speech recognit transact audio speech languag process johann abel tim fingscheidt dnn regress approach speech enhanc artifici width extens ieee workshop applic signal process audio acoust ieee sami nisarg kothari joonseok lee paul natsev georg toderici balakrishnan varadarajan heendra vijayanarasimhan video classif benchmark corr retriev http rami guillaum alain amjad almahairi christof angermuel dzmitri bahdanau nicola balla frédéric bastien justin bayer anatoli belikov alexand belopolski yoshua bengio arnaud bergeron jame bergstra valentin bisson josh bleecher snyder nicola bouchard nicola xavier bouthilli alexandr de brébisson olivi breuleux carrier kyunghyun cho jan chorowski paul christiano tim cooijman côté myriam côté aaron courvil yann dauphin olivi leau julien demouth guillaum desjardin sander dieleman laurent dinh mélani ducoff vincent dumoulin samira ebrahimi kahou dumitru erhan ziy fan orhan firat mathieu germain xavier glorot ian goodfellow matt graham caglar gulcehr philipp hamel iban harlouchet heng baláz hidasi sina honari arjun jain sébastien jean kai jia mikhail korobov vivek kulkarni alex lamb pascal lamblin eric larsen césar laurent sean lee simon lefrancoi simon lemieux nichola léonard zhouhan lin jess livezey cori lorenz jeremiah lowin qianli manzagol olivi mastropietro robert mcgibbon roland vic bart van merriënbo vincent michalski mehdi mirza alberto orlandi christoph pal razvan pascanu mohammad pezeshki colin raffel daniel renshaw matthew rocklin adriana romero marku roth peter owski john salvati françoi savard jan schlüter john schulman gabriel schwartz iulian vlad serban dmitriy serdyuk samira shabanian étienn simon sigurd spieckermann ramana subramanyam jakub sygnowski jérémie tanguay gij van tulder joseph turian sebastian urban pascal vincent francesco visin harm de vri david dustin webb matthew willson kelvin xu lijun xue li yao saizheng zhang ying zhang theano python framework fast comput mathemat express corr retriev http dario amodei sundaram ananthanarayanan rishita anubhai jingliang bai eric battenberg carl case jare casper bryan catanzaro qiang cheng guoliang chen jie chen jingdong chen zhiji chen mike chrzanowski adam coat greg diamo ke ding niandong du erich elsen jess engel weiwei fang linxi fan christoph fougner liang gao caixia gong awni hannun toni han lappi vaino johann bing jiang cai ju billi jun patrick legresley libbi lin junji liu yang liu weigao li xiangang li dongpeng sharan narang andrew ng sherjil ozair yipe peng ryan prenger sheng qian zongfeng quan jonathan raiman vinay rao sanjeev satheesh david seetapun shubho sengupta kavya srinet anuroop sriram haiyuan tang liliang tang chong wang jidong wang kaifu wang yi wang zhijian wang zhiqian wang shuang wu likai wei bo xiao wen xie yan xie dani yogatama bin yuan jun zhan zhenyao zhu deep speech speech tion english mandarin intern confer machin learn maria florina balcan kilian weinberg ed vol pmlr christof angermuel tanel pärnamaa leopold part oliv stegl deep learn comput biolog molecular system biolog acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic erfan azarkhish david rossi igor loi luca benini neurostream scalabl energi effici deep learn smart memori cube corr retriev http dzmitri bahdanau kyunghyun cho yoshua bengio neural machin translat jointli learn align translat corr retriev http nicola balla li yao chri pal aaron courvil delv deeper convolut network learn video represent corr retriev http yoshua bengio eric laufer guillaum alain jason yosinski deep gener stochast network abl backprop intern confer machin learn omnipress jonathan berant andrew chou roy frostig perci liang semant pars freebas answer pair empir method natur languag process vol associ comput linguist leo breiman statist model two cultur qualiti control appli statist david castelvecchi open black box ai natur chenyi chen ari seff alain kornhaus jianxiong xiao deepdriv learn afford direct percept autonom drive ieee intern confer comput vision ieee ting chen christoph chefd hotel deep learn base automat immun cell detect histochemistri imag intern workshop machin learn medic imag springer tianqi chen mu li yutian li min lin naiyan wang minji wang tianjun xiao bing xu chiyuan zhang zheng zhang mxnet flexibl effici machin learn librari heterogen distribut system corr retriev http sharan chetlur cliff woolley philipp vandermersch jonathan cohen john tran bryan catanzaro evan shelham cudnn effici primit deep learn corr retriev http chien hsieh nonstationari sourc separ use sequenti variat bayesian learn ieee transact neural network learn system kyunghyun cho bart van merrienbo çaglar gülçehr dzmitri bahdanau fethi bougar holger schwenk yoshua bengio learn phrase represent use rnn statist machin tion confer empir method natur languag process min chee choy dipti srinivasan ruey long cheu neural network continu onlin learn control ieee transact neural network cifar dataset retriev http cess april dan cireşan alessandro giusti luca gambardella jürgen schmidhub mitosi detect breast cancer histolog imag deep neural network intern confer medic imag comput intervent springer adam coat brodi huval tao wang david wu andrew ng bryan catanzaro deep learn cot hpc system intern confer machin learn omnipress ronan collobert sami bengio johnni mariéthoz torch modular machin learn softwar librari idiap georg dahl dong yu li deng alex acero deep neural network speech recognit ieee transact audio speech languag process navneet dalal bill trigg histogram orient gradient human detect ieee confer comput vision pattern recognit vol ieee jeffrey dean greg corrado rajat monga kai chen matthieu devin quoc le mark mao marc aurelio ranzato andrew senior paul tucker ke yang andrew ng larg scale distribut deep network intern confer neural inform process system curran associ li deng tutori survey architectur algorithm applic deep learn apsipa transact signal inform process li deng xiaodong jianfeng gao deep stack network inform retriev ieee tional confer acoust speech signal process ieee bill dolan chri quirk chri brockett unsupervis construct larg paraphras corpora ing massiv parallel news sourc intern confer comput linguist associ comput linguist jeffrey donahu lisa ann hendrick sergio guadarrama marcu rohrbach subhashini venugopalan kate saenko trevor darrel recurr convolut network visual recognit tion ieee confer comput vision pattern recognit ieee comput societi acm comput survey vol articl public date septemb pouyanfar et al li dong furu wei ming zhou ke xu question answer freebas convolut neural network annual meet associ comput linguist vol associ comput linguist timothi dozat incorpor nesterov momentum adam intern confer learn resent workshop john duchi elad hazan yoram singer adapt subgradi method onlin learn stochast optim confer learn theori omnipress moataz el ayadi moham kamel fakhri karray survey speech emot recognit featur classif scheme databas pattern recognit rasool fakoor faisal ladhak azad nazi manfr huber use deep learn enhanc cancer nosi classif intern confer machin learn omnipress christoph feichtenhof axel pinz andrew zisserman convolut network fusion video action recognit ieee confer comput vision pattern recognit ieee minwei feng bing xiang michael glass lidan wang bowen zhou appli deep learn answer select studi open task ieee workshop automat speech recognit understand ieee kunihiko fukushima neocognitron neural network model mechan pattern recognit unaffect shift posit biolog cybernet kavita ganesan chengxiang zhai jiawei han opinosi approach abstract rizat highli redund opinion intern confer comput linguist associ comput linguist john garofolo lori lamel william fisher jonathon fiscu david pallett darpa timit continu speech corpu nist speech disc nasa technic report n andrea geiger philip lenz christoph stiller raquel urtasun vision meet robot kitti dataset intern journal robot research ross girshick fast ieee intern confer comput vision ieee ross girshick jeff donahu trevor darrel jitendra malik rich featur hierarchi accur object detect semant segment ieee confer comput vision pattern recognit ieee xavier glorot yoshua bengio understand difficulti train deep feedforward neural network intern confer artifici intellig statist vol christoph goller andrea kuchler learn distribut represent tion structur ieee intern confer neural network vol ieee ian goodfellow yoshua bengio aaron courvil deep learn vol mit press ian goodfellow jean mehdi mirza bing xu david sherjil ozair aaron courvil yoshua bengio gener adversari net advanc neural inform process system curran associ googl alphago retriev http access april alex grave moham geoffrey hinton speech recognit deep recurr neural network ieee intern confer acoust speech signal process ieee hayit greenspan bram van ginneken ronald summer guest editori deep learn medic imag overview futur promis excit new techniqu ieee transact medic imag karol gregor yann lecun learn fast approxim spars code intern ferenc machin learn omnipress ha yimin yang samira pouyanfar haiman tian chen deep ing multimedia semant concept detect intern confer web inform system engin springer raia hadsel ays erkan pierr sermanet marco scoffier ur muller yann lecun deep belief net ing vision system autonom drive intern confer intellig robot system ieee kun han dong yu ivan tashev speech emot recognit use deep neural network extrem learn machin interspeech isca kaim georgia gkioxari piotr dollár ross girshick mask ieee intern confer comput vision ieee kaim xiangyu zhang shaoq ren jian sun deep residu learn imag recognit ieee confer comput vision pattern recognit ieee comput societi acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic irina higgin loic matthey xavier glorot arka pal benigno uria charl blundel shakir moham alexand lerchner earli visual concept learn unsupervis deep learn corr retriev http geoffrey hinton li deng dong yu georg dahl moham navdeep jaitli andrew senior vincent vanhouck patrick nguyen tara sainath brian kingsburi deep neural network acoust model speech recognit share view four research group ieee signal process magazin geoffrey hinton deep belief network scholarpedia geoffrey hinton simon osindero teh fast learn algorithm deep belief net neural comput juli sunpyo hong hyesoon kim integr gpu power perform model intern symposium comput architectur vol acm gao huang yu sun zhuang liu daniel sedra kilian weinberg deep network stochast depth european confer comput vision springer huang xiaodong jianfeng gao li deng alex acero larri heck learn deep structur semant model web search use clickthrough data acm intern confer inform knowledg manag acm huang minj kim mark pari smaragdi deep learn monaur speech separ ieee intern confer acoust speech signal process ieee david hubel torsten wiesel recept field binocular interact function architectur cat visual cortex journal physiolog imagenet retriev http access april intel nervana system neon deep learn framework retriev http access april anastasia ioannid elisavet chatzilari spiro nikolopoulo ioanni kompatsiari deep learn vanc comput vision data survey comput survey herbert jaeger harald haa har nonlinear predict chaotic system save energi wireless commun scienc yangq jia evan shelham jeff donahu sergey karayev jonathan long ross girshick sergio guadarrama trevor darrel caff convolut architectur fast featur embed acm intern ferenc multimedia acm michael jordan serial order parallel distribut process approach advanc psycholog junqua haton robust automat speech recognit fundament plicat vol springer scienc busi media mikael kågebäck olof mogren nina tahmasebi devdatt dubhashi extract summar use tinuou vector space model workshop continu vector space model composition cites associ comput linguist lukasz kaiser aidan gomez noam shazeer ashish vaswani niki parmar llion jone jakob uszkoreit one model learn corr retriev http andrej karpathi georg toderici sanketh shetti thoma leung rahul sukthankar li scale video classif convolut neural network ieee confer comput vision pattern recognit ieee comput societi kay joão carreira karen simonyan brian zhang chloe hillier sudheendra vijayanarasimhan fabio viola tim green trevor back paul natsev mustafa suleyman andrew zisserman kinet human action video dataset corr retriev http yoon kim convolut neural network sentenc classif corr retriev http diederik kingma jimmi ba adam method stochast optim corr retriev http diederik kingma max well variat bay corr retriev http morten kolbæk tan jesper jensen speech intellig potenti gener special deep neural network base speech enhanc system transact audio speech languag process jan koutník giusepp cuccu jürgen schmidhub faustino gomez evolv neural network reinforc learn annual confer genet evolutionari comput acm acm comput survey vol articl public date septemb pouyanfar et al vassili kovalev alexand kalinovski sergey kovalev deep learn theano torch caff sorflow one best speed accuraci intern confer pattern recognit inform process jonathan kraus benjamin sapp andrew howard howard zhou alexand toshev tom duerig jame philbin li unreason effect noisi data recognit european enc comput vision springer alex krizhevski ilya sutskev geoffrey hinton imagenet classif deep convolut neural network advanc neural inform process system pereira burg bottou weinberg curran associ michel lang helena kotthau peter marwedel clau weih jörg rahnenführ bernd bischl automat model select surviv analysi journal statist comput simul quoc le build featur use larg scale unsupervis learn ieee intern enc acoust speech signal process ieee yann lecun yoshua bengio convolut network imag speech time seri handbook brain theori neural network yann lecun yoshua bengio geoffrey hinton deep learn natur yann lecun léon bottou yoshua bengio patrick haffner learn appli document recognit proceed ieee mu li david andersen jun woo park alexand smola amr ahm vanja josifovski jame long eugen shekita su scale distribut machin learn paramet server usenix posium oper system design implement usenix associ xiangang li xihong wu construct long memori base deep recurr neural network larg vocabulari speech recognit ieee intern confer acoust speech signal process ieee yuxi li deep reinforc learn overview corr retriev http jifeng dai yi li kaim jian sun object detect via fulli convolut network advanc neural inform process system curran associ lin michael mair serg belongi jame hay pietro perona deva ramanan piotr dollár lawrenc zitnick microsoft coco common object context european confer comput vision springer geert litjen thij kooi babak ehteshami bejnordi arnaud arindra adiyoso setio francesco ciompi mohsen ghafoorian jeroen van der laak bram van ginneken clara sánchez survey deep learn medic imag analysi corr retriev http geert litjen clara sánchez nadya timofeeva meyk hermsen iri nagtega iringo kovac christina de kaa peter bult bram van ginneken jeroen van der laak deep learn tool increas accuraci effici histopatholog diagnosi scientif report feng liu bingquan liu chengji sun ming liu xiaolong wang deep belief approach link predict sign social network entropi wei liu dragomir anguelov dumitru erhan christian szegedi scott reed fu alexand berg ssd singl shot multibox detector european confer comput vision springer jonathan long evan shelham trevor darrel fulli convolut network semant segment ieee confer comput vision pattern recognit ieee comput societi david low object recognit local featur ieee intern confer comput vision vol ieee junji lu steven young itamar arel jeremi holleman analog deep engin storag μm cmo ieee journal circuit jianzhu michael ku yu samson fong keiichiro ono eric sage barri demchak rode sharan trey idek use deep learn model hierarch structur function cell natur method xiaolei haiyang yu yunpeng wang yinhai wang transport network congest evolut predict use deep learn theori plo one christoph man understand human languag nlp deep learn help nation acm sigir confer research develop inform retriev acm warren mcculloch walter pitt logic calculu idea imman nervou activ bulletin mathemat biophys acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic brendan mcmahan eider moor daniel ramag blais agüera arca feder learn deep network use model averag corr retriev http alessio mich neural network graph contextu construct approach ieee transact neural network azalia mirhoseini anna goldi hieu pham benoit steiner quoc le jeff dean hierarch model devic placement intern confer learn represent marc aurelio ranzato volodymyr mnih joshua susskind geoffrey hinton model natur imag use gate mrf ieee transact pattern analysi machin intellig mnist mnist databas handwritten digit retriev http cess april alexand mordvintsev christoph olah mike tyka inception go deeper neural work googl research blog retriev http access march igor mozet miha grcar jasmina smailov multilingu twitter sentiment classif role human annot corr retriev http maryam najafabadi flavio villanustr taghi khoshgoftaar naeem seliya randal wald edin muharemag deep learn applic challeng big data analyt journal big data preslav nakov alan ritter sara rosenth fabrizio sebastiani veselin stoyanov task sentiment analysi twitter intern workshop semant evalu associ comput linguist kazuhiro negi keisuk dohi yuichiro shibata kiyoshi oguri deep pipelin fpga tation human detect algorithm intern confer technolog ieee michael neumann ngoc thang vu attent convolut neural network base speech emot nition studi impact input featur signal length act speech corr retriev http evan newel yang cheng mass cytometri bless curs dimension natur ogi dat tien nguyen shafiq joti muhammad imran hassan sajjad prasenjit mitra applic line deep learn crisi respons use social media inform corr retriev http laisen nie dingd jiang lei guo shui yu houb song traffic matrix predict estim base deep learn data center network ieee globecom workshop ieee hyeonwoo noh seunghoon hong bohyung han learn deconvolut network semant tation ieee intern confer comput vision ieee pascal voc pascal visual object class retriev http cess april razvan pascanu caglar gulcehr kyunghyun cho yoshua bengio construct deep recurr neural network corr retriev http santiago pascual antonio bonafont joan serrà segan speech enhanc gener adversari network corr retriev http ryan poplin avinash varadarajan kati blumer yun liu michael mcconnel greg corrado lili peng dale webster predict cardiovascular risk factor retin fundu photograph via deep learn natur biomed engin samira pouyanfar chen automat video event detect imbal data use enhanc ensembl deep learn intern journal semant comput samira pouyanfar chen learn rate anneal deep neural network ieee intern confer multimedia big data ieee samira pouyanfar chen shyu effici deep network multimedia classif intern confer multimedia expo ieee alec radford luke metz soumith chintala unsupervis represent learn deep tional gener adversari network corr retriev http rajesh ranganath adler perott noémi elhadad david blei deep surviv analysi machin learn health care joseph redmon santosh divvala ross girshick ali farhadi onli look onc unifi object detect ieee confer comput vision pattern recognit ieee comput societi acm comput survey vol articl public date septemb pouyanfar et al shaoq ren kaim ross girshick jian sun faster toward object detect region propos network advanc neural inform process system mit press frank rosenblatt perceptron probabilist model inform storag organ brain psycholog review ruslan salakhutdinov geoffrey hinton deep boltzmann machin artifici intellig statist pmlr ruslan salakhutdinov geoffrey hinton effici learn procedur deep boltzmann machin neural comput dominik scherer andrea müller sven behnk evalu pool oper convolut tectur object recognit intern confer artifici neural network jürgen schmidhub deep learn neural network overview neural network frank seid gang li dong yu convers speech transcript use deep neural network annual confer intern speech commun associ isca pierr sermanet koray kavukcuoglu soumith chintala yann lecun pedestrian detect vise featur learn ieee confer comput vision pattern recognit ieee comput societi yelong shen xiaodong jianfeng gao li deng grégoir mesnil learn semant represent use convolut neural network web search intern world wide web confer acm karen simonyan andrew zisserman veri deep convolut network imag recognit corr retriev http skymind deep learn framework retriev http access april paul smolenski inform process dynam system foundat harmoni theori technic port dtic document richard socher eric huang jeffrey pennington andrew ng christoph man dynam pool unfold recurs autoencod paraphras detect advanc neural inform process system vol neural inform process system foundat richard socher cliff lin chri man andrew ng pars natur scene natur languag recurs neural network intern confer machin learn omnipress richard socher alex perelygin jean wu jason chuang christoph man andrew ng pher pott recurs deep model semant composition sentiment treebank enc empir method natur languag process cites associ comput linguist khurram soomro amir roshan zamir mubarak shah dataset human action class video wild corr retriev http hang su haoyu chen experi parallel train deep neural network use model averag corr retriev http ilya sutskev jame marten georg dahl geoffrey hinton import initi momentum deep learn intern confer machin learn christian szegedi wei liu yangq jia pierr sermanet scott reed dragomir anguelov dumitru erhan vincent vanhouck andrew rabinovich go deeper convolut ieee confer comput vision pattern recognit ieee comput societi bart thome david shamma gerald friedland benjamin elizald karl ni dougla poland damian borth li new data multimedia research commun acm haiman tian chen multipl correspond analysi base neural network disast inform detect ieee intern confer multimedia big data ieee haiman tian chen semant analyt system disast inform tion ieee intern confer multimedia big data ieee antonio torralba rob fergu william freeman million tini imag larg data set metric object scene recognit ieee transact pattern analysi machin intellig du tran lubomir bourdev rob fergu lorenzo torresani manohar paluri learn spatiotempor featur convolut network ieee intern confer comput vision ieee transfer learn convolut neural network visual recognit retriev http access april acm comput survey vol articl public date septemb survey deep learn algorithm techniqu applic trecvid trec video retriev evalu retriev http access april grigorio tsagkataki mustafa jaber panagioti tsakalid goal event detect sport video tronic imag nicola vasilach jeff johnson michaël mathieu soumith chintala serkan piantino yann lecun fast convolut net fbfft gpu perform evalu corr retriev http soroush vosoughi prashanth vijayaraghavan deb roy learn tweet embed use intern acm sigir confer research velop inform retriev acm chao wang lei gong qi yu xi li yuan xie xuehai zhou dlau scalabl deep learn acceler unit fpga ieee transact design integr circuit system peng wang baowen xu yurong wu xiaoyu zhou link predict social network art scienc china inform scienc joonata wehrmann willian becker henri cagnini rodrigo barro tional neural network twitter sentiment analysi intern joint confer neural network ieee chao weng dong yu michael seltzer jasha droppo deep neural network talker speech recognit transact audio speech languag process yonghui wu mike schuster zhifeng chen quoc le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klau macherey jeff klingner apurva shah melvin johnson xiaob liu lukasz kaiser stephan gouw yoshikiyo kato taku kudo hideto kazawa keith steven georg kurian nishant patil wei wang cliff young jason smith jason riesa alex rudnick oriol vinyal greg corrado macduff hugh jeffrey dean googl neural machin translat system bridg gap human machin translat corr retriev http sain xie ross girshick piotr dollár zhuowen tu kaim aggreg residu transform deep neural network corr retriev http omri yadan keith adam yaniv taigman marc aurelio ranzato train convnet corr yilin yan min chen saad sadiq shyu effici imbalanc multimedia concept retriev deep learn spark cluster intern journal multimedia data engin manag yilin yan min chen shyu chen deep learn imbalanc multimedia data classif ieee intern symposium multimedia ieee yilin yan qiusha zhu shyu chen classifi ensembl framework multimedia big data classif ieee intern confer inform reus integr ieee wenpeng yin hinrich schütze bing xiang bowen zhou abcnn convolut neural network model sentenc pair corr retriev http dong yu adam eversol michael seltzer kaisheng yao brian guenter oleksii kuchaiev frank seid huam wang jasha droppo zhiheng huang geoffrey zweig christoph rossbach jon currey duction comput network comput network toolkit annual confer intern speech commun associ isca dong yu morten kolbæk tan jesper jensen permut invari train deep model speech separ ieee intern confer acoust speech signal process ieee qi yu chao wang xiang xi li xuehai zhou deep learn predict process acceler base fpga intern symposium cluster cloud grid comput ieee matthew zeiler adadelta adapt learn rate method corr retriev http chen zhang peng li guangyu sun yijin guan bingjun xiao jason cong optim acceler design deep convolut neural network intern symposium programm gate array acm xueliang zhang deliang wang deep learn base binaur speech separ reverber ment transact audio speech languag process acm comput survey vol articl public date septemb pouyanfar et al xiang zhang junbo zhao yann lecun convolut network text classif advanc neural inform process system yue zhao xingyu jin xiaolin hu recurr convolut neural network speech process ieee intern confer acoust speech signal process ieee sigport zhiwei zhao youzheng wu convolut neural network sentenc classif annual confer intern speech commun associ isca receiv may revis april accept june acm comput survey vol articl public date septemb