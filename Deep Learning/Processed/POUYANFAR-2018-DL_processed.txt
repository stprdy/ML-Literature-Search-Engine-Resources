92 survey deep learning algorithm technique application samira pouyanfar florida international university saad sadiq yilin yan university miami haiman tian florida international university yudong tao university miami maria presa reyes florida international university shyu university miami chen iyengar florida international university field machine learning witnessing golden era deep learning slowly becomes leader domain deep learning us multiple layer represent abstraction data build computational model some key enabler deep learning algorithm generative adversarial network convolutional neural network model transfer completely changed perception information processing however exists aperture understanding behind tremendously domain wa never previously represented multiscope perspective lack core understanding render powerful method machine inhibit development fundamental level moreover deep learning ha repeatedly perceived silver bullet stumbling block machine learning far truth article present comprehensive review historical recent art approach visual audio text processing social network analysis natural language processing followed analysis pivoting groundbreaking advance deep learning application wa also undertaken review issue faced deep learning unsupervised learning model online learning illustrate challenge transformed prolific future research avenue cc concept computing methodology network machine learning algorithm allel algorithm distributed algorithm theory computation learning theory additional key word phrase deep learning neural network machine learning distributed processing big data survey acm reference format samira pouyanfar saad sadiq yilin yan haiman tian yudong tao maria presa reyes shyu ching chen iyengar survey deep learning algorithm technique application acm comput surv 51 5 article 92 september 2018 36 page author address pouyanfar tian reyes chen iyengar school computing information ences florida international university miami fl 33199 email chen iyengar sadiq yan tao shyu department electrical computer engineering university miami coral gable fl 33124 email shyu permission make digital hard copy part work personal classroom use granted without fee provided copy not made distributed profit commercial advantage copy bear notice full citation first page copyright component work owned others acm must honored abstracting credit permitted copy otherwise republish post server redistribute list requires prior specific permission fee request permission permission 2018 association computing machinery acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al 1 introduction recent year machine learning ha become popular research ha incorporated large number application including multimedia concept retrieval image sification video recommendation social network analysis text mining forth among iou algorithm deep learning also known representation learning 29 widely used application explosive growth availability data remarkable advancement hardware technology led emergence new study distributed deep learning deep learning ha root conventional neural network significantly outperforms predecessor utilizes graph technology transformation among neuron develop learning model many latest deep learning technique presented demonstrated promising result across different kind application natural language processing nlp visual data processing speech audio processing many application 169 170 traditionally efficiency algorithm highly relied goodness representation input data bad data representation often lead lower performance compared good data representation therefore feature engineering ha important research direction machine learning long time focus building feature raw data ha led lot research study furthermore feature engineering often domain specific requires significant human effort example computer vision different kind feature proposed compared including histogram oriented gradient hog 27 scale invariant feature transform sift 102 bag word bow new feature proposed performs well becomes trend year similar situation happened domain including speech recognition nlp comparatively deep learning algorithm perform feature extraction automated way allows researcher extract discriminative feature minimal domain knowledge human effort 115 algorithm include layered architecture data representation feature extracted last layer network feature extracted lower layer kind architecture originally inspired artificial intelligence ai simulating process key sensorial area human brain brain automatically extract data representation different scene input scene information received eye output classified object highlight major advantage deep mimic human brain work great success many field deep learning one hottest research direction society survey give overview deep learning different perspective including history challenge opportunity algorithm framework application parallel distributed computing technique history building machine simulate human brain dream sage century beginning deep learning traced back 300 aristotle proposed ationism started history human ambition trying understand brain since idea requires scientist understand mechanism human recognition system modern history deep learning started 1943 mcp model wa introduced became known prototype artificial neural model 107 created computer model based neural network functionally mimicking neocortex human brain 138 combination algorithm mathematics called threshold logic wa used model mimic human thought process not learn since deep learning ha evolved steadily significant milestone development acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application mcp model hebbian theory originally used biological system ral environment wa implemented 134 first electronic device called perceptron within context cognition system wa introduced 1958 though different typical perceptrons nowadays perceptron highly resembles modern one power substantiate associationism end first ai winter emergence propagandist became another milestone werbos introduced backpropagation use error training deep learning model opened gate modern neural network 1980 neocogitron inspired convolutional neural network wa introduced 40 current neural network rnns proposed 1986 73 next lenet made deep neural network dnns work practically however not get highly recognized 91 due hardware limitation structure lenet quite naive not applied large datasets around 2006 deep belief network dbns along pretraining framework developed 62 main idea wa train simple unsupervised model like restricted boltzmann machine rbms freeze parameter stick new layer top train parameter new layer researcher able train neural network much deeper previous attempt using technique prompted rebranding neural network deep learning originally artificial neural network anns decade development deep learning one efficient tool compared learning algorithm great performance seen deep learning method rooted initial anns including dbns rbms rnns convolutional neural network cnns 77 86 graphic processing unit gpus well known performance computing matrix network architecture single machine number distributed deep learning framework developed speed training deep learning model 8 108 171 vast amount data come without label noisy label some search study focus improving noise robustness training module using unsupervised semisupervised deep learning technique since current deep learning model only focus single modality lead limited representation data researcher paying attention structure may yield huge step forward deep learning 76 one recent inspirational application deep learning google alphago completely shocked world start year 2017 50 pseudonym name master 60 online game row human professional go player including three victory ke jie december 29 2016 january 4 alphago able defeat world champion go player us modern deep learning algorithm sufficient hardware resource research objective outline deep learning considered huge research field article aim draw big picture share research experience peer some previous survey paper only focused certain scope deep learning 36 70 novelty article focus different aspect deep learning presenting review paper author experience breakthrough research application deep neural network topmost challenge deep learning face today train massive datasets able hand datasets become bigger diverse complex deep learning ha path critical tool cater big data analysis survey challenge opportunity key area deep learning raised require attention ing parallelism scalability power optimization solve aforementioned issue different acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al table summary deep learning dl network dl network descriptive key point paper rvnn us structure goller et al 1996 47 preferred nlp socher et al 2011 146 rnn good sequential information cho et al 2014 20 preferred nlp speech processing li et al 2015 93 cnn originally image recognition extended nlp speech processing computer vision lecunn et al 1995 89 krizhevsky et al 2012 86 kim 2014 79 et al 2014 2 dbn unsupervised learning hinton 2009 61 directed connection hinton et al 2012 60 dbm unsupervised learning composite model rbms undirected connection salakhutdinov et al 2009 135 salakhutdinov et al 2012 136 gan unsupervised learning goodfellow et al 2014 49 framework radford et al 2015 130 vae unsupervised learning probabilistic graphical model kingma et al 2013 81 kind deep network introduced different domain rnns nlp cnns image processing article also introduces compare popular deep learning tool ing caffe tensorflow theano torch optimization technique deep learning tool addition various deep learning application reviewed help researcher expand view deep learning rest article organized follows section 2 popular deep learning network briefly presented section 3 discus several algorithm technique framework deep learning deep learning ha used nlp speech image recognition well application number deep learning application provided section section 5 point challenge potential research direction future finally section 6 concludes article 2 deep learning network section several popular deep learning network recursive neural network rvnn rnn cnn deep generative model discussed however since deep learning ha growing fast many new network new architecture appear every month scope article table 1 contains summary deep learning network duced section major key point representative paper recursive neural network rvnn rvnn make prediction hierarchical structure well classify output using compositional vector development rvnn wa mainly inspired recursive sociative memory raam 47 architecture created process object structured arbitrary shape tree graph approach wa take recursive data structure variable size generate distributed representation backpropagation acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application fig rvnn rnn cnn architecture structure bts learning scheme wa introduced train network 47 bts follows proach similar standard backpropagation algorithm also able support structure network trained autoassociation reproduce pattern input layer output layer rvnn ha especially successful nlp 2011 socher et al 146 proposed rvnn architecture handle input different modality 146 show two example using rvnn classify natural image natural language sentence image separated different segment interest sentence divided word rvnn calculates score possible pair merge build syntactic tree pair unit rvnn computes score plausibility merge pair highest score combined compositional vector merge rvnn generate 1 larger region multiple unit 2 compositional vector representing region 3 class label unit two noun word class label new region would noun phrase root rvnn tree structure compositional vector representation entire region figure 1 c show example rvnn tree recurrent neural network rnn another widely used popular algorithm deep learning especially nlp speech cessing rnn 20 unlike traditional neural network rnn utilizes sequential information network property essential many application embedded structure data sequence conveys useful knowledge example understand word sentence necessary know context therefore rnn seen memory unit include input layer x hidden state layer output layer figure 1 b depicts typical unfolded rnn diagram input sequence 124 three deep rnn approach including deep introduced based three solution deep rnn proposed not only take tage deeper rnn also reduces difficult learning deep network acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al one main issue rnn sensitivity vanishing exploding gradient 46 word gradient might decay explode exponentially due multiplication lot small big derivative training sensitivity reduces time mean network forgets initial input entrance new one therefore long memory lstm 93 utilized handle issue providing memory block recurrent connection memory block includes memory cell store network temporal state moreover includes gated unit control information flow furthermore residual tions deep network 58 alleviate vanishing gradient issue significantly discussed section convolutional neural network cnn cnn also popular widely used algorithm deep learning 89 ha extensively plied different application nlp 181 speech processing 26 computer vision 86 name similar traditional neural network structure inspired neuron animal human brain specifically simulates visual cortex cat brain containing complex sequence cell 67 described 48 cnn ha three main advantage namely parameter sharing sparse interaction equivalent representation fully utilize dimensional structure input data image signal local connection shared weight network utilized instead traditional fully connected network process result fewer parameter make network faster easier train operation similar one visual cortex cell cell sensitive small section scene rather whole scene word cell operate local filter input extract spatially local correlation existing data typical cnns number convolutional layer followed pooling subsampling layer final stage layer fully connected layer identical multilayer perceptron mlp usually used figure 1 c show example cnn architecture image classification layer cnns input x arranged three dimension r refers height width input r refers depth channel number r 3 rgb image convolutional layer several filter kernel k size n n n smaller input image q either smaller size mentioned earlier filter base local connection convolved input share parameter weight w k bias bk generate k feature map hk size similar mlp convolutional layer computes dot product weight input illustrated equation 1 input small region original input volume activation function f nonlinearity applied output convolutional layer hk f w k bk 1 thereafter subsampling layer feature map downsampled decrease eters network speed training process hence control overfitting pooling operation average max done p p p filter size contiguous region feature map finally final stage layer usually fully connected seen lar neural network layer take previous midlevel feature generate abstraction data last layer softmax svm used erate classification score score probability certain class given instance acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application fig structure generative model deep generative network four deep generative network dbn deep boltzmann machine dbm generative adversarial network gan variational autoencoder vae discussed dbn 61 brid probabilistic generative model typical rbm undirected connection formed top two layer lower layer use directed connection receive input layer lowest layer visible layer represents state input unit data vector dbn learns probabilistically reconstruct input unsupervised approach layer act feature detector input moreover training process supervised way give dbn capacity perform classification task dbn resembles composition several rbms 144 subnetwork hidden layer viewed visible layer next subnetwork figure 2 illustrates structure dbn rbms generative stochastic artificial neural network output probability distribution learned input energy configuration defined equation 2 calculate probability distribution based connection weight unit bias taking state vector v visible layer e v h 2 h binary configuration hidden layer unit b refer bias visible hidden unit respectively matrix w represents connection weight layer energy function provides probability possible visible hidden vector pair using equation 3 p v h v h 3 partition function defined sum v h possible configuration generally normalizing constant guarantee probability distribution aggregated 1 dbn includes greedy algorithm improve generative model allowing network sequentially receive different representation data since rbm not able model original data ideally initial weight learned data mapped transposed weighing matrix wt 0 create data next layer shown 62 log probability input data vector bounded approximating distribution furthermore time adding new layer dbn variational bound deeper layer improved compared previous one initializes new rbm block right direction like dbn dbm 135 learn complex internal representation considered robust deep learning model speech object recognition task hand unlike dbn approximate reasoning procedure allows dbm handle ambiguous input robustly figure 2 b present architecture dbm composite model rbms also clearly acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al show dbm differs dbn lower layer dbn build directed belief network instead undirected rbms dbm greedy training algorithm dbm easily calculated modifying procedure dbn factorial approximation posterior take either result first rbm probability second layer taking geometric average two distribution would better idea balance approximation posterior us second layer weight gan 49 consists generative model g discriminative model g capture distribution pд real data locally try differentiate sample come modeling data rather pд represented pm every iteration backpropagation generator discriminator like game cat mouse compete generator trying generate realistic data fool confuse discriminator latter try identify real data fake one generated minimax game established value function v g min g max v g loдd loд 1 g 4 represents probability came data rather pд pdata distribution data model considered stable reach point none improved pд pdata discriminator no longer identify two distribution figure 2 c show gan architecture another famous generative model vae 81 example vae architecture given figure 2 utilizes data leverage strategy deriving lower bound estimator directed graphical model continuous latent variable tive parameter θ generative model assist learning process variational parameter ϕ variational approximation model variational bayes aevb algorithm optimizes parameter ϕ θ probability encoder qϕ neural network approximation generative model pθ x z z latent variable simple distribution n 0 identity matrix aim maximize probability x training set entire generative process pθ x pθ z pθ dz 5 3 deep learning technique framework different deep learning algorithm help improve learning performance broaden scope application simplify calculation process however extremely long training time deep learning model remains major problem researcher furthermore classification accuracy drastically enhanced increasing size training data model parameter order accelerate deep learning processing several advanced technique proposed literature deep learning framework combine implementation modularized deep ing algorithm optimization technique distribution technique support infrastructure developed simplify implementation process boost ment research section some representative technique framework introduced unsupervised transfer learning contrary vast amount work done supervised deep learning study addressed unsupervised learning problem deep learning however recent year acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application benefit learning reusable feature using unsupervised technique ha shown promising result different application last decade idea learning framework ha widely discussed literature 88 130 140 recent year generative model gans vaes become dominant niques unsupervised deep learning instance gans trained reused fixed ture extractor supervised task 130 network based cnns ha shown supremacy unsupervised learning visual data analysis another work deep sparse toencoder trained image dataset learn feature 88 network ate feature extractor unlabeled data used face detection unsupervised manner generated feature also discriminative enough detect level object like animal face human body bengio et al 11 propose generative stochastic network unsupervised learning alternative maximum likelihood based transition operator markov chain monte carlo practice people luxury accessing gpus powerful hardware train deep network scratch reasonable time therefore pretraining deep network cnn datasets imagenet common technique also known transfer learning 157 done using pretrained network fixed feature extractor especially small new datasets weight pretrained model especially large new datasets similar original one latter model continue learning weight some level part deep network approach considered semisupervised learning labeled data insufficient train whole deep network online learning usually network topology architecture deep learning time static predefined learning start also time invariant 90 restriction time complexity pose serious challenge data streamed online online learning ously came mainstream research 21 only modest advancement ha observed online deep learning conventionally dnns built upon stochastic gradient descent sgd approach training sample used individually update model parameter known label need rather sequential processing sample update applied batch processing one approach wa presented 137 sample batch treated independent identically distributed iid batch processing approach proportionally balance computing resource execution time another challenge stack issue online learning data varying distribution challenge represents retail banking data pipeline hold tremendous business value current premise data largely close time safely assume piecewise stationarity thus similar distribution assumption characterizes data certain degree correlation develops model accordingly discussed 19 unfortunately nonstationary data stream not iid often longitudinal data stream moreover online learning often memory delimited harder parallelize requires linear learning rate input sample developing method capable online learning data would big leap forward big data deep learning optimization technique deep learning training dnn optimization process finding parameter network imize loss function practice sgd method 150 fundamental algorithm applied deep learning iteratively adjusts parameter based gradient training acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al sample computational complexity sgd lower original gradient descent method whole dataset considered every time parameter updated learning process updating speed controlled hyperparameter learning rate lower learning rate eventually lead optimal state long time higher learning rate decay loss faster may cause fluctuation training 128 order control oscillation sgd idea using momentum introduced inspired newton first law motion technique get faster convergence proper momentum improve optimization result sgd 150 hand several technique proposed determine proper learning rate itively weight decay learning rate decay introduced adjust learning rate erate convergence weight decay work penalty coefficient cost function avoid overfitting learning rate decay reduce learning rate dynamically improve performance moreover adapting learning rate respect gradient previous stage found helpful avoid fluctuation adagrad 35 first adaptive algorithm cessfully used deep learning amplifies learning rate infrequently updated parameter suppresses learning rate frequently updated parameter recording mulated squared gradient since squared gradient always positive learning rate adagrad become extremely small doe not optimize model anymore solve sue adadelta 176 proposed decay fraction introduced limit accumulation squared gradient follows e 1 6 e accumulated squared gradient stage squared gradient stage later adadelta improved introducing another decay fraction record accumulation gradient 80 shown adam performs better practice algorithm adaptive learning rate adamax also proposed paper extension adam l norm used adam replaced l norm achieve stable algorithm adam also incorporate nesterov accelerated gradient nag called nadam 34 show better convergence speed some case deep learning distributed system efficiency model training limited system distributed deep learning technique developed accelerate training process two main approach train model distributed system namely data parallelism model parallelism data parallelism model replicated computational node model trained assigned subset data certain period time weight update need synchronized among node comparatively model parallelism data processed one model node responsible partial estimation parameter model among approach straightforward algorithm combine result slave node parameter averaging 108 let wt parameter neural network node time n slave node used training time weight master node wt copy current parameter distributed slave node updated parameter sent back master node weight time 1 master node 1 n n 7 acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application parameter averaging would identical training parameter averaged ter minibatch worker process number data copy however network communication synchronization cost nullify benefit extra machine therefore averaging process usually applied certain number minibatches fed slave node frequency training model performance need balanced required popular approach data parallelism us sgd known data parallelism 149 update learning rate decay momentum ferred however synchronous weight update not scalable larger cluster overhead communication increase exponentially respect number node therefore rameter server framework proposed google process training asynchronously 92 instead waiting parameter updated master node asynchronous update allows node spend time computation meanwhile network communication cost significantly reduced decentralization transmitting update mode instead mode hand model parallelism approach split training step across multiple gpus straightforward strategy gpu computes only subset model example model two lstm layer system two gpus use calculate one lstm layer advantage strategy make training prediction massive deep neural network possible 28 instance cot hpc system trained neural network 11 billion parameter requires memory 24 impossible fit large model one machine therefore need partitioned using strategy however since model partitioned across node one drawback model parallelism node only compute subset result 8 synchronization thus needed get full result synchronization loss communication overhead strategy strategy since node former must synchronize gradient parameter value every update step word scalability model parallelism inferior handle issue google ha proposed automated device placement framework based deep reinforcement learning find best scheme model partition placement 110 framework take embedding representation operation place grouped operation different device show 60 performance improvement compared human expert strategy limitation one hand data parallelism ha many training module ha decrease learning rate make ing procedure smooth hand model parallelism ha many segmentation output node increase sharply reduce efficiency accordingly 168 generally speaking larger dataset beneficial data parallelism larger deep learning model suitable model parallelism besides compared data parallelism hard hide communication needed synchronization model parallelism only partial information included node whole batch though some vanced framework like tensorflow 1 support asynchronous kernel save communication cost thus necessary wait till synchronization step finish moving forward next layer since activity unable processed only partial information two kind strategy also fused hybrid model discussed 168 deep learning framework table 2 list smattering popular deep learning framework architecture design caffe 72 143 torch 25 neon 69 theano 5 mxnet 17 tensorflow 1 microsoft cognitive toolkit cntk 173 table 2 license core acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al table comparison different deep learning framework framework license core language interface support cnn rnn support dbn support caffe 72 bsd python matlab yes no 143 apache java java scala python yes yes torch 25 bsd c lua lua python yes yes neon 69 apache python python yes yes theano 5 bsd python python yes yes mxnet 17 apache python r scala perl julia etc yes yes tensorflow 1 apache python python java go yes yes cntk 173 mit python brainscript yes no language supported interface language framework support cnn rnn dbn also listed observed table 2 usually used implementation deep learning framework accelerates speed training since gpu significantly helpful speed matrix computation aforementioned framework also support gpu via terface provided cudnn 18 meanwhile python ha become common language deep learning architecture design since make programming efficient easier simplifying programming process also distributed calculation becomes common some cently released framework tensorflow mxnet cntk goal improve calculation efficiency deep learning moreover tensorflow also includes support customized deep learning integrated circuit asic called tensor processing unit tpu help increase efficiency decrease power consumption caffe implemented berkeley vision learning center one widely used framework 72 support commonly used layer cnn rnn doe not directly enable use dbn user caffe design architecture declaring structure computation graph convolutional layer pretrained model available wide range neural network alexnet 86 googlenet 151 resnet 58 thermore caffe framework word doe not support multinode execution calculation supported external offering like caffeonspark yahoo integrate caffe big data engine like spark popular framework implemented java developed maintained mind since 2014 143 cooperating hadoop spark capable distributed putation well however framework reported longer training time similar architecture benchmarked framework 84 torch wa first released 2002 extended deep learning feature 2011 25 combined facebook deep learning cuda library fbcunn 160 torch operate acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application fig some popular deep learning application parallel computation unlike framework torch built based dynamic graph representation instead static graph dynamic graph allows user update computational graph change model structure runtime static graph us certain function define graph advance recently torch released python interface pytorch usage framework ha greatly increased due flexibility neon 69 theano 5 two framework developed python intel sity montreal respectively perform code optimization system kernel level therefore training speed usually outperform framework however although only parallelism supported multinode calculation not designed framework mxnet support several interface including python r scala perl matlab javascript go julia 17 support computation graph declaration imperative computation declaration architecture design mxnet not only support data model parallelism also follows parameter server scheme support distributed calculation well mxnet ha comprehensive functionality performance not optimized much art framework tensorflow implemented google provides series internal function help ment any deep neural network based static computational graph 1 recently kera started support tensorflow via interface allowed user design architecture worrying internal design framework provides different level parallel distributed operation fatal tolerance robustness design attracts lot user ha become one popular deep learning framework since release cntk designed microsoft ha specific script language brainscript neural network implementation 173 cntk model neural network directed graph node graph represents operation filter edge refers data flow instead parameter server model message passing interface applied distributed calculation support 4 various application deep learning nowadays application deep learning include not limited nlp sentence fication translation etc visual data processing computer vision multimedia data analysis etc speech audio processing enhancement recognition etc social network analysis healthcare section provides detail different technique used application some main deep learning application also visualized figure acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al table popular deep learning method nlp paper nlp task architecture datasets socher et al 2013 147 sentiment analysis rntn sst kim 2014 79 sentiment analysis general classification cnn sst wehrmann et al 2017 164 sentiment analysis mtd bahdanau et al 2014 9 translation bidir rnn decoder cho et al 2014 20 translation rnn decoder wu et al 2016 166 translation gnmt socher et al 2011 145 paraphrase identification unfolding rae msrp yin et al 2015 172 paraphrase identification question answer abcnn wikiqa msrp kågebäck et al 2014 75 summarization unfolding rae od dong et al 2015 33 question answer mccnn wq feng et al 2015 39 question answer cnn iqa natural language processing nlp series algorithm technique mainly focus teaching computer stand human language some nlp task include document classification translation phrase identification text similarity summarization question answering nlp development challenging due complexity ambiguous structure human language moreover natural language highly context specific literal meaning change based form word sarcasm domain specificity deep learning method recently able strate several successful attempt achieving high accuracy nlp task table 3 contains mary some leading deep learning nlp solution architecture datasets nlp model follow similar preprocessing step 1 input text broken word tokenization 2 word reproduced form vector representing word low dimension important create accurate perception tie difference various word challenge arrives need decide length word contained procedure context specific requires prior domain knowledge some highly impactful approach solving nlp task presented sentiment analysis branch nlp deal examining text classifying feeling opinion writer datasets sentiment analysis labeled either positive negative neutral phrase removed subjectivity classification method one popular example standford sentiment treebank sst 147 dataset movie review labeled five category ranging negative positive along introduction sst socher et al 147 propose recursive neural tensor network rntn utilizes word vector par tree represent phrase capturing interaction element composition function recursive approach advantageous come classification since grammar often display structure acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application kim 79 improves accuracy sst following different approach even though cnn model first created image recognition classification mind implementation nlp ha proven success achieving excellent result kim present simple cnn model using one convolution layer top trained vector bow architecture model kept relatively simple small number hyperparameters tuning combination low tuning pretrained parameter managed achieve high accuracy several benchmark social medium popular source data studying sentiment multilingual twitter dataset mtd 114 one largest public datasets containing million manually annotated tweet 13 language applying sentiment analysis tweet challenging due short nature text address issue multilingual dataset small amount text 164 proposes architecture exempt dependence language although approach wa not capable outperforming embedding architecture author argue simplicity predictive power consumption good tradeoff machine translation deep learning ha played important role improvement traditional automatic translation method cho et al 20 introduced novel coding decoding architecture train word neural machine translation nmt rnn framework us two rnns one map input sequence length vector rnn decodes vector target symbol downside rnn performance deterioration input sequence symbol becomes larger bahdanau et al 9 address issue introducing vector jointly learning align translate procedure approach perform binary search look part speech predictive translation nonetheless recently proposed translation system known computationally expensive inefficient handling sentence containing rare word thus 166 google neural machine translation gnmt system proposed introducing balance flexibility provided level model efficiency model gnmt deep lstm network make use eight encoder eight decoder layer connected using mechanism method wa first introduced improve nmt general model achieved score wmt 14 benchmark paraphrase identification paraphrase identification process analyzing two tences projecting similar based underlying hidden semantics key feature beneficial several nlp job plagiarism detection answer question context detection summarization domain identification socher et al 145 propose use unfolding recursive autoencoders raes measure similarity two sentence using syntactic tree develop feature space measure tie even though similar rvnn rae useful unsupervised classification unlike rvnn rae computes reconstruction error instead supervised score merging two vector compositional vector paper also introduced dynamic pooling layer compare classify two sentence different size either paraphrase not several notable method investigated 31 monolingual semantic similarity detection also enlist some notable datasets paraphrase identification microsoft research paraphrase corpus msrp topically clustered news article dataset cnn abcnn recently proposed deep learning architecture goal determining interdependence two sentence 172 paraphrase detection ha also applied answer selection textual entailment acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al summarization automatic summarization extract significant relevant information large text document summary effectively reduces size text without losing important information considerably decrease time computation required analyze large datasets kågebäck et al 75 propose continuous vector model sentence model evaluates multiple combination composition meaningful representation new vector representation tested using rae compared simple vector addition paper make use rouge benchmarking metric evaluate effectiveness summarization framework ganesan et al 41 utilize model produce brief summary opinion dataset known opinosis dataset od model target user opinion term feedback product review customer satisfaction report without losing any educative material question answering automatic system able terpret natural language question use reasoning return appropriate reply modern knowledge base famous freebase dataset allow field flourish leap time feature rule set specific domain dong et al 33 came multicolumn cnn approach analyze question several aspect context choose underlying semantic meaning answer form answer use multitasking approach rank pair also neously learns correlation affiliation word semantics general deep learning architecture not limited any one language proposed 39 tion answering qa framework proposed paper based cnn us approach answer question insurance domain test many different setup based architecture compare result berant et al 12 propose highly scalable version model solution problem large datasets avoid logical form text learn model solely tuples abcnn 172 prof bility nlp task ranking candidate answer based closely interdependent question datasets used paper mentioned section web question wq insurance question answering iqa wikiqa visual data processing deep learning technique become main part various multimedia tems computer vision 54 specifically cnns shown significant result different task including image processing object detection video processing section discus detail recent deep learning framework algorithm proposed past year visual data processing image classification 1998 lecun et al presented first version 91 conventional cnn includes two convolutional layer along subsampling layer finally ending full connection last layer although since early cnn technique greatly leveraged different problem including segmentation detection classification image almost forsaken data mining research group one decade later cnn algorithm ha started prosperity computer vision community specifically alexnet 86 considered first cnn model substantially improved image classification result large dataset imagenet wa winner ilsvrc 2012 improved best result previous year almost 10 regarding top five test error improve efficiency speed training gpu implementation cnn utilized network data augmentation dropout technique also used substantially reduce overfitting problem acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application fig network top five error layer imagenet classification time since variety cnn method developed submitted ilsvrc tition 2014 two influential different model presented mostly focused depth neural network first one known vggnet 142 includes simple cnn network layer spatial size input reduced depth network increased achieve effective efficient model although vggnet wa not winner ilsvrc 2014 still show significant improvement top five error previous top model came two major specification simplicity depth contrast vggnet googlenet 151 winner competition error proposed new complex module named inception allowing several operation pooling convolutional etc work parallel microsoft deep residual network known resnet 58 took lead 2015 titions including ilsvrc 2015 coco detection segmentation task introducing residual connection cnns designing learning model 50 152 layer model achieved incredible performance top five error mean first time computer model could beat human brain 5 10 error image classification contrary extremely deep representation resnet handle vanishing gradient 46 well degradation problem saturated accuracy deep network utilizing residual block last year several variation resnet proposed first group method ha tried increase number layer current cnn model may include layer 64 finally 2017 resnext 167 wa proposed extension resnet vggnet simple model includes several branch residual block performing transformation finally aggregated summation operation general model reshaped technique alexnet resnext outperforms original version resnet using half layer also improves well network imagenet dataset figure 4 demonstrates revolution depth performance image classification imagenet time problem supervised image classification regarded solved imagenet classification challenge concluded 2017 object detection semantic segmentation deep learning technique play major role advancement object detection recent year best object detection performance came complex system several feature sift hog etc context however advent new deep learning technique object detection ha also reached new stage advancement advance driven successful method region proposal cnn 45 bridge acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al gap object detection image classification introducing object localization method using deep network addition transfer learning pertaining large dataset imagenet utilized since small object detection datasets pascal 123 include insufficient labeled data train large cnn network however training computational time memory expensive especially new network vggnet moreover object detection step slow later technique extended overcome aforementioned issue introducing two successful technique fast 44 faster 133 former leverage sharing computation speed original train deep vggnet latter proposes region proposal network rpn enables almost object detection object detection called yolo only look 132 contains single cnn convolutional network performs detection class probability tion box simultaneously benefit yolo include fast training testing 45 frame per second reasonable performance compared previous system unlike recent method called fully convolutional network 95 utilizes fully convolutional network share almost computation image method us resnet classifier object detector achieves speed faster faster method finally multibox detector ssd 100 proposed faster yolo performance accurate technique faster model based single cnn generates set bounding box fixed size well corresponding object score box semantic segmentation process understanding image pixel level necessary application autonomous driving robot vision medical system question convert image classification semantic segmentation recent year many research study apply deep learning technique classify image lutional network 122 instance includes deconvolution unpooling module detect classify segmentation region another work fully convolutional network fcn 101 proposed utilizes network alexnet vggnet googlenet recently mask wa proposed facebook ai research fair 57 object instance segmentation extends faster adding new branch generates segmentation mask prediction region interest time bounding box class label generated simple flexible model ha shown great performance result coco instance segmentation object detection video processing video analytics ha attracted considerable attention computer vision community considered challenging task since includes spatial poral information early work youtube video containing 487 sport class used train cnn model 77 model includes multiresolution architecture utilizes local motion information video includes context stream image ing fovea stream image processing module classify video event detection sport video using deep learning presented 159 work spatial temporal information encoded using cnns feature fusion via regularized coder recent year new technique called recurrent convolution network rcns 32 wa introduced video processing applies cnns video frame visual understanding feed frame rnns analyzing temporal information video new rcn model proposed 10 us rnn intermediate layer cnns addition gated recurrent unit used leverage sparsity locality rnn module model validated datasets acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application table popular visual datasets deep learning dataset data type num instance num class ground truth application imagenet 68 image yes image classification object localization object detection etc 22 image yes image classification pascal voc 123 image 20 yes image classification object detection semantic segmentation microsoft coco 96 image 80 yes object detection semantic segmentation mnist 112 image 10 yes handwritten digit classification 152 image video partially video image understanding 4 video automatic video classification trecvid 158 video varies varies partially video search event detection localization etc 148 video 101 yes human action detection kinetics 78 video 400 yes human action detection cnn 156 ha demonstrated better performance video analysis task traditional cnns automatically learns spatiotemporal feature video input model appearance motion time network 38 another set video analysis technique model spatial rgb frame temporal information optical flow separately average prediction last layer network network extended recent work called inflated convnet utilizing idea also pretrained kinetics dataset 78 proposed approach could significantly enhance performance action recognition datasets visual datasets significant advancement image video processing not only rely development new learning algorithm utilization powerful hardware also crucially depend public datasets several visual datasets used train deep learning algorithm listed table imagenet 68 considered important influential dataset deep learning used train popular network alexnet googlenet vggnet resnet due labeled image collection image dataset utilized many research study 22 dataset also used evaluating many dnns image classification task mentioned earlier pascal voc microsoft coco used various object detection semantic segmentation task finally 4 relatively new dataset generated google play role imagenet video processing utilized benchmark dataset various video analysis including event detection understanding classification speech audio processing audio processing process operates directly electrical analog audio signal essary speech recognition speech transcription speech enhancement phone classification acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al music classification speech processing active research area importance perfect interaction till century automatic speech recognition asr 74 technology ha risen unprecedented level however still far mimicking behavior communicate human asr system made many component including speech signal preprocessing feature extraction acoustic ing phonetic unit recognition language modeling traditional asr system integrate hidden markov model hmms gaussian mixture model gmms hmms used deal variation speech related time space gmms represent acoustic characteristic sound unit modeling process requires large training dataset order reach high accuracy anns 60 introduced composed many nonlinear computational element operating parallel however deeper architecture multiple layer needed settle limitation gmms sufficiently representing hmms dbn one commonly used deep learning model area significantly improves performance acoustic model model spectral variation speech rbms building block seide et al 139 use pretrained dbns demonstrate strength model publicly available benchmark switchboard transcription task introduce weight sparseness related learning strategy reduce recognition error model size followed widely studied dbn pretraining method dahl et al 26 propose novel acoustic model lvsr model integrates pretrained dnn using dbn pretraining algorithm dependent cd hidden markov model named use unsupervised dbn pretraining algorithm activate training process instead phoneme benchmark evaluation wa performed lvsr wa first application applied large vocabulary dataset pretrained dnn model many research study follow direction investigate improvement evaluate efficiency different investigating strength dbns graf et al 51 focus exploration deep rnns achieves testing error timit phoneme dataset 42 deep lstm performs better recognizing context using memory cell store information recent year interest speech recognition not restricted improvement acoustic model within asr system 6 large rnn including bidirectional layer multiple convolutional layer wa trained end end using connectionist temporal classification ctc loss function proposed deep rnn architecture called deep speech 2 take advantage capacity provided deep learning system keep robustness overall network noisy environment besides approach ha shown capability quickly applying new language recognizers scalability model deployment gpu server also evaluated model achieves higher efficiency transcription hybrid model rcnn introduced 180 work lvsr originally cnns introduced asr alleviate computational problem however tend challenging train slow converge core module inside rcnn recurrent convolutional layer rcl whose state evolves discrete time step comparison made lstm timit phoneme dataset besides speech recognition task many research study focus speech emotion tion ser 36 speech enhancement se seaker separation current proaches summarized table 5 speech emotion recognition ser emotion influence voice characteristic linguistic content speech ser relies heavily effectiveness speech feature used acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application table popular deep learning approach audio processing paper task architecture datasets abel tim 2017 3 se dnn timit sdc ntt han et al 2014 56 ser elm iemocap huang et al 2014 66 se dnn rnn timit kolbæk et al 2017 82 se dnn neumann vu 2017 118 ser attentive cnn iemocap pascual et al 2017 125 se gan voice bank corpus weng et al 2015 165 dnn brir yu et al 2017 174 dnn cnn zhang wang 2017 178 dnn 2006 speech separation challenge data classification classified two type 1 global model statistical function hsfs mean variance median linear regression coefficient etc 2 dynamic modeling approach dynamic descriptor lld like mel frequency cepstral coefficient mfcc voicing probability ratio forth 56 newly developed ann one hidden layer called extreme learning machine elm proposed classification using dnns method evaluated using audio track interactive emotional dyadic motion capture iemocap database contains audiovisual data 10 actor experimental result demonstrate elm performance enhanced compared approach besides showing strength attentive cnn model feature learning cnn also utilized speech emotion recognition 118 work achieves performance result improvised iemocap data speech enhancement se recently speech enhancement ha aimed improve speech quality using deep learning algorithm 3 dnn artificial speech bandwidth extension abe framework proposed deal speech enhancement task narrowband speech signal input timit database u sdc base used training dnn model pretrained sigmoid unit improvement achieved ntt database upper band cepstral distance mo point improved compared baseline huang et al 66 study monaural source separation deep learning study joint optimization dnns rnns extra masking layer proposed mance evaluation compared nonnegative matrix factorization nmf model using timit speech corpus pascual et al 125 also propose segan leverage gan speech enhancement speech separation viewed subtask speech enhancement aim rate reverberant target speech spatially diffuse background interference 178 different environment speaker separation focus reconstructing speech speaker mixed speech one speaker talking simultaneously early stage several method including soft mask modulation frequency analysis sparse sition proposed address issue input approach proposed attack multitalker speech recognition problem 165 posed approach showed remarkable noise robustness outperformed ibm superhuman tem 174 author implement speech separation model training technique model includes dnn three hidden layer one cnn acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al model generate separation view instead multiclass regression segmentation wa previously popular application aforementioned application deep learning algorithm also applied formation retrieval robotics transportation prediction autonomous driving biomedicine disaster management forth please note deep learning ha shown capability leveraged various application only some selected application introduced section social network analysis popularity many social network like facebook ter ha enabled user share large amount information including picture thought opinion due fact deep learning ha shown promising performance visual data nlp different deep learning approach adopted social network analysis including semantic evaluation 116 161 179 link prediction 99 163 crisis response 120 semantic evaluation important field social network analysis aim help chine understand semantic meaning post social network although variety niques proposed analyze text nlp approach may fail address several main challenge social network analysis spelling error abbreviation special ters informal language 161 twitter considered commonly used source sentiment classification cial network analysis general sentiment analysis aim determine attitude reviewer purpose semeval ha provided benchmark dataset based twitter run timent classification task since 2013 116 another similar example amazon started online bookstore world largest online retailer abundance purchase transaction vast amount review created customer making amazon dataset great source sentiment classification 179 field social network link prediction also commonly used many scenario recommendation network completion social tie prediction approach applied improve performance prediction tackle problem ity nonlinearity 163 since data social network highly dynamic conventional deep learning algorithm ha modified adapt characteristic deep learning approach use rbm perform link prediction since unknown link user directly modeled hidden layer rbm thus predicted liu et al propose pervised dbn approach based pretrained rbms link prediction 99 approach process separated three step pretrained dbn constructed part two layer rmbs contained dbn first step unsupervised link prediction encoded link used input feature generate predicted link unsupervised manner next representation original link generated based output unsupervised link prediction feature representation step final step link prediction step performed link representation generate predicted link supervised way different task semantic classification link prediction crisis response social network requires immediate detection natural disaster main goal crisis response identify informative piece post classify ing topical class like flood earthquake wildfire forth address topic nguyen et al propose deep learning framework combined online learning feature automatically detect possible disaster tweet sentence level identify type detected disaster 120 first goal performed binary classification network using acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application informative noninformative piece post label informative post classified specific type information retrieval deep learning ha great impact information retrieval structured semantic modeling dssm proposed document retrieval web search 65 latent semantic analysis conducted dnn query along data used determine result retrieval encoded query data mapped word hashing feature space generated multilayer nonlinear projection proposed dnn trained bridge given query semantic meaning help data however posed model treat word separately ignores connection word resentative improved version method convolutional dssm 141 word sequence mapped feature space convolutional structure grated generate several feature space subset word sequence end layer additional projection layer used generate final output general information retrieval task deep stacking network dsns proposed 30 atomic module dsn composed simple classifier nonlinear function step previous output module stacked original input generate new result using method original input feature represented abstract feature thus retrieval result improved transportation prediction transportation prediction another application deep ing et al 105 propose deep learning framework based architecture dict transportation network congestion evolution due congestion one location congestion status encoded binary representation historical data transportation congestion used visible unit input sequence model proposed method show least improvement accuracy approach take around 3 runtime however reasonable accuracy efficiency reached cost losing sensitivity specificity model instead traffic internet traffic complex due property analyzed deep learning approach traffic matrix prediction estimation method data center network proposed based dbn 121 prediction module logistic regression model contained output layer generate predicted traffic matrix value based model trained historical data estimation module dbm model trained prior link count input traffic matrix time output therefore current traffic matrix estimated using proposed model link count cost le computational time resource deep learning approach show least improvement prediction timation comparison approach autonomous driving large number big company unicorn startup including google tesla aurora uber working automotive technology back 2008 hadsell et al used relatively simple dbn two convolutional layer one max sampling layer extract deep feature 55 used learning technique vision terrain training classifier discriminate feature vector recently autonomous driving system categorized robotics approach recognizing object behavioral cloning approach learn direct mapping sensory input driving action traditional robotics approach involve recognition object combination sensor fusion object detection image classification path planning control theory geiger et al built rectified autonomous driving dataset acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al capture wide range interesting scenario including car pedestrian traffic lane road sign traffic light 43 behavioral cloning approach often based deep learning involves training dnns take sensor input produce steering throttle brake output koutník et al trained fully connected rnns using reinforcement learning proach 83 also used compressed network encoding reduce dimensionality search space utilizing inherent regularity environment keep car track network map image directly steering angle recent paper take advantage approach 15 constructing mapping image several possible affordance dicators road situation distance lane marking angle car relative road distance car current adjacent lane compact affordance representation perception output build automated work learn deep learning feature image affordance estimation make driving decision autonomous driving technology mature still ha long way go handle unpredictable complex situation biomedicine deep learning highly progressive research field reach domain histopathology open opportunity one early attempt includes sensing mitotic figure cell proposed 23 another method employed autoencoder section basal cell carcinoma breast cancer 98 framework applies cnns sentinel lymph node try accurately detect clump isolated tumor cell however method lag generalizing large datasets make harder evaluate relevance moreover several studied method using cnns train model single patient care center lab treatment stroke prostate cancer breast cancer survival risk prediction dures highly relevant huge gap deep learning method domain only notable paper concentrating deep survival analysis 97 survival analysis founded structured attribute like patient age marital status bmi recent advancement medical imaging provide unstructured image also predict survival probability tionally feature obtained human design however researcher challenged feature provide limited insight depicting highly conceptual data 87 deep learning el cnns perfectly suited represent conceptual attribute survival analysis successfully outperform existing cox framework nonetheless still limitation challenge require attention research community 131 newest research progress machine learning complicated biomedicine task accomplished deep learning technique even fascinating news machine learn reveal thing undetectable human recently research team google stanford 126 used deep learning discover new knowledge retinal fundus image predict cardiovascular risk factor not previously thought quantifiable present retinal image beyond current human knowledge disaster management system another application disaster management system attracted great attention community disaster affect munity human life economy structure disaster information system help general public personnel emergency operation center eoc aware rent hazard situation assist disaster relief process 154 currently major challenge applying deep learning method disaster information system system need deal data provide accurate assistance acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application nearly manner accident natural catastrophe suddenly happens great amount data need collected analyzed tian et al 153 apply traditional neural work build prototype disaster information system mlp integrated feature translation algorithm perform multilayer learning structure though research study utilize deep learning disaster information management 127 129 still early stage ha great potential deep learning 5 deep learning challenge future direction acute development deep learning research venue limelight deep learning ha gained extraordinary momentum speech language visual detection system however several domain practically still untouched dnns due either challenging nature lack data availability general public creates significant tie fertile ground rewarding future research avenue section domain key insight challenge likely future direction major deep learning method discussed lingering perception dnns meaning deep learning model assessed based final output without understanding get decision weak statistical interpretability ha also identified 52 especially application data produced not any type physical manifestation et al explain neural network using cell biology molecular scale 104 mapped layer neural network component yeast cell starting microscopic nucleotide make dna moving upward larger structure ribosome take instruction dna make protein finally moving organelle like mitochondrion nucleus run cell operation since visible neural network could easily observe change cellular mechanism dna wa altered one unique technique google brain peer synthetic brain dnn method called inceptionism 113 isolates specific part data neuron estimate see certainty neuron process coupled deep dream technique map network response any given image 14 instance image cat dog relevant neuron almost always pretty sure dog floppy ear cat pointy ear help dissect datasets interpret part network manning et al 106 also talk similar method understand semantics behind given dataset peeking various network path activated part data however lack attention problem largely attributed different way statistician professional use deep learning 37 plausible way forward relate neural network existing physical biological phenomenon aid developing metaphysical relationship help demystify dnn brain moreover consensus literature deep learning researcher need simplify interface low processing overhead model analyzed better understanding lead u next challenge relevant future problem not sufficient training sample label 90 apart zettabyte currently available data petabyte data added every day exponential growth piling data never labeled human assistance current sentiment favor supervised learning mostly readily available label small size current datasets 53 however rapid increase size complexity data unsupervised learning dominant solution future 111 current deep learning model also need adapt rising issue data sparsity missing data messy data order capture approximated information observation rather training furthermore incomplete acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al heterogeneous unlabeled datasets open venue deep learning method exciting inherent agnostic nature dnns give unique ability work unsupervised data 59 advanced deep learning model built handle noisy messy data 85 author 155 attempt tackle challenging database 80 million tiny image contains rgb photo query used novel robust cost function reduce noisy label data moreover increasing number application involve huge amount data streaming live format including time series sequencing xml file social network data store suffer incompleteness heterogeneity unlabeled data deep learning model learn domain ha discussion relevant problem time 6 another landmark challenge faced deep learning method reduction dimensionality without losing critical information needed classification 119 medical application like cancer rna sequencing analysis common number sample label far le number feature current deep learning model cause severe overfitting problem inhibits proper classification untrained case 7 method try empirically deduce variable predictability 13 reduce feature set supervised manner often result loss resolution detail similar challenge faced analyzing medical image training data tremendously costly obtain foundational paper attempted build model require minimal number sample learning 16 98 23 stand pioneer publication applying cnns breast prostate cancer detection strong way forward known deep reinforcement learning 94 idea inferred behavioral psychology agent take action minimize aggregate cost method use game theory control theory multiagent system forth learn perform action given data limited multimedia data start feeding image network say give generates feedback loop cloud look similar rabbit neural network reinforce look like rabbit several iteration process consequently make network predict rabbit distinctly till elaborate bunny appears result fascinating even relatively small network trained tumor cell used overinterpret image detect minute detail currently undetected deep learning one growing pain deep learning relates issue computational efficiency achieving maximum throughput consuming least amount resource 103 rent deep learning framework require considerable amount computational resource proach performance 177 one method attempt overcome challenge using reservoir computing 71 another alternative use incremental approach exploit medium large datasets offline training 109 current year many researcher shifted focus build parallel scalable deep learning framework 26 60 lately focus ha shifted migrate learning process gpus however gpus notorious leakage current abstract any plausible realization deep learning el portable device 63 one solution use gate array fpgas deep learning accelerator order optimize data access pipeline achieve significantly better result 175 wang et al 162 use deep learning accelerator unit dlau able architecture us three pipelined processing unit use tile method locality technique attain time increase speed compared cpu power consumption another approach target architecture based fpgas arc loss leakage forth still manages achieve 97 detection rate 117 able acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application achieve time faster processing speed software implementation although gpus provide peak performance fpgas require le power similar performance throughput mounted motherboard unique approach proposed 177 ment cnns using roofline model since memory bandwidth fpga design critical evaluate required memory bandwidth using loop tiling implementation achieved gigaflops significantly reduces power consumption gigaflops unit measuring performance unit processor unfortunately no deep learning fpga test bed available time limit exploration area only well versed fpga design 6 summary deep learning new hot topic machine learning defined cascade layer performing nonlinear processing learn multiple level data representation decade researcher tried discover pattern data representation raw data method called representation learning unlike conventional data mining technique deep learning able generate data representation massive volume raw data therefore ha provided solution many plication article survey algorithm technique deep learning start history artificial neural network since 1940 move recent deep learning algorithm major breakthrough different application key algorithm framework area well popular technique deep learning presented first briefly introduces traditional neural network several supervised deep learning algorithm including recurrent recursive convolutional neural network well deep belief network boltzmann machine thereafter advanced deep learning approach unsupervised online learning discussed moreover several optimization technique also provided ular framework area include tensorflow caffe theano addition handle big data challenge distributed technique deep learning briefly discussed thereafter article review successful deep learning method various application including nlp visual data processing speech audio processing social network analysis article discus challenge provides several existing solution challenge however still several issue need addressed future deep learning several finding article possible future work summarized although deep learning memorize massive amount data information weak reasoning understanding data make solution many tions interpretability deep learning investigated future deep learning still ha difficulty modeling multiple complex data modality time multimodal deep learning another popular direction recent deep learning search unlike human brain deep learning need extensive datasets preferably labeled data training machine predicting unseen data problem becomes daunting available datasets small healthcare data data need processed real time learning learning studied recent year alleviate problem majority existing deep learning implementation supervised algorithm machine learning gradually shifting unsupervised semisupervised learning handle data without manual human label acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al spite deep learning advancement recent year many application still untouched deep learning early stage leveraging deep learning niques disaster information management finance medical data analytics deep learning new method provides numerous challenge well opportunity solution variety application importantly transfer machine learning new stage namely smarter reference 1 martín abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat ian goodfellow andrew harp geoffrey irving michael isard yangqing jia rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg dan mané rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda viégas oriol vinyals pete warden martin wattenberg martin wicke yuan yu xiaoqiang zheng tensorflow machine learning heterogeneous distributed system corr 2016 retrieved 2 ossama mohamed hui jiang li deng gerald penn dong yu convolutional neural network speech recognition transaction audio speech language processing 22 10 2014 3 johannes abel tim fingscheidt dnn regression approach speech enhancement artificial width extension ieee workshop application signal processing audio acoustic ieee 4 sami nisarg kothari joonseok lee paul natsev george toderici balakrishnan varadarajan heendra vijayanarasimhan video classification benchmark corr 2016 retrieved 5 ramus guillaume alain amjad almahairi christof angermueller dzmitry bahdanau nicolas ballas frédéric bastien justin bayer anatoly belikov alexander belopolsky yoshua bengio arnaud bergeron james bergstra valentin bisson josh bleecher snyder nicolas bouchard nicolas xavier bouthillier alexandre de brébisson olivier breuleux carrier kyunghyun cho jan chorowski paul christiano tim cooijmans côté myriam côté aaron courville yann dauphin olivier leau julien demouth guillaume desjardins sander dieleman laurent dinh mélanie ducoffe vincent dumoulin samira ebrahimi kahou dumitru erhan ziye fan orhan firat mathieu germain xavier glorot ian goodfellow matt graham caglar gulcehre philippe hamel iban harlouchet heng balázs hidasi sina honari arjun jain sébastien jean kai jia mikhail korobov vivek kulkarni alex lamb pascal lamblin eric larsen césar laurent sean lee simon lefrancois simon lemieux nicholas léonard zhouhan lin jesse livezey cory lorenz jeremiah lowin qianli manzagol olivier mastropietro robert mcgibbon roland vic bart van merriënboer vincent michalski mehdi mirza alberto orlandi christopher pal razvan pascanu mohammad pezeshki colin raffel daniel renshaw matthew rocklin adriana romero markus roth peter owski john salvatier françois savard jan schlüter john schulman gabriel schwartz iulian vlad serban dmitriy serdyuk samira shabanian étienne simon sigurd spieckermann ramana subramanyam jakub sygnowski jérémie tanguay gijs van tulder joseph turian sebastian urban pascal vincent francesco visin harm de vries david dustin webb matthew willson kelvin xu lijun xue li yao saizheng zhang ying zhang theano python framework fast computation mathematical expression corr 2016 retrieved 6 dario amodei sundaram ananthanarayanan rishita anubhai jingliang bai eric battenberg carl case jared casper bryan catanzaro qiang cheng guoliang chen jie chen jingdong chen zhijie chen mike chrzanowski adam coates greg diamos ke ding niandong du erich elsen jesse engel weiwei fang linxi fan christopher fougner liang gao caixia gong awni hannun tony han lappi vaino johannes bing jiang cai ju billy jun patrick legresley libby lin junjie liu yang liu weigao li xiangang li dongpeng sharan narang andrew ng sherjil ozair yiping peng ryan prenger sheng qian zongfeng quan jonathan raiman vinay rao sanjeev satheesh david seetapun shubho sengupta kavya srinet anuroop sriram haiyuan tang liliang tang chong wang jidong wang kaifu wang yi wang zhijian wang zhiqian wang shuang wu likai wei bo xiao wen xie yan xie dani yogatama bin yuan jun zhan zhenyao zhu deep speech 2 speech tion english mandarin international conference machine learning maria florina balcan kilian weinberger ed vol pmlr 7 christof angermueller tanel pärnamaa leopold part oliver stegle deep learning computational biology molecular system biology 12 7 2016 acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application 8 erfan azarkhish davide rossi igor loi luca benini neurostream scalable energy efficient deep learning smart memory cube corr 2017 retrieved 9 dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate corr 2014 retrieved 10 nicolas ballas li yao chris pal aaron courville delving deeper convolutional network learning video representation corr 2015 retrieved 11 yoshua bengio eric laufer guillaume alain jason yosinski deep generative stochastic network able backprop international conference machine learning omnipress 12 jonathan berant andrew chou roy frostig percy liang semantic parsing freebase answer pair empirical method natural language processing vol association computational linguistics 6 13 leo breiman statistical modeling two culture quality control applied statistic 48 1 2003 14 davide castelvecchi open black box ai nature 538 7623 2016 15 chenyi chen ari seff alain kornhauser jianxiong xiao deepdriving learning affordance direct perception autonomous driving ieee international conference computer vision ieee 16 ting chen christophe chefd hotel deep learning based automatic immune cell detection histochemistry image international workshop machine learning medical imaging springer 17 tianqi chen mu li yutian li min lin naiyan wang minjie wang tianjun xiao bing xu chiyuan zhang zheng zhang mxnet flexible efficient machine learning library heterogeneous distributed system corr 2015 retrieved 18 sharan chetlur cliff woolley philippe vandermersch jonathan cohen john tran bryan catanzaro evan shelhamer cudnn efficient primitive deep learning corr 2014 retrieved http 19 chien hsieh nonstationary source separation using sequential variational bayesian learning ieee transaction neural network learning system 24 5 2013 20 kyunghyun cho bart van merrienboer çaglar gülçehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio learning phrase representation using rnn statistical machine tion conference empirical method natural language processing 21 min chee choy dipti srinivasan ruey long cheu neural network continuous online learning control ieee transaction neural network 17 6 2006 22 cifar datasets retrieved cessed april 18 2017 23 dan cireşan alessandro giusti luca gambardella jürgen schmidhuber mitosis detection breast cancer histology image deep neural network international conference medical image computing intervention springer 24 adam coates brody huval tao wang david wu andrew ng bryan catanzaro deep learning cot hpc system international conference machine learning omnipress 25 ronan collobert samy bengio johnny mariéthoz torch modular machine learning software library idiap 26 george dahl dong yu li deng alex acero deep neural network speech recognition ieee transaction audio speech language processing 20 1 2012 27 navneet dalal bill triggs histogram oriented gradient human detection ieee conference computer vision pattern recognition vol ieee 28 jeffrey dean greg corrado rajat monga kai chen matthieu devin quoc le mark mao marc aurelio ranzato andrew senior paul tucker ke yang andrew ng large scale distributed deep network international conference neural information processing system curran associate 29 li deng tutorial survey architecture algorithm application deep learning apsipa transaction signal information processing 3 2014 30 li deng xiaodong jianfeng gao deep stacking network information retrieval ieee tional conference acoustic speech signal processing ieee 31 bill dolan chris quirk chris brockett unsupervised construction large paraphrase corpus ing massively parallel news source international conference computational linguistics association computational linguistics 350 32 jeffrey donahue lisa anne hendricks sergio guadarrama marcus rohrbach subhashini venugopalan kate saenko trevor darrell recurrent convolutional network visual recognition tion ieee conference computer vision pattern recognition ieee computer society acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al 33 li dong furu wei ming zhou ke xu question answering freebase convolutional neural network annual meeting association computational linguistics vol association computational linguistics 34 timothy dozat incorporating nesterov momentum adam international conference learning resentations workshop 35 john duchi elad hazan yoram singer adaptive subgradient method online learning stochastic optimization conference learning theory omnipress 36 moataz el ayadi mohamed kamel fakhri karray survey speech emotion recognition feature classification scheme database pattern recognition 44 3 2011 37 rasool fakoor faisal ladhak azade nazi manfred huber using deep learning enhance cancer nosis classification international conference machine learning omnipress 38 christoph feichtenhofer axel pinz andrew zisserman convolutional network fusion video action recognition ieee conference computer vision pattern recognition ieee 39 minwei feng bing xiang michael glass lidan wang bowen zhou applying deep learning answer selection study open task ieee workshop automatic speech recognition understanding ieee 40 kunihiko fukushima neocognitron neural network model mechanism pattern recognition unaffected shift position biological cybernetics 36 4 1980 41 kavita ganesan chengxiang zhai jiawei han opinosis approach abstractive rization highly redundant opinion international conference computational linguistics association computational linguistics 42 john garofolo lori lamel william fisher jonathon fiscus david pallett darpa timit continuous speech corpus nist speech disc nasa technical report n 93 1993 43 andreas geiger philip lenz christoph stiller raquel urtasun vision meet robotics kitti dataset international journal robotics research 32 11 2013 44 ross girshick fast ieee international conference computer vision ieee 45 ross girshick jeff donahue trevor darrell jitendra malik rich feature hierarchy accurate object detection semantic segmentation ieee conference computer vision pattern recognition ieee 46 xavier glorot yoshua bengio understanding difficulty training deep feedforward neural network international conference artificial intelligence statistic vol 47 christoph goller andreas kuchler learning distributed representation tion structure ieee international conference neural network vol ieee 48 ian goodfellow yoshua bengio aaron courville deep learning vol mit press 49 ian goodfellow jean mehdi mirza bing xu david sherjil ozair aaron courville yoshua bengio generative adversarial net advance neural information processing system curran associate 50 google alphago retrieved accessed april 18 2017 51 alex graf mohamed geoffrey hinton speech recognition deep recurrent neural network ieee international conference acoustic speech signal processing ieee 52 hayit greenspan bram van ginneken ronald summer guest editorial deep learning medical imaging overview future promise exciting new technique ieee transaction medical imaging 35 5 2016 53 karol gregor yann lecun learning fast approximation sparse coding international ference machine learning omnipress 54 ha yimin yang samira pouyanfar haiman tian chen deep ing multimedia semantic concept detection international conference web information system engineering springer 55 raia hadsell ayse erkan pierre sermanet marco scoffier ur muller yann lecun deep belief net ing vision system autonomous driving international conference intelligent robot system ieee 56 kun han dong yu ivan tashev speech emotion recognition using deep neural network extreme learning machine interspeech isca 57 kaiming georgia gkioxari piotr dollár ross girshick mask ieee international conference computer vision ieee 58 kaiming xiangyu zhang shaoqing ren jian sun deep residual learning image recognition ieee conference computer vision pattern recognition ieee computer society acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application 59 irina higgins loic matthey xavier glorot arka pal benigno uria charles blundell shakir mohamed alexander lerchner early visual concept learning unsupervised deep learning corr 2016 retrieved 60 geoffrey hinton li deng dong yu george dahl mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath brian kingsbury deep neural network acoustic modeling speech recognition shared view four research group ieee signal processing magazine 29 6 2012 61 geoffrey hinton deep belief network scholarpedia 4 5 2009 5947 62 geoffrey hinton simon osindero teh fast learning algorithm deep belief net neural computation 18 7 july 2006 63 sunpyo hong hyesoon kim integrated gpu power performance model international symposium computer architecture vol acm 64 gao huang yu sun zhuang liu daniel sedra kilian weinberger deep network stochastic depth european conference computer vision springer 65 huang xiaodong jianfeng gao li deng alex acero larry heck learning deep structured semantic model web search using clickthrough data acm international conference information knowledge management acm 66 huang minje kim mark paris smaragdis deep learning monaural speech separation ieee international conference acoustic speech signal processing ieee 67 david hubel torsten wiesel receptive field binocular interaction functional architecture cat visual cortex journal physiology 160 1 1962 68 imagenet retrieved accessed april 18 2017 69 intel nervana system neon deep learning framework retrieved accessed april 4 2017 70 anastasia ioannidou elisavet chatzilari spiros nikolopoulos ioannis kompatsiaris deep learning vances computer vision data survey computing survey 50 2 2017 20 71 herbert jaeger harald haas harnessing nonlinearity predicting chaotic system saving energy wireless communication science 304 5667 2004 72 yangqing jia evan shelhamer jeff donahue sergey karayev jonathan long ross girshick sergio guadarrama trevor darrell caffe convolutional architecture fast feature embedding acm international ference multimedia acm 73 michael jordan serial order parallel distributed processing approach advance psychology 121 1986 74 junqua haton robustness automatic speech recognition fundamental plication vol springer science business medium 75 mikael kågebäck olof mogren nina tahmasebi devdatt dubhashi extractive summarization using tinuous vector space model workshop continuous vector space model compositionality citeseer association computational linguistics 76 lukasz kaiser aidan gomez noam shazeer ashish vaswani niki parmar llion jones jakob uszkoreit one model learn corr 2017 retrieved 77 andrej karpathy george toderici sanketh shetty thomas leung rahul sukthankar li scale video classification convolutional neural network ieee conference computer vision pattern recognition ieee computer society 78 kay joão carreira karen simonyan brian zhang chloe hillier sudheendra vijayanarasimhan fabio viola tim green trevor back paul natsev mustafa suleyman andrew zisserman kinetics human action video dataset corr 2017 retrieved 79 yoon kim convolutional neural network sentence classification corr 2014 retrieved 80 diederik kingma jimmy ba adam method stochastic optimization corr 2014 retrieved 81 diederik kingma max welling variational bayes corr 2013 retrieved 82 morten kolbæk tan jesper jensen speech intelligibility potential general specialized deep neural network based speech enhancement system transaction audio speech language processing 25 1 2017 83 jan koutník giuseppe cuccu jürgen schmidhuber faustino gomez evolving neural network reinforcement learning annual conference genetic evolutionary computation acm acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al 84 vassili kovalev alexander kalinovsky sergey kovalev deep learning theano torch caffe sorflow one best speed accuracy international conference pattern recognition information processing 85 jonathan krause benjamin sapp andrew howard howard zhou alexander toshev tom duerig james philbin li unreasonable effectiveness noisy data recognition european ence computer vision springer 86 alex krizhevsky ilya sutskever geoffrey hinton imagenet classification deep convolutional neural network advance neural information processing system 25 pereira burges bottou weinberger curran associate 87 michel lang helena kotthaus peter marwedel claus weihs jörg rahnenführer bernd bischl automatic model selection survival analysis journal statistical computation simulation 85 1 2015 88 quoc le building feature using large scale unsupervised learning ieee international ence acoustic speech signal processing ieee 89 yann lecun yoshua bengio convolutional network image speech time series handbook brain theory neural network 3361 10 1995 90 yann lecun yoshua bengio geoffrey hinton deep learning nature 521 7553 2015 91 yann lecun léon bottou yoshua bengio patrick haffner learning applied document recognition proceeding ieee 86 11 1998 92 mu li david andersen jun woo park alexander smola amr ahmed vanja josifovski james long eugene shekita su scaling distributed machine learning parameter server usenix posium operating system design implementation usenix association 93 xiangang li xihong wu constructing long memory based deep recurrent neural network large vocabulary speech recognition ieee international conference acoustic speech signal processing ieee 94 yuxi li deep reinforcement learning overview corr 2017 retrieved 95 jifeng dai yi li kaiming jian sun object detection via fully convolutional network advance neural information processing system curran associate 96 lin michael maire serge belongie james hay pietro perona deva ramanan piotr dollár lawrence zitnick microsoft coco common object context european conference computer vision springer 97 geert litjens thijs kooi babak ehteshami bejnordi arnaud arindra adiyoso setio francesco ciompi mohsen ghafoorian jeroen van der laak bram van ginneken clara sánchez survey deep learning medical image analysis corr 2017 retrieved 98 geert litjens clara sánchez nadya timofeeva meyke hermsen iris nagtegaal iringo kovacs christina de kaa peter bult bram van ginneken jeroen van der laak deep learning tool increased accuracy efficiency histopathological diagnosis scientific report 6 2016 26286 99 feng liu bingquan liu chengjie sun ming liu xiaolong wang deep belief approach link prediction signed social network entropy 17 4 2015 100 wei liu dragomir anguelov dumitru erhan christian szegedy scott reed fu alexander berg ssd single shot multibox detector european conference computer vision springer 101 jonathan long evan shelhamer trevor darrell fully convolutional network semantic segmentation ieee conference computer vision pattern recognition ieee computer society 102 david lowe object recognition local feature ieee international conference computer vision vol ieee 103 junjie lu steven young itamar arel jeremy holleman 1 analog deep engine storage μm cmos ieee journal circuit 50 1 2015 104 jianzhu michael ku yu samson fong keiichiro ono eric sage barry demchak roded sharan trey ideker using deep learning model hierarchical structure function cell nature method 15 4 2018 105 xiaolei haiyang yu yunpeng wang yinhai wang transportation network congestion evolution prediction using deep learning theory plo one 10 3 2015 106 christopher manning understanding human language nlp deep learning help national acm sigir conference research development information retrieval acm 107 warren mcculloch walter pitt logical calculus idea immanent nervous activity bulletin mathematical biophysics 5 4 1943 acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application 108 brendan mcmahan eider moore daniel ramage blaise agüera arca federated learning deep network using model averaging corr 2016 retrieved 109 alessio micheli neural network graph contextual constructive approach ieee transaction neural network 20 3 2009 110 azalia mirhoseini anna goldie hieu pham benoit steiner quoc le jeff dean hierarchical model device placement international conference learning representation 111 marc aurelio ranzato volodymyr mnih joshua susskind geoffrey hinton modeling natural image using gated mrfs ieee transaction pattern analysis machine intelligence 35 9 2013 112 mnist mnist database handwritten digit retrieved cessed april 18 2017 113 alexander mordvintsev christopher olah mike tyka inceptionism going deeper neural work google research blog retrieved accessed march 26 2018 114 igor mozetic miha grcar jasmina smailovic multilingual twitter sentiment classification role human annotator corr 2016 retrieved 115 maryam najafabadi flavio villanustre taghi khoshgoftaar naeem seliya randall wald edin muharemagic deep learning application challenge big data analytics journal big data 2 1 2015 116 preslav nakov alan ritter sara rosenthal fabrizio sebastiani veselin stoyanov task 4 sentiment analysis twitter international workshop semantic evaluation association computer linguistics 117 kazuhiro negi keisuke dohi yuichiro shibata kiyoshi oguri deep pipelined fpga tation human detection algorithm international conference technology ieee 118 michael neumann ngoc thang vu attentive convolutional neural network based speech emotion nition study impact input feature signal length acted speech corr 2017 retrieved 119 evan newell yang cheng mass cytometry blessed curse dimensionality nature ogy 17 8 2016 120 dat tien nguyen shafiq joty muhammad imran hassan sajjad prasenjit mitra application line deep learning crisis response using social medium information corr 2016 retrieved 121 laisen nie dingde jiang lei guo shui yu houbing song traffic matrix prediction estimation based deep learning data center network ieee globecom workshop ieee 122 hyeonwoo noh seunghoon hong bohyung han learning deconvolution network semantic tation ieee international conference computer vision ieee 123 pascal voc pascal visual object class retrieved cessed april 18 2017 124 razvan pascanu caglar gulcehre kyunghyun cho yoshua bengio construct deep recurrent neural network corr 2013 retrieved 125 santiago pascual antonio bonafonte joan serrà segan speech enhancement generative adversarial network corr 2017 retrieved 126 ryan poplin avinash varadarajan katy blumer yun liu michael mcconnell greg corrado lily peng dale webster prediction cardiovascular risk factor retinal fundus photograph via deep learning nature biomedical engineering 2 3 2018 127 samira pouyanfar chen automatic video event detection imbalance data using enhanced ensemble deep learning international journal semantic computing 11 1 2017 128 samira pouyanfar chen learning rate annealing deep neural network ieee international conference multimedia big data ieee 129 samira pouyanfar chen shyu efficient deep network multimedia classification international conference multimedia expo ieee 130 alec radford luke metz soumith chintala unsupervised representation learning deep tional generative adversarial network corr 2015 retrieved 131 rajesh ranganath adler perotte noémie elhadad david blei deep survival analysis machine learning health care 132 joseph redmon santosh divvala ross girshick ali farhadi only look unified object detection ieee conference computer vision pattern recognition ieee computer society acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al 133 shaoqing ren kaiming ross girshick jian sun faster towards object detection region proposal network advance neural information processing system mit press 134 frank rosenblatt perceptron probabilistic model information storage organization brain psychological review 65 6 1958 386 135 ruslan salakhutdinov geoffrey hinton deep boltzmann machine artificial intelligence statistic pmlr 136 ruslan salakhutdinov geoffrey hinton efficient learning procedure deep boltzmann machine neural computation 24 8 2012 137 dominik scherer andreas müller sven behnke evaluation pooling operation convolutional tectures object recognition international conference artificial neural network 6354 2010 138 jürgen schmidhuber deep learning neural network overview neural network 61 2015 139 frank seide gang li dong yu conversational speech transcription using deep neural network annual conference international speech communication association isca 140 pierre sermanet koray kavukcuoglu soumith chintala yann lecun pedestrian detection vised feature learning ieee conference computer vision pattern recognition ieee computer society 141 yelong shen xiaodong jianfeng gao li deng grégoire mesnil learning semantic representation using convolutional neural network web search international world wide web conference acm 142 karen simonyan andrew zisserman deep convolutional network image recognition corr 2014 retrieved 143 skymind deep learning framework retrieved accessed april 18 2017 144 paul smolensky information processing dynamical system foundation harmony theory technical port dtic document 145 richard socher eric huang jeffrey pennington andrew ng christopher manning dynamic pooling unfolding recursive autoencoders paraphrase detection advance neural information processing system vol neural information processing system foundation 146 richard socher cliff lin chris manning andrew ng parsing natural scene natural language recursive neural network international conference machine learning omnipress 147 richard socher alex perelygin jean wu jason chuang christopher manning andrew ng pher potts recursive deep model semantic compositionality sentiment treebank ence empirical method natural language processing citeseer association computational linguistics 148 khurram soomro amir roshan zamir mubarak shah dataset 101 human action class video wild corr 2012 retrieved 149 hang su haoyu chen experiment parallel training deep neural network using model averaging corr 2015 retrieved 150 ilya sutskever james marten george dahl geoffrey hinton importance initialization momentum deep learning international conference machine learning 151 christian szegedy wei liu yangqing jia pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich going deeper convolution ieee conference computer vision pattern recognition ieee computer society 152 bart thomee david shamma gerald friedland benjamin elizalde karl ni douglas poland damian borth li new data multimedia research communication acm 59 2 2016 153 haiman tian chen multiple correspondence analysis based neural network disaster information detection ieee international conference multimedia big data ieee 154 haiman tian chen semantic analytics system disaster information tion ieee international conference multimedia big data ieee 155 antonio torralba rob fergus william freeman 2008 80 million tiny image large data set metric object scene recognition ieee transaction pattern analysis machine intelligence 30 11 2008 156 du tran lubomir bourdev rob fergus lorenzo torresani manohar paluri learning spatiotemporal feature convolutional network ieee international conference computer vision ieee 157 transfer learning convolutional neural network visual recognition retrieved accessed april 25 acm computing survey vol 51 no 5 article publication date september survey deep learning algorithm technique application 158 trecvid trec video retrieval evaluation retrieved accessed april 18 2017 159 grigorios tsagkatakis mustafa jaber panagiotis tsakalides 2017 goal event detection sport video tronic imaging 2017 16 2017 160 nicolas vasilache jeff johnson michaël mathieu soumith chintala serkan piantino yann lecun fast convolutional net fbfft gpu performance evaluation corr 2014 retrieved http 161 soroush vosoughi prashanth vijayaraghavan deb roy learning tweet embeddings using international acm sigir conference research velopment information retrieval acm 162 chao wang lei gong qi yu xi li yuan xie xuehai zhou dlau scalable deep learning accelerator unit fpga ieee transaction design integrated circuit system 36 3 2016 517 163 peng wang baowen xu yurong wu xiaoyu zhou link prediction social network art science china information science 58 1 2015 164 joonatas wehrmann willian becker henry cagnini rodrigo barros tional neural network twitter sentiment analysis international joint conference neural network ieee 165 chao weng dong yu michael seltzer jasha droppo deep neural network talker speech recognition transaction audio speech language processing 23 10 2015 1679 166 yonghui wu mike schuster zhifeng chen quoc le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin johnson xiaobing liu lukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wei wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes jeffrey dean google neural machine translation system bridging gap human machine translation corr 2016 retrieved 167 saining xie ross girshick piotr dollár zhuowen tu kaiming aggregated residual transformation deep neural network corr 2016 retrieved 168 omry yadan keith adam yaniv taigman marc aurelio ranzato training convnets corr 2013 169 yilin yan min chen saad sadiq shyu efficient imbalanced multimedia concept retrieval deep learning spark cluster international journal multimedia data engineering management 8 1 2017 170 yilin yan min chen shyu chen deep learning imbalanced multimedia data classification ieee international symposium multimedia ieee 171 yilin yan qiusha zhu shyu chen classifier ensemble framework multimedia big data classification ieee international conference information reuse integration ieee 622 172 wenpeng yin hinrich schütze bing xiang bowen zhou abcnn convolutional neural network modeling sentence pair corr 2015 retrieved 173 dong yu adam eversole michael seltzer kaisheng yao brian guenter oleksii kuchaiev frank seide huaming wang jasha droppo zhiheng huang geoffrey zweig christopher rossbach jon currey duction computational network computational network toolkit annual conference international speech communication association isca 174 dong yu morten kolbæk tan jesper jensen permutation invariant training deep model speech separation ieee international conference acoustic speech signal processing ieee 175 qi yu chao wang xiang xi li xuehai zhou deep learning prediction process accelerator based fpga international symposium cluster cloud grid computing ieee 176 matthew zeiler adadelta adaptive learning rate method corr 2012 retrieved 177 chen zhang peng li guangyu sun yijin guan bingjun xiao jason cong optimizing accelerator design deep convolutional neural network international symposium programmable gate array acm 178 xueliang zhang deliang wang deep learning based binaural speech separation reverberant ments transaction audio speech language processing 25 2017 acm computing survey vol 51 no 5 article publication date september 2018 pouyanfar et al 179 xiang zhang junbo zhao yann lecun convolutional network text classification advance neural information processing system 180 yue zhao xingyu jin xiaolin hu recurrent convolutional neural network speech processing ieee international conference acoustic speech signal processing ieee sigport 181 zhiwei zhao youzheng wu convolutional neural network sentence classification annual conference international speech communication association isca received may 2017 revised april 2018 accepted june 2018 acm computing survey vol 51 no 5 article publication date september 2018