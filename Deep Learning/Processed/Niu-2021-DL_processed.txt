review attent mechan deep learn zhaoyang niu guoqiang zhong hui yu b depart comput scienc technolog ocean univers china qingdao china b school creativ technolog univers portsmouth portsmouth uk r c l e n f articl histori receiv februari revis march accept march avail onlin april commun zidong wang keyword attent mechan deep learn recurr neural network rnn convolut neural network cnn uniﬁ attent model comput vision applic natur languag process applic b r c attent ha arguabl becom one import concept deep learn ﬁeld inspir biolog system human tend focu distinct part process larg amount inform develop deep neural network attent mechan ha wide use divers applic domain thi paper aim give overview art attent model propos recent year toward better gener understand attent anism deﬁn uniﬁ model suitabl attent structur step attent mechan implement model describ detail furthermor classifi exist attent model accord four criteria soft attent form input featur input represent output represent besid summar network architectur use conjunct attent mechan describ typic applic attent mechan final discuss interpret attent bring deep learn present potenti futur trend elsevi right reserv introduct attent complex cognit function indispens human one import properti percept human tend process whole inform entireti onc instead human tend select concentr part inform need ignor perceiv inform time instanc human usual see scene ning end visual perceiv thing instead observ pay attent speciﬁc part need human ﬁnd scene often ha someth want observ certain part learn focu part similar scene appear focu attent use part thi mean human quickli select mation massiv inform use limit process resourc attent mechan greatli improv cienci accuraci perceptu inform process attent mechan human divid two categori accord gener manner ﬁrst gori unconsci attent call base attent driven extern stimuli exampl peopl like hear loud voic dure convers similar gate mechan deep learn pass appropri valu larger valu next step second categori consciou tion call focus attent focus attent refer tion ha predetermin purpos reli speciﬁc task enabl human focu attent certain object sciousli activ attent mechan deep learn design accord speciﬁc task focus attent attent mechan introduc thi paper usual refer focus attent except special statement mention abov attent mechan use resourc alloc scheme main mean solv problem inform overload case limit pute power process import inform ite comput resourc henc research bring attent comput vision area propos visual attent model extract local visual featur get potenti salient region neural network area use attent mechan recurr neural network model classifi imag bahdanau et al use attent mechan simultan perform translat align machin translat task subsequ attent mechan ha becom increasingli common ingredi neural ture ha appli variou task imag caption http elsevi right reserv author address nnniuzi niu gqzhong zhong yu neurocomput content list avail sciencedirect neurocomput journal homepag gener text classiﬁc machin translat action recognit analysi speech recognit recommend graph fig show overview sever typic attent method describ detail section addit provid perform improv tion mechan also use tool explain sibl neural architectur behavior recent year neural network ha achiev great success ﬁnancial materi rolog medic autonom drive interact behavior action si industri emot detect lack interpret face practic ethic issu interpret deep learn ha problem far although whether attent mechan use reliabl method explain deep network still controversi issu provid intuit explan certain extent instanc fig show exampl aliz attent weight thi survey structur follow section introduc model propos deﬁn gener attent model section describ classiﬁc attent model section summar network architectur conjunct attent mechan section elabor use tion variou comput vision cv natur languag ing nlp task section discuss interpret attent bring neural network section dissect open challeng current trend innov way attent mechan section conclud thi paper attent mechan thi section ﬁrst describ machin tion architectur use attent introduc take exampl deﬁn gener attent model exampl attent model rnnsearch bahdanau et al propos attent model rnnsearch ﬁrst appli attent mechan machin lation task rnnsearch consist bidirect recurr ral network birnn encod decod emul search sourc sentenc decod translat illustr fig encod calcul annot htþ den state birnn base input sequenc xtþ htþ xtþ one shortcom regular rnn onli util ou context birnn train use avail input inform past futur speciﬁc time frame ical shown fig hidden state ht g ht g extract forward rnn backward rnn respect encod obtain annot one word xi caten forward hidden state hi backward one hi hi hi decod consist attent block recurr ral network rnn function attent block pute context vector c repres context relationship current output symbol term entir input sequenc time step context vector ct pute weight sum annot hj ct x atjhj attent weight atj annot hj comput etj hjþ atj expðetjþ pt expðetkþ learnabl function reﬂect import annot hj next hidden state st accord state rnn output probabl symbol yt current step xþ rnnðctþ thi way inform sourc sentenc tribut entir sequenc instead encod tion vector encod fig sever typic approach attent mechan niu zhong yu neurocomput decod select retriev time step thi tion enabl neural network focu relev element input irrelev part uniﬁ attent model appli attent mechan machin tion task attent model variant use variou applic domain evolv rapidli gener implement cess attent mechan divid two step one comput attent distribut input inform comput context vector accord attent distribut fig show uniﬁ attent model deﬁn compris core part share attent model found survey literatur comput attent distribut neural network ﬁrst encod sourc data featur k call key k express variou represent accord speciﬁc task neural architectur instanc k may featur tain area imag word embed document den state rnn happen annot rnnsearch addit usual necessari introduc represent vector q queri like ou hidden state output rnnsearch case q also form matrix two vector ing speciﬁc task neural network comput correl queri key score function f also call energi function compat function obtain energi score e reﬂect import queri respect key decid next output e fðq kþ score function f crucial part attent model becaus deﬁn key queri match combin tabl list common score function two commonli use attent mechan addit attent like align model rnnsearch comput less expens multipl attent britz et al made empir comparison two score function experi wmt english german task found parameter addit attent nism slightli consist outperform multipl one moreov vaswani et al propos variant plic attent ad scale factor ﬃ dk p dk dimens key small valu dk two anism perform similarli addit attent outperform plic attent without scale larger valu dk also luong et al present gener attent concat attent attent gener attent extend cept multipl attent introduc learnabl matrix paramet w appli key queri ferent represent concat attent aim deriv joint represent key queri instead compar similar addit attent except comput q k arat attent align score sole comput target hidden state word energi score onli relat q convers fig heatmap star yelp review heavier color indic higher attent weight fig illustr singl step decod neural machin translat fig architectur uniﬁ attent model niu zhong yu neurocomput comput onli base k without need grave et al present model compar ilar k q reli cosin similar energi score e map attent weight attent distribut function g gðeþ distribut function g correspond softmax rnnsearch normal energi score probabl distribut addit softmax research tri distribut function limit softmax function result probabl distribut alway ha full port softmaxðzþ everi term thi disadvantag applic spars probabl distribut desir case propos sparsemax may assign exactli zero probabl output variabl besid use anoth distribut function logist sigmoid scale energi score also compar sigmoid softmax experi result show sigmoid function perform better wors differ task neural network comput context vector often necessari introduc new data featur represent v call valu element v correspond one onli one element mani architectur two represent input data like annot rnnsearch base ou work daniluk et al hypothes load use represent make difﬁcult train model propos modiﬁc attent nism separ function explicitli use ent represent input comput attent distribut contextu inform word v k differ represent data valu pair attent mechan particular q k v three differ represent data mechan onc attent weight valu comput context vector c comput c fvigþ function return singl vector given set valu correspond weight common implement function perform weight sum v zi aivi c x n zi zi weight represent element valu n dimens besid anoth way implement function elabor section either way context vector determin primarili due higher attent weight associ valu abov descript common architectur attent model quot vaswani et al tion mechan describ map queri set pair output queri key valu output vector output comput weight sum valu weight assign valu pute compat function queri spond addit would like discuss perform indic evalu attent mechan gener attent nism deep learn attach neural network model enhanc abil process inform therefor hard evalu perform attent mechan without deep learn model common approach ablat studi mean analyz perform gap model attent mechan besid attent mechan evalu visual attent weight shown fig thi manner quantiﬁ taxonomi attent previou section summar gener attent model explain step attent mechan tation detail method improv inform process abil neural network attent mechan appli model variou ﬁeld deep learn although cipl attent model research made modiﬁc improv attent mechan order better adapt speciﬁc task categor tion mechan accord four criteria shown tabl thi section elabor differ type attent anism criterion review sever semin paper addit would like emphas attent nism differ criteria mutual exclus may combin multipl criteria attent model see section tabl soft attent attent propos bahdanau et al mention abov belong soft determinist attent use weight averag key build context vector soft attent attent modul differenti respect input whole system still train standard method correspondingli hard stochast attent wa propos xu et al context vector comput stochast sampl key thi way function eq implement follow one multinoulliðfaigþ tabl summari score function k element k v b w learnabl paramet dk dimens input vector act nonlinear activ function tanh relu name equat ref addit f ðq kþ þ þ bþ multipl f ðq kþ qtk scale multipl f ðq kþ qtk ﬃﬃﬃﬃ dk p gener f ðq kþ qtwk concat f ðq kþ vtact þ b ð þ f ðq kþ f ðqþ similar f ðq kþ jq j jk j jj tabl four criteria categor attent mechan type attent within criterion criterion type soft attent form input featur input represent distinct self hierarch output represent niu zhong yu neurocomput c x n aivi compar soft attent model hard attent model comput less expens becaus doe need late attent weight element time howev ing hard decis posit input featur make modul difﬁcult optim whole system train maxim approxim ation lower bound equival reinforc thi basi luong et al present global attent local attent mechan machin translat global attent similar soft attent local attent view interest blend hard soft tion onli subset sourc word consid time thi approach comput less expens global attent soft attent time unlik hard attent thi approach differenti almost everywher make easier implement train form input featur attent mechan divid accord whether input featur sequenc item attent requir input either explicit item addit preprocess step ad erat sequenc item sourc data instanc item singl word embed rnnsearch singl featur map senet attent model encod encod item separ code assign differ weight dure decod contrari attent aim task difﬁcult obtain distinct input item gener attent mechan use visual task exampl decod process crop input imag step transform relat region canon expect pose simplifi infer subsequ layer anoth differ way calcul combin attent mechan soft attent calcul weight item make linear nation soft attent accept entir featur map input gener transform version attent modul instead linear combin item hard attent stochast pick one item base probabl hard tion stochast pick input locat pick calcul attent modul input represent two featur input represent attent model mention abov model includ singl input correspond output sequenc key queri belong two independ sequenc thi kind attent call distinct attent addit tion mechan ha mani differ form input represent fig lu et al present attent model visual question answer task jointli son imag question attent form parallelli altern former gener imag question attent simultan latter sequenti altern gener imag question attent thermor attent comput attent input use embed input queri attent evalu element input affect element input ha use success varieti task includ sentiment classiﬁc text match name entiti recognit entiti disambigu tion caus analysi sentiment classiﬁc wang et al present self inner attent comput attent onli base input sequenc word queri key valu differ represent input sequenc model ha proven effect sever author exploit differ fashion applic transform ﬁrst sequenc transduct model base entir without rnn applic thi model variou ﬁeld describ section attent weight comput onli origin input sequenc also differ abstract level refer hierarch attent yang et al propos hierarch attent network ham document classiﬁc ha two level attent mechan hierarch attent allow ham aggreg import word sentenc aggreg import sentenc document furthermor hierarchi extend wu et al ad user level top appli attent also document level contrari abov model weight learn lower level higher level model propos zhao zhang also use hierarch attent attent weight learn higher level lower level except natur languag process hierarch attent also use comput vision instanc xiao et al propos attent method level attent ﬁrst imag siﬁcat method doe use addit part inform onli reli model gener attent weight output represent thi part discuss variou type output represent attent model among common one attent refer singl featur represent time step speciﬁc energi score repres one onli tabl exampl combin differ categori refer applic categori soft attent form input featur input represent output represent machin translat soft distinct imag classiﬁc hard distinct machin translat soft distinct visual question answer soft hierarch languag understand soft distinct imag classiﬁc soft distinct document classiﬁc soft hierarch niu zhong yu neurocomput one vector time step howev case use singl featur represent may abl complet downstream task well next describ two attent model illustr fig mani applic convolut neural network ha prove multipl channel express input data comprehens singl channel also attent model case use singl attent distribut input sequenc may sufﬁc downstream task vaswani et al propos attent linearli project input sequenc q k v multipl subspac base learnabl paramet appli scale attent sentat subspac ﬁnalli concaten output thi way allow model jointli attend inform differ represent subspac differ posit li et al propos three disagr regular augment attent model subspac attend posit output represent respect thi approach encourag divers among attent head ferent head learn distinct featur effect idat translat task anoth approach propos shen et al comput score vector key replac weight score vector matrix thi way neural network calcul multipl attent distribut data thi especi use natur languag cess word embed suffer polysemi lem furthermor lin et al du et al ad frobeniu penalti enforc distinct model relev success appli variou task includ fig exampl hierarch attent figur fig illustr represent niu zhong yu neurocomput author proﬁl sentiment classiﬁc textual entail distantli supervis relat extract network architectur attent thi section describ three neural network architectur use conjunct attent ﬁrst elabor framework use attent model describ special network architectur attent model memori network equip extern memori final introduc special neural network structur combin attent mechan rnn use process captur depend shown fig gener framework base neural network aim handl map highli structur input output ﬁrst brieﬂi describ rnn framework propos cho et al sutskev et al machin translat task thi architectur consist two rnn act encod decod pair encod map sourc sequenc x xtxþ vector common approach use rnn ht f xt ð þ c q htx f g ð þ ht hidden state time step c context vector gener sequenc hidden state f q activ function exampl f may simpl logist sigmoid function complex gru lstm q may hidden state htx last time step decod train gener target sequenc ytyþ predict next symbol yt given hidden state ht howev unlik encod yt ht also condit context vector henc hidden state decod time step comput ht cþ condit probabl model xþ ht cþ encod decod propos model jointli train maxim condit probabl target sequenc given sourc sequenc potenti issu framework neural network need abl compress necessari inform sourc data vector thi may make difﬁcult neural network cope long sentenc especi longer tenc train corpu thi problem allevi ad attent mechan framework ct att htx f g ð þ xþ ht ctþ att attent method mention section ct context vector gener attent mechan decod time introduct attent mechan ensur contribut element sourc sequenc differ decod differ target element shown fig sinc thi framework doe limit length input output sequenc ha wide rang tion imag video caption gener log system visual question answer speech recognit moreov encod decod also construct use architectur necessarili onli rnn although attent model regard gener idea doe depend speciﬁc framework attent model current accompani decod framework memori network addit framework previou section attent mechan also use conjunct memori network inspir human brain mechan handl inform overload memori network introduc extra extern memori neural network concret ori network save tion auxiliari memori introduc extern auxiliari memori unit read need onli effect increas network capac also improv network comput efﬁcienc compar gener attent mechan memori network replac key term auxiliari memori match content attent mechan exampl differenti memori network propos read inform fig illustr framework without attent mechan niu zhong yu neurocomput nal inform multipl time core idea convert input set two extern memori unit one address anoth output shown fig memori network regard form attent pair attent mechan differ usual attent instead model attent onli singl sequenc use two nal memori unit model larg databas sequenc word regard attent mechan face separ storag inform calcul network capac greatli increas small increas network paramet network without rnn mention abov encod decod framework implement multipl way shown fig architectur base rnn typic factor comput along symbol posit input output sequenc thi inher sequenti natur result comput inefﬁci process parallel hand captur cie essenti becaus attent mechan decod framework need obtain contextu inform ever comput complex establish depend sequenc length n rnn oðnþ thi part describ implement framework combin attent mechan card rnn gehr et al propos architectur reli entir convolut neural network combin attent mechan contrast fact rent network maintain hidden state entir past tional network reli comput previou time step allow parallel element sequenc thi architectur enabl network captur distanc depend stack multipl layer cnn comput complex becom cnn convolut kernel size moreov thi convolut method discov composit structur sequenc easili becaus hierarch represent vaswani et al propos anoth network architectur transform reli entir mechan comput represent input output without ing rnn cnn transform compos two nent network ffn layer head attent layer ffn fulli connect forward network appli posit separ ident thi method ensur posit inform symbol input sequenc dure oper attent allow model focu inform differ represent subspac differ posit stack multipl layer like multipl nel cnn addit paralleliz iti establish depend attent mechan applic previou section shown attent model wide appli variou task introduc speciﬁc applic attent model cv nlp intend provid comprehens review neural architectur use attent mechan focu eral gener attent method attent model fig illustr framework use attent mechan fig singl layer version memori network question input output correspond queri key valu uniﬁ attent model respect niu zhong yu neurocomput applic comput vision thi part describ attent mechan cv duce sever typic paper differ aspect neural architectur spatial attent allow neural network learn posit focus shown fig thi tion mechan spatial inform origin pictur transform anoth space key inform retain mnih et al present spatial attent model formul singl rnn take glimps window input use intern state network select next locat focu well gener control signal dynam environ jaderberg et al introduc tiabl spatial transform network stn ﬁnd area need paid attent featur map format crop translat rotat scale skew unlik pool layer spatial transform modul dynam mechan activ spatial transform imag featur map produc appropri transform input sampl channel attent allow neural network learn focus shown fig hu et al propos se network adapt recalibr featur respons explicitli model denci channel modul use global featur comput attent li et al propos sknet improv cienci effect object recognit adapt kernel select channel attent manner besid stollenga et al propos channel hard attent mechan improv classiﬁc perform allow network iter focu attent ﬁlter captur depend central import deep neural network beneﬁci visual understand problem appli mechan puter vision task solv thi problem call attent shown fig propos modul got attent mask calcul correl matrix spatial point featur map attent guid dens contextu inform aggreg howev thi method also ha follow problem onli posit attent modul involv commonli use channel attent fig illustr three structur use captur long distanc depend fig exampl spatial attent niu zhong yu neurocomput mechan input featur map veri larg problem low efﬁcienc although method solv thi problem scale lose inform best way deal thi problem address lem research improv method bine channel attent propos mix attent differ previou studi ccnet use attent gener huge attent map record relationship featur map shown fig attent modul lect contextu inform horizont vertic direct enhanc repres capabl moreov recurr attent allow captur dens rang contextu inform pixel less comput cost less memori cost applic natur languag process thi part ﬁrst introduc attent method use differ task nlp describ common train word represent implement attent mechan nlp task neural machin translat use neural network translat text one languag anoth process translat align sentenc differ languag crucial lem especi longer sentenc bahdanau et al introduc attent mechan neural network improv ral machin translat select focus part sourc sentenc dure translat afterward sever work propos improv local attent supervis attent hierarch attent self attent use differ attent architectur improv align sentenc enhanc perform translat text classiﬁc aim assign label text ha broad applic includ topic label sentiment classiﬁc spam detect classiﬁc task self attent mainli use construct effect document represent thi basi work combin mechan attent method hierarch attent besid task also appli attent model architectur see section transform memori network text match also core research problem nlp mation retriev includ question answer document search entail classiﬁc paraphras identiﬁc recommend review mani research come new approach conjunct attent sion memori network inner attent structur attent attent word represent key compon mani neural languag understand model howev previou studi onli determin one embed word could achiev contextu word embed peter et al introduc gener approach represent solv thi problem inspir transform model research propos tional encod represent transform bert gener gpt method accord encod decod part bert bidirect languag model ha follow two task mask languag model mlm simpli mask percentag input token random predict mask token next tenc predict use linear binari classiﬁ determin whether two sentenc connect gpt model train method roughli use previou word predict next word experi show larg improv appli broad rang nlp task attent interpret recent year artiﬁci intellig ha develop rapidli especi ﬁeld deep learn fig illustr channel attent fig illustr attent niu zhong yu neurocomput ever interpret major concern mani current deep learn model deep learn model becom increasingli plex critic learn function data ensur understand whi particular decis occur attent layer neural network model provid way reason model behind predict thi often critic opaqu shown fig bahdanau et al visual attent weight annot clearli show align word gener translat french sourc sentenc english fig b show attend region correspond underlin word neural imag caption gener process see model learn align correspond veri strongli human intuit furthermor fig c show exampl visual encod layer transform model propos vaswani et al differ color repres differ head moreov voita et al evalu contribut made individu attent head transform model tion discov differ head show differ level import prune oper besid chan et al observ attent mechan could identifi start posit audio sequenc ﬁrst acter correctli howev recent studi suggest attent could consid reliabl mean interpret deep neural network jain wallac perform extens experi across varieti nlp task prove attent wa consist explain metric ﬁnd veri often construct adversari attent distribut mean differ attent weight distribut yield equival predict serrano smith appli differ analysi base intermedi represent found attent weight onli noisi predictor intermedi ponent import contrari wiegreff pinter made rebutt work four altern test thu whether attent use mean explain neural work still open topic challeng prospect attent model becom ubiquit deep neural work sinc bahdanau et al use attent mechan fig illustr attent fig exampl visual weight attent mechan niu zhong yu neurocomput align machin translat task variou attent mechan variant emerg endlessli transform model onli use mechan propos vaswani et al also import mileston attent mechan variant ful appli variou ﬁeld investig attent model ﬁnd attent mechan innov eral aspect multipl implement score tion distribut function combin valu attent weight network architectur still much room improv design attent mechan summar promis tion follow present model repres queri key independ howev recent studi achiev good perform combin queri key whether queri key necessari exist independ still open question attent distribut function ha great inﬂuenc comput complex whole attent model recent studi show complex attent comput may reduc improv tion distribut function attent techniqu develop one area appli area also interest direct exampl attent method nlp appli cv improv perform also reduc efﬁcienc like network combin adapt mechan attent nism may automat achiev effect hierarch tion without manual design structur layer explor effect perform indic evalu attent mechan also interest topic sen et al design set evalu method quantifi similar human neural model use novel similar metric thi also open direct futur research conclus thi paper illustr uniﬁ attent model work describ detail classiﬁc attent nism summar network architectur use tion attent mechan introduc typic applic attent mechan use comput vision natur languag process final discuss pretabl attent mechan provid model erat process challeng prospect current attent model conclus attent mechan ha success use variou ﬁeld deep learn applic still mani interest question explor credit authorship contribut statement zhaoyang niu investig methodolog formal analysi write review edit guoqiang zhong conceptu methodolog investig formal analysi write review edit project administr supervis hui yu izat investig methodolog write review edit supervis declar compet interest author declar known compet cial interest person relationship could appear inﬂuenc work report thi paper acknowledg thi work wa support nation key research develop program china grant joint fund equip ministri educ china grant natur scienc foundat shandong vinc grant scienc ogi program qingdao grant refer rensink dynam represent scene visual cogn corbetta shulman control attent brain nat rev neurosci tsotso culhan wai lai davi nuﬂo model visual attent via select tune artif intel hochreit schmidhub long memori neural comput cho van merrienbo gülçehr bahdanau bougar schwenk bengio learn phrase represent use rnn decod statist machin translat emnlp acl pp itti koch niebur model visual attent rapid scene analysi ieee tran pattern anal mach intel mnih heess grave kavukcuoglu recurr model visual attent nip pp bahdanau cho bengio neural machin translat jointli learn align translat iclr xu ba kiro cho courvil salakhutdinov zemel bengio show attend tell neural imag caption gener visual attent icml volum jmlr workshop confer proceed pp lu xiong parikh socher know look adapt attent via visual sentinel imag caption cvpr ieee comput societi pp liu guo bidirect lstm attent mechan convolut layer text classiﬁc neurocomput li yang xu wang lin improv user attribut classiﬁc text social network attent cogn comput sutskev vinyal le sequenc sequenc learn neural network nip pp luong pham man effect approach neural machin translat emnlp associ comput linguist pp britz goldi luong le massiv explor neural machin translat architectur corr vaswani shazeer parmar uszkoreit jone gomez kaiser polosukhin attent need nip pp song lan xing zeng liu attent model human action recognit skeleton data aaai aaai press pp tian hu jiang wu dens connect attent pyramid residu network human pose estim neurocomput zhao qi li dong yu lstm diagnosi neurodegen diseas use gait data yu dong ed ninth intern confer graphic imag process icgip vol zhang xue lan zeng gao zheng ad attent neuron recurr neural network eccv volum lectur note comput scienc springer pp song yao ling mei boost imag sentiment analysi visual attent neurocomput yan hu mao ye yu deep learn method review neurocomput chorowski bahdanau cho bengio continu speech recognit use recurr nn ﬁrst result corr niu zhong yu neurocomput chan jaitli le vinyal listen attend spell neural network larg vocabulari convers speech recognit icassp ieee pp sperber niehu neubig stüker waibel acoust model interspeech isca pp wang hu cao huang lian liu transact context embed recommend aaai aaai press pp ying zhuang zhang liu xu xie xiong wu sequenti recommend system base hierarch attent network ijcai pp velickov cucurul casanova romero liò bengio graph attent network iclr poster xu wu wang feng sheinin graph sequenc learn neural network corr lin feng santo yu xiang zhou bengio structur sentenc embed iclr poster openreview net zhang zhong dong wang wang stock market predict base gener adversari network bie sun yu ed intern confer identiﬁc inform knowledg internet thing iiki beij china octob volum procedia comput scienc elsevi pp ieracitano paviglian campolo hussain pasero morabito novel automat classiﬁc system base hybrid unsupervis supervis machin learn electrospun nanoﬁb ieee caa autom sinica fan zhong li featur fusion network mesoscal eddi detect yang pasupa leung kwok chan king ed neural inform process intern confer iconip bangkok thailand novemb proceed part volum lectur note comput scienc springer pp yu garrod jack schyn framework automat perceptu valid facial express gener multimedia tool appl li fan zhong bednet edg detect network ocean front detect yang pasupa leung kwok chan king ed neural inform process intern confer iconip bangkok thailand novemb proceed part iv volum commun comput inform scienc springer pp fan zhong wei li ednet mesoscal eddi detect network data intern joint confer neural network ijcnn glasgow unit kingdom juli ieee pp liu xia yu dong jian pham region base parallel hierarchi convolut neural network automat facial nerv paralysi evalu ieee tran neural syst rehab eng yue wang liu tian lauria liu optim weight collabor ﬁltere approach predict baselin data friedreich ataxia patient neurocomput zeng li wang liu liu alsaadi liu imag segment quantit analysi gold immunochromatograph strip neurocomput liu wang liu zeng bell novel particl swarm optim approach patient cluster emerg depart ieee tran evol comput zeng wang zhang kim li liu improv particl ﬁlter novel hybrid propos distribut quantit analysi gold immunochromatograph strip ieee tran nanotechnol ming meng fan yu deep learn monocular depth estim review neurocomput xia yu wang accur robust eye center local via fulli convolut network ieee caa autom sinica guo xia wang yu chen facial affect comput mobil devic sensor wang dong li dong yu cascad face frontal dynam facial express analysi cogn comput zhang yu huang howel steven scene percept guid crowd anomali detect neurocomput roy banerje hussain poria discrimin dictionari design action classiﬁc still imag video cogn comput liu xia shi yu li lin deep learn sheet metal bend novel deep neural network autom sinica luqu snchez hupont tabik herrera revisit crowd behaviour analysi deep learn taxonomi anomali detect crowd emot dataset opportun prospect inf fusion zhang yang zhang li yu crowd emot evalu base fuzzi infer arous valenc neurocomput guidotti monreal ruggieri turini giannotti pedreschi survey method explain black box model acm comput surv jain wallac attent explan associ comput linguist pp serrano smith attent interpret acl associ comput linguist pp li yatskar yin hsieh chang doe bert vision look acl associ comput linguist pp letart paradi giguèr laviolett import sentiment analysi blackboxnlp emnlp associ comput linguist pp vashishth upadhyay tomar faruqui attent interpret across nlp task corr wiegreff pinter attent explan associ comput linguist pp schuster paliw bidirect recurr neural network ieee tran signal process sordoni bachman bengio iter altern neural attent machin read corr grave wayn danihelka neural ture machin corr zhao zhang neural machin translat aaai aaai press pp galassi lippi torroni attent pleas critic review neural attent model natur languag process corr yang yang dyer smola hovi hierarch attent network document classiﬁc associ comput linguist pp martin astudillo softmax sparsemax spars model attent classiﬁc icml volum jmlr workshop confer proceed pp kim denton hoang rush structur attent network arxiv comput languag miller fisch dodg karimi bord weston memori network directli read document emnlp associ comput linguist pp ba hinton mnih leibo ionescu use fast weight attend recent past nip pp gülçehr chandar cho bengio dynam neural ture machin soft hard address scheme corr daniluk rocktäschel welbl riedel frustratingli short attent span neural languag model iclr poster william simpl statist algorithm connectionist reinforc learn mach learn hu shen albani sun wu network ieee tran pattern anal mach intel ba mnih kavukcuoglu multipl object recognit visual attent bengio lecun ed intern confer learn represent iclr san diego ca usa may confer track proceed jaderberg simonyan zisserman et spatial transform network advanc neural inform process system pp chaudhari polatkan ramanath mithal attent survey attent model corr lu yang batra parikh hierarch visual question answer nip pp fan feng zhao attent network sentiment classiﬁc emnlp associ comput linguist pp wang pan dahlmeier xiao coupl attent aspect opinion term aaai aaai press pp tay luu hui hermitian network text match asymmetr domain ijcai pp zhang fu liu huang adapt network name entiti recognit tweet aaai aaai press pp nie cao wang lin pan mention entiti descript attent entiti disambigu aaai aaai press pp li song feng wang zhang neural network model emot caus analysi emot context awar emnlp associ comput linguist pp tay luu hui su attent gate lexicon reader contrast contextu sentiment classiﬁc emnlp associ comput linguist pp wang liu zhao inner attent base recurr neural network answer select acl associ comput linguist wu tian zhao lai liu word attent sequenc sequenc text understand aaai aaai press pp pavlopoulo malakasioti androutsopoulo deeper attent abus user content moder emnlp associ comput linguist pp niu zhong yu neurocomput li wei zhang yang hierarch attent transfer network sentiment classiﬁc aaai aaai press pp wang girshick gupta neural network cvpr ieee comput societi pp wu wu liu huang hierarch user item represent attent recommend associ comput linguist pp xiao xu yang zhang peng zhang applic attent model deep convolut neural network imag classiﬁc cvpr ieee comput societi pp li tu yang lyu zhang attent disagr regular emnlp associ comput linguist pp shen zhou long jiang pan zhang disan direct attent network languag understand aaai aaai press pp du han way wan structur distantli supervis relat extract emnlp associ comput linguist pp venugopalan rohrbach donahu mooney darrel saenko sequenc sequenc video text ieee intern confer comput vision iccv santiago chile decemb ieee comput societi pp ding chen zhao chen han liu neural imag caption gener weight train refer cogn comput zhang yang transfer hierarch attent network gener dialog system int autom comput prabhavalkar rao sainath li johnson jaitli comparison model speech recognit lacerda ed interspeech annual confer intern speech commun associ stockholm sweden august isca pp wang zhang zong learn sentenc represent guidanc human attent ijcai pp sukhbaatar szlam weston fergu memori network nip pp weston chopra bord memori network iclr kumar irsoy ondruska iyyer bradburi gulrajani zhong paulu socher ask anyth dynam memori network natur languag process icml volum jmlr workshop confer proceed pp henaff weston szlam bord lecun track world state recurr entiti network intern confer learn represent iclr toulon franc april confer track proceed gehr auli grangier yarat dauphin convolut sequenc sequenc learn icml volum proceed machin learn research pmlr pp li wang hu yang select kernel network ieee confer comput vision pattern recognit cvpr long beach ca usa june comput vision ieee pp stollenga masci gomez schmidhub deep network intern select attent feedback connect ghahramani well cort lawrenc weinberg ed advanc neural inform process system annual confer neural inform process system decemb montreal quebec canada pp fu liu tian li bao fang lu dual attent network scene segment ieee confer comput vision pattern recognit cvpr long beach ca usa june comput vision ieee pp yuan wang ocnet object context network scene pars corr zhao zhang liu shi loy lin jia psanet spatial attent network scene pars ferrari hebert sminchisescu weiss ed comput vision eccv european confer munich germani septemb proceed part ix volum lectur note comput scienc springer pp cao xu lin wei hu gcnet network meet excit network beyond intern confer comput vision workshop iccv workshop seoul korea south octob ieee pp wang jiang qian yang li zhang wang tang residu attent network imag classiﬁc ieee confer comput vision pattern recognit cvpr honolulu hi usa juli ieee comput societi pp yue sun yuan zhou ding xu compact gener network bengio wallach larochel grauman bianchi garnett ed advanc neural inform process system annual confer neural inform process system neurip decemb montréal canada pp woo park lee kweon cbam convolut block attent modul ferrari hebert sminchisescu weiss ed comput vision eccv european confer munich germani septemb proceed part vii volum lectur note comput scienc springer pp huang wang huang huang wei liu ccnet attent semant segment intern confer comput vision iccv seoul korea south octob ieee pp mi wang ittycheriah supervis attent neural machin translat su carrera duh ed proceed confer empir method natur languag process emnlp austin texa usa novemb associ comput linguist pp liu utiyama finch sumita neural machin translat supervis attent calzolari matsumoto prasad ed cole intern confer comput linguist proceed confer technic paper decemb osaka japan acl pp yang tu wong meng chao zhang model local network riloff chiang hockenmai tsujii ed proceed confer empir method natur languag process brussel belgium octob associ comput linguist pp wang man baselin bigram simpl good sentiment topic classiﬁc annual meet associ comput linguist proceed confer juli jeju island korea volum short paper associ comput linguist pp maa dali pham huang ng pott learn word vector sentiment analysi lin matsumoto mihalcea ed annual meet associ comput linguist human languag technolog proceed confer june portland oregon usa associ comput linguist pp pang lee opinion mine sentiment analysi found trend inf retr sahami dumai heckerman horvitz bayesian approach ﬁltere junk learn text categor paper workshop vol madison wisconsin pp song wang jiang liu rao attent encod network target sentiment classiﬁc corr ambartsoumian popowich better build block sentiment analysi neural network classiﬁ balahur mohammad host klinger ed proceed workshop comput approach subject sentiment social media analysi wassa emnlp brussel belgium octob associ comput linguist pp tang qin liu aspect level sentiment classiﬁc deep memori network su carrera duh ed proceed confer empir method natur languag process emnlp austin texa usa novemb associ comput linguist pp zhu qian enhanc aspect level sentiment classiﬁc auxiliari memori bender derczynski isabel ed proceed intern confer comput linguist cole santa fe new mexico usa august associ comput linguist pp cui chen wei wang liu hu neural network read comprehens barzilay kan ed proceed annual meet associ comput linguist acl vancouv canada juli volum long paper associ comput linguist pp mikolov chen corrado dean efﬁcient estim word represent vector space bengio lecun ed intern confer learn represent iclr scottsdal arizona usa may workshop track proceed pennington socher man glove global vector word represent moschitti pang daeleman ed proceed confer empir method natur languag process emnlp octob doha qatar meet sigdat special interest group acl acl pp peter neumann iyyer gardner clark lee zettlemoy deep contextu word represent walker ji stent ed proceed confer north american chapter associ comput linguist human languag technolog new orlean louisiana usa june volum long paper associ comput linguist pp devlin chang lee toutanova bert deep bidirect transform languag understand burstein doran solorio ed proceed confer north american chapter associ comput linguist human languag technolog minneapoli mn usa june niu zhong yu neurocomput volum long short paper associ comput linguist pp radford narasimhan saliman sutskev improv languag understand gener radford wu child luan amodei sutskev languag model unsupervis multitask learner openai blog brown mann ryder subbiah kaplan dhariw neelakantan shyam sastri askel agarw krueger henighan child ramesh ziegler wu winter hess chen sigler litwin gray chess clark berner mccandlish radford sutskev amodei languag model shot learner corr liu wang zeng yuan alsaadi liu novel randomis particl swarm optim int mach learn cybern zeng wang liu zhang hone liu dynam base switch particl swarm optim algorithm ieee tran cybern liu wang yuan zeng hone liu novel base adapt weight particl swarm optim ieee tran cybern rahman wang liu ye zakarya liu markovian jump particl swarm optim algorithm ieee tran man cybern syst luo yuan chen zeng wang particl swarm latent factor analysi ieee tran knowl data eng zeng song li liu alsaadi competit mechan integr whale optim algorithm differenti evolut neurocomput li monro jurafski understand neural network represent erasur corr voita talbot moiseev sennrich titov analyz attent special head heavi lift rest prune korhonen traum màrquez ed proceed confer associ comput linguist acl florenc itali juli volum long paper associ comput linguist pp dai yang yang carbonel le salakhutdinov xl attent languag model beyond context korhonen traum màrquez ed proceed confer associ comput linguist acl florenc itali juli august volum long paper associ comput linguist pp dehghani gouw vinyal uszkoreit kaiser univers transform intern confer learn represent iclr new orlean la usa may guo qiu liu shao xue zhang burstein doran solorio ed proceed confer north american chapter associ comput linguist human languag technolog minneapoli mn usa june volum long short paper associ comput linguist pp zhu cheng zhang lin dai empir studi spatial attent mechan deep network intern confer comput vision iccv seoul korea south octob ieee pp tay bahri metzler juan zhao zheng synthes rethink transform model corr tsai bai yamada morenc salakhutdinov transform dissect uniﬁ understand transform attent via len kernel inui jiang ng wan ed proceed confer empir method natur languag process intern joint confer natur languag process ijcnlp hong kong china novemb associ comput linguist pp katharopoulo vya pappa fleuret transform rnn fast autoregress transform linear attent corr sen hartvigsen yin kong rundenstein human attent map text classiﬁc human neural network focu word jurafski chai schluter tetreault ed proceed annual meet associ comput linguist acl onlin juli associ comput linguist pp zhaoyang niu receiv hi degre school data scienc softwar engin qingdao univers qingdao china studi hi bsc degre comput technolog ocean univers china qingdao china hi research interest includ comput vision deep learn attent mechan guoqiang zhong receiv hi degre matic hebei normal univers shijiazhuang china hi degre oper research cybernet beij univers technolog bjut beij china hi degre pattern recognit intellig system institut autom chines academi scienc casia beij china respect octob juli wa doctor fellow synchromedia laboratori multimedia commun telepres univers quebec montreal canada march decemb wa associ professor depart comput scienc technolog ocean univers china qingdao china sinc januari ha full professor depart comput scienc technolog ocean univers china ha publish book book chapter technic paper area artiﬁci intellig pattern recognit machin learn comput vision hi research interest includ pattern recognit machin learn comput vision ha serv mani intern confer top journal ieee tnnl ieee tkde ieee tcsvt pattern recognit system ing acm tkdd aaai aistat icpr ijcnn iconip icdar ha award outstand review sever journal pattern recognit system neurocomput cognit system research ha best paper award apnn young research award member acm ieee iapr apnn ccf profession committe member truste shandong associ artiﬁci intellig hui yu professor univers portsmouth uk yu receiv phd brunel univers use work univers glasgow befor move univers portsmouth hi research interest includ method practic develop vision machin learn ai applic interact virtual augment realiti robot facial express serv associ editor ieee transact system neurocomput journal niu zhong yu neurocomput