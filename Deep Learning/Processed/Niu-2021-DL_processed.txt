review attention mechanism deep learning zhaoyang niu guoqiang zhong hui yu b department computer science technology ocean university china qingdao 266100 china b school creative technology university portsmouth portsmouth uk r c l e n f article history received 7 february 2021 revised 23 march 2021 accepted 29 march 2021 available online 1 april 2021 communicated zidong wang keywords attention mechanism deep learning recurrent neural network rnn convolutional neural network cnn uniﬁed attention model computer vision application natural language processing application b r c attention ha arguably become one important concept deep learning ﬁeld inspired biological system human tend focus distinctive part processing large amount information development deep neural network attention mechanism ha widely used diverse application domain paper aim give overview art attention model proposed recent year toward better general understanding attention anisms deﬁne uniﬁed model suitable attention structure step attention mechanism implemented model described detail furthermore classify existing attention model according four criterion softness attention form input feature input representation output representation besides summarize network architecture used conjunction attention mechanism describe some typical application attention mechanism finally discus interpretability attention brings deep learning present potential future trend 2021 elsevier right reserved introduction attention complex cognitive function indispensable human one important property perception human not tend process whole information entirety instead human tend selectively concentrate part information needed ignore perceivable information time instance human usually see scene ning end visually perceiving thing instead observe pay attention speciﬁc part needed human ﬁnd scene often ha something want observe certain part learn focus part similar scene appear focus attention useful part mean human quickly select mation massive information using limited processing resource attention mechanism greatly improves ciency accuracy perceptual information processing attention mechanism human divided two category according generation manner 3 ﬁrst gory unconscious attention called based attention driven external stimulus example people likely hear loud voice conversation similar gating mechanism deep learning pass appropriate value larger value next step second category conscious tion called focused attention focused attention refers tion ha predetermined purpose relies speciﬁc task enables human focus attention certain object sciously actively attention mechanism deep learning designed according speciﬁc task focused attention attention mechanism introduced paper usually refers focused attention except special statement mentioned attention mechanism used resource allocation scheme main mean solve problem information overload case limited puting power process important information ited computing resource hence some researcher bring attention computer vision area 6 proposed visual attention model extract local visual feature get some potential salient region neural network area 7 used attention mechanism recurrent neural network model classify image bahdanau et al 8 used attention mechanism simultaneously perform translation alignment machine translation task subsequently attention mechanism ha become increasingly common ingredient neural tures ha applied various task image caption 2021 elsevier right reserved author address nnniuzy niu gqzhong zhong yu neurocomputing 452 2021 content list available sciencedirect neurocomputing journal homepage generation text classiﬁcation machine translation action recognition analysis speech recognition recommendation graph fig 1 show overview several typical attention method described detail section addition providing performance improvement tion mechanism also used tool explain sible neural architecture behavior recent year neural network ha achieved great success ﬁnancial 31 material 32 rology medical autonomous driving 42 interaction behavior action si industry 48 emotion detection lack interpretability facing practical ethical issue 51 interpretability deep learning ha problem far although whether attention mechanism used reliable method explain deep network still controversial issue provide intuitive explanation certain extent instance fig 2 show example alization attention weight survey structured follows section 2 introduce model proposed 8 deﬁne general attention model section 3 describes classiﬁcation attention model section 4 summarizes network architecture conjunction attention mechanism section 5 elaborates us tion various computer vision cv natural language ing nlp task section 6 discus interpretability attention brings neural network section 7 dissect open challenge current trend innovative way attention mechanism section 8 conclude paper attention mechanism section ﬁrst describe machine tion architecture using attention introduced 8 take example deﬁne general attention model example attention model rnnsearch bahdanau et al 8 proposed attention model rnnsearch ﬁrst applied attention mechanism machine lation task rnnsearch consists bidirectional recurrent ral network birnn 58 encoder decoder emulates searching source sentence decoding translation illustrated fig encoder calculates annotation htþ den state birnn based input sequence xtþ htþ xtþ one shortcoming regular rnns only utilize ous context birnn trained using available input information past future speciﬁc time frame ically shown fig 3 hidden state ht g ht g extracted forward rnn backward rnn respectively encoder obtains annotation one word xi catenating forward hidden state hi backward one hi hi hi decoder consists attention block recurrent ral network rnn function attention block pute context vector c represents context relationship current output symbol term entire input sequence time step context vector ct puted weighted sum annotation hj ct x atjhj attention weight atj annotation hj computed etj hjþ atj expðetjþ pt expðetkþ learnable function reﬂects importance annotation hj next hidden state st according state rnn output probable symbol yt current step xþ rnnðctþ way information source sentence tributed entire sequence instead encoding tion vector encoder fig several typical approach attention mechanism niu zhong yu neurocomputing 452 2021 49 decoder selectively retrieve time step tion enables neural network focus relevant element input irrelevant part uniﬁed attention model 8 applied attention mechanism machine tion task attention model variant used various application domain evolved rapidly generally implementation ce attention mechanism divided two step one compute attention distribution input information compute context vector according attention distribution fig 4 show uniﬁed attention model deﬁned comprises core part shared attention model found surveyed literature computing attention distribution neural network ﬁrst encodes source data feature k called key k expressed various representation according speciﬁc task neural architecture instance k may feature tain area image word embeddings document den state rnns happens annotation rnnsearch addition usually necessary introduce representation vector q query like ous hidden state output rnnsearch some case q also form matrix 16 two vector 59 ing speciﬁc task neural network computes correlation query key score function f also called energy function 61 compatibility function 62 obtain energy score e reﬂects importance query respect key deciding next output e fðq kþ score function f crucial part attention model deﬁnes key query matched combined table 1 list some common score function two commonly used attention mechanism additive attention like alignment model rnnsearch 8 computationally le expensive multiplicative attention 14 britz et al 15 made empirical comparison two score function experiment wmt 15 english german task found parameterized additive attention nisms slightly consistently outperformed multiplicative one moreover vaswani et al 16 proposed variant plicative attention adding scaling factor ﬃ dk p dk dimension key small value dk two anisms perform similarly additive attention outperforms plicative attention without scaling larger value dk also luong et al 14 presented general attention concat attention attention general attention extends cept multiplicative attention introducing learnable matrix parameter w applied key query ferent representation concat attention aim derive joint representation key query instead comparing similar additive attention except computing q k arately attention alignment score solely computed target hidden state word energy score only related q conversely fig heatmap 5 star yelp review 30 heavier color indicate higher attention weight fig illustration single step decoding neural machine translation 8 fig architecture uniﬁed attention model niu zhong yu neurocomputing 452 2021 50 63 computed only based k without need graf et al 60 presented model compare ilarity k q relied cosine similarity energy score e mapped attention weight attention distribution function g gðeþ distribution function g corresponds softmax rnnsearch normalizes energy score probability distribution addition softmax some researcher tried distribution function limitation softmax function resulting probability distribution always ha full port softmaxðzþ 0 every term disadvantage application sparse probability distribution desired case 64 proposed sparsemax may assign exactly zero probability some output variable besides 65 used another distribution function logistic sigmoid scaled energy score 0 also compared sigmoid softmax experiment result showed sigmoid function performed better worse different task neural network computes context vector often necessary introduce new data feature representation v called value element v corresponds one only one element many architecture two representation input data like annotation rnnsearch based ous work daniluk et al 69 hypothesized loaded use representation make difﬁcult train model proposed modiﬁcation attention nism separate function explicitly used ent representation input compute attention distribution contextual information word v k different representation data value pair attention mechanism particular q k v three different representation data mechanism 16 attention weight value computed context vector c computed c fvigþ function return single vector given set value corresponding weight common implementation function perform weighted sum v zi aivi c x n zi zi weighted representation element value n dimension besides another way implement function elaborated section either way context vector determined primarily due higher attention weight associated value description common architecture attention model quote vaswani et al 16 tion mechanism described mapping query set pair output query key value output vector output computed weighted sum value weight assigned value puted compatibility function query sponding addition would like discus performance indicator evaluating attention mechanism generally attention nisms deep learning attached neural network model enhance ability process information therefore hard evaluate performance attention mechanism without deep learning model common approach ablation study mean analyze performance gap model attention mechanism besides attention mechanism evaluated visualizing attention weight shown fig 2 manner not quantiﬁed taxonomy attention previous section summarized general attention model explained step attention mechanism tation detail method improve information processing ability neural network attention mechanism applied model various ﬁelds deep learning although ciple attention model researcher made some modiﬁcations improvement attention mechanism order better adapt speciﬁc task categorize tion mechanism according four criterion shown table 2 section elaborate different type attention anisms criterion reviewing several seminal paper addition would like emphasize attention nisms different criterion not mutually exclusive may combination multiple criterion attention model see section 5 table 3 softness attention attention proposed bahdanau et al 8 mentioned belongs soft deterministic attention us weighted average key building context vector soft attention attention module differentiable respect input whole system still trained standard method correspondingly hard stochastic attention wa proposed xu et al 9 context vector computed stochastically sampled key way function eq 8 implemented following one multinoulliðfaigþ table 1 summary score function k element k v b w learnable parameter dk dimension input vector act nonlinear activation function tanh relu name equation ref additive f ðq kþ þ þ bþ 15 multiplicative f ðq kþ qtk 15 scaled multiplicative f ðq kþ qtk ﬃﬃﬃﬃ dk p 16 general f ðq kþ qtwk 14 concat f ðq kþ vtact þ b ð þ 14 f ðq kþ f ðqþ 14 similarity f ðq kþ jq j jk j jj 60 table 2 four criterion categorizing attention mechanism type attention within criterion criterion type softness attention form input feature input representation distinctive self hierarchical output representation niu zhong yu neurocomputing 452 2021 51 c x n aivi compared soft attention model hard attention model computationally le expensive doe not need late attention weight element time however ing hard decision position input feature make module difﬁcult optimize whole system trained maximizing approximate ational lower bound equivalently reinforce 70 basis luong et al 14 presented global attention local attention mechanism machine translation global attention similar soft attention local attention viewed interesting blend hard soft tion only subset source word considered time approach computationally le expensive global attention soft attention time unlike hard attention approach differentiable almost everywhere making easier implement train form input feature attention mechanism divided according whether input feature sequence item attention requires input either explicit item additional preprocessing step added erate sequence item source data instance item single word embedding rnnsearch 8 single feature map senet 71 attention model encoder encodes item separate code assigns different weight decoding contrary attention aimed task difﬁcult obtain distinct input item generally attention mechanism used visual task example decoder process crop input image step transforms related region canonical expected pose simplify inference subsequent layer 73 another difference way calculated combined attention mechanism soft attention calculates weight item make linear nation soft attention accepts entire feature map input generates transformed version attention module instead linear combination item hard attention stochastically pick one some item based probability hard tion stochastically pick input location picked calculated attention module input representation two feature input representation attention model mentioned 1 model include single input corresponding output sequence 2 key query belong two independent sequence kind attention called distinctive attention 74 addition tion mechanism ha many different form input representation fig 5 lu et al 75 presented attention model visual question answering task jointly son image question attention formed parallelly alternatively former generates image question attention simultaneously latter sequentially alternate generating image question attention thermore 76 attention computes attention input using embedding input query attention evaluates element input affect element input ha used successfully variety task including sentiment classiﬁcation 77 text matching 78 named entity recognition 79 entity disambiguation 80 tion cause analysis 81 sentiment classiﬁcation 82 wang et al 83 presented self inner attention computes attention only based input sequence word query key value different representation input sequence model ha proven effective several author exploited different fashion application transformer 16 ﬁrst sequence transduction model based entirely without rnns application model various ﬁelds described section attention weight computed not only original input sequence also different abstraction level refer hierarchical attention yang et al 63 proposed hierarchical attention network ham document classiﬁcation ha two level attention mechanism hierarchical attention allows ham aggregate important word sentence aggregate important sentence document furthermore hierarchy extended wu et al 88 added user level top applying attention also document level contrary model weight learning lower level higher level model proposed zhao zhang 61 also used hierarchical attention attention weight learned higher level lower level except natural language processing hierarchical attention also used computer vision instance xiao et al 89 proposed attention method level attention ﬁrst image siﬁcation method doe not use additional part information only relies model generate attention weight output representation part discus various type output representation attention model among common one attention refers single feature representation time step speciﬁcally energy score represented one only table 3 example combination different category reference application category softness attention form input feature input representation output representation 8 machine translation soft distinctive 7 image classiﬁcation hard distinctive 16 machine translation soft distinctive 75 visual question answering soft hierarchical 91 language understanding soft distinctive 73 image classiﬁcation soft distinctive 63 document classiﬁcation soft hierarchical niu zhong yu neurocomputing 452 2021 52 one vector time step however some case using single feature representation may not able complete downstream task well next describe two attention model illustrated fig many application convolutional neural network ha proved multiple channel express input data comprehensively single channel also attention model some case using single attention distribution input sequence may not sufﬁce downstream task vaswani et al 16 proposed attention linearly project input sequence q k v multiple subspace based learnable parameter applies scaled attention sentation subspace ﬁnally concatenates output way allows model jointly attend information different representation subspace different position li et al 90 proposed three disagreement regularization augment attention model subspace attended position output representation respectively approach encourages diversity among attention head ferent head learn distinct feature effectiveness idated translation task another approach proposed shen et al 91 computes score vector key replacing weight score vector matrix way neural network calculate multiple attention distribution data especially useful natural language cessing word embeddings suffer polysemy lem furthermore lin et al 30 du et al 92 added frobenius penalty enforce distinction model relevance successfully applied various task including fig example hierarchical attention figure 75 fig illustration representation niu zhong yu neurocomputing 452 2021 53 author proﬁling sentiment classiﬁcation textual entailment distantly supervised relation extraction network architecture attention section describe three neural network architecture used conjunction attention ﬁrst elaborate framework used attention model describe special network architecture attention model memory network equipped external memory finally introduce special neural network structure combined attention mechanism rnns not used process capturing dependency shown fig 7 general framework based neural network aim handle mapping highly structured input output ﬁrst brieﬂy describe rnn framework proposed cho et al 5 sutskever et al 13 machine translation task architecture consists two rnns act encoder decoder pair encoder map source sequence x xtxþ vector common approach use rnn ht f xt ð þ c q htx f g ð þ ht hidden state time step c context vector generated sequence hidden state f q activation function example f may simple logistic sigmoid function complex gru 5 lstm 4 q may hidden state htx last time step decoder trained generate target sequence ytyþ predicting next symbol yt given hidden state ht however unlike encoder yt ht also conditioned context vector hence hidden state decoder time step computed ht cþ conditional probability modeled xþ ht cþ encoder decoder proposed model jointly trained maximize conditional probability target sequence given source sequence potential issue framework neural network need able compress necessary information source data vector may make difﬁcult neural network cope long sentence especially longer tences training corpus problem alleviated adding attention mechanism framework ct att htx f g ð þ xþ ht ctþ att attention method mentioned section 2 ct context vector generated attention mechanism decoding time introduction attention mechanism ensure contribution element source sequence different decoding different target element shown fig since framework doe not limit length input output sequence ha wide range tions image video captioning generative log system 95 visual question answering 75 speech recognition moreover encoder decoder also constructed using architecture not necessarily only rnns although attention model regarded general idea doe not depend speciﬁc framework attention model currently accompanied decoder framework memory network addition framework previous section attention mechanism also used conjunction memory network inspired human brain mechanism handling information overload memory network introduces extra external memory neural network concretely ory network save some tion auxiliary memory introducing external auxiliary memory unit read needed not only effectively increase network capacity also improves network computing efﬁciency compared general attention mechanism memory network replaces key term auxiliary memory match content attention mechanism example differentiable memory network proposed 98 read information fig illustration framework without attention mechanism niu zhong yu neurocomputing 452 2021 54 nal information multiple time core idea convert input set two external memory unit one addressing another output shown fig memory network regarded form attention pair attention mechanism different usual attention instead modeling attention only single sequence use two nal memory unit model large database sequence word regard attention mechanism face separate storage information calculation network capacity greatly increased small increase network parameter network without rnns mentioned encoder decoder framework implemented multiple way shown fig architecture based rnns typically factor computation along symbol position input output sequence inherently sequential nature result computational inefﬁciency processing not parallelized hand capturing cies essential attention mechanism decoder framework need obtain contextual information ever computational complexity establishing dependence sequence length n rnn oðnþ part describe implementation framework combined attention mechanism card rnns gehring et al 102 proposed architecture relied entirely convolutional neural network combined attention mechanism contrast fact rent network maintain hidden state entire past tional network not rely computation previous time step allows parallelization element sequence architecture enables network capture distance dependency stacking multiple layer cnn computational complexity becomes cnn convolution kernel size moreover convolution method discover compositional structure sequence easily hierarchical representation vaswani et al 16 proposed another network architecture transformer relies entirely mechanism compute representation input output without ing rnns cnns transformer composed two nents network ffn layer head attention layer ffn fully connected forward network applied position separately identically method ensure position information symbol input sequence operation attention allows model focus information different representation subspace different position stacking multiple layer like multiple nels cnn addition parallelizable ity establishing dependence attention mechanism application previous section shown attention model widely applied various task introduce some speciﬁc application attention model cv nlp not intend provide comprehensive review neural architecture use attention mechanism focus eral general attention method attention model fig illustration framework using attention mechanism fig single layer version memory network 98 question input output correspond query key value uniﬁed attention model respectively niu zhong yu neurocomputing 452 2021 55 application computer vision part describe attention mechanism cv ducing several typical paper different aspect neural architecture spatial attention allows neural network learn position focused shown fig tion mechanism spatial information original picture transformed another space key information retained mnih et al 7 presented spatial attention model formulated single rnn take glimpse window input us internal state network select next location focus well generate control signal dynamic environment jaderberg et al 73 introduced tiable spatial transformer network stn ﬁnd area need paid attention feature map formation cropping translation rotation scale skew unlike pooling layer spatial transformer module dynamic mechanism actively spatially transform image feature map producing appropriate transformation input sample channel attention allows neural network learn focused shown fig hu et al 71 proposed se network adaptively recalibrated feature response explicitly modeling dencies channel module used global feature compute attention li et al 103 proposed sknet improved ciency effectiveness object recognition adaptive kernel selection channel attention manner besides stollenga et al 104 proposed channel hard attention mechanism improved classiﬁcation performance allowing network iteratively focus attention ﬁlters capturing dependency central importance deep neural network beneﬁcial visual understanding problem 87 applied mechanism puter vision task solve problem called attention shown fig proposed module got attention mask calculating correlation matrix spatial point feature map attention guided dense contextual information aggregate however method also ha following problem 1 only positional attention module involved not commonly used channel attention fig illustration three structure used capture long distance dependency fig example spatial attention 73 niu zhong yu neurocomputing 452 2021 56 mechanism 2 input feature map large problem low efﬁciency although method solve problem scaling lose information not best way deal problem address lem researcher improved method bined channel attention propose mixed attention 111 different previous study ccnet 112 used attention generated huge attention map record relationship feature map shown fig attention module lected contextual information horizontal vertical direction enhance representative capability moreover recurrent attention allowed capturing dense range contextual information pixel le computing cost le memory cost application natural language processing part ﬁrst introduce some attention method used different task nlp describe some common training word representation implemented attention mechanism nlp task neural machine translation us neural network translate text one language another process translation alignment sentence different language crucial lem especially longer sentence bahdanau et al 8 introduced attention mechanism neural network improve ral machine translation selectively focusing part source sentence translation afterwards several work proposed improvement local attention 14 supervised attention hierarchical attention 61 self attention used different attention architecture improve alignment sentence enhance performance translation text classiﬁcation aim assigning label text ha broad application including topic labeling 116 sentiment classiﬁcation spam detection 119 classiﬁcation task self attention mainly used construct effective document representation basis some work combined mechanism attention method hierarchical 63 attention 30 besides task also applied attention model architecture see section 4 transformer memory network text matching also core research problem nlp mation retrieval includes question answering document search entailment classiﬁcation paraphrase identiﬁcation recommendation review many researcher come new approach conjunction attention sion memory network 98 124 inner attention 83 structured attention 65 attention word representation key component many neural language understanding model however previous study only determined one embedding word could not achieve contextual word embedding peter et al 127 introduced general approach representation solve problem inspired transformer model 62 researcher proposed tional encoder representation transformer bert 128 generative gpt method according encoder decoder part bert bidirectional language model ha following two task 1 masked language model mlm simply mask some percentage input token random predicts masked token 2 next tence prediction us linear binary classiﬁer determine whether two sentence connected gpt model training method roughly use previous word predict next word experiment show large improvement applying broad range nlp task attention interpretability recent year artiﬁcial intelligence ha developed rapidly especially ﬁeld deep learning fig illustration channel attention 71 fig illustration attention 87 niu zhong yu neurocomputing 452 2021 57 ever interpretability major concern many current deep learning model deep learning model become increasingly plex critical learn function data ensure understand particular decision occurs 51 attention layer neural network model provides way reason model behind prediction often criticized opaque shown fig 15 bahdanau et al 8 visualized attention weight annotation clearly show alignment word generated translation french source sentence english fig 15 b show attended region corresponding underlined word neural image caption generation process 9 see model learns alignment correspond strongly human intuition furthermore fig 15 c show example visualization encoder layer transformer model proposed vaswani et al 16 different color represent different head moreover voita et al 139 evaluated contribution made individual attention head transformer model tion discovered different head showed different level importance pruning operation besides chan et al 24 observed attention mechanism could identify start position audio sequence ﬁrst acter correctly however some recent study suggested attention could not considered reliable mean interpret deep neural network jain wallace 52 performed extensive experiment across variety nlp task proved attention wa not consistent explainability metric ﬁnd often construct adversarial attention distribution mean different attention weight distribution yield equivalent prediction serrano smith 53 applied different analysis based intermediate representation found attention weight only noisy predictor intermediate ponents importance contrary wiegreffe pinter 57 made rebuttal work four alternative test thus whether not attention used mean explain neural work still open topic challenge prospect attention model become ubiquitous deep neural work since bahdanau et al 8 used attention mechanism fig illustration attention 112 fig example visualizing weight attention mechanism niu zhong yu neurocomputing 452 2021 58 alignment machine translation task various attention mechanism variant emerged endlessly transformer model only us mechanism proposed vaswani et al 16 also important milestone attention mechanism variant ful applied various ﬁelds investigating attention model ﬁnd attention mechanism innovative eral aspect multiple implementation score tion distribution function combination value attention weight network architecture still much room improvement design attention mechanism summarize promising tions follows present model represent query key independently however some recent study achieved good performance combining query key whether query key necessary exist independently still open question attention distribution function ha great inﬂuence computational complexity whole attention model some recent study show complexity attention computation may reduced improving tion distribution function attention technique developed one area applied area also interesting direction example attention method nlp applied cv improves performance also reducing efﬁciency like network 87 combination adaptive mechanism attention nism may automatically achieve effect hierarchical tion without manually designing structure layer explore effective performance indicator evaluating attention mechanism also interesting topic sen et al 147 designed set evaluation method quantify similarity human neural model using novel similarity metric also open direction future research conclusion paper illustrate uniﬁed attention model work describe detail classiﬁcation attention nisms summarize network architecture used tion attention mechanism introduced typical application attention mechanism used computer vision natural language processing finally discus pretability attention mechanism provide model eration process challenge prospect current attention model conclusion attention mechanism ha successfully used various ﬁelds deep learning application still many interesting question explored credit authorship contribution statement zhaoyang niu investigation methodology formal analysis writing review editing guoqiang zhong conceptualization methodology investigation formal analysis writing review editing project administration supervision hui yu ization investigation methodology writing review editing supervision declaration competing interest author declare no known competing cial interest personal relationship could appeared inﬂuence work reported paper acknowledgement work wa supported national key research development program china grant no joint fund equipment ministry education china grant no natural science foundation shandong vince grant no science ogy program qingdao grant no reference 1 rensink dynamic representation scene visual cogn 7 2000 42 2 corbetta shulman control attention brain nat rev neurosci 3 2002 3 tsotsos culhane wai lai davis nuﬂo modeling visual attention via selective tuning artif intell 78 1995 4 hochreiter schmidhuber long memory neural comput 9 1997 5 cho van merrienboer gülçehre bahdanau bougares schwenk bengio learning phrase representation using rnn decoder statistical machine translation emnlp acl 2014 pp 1734 6 itti koch niebur model visual attention rapid scene analysis ieee trans pattern anal mach intell 20 1998 7 mnih heess graf kavukcuoglu recurrent model visual attention nip pp 8 bahdanau cho bengio neural machine translation jointly learning align translate iclr 9 xu ba kiros cho courville salakhutdinov zemel bengio show attend tell neural image caption generation visual attention icml volume 37 jmlr workshop conference proceeding 2015 pp 10 lu xiong parikh socher knowing look adaptive attention via visual sentinel image captioning cvpr ieee computer society 2017 pp 11 liu guo bidirectional lstm attention mechanism convolutional layer text classiﬁcation neurocomputing 337 2019 12 li yang xu wang lin improving user attribute classiﬁcation text social network attention cogn comput 11 2019 13 sutskever vinyals le sequence sequence learning neural network nip pp 14 luong pham manning effective approach neural machine translation emnlp association computational linguistics 2015 pp 15 britz goldie luong le massive exploration neural machine translation architecture corr 2017 16 vaswani shazeer parmar uszkoreit jones gomez kaiser polosukhin attention need nip pp 17 song lan xing zeng liu attention model human action recognition skeleton data aaai aaai press 2017 pp 18 tian hu jiang wu densely connected attentional pyramid residual network human pose estimation neurocomputing 347 2019 19 zhao qi li dong yu lstm diagnosis neurodegenerative disease using gait data yu dong ed ninth international conference graphic image processing icgip 2017 vol 10615 20 zhang xue lan zeng gao zheng adding attentiveness neuron recurrent neural network eccv 9 volume 11213 lecture note computer science springer 2018 pp 21 song yao ling mei boosting image sentiment analysis visual attention neurocomputing 312 2018 22 yan hu mao ye yu deep learning method review neurocomputing 2021 23 chorowski bahdanau cho bengio continuous speech recognition using recurrent nn ﬁrst result corr 2014 niu zhong yu neurocomputing 452 2021 59 24 chan jaitly le vinyals listen attend spell neural network large vocabulary conversational speech recognition icassp ieee 2016 pp 25 sperber niehues neubig stüker waibel acoustic model interspeech isca 2018 pp 26 wang hu cao huang lian liu transactional context embedding recommendation aaai aaai press 2018 pp 27 ying zhuang zhang liu xu xie xiong wu sequential recommender system based hierarchical attention network ijcai 2018 pp 28 velickovic cucurull casanova romero liò bengio graph attention network iclr poster 2018 29 xu wu wang feng sheinin graph sequence learning neural network corr 2018 30 lin feng santos yu xiang zhou bengio structured sentence embedding iclr poster openreview net 2017 31 zhang zhong dong wang wang stock market prediction based generative adversarial network bie sun yu ed 2018 international conference identiﬁcation information knowledge internet thing iiki 2018 beijing china october 2018 volume 147 procedia computer science elsevier 2018 pp 32 ieracitano paviglianiti campolo hussain pasero morabito novel automatic classiﬁcation system based hybrid unsupervised supervised machine learning electrospun nanoﬁbers ieee caa autom sinica 8 2021 33 fan zhong li feature fusion network mesoscale eddy detection yang pasupa leung kwok chan king ed neural information processing international conference iconip 2020 bangkok thailand november 2020 proceeding part volume 12532 lecture note computer science springer 2020 pp 34 yu garrod jack schyns framework automatic perceptually valid facial expression generation multimedia tool appl 74 2015 35 li fan zhong bednet edge detection network ocean front detection yang pasupa leung kwok chan king ed neural information processing international conference iconip 2020 bangkok thailand november 2020 proceeding part iv volume 1332 communication computer information science springer 2020 pp 36 fan zhong wei li ednet mesoscale eddy detection network data 2020 international joint conference neural network ijcnn 2020 glasgow united kingdom july 2020 ieee 2020 pp 37 liu xia yu dong jian pham region based parallel hierarchy convolutional neural network automatic facial nerve paralysis evaluation ieee trans neural syst rehab eng 28 2020 38 yue wang liu tian lauria liu optimally weighted collaborative ﬁltering approach predicting baseline data friedreich ataxia patient neurocomputing 419 2021 39 zeng li wang liu liu alsaadi liu image segmentation quantitative analysis gold immunochromatographic strip neurocomputing 2020 40 liu wang liu zeng bell novel particle swarm optimization approach patient clustering emergency department ieee trans evol comput 23 2019 41 zeng wang zhang kim li liu improved particle ﬁlter novel hybrid proposal distribution quantitative analysis gold immunochromatographic strip ieee trans nanotechnol 18 2019 829 42 ming meng fan yu deep learning monocular depth estimation review neurocomputing 438 2021 43 xia yu wang accurate robust eye center localization via fully convolutional network ieee caa autom sinica 6 2019 44 guo xia wang yu chen facial affective computing mobile device sensor 20 2020 870 45 wang dong li dong yu cascade face frontalization dynamic facial expression analysis cogn comput 2021 46 zhang yu huang howell stevens scene perception guided crowd anomaly detection neurocomputing 414 2020 47 roy banerjee hussain poria discriminative dictionary design action classiﬁcation still image video cogn comput 2021 48 liu xia shi yu li lin deep learning sheet metal bending novel deep neural network autom sinica 8 2021 49 luque snchez hupont tabik herrera revisiting crowd behaviour analysis deep learning taxonomy anomaly detection crowd emotion datasets opportunity prospect inf fusion 64 2020 335 50 zhang yang zhang li yu crowd emotion evaluation based fuzzy inference arousal valence neurocomputing 445 2021 51 guidotti monreale ruggieri turini giannotti pedreschi survey method explaining black box model acm comput surv 51 2019 52 jain wallace attention not explanation 1 association computational linguistics 2019 pp 53 serrano smith attention interpretable acl 1 association computational linguistics 2019 pp 54 li yatskar yin hsieh chang doe bert vision look acl association computational linguistics 2020 pp 55 letarte paradis giguère laviolette importance sentiment analysis blackboxnlp emnlp association computational linguistics 2018 pp 56 vashishth upadhyay tomar faruqui attention interpretability across nlp task corr 2019 57 wiegreffe pinter attention not not explanation 1 association computational linguistics 2019 pp 58 schuster paliwal bidirectional recurrent neural network ieee trans signal process 45 1997 59 sordoni bachman bengio iterative alternating neural attention machine reading corr 2016 60 graf wayne danihelka neural turing machine corr 2014 61 zhao zhang neural machine translation aaai aaai press 2018 pp 62 galassi lippi torroni attention please critical review neural attention model natural language processing corr 2019 63 yang yang dyer smola hovy hierarchical attention network document classiﬁcation association computational linguistics 2016 pp 64 martin astudillo softmax sparsemax sparse model attention classiﬁcation icml volume 48 jmlr workshop conference proceeding 2016 pp 65 kim denton hoang rush structured attention network arxiv computation language 2017 66 miller fisch dodge karimi bordes weston memory network directly reading document emnlp association computational linguistics 2016 pp 67 ba hinton mnih leibo ionescu using fast weight attend recent past nip pp 68 gülçehre chandar cho bengio dynamic neural turing machine soft hard addressing scheme corr 2016 69 daniluk rocktäschel welbl riedel frustratingly short attention span neural language modeling iclr poster 2017 70 williams simple statistical algorithm connectionist reinforcement learning mach learn 8 1992 71 hu shen albanie sun wu network ieee trans pattern anal mach intell 42 2020 72 ba mnih kavukcuoglu multiple object recognition visual attention bengio lecun ed international conference learning representation iclr 2015 san diego ca usa may 2015 conference track proceeding 73 jaderberg simonyan zisserman et spatial transformer network advance neural information processing system pp 74 chaudhari polatkan ramanath mithal attentive survey attention model corr 2019 75 lu yang batra parikh hierarchical visual question answering nip pp 76 fan feng zhao attention network sentiment classiﬁcation emnlp association computational linguistics 2018 pp 77 wang pan dahlmeier xiao coupled attention aspect opinion term aaai aaai press 2017 pp 78 tay luu hui hermitian network text matching asymmetrical domain ijcai 2018 pp 79 zhang fu liu huang adaptive network named entity recognition tweet aaai aaai press 2018 pp 80 nie cao wang lin pan mention entity description attention entity disambiguation aaai aaai press 2018 pp 5915 81 li song feng wang zhang neural network model emotion cause analysis emotional context awareness emnlp association computational linguistics 2018 pp 82 tay luu hui su attentive gated lexicon reader contrastive contextual sentiment classiﬁcation emnlp association computational linguistics 2018 pp 83 wang liu zhao inner attention based recurrent neural network answer selection acl 1 association computer linguistics 2016 84 wu tian zhao lai liu word attention sequence sequence text understanding aaai aaai press 2018 pp 85 pavlopoulos malakasiotis androutsopoulos deeper attention abusive user content moderation emnlp association computational linguistics 2017 pp niu zhong yu neurocomputing 452 2021 60 86 li wei zhang yang hierarchical attention transfer network sentiment classiﬁcation aaai aaai press 2018 pp 5859 87 wang girshick gupta neural network cvpr ieee computer society 2018 pp 88 wu wu liu huang hierarchical user item representation attention recommendation 1 association computational linguistics 2019 pp 89 xiao xu yang zhang peng zhang application attention model deep convolutional neural network image classiﬁcation cvpr ieee computer society 2015 pp 90 li tu yang lyu zhang attention disagreement regularization emnlp association computational linguistics 2018 pp 91 shen zhou long jiang pan zhang disan directional attention network language understanding aaai aaai press 2018 pp 92 du han way wan structured distantly supervised relation extraction emnlp association computational linguistics 2018 pp 93 venugopalan rohrbach donahue mooney darrell saenko sequence sequence video text 2015 ieee international conference computer vision iccv 2015 santiago chile december 2015 ieee computer society 2015 pp 94 ding chen zhao chen han liu neural image caption generation weighted training reference cogn comput 11 2019 95 zhang yang transfer hierarchical attention network generative dialog system int autom comput 16 2019 96 prabhavalkar rao sainath li johnson jaitly comparison model speech recognition lacerda ed interspeech 2017 annual conference international speech communication association stockholm sweden august 2017 isca 2017 pp 97 wang zhang zong learning sentence representation guidance human attention ijcai 2017 pp 98 sukhbaatar szlam weston fergus memory network nip pp 99 weston chopra bordes memory network iclr 100 kumar irsoy ondruska iyyer bradbury gulrajani zhong paulus socher ask anything dynamic memory network natural language processing icml volume 48 jmlr workshop conference proceeding 2016 pp 101 henaff weston szlam bordes lecun tracking world state recurrent entity network international conference learning representation iclr 2017 toulon france april 2017 conference track proceeding 2017 102 gehring auli grangier yarats dauphin convolutional sequence sequence learning icml volume 70 proceeding machine learning research pmlr 2017 pp 103 li wang hu yang selective kernel network ieee conference computer vision pattern recognition cvpr 2019 long beach ca usa june 2019 computer vision ieee 2019 pp 519 104 stollenga masci gomez schmidhuber deep network internal selective attention feedback connection ghahramani welling cortes lawrence weinberger ed advance neural information processing system 27 annual conference neural information processing system 2014 december 2014 montreal quebec canada pp 105 fu liu tian li bao fang lu dual attention network scene segmentation ieee conference computer vision pattern recognition cvpr 2019 long beach ca usa june 2019 computer vision ieee 2019 pp 106 yuan wang ocnet object context network scene parsing corr 2018 107 zhao zhang liu shi loy lin jia psanet spatial attention network scene parsing ferrari hebert sminchisescu wei ed computer vision eccv european conference munich germany september 2018 proceeding part ix volume 11213 lecture note computer science springer 2018 pp 108 cao xu lin wei hu gcnet network meet excitation network beyond 2019 international conference computer vision workshop iccv workshop 2019 seoul korea south october 2019 ieee 2019 pp 109 wang jiang qian yang li zhang wang tang residual attention network image classiﬁcation 2017 ieee conference computer vision pattern recognition cvpr 2017 honolulu hi usa july 2017 ieee computer society 2017 pp 110 yue sun yuan zhou ding xu compact generalized network bengio wallach larochelle grauman bianchi garnett ed advance neural information processing system 31 annual conference neural information processing system 2018 neurips 2018 december 2018 montréal canada pp 111 woo park lee kweon cbam convolutional block attention module ferrari hebert sminchisescu wei ed computer vision eccv european conference munich germany september 2018 proceeding part vii volume 11211 lecture note computer science springer 2018 pp 112 huang wang huang huang wei liu ccnet attention semantic segmentation 2019 international conference computer vision iccv 2019 seoul korea south october 2 2019 ieee 2019 pp 113 mi wang ittycheriah supervised attention neural machine translation su carreras duh ed proceeding 2016 conference empirical method natural language processing emnlp 2016 austin texas usa november 2016 association computational linguistics 2016 pp 114 liu utiyama finch sumita neural machine translation supervised attention calzolari matsumoto prasad ed coling 2016 international conference computational linguistics proceeding conference technical paper december 2016 osaka japan acl 2016 pp 115 yang tu wong meng chao zhang modeling localness network riloff chiang hockenmaier tsujii ed proceeding 2018 conference empirical method natural language processing brussels belgium october 4 2018 association computational linguistics 2018 pp 116 wang manning baseline bigram simple good sentiment topic classiﬁcation annual meeting association computational linguistics proceeding conference july 2012 jeju island korea volume 2 short paper association computer linguistics 2012 pp 117 maas daly pham huang ng potts learning word vector sentiment analysis lin matsumoto mihalcea ed annual meeting association computational linguistics human language technology proceeding conference june 2011 portland oregon usa association computer linguistics 2011 pp 118 pang lee opinion mining sentiment analysis found trend inf retr 2 2007 119 sahami dumais heckerman horvitz bayesian approach ﬁltering junk learning text categorization paper 1998 workshop vol 62 madison wisconsin pp 120 song wang jiang liu rao attentional encoder network targeted sentiment classiﬁcation corr 2019 121 ambartsoumian popowich better building block sentiment analysis neural network classiﬁers balahur mohammad hoste klinger ed proceeding workshop computational approach subjectivity sentiment social medium analysis wassa emnlp 2018 brussels belgium october 31 2018 association computational linguistics 2018 pp 122 tang qin liu aspect level sentiment classiﬁcation deep memory network su carreras duh ed proceeding 2016 conference empirical method natural language processing emnlp 2016 austin texas usa november 2016 association computational linguistics 2016 pp 123 zhu qian enhanced aspect level sentiment classiﬁcation auxiliary memory bender derczynski isabelle ed proceeding international conference computational linguistics coling 2018 santa fe new mexico usa august 2018 association computational linguistics 2018 pp 124 cui chen wei wang liu hu neural network reading comprehension barzilay kan ed proceeding annual meeting association computational linguistics acl 2017 vancouver canada july 4 volume 1 long paper association computational linguistics 2017 pp 125 mikolov chen corrado dean efﬁcient estimation word representation vector space bengio lecun ed international conference learning representation iclr 2013 scottsdale arizona usa may 2013 workshop track proceeding 126 pennington socher manning glove global vector word representation moschitti pang daelemans ed proceeding 2014 conference empirical method natural language processing emnlp 2014 october 2014 doha qatar meeting sigdat special interest group acl acl 2014 pp 127 peter neumann iyyer gardner clark lee zettlemoyer deep contextualized word representation walker ji stent ed proceeding 2018 conference north american chapter association computational linguistics human language technology 2018 new orleans louisiana usa june 2018 volume 1 long paper association computational linguistics 2018 pp 128 devlin chang lee toutanova bert deep bidirectional transformer language understanding burstein doran solorio ed proceeding 2019 conference north american chapter association computational linguistics human language technology 2019 minneapolis mn usa june niu zhong yu neurocomputing 452 2021 61 2019 volume 1 long short paper association computational linguistics 2019 pp 129 radford narasimhan salimans sutskever improving language understanding generative 2018 130 radford wu child luan amodei sutskever language model unsupervised multitask learner openai blog 1 2019 9 131 brown mann ryder subbiah kaplan dhariwal neelakantan shyam sastry askell agarwal krueger henighan child ramesh ziegler wu winter hesse chen sigler litwin gray chess clark berner mccandlish radford sutskever amodei language model shot learner corr 2020 132 liu wang zeng yuan alsaadi liu novel randomised particle swarm optimizer int mach learn cybern 12 2021 133 zeng wang liu zhang hone liu dynamic based switching particle swarm optimization algorithm ieee trans cybern 2020 134 liu wang yuan zeng hone liu novel based adaptive weighted particle swarm optimizer ieee trans cybern 51 2021 135 rahman wang liu ye zakarya liu markovian jumping particle swarm optimization algorithm ieee trans man cybern syst 2020 136 luo yuan chen zeng wang particle swarm latent factor analysis ieee trans knowl data eng 2020 137 zeng song li liu alsaadi competitive mechanism integrated whale optimization algorithm differential evolution neurocomputing 432 2021 138 li monroe jurafsky understanding neural network representation erasure corr 2016 139 voita talbot moiseev sennrich titov analyzing attention specialized head heavy lifting rest pruned korhonen traum màrquez ed proceeding conference association computational linguistics acl 2019 florence italy july 2 2019 volume 1 long paper association computational linguistics 2019 pp 140 dai yang yang carbonell le salakhutdinov xl attentive language model beyond context korhonen traum màrquez ed proceeding conference association computational linguistics acl 2019 florence italy july august 2 2019 volume 1 long paper association computational linguistics 2019 pp 141 dehghani gouws vinyals uszkoreit kaiser universal transformer international conference learning representation iclr 2019 new orleans la usa may 2019 2019 142 guo qiu liu shao xue zhang burstein doran solorio ed proceeding 2019 conference north american chapter association computational linguistics human language technology 2019 minneapolis mn usa june 2019 volume 1 long short paper association computational linguistics 2019 pp 143 zhu cheng zhang lin dai empirical study spatial attention mechanism deep network 2019 international conference computer vision iccv 2019 seoul korea south october 2 2019 ieee 2019 pp 144 tay bahri metzler juan zhao zheng synthesizer rethinking transformer model corr 2020 145 tsai bai yamada morency salakhutdinov transformer dissection uniﬁed understanding transformer attention via lens kernel inui jiang ng wan ed proceeding 2019 conference empirical method natural language processing international joint conference natural language processing ijcnlp 2019 hong kong china november 2019 association computational linguistics 2019 pp 146 katharopoulos vyas pappa fleuret transformer rnns fast autoregressive transformer linear attention corr 2020 147 sen hartvigsen yin kong rundensteiner human attention map text classiﬁcation human neural network focus word jurafsky chai schluter tetreault ed proceeding annual meeting association computational linguistics acl 2020 online july 2020 association computational linguistics 2020 pp zhaoyang niu received degree school data science software engineering qingdao university qingdao china studying bsc degree computer technology ocean university china qingdao china research interest include computer vision deep learning attention mechanism guoqiang zhong received degree matics hebei normal university shijiazhuang china degree operation research cybernetics beijing university technology bjut beijing china degree pattern recognition intelligent system institute automation chinese academy science casia beijing china 2004 2007 2011 respectively october 2011 july 2013 wa doctoral fellow synchromedia laboratory multimedia communication telepresence university quebec montreal canada march 2014 december 2020 wa associate professor department computer science technology ocean university china qingdao china since january 2021 ha full professor department computer science technology ocean university china ha published 4 book 4 book chapter 80 technical paper area artiﬁcial intelligence pattern recognition machine learning computer vision research interest include pattern recognition machine learning computer vision ha served many international conference top journal ieee tnnls ieee tkde ieee tcsvt pattern recognition system ing acm tkdd aaai aistats icpr ijcnn iconip icdar ha awarded outstanding reviewer several journal pattern recognition system neurocomputing cognitive system research ha best paper award apnns young researcher award member acm ieee iapr apnns ccf professional committee member trustee shandong association artiﬁcial intelligence hui yu professor university portsmouth uk yu received phd brunel university used work university glasgow moving university portsmouth research interest include method practical development vision machine learning ai application interaction virtual augmented reality robotics facial expression serf associate editor ieee transaction system neurocomputing journal niu zhong yu neurocomputing 452 2021 62