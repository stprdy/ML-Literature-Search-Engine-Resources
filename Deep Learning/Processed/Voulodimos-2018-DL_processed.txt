review article deep learning computer vision brief review athanasios voulodimos nikolaos anastasios eftychios informatics technological educational institute athens 12210 athens greece technical university athens 15780 athens greece correspondence addressed athanasios voulodimos thanosv received 17 june 2017 accepted 27 november 2017 published 1 february 2018 academic editor diego andina copyright 2018 athanasios voulodimos et al open access article distributed creative common attribution license permit unrestricted use distribution reproduction any medium provided original work properly cited last year deep learning method shown outperform previous machine learning technique several field computer vision one prominent case review paper provides brief overview some significant deep learning scheme used computer vision problem convolutional neural network deep boltzmann machine deep belief network stacked denoising autoencoders brief account history structure advantage limitation given followed description application various computer vision task object detection face recognition action activity recognition human pose estimation finally brief overview given future direction designing deep learning scheme computer vision problem challenge involved therein introduction deep learning allows computational model multiple processing layer learn represent data multiple level abstraction mimicking brain perceives understands multimodal information thus implicitly ing intricate structure data deep learning rich family method encompassing neural network hierarchical probabilistic model variety vised supervised feature learning algorithm recent surge interest deep learning method due fact shown outperform previous technique several task well abundance complex data different source visual audio medical social sensor ambition create system simulates human brain fueled initial development neural network 1943 mcculloch pitt 1 tried understand brain could produce highly complex pattern using connected basic cell called neuron mcculloch pitt model neuron called mcp model ha made important contribution development artificial neural network series major contribution field sented table 1 including lenet 2 long memory 3 leading today era deep one substantial breakthrough deep learning came 2006 hinton et al 4 introduced deep belief network multiple layer restricted boltzmann machine greedily training one layer time pervised way guiding training intermediate level representation using unsupervised learning performed locally level wa main principle behind series development brought last decade surge deep architecture deep learning algorithm among prominent factor contributed huge boost deep learning appearance large publicly available labelled datasets along empowerment parallel gpu computing enabled transition training thus allowing significant acceleration deep model training additional factor may played lesser role well alleviation vanishing gradient problem owing disengagement saturating activation function hyperbolic tangent logistic function proposal hindawi computational intelligence neuroscience volume 2018 article id 7068349 13 page 2 computational intelligence neuroscience table 1 important milestone history neural network machine learning leading era deep learning contributor year mcp model regarded ancestor artificial neural network mcculloch pitt 1943 hebbian learning rule hebb 1949 first perceptron rosenblatt 1958 backpropagation werbos 1974 neocognitron regarded ancestor convolutional neural network fukushima 1980 boltzmann machine ackley hinton sejnowski 1985 restricted boltzmann machine initially known harmonium smolensky 1986 recurrent neural network jordan 1986 autoencoders rumelhart hinton williams 1986 ballard 1987 lenet starting era convolutional neural network lecun 1990 lstm hochreiter schmidhuber 1997 deep belief network ushering age deep learning hinton 2006 deep boltzmann machine salakhutdinov hinton 2009 alexnet starting age cnn used imagenet classification krizhevsky sutskever hinton 2012 new regularization technique dropout batch malization data augmentation appearance powerful framework like tensorflow 5 theano 6 mxnet 7 allow faster prototyping deep learning ha fueled great stride variety computer vision problem object detection 8 9 motion tracking 10 11 action recognition 12 13 human pose estimation 14 15 semantic segmentation 16 17 overview cisely review main development deep learning tectures algorithm computer vision application context focus three important type deep learning model respect bility visual understanding convolutional neural network cnns boltzmann family including deep belief network dbns deep boltzmann machine dbms stacked denoising autoencoders needle say current coverage no mean exhaustive example long memory lstm category recurrent neural network although great significance deep learning scheme not presented review since predominantly applied problem language modeling text classification handwriting tion machine translation recognition le computer vision problem overview intended useful computer vision multimedia analysis researcher well general machine learning er interested state art deep learning computer vision task object detection recognition face recognition recognition human pose estimation remainder paper organized follows section 2 three aforementioned group deep learning model reviewed convolutional neural network deep belief network deep boltzmann machine stacked autoencoders basic architecture training process recent development advantage limitation group presented section 3 describe tion deep learning algorithm key computer vision task object detection recognition face recognition recognition human pose estimation also provide list important datasets resource benchmarking validation deep learning algorithm finally section 4 concludes paper summary finding deep learning method development convolutional neural network convolutional neural network cnns inspired visual system ture particular model proposed 18 first computational model based local nectivities neuron hierarchically organized transformation image found neocognitron 19 describes neuron parameter applied patch previous layer different location form translational invariance acquired yann lecun collaborator later designed convolutional neural network employing error gradient attaining good result variety pattern recognition task 22 cnn comprises three main type neural layer namely convolutional layer ii pooling layer iii fully connected layer type layer play different role figure 1 show cnn architecture object detection image task every layer cnn transforms input volume output volume neuron activation eventually leading final fully connected layer resulting mapping input data feature vector cnns extremely successful computer vision application face recognition object detection powering vision robotics car convolutional layer convolutional layer cnn utilizes various kernel convolve whole image computational intelligence neuroscience 3 convolution input data pooling convs linear classiÔ¨Åer object map feature map feature map feature map xi yi xj yj xk yk figure 1 example architecture cnn computer vision task object detection well intermediate feature map generating various feature map advantage convolution operation several work 23 24 proposed substitute fully connected layer view attaining faster learning time ii pooling layer pooling layer charge reducing spatial dimension width height input volume next convolutional layer pooling layer doe not affect depth dimension volume operation performed layer also called subsampling downsampling reduction size lead simultaneous loss mation however loss beneficial network decrease size lead le computational head upcoming layer network also work overfitting average pooling max pooling commonly used strategy 25 detailed theoretical analysis max pooling average pooling performance given whereas 26 wa shown max pooling lead faster convergence select superior invariant feature improve generalization also number variation pooling layer literature inspired different motivation serving distinct need example stochastic pooling 27 spatial pyramid pooling 28 29 30 iii fully connected layer following several convolutional pooling layer reasoning neural network performed via fully connected layer neuron fully connected layer full connection activation previous layer name implies activation hence computed matrix multiplication followed bias offset fully connected layer eventually convert feature map feature vector derived vector either could fed forward certain number category classification 31 could considered feature vector processing 32 architecture cnns employ three concrete idea local receptive field b tied weight c spatial subsampling based local receptive field unit convolutional layer receives input set neighboring unit belonging previous layer way neuron capable extracting elementary visual feature edge corner feature combined quent convolutional layer order detect higher order feature furthermore idea elementary feature tor useful part image likely useful across entire image implemented concept tied weight concept tied weight constraint set unit identical weight concretely unit convolutional layer organized plane unit plane share set weight thus plane ponsible constructing specific feature output plane called feature map convolutional layer consists several plane multiple feature map constructed location construction feature map entire image scanned unit whose state stored corresponding location feature map construction equivalent convolution operation followed additive bias term sigmoid function ùëë ùúé wy b 1 ùëëstands depth convolutional layer w weight matrix b bias term fully connected neural network weight matrix full connects every input every unit different weight cnns weight matrix w sparse due concept tied weight thus w ha form w 0 0 w 0 0 w 2 w matrix dimension unit receptive field employing sparse weight matrix reduces number network tunable parameter thus increase generalization ability multiplying w layer input like convolving input w seen trainable filter input convolutional layer 4 computational intelligence neuroscience dimension ùëÅand receptive field unit specific plane convolutional layer ùëëis dimension ùëö constructed feature map matrix dimension 1 1 specifically element feature map ùëñ ùëó location ùëë ùúé ùë• ùëë ùëè 3 ùë• ùëë ùë§ùõºùëèy 4 bias term ùëèis scalar using 4 3 sequentially ùëñ ùëó position input feature map ponding plane constructed one difficulty may arise training cnns ha large number parameter learned may lead problem overfitting end technique stochastic pooling dropout data augmentation proposed thermore cnns often subjected pretraining process initializes network pretrained eters instead randomly set one pretraining accelerate learning process also enhance generalization capability network overall cnns shown significantly outperform traditional machine learning approach wide range computer vision pattern recognition task 33 example presented section exceptional performance combined relative easiness training main reason explain great surge popularity last year deep belief network deep boltzmann machine deep belief network deep boltzmann machine deep learning model belong boltzmann family sense utilize restricted boltzmann machine rbm learning module restricted mann machine rbm generative stochastic neural work dbns undirected connection top two layer form rbm directed connection lower layer dbms undirected connection layer network graphic depiction dbns dbms found figure following subsection describe basic characteristic dbns dbms presenting basic building block rbm restricted boltzmann machine restricted mann machine 34 35 undirected graphical model stochastic visible variable k 0 1 ùê∑and stochastic hidden variable h 0 1 ùêπ visible variable connected hidden variable rbm variant boltzmann machine restriction visible unit hidden unit must form bipartite graph restriction allows efficient training algorithm particular contrastive divergence algorithm 36 model defines energy function ùê∏ 0 1 0 1 ùê∏ k h ùúÉ ùê∑ ùêπ ùëä ùê∑ ùêπ ùõºùëó‚Ñéùëó 5 b w model parameter ùëä ùëñùëó represents symmetric interaction term visible unit ùëñand hidden unit ùëó ùëèùëñ ùëéùëóare bias term joint distribution visible hidden unit given ùëÉ k h ùúÉ 1 z ùúÉ exp k h ùúÉ z ùúÉ k h exp k h ùúÉ 6 z ùúÉ normalizing constant conditional tributions hidden h visible v vector derived 5 6 ùëÉ h k ùúÉ ùêπ ùëù k ùëÉ k h ùúÉ ùê∑ ùëù h 7 given set observation kùëõ ùëÅ derivative likelihood respect model parameter rived 6 1 ùëÅ ùëÅ ùëÉ kùëõ ùúÉ eùëÉ data vùëñ‚Ñéùëó model vùëñ‚Ñéùëó 8 eùëÉ data denotes expectation respect data distribution ùëÉ data h k ùúÉ ùëÉ h k ùúÉ ùëÉdata k ùëÉdata k k representing empirical distribution eùëÉmodel expectation respect distribution defined model 6 detailed explanation along description practical way train rbms wa given 37 whereas 38 discus main difficulty training rbms underlying reason proposes new algorithm adaptive learning rate enhanced gradient address aforementioned difficulty deep belief network deep belief network dbns probabilistic generative model provide joint probability distribution observable data label formed stacking rbms training greedy manner wa proposed 39 dbn initially employ efficient greedy learning strategy initialize deep network sequel weight jointly desired output dbns graphical model learn extract deep hierarchical representation computational intelligence neuroscience 5 deep belief network deep boltzmann machine v v figure 2 deep belief network dbn deep boltzmann machine dbm top two layer dbn form undirected graph remaining layer form belief network directed connection dbm connection undirected training data model joint distribution observed vector x ùëôhidden layer hùëòas follows ùëÉ x hùëô ùëÉ ùëÉ hùëô 9 x ùëÉ conditional distribution visible unit level ùëòconditioned hidden unit rbm level 1 ùëÉ hùëô joint distribution rbm principle greedy unsupervised training applied dbns rbms building block layer 33 39 brief description process follows 1 train first layer rbm model raw input x visible layer 2 use first layer obtain representation input used data second layer two common solution exist representation chosen mean activation ùëÉ 1 sample ùëÉ 3 train second layer rbm taking formed data sample mean activation training example visible layer rbm 4 iterate step 2 3 desired number layer time propagating upward either sample mean value 5 parameter deep architecture respect proxy dbn likelihood respect supervised training criterion adding extra learning machinery convert learned representation supervised prediction linear classifier two main advantage greedy learning process thedbns 40 ittackles challenge appropriate selection parameter some case lead poor local optimum thereby ensuring work appropriately initialized second no ment labelled data since process unsupervised nevertheless dbns also plagued number coming computational cost associated training dbn fact step towards optimization network based maximum likelihood training approximation unclear 41 furthermore significant disadvantage dbns not account structure input image may significantly affect performance ity computer vision multimedia analysis problem however later variation dbn convolutional deep belief network cdbn 42 43 us spatial information neighboring pixel introducing tional rbms thus producing translation invariant ative model successfully scale come high dimensional image evidenced 44 deep boltzmann machine deep boltzmann machine dbms 45 another type deep model using rbm building block difference architecture dbns latter top two layer form undirected graphical model lower layer form directed erative model whereas dbm connection undirected dbms multiple layer hidden unit unit layer conditionally dent layer vice versa result inference dbm generally intractable nonetheless appropriate selection interaction visible hidden unit lead tractable version model network training dbm jointly train layer specific unsupervised model instead maximizing likelihood directly dbm us stochastic maximum likelihood sml 46 based algorithm maximize lower 6 computational intelligence neuroscience bound likelihood process would seem able falling poor local minimum 45 leaving several unit effectively dead instead greedy training strategy wa proposed 47 essentially consists pretraining layer dbm similarly dbn namely stacking rbms training layer independently model output previous layer followed final joint tuning regarding advantage dbms capture many layer complex representation input data appropriate unsupervised learning since trained unlabeled data also tuned particular task supervised fashion one attribute set dbms apart deep model approximate inference process dbms includes apart usual process back thus incorporating uncertainty input effective manner furthermore dbms following approximate gradient variational lower bound likelihood objective one jointly optimize parameter layer beneficial especially case learning model heterogeneous data originating different modality 48 far drawback dbms concerned one important one mentioned high computational cost inference almost prohibitive come joint optimization sizeable datasets several method proposed improve ness dbms include accelerating inference using separate model initialize value hidden unit layer 47 49 improvement pretraining stage 50 51 training stage 52 53 stacked denoising autoencoders stacked coder use autoencoder main building block similarly way deep belief network use restricted boltzmann machine component therefore important briefly present basic autoencoder ing version describing deep learning architecture stacked denoising autoencoders autoencoders autoencoder trained encode input x representation r x way input reconstructed r x 33 target output coder thus autoencoder input hence output vector dimensionality input vector course process reconstruction error minimized corresponding code learned feature one linear hidden layer mean squared error criterion used train network ùëò hidden unit learn project input span first ùëòprincipal component data 54 hidden layer nonlinear autoencoder behaves differently pca ability capture multimodal aspect input distribution 55 parameter model optimized average reconstruction error minimized many alternative measure reconstruction error including traditional squared error hidden node reconstruct error reconstruction input corrupted input figure 3 denoising autoencoder 56 r x 10 function f decoder f r x tion produced model input interpreted bit vector vector bit probability loss function reconstruction could represented ùëñ xùëñlog fùëñ r x 1 log 1 r x 11 goal representation code r x distributed representation manages capture dinates along main variation data similarly principle principal component analysis pca given r x not lossless impossible constitute successful compression input aforementioned optimization process result low reconstruction error test example distribution training example generally high reconstruction error sample arbitrarily chosen input space denoising autoencoders denoising autoencoder 56 stochastic version autoencoder input stochastically corrupted uncorrupted input still used target reconstruction simple term two main aspect function denoising coder first try encode input namely preserve information input second try undo effect corruption process stochastically applied input autoencoder see figure 3 latter only done capturing statistical dependency input shown denoising autoencoder imizes lower bound generative model 56 stochastic corruption process arbitrarily set number input zero denoising autoencoder trying predict corrupted value uncorrupted one randomly selected subset missing pattern essence ability predict any subset variable remaining one sufficient condition completely capturing joint distribution set variable mentioned using autoencoders denoising wa introduced earlier work 57 substantial contribution 56 lie demonstration ful use method unsupervised pretraining deep architecture linking denoising autoencoder generative model computational intelligence neuroscience 7 stacked denoising autoencoders possible stack denoising autoencoders order form deep network feeding latent representation output code ing autoencoder layer input current layer unsupervised pretraining architecture done one layer time layer trained denoising autoencoder minimizing error reconstructing input output code previous layer first ùëòlayers trained train 1 th layer since possible compute latent tion layer underneath pretraining layer completed network go second stage training called supervised considered goal optimize prediction error supervised task end logistic regression layer added output code output layer network derived network trained like multilayer perceptron considering only encoding part autoencoder point stage supervised since target class taken account training easily seen principle training stacked encoders one previously described deep belief network using autoencoders instead restricted boltzmann machine number comparative experimental study show deep belief network tend outperform stacked autoencoders 58 59 not always case especially dbns compared stacked denoising autoencoders 56 one strength autoencoders basic unsupervised component deep architecture unlike rbms allow almost any parametrization layer condition training criterion continuous parameter contrast one shortcoming sa not correspond generative model generative model like rbms dbns sample drawn check output learning process discussion some strength limitation presented deep learning model already discussed respective subsection attempt compare el summary see table 2 say cnns generally performed better dbns current literature benchmark computer vision datasets mnist case input nonvisual dbns often outperform model difficulty accurately estimating joint probability well computational cost creating dbn constitutes drawback major positive aspect cnns feature learning bypassing handcrafted feature necessary type network however cnns feature automatically learned hand cnns rely availability ground truth labelled training data whereas sa not limitation work unsupervised manner different note one disadvantage autoencoders lie fact could become tive error present first layer error may cause network learn reconstruct average training data denoising autoencoders 56 however table 2 comparison cnns sdas respect number property denotes good performance property bad performance complete lack thereof model property cnns sdas unsupervised learning training efficiency feature learning invariance generalization retrieve correct input corrupted version thus ing network grasp structure input tion term efficiency training process only case sa training possible whereas cnns training process finally one strength cnns fact invariant transformation translation scale rotation invariance translation rotation scale one important asset cnns especially computer vision problem object detection allows abstracting object identity category specific visual input relative camera object thus enabling network tively recognize given object case actual pixel value image significantly differ application computer vision section survey work leveraged deep learning method address key task computer vision object detection face recognition action activity recognition human pose estimation object detection object detection process detecting instance semantic object certain class human airplane bird digital image video figure 4 common approach object detection framework includes creation large set candidate window sequel classified using cnn feature example method described 32 employ selective search 60 derive object proposal extract cnn feature proposal feed feature svm classifier decide whether window include object not large number work based concept region cnn feature proposed 32 approach following region cnn paradigm usually good detection accuracy 61 62 however significant number method trying improve performance region cnn approach some succeed finding approximate object position often not precisely determine exact position object 63 end method often follow joint object segmentation approach usually attaining good result vast majority work object detection using deep learning apply variation cnns example 8 67 68 8 computational intelligence neuroscience b c figure 4 object detection result comparison 66 ground truth b bounding box obtained 32 c bounding box obtained 66 new layer new learning strategy proposed 9 weakly supervised cascaded cnns 69 cnns however doe exist relatively small number object detection attempt using deep model example 70 proposes coarse object locating method based saliency mechanism conjunction dbn object detection remote sensing image 71 present new dbn object nition model mann machine trained using hybrid algorithm bine generative discriminative gradient 72 employ fused deep learning approach 73 explores representation capability deep model pervised paradigm finally 74 leverage stacked coder multiple organ detection medical image 75 exploit stacked autoencoders based salient object detection face recognition face recognition one hottest computer vision application great commercial interest well variety face recognition system based extraction handcrafted feature proposed 79 case feature extractor extract feature aligned face obtain representation based classifier make prediction cnns brought change face recognition field thanks feature learning transformation invariance property first work employing cnns face recognition wa 80 today light cnns 81 vgg face descriptor 82 among state art 44 convolutional dbn achieved great performance face verification moreover google facenet 83 facebook face 84 based cnns deepface 84 model face aligns appear frontal face normalized input fed single convolution filter followed three locally connected layer two fully connected layer used make final tions although deepface attains great performance rate representation not easy interpret face person not necessarily clustered training process hand facenet defines triplet loss function representation make training process learn cluster face representation person furthermore cnns constitute core openface 85 face recognition tool comparable albeit little lower accuracy suitable mobile computing smaller size fast execution time action activity recognition human action activity recognition research issue ha received lot attention researcher 86 87 many work human activity recognition based deep learning technique proposed literature last year 88 89 deep learning wa used complex event detection recognition video sequence first saliency map used computational intelligence neuroscience 9 detecting localizing event deep learning wa applied pretrained feature identifying important frame correspond underlying event 90 author successfully employ approach activity recognition beach volleyball ilarly approach 91 event classification video datasets 92 cnn model used activity recognition based smartphone sensor data author 12 incorporate bound ularization term deep cnn model effectively improves generalization performance cnn activity classification 13 author scrutinize cability cnn joint feature extraction classification model activity find due challenge large intraclass variance small interclass ances limited training sample per activity approach directly us deep feature learned imagenet svm classifier preferable driven adaptability model availability variety different sensor increasingly popular strategy human activity recognition consists fusing multimodal feature data 93 author mixed appearance motion feature recognizing group activity crowded scene collected web combination different modality author applied multitask deep learning work 94 explores tion heterogeneous feature complex event recognition problem viewed two different task first informative feature recognizing event estimated different feature combined using graph structure also number work combining one type model apart several data ities 95 author propose multimodal multistream deep learning framework tackle egocentric activity recognition problem using video sensor data employing dual cnns long memory architecture multimodal fusion combined cnn lstm architecture also proposed 96 finally 97 us dbns activity recognition using input video sequence also include depth information human pose estimation goal human pose mation determine position human joint image image sequence depth image skeleton data provided motion capturing hardware 98 human pose estimation challenging task owing vast range human silhouette appearance difficult illumination cluttered background era deep learning pose estimation wa based detection body part example pictorial structure 99 moving deep learning method human pose estimation group holistic method depending way input image cessed holistic processing method tend accomplish task global fashion not explicitly define model individual part spatial relationship deeppose 14 holistic model formulates human pose estimation method joint regression problem doe not explicitly define graphical model part tor human pose estimation nevertheless based method tend plagued inaccuracy region due difficulty learning direct regression complex pose vector image hand processing method focus detecting human body part individually lowed graphic model incorporate spatial tion 15 author instead training network using whole image use local part patch background patch train cnn order learn conditional ability part presence spatial relationship 100 approach train multiple smaller cnns perform independent binary classification followed weak spatial model remove strong outlier enforce global pose consistency finally 101 resolution cnn designed perform likelihood regression body part followed implicit graphic model promote joint consistency datasets applicability deep learning approach ha evaluated numerous datasets whose content varied greatly according application scenario regardless investigated case main application domain natural image brief description utilized datasets traditional new one benchmarking purpose provided 1 grayscale image used grayscale image dataset mnist 20 variation nist perturbed nist application scenario recognition written digit 2 rgb natural image caltech rgb image datasets 102 example caltech 256 caltech houettes contain picture object belonging category cifar datasets 103 consist thousand 32 32 color image various class coil datasets 104 consist different object imaged every angle 360 rotation 3 hyperspectral image scien hyperspectral image data 105 aviris sensor based datasets 106 example contain hyperspectral image 4 facial characteristic image adience benchmark dataset 107 used facial attribute identification age gender image face face recognition unconstrained environment 108 another commonly used dataset 5 medical image chest dataset 109 comprises 112120 image 30805 unique patient fourteen disease image label image multilabels lymph node detection segmentation datasets 110 consist computed phy image mediastinum abdomen 6 video stream wr datasets 111 112 used activity recognition assembly line 113 containing sequence 7 category industrial task 114 dataset 8 million youtube video url along label diverse set 4800 knowledge graph entity 10 computational intelligence neuroscience conclusion surge deep learning last year great tent due stride ha enabled field computer vision three key category deep learning computer vision reviewed paper namely cnns boltzmann family including dbns dbms sdas employed achieve significant performance rate variety visual understanding task object detection face recognition action activity recognition human pose estimation image retrieval semantic mentation however category ha distinct advantage disadvantage cnns unique capability feature learning automatically learning feature based given dataset cnns also invariant formation great asset certain computer vision application hand heavily rely existence labelled data contrast sdas work unsupervised fashion model investigated cnns putationally demanding come training whereas sdas trained real time certain circumstance closing note spite some case documented ture significant challenge remain especially far theoretical groundwork would clearly explain way define optimal selection model type structure given task profoundly comprehend reason specific architecture algorithm effective given task not among tant issue continue attract interest machine learning research community year come conflict interest author declare no conflict interest regarding publication paper acknowledgment research implemented iky scholarship gramme cofinanced european union european social greek national fund action titled reinforcement postdoctoral researcher framework operational programme human resource development program education lifelong learning national strategic reference framework nsrf reference 1 mcculloch pitt logical calculus idea immanent nervous activity bulletin mathematical biology vol 5 no 4 pp 1943 2 lecun boser denker et handwritten digit nition network advance neural information processing system 2 nip 89 touretzky denver co usa 1990 3 hochreiter schmidhuber long memory neural computation vol 9 no 8 pp 1997 4 hinton osindero teh fast learning algorithm deep belief net neural computation vol 18 no 7 pp 2006 5 tensorflow available online 6 frederic lamblin pascanu et theano new feature speed improvement deep learning vised feature learning nip 2012 workshop 2012 7 mxnet available online 8 ouyang zeng wang et object detection deformable part based convolutional neural network ieee transaction pattern analysis machine intelligence vol 39 no 7 pp 2017 9 diba sharma pazandeh pirsiavash gool weakly supervised cascaded convolutional network proceeding 2017 ieee conference computer vision pattern recognition cvpr pp honolulu hi july 2017 10 doulamis voulodimos fast adaptive supervised training deep learning model consistent object tracking classification proceeding 2016 ieee international conference imaging system technique ist 2016 pp october 2016 11 doulamis adaptable deep learning structure object dynamic visual environment medium tool application pp 2017 12 lin wang zuo wang luo zhang deep structured model bound human activity recognition international journal computer vision vol 118 no 2 pp 2016 13 cao nevatia exploring deep learning based solution fine grained activity recognition wild proceeding 2016 international conference pattern recognition icpr pp cancun december 2016 14 toshev szegedy deeppose human pose estimation via deep neural network proceeding ieee conference computer vision pattern recognition cvpr 2014 pp usa june 2014 15 chen yuille articulated pose estimation graphical model image dependent pairwise relation proceeding nip 2014 16 noh hong han learning deconvolution network semantic segmentation proceeding ieee international conference computer vision iccv 2015 pp santiago chile december 2015 17 long shelhamer darrell fully convolutional work semantic segmentation proceeding ieee conference computer vision pattern recognition cvpr 15 pp ieee boston mass usa june 2015 18 hubel wiesel receptive field binocular interaction functional architecture cat visual cortex journal physiology vol 160 pp 1962 19 fukushima neocognitron neural work model mechanism pattern recognition unaffected shift position biological cybernetics vol 36 no 4 pp 1980 20 lecun bottou bengio haffner learning applied document recognition proceeding ieee vol 86 no 11 pp computational intelligence neuroscience 11 21 lecun boser denker et backpropagation applied handwritten zip code recognition neural computation vol 1 no 4 pp 1989 22 tygert bruna chintala lecun piantino szlam mathematical motivation lutional network neural computation vol 28 no 5 pp 825 2016 23 oquab bottou laptev sivic object tion free learning convolutional neural network proceeding ieee conference computer vision pattern recognition cvpr 2015 pp 694 june 2015 24 szegedy liu jia et going deeper tions proceeding ieee conference computer vision pattern recognition cvpr 15 pp boston mass usa june 2015 25 boureau ponce lecun theoretical analysis feature pooling visual recognition proceeding icml 2010 26 scherer uller behnke evaluation pooling operation convolutional architecture object tion lecture note computer science including subseries lecture note artificial intelligence lecture note bioinformatics preface vol 6354 no 3 pp 2010 27 wu gu dropout regularization convolutional neural network neural information cessing vol 9489 lecture note computer science pp 54 springer international publishing cham 2015 28 zhang ren sun spatial pyramid pooling deep convolutional network visual recognition puter vision eccv 2014 vol 8691 lecture note computer science pp springer international publishing cham 2014 29 zhang ren sun spatial pyramid pooling convolutional network visual recognition ieee tions pattern analysis machine intelligence vol 37 no 9 pp 2015 30 ouyang wang zeng et deformable deep convolutional neural network object detection proceeding ieee conference computer vision pattern recognition cvpr 2015 pp usa june 2015 31 krizhevsky sutskever hinton imagenet cation deep convolutional neural network proceeding annual conference neural information processing system nip 12 pp lake tahoe nev usa december 2012 32 girshick donahue darrell malik rich ture hierarchy accurate object detection semantic segmentation proceeding ieee conference computer vision pattern recognition cvpr 14 pp 587 columbus ohio usa june 2014 33 bengio learning deep architecture ai foundation trend machine learning vol 2 no 1 pp 2009 34 smolensky information processing dynamical system foundation harmony theory parallel distributed processing exploration microstructure cognition vol 1 pp mit press cambridge usa 1986 35 hinton sejnowski learning relearning boltzmann machine vol 1 mit press cambridge 1986 36 hinton contrastive divergence learning proceeding tenth international workshop artificial intelligence np society artificial intelligence statistic pp 2005 37 hinton practical guide training restricted boltzmann machine momentum vol 9 926 2010 38 cho raiko ilin enhanced gradient training restricted boltzmann machine neural computation vol 25 no 3 pp 2013 39 hinton salakhutdinov reducing sionality data neural network american association advancement science science vol 313 no 5786 pp 2006 40 arel rose karnowski deep machine new frontier artificial intelligence research ieee computational intelligence magazine vol 5 no 4 pp 2010 41 bengio courville vincent representation ing review new perspective ieee transaction pattern analysis machine intelligence vol 35 no 8 pp 2013 42 lee grosse ranganath ng convolutional deep belief network scalable unsupervised learning hierarchical representation proceeding annual international conference icml 09 pp acm treal canada june 2009 43 lee grosse ranganath ng unsupervised learning hierarchical representation convolutional deep belief network communication acm vol 54 no 10 pp 2011 44 huang lee learning chical representation face verification convolutional deep belief network proceeding ieee conference computer vision pattern recognition cvpr 12 pp 2525 june 2012 45 salakhutdinov hinton deep boltzmann machine proceeding international conference artificial intelligence statistic vol 24 pp 2009 46 younes convergence markovian stochastic rithms rapidly decreasing ergodicity rate stochastics stochastics report vol 65 no pp 1999 47 salakhutdinov larochelle efficient learning deep boltzmann machine proceeding aistats 2010 48 srivastava salakhutdinov multimodal learning deep boltzmann machine journal machine learning research vol 15 pp 2014 49 salakhutdinov hinton efficient learning dure deep boltzmann machine neural computation vol 24 no 8 pp 2012 50 salakhutdinov hinton better way pretrain deep boltzmann machine proceeding annual ence neural information processing system 2012 nip 2012 pp usa december 2012 51 cho raiko ilin karhunen ing algorithm deep boltzmann machine lecture note computer science including subseries lecture note artificial intelligence lecture note bioinformatics preface vol 8131 pp 2013 52 montavon uller deep boltzmann machine centering trick neural network trick trade vol 7700 lecture note computer science pp springer berlin heidelberg berlin heidelberg 2012 12 computational intelligence neuroscience 53 goodfellow mirza courville et deep boltzmann machine proceeding nip 2013 54 bourlard kamp multilayer ceptrons singular value decomposition biological netics vol 59 no pp 1988 55 japkowicz hanson gluck nonlinear association not equivalent pca neural computation vol 12 no 3 pp 2000 56 vincent larochelle bengio manzagol tracting composing robust feature denoising coder proceeding international conference machine learning icml 08 cohen mccallum roweis pp acm 2008 57 gallinari lecun thiria oires associatives distribuees proceeding ings cognitiva 87 paris la villette 1987 58 larochelle erhan courville bergstra bengio empirical evaluation deep architecture problem many factor variation proceeding international conference machine learning icml 07 pp corvallis ore ua june 2007 59 bengio lamblin popovici larochelle greedy training deep network advance neural information processing system sch platt hoffman olkopf vol 19 pp mit press 2007 60 uijlings van de sande gevers smeulders selective search object recognition tional journal computer vision vol 104 no 2 pp 2013 61 girshick fast proceeding ieee international conference computer vision iccv 15 pp december 2015 62 ren girshick sun faster towards object detection region proposal network ieee transaction pattern analysis machine intelligence vol 39 no 6 pp 2017 63 hosang benenson schiele good tion proposal really proceeding british machine vision conference bmvc 2014 gbr september 2014 64 hariharan aez girshick malik ous detection segmentation computer 2014 vol 8695 lecture note computer science pp 312 springer 2014 65 dong chen yan yuille towards unified object detection semantic segmentation lecture note computer science including subseries lecture note artificial intelligence lecture note bioinformatics preface vol 8693 no 5 pp 2014 66 zhu urtasun salakhutdinov fidler segdeepm exploiting segmentation context deep neural network object detection proceeding ieee conference computer vision pattern recognition cvpr 2015 pp 4711 usa june 2015 67 liu lay wei et colitis detection abdominal ct scan rich feature hierarchy proceeding medical imaging 2016 diagnosis vol 9785 ings spie san diego calif usa february 2016 68 luo wang dong zhang deep ing network right ventricle segmentation short axis mri proceeding 2016 computing cardiology conference 69 chen lu fan lutional network object detection ieee transaction pattern analysis machine intelligence 2017 70 diao sun zheng dou wang fu efficient object detection remote sensing image using deep belief network ieee geoscience remote sensing letter vol 13 no 2 pp 2016 71 nair hinton object recognition deep belief net proceeding nip 2009 72 doulamis doulamis fast adaptive deep fusion learning detecting visual object lecture note computer science including subseries lecture note artificial intelligence lecture note bioinformatics preface vol 7585 no 3 pp 2012 73 doulamis doulamis deep learning object tracking classification pp 74 shin orton collins doran leach stacked autoencoders unsupervised feature learning multiple organ detection pilot study using patient data ieee transaction pattern analysis machine intelligence vol 35 no 8 pp 2013 75 li xia chen benchmark dataset guided stacked autoencoders salient object detection ieee transaction image processing 2017 76 chen cao wen sun blessing dimensionality feature efficient compression face verification proceeding ieee conference computer vision pattern recognition cvpr 13 pp 3032 june 2013 77 cao wipf wen duan sun practical fer learning algorithm face verification proceeding ieee international conference computer vision iccv 13 pp december 2013 78 berg belhumeur classifier alignment face verification proceeding british machine vision conference bmvc 12 pp september 2012 79 chen cao wang wen sun bayesian face revisited joint formulation computer 2012 european conference computer vision florence italy october 2012 proceeding part iii vol 7574 lecture note computer science pp springer berlin germany 2012 80 lawrence giles tsoi back face recognition convolutional approach ieee transaction neural network learning system vol 8 no 1 pp 1997 81 wu sun tan light cnn deep face resentation noisy label 82 parkhi vedaldi zisserman deep face nition proceeding british machine vision conference 2015 pp swansea 83 schroff kalenichenko philbin facenet unified embedding face recognition clustering proceeding ieee conference computer vision pattern nition cvpr 15 pp ieee boston mass usa june 2015 84 taigman yang ranzato wolf deepface ing gap performance face verification proceeding ieee conference computer vision pattern recognition cvpr 14 pp columbus ohio usa june computational intelligence neuroscience 13 85 amos ludwiczuk satyanarayanan openface face recognition library mobile tions cmu school computer science 2016 86 voulodimos kosmopoulos doulamis varvarigou approach concurrent activity recognition multimedia tool application vol 69 no 2 pp 2014 87 voulodimos doulamis kosmopoulos varvarigou improving activity recognition employing neural network based readjustment applied ficial intelligence vol 26 no pp 2012 88 makantasis doulamis doulamis psychas deep learning based human behavior recognition trial workflow proceeding ieee international conference image processing icip 2016 pp september 2016 89 gan wang yang yeung hauptmann devnet deep event network multimedia event tion evidence recounting proceeding ieee conference computer vision pattern recognition cvpr 2015 pp usa june 2015 90 kautz groh hannink jensen strubberg eskofier activity recognition beach volleyball using deep convolutional neural network leveraging potential deep learning sport data mining knowledge discovery vol 31 no 6 pp 2017 91 karpathy toderici shetty leung sukthankar li video classification convolutional neural network proceeding ieee conference computer vision pattern recognition cvpr 14 pp 1732 columbus oh usa june 2014 92 ronao cho human activity recognition smartphone sensor using deep learning neural network expert system application vol 59 pp 2016 93 shao loy kang wang crowded scene understanding deeply learned volumetric slice ieee transaction circuit system video technology vol 27 no 3 pp 2017 94 tang yao koller combining right feature complex event recognition proceeding 2013 ieee international conference computer vision iccv 2013 pp australia december 2013 95 song chandrasekhar mandal et multimodal stream deep learning egocentric activity recognition proceeding ieee conference computer vision pattern recognition workshop cvprw 2016 pp usa july 2016 96 kavi kulathumani rohit kecojevic multiview fusion activity recognition using deep neural network journal electronic imaging vol 25 no 4 article id 043010 2016 97 yalcin human activity recognition using deep belief work proceeding signal processing munication application conference siu 2016 pp tur may 2016 98 kitsikidis dimitropoulos douka grammalidis dance analysis using multiple kinect sensor proceeding international conference computer vision theory application visapp 2014 pp prt january 2014 99 felzenszwalb huttenlocher pictorial tures object recognition international journal computer vision vol 61 no 1 pp 2005 100 jain tompson andriluka learning human pose estimation feature convolutional network ings iclr 2014 101 tompson jain lecun et joint training volutional network graphical model human pose estimation proceeding nip 2014 102 fergus perona learning object category ieee transaction pattern analysis machine intelligence vol 28 no 4 pp 2006 103 krizhevsky hinton learning multiple layer tures tiny image 2009 104 nene nayar murase columbia object image library 1996 105 skauli farrell collection hyperspectral image imaging system research proceeding digital raphy ix usa february 2013 106 baumgardner biehl landgrebe 220 band aviris hyperspectral image data set june 12 1992 indian pine test site 3 datasets 2015 107 eidinger enbar hassner age gender tion unfiltered face ieee transaction information ensics security vol 9 no 12 pp 2014 108 huang ramesh berg labeled face wild database studying face nition unconstrained environment tech university massachusetts amherst 2007 109 wang peng lu lu bagheri summer chest database mark classification localization common thorax disease proceeding 2017 ieee ference computer vision pattern recognition cvpr pp honolulu hi may 2017 110 seff lu barbu roth shin mers leveraging semantic boundary cue mated lymph node detection lecture note computer science including subseries lecture note artificial intelligence lecture note bioinformatics preface vol 9350 pp 61 2015 111 voulodimos kosmopoulos vasileiou et dataset workflow recognition industrial scene proceeding 2011 ieee international conference image processing icip 2011 pp belgium september 2011 112 voulodimos kosmopoulos vasileiou et fold dataset activity workflow recognition complex industrial environment ieee multimedia vol 19 no 3 pp 2012 113 kosmopoulos voulodimos doulamis system multicamera task recognition summarization structured environment ieee transaction industrial informatics vol 9 no 1 pp 2013 114 et video sification benchmark tech 2016 computer game technology international journal hindawi volume 2018 hindawi journal engineering volume 2018 advance fuzzy system hindawi volume 2018 international journal reconÔ¨Ågurable computing hindawi volume 2018 hindawi volume 2018 applied computational intelligence soft computing advance artificial intelligence hindawi volume 2018 hindawi volume 2018 civil engineering advance hindawi volume 2018 electrical computer engineering journal journal computer network communication hindawi volume 2018 hindawi volume 2018 advance multimedia international journal biomedical imaging hindawi volume 2018 hindawi volume 2018 engineering mathematics international journal robotics journal hindawi volume 2018 hindawi volume 2018 computational intelligence neuroscience hindawi volume 2018 mathematical problem engineering modelling simulation engineering hindawi volume 2018 hindawi publishing corporation volume 2013 hindawi scientific world journal volume 2018 hindawi volume 2018 interaction advance hindawi volume 2018 scientifc programming submit manuscript