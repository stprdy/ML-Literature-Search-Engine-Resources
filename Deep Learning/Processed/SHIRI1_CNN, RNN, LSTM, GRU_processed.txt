comprehensive overview comparative analysis deep learning model cnn rnn lstm gru farhad mortezapour university putra malaysia upm malaysia thinagaran perumal university putra malaysia upm malaysia norwati mustapha university putra malaysia upm malaysia raihani mohamed university putra malaysia upm malaysia abstract deep learning dl ha emerged powerful subset machine learning ml artificial intelligence ai outperforming traditional ml method especially handling unstructured large datasets impact span across various domain including speech recognition healthcare autonomous vehicle cybersecurity predictive analytics however complexity dynamic nature problem present challenge designing effective deep learning model consequently several deep learning model developed address different problem application article conduct comprehensive survey various deep learning model including convolutional neural network cnns recurrent neural network rnns generative model deep reinforcement learning drl deep transfer learning examine structure application benefit limitation model furthermore perform analysis using three publicly available datasets imdb ara compare performance six renowned deep learning model cnn simple rnn long memory lstm bidirectional lstm gated recurrent unit gru bidirectional gru keywords deep learning machine learning convolutional neural network cnn recurrent neural network rnn long memory lstm gated recurrent unit gru generative model autoencoder ae generative adversarial network gan deep reinforcement learning drl deep transfer learning introduction artificial intelligence ai aim emulate intelligence machine computer science ai refers study intelligent agent object capable perceiving environment taking action maximize chance achieving specific goal 1 machine learning ml field focus development application method capable learning datasets 2 ml find extensive use various domain speech recognition computer vision text analysis video game medical science cybersecurity deep learning dl subset machine learning excels processing unstructured data currently deep learning method outperform traditional machine learning approach 3 deep learning model draw inspiration structure functionality human nervous system brain model employ input hidden output layer organize processing unit within layer node unit interconnected layer 1 connection assigned weight value unit sum input multiplying corresponding weight 4 figure 1 illustrates relationship ai ml dl highlighting machine learning deep learning subfields artificial intelligence objective research provide overview various deep learning model compare performance across different application section 2 discus different deep learning model including perceptron mlp convolutional neural network cnn recurrent neural network rnn generative model deep reinforcement learning drl deep transfer learning section 3 conduct experiment analyze six deep learning model namely convolutional neural network cnn simple recurrent neural network rnn long memory lstm bidirectional lstm gated recurrent unit gru 1 bidirectional gru using three datasets finally section 4 concludes paper figure relationship artificial intelligence machine learning deep learning deep learning model deep learning dl involves process learning hierarchical representation data utilizing architecture multiple hidden layer advancement performance computing facility deep learning technique using deep neural network gained increasing popularity 5 deep learning algorithm data passed multiple layer layer progressively extracting feature transmitting information subsequent layer initial layer extract characteristic combined later layer form comprehensive representation 3 traditional machine learning technique classification task typically involves sequential process includes feature extraction meticulous feature selection learning classification effectiveness machine learning method heavily relies accurate feature selection biased feature selection lead incorrect class classification contrast deep learning model enable simultaneous learning classification eliminating need separate step capability make deep learning particularly advantageous automating feature learning across diverse task 6 figure 2 visually illustrates distinction deep learning traditional machine learning term feature extraction learning era deep learning wide array method architecture developed model broadly categorized two main group discriminative supervised generative unsupervised approach among discriminative model two prominent group convolutional neural network cnns recurrent neural network rnns 7 additionally generative approach encompass various model generative adversarial network gans aes following section provide comprehensive survey different type deep learning model figure visual illustration distinction deep learning traditional machine learning term feature extraction learning 6 multi layer perceptron mlp perceptron mlp model type feedforward artificial neural network ann serf foundation architecture deep learning deep neural network dnns 7 operates supervised learning approach mlp consists three layer input layer output layer one hidden layer fully connected network meaning neuron one layer connected neuron subsequent layer mlp input layer receives input data performs feature normalization hidden layer vary number process input signal output layer make decision prediction based processed information 8 figure 3 depicts perceptron model activation function œÜ equation 1 linear function used map summation function ùëè output value ùë¶ ùúë ùëè 1 equation 1 term x w b represent input vector weighting vector bias output value respectively 9 figure 4 illustrates structure multi layer perceptron mlp model figure perceptron model 9 deep learning machine learning artificial intelligen figure structure multilayer perceptron mlp 9 convolutional neural network cnn convolutional neural network cnns powerful class deep learning model widely applied various task including object detection speech recognition computer vision image classification bioinformatics 10 also demonstrated success time series prediction task 11 cnns feedforward neural network leverage convolutional structure extract feature data 12 unlike traditional method cnns automatically learn recognize feature data without need manual feature extraction human 13 design cnns inspired visual perception 12 major component cnns include convolutional layer pooling layer fully connected layer 14 figure 5 present typical cnn architecture image classification task 15 convolutional layer convolutional layer pivotal component cnns multiple convolutional layer convolution operation extract distinct feature input image classification lower layer tend capture basic feature texture line edge higher layer extract abstract feature 16 convolutional layer comprises learnable convolution kernel weight matrix typically equal length width odd number kernel convolved input feature map sliding region feature map executing convolution operation 16 figure 6 illustrates schematic diagram convolution process figure cnn pipeline image classification 15 figure schematic diagram convolution process 16 pooling layer typically following convolutional layer pooling layer reduces number connection network performing dimensionality reduction input data 17 primary purpose alleviate computational burden address overfitting issue 18 moreover pooling layer enables cnns recognize object even shape distorted viewed different angle incorporating various dimension image pooling 19 pooling operation produce output feature map robust distortion error individual neuron 20 various pooling method including max pooling average pooling spatial pyramid pooling mixed pooling stochastic pooling figure 7 depicts example max pooling window slide across input content window processed pooling function 25 fully connected fc layer fc layer typically located end cnn architecture layer every neuron connected neuron preceding layer adhering principle conventional perceptron neural network fc layer receives input last pooling convolutional layer vector created flattening feature map fc layer serf classifier cnn enabling network make prediction 6 figure computing output value 3 3 max pooling operation 5 5 input 25 recurrent neural network rnn recurrent neural network rnns class deep learning model posse internal memory enabling capture sequential dependency unlike traditional neural network treat input independent entity rnns consider temporal order input making suitable task involving sequential information 26 employing loop rnns apply operation element series current computation depending current input previous computation 27 ability rnns utilize contextual information particularly valuable task natural language processing video classification speech recognition example language modeling understanding preceding word sentence crucial predicting next word rnns excel capturing dependency due recurrent nature however limitation simple rnns memory restricts ability retain information long sequence 31 overcome advanced rnn variant developed including long term memory lstm 32 bidirectional lstm 33 gated recurrent unit gru 34 bidirectional gru 35 bayesian rnn 36 others figure 8 depicts simple recurrent neural network internal memory ‚Ñéùë° computed using equation 2 37 ùëî ùëè 2 equation ùëî represents activation function typically hyperbolic tangent ùëà ùëä adjustable weight matrix hidden state ‚Ñé ùëè bias term ùë• denotes input vector rnns proven powerful model processing sequential data leveraging ability capture dependency time various type rnn model lstm bidirectional lstm gru bidirectional gru developed address specific challenge different application figure simple rnn internal operation 37 long memory lstm long memory lstm advanced variant recurrent neural network rnn address issue capturing dependency lstm wa initially introduced 32 1997 improved 38 2013 gaining significant popularity deep learning community compared standard rnns lstm model proven effective retaining utilizing information longer sequence 39 lstm network current input specific time step output previous time step fed lstm unit generates output passed next time step final hidden layer last time step sometimes along hidden layer commonly employed classification purpose 40 overall architecture lstm network depicted figure lstm consists three gate input gate forget gate output gate gate performs specific function controlling flow information input gate decides update internal state based current input previous internal state forget gate determines much previous internal state forgotten finally output gate regulates influence internal state system 27 figure 10 illustrates update mechanism within inner structure lstm update equation lstm unit expressed equation 3 ‚Ñé ùë° ùë° ùëì ‚Ñé ùë† ùë° ùë† ùëîùëì ùë° ùë† ùëîùëñ ùë° ùëì ùë† ùë§‚Ñé ùë¢ùëã ùë° ùëè ùëîùëñ ùë° ùë†ùëñùëîùëöùëúùëñùëë ùë§ùëñ‚Ñé ùë¢ùëñùëã ùë° ùëèùëñ 3 ùëîùëì ùë° ùë†ùëñùëîùëöùëúùëñùëë ùë§ùëì‚Ñé ùë¢ùëìùëã ùë° ùëèùëì ùëîùëú ùë° ùë†ùëñùëîùëöùëúùëñùëë ùë§ùëú‚Ñé ùë¢ùëúùëã ùë° ùëèùëú ùëì ‚Ñé ùëì ùë† represent activation function system state internal state typically utilizing hyperbolic tangent function gating operation denoted g feedforward neural network sigmoid activation function ensuring output value within range 0 1 interpreted set weight subscript ùëñ ùëú ùëì correspond input gate output gate forget gate respectively figure level architecture lstm model figure inner architecture standard lstm module 27 standard lstm ha demonstrated promising performance various task may struggle comprehend input structure complex sequential format address limitation lstm network known wa proposed 41 consists memory block comprising input gate two forget gate cell gate output gate exhibit superior performance challenging sequential modeling problem come higher computational complexity compared standard lstm 42 bidirectional lstm bidirectional long memory extension lstm architecture address limitation standard lstm model considering past future context sequence modeling task traditional lstm model process input data only forward direction overcomes limitation training model two direction forward backward 43 consists two parallel lstm layer one process input sequence forward direction process backward direction forward lstm layer read input data left right indicated green arrow figure simultaneously backward lstm layer read input data right left represented red arrow 44 bidirectional processing enables model capture information past future context allowing comprehensive understanding temporal dependency within sequence training phase forward backward lstm layer independently extract feature update internal state based input sequence output lstm layer time step prediction score prediction score combined using weighted sum generate final output result 44 incorporating information direction model capture broader context improve model ability model temporal dependency sequential data figure architecture bidirectional lstm model 44 ha widely applied various sequence modeling task natural language processing speech recognition sentiment analysis ha shown promising result capturing complex pattern dependency sequential data making popular choice task require understanding past future context gated recurrent unit gru gated recurrent unit gru another variant rnn architecture address memory issue offer simpler structure compared lstm 26 gru combine input gate forget gate lstm single update gate resulting streamlined design unlike lstm gru doe not include separate cell state gru unit consists three main component update gate reset gate current memory content gate enable gru selectively update utilize information previous time step allowing capture dependency sequence 45 figure 12 illustrates structure gru unit 46 figure structure gru unit 46 update gate equation 4 determines much past information retained combined current input specific time step computed based concatenation previous hidden state current input ùë•ùë° followed linear transformation sigmoid activation function ùúé ùëä ùëß ùë•ùë° ùëèùëß 4 reset gate equation 5 decides much past information forgotten computed similar manner update gate using concatenation previous hidden state current input ùëü ùúé ùëä ùëü ùë•ùë° ùëèùëü 5 current memory content equation 6 calculated based reset gate concatenation transformed previous hidden state current input result passed hyperbolic tangent activation function produce candidate activation ‚Ñé ùë°ùëéùëõ‚Ñé ùëä ‚Ñé ùëü ùë•ùë° 6 finally final memory state ‚Ñéùë° determined combination previous hidden state candidate activation equation 7 update gate determines balance previous hidden state candidate activation additionally output gate ùëúùë° introduced control information flow current memory content output equation 8 output gate computed using current memory state ‚Ñéùë° typically followed activation function sigmoid function 1 ùëßùë°‚Ñé 7 ùúéùëú ùëä ùëèùëú 8 weight matrix output layer ùëä ùëú bias vector output layer ùëèùëú gru offer simpler alternative lstm fewer tensor operation allowing faster training however choice gru lstm depends specific use case problem hand architecture advantage disadvantage performance may vary depending nature task 26 model supervised machine learning widely used artificial intelligence ai unsupervised learning remains active area research numerous unresolved question however recent advancement deep learning generative modeling injected new possibility unsupervised learning rapidly evolving domain within computer vision research generative model gm model leverage training data originating unknown distribution produce novel sample adhere distribution ultimate goal generative model generate data sample closely resemble real data distribution 47 various generative model developed applied different context 48 generative adversarial network gan 49 restricted boltzmann machine rbm 50 deep belief network dbn 51 map som 52 autoencoder concept autoencoder originated neural network designed reconstruct input data fundamental objective learn meaningful representation data unsupervised manner various application including clustering 48 autoencoder neural network aim replicate input output consists internal hidden layer defines code representing input data autoencoder network comprised two main component encoder function denoted ùëß ùëì ùë• decoder function generates reconstruction denoted ùëü ùëî ùëß 53 function ùëì ùë• transforms data point ùë• data space feature space function ùëî ùëß transforms ùëß feature space back data space reconstruct original data point modern autoencoders function ùëß ùëì ùë• ùëü ùëî ùëß considered stochastic function represented ùëùùëíùëõùëêùëúùëëùëíùëü ùëùùëëùëíùëõùëêùëúùëëùëíùëü respectively ùëü denotes reconstruction ùë• 54 figure 13 illustrates autoencoder model autoencoder model find utility various unsupervised learning task generative modeling 55 dimensionality reduction 56 feature extraction 57 anomaly outlier detection 58 denoising 59 general autoencoder model categorized two major group regularized autoencoders valuable learning representation subsequent classification task variational autoencoders 60 function generative model example regularized autoencoder model include sparse autoencoder sae 61 contractive autoencoder cae 62 denoising autoencoder dae 63 variational autoencoder vae generative model employ probabilistic distribution mean variance gaussian distribution data generation 48 vaes provide principled framework learning deep model associated inference model vae consists two coupled independently parameterized model encoder recognition model decoder generative model expectation maximization learning iteration generative model receives approximate posterior estimation latent random variable recognition model us update parameter conversely generative model act scaffold recognition model enabling learn meaningful representation data potential class label term bayes rule recognition model roughly inverse generative model 64 figure structure autoencoders 48 generative adversarial network gan notable neural network architecture generative modeling capable producing realistic novel sample demand generative adversarial network gan initially proposed ian goodfellow 49 gan consists two key component generative model discriminative model generative model aim generate image resemble real one discriminative model aim differentiate real synthetic image model typically implemented using multilayer perceptrons 65 figure 14 depicts framework gan adversarial game played generator g discriminator generator updating gradient determined discriminator adaptive objective 66 previously mentioned gans operate based principle derived neural network utilizing training set input generate new data resembles training set case gans trained image data generate new image exhibiting characteristic following outline operation gan 67 generator created discriminative network generates content based real data distribution system undergoes training increase discriminator ability distinguish synthesized real candidate allowing generator better fool discriminator discriminator initially train using dataset training data training sample datasets repeatedly presented desired accuracy achieved generator trained process random input generate candidate deceive discriminator backpropagation employed update discriminator generator former improving ability identify real image latter becoming adept producing realistic synthetic image convolutional neural network cnns commonly used discriminator deconvolutional neural network utilized generative network figure framework gru 66 generative adversarial network gans introduced numerous application across various domain including image blending 68 object generation 69 face aging 70 medicine 71 72 steganography 73 image manipulation 74 text transfer 75 language speech synthesis 76 traffic control 77 video generation 78 furthermore several model developed based generative adversarial network gan framework address specific task model include coupled gan 65 markovian gan 79 evolutionary gan 66 unrolled gan 80 bayesian conditional gan 81 relativistic gan 82 laplacian gan gan 83 graph embedding gan 77 wasserstein gan wgan 84 boundary equilibrium gan began 85 deep reinforcement learning reinforcement learning rl machine learning approach deal sequential aiming map situation action way maximizes associated reward unlike supervised learning explicit instruction given system action rl framework learner known agent not provided explicit guidance action take timestep rl agent must explore trial error determine action yield highest reward 86 furthermore unlike supervised learning correct output obtained model updated based loss error rl us gradient without differentiable loss function teach model explore randomly learn make optimal decision 87 figure 15 depicts environment interaction reinforcement learning rl standard theoretical framework rl based markov decision process mdp extends concept markov process used model based state action reward 88 figure interaction rl 88 deep reinforcement learning combine making capability reinforcement learning perception function deep learning considered form real ai aligns closely human thinking figure 16 illustrates basic structure deep reinforcement learning deep learning process sensory input environment provides current state data reinforcement learning process link current state appropriate action evaluates value based anticipated reward 89 one renowned deep reinforcement learning model deep network dqn 90 directly learns policy input using convolutional neural network cnns common model deep reinforcement learning include double dqn 91 dueling dqn 92 monte carlo tree search mcts 93 deep reinforcement learning drl model find application various domain video game playing 94 robotic manipulation 95 image segmentation 96 video analysis 97 energy management 98 figure basic structure drl 89 deep transfer learning deep neural network significantly improved performance across various machine learning task application however achieving remarkable performance gain often requires large amount labeled data supervised learning relies capturing latent pattern within data 99 unfortunately certain specialized domain availability sufficient training data major challenge constructing quality annotated dataset costly 100 address issue limited training data transfer learning ha emerged crucial tool machine learning concept transfer learning find root educational psychology theory generalization suggests transferring knowledge one context another facilitated generalizing experience order achieve successful transfer need connection two learning task example someone ha learned play violin likely learn piano quickly due shared characteristic musical instrument 101 figure 17 depicts learning process transfer learning growing popularity deep neural network various field numerous deep transfer learning technique proposed deep transfer learning categorized four main type based technique employed 100 deep transfer learning deep transfer learning deep transfer learning deep transfer learning deep transfer learning involves selecting subset instance source domain assigning appropriate weight value selected instance supplement training set target domain algorithm tasktradaboost 102 103 approach based strategy deep transfer learning focus mapping instance source target domain new data space instance two domain exhibit similarity suitable training unified deep neural network successful method based approach include extend mmd 104 105 figure learning process transfer learning 100 deep transfer learning revolves around reusing portion network ha already trained source domain along network structure connection parameter transferring deep neural network employed target domain deep transfer learning aim identify transferable representation using generative adversarial network gans model applicable source target domain deep transfer learning technique proven effective overcoming challenge limited training data enabling knowledge transfer across domain facilitating improved performance various machine learning application analysis deep learning model experimental analysis utilized three publicly available datasets imdb 106 ara 107 108 purpose wa perform comparative analysis various deep learning model specifically examined six different model cnn simple rnn lstm bidirectional lstm gru bidirectional gru evaluating model testing data aimed ass performance using multiple evaluation metric accuracy precision recall imdb dataset stand internet movie database provides collection movie review categorized positive negative sentiment ara dataset comprising annotated sensor event human activity recognition task dataset consisting image various fruit type classification purpose analysis focused comparing performance different deep learning model across diverse datasets cnn model known convolutional neural network particularly effective image analysis task due ability capture spatial dependency simple rnn recurrent neural network suitable sequential data analysis lstm long memory gru gated recurrent unit model excel capturing dependency sequential data bidirectional lstm bidirectional gru model offer advantage processing information forward backward direction evaluate performance model employed assessment metric accuracy precision recall accuracy measure overall correctness model prediction precision evaluates proportion correctly predicted positive instance recall ass model ability correctly identify positive instance provides balanced measure precision recall ùëáùëõ ùêπùëõ 9 ùëáùëù ùêπùëù 10 ùëáùëù ùêπùëõ 11 2 ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ 12 ùëáùëù true positive ùëáùëõ true negative ùêπùëù false positive ùêπùëõ false negative 39 conducting comprehensive analysis using metric gain insight strength weakness deep learning model comparative evaluation enables u identify effective model specific datasets application ultimately advancing field deep learning practical application model imdb dataset imdb dataset widely used dataset sentiment analysis task consists movie review along corresponding binary sentiment polarity label dataset contains total review evenly split training sample testing sample equal distribution positive negative label instance sentiment reduce correlation review given movie only 30 review included dataset 106 positive review often contain word like great well love negative review frequently use word like bad ca however certain word one character well appear frequently positive negative review although usage may differ term frequency two sentiment class 40 analysis employed six different deep learning model sentiment classification using imdb dataset compare performance model utilized accuracy loss diagram diagram provide insight well model learning data help evaluating effectiveness sentiment classification task figure accuracy deep learning model imdb dataset figure accuracy deep learning model imdb dataset figure 18 accuracy diagram provides visual representation different deep learning model perform term accuracy training process also figure 19 show trend accuracy value validation set across multiple epoch model figure 20 illustrates loss diagram visual representation loss value training process six different model figure 21 diagram depicts variation loss value testing set evaluation process different model loss function measure discrepancy predicted sentiment label actual label experiment parameter used model set epoch 8 cnn model utilized 200 filter kernel size 3 rectified linear unit relu activation function simple rnn lstm gru model 128 unit employed along 20 dropout rate recurrent dropout hand model used 64 unit parameter setting architectural choice allow standardized comparison deep learning model imdb dataset facilitating analysis performance comparison accuracy loss value figure loss deep learning model imdb dataset figure loss deep learning model imdb dataset also table 1 show result different deep learning model imdb review dataset base various metrices including accuracy precision recall time training cpu table result different deep learning model imdb dataset model accuracy precision recall score time cnn rnn lstm gru based result provided concluded gru cnn model achieved best performance imdb review dataset sentiment analysis model demonstrated high accuracy classifying sentiment movie review however worth noting training time cnn model wa significantly le gru model suggests cnn model wa faster train compared gru model still achieving excellent performance additionally model also exhibited good accuracy sentiment classification required le training time gru model indicates model could favorable choice offering balance accuracy training efficiency overall result suggest gru cnn model effective deep learning model sentiment analysis imdb review dataset varying performance training time model ara dataset based provided information ara dataset 107 valuable resource automatic recognition human activity smart environment consists data stream collected two house period 60 day 20 binary sensor installed monitor resident activity dataset includes information 27 different activity performed resident sensor event recorded basis order analyze classify human activity ara dataset six deep learning model employed training data 10 day house b comprising sample used train model 4 day data amounting sample used testing figure 22 present accuracy diagram deep learning model figure 23 illustrates validation accuracy diagram also figure 24 show loss diagram figure 25 provides diagram deep learning model model trained fixed set parameter batch cnn model utilized 32 filter kernel size 7 activation function relu pooling size simple rnn lstm gru model configured 256 unit along 20 dropout recurrent dropout figure accuracy deep learning model ara dataset figure accuracy deep learning model ara dataset figure loss diagram deep learning model ara dataset figure loss deep learning model ara dataset also table 2 illustrates result experiment ara dataset various metrices including accuracy precision recall time training cpu table result deep learning model ara dataset model accuracy precision recall score time cnn rnn lstm gru based provided result observed recurrent model specifically lstm gru outperformed convolutional model cnn working data ara dataset finding consistent nature dataset involves temporal sequence sensor event among recurrent model gru demonstrated best performance term accuracy evaluation metric additionally advantage lower training time compared recurrent model indicating efficiency processing learning data result suggest task human activity recognition smart environment using ara dataset recurrent model like lstm gru due ability capture temporal dependency among model gru stand particularly efficient option achieving good performance requiring le training time model dataset analysis deep learning model dataset image classification task involved three model cnn lstm dataset consists image 81 different fruit class resolution pixel experiment subset 20 fruit class wa used image training image testing figure 26 show accuracy diagram three deep learning model figure 27 illustrates accuracy model also figure 28 illustrates loss diagram figure 29 providing diagram three deep learning model figure accuracy cnn lstm model 360 dataset figure accuracy cnn lstm model dataset figure loss diagram cnn lstm model dataset figure loss cnn lstm model dataset experimental setup included fixed number epoch 8 batch size 256 model cnn model 200 filter kernel size activation function employed furthermore table 3 show result experiment based different metrices including accuracy precision recall time training dataset cnn lstm model table result deep learning model dataset model accuracy precision recall score time cnn lstm based result appears cnn model outperformed lstm model term accuracy training time cnn model superior performance attributed ability effectively capture spatial feature image convolutional layer since image data not inherently sequential dependent recurrent model may not suited particular task furthermore faster training time cnn model attributed parallel processing nature allows efficient computation gpus accelerates training process overall result suggest image classification task dataset focus capturing spatial feature cnn model tend effective efficient compared recurrent model like lstm conclusion conclusion article provides extensive overview deep learning technology application machine learning artificial intelligence article cover various aspect deep learning including neural network mlp model different type deep learning model cnn rnn generative model drl transfer learning classification deep learning model allows better understanding specific application characteristic rnn model including lstm lstm gru particularly suited time series data due ability capture temporal dependency hand cnn model excel image data analysis effectively capturing spatial feature experiment conducted three public datasets namely imdb ara reinforce suitability specific deep learning model different data type result demonstrate cnn model performs exceptionally well image classification task rnn model lstm gru show strong performance time series analysis additionally gru model stand faster training time compared lstm attributed simplified architecture two gate instead three overall article highlight diverse application effectiveness deep learning model various domain emphasizes importance selecting appropriate deep learning model based nature data task hand insight gained experiment contribute better understanding strength weakness different deep learning model facilitating informed practical application reference 1 shinde shah review machine learning deep learning application 2018 fourth international conference computing communication control automation iccubea 2018 ieee pp 2 rathore mannepalli review machine learning technique application health care 2021 international conference advance technology management education icatme 2021 ieee pp 3 mathew amudha sivakumari deep learning technique overview advanced machine learning technology application proceeding amlta 2020 pp 2021 4 shrestha mahmood review deep learning algorithm architecture ieee access vol 7 pp 2019 5 wani bhat afzal khan advance deep learning springer 2020 6 alzubaidi et review deep learning concept cnn architecture challenge application future direction journal big data vol 8 pp 2021 7 sarker deep learning comprehensive overview technique taxonomy application research direction sn computer science vol 2 no 6 420 2021 8 gaikwad tiwari keskar shivaprakash efficient fpga implementation multilayer perceptron human activity classification ieee access vol 7 pp 2019 9 ke huang quality prediction injection molding using multilayer perceptron neural network polymer vol 12 no 8 1812 2020 10 tasdelen sen hybrid model classification scientific report vol 11 no 1 pp 2021 11 qin yu zhao applying convolutional neural network deep learning technology behavioural recognition intelligent video tehniƒçki vjesnik vol 25 no 2 pp 2018 12 li liu yang peng zhou survey convolutional neural network analysis application prospect ieee trans neural netw learn syst vol 33 no 12 pp dec 2022 doi 13 mekruksavanich jitpattanakul deep convolutional neural network rnns complex activity recognition using wearable sensor data electronics vol 10 no 14 1685 2021 14 lu li wang qin method stock price prediction neural computing application vol 33 pp 2021 15 rawat wang deep convolutional neural network image classification comprehensive review neural computation vol 29 no 9 pp 2017 16 chen li bai yang jiang miao review image classification algorithm based convolutional neural network remote sensing vol 13 no 22 4712 2021 17 gu et recent advance convolutional neural network pattern recognition vol 77 pp 2018 18 ying overview overfitting solution journal physic conference series 2019 vol 1168 iop publishing 022022 19 ajit acharya samanta review convolutional neural network 2020 international conference emerging trend information technology engineering 2020 ieee pp 20 liu wang liu zeng liu alsaadi survey deep neural network architecture application neurocomputing vol 234 pp 2017 21 zhang ren sun spatial pyramid pooling deep convolutional network visual recognition ieee transaction pattern analysis machine intelligence vol 37 no 9 pp 2015 22 yu wang chen wei mixed pooling convolutional neural network rough set knowledge technology international conference rskt 2014 shanghai china october 2014 proceeding 9 2014 springer pp 23 gong wang guo lazebnik orderless pooling deep convolutional activation feature computer 2014 european conference zurich switzerland september 2014 proceeding part vii 13 2014 springer pp 24 zeiler fergus stochastic pooling regularization deep convolutional neural network arxiv preprint 2013 25 dumoulin visin guide convolution arithmetic deep learning arxiv preprint 2016 26 abbaspour fotouhi sedaghatbaf fotouhi vahabi linden comparative analysis hybrid deep learning model human activity recognition sensor vol 20 no 19 2020 doi 27 fang chen xue survey research sequence prediction algorithm journal big data vol 3 no 3 97 2021 28 xiao zhou research progress rnn language model 2020 ieee international conference artificial intelligence computer application icaica 2020 ieee pp 29 ng hausknecht vijayanarasimhan vinyals monga toderici beyond short snippet deep network video classification proceeding ieee conference computer vision pattern recognition 2015 pp 30 shewalkar nyavanandi ludwig performance evaluation deep neural network applied speech recognition rnn lstm gru journal artificial intelligence soft computing research vol 9 no 4 pp 2019 31 apaydin feizi sattari colak shamshirband chau comparative analysis recurrent neural network architecture reservoir inflow forecasting water vol 12 no 5 1500 2020 32 hochreiter schmidhuber long memory neural computation vol 9 no 8 pp 1997 33 graf liwicki fern√°ndez bertolami bunke schmidhuber novel connectionist system unconstrained handwriting recognition ieee transaction pattern analysis machine intelligence vol 31 no 5 pp 2008 34 chung gulcehre cho bengio empirical evaluation gated recurrent neural network sequence modeling arxiv preprint 2014 35 chen jiang zhang hierarchical bidirectional gru model attention emotion classification ieee access vol 7 pp 118540 2019 36 fortunato blundell vinyals bayesian recurrent neural network arxiv preprint 2017 37 kratzert klotz brenner schulz herrnegger modelling using long term memory lstm network hydrology earth system science vol 22 no 11 pp 2018 38 graf generating sequence recurrent neural network arxiv preprint 2013 39 shiri perumal mustapha mohamed ahmadon yamaguchi survey activity recognition smart environment 2023 40 minaee azimi abdolrashidi sentiment analysis using ensemble cnn model arxiv preprint 2019 41 zhu sobihani guo long memory recursive structure international conference machine learning 2015 pmlr pp 42 gu chung chignell valaee zhou liu survey deep learning human activity recognition acm computing survey csur vol 54 no 8 pp 2021 43 aldhyani alkahtani bidirectional long memory model algorithm predicting 19 gulf country life vol 11 no 11 1118 2021 44 liciotti bernardini romeo frontoni sequential deep learning application recognising human activity smart home neurocomputing vol 396 pp 2020 45 dutta kumar basu gated recurrent unit approach bitcoin price prediction journal risk financial management vol 13 no 2 23 2020 46 gumaei hassan alelaiwi alsalman hybrid deep learning model human activity recognition using multimodal body sensing data ieee access vol 7 pp 2019 doi 47 jabbar li omar survey generative adversarial network variant application training acm computing survey csur vol 54 no 8 pp 2021 48 bank koenigstein giryes autoencoders arxiv preprint 2020 49 goodfellow et generative adversarial network communication acm vol 63 no 11 pp 2020 50 zhang ding zhang xue overview restricted boltzmann machine neurocomputing vol 275 pp 2018 51 hinton deep belief network scholarpedia vol 4 no 5 5947 2009 52 kohonen map springer science business medium 2012 53 goodfellow bengio courville deep learning mit press 2016 54 zhai zhang chen autoencoder various variant 2018 ieee international conference system man cybernetics smc 2018 ieee pp 419 55 makhzani shlens jaitly goodfellow frey adversarial autoencoders arxiv preprint 2015 56 wang yao zhao based dimensionality reduction neurocomputing vol 184 pp 2016 57 kunang nurmaini stiawan zarkasi automatic feature extraction using autoencoder intrusion detection system 2018 international conference electrical engineering computer science icecos 2018 ieee pp 58 zhou paffenroth anomaly detection robust deep autoencoders proceeding acm sigkdd international conference knowledge discovery data mining 2017 pp 59 creswell bharath denoising adversarial autoencoders ieee transaction neural network learning system vol 30 no 4 pp 2018 60 kingma welling variational bayes arxiv preprint 2013 61 ng sparse autoencoder lecture note vol 72 no 2011 pp 2011 62 rifai et higher order contractive machine learning knowledge discovery database european conference ecml pkdd 2011 athens greece september 2011 proceeding part ii 22 2011 springer pp 63 vincent larochelle bengio manzagol extracting composing robust feature denoising autoencoders proceeding international conference machine learning 2008 pp 64 kingma welling introduction variational autoencoders foundation machine learning vol 12 no 4 pp 2019 65 liu tuzel coupled generative adversarial network advance neural information processing system vol 29 2016 66 wang xu yao tao evolutionary generative adversarial network ieee transaction evolutionary computation vol 23 no 6 pp 2019 67 aggarwal mittal battineni generative adversarial network overview theory application international journal information management data insight vol 1 no 1 100004 2021 68 chen kae toward realistic image compositing adversarial learning proceeding conference computer vision pattern recognition 2019 pp 69 jaiswal kumar badr towards artificial intelligence aided design approach application anime face generative adversarial network procedia computer science vol 168 pp 2020 70 liu li sun face aging generative adversarial network proceeding conference computer vision pattern recognition 2019 pp 71 islam zhang synthetic brain pet image generation brain informatics vol 7 pp 2020 72 lan initiative toga sepehrband conditional gan spectral normalization neuroimaging synthesis biorxiv 2020 73 zhang xu veeramachaneni steganogan high capacity image steganography gans arxiv preprint 2019 74 nam kim kim generative adversarial network manipulating image natural language advance neural information processing system vol 31 2018 75 sixt wild landgraf rendergan generating realistic labeled data frontier robotics ai vol 5 66 2018 76 lin li zhang sun adversarial ranking language generation advance neural information processing system vol 30 2017 77 xu wei peng xuan guo novel deep learning framework road traffic state estimation transportation research part c emerging technology vol 117 102635 2020 78 clark donahue simonyan adversarial video generation complex datasets arxiv preprint 2019 79 li wand precomputed texture synthesis markovian generative adversarial network computer 2016 european conference amsterdam netherlands october 2016 proceeding part iii 14 2016 springer pp 80 metz poole pfau unrolled generative adversarial network arxiv preprint 2016 81 zhao meyerand birn bayesian conditional gan mri brain image synthesis arxiv preprint 2020 82 relativistic discriminator key element missing standard gan arxiv preprint 2018 83 denton chintala fergus deep generative image model using laplacian pyramid adversarial network advance neural information processing system vol 28 2015 84 arjovsky chintala bottou wasserstein generative adversarial network international conference machine learning 2017 pmlr pp 85 berthelot schumm metz began boundary equilibrium generative adversarial network arxiv preprint 2017 86 vithayathil varghese mahmoud survey deep reinforcement learning electronics vol 9 no 9 1363 2020 87 le rathour yamazaki luu savvides deep reinforcement learning computer vision comprehensive survey artificial intelligence review pp 2022 88 puterman markov decision process discrete stochastic dynamic programming john wiley son 2014 89 zhang zhang qiu deep reinforcement learning power system application overview csee journal power energy system vol 6 no 1 pp 2019 90 mnih et control deep reinforcement learning nature vol 518 no 7540 pp 533 2015 91 van hasselt guez silver deep reinforcement learning double proceeding aaai conference artificial intelligence 2016 vol 30 no 1 92 wang schaul hessel hasselt lanctot freitas dueling network architecture deep reinforcement learning international conference machine learning 2016 pmlr pp 93 coulom efficient selectivity backup operator tree search computer game international conference cg 2006 turin italy may 31 revised paper 5 2007 springer pp 94 justesen bontrager togelius risi deep learning video game playing ieee transaction game vol 12 no 1 pp 2019 95 gu holly lillicrap levine deep reinforcement learning robotic manipulation asynchronous update 2017 ieee international conference robotics automation icra 2017 ieee pp 96 lee myeong song seednet automatic seed generation deep reinforcement learning robust interactive segmentation presented 2018 conference computer vision pattern recognition cvpr 2018 online available 0189 97 sahba deep reinforcement learning object segmentation video sequence 2016 international conference computational science computational intelligence csci 2016 ieee pp 98 shojaeighadikolaei ghasemi bardas ahmadi hashemi microgrid energy management using deep reinforcement learning 2021 north american power symposium nap 2021 ieee pp 99 long zhu wang jordan deep transfer learning joint adaptation network international conference machine learning 2017 pmlr pp 2217 100 tan sun kong zhang yang liu survey deep transfer learning artificial neural network machine 2018 international conference artificial neural network rhodes greece october 2018 proceeding part iii 27 2018 springer pp 101 zhuang et comprehensive survey transfer learning proceeding ieee vol 109 no 1 pp 76 2020 102 yao doretto boosting transfer learning multiple source 2010 ieee computer society conference computer vision pattern recognition 2010 ieee pp 103 pardoe stone boosting regression transfer proceeding international conference international conference machine learning 2010 pp 104 tzeng hoffman zhang saenko darrell deep domain confusion maximizing domain invariance arxiv preprint 2014 105 long cao wang jordan learning transferable feature deep adaptation network international conference machine learning 2015 pmlr pp 106 maas daly pham huang ng potts learning word vector sentiment analysis proceeding annual meeting association computational linguistics human language technology 2011 pp 107 alemdar ertan incel ersoy ara human activity datasets multiple home multiple resident 2013 international conference pervasive computing technology healthcare workshop 2013 ieee pp 108 mure≈üan oltean fruit recognition image using deep learning arxiv preprint 2017