comprehens overview compar analysi deep learn model cnn rnn lstm gru farhad mortezapour univers putra malaysia upm malaysia thinagaran perum univers putra malaysia upm malaysia norwati mustapha univers putra malaysia upm malaysia raihani moham univers putra malaysia upm malaysia abstract deep learn dl ha emerg power subset machin learn ml artifici intellig ai outperform tradit ml method especi handl unstructur larg dataset impact span across variou domain includ speech recognit healthcar autonom vehicl cybersecur predict analyt howev complex dynam natur problem present challeng design effect deep learn model consequ sever deep learn model develop address differ problem applic thi articl conduct comprehens survey variou deep learn model includ convolut neural network cnn recurr neural network rnn gener model deep reinforc learn drl deep transfer learn examin structur applic benefit limit model furthermor perform analysi use three publicli avail dataset imdb ara compar perform six renown deep learn model cnn simpl rnn long memori lstm bidirect lstm gate recurr unit gru bidirect gru keyword deep learn machin learn convolut neural network cnn recurr neural network rnn long memori lstm gate recurr unit gru gener model autoencod ae gener adversari network gan deep reinforc learn drl deep transfer learn introduct artifici intellig ai aim emul intellig machin comput scienc ai refer studi intellig agent object capabl perceiv environ take action maxim chanc achiev specif goal machin learn ml field focus develop applic method capabl learn dataset ml find extens use variou domain speech recognit comput vision text analysi video game medic scienc cybersecur deep learn dl subset machin learn excel process unstructur data current deep learn method outperform tradit machin learn approach deep learn model draw inspir structur function human nervou system brain model employ input hidden output layer organ process unit within layer node unit interconnect layer connect assign weight valu unit sum input multipli correspond weight figur illustr relationship ai ml dl highlight machin learn deep learn subfield artifici intellig object thi research provid overview variou deep learn model compar perform across differ applic section discuss differ deep learn model includ perceptron mlp convolut neural network cnn recurr neural network rnn gener model deep reinforc learn drl deep transfer learn section conduct experi analyz six deep learn model name convolut neural network cnn simpl recurr neural network rnn long memori lstm bidirect lstm gate recurr unit gru bidirect gru use three dataset final section conclud paper figur relationship artifici intellig machin learn deep learn deep learn model deep learn dl involv process learn hierarch represent data util architectur multipl hidden layer advanc perform comput facil deep learn techniqu use deep neural network gain increas popular deep learn algorithm data pass multipl layer layer progress extract featur transmit inform subsequ layer initi layer extract characterist combin later layer form comprehens represent tradit machin learn techniqu classif task typic involv sequenti process includ featur extract meticul featur select learn classif effect machin learn method heavili reli accur featur select bias featur select lead incorrect class classif contrast deep learn model enabl simultan learn classif elimin need separ step thi capabl make deep learn particularli advantag autom featur learn across divers task figur visual illustr distinct deep learn tradit machin learn term featur extract learn era deep learn wide array method architectur develop model broadli categor two main group discrimin supervis gener unsupervis approach among discrimin model two promin group convolut neural network cnn recurr neural network rnn addit gener approach encompass variou model gener adversari network gan ae follow section provid comprehens survey differ type deep learn model figur visual illustr distinct deep learn tradit machin learn term featur extract learn multi layer perceptron mlp perceptron mlp model type feedforward artifici neural network ann serv foundat architectur deep learn deep neural network dnn oper supervis learn approach mlp consist three layer input layer output layer one hidden layer fulli connect network mean neuron one layer connect neuron subsequ layer mlp input layer receiv input data perform featur normal hidden layer vari number process input signal output layer make decis predict base process inform figur depict perceptron model activ function φ equat linear function use map summat function 𝑏 output valu 𝑦 𝜑 𝑏 equat term x w b repres input vector weight vector bia output valu respect figur illustr structur multi layer perceptron mlp model figur perceptron model deep learn machin learn artifici intelligen figur structur multilay perceptron mlp convolut neural network cnn convolut neural network cnn power class deep learn model wide appli variou task includ object detect speech recognit comput vision imag classif bioinformat also demonstr success time seri predict task cnn feedforward neural network leverag convolut structur extract featur data unlik tradit method cnn automat learn recogn featur data without need manual featur extract human design cnn inspir visual percept major compon cnn includ convolut layer pool layer fulli connect layer figur present typic cnn architectur imag classif task convolut layer convolut layer pivot compon cnn multipl convolut layer convolut oper extract distinct featur input imag classif lower layer tend captur basic featur textur line edg higher layer extract abstract featur convolut layer compris learnabl convolut kernel weight matric typic equal length width odd number kernel convolv input featur map slide region featur map execut convolut oper figur illustr schemat diagram convolut process figur cnn pipelin imag classif figur schemat diagram convolut process pool layer typic follow convolut layer pool layer reduc number connect network perform dimension reduct input data primari purpos allevi comput burden address overfit issu moreov pool layer enabl cnn recogn object even shape distort view differ angl incorpor variou dimens imag pool pool oper produc output featur map robust distort error individu neuron variou pool method includ max pool averag pool spatial pyramid pool mix pool stochast pool figur depict exampl max pool window slide across input content window process pool function fulli connect fc layer fc layer typic locat end cnn architectur thi layer everi neuron connect neuron preced layer adher principl convent perceptron neural network fc layer receiv input last pool convolut layer vector creat flatten featur map fc layer serv classifi cnn enabl network make predict figur comput output valu max pool oper input recurr neural network rnn recurr neural network rnn class deep learn model possess intern memori enabl captur sequenti depend unlik tradit neural network treat input independ entiti rnn consid tempor order input make suitabl task involv sequenti inform employ loop rnn appli oper element seri current comput depend current input previou comput abil rnn util contextu inform particularli valuabl task natur languag process video classif speech recognit exampl languag model understand preced word sentenc crucial predict next word rnn excel captur depend due recurr natur howev limit simpl rnn memori restrict abil retain inform long sequenc overcom thi advanc rnn variant develop includ long term memori lstm bidirect lstm gate recurr unit gru bidirect gru bayesian rnn figur depict simpl recurr neural network intern memori ℎ𝑡 comput use equat 𝑔 𝑏 thi equat 𝑔 repres activ function typic hyperbol tangent 𝑈 𝑊 adjust weight matric hidden state ℎ 𝑏 bia term 𝑥 denot input vector rnn proven power model process sequenti data leverag abil captur depend time variou type rnn model lstm bidirect lstm gru bidirect gru develop address specif challeng differ applic figur simpl rnn intern oper long memori lstm long memori lstm advanc variant recurr neural network rnn address issu captur depend lstm wa initi introduc improv gain signific popular deep learn commun compar standard rnn lstm model proven effect retain util inform longer sequenc lstm network current input specif time step output previou time step fed lstm unit gener output pass next time step final hidden layer last time step sometim along hidden layer commonli employ classif purpos overal architectur lstm network depict figur lstm consist three gate input gate forget gate output gate gate perform specif function control flow inform input gate decid updat intern state base current input previou intern state forget gate determin much previou intern state forgotten final output gate regul influenc intern state system figur illustr updat mechan within inner structur lstm updat equat lstm unit express equat ℎ 𝑡 𝑡 𝑓 ℎ 𝑠 𝑡 𝑠 𝑔𝑓 𝑡 𝑠 𝑔𝑖 𝑡 𝑓 𝑠 𝑤ℎ 𝑢𝑋 𝑡 𝑏 𝑔𝑖 𝑡 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 𝑤𝑖ℎ 𝑢𝑖𝑋 𝑡 𝑏𝑖 𝑔𝑓 𝑡 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 𝑤𝑓ℎ 𝑢𝑓𝑋 𝑡 𝑏𝑓 𝑔𝑜 𝑡 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 𝑤𝑜ℎ 𝑢𝑜𝑋 𝑡 𝑏𝑜 𝑓 ℎ 𝑓 𝑠 repres activ function system state intern state typic util hyperbol tangent function gate oper denot g feedforward neural network sigmoid activ function ensur output valu within rang interpret set weight subscript 𝑖 𝑜 𝑓 correspond input gate output gate forget gate respect figur level architectur lstm model figur inner architectur standard lstm modul standard lstm ha demonstr promis perform variou task may struggl comprehend input structur complex sequenti format address thi limit lstm network known wa propos consist memori block compris input gate two forget gate cell gate output gate exhibit superior perform challeng sequenti model problem come higher comput complex compar standard lstm bidirect lstm bidirect long memori extens lstm architectur address limit standard lstm model consid past futur context sequenc model task tradit lstm model process input data onli forward direct overcom thi limit train model two direct forward backward consist two parallel lstm layer one process input sequenc forward direct process backward direct forward lstm layer read input data left right indic green arrow figur simultan backward lstm layer read input data right left repres red arrow thi bidirect process enabl model captur inform past futur context allow comprehens understand tempor depend within sequenc dure train phase forward backward lstm layer independ extract featur updat intern state base input sequenc output lstm layer time step predict score predict score combin use weight sum gener final output result incorpor inform direct model captur broader context improv model abil model tempor depend sequenti data figur architectur bidirect lstm model ha wide appli variou sequenc model task natur languag process speech recognit sentiment analysi ha shown promis result captur complex pattern depend sequenti data make popular choic task requir understand past futur context gate recurr unit gru gate recurr unit gru anoth variant rnn architectur address memori issu offer simpler structur compar lstm gru combin input gate forget gate lstm singl updat gate result streamlin design unlik lstm gru doe includ separ cell state gru unit consist three main compon updat gate reset gate current memori content gate enabl gru select updat util inform previou time step allow captur depend sequenc figur illustr structur gru unit figur structur gru unit updat gate equat determin much past inform retain combin current input specif time step comput base concaten previou hidden state current input 𝑥𝑡 follow linear transform sigmoid activ function 𝜎 𝑊 𝑧 𝑥𝑡 𝑏𝑧 reset gate equat decid much past inform forgotten comput similar manner updat gate use concaten previou hidden state current input 𝑟 𝜎 𝑊 𝑟 𝑥𝑡 𝑏𝑟 current memori content equat calcul base reset gate concaten transform previou hidden state current input result pass hyperbol tangent activ function produc candid activ ℎ 𝑡𝑎𝑛ℎ 𝑊 ℎ 𝑟 𝑥𝑡 final final memori state ℎ𝑡 determin combin previou hidden state candid activ equat updat gate determin balanc previou hidden state candid activ addit output gate 𝑜𝑡 introduc control inform flow current memori content output equat output gate comput use current memori state ℎ𝑡 typic follow activ function sigmoid function 𝑧𝑡ℎ 𝜎𝑜 𝑊 𝑏𝑜 weight matrix output layer 𝑊 𝑜 bia vector output layer 𝑏𝑜 gru offer simpler altern lstm fewer tensor oper allow faster train howev choic gru lstm depend specif use case problem hand architectur advantag disadvantag perform may vari depend natur task model supervis machin learn wide use artifici intellig ai unsupervis learn remain activ area research numer unresolv question howev recent advanc deep learn gener model inject new possibl unsupervis learn rapidli evolv domain within comput vision research gener model gm model leverag train data origin unknown distribut produc novel sampl adher distribut ultim goal gener model gener data sampl close resembl real data distribut variou gener model develop appli differ context gener adversari network gan restrict boltzmann machin rbm deep belief network dbn map som autoencod concept autoencod origin neural network design reconstruct input data fundament object learn meaning represent data unsupervis manner variou applic includ cluster autoencod neural network aim replic input output consist intern hidden layer defin code repres input data autoencod network compris two main compon encod function denot 𝑧 𝑓 𝑥 decod function gener reconstruct denot 𝑟 𝑔 𝑧 function 𝑓 𝑥 transform data point 𝑥 data space featur space function 𝑔 𝑧 transform 𝑧 featur space back data space reconstruct origin data point modern autoencod function 𝑧 𝑓 𝑥 𝑟 𝑔 𝑧 consid stochast function repres 𝑝𝑒𝑛𝑐𝑜𝑑𝑒𝑟 𝑝𝑑𝑒𝑛𝑐𝑜𝑑𝑒𝑟 respect 𝑟 denot reconstruct 𝑥 figur illustr autoencod model autoencod model find util variou unsupervis learn task gener model dimension reduct featur extract anomali outlier detect denois gener autoencod model categor two major group regular autoencod valuabl learn represent subsequ classif task variat autoencod function gener model exampl regular autoencod model includ spars autoencod sae contract autoencod cae denois autoencod dae variat autoencod vae gener model employ probabilist distribut mean varianc gaussian distribut data gener vae provid principl framework learn deep model associ infer model vae consist two coupl independ parameter model encod recognit model decod gener model dure expect maxim learn iter gener model receiv approxim posterior estim latent random variabl recognit model use updat paramet convers gener model act scaffold recognit model enabl learn meaning represent data potenti class label term bay rule recognit model roughli invers gener model figur structur autoencod gener adversari network gan notabl neural network architectur gener model capabl produc realist novel sampl demand gener adversari network gan initi propos ian goodfellow gan consist two key compon gener model discrimin model gener model aim gener imag resembl real one discrimin model aim differenti real synthet imag model typic implement use multilay perceptron figur depict framework gan adversari game play gener g discrimin gener updat gradient determin discrimin adapt object previous mention gan oper base principl deriv neural network util train set input gener new data resembl train set case gan train imag data gener new imag exhibit characterist follow outlin oper gan gener creat discrimin network gener content base real data distribut system undergo train increas discrimin abil distinguish synthes real candid allow gener better fool discrimin discrimin initi train use dataset train data train sampl dataset repeatedli present desir accuraci achiev gener train process random input gener candid deceiv discrimin backpropag employ updat discrimin gener former improv abil identifi real imag latter becom adept produc realist synthet imag convolut neural network cnn commonli use discrimin deconvolut neural network util gener network figur framework gru gener adversari network gan introduc numer applic across variou domain includ imag blend object gener face age medicin steganographi imag manipul text transfer languag speech synthesi traffic control video gener furthermor sever model develop base gener adversari network gan framework address specif task model includ coupl gan markovian gan evolutionari gan unrol gan bayesian condit gan relativist gan laplacian gan gan graph embed gan wasserstein gan wgan boundari equilibrium gan began deep reinforc learn reinforc learn rl machin learn approach deal sequenti aim map situat action way maxim associ reward unlik supervis learn explicit instruct given system action rl framework learner known agent provid explicit guidanc action take timestep rl agent must explor trial error determin action yield highest reward furthermor unlik supervis learn correct output obtain model updat base loss error rl use gradient without differenti loss function teach model explor randomli learn make optim decis figur depict environ interact reinforc learn rl standard theoret framework rl base markov decis process mdp extend concept markov process use model base state action reward figur interact rl deep reinforc learn combin make capabl reinforc learn percept function deep learn consid form real ai align close human think figur illustr basic structur deep reinforc learn deep learn process sensori input environ provid current state data reinforc learn process link current state appropri action evalu valu base anticip reward one renown deep reinforc learn model deep network dqn directli learn polici input use convolut neural network cnn common model deep reinforc learn includ doubl dqn duel dqn mont carlo tree search mct deep reinforc learn drl model find applic variou domain video game play robot manipul imag segment video analysi energi manag figur basic structur drl deep transfer learn deep neural network significantli improv perform across variou machin learn task applic howev achiev remark perform gain often requir larg amount label data supervis learn reli captur latent pattern within data unfortun certain special domain avail suffici train data major challeng construct qualiti annot dataset costli address issu limit train data transfer learn ha emerg crucial tool machin learn concept transfer learn find root educ psycholog theori gener suggest transfer knowledg one context anoth facilit gener experi order achiev success transfer need connect two learn task exampl someon ha learn play violin like learn piano quickli due share characterist music instrument figur depict learn process transfer learn grow popular deep neural network variou field numer deep transfer learn techniqu propos deep transfer learn categor four main type base techniqu employ deep transfer learn deep transfer learn deep transfer learn deep transfer learn deep transfer learn involv select subset instanc sourc domain assign appropri weight valu select instanc supplement train set target domain algorithm tasktradaboost approach base thi strategi deep transfer learn focus map instanc sourc target domain new data space instanc two domain exhibit similar suitabl train unifi deep neural network success method base thi approach includ extend mmd figur learn process transfer learn deep transfer learn revolv around reus portion network ha alreadi train sourc domain along network structur connect paramet transfer deep neural network employ target domain deep transfer learn aim identifi transfer represent use gener adversari network gan model applic sourc target domain deep transfer learn techniqu proven effect overcom challeng limit train data enabl knowledg transfer across domain facilit improv perform variou machin learn applic analysi deep learn model experiment analysi util three publicli avail dataset imdb ara purpos wa perform compar analysi variou deep learn model specif examin six differ model cnn simpl rnn lstm bidirect lstm gru bidirect gru evalu model test data aim assess perform use multipl evalu metric accuraci precis recal imdb dataset stand internet movi databas provid collect movi review categor posit neg sentiment ara dataset compris annot sensor event human activ recognit task dataset consist imag variou fruit type classif purpos analysi focus compar perform differ deep learn model across divers dataset cnn model known convolut neural network particularli effect imag analysi task due abil captur spatial depend simpl rnn recurr neural network suitabl sequenti data analysi lstm long memori gru gate recurr unit model excel captur depend sequenti data bidirect lstm bidirect gru model offer advantag process inform forward backward direct evalu perform model employ assess metric accuraci precis recal accuraci measur overal correct model predict precis evalu proport correctli predict posit instanc recal assess model abil correctli identifi posit instanc provid balanc measur precis recal 𝑇𝑛 𝐹𝑛 𝑇𝑝 𝐹𝑝 𝑇𝑝 𝐹𝑛 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 𝑇𝑝 true posit 𝑇𝑛 true neg 𝐹𝑝 fals posit 𝐹𝑛 fals neg conduct comprehens analysi use metric gain insight strength weak deep learn model thi compar evalu enabl us identifi effect model specif dataset applic ultim advanc field deep learn practic applic model imdb dataset imdb dataset wide use dataset sentiment analysi task consist movi review along correspond binari sentiment polar label dataset contain total review evenli split train sampl test sampl equal distribut posit neg label instanc sentiment reduc correl review given movi onli review includ dataset posit review often contain word like great well love neg review frequent use word like bad ca howev certain word one charact well appear frequent posit neg review although usag may differ term frequenc two sentiment class analysi employ six differ deep learn model sentiment classif use imdb dataset compar perform model util accuraci loss diagram diagram provid insight well model learn data help evalu effect sentiment classif task figur accuraci deep learn model imdb dataset figur accuraci deep learn model imdb dataset figur accuraci diagram provid visual represent differ deep learn model perform term accuraci dure train process also figur show trend accuraci valu valid set across multipl epoch model figur illustr loss diagram visual represent loss valu dure train process six differ model figur diagram depict variat loss valu test set dure evalu process differ model loss function measur discrep predict sentiment label actual label experi paramet use model set epoch cnn model util filter kernel size rectifi linear unit relu activ function simpl rnn lstm gru model unit employ along dropout rate recurr dropout hand model use unit paramet set architectur choic allow standard comparison deep learn model imdb dataset facilit analysi perform comparison accuraci loss valu figur loss deep learn model imdb dataset figur loss deep learn model imdb dataset also tabl show result differ deep learn model imdb review dataset base variou metric includ accuraci precis recal time train cpu tabl result differ deep learn model imdb dataset model accuraci precis recal score time cnn rnn lstm gru base result provid conclud gru cnn model achiev best perform imdb review dataset sentiment analysi model demonstr high accuraci classifi sentiment movi review howev worth note train time cnn model wa significantli less gru model thi suggest cnn model wa faster train compar gru model still achiev excel perform addit model also exhibit good accuraci sentiment classif requir less train time gru model thi indic model could favor choic offer balanc accuraci train effici overal result suggest gru cnn model effect deep learn model sentiment analysi imdb review dataset vari perform train time model ara dataset base provid inform ara dataset valuabl resourc automat recognit human activ smart environ consist data stream collect two hous period day binari sensor instal monitor resid activ dataset includ inform differ activ perform resid sensor event record basi order analyz classifi human activ ara dataset six deep learn model employ train data day hous b compris sampl use train model day data amount sampl use test figur present accuraci diagram deep learn model figur illustr valid accuraci diagram also figur show loss diagram figur provid diagram deep learn model model train fix set paramet batch cnn model util filter kernel size activ function relu pool size simpl rnn lstm gru model configur unit along dropout recurr dropout figur accuraci deep learn model ara dataset figur accuraci deep learn model ara dataset figur loss diagram deep learn model ara dataset figur loss deep learn model ara dataset also tabl illustr result experi ara dataset variou metric includ accuraci precis recal time train cpu tabl result deep learn model ara dataset model accuraci precis recal score time cnn rnn lstm gru base provid result observ recurr model specif lstm gru outperform convolut model cnn work data ara dataset thi find consist natur dataset involv tempor sequenc sensor event among recurr model gru demonstr best perform term accuraci evalu metric addit advantag lower train time compar recurr model indic effici process learn data result suggest task human activ recognit smart environ use ara dataset recurr model like lstm gru due abil captur tempor depend among model gru stand particularli effici option achiev good perform requir less train time model dataset analysi deep learn model dataset imag classif task involv three model cnn lstm dataset consist imag differ fruit class resolut pixel thi experi subset fruit class wa use imag train imag test figur show accuraci diagram three deep learn model figur illustr accuraci model also figur illustr loss diagram figur provid diagram three deep learn model figur accuraci cnn lstm model dataset figur accuraci cnn lstm model dataset figur loss diagram cnn lstm model dataset figur loss cnn lstm model dataset experiment setup includ fix number epoch batch size model cnn model filter kernel size activ function employ furthermor tabl show result experi base differ metric includ accuraci precis recal time train dataset cnn lstm model tabl result deep learn model dataset model accuraci precis recal score time cnn lstm base result appear cnn model outperform lstm model term accuraci train time cnn model superior perform attribut abil effect captur spatial featur imag convolut layer sinc imag data inher sequenti depend recurr model may suit thi particular task furthermor faster train time cnn model attribut parallel process natur allow effici comput gpu acceler train process overal result suggest imag classif task dataset focu captur spatial featur cnn model tend effect effici compar recurr model like lstm conclus conclus thi articl provid extens overview deep learn technolog applic machin learn artifici intellig articl cover variou aspect deep learn includ neural network mlp model differ type deep learn model cnn rnn gener model drl transfer learn classif deep learn model allow better understand specif applic characterist rnn model includ lstm lstm gru particularli suit time seri data due abil captur tempor depend hand cnn model excel imag data analysi effect captur spatial featur experi conduct three public dataset name imdb ara reinforc suitabl specif deep learn model differ data type result demonstr cnn model perform except well imag classif task rnn model lstm gru show strong perform time seri analysi addit gru model stand faster train time compar lstm attribut simplifi architectur two gate instead three overal thi articl highlight divers applic effect deep learn model variou domain emphas import select appropri deep learn model base natur data task hand insight gain experi contribut better understand strength weak differ deep learn model facilit inform practic applic refer shind shah review machin learn deep learn applic fourth intern confer comput commun control autom iccubea ieee pp rathor mannep review machin learn techniqu applic health care intern confer advanc technolog manag educ icatm ieee pp mathew amudha sivakumari deep learn techniqu overview advanc machin learn technolog applic proceed amlta pp shrestha mahmood review deep learn algorithm architectur ieee access vol pp wani bhat afzal khan advanc deep learn springer alzubaidi et review deep learn concept cnn architectur challeng applic futur direct journal big data vol pp sarker deep learn comprehens overview techniqu taxonomi applic research direct sn comput scienc vol gaikwad tiwari keskar shivaprakash effici fpga implement multilay perceptron human activ classif ieee access vol pp ke huang qualiti predict inject mold use multilay perceptron neural network polym vol tasdelen sen hybrid model classif scientif report vol pp qin yu zhao appli convolut neural network deep learn technolog behaviour recognit intellig video tehnički vjesnik vol pp li liu yang peng zhou survey convolut neural network analysi applic prospect ieee tran neural netw learn syst vol pp dec doi mekruksavanich jitpattanakul deep convolut neural network rnn complex activ recognit use wearabl sensor data electron vol lu li wang qin method stock price predict neural comput applic vol pp rawat wang deep convolut neural network imag classif comprehens review neural comput vol pp chen li bai yang jiang miao review imag classif algorithm base convolut neural network remot sens vol gu et recent advanc convolut neural network pattern recognit vol pp ying overview overfit solut journal physic confer seri vol iop publish ajit acharya samanta review convolut neural network intern confer emerg trend inform technolog engin ieee pp liu wang liu zeng liu alsaadi survey deep neural network architectur applic neurocomput vol pp zhang ren sun spatial pyramid pool deep convolut network visual recognit ieee transact pattern analysi machin intellig vol pp yu wang chen wei mix pool convolut neural network rough set knowledg technolog intern confer rskt shanghai china octob proceed springer pp gong wang guo lazebnik orderless pool deep convolut activ featur comput european confer zurich switzerland septemb proceed part vii springer pp zeiler fergu stochast pool regular deep convolut neural network arxiv preprint dumoulin visin guid convolut arithmet deep learn arxiv preprint abbaspour fotouhi sedaghatbaf fotouhi vahabi linden compar analysi hybrid deep learn model human activ recognit sensor vol doi fang chen xue survey research sequenc predict algorithm journal big data vol xiao zhou research progress rnn languag model ieee intern confer artifici intellig comput applic icaica ieee pp ng hausknecht vijayanarasimhan vinyal monga toderici beyond short snippet deep network video classif proceed ieee confer comput vision pattern recognit pp shewalkar nyavanandi ludwig perform evalu deep neural network appli speech recognit rnn lstm gru journal artifici intellig soft comput research vol pp apaydin feizi sattari colak shamshirband chau compar analysi recurr neural network architectur reservoir inflow forecast water vol hochreit schmidhub long memori neural comput vol pp grave liwicki fernández bertolami bunk schmidhub novel connectionist system unconstrain handwrit recognit ieee transact pattern analysi machin intellig vol pp chung gulcehr cho bengio empir evalu gate recurr neural network sequenc model arxiv preprint chen jiang zhang hierarch bidirect gru model attent emot classif ieee access vol pp fortunato blundel vinyal bayesian recurr neural network arxiv preprint kratzert klotz brenner schulz herrnegg model use long term memori lstm network hydrolog earth system scienc vol pp grave gener sequenc recurr neural network arxiv preprint shiri perum mustapha moham ahmadon yamaguchi survey activ recognit smart environ minae azimi abdolrashidi sentiment analysi use ensembl cnn model arxiv preprint zhu sobihani guo long memori recurs structur intern confer machin learn pmlr pp gu chung chignel valae zhou liu survey deep learn human activ recognit acm comput survey csur vol pp aldhyani alkahtani bidirect long memori model algorithm predict gulf countri life vol liciotti bernardini romeo frontoni sequenti deep learn applic recognis human activ smart home neurocomput vol pp dutta kumar basu gate recurr unit approach bitcoin price predict journal risk financi manag vol gumaei hassan alelaiwi alsalman hybrid deep learn model human activ recognit use multimod bodi sens data ieee access vol pp doi jabbar li omar survey gener adversari network variant applic train acm comput survey csur vol pp bank koenigstein giry autoencod arxiv preprint goodfellow et gener adversari network commun acm vol pp zhang ding zhang xue overview restrict boltzmann machin neurocomput vol pp hinton deep belief network scholarpedia vol kohonen map springer scienc busi media goodfellow bengio courvil deep learn mit press zhai zhang chen autoencod variou variant ieee intern confer system man cybernet smc ieee pp makhzani shlen jaitli goodfellow frey adversari autoencod arxiv preprint wang yao zhao base dimension reduct neurocomput vol pp kunang nurmaini stiawan zarkasi automat featur extract use autoencod intrus detect system intern confer electr engin comput scienc iceco ieee pp zhou paffenroth anomali detect robust deep autoencod proceed acm sigkdd intern confer knowledg discoveri data mine pp creswel bharath denois adversari autoencod ieee transact neural network learn system vol pp kingma well variat bay arxiv preprint ng spars autoencod lectur note vol pp rifai et higher order contract machin learn knowledg discoveri databas european confer ecml pkdd athen greec septemb proceed part ii springer pp vincent larochel bengio manzagol extract compos robust featur denois autoencod proceed intern confer machin learn pp kingma well introduct variat autoencod foundat machin learn vol pp liu tuzel coupl gener adversari network advanc neural inform process system vol wang xu yao tao evolutionari gener adversari network ieee transact evolutionari comput vol pp aggarw mittal battineni gener adversari network overview theori applic intern journal inform manag data insight vol chen kae toward realist imag composit adversari learn proceed confer comput vision pattern recognit pp jaiswal kumar badr toward artifici intellig aid design approach applic anim face gener adversari network procedia comput scienc vol pp liu li sun face age gener adversari network proceed confer comput vision pattern recognit pp islam zhang synthet brain pet imag gener brain informat vol pp lan initi toga sepehrband condit gan spectral normal neuroimag synthesi biorxiv zhang xu veeramachaneni steganogan high capac imag steganographi gan arxiv preprint nam kim kim gener adversari network manipul imag natur languag advanc neural inform process system vol sixt wild landgraf rendergan gener realist label data frontier robot ai vol lin li zhang sun adversari rank languag gener advanc neural inform process system vol xu wei peng xuan guo novel deep learn framework road traffic state estim transport research part c emerg technolog vol clark donahu simonyan adversari video gener complex dataset arxiv preprint li wand precomput textur synthesi markovian gener adversari network comput european confer amsterdam netherland octob proceed part iii springer pp metz pool pfau unrol gener adversari network arxiv preprint zhao meyerand birn bayesian condit gan mri brain imag synthesi arxiv preprint relativist discrimin key element miss standard gan arxiv preprint denton chintala fergu deep gener imag model use laplacian pyramid adversari network advanc neural inform process system vol arjovski chintala bottou wasserstein gener adversari network intern confer machin learn pmlr pp berthelot schumm metz began boundari equilibrium gener adversari network arxiv preprint vithayathil varghes mahmoud survey deep reinforc learn electron vol le rathour yamazaki luu savvid deep reinforc learn comput vision comprehens survey artifici intellig review pp puterman markov decis process discret stochast dynam program john wiley son zhang zhang qiu deep reinforc learn power system applic overview csee journal power energi system vol pp mnih et control deep reinforc learn natur vol pp van hasselt guez silver deep reinforc learn doubl proceed aaai confer artifici intellig vol wang schaul hessel hasselt lanctot freita duel network architectur deep reinforc learn intern confer machin learn pmlr pp coulom effici select backup oper tree search comput game intern confer cg turin itali may revis paper springer pp justesen bontrag togeliu risi deep learn video game play ieee transact game vol pp gu holli lillicrap levin deep reinforc learn robot manipul asynchron updat ieee intern confer robot autom icra ieee pp lee myeong song seednet automat seed gener deep reinforc learn robust interact segment present confer comput vision pattern recognit cvpr onlin avail http sahba deep reinforc learn object segment video sequenc intern confer comput scienc comput intellig csci ieee pp shojaeighadikolaei ghasemi barda ahmadi hashemi microgrid energi manag use deep reinforc learn north american power symposium nap ieee pp long zhu wang jordan deep transfer learn joint adapt network intern confer machin learn pmlr pp tan sun kong zhang yang liu survey deep transfer learn artifici neural network machin intern confer artifici neural network rhode greec octob proceed part iii springer pp zhuang et comprehens survey transfer learn proceed ieee vol pp yao doretto boost transfer learn multipl sourc ieee comput societi confer comput vision pattern recognit ieee pp pardo stone boost regress transfer proceed intern confer intern confer machin learn pp tzeng hoffman zhang saenko darrel deep domain confus maxim domain invari arxiv preprint long cao wang jordan learn transfer featur deep adapt network intern confer machin learn pmlr pp maa dali pham huang ng pott learn word vector sentiment analysi proceed annual meet associ comput linguist human languag technolog pp alemdar ertan incel ersoy ara human activ dataset multipl home multipl resid intern confer pervas comput technolog healthcar workshop ieee pp mureşan oltean fruit recognit imag use deep learn arxiv preprint