ai research broadway new york new york usa york univers broadway new york new york usa comput scienc oper research université de montréal pavillon po box stn montréal quebec canada amphitheatr parkway mountain view california usa comput scienc univers toronto king colleg road toronto ontario canada technolog power mani aspect modern societi web search content filter social work recommend websit increasingli present consum product camera smartphon system use identifi object imag transcrib speech text match news item post product user interest select relev result search increasingli applic make use class techniqu call deep learn convent techniqu limit abil process natur data raw form decad struct system requir care engin consider domain expertis design ture extractor transform raw data pixel valu imag suitabl intern represent featur vector learn subsystem often classifi could detect classifi pattern input represent learn set method allow machin fed raw data automat discov represent need detect classif method method multipl level tion obtain compos simpl modul transform represent one level start raw input represent higher slightli abstract level composit enough transform veri complex function learn classif task higher layer represent amplifi aspect input import discrimin suppress irrelev variat imag exampl come form array pixel valu learn featur first layer represent typic repres presenc absenc edg particular orient locat imag second layer typic detect motif spot particular arrang edg regardless small variat edg posit third layer may assembl motif larger combin correspond part familiar object subsequ layer would detect object combin part key aspect deep learn layer featur design human engin learn data use learn procedur deep learn make major advanc solv problem resist best attempt artifici intellig niti mani year ha turn veri good discov intric structur data therefor ble mani domain scienc busi govern addit beat record imag speech ha beaten techniqu predict iti potenti drug analys particl acceler reconstruct brain predict effect mutat dna gene express perhap surprisingli deep learn ha produc extrem promis result variou task natur languag particularli topic classif sentiment analysi question guag think deep learn mani success near futur becaus requir veri littl engin hand easili take advantag increas amount avail putat data new learn algorithm architectur current develop deep neural network onli ate thi progress supervis learn common form machin learn deep vise learn imagin want build system classifi imag contain say hous car person pet first collect larg data set imag hous car peopl pet label categori dure train machin shown imag produc output form vector score one categori want desir categori highest score categori thi unlik happen befor train comput object function measur error tanc output score desir pattern score machin modifi intern adjust paramet reduc thi error adjust paramet often call weight real number seen knob defin tion machin typic system may hundr million adjust weight hundr million label exampl train machin properli adjust weight vector learn algorithm pute gradient vector weight indic amount error would increas decreas weight increas tini amount weight vector adjust opposit tion gradient vector object function averag train exampl deep learn allow comput model compos multipl process layer learn represent data multipl level abstract method dramat improv speech ognit visual object recognit object detect mani domain drug discoveri genom deep learn discov intric structur larg data set use backpropag algorithm indic machin chang intern paramet use comput represent layer represent previou layer deep convolut net brought breakthrough process imag video speech audio wherea recurr net shone light sequenti data text speech deep learn yann yoshua geoffrey n u r e v l review macmillan publish limit right reserv seen kind hilli landscap space weight valu neg gradient vector indic direct steepest descent thi landscap take closer minimum output error low averag practic practition use procedur call stochast gradient descent sgd thi consist show input vector exampl comput output error comput averag gradient exampl adjust weight accordingli process repeat mani small set exampl train set averag object function stop decreas call stochast becaus small set exampl give noisi estim averag gradient exampl thi simpl procedur usual find good set weight surprisingli quickli compar far elabor optim train perform system measur differ set exampl call test set thi serv test gener abil machin abil produc sensibl answer new input ha never seen dure train mani current practic applic machin learn use linear classifi top featur linear classifi comput weight sum featur vector compon weight sum abov threshold input classifi belong particular categori sinc known linear classifi onli carv input space veri simpl region name rate problem imag speech nition requir function insensit irrelev variat input variat posit orient illumin object variat pitch accent speech veri sensit particular minut variat exampl differ white wolf breed white dog call samoy pixel level imag two samoy differ pose differ environ may veri differ wherea two imag samoy wolf posit similar background may veri similar linear classifi ani shallow classifi oper figur multilay neural network backpropag layer neural network shown connect dot distort input space make class data exampl red blue line linearli separ note regular grid shown left input space also transform shown middl panel hidden unit thi illustr exampl onli two input unit two hidden unit one output unit network use object recognit natur languag process contain ten hundr thousand unit reproduc permiss olah http b chain rule deriv tell us two small effect small chang x z compos small chang δx x get transform first small chang δy get multipli definit partial deriv similarli chang δy creat chang δz substitut one equat give chain rule deriv δx get turn δz multipl product also work x z vector deriv jacobian matric c equat use comput forward pass neural net two hidden layer one output layer constitut modul one backpropag gradient layer first comput total input z unit weight sum output unit layer function f appli z get output unit simplic omit bia term function use neural network includ rectifi linear unit relu f z max z commonli use recent year well convent sigmoid hyberbol tangent f z exp z exp exp z exp logist function logist f z exp equat use comput backward pass hidden layer comput error deriv respect output unit weight sum error deriv respect total input unit layer abov convert error deriv respect output error deriv respect input multipli gradient f z output layer error deriv respect output unit comput differenti cost function thi give yl tl cost function unit l yl tl tl target valu onc known weight wjk connect unit j layer yj input output sigmoid hidden sigmoid b c x x z x z z z δ δ δ δ δ δ z z x x x z z x x compar output correct answer get error deriv j k e yl tl e zl e yl yl zl l e yj wjk e zk e zj e yj yj zj e yk wkl e zl e zk e yk yk zk wkl wjk wij j k yl f zl zl wkl yk l yj f zj zj wij xi yk f zk zk wjk yj output unit input unit hidden unit hidden unit wkl wjk wij k k j input v l n u r e review insight macmillan publish limit right reserv raw pixel could possibl distinguish latter two put former two categori thi whi shallow classifi requir good featur extractor solv dilemma one produc represent select aspect imag import discrimin invari irrelev aspect pose anim make classifi power one use gener featur kernel gener featur aris gaussian kernel allow learner ize well far train convent option hand design good featur extractor requir abl amount engin skill domain expertis thi avoid good featur learn automat use learn procedur thi key advantag deep learn architectur multilay stack simpl ule subject learn mani comput map modul stack transform input increas select invari represent multipl layer say depth system implement extrem intric tion input simultan sensit minut detail distinguish samoy white wolv insensit larg irrelev variat background pose light surround object backpropag train multilay architectur earliest day pattern aim er ha replac featur trainabl multilay network despit simplic solut wa wide understood mid turn multilay architectur train simpl stochast gradient descent long modul rel smooth function input intern weight one comput gradient use backpropag procedur idea thi could done work wa discov independ sever differ group dure backpropag procedur comput gradient object function respect weight multilay stack modul noth practic applic chain rule deriv key insight deriv ent object respect input modul comput work backward gradient respect output modul input subsequ modul backpropag equat appli repeatedli propag gradient modul start output top network produc predict way bottom extern input fed onc gradient comput straightforward comput gradient respect weight modul mani applic deep learn use feedforward neural work architectur fig learn map input exampl imag output exampl abil sever categori go one layer next set unit comput weight sum input previou layer pass result function present popular function rectifi linear unit relu simpli rectifi f z max z past decad neural net use smoother tanh z exp relu typic learn much faster network mani layer allow train deep supervis network without unsupervis unit input output layer convent call hidden unit hidden layer seen distort input way categori becom linearli separ last layer late neural net backpropag larg forsaken commun ignor commun wa wide thought learn use multistag featur extractor tle prior knowledg wa infeas particular wa commonli thought simpl gradient descent would get trap poor local minima weight configur small chang would reduc averag error practic poor local minima rare problem larg work regardless initi condit system nearli alway reach solut veri similar qualiti recent theoret empir result strongli suggest local minima seriou issu gener instead landscap pack rialli larg number saddl point gradient zero surfac curv dimens curv figur insid convolut network output filter layer horizont typic convolut network architectur appli imag samoy dog bottom left rgb red green blue input bottom right rectangular imag featur map correspond output one learn featur detect imag posit inform flow bottom featur act orient edg detector score comput imag class output relu rectifi linear unit red green blue samoy papillon pomeranian arctic fox eskimo dog white wolf siberian huski convolut relu max pool max pool convolut relu convolut relu n u r e v l review insight macmillan publish limit right reserv analysi seem show saddl point onli downward curv direct present veri larg number almost veri similar valu tive function henc doe much matter saddl point algorithm get stuck interest deep feedforward network wa reviv around ref group research brought togeth dian institut advanc research cifar research duce unsupervis learn procedur could creat layer featur detector without requir label data object learn layer featur detector wa abl reconstruct model activ featur detector raw input layer sever layer progress complex featur detector use thi reconstruct object weight deep network could initi sensibl valu final layer output unit could ad top network whole deep system could use standard thi work remark well recogn handwritten digit detect pedestrian especi amount label data wa veri first major applic thi approach wa speech recognit wa made possibl advent fast graphic process unit gpu conveni allow research train network time faster approach wa use map short tempor window ficient extract sound wave set probabl variou fragment speech might repres frame centr window achiev result standard speech recognit benchmark use small wa quickli develop give result larg vocabulari version deep net develop mani major speech alreadi deploy android phone smaller data set unsupervis help prevent lead significantli better gener number label ple small transfer set lot exampl sourc task veri target task onc deep learn rehabilit turn stage wa onli need small data set wa howev one particular type deep feedforward work wa much easier train gener much better network full connect adjac layer thi wa convolut neural network convnet achiev mani practic success dure period neural network favour ha recent wide adopt vision commun convolut neural network convnet design process data come form multipl array exampl colour imag compos three array contain pixel intens three colour channel mani data modal form multipl array signal sequenc includ languag imag audio spectrogram video volumetr imag four key idea behind convnet take advantag properti natur signal local connect share weight pool use mani layer architectur typic convnet fig structur seri stage first stage compos two type layer convolut layer pool layer unit tional layer organ featur map within unit connect local patch featur map previou layer set weight call filter bank result thi local weight sum pass relu unit featur map share filter bank ent featur map layer use differ filter bank reason thi architectur twofold first array data imag local group valu often highli correl form distinct local motif easili detect second local statist imag signal invari locat word motif appear one part imag could appear anywher henc idea unit differ locat share weight detect pattern differ part array calli filter oper perform featur map discret convolut henc name although role convolut layer detect local junction featur previou layer role pool layer merg semant similar featur one becaus rel posit featur form motif vari somewhat reliabl detect motif done tion featur typic pool unit comput maximum local patch unit one featur map featur map neighbour pool unit take input patch shift one row column therebi reduc dimens represent creat invari small shift tortion two three stage convolut ing stack follow convolut layer backpropag gradient convnet simpl regular deep network allow weight filter bank train deep neural network exploit properti mani natur nal composit hierarchi featur obtain compos one imag local nation edg form motif motif assembl part part form object similar hierarchi exist speech text sound phone phonem syllabl word sentenc pool allow represent vari veri littl element ou layer vari posit appear convolut pool layer convnet directli inspir classic notion simpl cell complex cell visual overal architectur reminisc hierarchi visual cortex ventral convnet model monkey shown ture activ unit convnet explain half varianc random set neuron monkey otempor convnet root architectur wa somewhat similar algorithm backpropag primit convnet call neural net wa use recognit phonem simpl numer applic convolut work go back earli start ral network speech document document read system use convnet train jointli probabilist model implement languag constraint late thi system wa read chequ unit state number optic charact nition handwrit recognit system later deploy convnet also experi earli object detect natur imag includ face face imag understand deep convolut network sinc earli convnet appli great success detect segment recognit object region imag task label data wa rel dant traffic sign segment biolog particularli detect face text pedestrian human bodi natur major recent practic success convnet face importantli imag label pixel level applic technolog includ autonom mobil robot v l n u r e review insight macmillan publish limit right reserv compani mobiley nvidia use method upcom vision tem car applic gain import involv natur languag speech despit success convnet larg forsaken mainstream commun imagenet competit deep convolut network appli data set million imag web contain differ class achiev tacular result almost halv error rate best ing thi success came effici use gpu relu new regular techniqu call niqu gener train exampl deform exist one thi success ha brought revolut comput vision convnet domin approach almost recognit detect approach human perform task recent stun demonstr combin convnet recurr net modul gener imag caption recent convnet architectur layer relu dred million weight billion connect unit wherea train larg network could taken week onli two year ago progress hardwar softwar algorithm parallel reduc train time hour perform vision system ha caus major technolog compani includ googl facebook microsoft ibm yahoo twitter adob well quickli grow number initi research develop project deploy imag understand product servic convnet easili amen effici hardwar tation chip gate number compani nvidia mobiley intel qualcomm samsung develop convnet chip enabl vision applic smartphon camera robot car distribut represent languag process theori show deep net two differ nential advantag classic learn algorithm use distribut advantag aris power composit depend underli distribut appropri componenti first learn distribut represent enabl gener new combin valu learn featur beyond seen dure train exampl combin possibl n binari featur second compos layer represent deep net bring potenti anoth exponenti exponenti depth hidden layer multilay neural network learn sent network input way make easi predict target output thi nice demonstr train multilay neural network predict next word sequenc local figur imag text caption gener recurr neural network rnn take extra input represent extract deep convolut neural network cnn test imag rnn train translat represent imag caption top reproduc permiss ref rnn given abil focu attent differ locat input imag middl bottom lighter patch given attent gener word bold exploit thi achiev better translat imag caption vision deep cnn languag gener rnn group peopl shop outdoor market mani veget fruit stand woman throw frisbe park littl girl sit bed teddi bear group peopl sit boat water giraf stand forest tree background dog stand hardwood foor stop sign road mountain background n u r e v l review insight macmillan publish limit right reserv context earlier word context present network vector one compon ha valu rest first layer word creat differ pattern activ word vector languag model layer network learn convert input word tor output word vector predict next word use predict probabl ani word vocabulari appear next word network learn word vector contain mani activ compon interpret separ featur word wa first context learn distribut represent symbol semant featur explicitli present input discov learn procedur good way factor structur relationship input output symbol multipl learn word vector turn also work veri well word sequenc come larg corpu real text individu train predict next word news stori exampl learn word vector tuesday wednesday veri similar word vector sweden norway represent call distribut represent becaus element featur mutual exclus mani configur correspond variat seen observ data word vector compos learn featur determin ahead time expert automat discov neural network vector represent word learn text veri wide use natur languag issu represent lie heart debat paradigm cognit paradigm instanc symbol someth onli properti either ident symbol instanc ha intern structur relev use reason symbol must bound variabl judici chosen rule infer contrast neural network use big activ vector big weight matric scalar perform type fast tive infer underpin effortless commonsens reason befor introduct neural languag standard approach statist model languag exploit ute represent wa base count frequenc renc short symbol sequenc length n call number possibl order vn v vocabulari size take account context hand word would requir veri larg train corpora treat word atom unit gener across semant relat sequenc word wherea neural languag model becaus associ word vector real valu featur semant relat word end close vector space recurr neural network backpropag wa first introduc excit use wa train recurr neural network rnn task involv sequenti input speech languag often better use rnn fig rnn process input sequenc one element time maintain hidden unit state vector implicitli contain inform histori past element sequenc consid output hidden unit differ discret time step output differ neuron deep multilay network fig right becom clear appli backpropag train rnn rnn veri power dynam system train ha prove problemat becaus backpropag gradient either grow shrink time step mani time step typic explod thank advanc way train rnn found veri good predict next charact next word also use complex task exampl read english sentenc one word time english encod network train final state vector hidden unit good represent thought express sentenc thi thought vector use initi hidden state extra input jointli train french decod network output abil distribut first word french translat particular first word chosen thi distribut provid input decod network output probabl tribut second word translat full stop overal thi process gener sequenc french word accord probabl distribut depend english sentenc thi rather naiv way perform machin translat ha quickli becom competit thi rais seriou doubt whether understand tenc requir anyth like intern symbol express manipul use infer rule compat view everyday reason involv mani simultan analog figur visual learn word vector left illustr word represent learn model languag project visual use right represent phrase learn recurr neural one observ semant similar word sequenc word map nearbi represent distribut represent word obtain use backpropag jointli learn represent word function predict target quantiti next word sequenc languag model whole sequenc translat word machin translat commun organ institut societi industri compani organ school compani commun ofc agenc commun associ bodi school agenc past month day last day past day month come month month ago quot two group two group last month disput two last two decad next six month two month befor nearli two month last two decad within month v l n u r e review insight macmillan publish limit right reserv contribut plausibl instead translat mean french sentenc english sentenc one learn translat mean imag english sentenc fig encod deep vnet convert pixel activ vector last hidden layer decod rnn similar one use machin translat neural languag model ha surg interest system recent see exampl mention ref rnn onc unfold time fig seen veri deep feedforward network layer share weight although main purpos learn depend theoret empir evid show difficult learn store inform veri correct one idea augment network explicit memori first propos thi kind long memori lstm network use special hidden unit natur behaviour rememb input long special unit call memori cell act like accumul gate leaki neuron ha connect next time step ha weight one copi state accumul extern signal thi multipl gate anoth unit learn decid clear content memori lstm network subsequ prove effect convent rnn especi sever layer time enabl entir speech recognit system goe way acoust sequenc charact transcript lstm network relat form gate unit also current use encod decod network perform well machin past year sever author made differ propos augment rnn memori modul propos includ neural ture machin network augment memori rnn choos read write memori network regular network augment kind associ memori network yield lent perform standard benchmark memori use rememb stori network later ask answer question beyond simpl memor neural ture machin ori network use task would normal requir reason symbol manipul neural ture machin taught algorithm among thing learn output sort list symbol input consist unsort sequenc symbol accompani real valu indic prioriti memori network train keep track state world set similar text adventur game read stori answer question requir complex one test exampl network shown version lord ring correctli answer question frodo futur deep learn unsupervis catalyt effect reviv interest deep learn ha sinc overshadow success pure supervis learn although focus thi review expect unsupervis learn becom far import longer term human anim learn larg unsupervis discov structur world observ told name everi object human vision activ process sequenti sampl optic array intellig way use small fovea larg surround expect much futur progress vision come system train end combin convnet rnn use reinforc learn decid look system combin deep learn forcement learn infanc alreadi outperform passiv vision classif task produc impress result learn play mani differ video natur languag understand anoth area deep ing pois make larg impact next year expect system use rnn understand sentenc whole document becom much better learn strategi select attend one part ultim major progress artifici intellig come system combin represent learn complex reason although deep learn simpl reason use speech handwrit recognit long time new paradigm need replac manipul symbol express oper larg receiv februari accept may krizhevski sutskev hinton imagenet classif deep convolut neural network proc advanc neural inform process system thi report wa breakthrough use convolut net almost halv error rate object recognit precipit rapid adopt deep learn comput vision commun farabet coupri najman lecun learn hierarch featur scene label ieee tran pattern anal mach intel tompson jain lecun bregler joint train convolut network graphic model human pose estim proc advanc neural inform process system szegedi et al go deeper convolut preprint http mikolov deora povey burget cernocki strategi train larg scale neural network languag model proc automat speech recognit understand hinton et al deep neural network acoust model speech recognit ieee signal process magazin thi joint paper major speech recognit laboratori summar breakthrough achiev deep learn task phonet classif automat speech recognit wa first major industri applic deep learn sainath moham kingsburi b ramabhadran deep convolut neural network lvcsr proc acoust speech signal process sheridan liaw dahl svetnik deep neural net method quantit relationship chem inf model ciodaro deva de seixa j damazio onlin particl detect neural network base topolog calorimetri inform phi conf seri kaggl higg boson machin learn challeng kaggl http helmstaedt et al connectom reconstruct inner plexiform layer mous retina natur xt x unfold v w w w w w v v v u u u u ot st figur recurr neural network unfold time comput involv forward comput artifici neuron exampl hidden unit group node valu st time get input neuron previou time step thi repres black squar repres delay one time step left thi way recurr neural network map input sequenc element xt output sequenc element ot ot depend previou xtʹ tʹ paramet matric u v w use time step mani architectur possibl includ variant network gener sequenc output exampl word use input next time step backpropag algorithm fig directli appli comput graph unfold network right comput deriv total error exampl gener right sequenc output respect state st paramet n u r e v l review insight macmillan publish limit right reserv leung xiong lee j frey deep learn regul splice code bioinformat xiong et al human splice code reveal new insight genet determin diseas scienc collobert et al natur languag process almost scratch mach learn bord chopra weston question answer subgraph embed proc empir method natur languag process http jean cho memisev bengio use veri larg target vocabulari neural machin translat proc http sutskev vinyal le sequenc sequenc learn neural network proc advanc neural inform process system thi paper show machin translat result architectur introduc ref recurr network train read sentenc one languag produc semant represent mean gener translat anoth languag bottou bousquet tradeoff larg scale learn proc advanc neural inform process system duda hart pattern classiﬁc scene analysi wiley schölkopf b smola learn kernel mit press bengio delalleau le roux curs highli variabl function local kernel machin proc advanc neural inform process system selfridg pandemonium paradigm learn mechanis thought process proc symposium mechanis thought process rosenblatt perceptron perceiv recogn automaton tech cornel aeronaut laboratori werbo beyond regress new tool predict analysi behavior scienc phd thesi harvard univ parker learn logic report mit press lecun une procédur apprentissag pour réseau à seuil assymétriqu cognitiva la frontièr de l intellig artiﬁciel de scienc de la connaiss et de neurosci french rumelhart hinton william learn represent error natur glorot bord bengio deep spars rectiﬁ neural network proc intern confer artifici intellig statist thi paper show supervis train veri deep neural network much faster hidden layer compos relu dauphin et al identifi attack saddl point problem dimension optim proc advanc neural inform process system choromanska henaff mathieu arou b lecun loss surfac multilay network proc confer ai statist http hinton kind graphic model brain proc intern joint confer artifici intellig hinton osindero teh fast learn algorithm deep belief net neural comp thi paper introduc novel effect way train veri deep neural network one hidden layer time use unsupervis learn procedur restrict boltzmann machin bengio lamblin popovici larochel greedi train deep network proc advanc neural inform process system thi report demonstr unsupervis method introduc ref significantli improv perform test data gener method unsupervis techniqu ranzato poultney chopra lecun efﬁcient learn spars represent model proc advanc neural inform process system hinton salakhutdinov reduc dimension data neural network scienc sermanet kavukcuoglu chintala lecun pedestrian detect unsupervis featur learn proc intern confer comput vision pattern recognit http raina madhavan ng deep unsupervis learn use graphic processor proc annual intern confer machin learn moham dahl hinton acoust model use deep belief network ieee tran audio speech lang process dahl yu deng acero deep neural network larg vocabulari speech recognit ieee tran audio speech lang process bengio courvil vincent represent learn review new perspect ieee tran pattern anal machin intel lecun et al handwritten digit recognit network proc advanc neural inform process system thi first paper convolut network train backpropag task classifi imag handwritten digit lecun bottou bengio haffner learn appli document recognit proc ieee thi overview paper principl train modular system deep neural network use optim show neural network particular convolut net combin search infer mechan model complex output interdepend sequenc charact associ content document hubel wiesel recept ﬁeld binocular interact function architectur cat visual cortex physiol felleman j essen distribut hierarch process primat cerebr cortex cereb cortex cadieu et al deep neural network rival represent primat cortex core visual object recognit plo comp biol fukushima miyak neocognitron new algorithm pattern recognit toler deform shift posit pattern recognit waibel hanazawa hinton shikano lang phonem recognit use neural network ieee tran acoust speech signal process bottou blanchet lienard experi time delay network dynam time warp speaker independ isol digit recognit proc eurospeech simard steinkrau platt best practic convolut neural network proc document analysi recognit vaillant monrocq lecun origin approach localis object imag proc vision imag signal process nowlan platt neural inform process system lawrenc gile tsoi back face recognit convolut approach ieee tran neural network ciresan meier masci j schmidhub deep neural network trafﬁc sign classiﬁc neural network ning et al toward automat phenotyp develop embryo video ieee tran imag process turaga et al convolut network learn gener afﬁniti graph imag segment neural comput garcia delaki convolut face ﬁnder neural architectur fast robust face detect ieee tran pattern anal machin intel osadchi lecun miller synergist face detect pose estim model mach learn tompson goroshin jain lecun bregler efﬁcient object local use convolut network proc confer comput vision pattern recognit http taigman yang ranzato wolf deepfac close gap perform face veriﬁc proc confer comput vision pattern recognit hadsel et al learn vision autonom drive field robot farabet coupri najman lecun scene pars multiscal featur learn puriti tree optim cover proc intern confer machin learn http srivastava hinton krizhevski sutskev salakhutdinov dropout simpl way prevent neural network overﬁt machin learn sermanet et al overfeat integr recognit local detect use convolut network proc intern confer learn represent http girshick donahu darrel malik rich featur hierarchi accur object detect semant segment proc confer comput vision pattern recognit simonyan zisserman veri deep convolut network imag recognit proc intern confer learn represent http boser sacking bromley lecun jackel analog neural network processor programm topolog solid state circuit farabet et al convolut network scale machin learn parallel distribut approach ed bekkerman bilenko langford j cambridg univ press bengio learn deep architectur ai montufar morton doe mixtur product contain product mixtur discret math montufar pascanu cho bengio number linear region deep neural network proc advanc neural inform process system bengio ducharm vincent neural probabilist languag model proc advanc neural inform process system thi paper introduc neural languag model learn convert word symbol word vector word embed compos learn semant featur order predict next word sequenc cho et al learn phrase represent use rnn v l n u r e review insight macmillan publish limit right reserv statist machin translat proc confer empir method natur languag process schwenk continu space languag model comput speech lang socher lin c man ng pars natur scene natur languag recurs neural network proc intern confer machin learn mikolov sutskev chen corrado dean distribut represent word phrase composition proc advanc neural inform process system bahdanau cho bengio neural machin translat jointli learn align translat proc intern confer learn represent http hochreit untersuchungen zu dynamischen neuronalen netzen german diploma thesi münich bengio simard frasconi learn depend gradient descent difﬁcult ieee tran neural network hochreit schmidhub long memori neural comput thi paper introduc lstm recurr network becom crucial ingredi recent advanc recurr network becaus good learn depend elhihi bengio hierarch recurr neural network depend proc advanc neural inform process system http sutskev train recurr neural network phd thesi univ toronto pascanu mikolov bengio difﬁculti train recurr neural network proc intern confer machin learn sutskev marten j hinton gener text recurr neural network proc intern confer machin learn lakoff johnson metaphor live univ chicago press roger mcclelland semant cognit parallel distribut process approach mit press xu et al show attend tell neural imag caption gener visual attent proc intern confer learn represent http grave moham hinton speech recognit deep recurr neural network proc intern confer acoust speech signal process grave wayn danihelka neural ture machin http weston chopra bord memori network http weston bord chopra mikolov toward question answer set prerequisit toy task http hinton dayan frey j neal algorithm unsupervis neural network scienc salakhutdinov hinton deep boltzmann machin proc intern confer artifici intellig statist vincent larochel bengio manzagol extract compos robust featur denois autoencod proc intern confer machin learn kavukcuoglu et al learn convolut featur hierarchi visual recognit proc advanc neural inform process system gregor lecun learn fast approxim spars code proc intern confer machin learn ranzato mnih susskind hinton model natur imag use gate mrf ieee tran pattern anal machin intel bengio alain yosinski deep gener stochast network trainabl backprop proc intern confer machin learn kingma rezend moham well learn deep gener model proc advanc neural inform process system ba mnih kavukcuoglu multipl object recognit visual attent proc intern confer learn represent http mnih et al control deep reinforc learn natur bottou machin learn machin reason mach learn vinyal toshev bengio erhan show tell neural imag caption gener proc intern confer machin learn http van der maaten hinton visual data use mach learn research acknowledg author would like thank natur scienc engin research council canada canadian institut advanc research cifar nation scienc foundat offic naval research support cifar fellow author inform reprint permiss inform avail author declar compet financi interest reader welcom comment onlin version thi paper correspond address yann n u r e v l review insight macmillan publish limit right reserv