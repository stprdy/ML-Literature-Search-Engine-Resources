neural network content list avail sciencedirect neural network journal homepag review deep learn neural network overview jürgen schmidhub swiss ai lab idsia istituto dall moll di studi sull intelligenza artificial univers lugano supsi galleria switzerland r c l e n f articl histori receiv may receiv revis form septemb accept septemb avail onlin octob keyword deep learn supervis learn unsupervis learn reinforc learn evolutionari comput b r c recent year deep artifici neural network includ recurr one numer contest pattern recognit machin learn thi histor survey compactli summar relev work much previou millennium shallow deep learner distinguish depth credit assign path chain possibl learnabl causal link action effect review deep supervis learn also recapitul histori backpropag unsupervis learn reinforc learn evolutionari comput indirect search short program encod deep larg network publish elsevi content introduct deep learn dl neural network nn notat activ spread nn depth credit assign path cap problem recur theme deep learn dynam program learn unsupervis learn ul facilit sl rl learn hierarch represent deep sl ul rl occam razor compress minimum descript length mdl fast graphic process unit gpu dl nn supervis nn help unsupervis nn earli nn sinc around visual cortex provid inspir dl section deep network base group method data handl convolut weight replic subsampl neocognitron beyond develop backpropag bp nn bp feedforward nn fnn recurr nn rnn late beyond numer improv nn idea deal long time lag deep cap better bp advanc gradient descent compar section search simpl nn section potenti benefit ul sl compar section ul autoencod ae hierarchi compar section bp convolut nn cnn section fundament deep learn problem gradient descent histori compress deep stack rnn mp toward mpcnn compar section address juergen http publish elsevi schmidhub neural network earli nn supervis recurr veri deep learner lstm rnn nn success deep nn ul deep belief stack bp improv stack first offici competit rnn mpcnn plain backprop distort gpu break mnist record mpcnn gpu achiev superhuman vision perform optim rnn first contest imagenet object detect segment contest benchmark record current success techniqu lstm rnn recent trick improv sl deep nn compar section consequ neurosci dl spike neuron dl fnn rnn reinforc learn rl rl nn world model yield rnn deep cap deep fnn tradit rl markov decis process mdp deep rl rnn partial observ mdp pomdp rl facilit deep ul fnn rnn deep hierarch rl hrl subgoal learn fnn rnn deep rl direct nn deep rl indirect polici nn search univers rl conclus outlook acknowledg refer prefac thi preprint invit deep learn dl overview one goal assign credit contribut present state art acknowledg limit ing achiev thi goal dl research commun may view continu evolv deep network scientist influenc complex way start recent dl result tri trace back origin relev idea past half centuri beyond sometim use local search follow citat citat backward time sinc dl public properli acknowledg earlier relev work tional global search strategi employ aid consult numer neural network expert result present preprint mostli consist refer nevertheless expert lection bia may miss import work relat bia wa sure introduc special familiar work dl research group past son thi work view mere snapshot go credit assign process help improv pleas hesit send correct suggest juergen introduct deep learn dl neural network nn modifi compon learn system sibl success failur chang improv formanc thi ha call fundament credit assign problem minski gener credit assign od univers problem solver variou theoret sens section present survey howev focu narrow commerci import subfield deep learn dl artifici neural network nn standard neural network nn consist mani simpl nect processor call neuron produc sequenc activ input neuron get activ sor perceiv environ neuron get activ weight connect previous activ neuron tail section neuron may influenc environ trigger action learn credit assign find weight make nn exhibit desir behavior drive car depend problem neuron nect behavior may requir long causal chain tation stage section stage transform often way aggreg activ network deep learn accur assign credit across mani stage shallow model stage around mani decad centuri section model eral success nonlinear layer neuron date back least section section effici dient descent method supervis learn sl discret differenti network arbitrari depth call propag bp wa develop pli nn section train deep nn mani layer howev found difficult tice late section becom explicit research subject earli section dl becam tical feasibl extent help unsupervis learn ul section section also saw mani improv pure vise dl section new millennium deep nn nalli attract attent mainli outperform altern machin learn method kernel machin schölkopf burg smola vapnik numer portant applic fact sinc supervis deep nn mani offici intern pattern recognit competit section achiev first human visual pattern recognit result limit domain tion deep nn also becom relev gener field reinforc learn rl pervis teacher section feedforward acycl nn fnn recurr cyclic nn rnn contest section sens rnn deepest nn section gener comput power fnn principl creat process memori arbitrari sequenc input pattern schmidhub siegelmann sontag unlik tradit method automat sequenti gram synthesi balzer devil lau soloway schmidhub neural network abbrevi alphabet order ae autoencod ai artifici intellig ann artifici neural network bfg bnn biolog neural network bm boltzmann machin bp backpropag brnn recurr neural network cap credit assign path cec constant error carousel cfl context free languag covari matrix estim es cnn convolut neural network cosyn csl context sensit languag ctc connectionist tempor classif dbn deep belief network dct discret cosin transform dl deep learn dp dynam program ds direct polici search ea evolutionari algorithm em expect maxim es evolut strategi fm flat minimum search fnn feedforward neural network fsa finit state automaton gmdh group method data handl gofai good ai gp genet program gpu graphic process unit mpcnn hmm hidden markov model hrl hierarch reinforc learn htm hierarch tempor memori hmax hierarch model x lstm long memori rnn mdl minimum descript length mdp markov decis process mnist mix nation institut standard ogi databas mp mpcnn cnn ne neuroevolut neat ne augment topolog ne natur evolut strategi nfq neural fit nn neural network ocr optic charact recognit pcc potenti causal connect pdcc potenti direct causal connect pm predict minim pomdp partial observ mdp raam recurs memori rbm restrict boltzmann machin relu rectifi linear unit rl reinforc learn rnn recurr neural network resili backpropag sl supervis learn slim nn neural network sota tree algorithm svm support vector machin tdnn neural network timit continu speech corpu ul unsupervis learn wta walding lee rnn learn program mix sequenti parallel inform process natur ficient way exploit massiv parallel view crucial sustain rapid declin comput cost observ past year rest thi paper structur follow section duce compact notat simpl yet gener enough accommod fnn rnn section introduc concept credit assign path cap measur whether learn given nn applic deep shallow type section list recur theme dl sl ul rl section cuse sl ul ul facilit sl although pure sl ha becom domin recent competit section section arrang histor timelin format subsect import inspir technic contribut section deep rl discuss tradit dynam program dp rl combin search techniqu sl ul deep nn well gener method direct indirect search weight space deep fnn rnn ing success polici gradient evolutionari method notat activ spread nn throughout thi paper let j k p q r denot posit integ variabl assum rang implicit given context let n denot posit integ constant nn topolog may chang time section ani given moment describ finit subset unit node neuron n finit set h n direct edg connect node fnn acycl graph rnn cyclic first input layer set input unit subset fnn kth layer k set node u edg path length k longer path input unit may shortcut connect distant layer process fulli connect rnn unit connect unit nn behavior program determin set valu possibl modifi paramet weight wi n focu singl finit episod epoch tion process activ spread without learn weight chang follow slightli unconvent notat design compactli describ happen dure time system dure episod partial causal sequenc xt real valu call event xt either put set environ activ unit may directli depend xk k current nn depend set int indic k repres incom causal nection link let function v encod topolog inform map event index pair k weight indic ampl case may xt ft nett nett xkwv k addit case nett xkwv k multipl case ft typic linear activ function tanh mani recent nn section also event type xt xk network type may also use complex polynomi activ function section xt may directli affect certain xk k outgo connect link repres current set outt indic k event call output event note mani xt may refer differ activ unit rnn william unfold time also fnn sequenti expos input pattern larg train set encod input event dure episod weight schmidhub neural network may get reus way rnn convolut nn section call thi weight share across space time weight share may greatli reduc nn descript complex number bit inform requir describ nn section supervis learn sl certain nn output event xt may associ label target dt yield error et et xt typic goal vise nn train find weight yield episod small total error e sum et hope nn gener well later episod caus onli small error viousli unseen sequenc input event mani altern error function sl ul possibl sl assum input event independ earlier output event may affect environ action caus subsequ percept thi assumpt doe hold broader field sequenti decis make reinforc learn rl hutter kaelbl littman moor sutton barto wier van otterlo section rl input event may encod reward signal given environ typic goal find weight yield episod high sum reward signal sequenc appropri output action section use notat abov compactli describ central algorithm dl name backpropag bp supervis fnn rnn fnn may view rnn certain fix zero weight section address gener rl case depth credit assign path cap problem measur whether credit assign given nn tion deep shallow type introduc concept credit assign path cap chain possibl causal link event section input hidden output layer fnn transform time rnn let us first focu sl consid two event xp xq q depend applic may potenti direct causal connect pdcc express boolean cate pdcc p q true onli p element list p q defin cap minim one p learn algorithm may allow chang wv p q prove perform futur episod gener possibl indirect potenti causal connect pcc express recurs defin boolean predic pcc p q sl case true onli pdcc p q pcc p k k pdcc k q latter case append q ani cap p k yield cap p q thi sive definit set cap may larg finit note weight may affect mani differ pdcc tween success event list given cap case rnn fnn suppos cap ha form k q k possibl q first success element modifi wv k length suffix list q call cap depth modifi link thi depth limit far backward credit assign move causal chain find modifi suppos episod event sequenc xt satisfi comput criterion use decid whether given problem ha solv total error e threshold set altern would count onli modifi link measur depth mani typic nn applic thi would make differ would section use weight call solut problem depth deepest cap within sequenc call solut depth may solut yield differ event sequenc differ depth given fix nn topolog smallest depth ani solut call problem depth sometim also speak depth architectur sl fnn fix topolog impli maxim lem depth bound number layer certain sl rnn fix weight connect except output unit jaeger maass natschläger markram schrauwen verstraeten van campenhout imal problem depth becaus onli final link respond cap modifi gener howev rnn may learn solv problem potenti unlimit depth note definit abov sole base depth causal chain agnost tempor distanc event exampl shallow fnn perceiv larg time dow input event may correctli classifi long input sequenc appropri output event thu solv shallow lem involv long time lag relev event problem depth doe shallow learn end deep learn begin discuss dl expert yet yield conclus respons thi question instead commit precis answer let defin purpos thi overview problem depth requir veri deep learn difficulti problem may littl depth nn quickli learn solv certain deep problem random weight guess section type direct search section indirect search section weight space train nn first shallow problem whose solut may gener deep problem collaps sequenc non linear oper singl non linear oper see analysi aspect deep linear network baldi hornik section b gener howev find nn precis model given train set problem blum rivest judd also case deep nn de souto souto oliveira síma windisch compar survey neg result síma section abov focus sl gener case rl unknown environ pcc p q also true xp output event xq ani later input action may affect viron thu ani later percept real world environ may even influenc event comput physic hardwar entangl entir univers thi ignor possibl model replac ifiabl environment pcc part nn ha readi learn predict unit input event includ reward signal former input event action section weight frozen help assign credit still modifi weight use comput action tion thi approach may lead veri deep cap though dl research automat rephras problem depth reduc section particular time ul use make sl problem less deep section often dynam program section use facilit tain tradit rl problem section section focus cap sl section complex case rl recur theme deep learn dynam program learn one recur theme dl dynam program dp man help facilit credit assign schmidhub neural network certain assumpt exampl sl nn backpropag self view method section dition rl base strong markovian assumpt method help greatli reduc problem depth section dp algorithm also essenti system combin cept nn graphic model hidden markov model hmm baum petri stratonovich tion maxim em dempster laird rubin friedman hasti tibshirani baldi chauvin bengio bishop bottou bourlard morgan dahl yu deng acero hasti tibshirani friedman hinton deng et al jordan sejnowski poon domingo wu shao unsupervis learn ul facilit sl rl anoth recur theme ul facilit sl tion rl section ul section normal use encod raw incom data video speech stream form conveni subsequ ing particular code describ origin data less dundant compact way fed sl section rl machin section whose search space may thu becom smaller whose cap shallow necessari deal raw data ul close connect topic regular compress section learn hierarch represent deep sl ul rl mani method good artifici intellig fai nilsson well recent approach ai sell norvig canni malik edward machin ing mitchel learn hierarchi abstract data represent exampl certain method syntact pattern recognit fu grammar induct discov hierarchi formal rule model observ partial un supervis autom lenat lenat brown continu learn concept combin previous learnt concept hierarch represent ing bengio courvil vincent deng yu ring also recur theme dl nn sl section sl section hierarch rl tion often abstract hierarch represent natur data compress section section occam razor compress minimum descript length mdl occam razor favor simpl solut complex one given program languag principl minimum tion length mdl use measur complex solut candid length shortest program pute blumer ehrenfeucht haussler warmuth chaitin grünwald myung pitt kolmogorov levin li vitányi rissanen solomonoff wallac boulton method explicitli take account program runtim allend schmidhub watanab mani consid onli program stant runtim written program languag hinton van camp rissanen nn case mdl principl suggest low nn weight complex spond high nn probabl bayesian view buntin weigend de freita mackay neal high gener perform baum haussler without overfit train data mani method propos regular nn search comput simpl sl nn section rl nn section thi close relat certain ul method section fast graphic process unit gpu dl nn previou millennium saw sever attempt ate fast hardwar faggin heemskerk jackel et korkin de gari ger hemmi ramach et urlb widrow rumelhart lehr exploit standard hardwar anguita gome anguita parodi zunino muller gunzing guggenbühl new millennium brought dl form cheap graphic card gpu gpu wide use video game huge competit market ha driven hardwar price gpu excel fast matrix vector multipl requir onli vinc virtual realiti also nn train speed learn factor base fnn implement section greatli tribut recent success contest pattern recognit section imag segment section ject detect section supervis nn help unsupervis nn main focu current practic applic supervis learn sl ha domin recent pattern recognit contest section sever method howev use addit unsupervis learn ul facilit sl section doe make sens treat sl ul section often method bp section use optim object function ul sl boundari sl ul may blur exampl come time seri predict sequenc classif section histor timelin format help arrang subsect import inspir technic contribut although subsect may span time interv mani year section briefli mention earli shallow nn model sinc section addit earli neurobiolog inspir relev modern deep learn dl section gmdh network sinc knowledg first feedforward dl system section rel deep neocognitron nn veri similar certain modern deep fnn architectur combin convolut nn cnn weight pattern replic subsampl mechan section use notat section compactli describ central algorithm dl name backpropag bp supervis fnn rnn also summar histori bp beyond section describ problem encount late bp deep nn mention sever idea previou millennium overcom section discuss first hierarch stack coupl autoencod ae concept resurfac new millennium section section appli bp cnn import today dl applic section explain bp fundament dl problem gradient discov section explain deep rnn stack histori compressor ul help solv previous unlearn dl benchmark requir credit assign path cap section depth section discuss particular wta method call mp wide use today deep fnn section mention first import contest sl nn section describ pure supervis dl rnn long memori lstm problem depth section mention earli contest ensembl shallow fnn well good pattern recognit result cnn deep fnn lstm rnn schmidhub neural network section mostli deep belief network dbn relat stack autoencod ae section ul facilit subsequ sl compar section section mention first mpcnn lstm stack section focu offici competit secret test set mostli pure supervis deep nn sinc sequenc recognit imag classif imag segment object detect mani rnn result depend lstm section mani fnn result depend fnn code develop sinc section particular section section mention recent trick improv dl nn mani close relat earlier trick previou millennium section section discuss artifici nn help understand biolog nn section address possibl dl nn spike neuron earli nn sinc earli nn architectur mcculloch pitt learn first idea ul publish year later hebb follow decad brought simpl nn train sl narendra thathatchar rosenblatt widrow hoff ul grossberg kohonen von der malsburg willshaw von der malsburg well close relat associ memori field palm sens nn around even longer sinc earli pervis nn essenti variant linear regress od go back least earli gauss legendr gauss also refer hi work earli nn maxim cap depth section around visual cortex provid inspir dl section simpl cell complex cell found cat visual cortex hubel wiesel wiesel hubel cell fire respons certain properti visual sensori input orient edg complex cell exhibit spatial invari simpl cell thi inspir later deep nn architectur section use certain modern win deep learner section deep network base group method data handl network train group method data handl gmdh ivakhnenko ivakhnenko lapa ivakhnenko lapa mcdonough perhap first dl system feedforward multilay perceptron type although wa earlier work nn singl hidden layer joseph viglion unit gmdh net may polynomi activ function implement polynomi gener wide use nn activ function section given train set er increment grown train regress analysi gauss legendr section prune help separ valid set use today ogi decis regular use weed superflu unit compar section number layer unit per layer learn fashion edg thi wa first exampl hierarch resent learn nn section paper alreadi describ deep gmdh network layer ivakhnenko numer applic net farlow ikeda ochiai sawaragi ivakhnenko kondo kondo ueno kordík náplava snorek madala ivakhnenko witczak korbicz mrugalski patton convolut weight replic subsampl nitron apart deep gmdh network section tron fukushima wa perhap first artifici nn deserv attribut deep first incorpor neurophysiolog insight section introduc volut nn today often call cnn convnet typic rectangular recept field convolut unit given weight vector filter shift step step across dimension array input valu pixel imag usual sever filter result array subsequ activ event thi unit provid input unit due massiv weight replic section rel paramet section may sari describ behavior convolut layer subsampl downsampl layer consist unit whose connect origin physic neighbor convolut layer subsampl unit becom activ least one input activ respons insensit certain small imag shift compar section neocognitron veri similar architectur modern pure supervis feedforward deep learner altern convolut downsampl layer section fukushima howev set weight supervis backpropag section local unsupervis learn rule fukushima sens care dl problem section although hi architectur wa ativ deep inde downsampl purpos use spatial averag fukushima instead mp section current particularli conveni popular wta mechan today dl combin cnn mp bp also profit lot later work section beyond develop backpropag bp nn minim error gradient descent hadamard paramet space complex nonlinear differenti leibniz system ha cuss least sinc earli amari bryson bryson denham bryson ho director rohrer dreyfu kelley pontryagin skii gamrelidz mishchenko wilkinson initi within framework equat calculu variat euler steepest descent weight space system form bryson bryson ho kelley ate chain rule leibniz l hôpital à la dynam program dp bellman simplifi deriv thi backpropag method use chain rule onli dreyfu system alreadi effici dp sens howev backpropag deriv inform standard jacobian matrix calcul one layer viou one without explicitli address either direct link across sever layer potenti addit effici gain due work sparsiti perhap enhanc seem obviou author given prior work learn multilay system see also section deep nonlinear net sinc schmidhub neural network seem surpris hindsight book minski pert limit simpl linear perceptron singl layer section discourag research ther studi nn explicit effici error backpropag bp arbitrari crete possibl spars connect network appar wa first describ master thesi linnainmaa albeit without refer nn bp also known vers mode automat differenti griewank cost forward activ spread essenti equal cost backward deriv calcul see earli fortran code linnainmaa close relat work ostrovskii volin borisov effici bp wa soon explicitli use minim cost function adapt control paramet weight dreyfu pare preliminari discuss werbo tion method multilay threshold nn bobrowski comput program automat deriv implement bp given differenti system speelpen knowledg first applic effici bp abov wa describ werbo late work wa publish sever year later lecun parker paper significantli contribut ular bp nn rumelhart hinton william experiment demonstr emerg use intern represent hidden layer see gener process recurr nn atiya parlo baldi gherriti kremer kolen pearlmutt robinson fallsid rohwer schmidhub werbo william william peng william zipser also equilibrium rnn almeida pineda stationari input bp feedforward nn fnn recurr nn rnn use notat section fnn rnn episod activ spread tiabl ft singl iter gradient descent bp comput chang wi proport rithm addit case weight wi ate variabl initi algorithm one iter bp fnn rnn comput initi error signal variabl δt xt input event continu next iter error et δt xt add δt valu wv k δk thi eleg effici recurs chain rule applic collect impact nett futur event multipli δt f nett k add k valu xkδt end chang wi proport small learn rate comput cost backward bp pass essenti forward pass section forward backward pass suffici perform reach thi simpl bp method still central learn algorithm fnn rnn notabl nn section augment supervis bp sort unsupervis learn discuss section late beyond numer improv nn late seem clear bp section wa panacea fnn applic focus fnn hidden layer addit hidden layer often seem offer empir benefit mani practition found solac theorem hornik stinchcomb white kolmogorov state nn singl layer enough hidden unit approxim ani multivari continu function arbitrari accuraci likewis rnn applic requir ing error far mani research help rnn first ing shallow problem section whose solut gener deeper problem fact popular rnn rithm restrict credit assign singl step backward man jordan also recent studi jaeger maass et gener speak although bp allow deep problem principl seem work onli shallow problem late earli saw idea potenti come thi problem wa fulli understood onli tion idea deal long time lag deep cap deal long time lag relev event eral sequenc process method propos includ cuse bp base decay factor activ unit rnn mozer neural network tdnn lang waibel hinton adapt extens hausen waibel nonlinear autoregress exogen input narx rnn lin horn tino gile certain archic rnn hihi bengio compar section rl economi rnn wta unit local learn rule schmidhub method bengio simard frasconi de vri princip plate ring sun chen lee howev algorithm either work shallow cap onli could gener unseen cap depth problem greatli vari time lag vant event need extern fine tune delay constant fere problem fact turn certain simpl deep benchmark problem use evalu method quickli solv randomli guess rnn weight lution found hochreit schmidhub rnn method abov design dl poral sequenc neural heat exchang schmidhub consist two parallel deep fnn opposit flow direct input pattern enter first fnn propag sire output target enter opposit fnn gate use local learn rule layer net tri similar inform content preced layer adjac layer net input enter first net slowli heat becom target target ing opposit net slowli cool becom input helmholtz machin dayan hinton dayan hinton neal zemel may view unsupervis section variant thereof peter dayan person commun hybrid approach shavlik towel towel shavlik initi potenti deep fnn domain theori proposit logic may acquir base learn dejong mooney minton et mitchel keller nn bp section nn depth reflect longest chain schmidhub neural network reason origin set logic rule extens thi approach maclin shavlik shavlik initi rnn domain knowledg express finit state automaton fsa ha becom import later dl system ul section better bp advanc gradient descent compar tion numer improv steepest descent bp section propos method newton gauss levenberg marquardt newton schaback werner method bfg broyden et fletcher powel goldfarb shanno comput expens larg nn partial bfg battiti saito nakano conjug gradient hesten stiefel møller well method cauwenbergh schmidhub solla provid sometim use fast altern bp treat linear problem bärmann gradient inform pass back preced layer speed bp momentum wa introduc rumelhart et constant ad slope linear activ function fahlman nonlinear slope wa exagger west saad onli sign error deriv taken account success wide use bp variant riedmil braun robust variat igel hüsken wa also success appli rnn local gradient normal base nn ture schraudolph sejnowski diagon sian approach becker le cun relat effici method schraudolph algorithm control bp step size adapt global learn rate battiti laped farber lecun simard pearlmutt vogl mangi rigler zink alkon yu chen cheng comput individu learn rate weight jacob silva almeida line learn bp appli pattern present algorithm neuneier zimmermann set weight learn rate invers proport empir dard deviat local gradient thu normal tic weight fluctuat compar local onlin step size adapt method nonlinear nn almeida almeida langloi amar redol mani addit trick improv nn describ montavon orr müller orr müller compar section recent develop mention section search simpl nn section mani research use method search simpl nn section high gener biliti approach address dilemma geman bienenstock doursat strong prior assumpt exampl weight decay hanson pratt krogh hertz weigend rumelhart huberman encourag zero weight penal larg weight bayesian work bay weight decay deriv hinton van camp gaussian laplacian weight prior gauss laplac see also murray edward tension thi approach postul distribut network mani similar weight gener gaussian mixtur ter priori nowlan hinton often weight prior implicit addit penalti term mackay method base valid set craven wahba eubank golub heath wahba hasti tibshirani mostel tukey stone akaik inform criterion final predict error akaik gener predict error moodi moodi utan see also amari murata guyon vapnik boser bottou solla holden vapnik wang venkatesh judd wolpert similar prior bias toward simplic implicit construct prune algorithm sequenti network construct ash burgess fahlman fritzk gallant honavar uhr ivakhnenko moodi parekh yang honavar ring utgoff stracuzzi weng ahuja huang see also section input prune moodi refen zaprani franci unit prune ivakhnenko levin leen moodi mozer smolenski white weight prune optim brain damag lecun denker solla optim brain surgeon hassibi stork veri gener alway practic approach ere sl nn rl nn search among weight program written univers program languag bia toward fast short program ber section flat minimum search fm hochreit schmidhub search flat minimum error function larg connect region weight space error low remain approxim constant bit inform requir describ weight high varianc compar perturb toler condit bishop carter rudolph nucci hanson kerlirzin vallet matsuoka minai william murray edward neti schneider young bayesian argument suggest flat minima correspond simpl nn low expect overfit compar section recent develop mention section potenti benefit ul sl compar section notat section introduc label dt mani paper previou millennium howev unsupervis learn ul without teacher atick li redlich baldi hornik barlow kaushal mitchison barrow deco parra field földiák földiák young grossberg hebb kohonen kosko martinetz ritter schulten miller mozer oja palm pearlmutt hinton ritter kohonen rubner schulten sanger saund von der malsburg watanab willshaw von der malsburg see also work franziu sprekel wiskott waydo koch wiskott sejnowski mani ul method design maxim boltzmann kullback leibler shannon object amari cichocki yang barlow et dayan zemel deco parra field hinton dayan frey neal linsker mackay miller plumbley redlich schmidhub schraudolph sejnowski zemel zemel hinton mani thi uncov disentangl hidden ing sourc signal andrad chacon merelo moran bell sejnowski belouchrani doso moulin cardoso comon hyvärinen schmidhub neural network karhunen oja jutten herault karhunen sensalo molgedey schuster schuster cottrel zhang cottrel szabó póczo lőrincz mani ul method automat robustli gener tribut spars represent input pattern falconbridg stamp badcock földiák hinton ghahramani hochreit schmidhub hyvärinen hoyer oja lewicki olshausen featur tector olshausen field schmidhub eldrach foltin structur well orient sensit edg detector gabor filter gabor extract simpl featur relat observ earli visual stage biolog system de valoi albrecht thorel jone palmer ul also serv extract invari featur differ data item becker coupl nn observ two differ input schmidhub preling also call siames nn bromley et chen salman hadsel chopra lecun taylor spiro bregler fergu ul help encod input data form advantag process context dl one import goal ul redund reduct ideal given ensembl input pattern redund reduct deep nn creat factori code code statist independ compon ensembl barlow barlow et disentangl unknown factor variat compar bengio et code may spars advantag data compress speed subsequ bp becker trivial task subsequ naiv yet optim bay classifi schmidhub et earli ul fnn singl layer method deeper ul fnn includ hierarch section kohonen map dittenbach merkl rauber koikkalainen oja lampinen oja rauber merkl dittenbach versino gambardella hierarch gaussian potenti function network lee kil ul featur hierarchi fed sl classifi behnk organ tree algorithm sota herrero valencia dopazo nonlinear autoencod ae layer demer cottrel kramer oja ae nn rumelhart et train map input pattern themselv exampl compactli encod activ unit narrow bottleneck hidden layer certain nonlinear ae suffer certain limit baldi lococod hochreit schmidhub use fm tion find ae weight describ bit inform often produc spars factori code predict minim pm schmidhub search factori code nonlinear featur tector fight nonlinear predictor tri becom inform unpredict possibl ul wa appli onli fnn also rnn lindstädt schmidhub compar section rnn stack well later ul rnn schraudolph schmidhub steil ul autoencod ae hierarchi compar section perhap first work studi potenti benefit wa publish propos unsupervis ae hierarchi ballard close relat certain feedforward deep learner base ul section level ae nn singl hidden layer train map input pattern themselv hidden layer code fed ae type hope code hidden ae layer properti facilit subsequ learn one experi particular learn algorithm differ tradit bp section wa use learn map ae stack thi type ul ballard thi wa faster learn equival map bp singl deeper ae without hand task realli requir deep ae benefit ul obviou thi experi compar earli survey hinton somewhat relat recurs memori raam melnik levi pollack pollack origin use encod sequenti linguist structur arbitrari size fix number hidden unit recent raam also use unsupervis facilit deep credit assign rl gisslen luciw graziano schmidhub section principl mani ul method section could stack like ae abov rnn section restrict boltzmann machin rbm section hierarch kohonen net section facilit subsequ sl compar stack gener ting witten wolpert fnn profit competit ul rumelhart zipser prior maclin shavlik see also recent method use ul improv subsequ sl behnk wiskott bp convolut nn cnn section backpropag section wa appli lecun et lecun boser et lecun bottou bengio haffner tional neural layer section adapt connect thi combin augment mp section sped graphic card section ha becom sential ingredi mani modern ward visual deep learner section thi work also introduc mnist data set handwritten digit lecun et time ha becom perhap famou benchmark machin learn cnn help achiev good formanc mnist lecun boser et cap depth fingerprint recognit baldi chauvin similar cnn use commerci fundament deep learn problem gradient descent diploma thesi hochreit repres mileston explicit dl research mention section late experi indic tradit deep feedforward recurr network hard train backpropag bp section hochreit work formal identifi major reason typic deep nn suffer famou problem vanish explod gradient standard activ function section cumul backpropag error signal section either shrink rapidli grow bound fact decay exponenti number layer cap depth section explod thi also known long time lag problem much subsequ dl research wa motiv thi insight later work bengio et also studi basin attract stabil nois dynam system point view either dynam robust nois gradient vanish see also hochreit bengio frasconi schmidhub tiňo hammer year sever way partial overcom fundament deep learn problem explor schmidhub neural network veri deep learner histori compressor section allevi problem unsupervis hierarchi rnn thi greatli facilit sequent supervis credit assign bp section fnn case similar effect achiev ceptual relat ae stack section deep belief network dbn section ii network section viat problem special architectur unaffect iii today comput million time putat power desktop machin earli thi allow propag error layer within reason time even tradit nn section basic win mani imag recognit petit section although thi doe realli overcom problem fundament way iv optim section allevi problem fnn marten møller pearlmutt schraudolph section rnn marten sutskev section space nn weight matric also search without reli error gradient thu avoid fundament deep learn problem altogeth random weight guess time work better sophist method iter schmidhub certain complex problem better solv use univers search levin weight program written univers gram languag schmidhub better solv use linear method obtain optim weight connect output event section evolv weight connect call evolino huber wierstra gagliolo gomez compar also late rnn certain ul rule steil also case spike neuron klampfl maass yin meng jin section direct search method relev onli sl also gener rl discuss detail section histori compress deep stack rnn work veri deep learner section schmidhub could perform credit assign across hundr nonlinear oper neural layer use unsupervis train hierarchi rnn basic idea still relev today rnn train unsupervis fashion predict next input martin atla dorffner onli expect input error convey new inform get fed next higher rnn thu tick slower time scale easili shown inform get lost get compress much machin learn essenti compress section ual input sequenc get seri less less redund code deeper deeper level thi histori compressor neural sequenc chunker compress data space like feedforward nn time thi anoth good exampl hierarch represent learn section also continu variant histori compressor schmidhub mozer preling rnn stack essenti deep gener model data reconstruct compress form ad anoth rnn stack improv bound data tion length equival neg logarithm iti huffman shannon long remain local learnabl predict data represent respond level hierarchi compar similar observ feedforward deep belief network dbn section system wa abl learn mani previous unlearn dl task one ancient illustr dl experi schmidhub requir cap section depth top level code initi unsupervis rnn stack howev got compact previous infeas sequenc classif tional sl becam possibl essenti system use ul greatli reduc problem depth compar earlier tune nn initi rule proposit logic shavlik towel section way compress higher level lower level thu fulli partial collaps rnn stack trick retrain rnn continu imit predict hidden unit alreadi train slower rnn consciou chunker addit predict output ron schmidhub thi help lower rnn atiz develop appropri rare chang memori may bridg veri long time lag thi procedur greatli reduc requir depth bp process system wa work deep learner modern sens also first neural hierarch tempor ori htm conceptu similar earlier ae hierarchi section later deep belief network section gener sens use rnn instead fnn unchang input recent known entrepreneur hawkin georg kurzweil also got interest htm compar also hierarch hmm fine singer tishbi well later rent system klampfl maass et steil young davi mishtal arel clockwork rnn koutník greff gomez schmidhub also consist teract rnn modul differ clock rate use ul set rate stack rnn use later work sl great success section mp toward mpcnn compar section neocognitron section inspir cresceptron weng et adapt topolog dure train tion compar increment grow shrink gmdh network section instead use altern local subsampl wta od fukushima maass schmidhub cresceptron use mp layer dimension layer array unit activ partit smaller rectangular array replac downsampl layer activ maxim activ unit later complex version cresceptron weng ahuja huang also includ blur layer improv object locat anc neurophysiolog plausibl topolog feedforward hmax model riesenhub poggio veri similar one cresceptron thu neocognitron hmax doe learn though unit weight biolog plausibl learn rule later propos similar model serr riesenhub louie poggio teichmann wiltschut hamker cnn convnet section combin mp becom mpcnn altern convolut layer unlik cresceptron hmax howev mpcnn train bp section ranzato huang boureau lecun advantag thi point subsequ scherer müller behnk mpcnn becom central mani modern feedforward visual deep learner section schmidhub neural network earli nn back certain nn alreadi certain control pattern recognit contest secret test set notabl nn intern delay line santa fe competit chaotic intens pulsat laser wan weigend gershenfeld veri deep cap section need though supervis recurr veri deep learner lstm rnn supervis long memori lstm rnn ger schmidhub cummin hochreit schmidhub ger eck schmidhub could eventu perform similar feat deep rnn hierarchi section overcom fundament deep learn problem section without ani unsupervis lstm could also learn dl task without local sequenc predict thu unlearn partial unsupervis histori compressor section deal veri deep problem section ger schraudolph schmidhub basic lstm idea veri simpl unit call constant error carousel cec cec use activ function f ident function ha connect fix weight due f constant deriv ror backpropag cec vanish explod tion stay unless flow cec typic adapt part nn cec connect sever nonlinear adapt unit multipl vation function need learn nonlinear behavior weight chang unit often profit error signal propag far back time cec cec main reason whi lstm net learn discov import memor event happen thousand discret time step ago previou rnn alreadi fail case minim time lag step mani differ lstm variant topolog allow possibl evolv good topolog bayer wierstra togeliu schmidhub lstm variant also use modifi cec ger schmidhub certain extent lstm biolog plausibl reilli lstm learn solv mani previous unlearn dl task involv recognit tempor order wide separ event noisi input stream robust storag precis real number across extend time interv arithmet oper continu input stream extract inform convey tempor distanc event recognit tempor extend pattern noisi input sequenc ger et hochreit schmidhub stabl gener precis time rhythm well smooth period trajectori ger schmidhub lstm clearli outperform previou rnn task requir learn rule regular languag describ determinist finit state automata fsa blair pollack casey kalink lehmann manolio fanelli omlin gile siegelmann vahe omlin watrou kuhn zeng goodman smyth term reliabl speed lstm also work task involv context free languag cfl repres hmm similar fsa discuss rnn literatur andrew diederich tickl rodriguez wile rodriguez wile elman steijver grunwald sun gile chen lee tonk wile wile elman cfl recognit lee requir function equival runtim stack previou rnn fail learn small cfl train set rodriguez wile bodén wile rodriguez et fail extract gener rule gener well substanti larger test set similar languag csl chalup blair lstm gener well though requir onli shortest exemplar n csl anbncn correctli predict possibl continu sequenc prefix n combin decoupl extend kalman filter feldkamp prokhorov eagen yuan feldkamp prokhorov feldkamp haykin kalman puskoriu feldkamp william lstm rnn et learn deal correctli valu n million train network wa abl read sequenc symbol one symbol time final detect subtl differ legal string veri similar illeg string compar also recent rnn algorithm abl deal long time lag koutník et marten sutskev schäfer udluft zimmermann zimmermann tietz grothmann rnn brnn schuster schuster paliw design input sequenc whose start end known advanc spoken sentenc label phonem compar fukada schuster sagisaka take past futur context sequenc element account one rnn process sequenc start end backward end start time step combin output predict correspond label ani brnn success appli secondari protein structur predict baldi brunak frasconi pollastri soda baldi pollastri wu baldi gener brnn multipl dimens learn predict properti small organ molecul lusci pollastri baldi well protein contact map tegg wang eickholt cheng also conjunct grow deep fnn di lena nagata baldi section brnn unfold full potenti combin lstm concept grave et grave schmidhub particularli success recent competit stack tion lstm rnn fernandez grave schmidhub grave schmidhub train connectionist poral classif ctc grave fernandez gomez ber method find rnn weight maxim probabl label sequenc given typic much longer stream input vector perform simultan mentat align recognit section earli speech recognit wa domin hmm combin fnn bourlard morgan nevertheless train scratch utter tidigit speech databas lstm alreadi obtain result compar system bering grave schiel schmidhub grave eck bering schmidhub grave et lstm outperform hmm keyword spot task fernández grave schmidhub compar recent improv indermuhl frinken fischer bunk wöllmer schuller rigol lstm also achiev best known result famou timit phonem recognit benchmark grave moham hinton section recent lstm hybrid obtain best known perform geiger zhang wening schuller rigol speech recognit sak senior beaufay lstm also applic robot local förster grave schmidhub robot control mayer et line driver distract detect wöllmer et mani schmidhub neural network task exampl help improv state art divers applic protein analysi hochreit obermay handwrit recognit bluch et grave fernandez liwicki bunk schmidhub grave et grave schmidhub voic activ tion eyben wening squartini schuller optic acter recognit breuel shafait languag identif sak moreno prosodi contour predict fernandez rendel ramabhadran hoori audio onset tection marchi et synthesi fan qian xie soong social signal classif brueckner ter machin translat sutskev vinyal le rnn also use metalearn prokhorov feldkamp tyukin schaul schmidhub schmidhub becaus principl learn run weight chang algorithm schmidhub success metalearn hochreit younger conwel use lstm rnn quickli learn learn algorithm quadrat function compar section recent lstm rnn sever intern pattern nition competit set numer benchmark record larg complex data set section base lstm panacea method sometim perform least certain task jaeger koutník et marten sutskev pascanu mikolov bengio schmidhub et compar section nn success deep nn decad around mani practic commerci tern recognit applic domin chine learn method support vector machin svm schölkopf et vapnik nevertheless least tain domain nn outperform techniqu bay nn neal base ensembl breiman dietterich hashem schmeiser schapir ueda wolpert nn nip featur select challeng secret test set neal zhang nn wa veri deep two hidden layer thu rather shallow cap section depth import mani present pattern recogn section develop cnn depart lecun et cnn section set new mnist record simard steinkrau platt use train pattern deform baird unsupervis section standard bp net achiev simard et correspond cap depth wa low compar improv section good imag interpret result behnk achiev rather deep nn train bp variant riedmil braun section feedback recurr connect help improv imag interpret fnn cap depth use success classifi data vieira barrada deep lstm rnn start obtain certain first speech tion result compar system grave et compar section ul deep belief stack bp learn network numer layer date back least section explicit dl research result publish least sinc section express deep learn wa actual coin around unsupervis deep fnn help acceler subsequ sl bp hinton osindero teh hinton salakhutdinov compar earlier terminolog load deep network síma windisch learn deep memori gomez schmidhub compar also section section deep fnn train competit ul maclin shavlik deep belief network dbn stack restrict boltzmann machin rbm smolenski turn boltzmann machin bm hinton sejnowski singl layer unit compar also bm memisev hinton rbm perceiv pattern represent level learn encod unsupervis fashion least theori certain assumpt ad layer improv bound data neg log probabl hinton et equival data descript correspond observ rnn stack section extens tempor rbm sutskev hinton taylor without ani train pattern deform section dbn bp achiev error rate hinton salakhutdinov mnist handwritten digit section thi result help arous interest dbn dbn also achiev good result phonem recognit error rate timit core test set moham hinton compar improv fnn deng yu hinton deng et lstm rnn section techniqu call semant hash salakhutdinov hinton map semant similar document variabl size nearbi address space document represent outperform previou searcher similar document local sensit hash buhler datar immorlica indyk mirrokni see tutori fischer igel autoencod ae stack ballard section becam popular altern way deep fnn unsupervis fashion befor section bp section bengio lamblin popovici larochel erhan et vincent hugo bengio manzagol spars code section wa formul combin convex optim problem lee battl raina ng recent survey stack rbm ae method focu develop arel rose karnowski bengio unsupervis dbn ae stack conceptu similar certain sens less gener unsupervis rnn histori compressor section process onli stationari input pattern entir pattern sequenc improv stack also lecun et cnn section set new mnist record ranzato poultney chopra lecun use train pattern deform section unsupervis compar improv section similar cnn use obstacl avoid lecun muller cosatto flepp combin cnn tdnn later learn map represent sentenc featur relev languag process use combin sl ul collobert weston also saw earli cnn implement chellapilla puri simard time faster cnn compar also earlier gpu implement standard fnn report factor oh jung gpu schmidhub neural network graphic card becom import dl subsequ year section bp section wa appli first time ranzato et section like mpcnn section altern convolut layer mpcnn becom essenti ingredi mani modern win feedforward visual deep learner section also hierarch stack lstm rnn introduc fernandez et train hierarch connectionist tempor classif ctc grave et task sequenc label everi lstm rnn level section predict sequenc label fed next level error signal everi level lower level spoken digit recognit lstm stack outperform hmm despit make fewer assumpt domain lstm stack necessarili requir unsupervis like earlier rnn stack schmidhub section first offici competit rnn mpcnn stack lstm rnn train ctc section came first rnn win offici intern pattern tion contest secret test set known onli organ precis three connect handwrit competit dar three differ languag french arab farsi deep lstm rnn without ani priori linguist knowledg perform simultan segment recognit compar grave jaitli grave schmidhub grave et al grave et al schmidhub ciresan meier masci grave section detect human action surveil video cnn jain seung prokhorov combin svm wa part larger system yang et use bag featur approach nowak juri trigg extract region interest system three trecvid competit possibl first offici intern contest help mp cnn section improv version method wa publish later ji xu yang yu also saw implement raina madhavan ng order magnitud faster previou see section see also coat et al convolut dbn lee gross ranganath ng probabilist variant mp section combin idea cnn dbn wa success appli audio classif lee pham largman ng plain backprop distort gpu break mnist record new mnist section record error rate wa set good old bp section deep otherwis standard nn ciresan meier gambardella schmidhub use neither unsupervis section convolut section howev train pattern deform section import gener big train set avoid overfit thi success wa made possibl mainli gpu implement bp wa time faster standard cpu version good valu wa obtain without distort except small saccad eye section sinc bp wa decad old section pattern deform decad baird section result seem suggest advanc exploit modern comput hardwar import advanc algorithm mpcnn gpu achiev superhuman vision perform flexibl ciresan meier masci gambardella schmidhub mp cnn convnet wa describ build earlier mp work weng et section cnn fukushima lecun et section earli base cnn without mp chellapilla et section compar earli oh jung raina et section mpcnn altern convolut layer section layer mp section top standard fulli connect layer weight train bp section ranzato et scherer et becom essenti mani win fnn section ciresan meier masci huber committe breiman dietterich hashem schmeiser schapir ueda wolpert simpl democrat output averag sever mpcnn see input output vector use assign probabl variou possibl class class averag highest probabl chosen system ficat present input compar earlier sophist ensembl method schapir ble neal section recent relat work shao wu li ensembl wa first system achiev superhuman visual pattern recognit ciresan meier masci schmidhub ciresan meier masci schmidhub control competit name ijcnn traffic sign recognit contest san jose ca stallkamp schlips salmen igel thi interest fulli autonom drive car traffic dickmann et mpcnn ensembl obtain error rate wa twice better human test subject three time better closest artifici nn competitor sermanet lecun six time better best method month earlier qualifi round wa stage onlin competit albeit much smaller margin ciresan meier masci schmidhub vs second place sermanet lecun deadlin organ reveal human perform test set wa best method alreadi seem howev dure qualifi wa possibl increment gain mation test set probe repeat sion thi illustr better better result obtain variou team time stallkamp et organ eventu impos limit resubmiss final petit thi wa possibl thi illustr gener problem benchmark whose test set public least probe extent compet team tend overfit test set even directli use train onli evalu mani thought big deal human chess world champion kasparov wa beaten ibm comput back comput could compet littl kid visual pattern recognit seem much harder chess comput perspect cours traffic sign domain highli restrict kid still much better gener pattern recogn nevertheless deep nn could alreadi learn rival import limit visual domain ensembl wa also first method achiev perform around mnist ciresan meier schmidhub thi repres dramat improv sinc mnist record hover around almost decad section schmidhub neural network given prior work mp cnn section section breakthrough scientif sens commerci relev breakthrough effici code ha made differ sever contest sinc today feedforward deep nn ensembl section optim rnn also wa shown marten sutskev optim møller pearlmutt schraudolph section allevi fundament deep learn problem section rnn outperform standard lstm rnn section sever task compar rnn algorithm jaeger koutník et pascanu mikolov et schmidhub et also least sometim yield better result steepest descent lstm rnn first contest imagenet object detect segment ensembl section achiev best result imagenet classif benchmark krizhevski sutskev hinton popular comput vision commun rel larg imag size pixel necessari oppos onli pixel traffic sign competit section see improv section also biggest nn far free paramet wa train unsupervis mode section unlabel data le et appli imagenet code across top layer use train simpl supervis classifi achiev best result far class instead reli effici gpu program thi wa done brute forc standard machin core excel result achiev deep learner imag recognit classif section comput vision commun howev especi interest object detect larg imag applic search engin biomed diagnosi goal may automat detect tumor etc imag human tissu object detect present addit challeng one natur approach train deep nn classifi patch big imag use featur detector shift across unknown visual scene use variou rotat zoom factor imag part yield highli activ output unit like contain object similar nn wa train final saw first dl system ensembl mpcnn section win contest visual object detect ciresan giusti gambardella schmidhub larg imag sever million pixel icpr roux et biomed applic may turn among import applic dl world spend gdp healthcar trillion usd per year much medic diagnosi expens expert partial autom thi could onli save lot money also make expert diagnost access mani current afford gratifi observ today deep nn may actual help improv healthcar perhap save human live also saw first pure imag segment contest dl ciresan giusti gambardella schmidhub ensembl segment neuron structur em stack challeng em stack relev recent approv huge brain project europ us markram given electron microscopi imag stack thin slice anim brain goal build detail model brain neuron dendrit human expert need mani hour day week annot imag part depict neuron membran part irrelev background thi need autom turaga et deep learn solv thi task experi mani train imag contest three evalu metric larg margin superhuman perform term pixel error object detect ciresan et imag tation ciresan giusti et profit fast imag scan avoid redund comput recent mpcnn scanner speed naiv implement three order magnitud giusti ciresan masci gambardella ber masci giusti ciresan fricout schmidhub compar earlier effici method cnn without mp vaillant monrocq lecun also system consist grow deep fnn di lena et casp contest protein contact map predict benchmark lstm rnn section outperform method hmm svm onlin mode detect indermuhl frinken bunk ott krechel liwicki dengel keyword spot indermuhl et long time lag problem languag model lstm rnn outperform statist approach benchmark frinken et improv result later obtain combin nn hmm et compar earlier rnn object recognit iter imag interpret behnk behnk roja see also recent public reilli wyatt herd mingu jilk wyatt curran reilli extend work biolog plausibl learn rule rnn reilli contest benchmark record stack fernandez et grave schmidhub section lstm rnn grave schmidhub train ctc section broke famou timit speech phonem recognit record achiev test set error rate grave et despit thousand man year previous spent hidden markov model hmm speech recognit research compar earlier dbn result section also help score first nist evalu bluch et optic charact recognit ocr lstm rnn outperform commerci recogn histor data breuel et system also set benchmark record languag identif dominguez et speech recognit geiger et prosodi contour predict fernandez et audio onset detect marchi et synthesi fan et social signal classif brueckner schulter lstm rnn wa use estim state posterior hmm thi system beat previou state art larg vocabulari speech recognit sak senior et sak vinyal et anoth lstm rnn hundr million mention howev lstm rnn alreadi perform simultan segment recognit becam first recurr deep learner win offici intern pattern recognit section schmidhub neural network connect wa use rerank hypothes statist machin translat system thi system beat previou state art english french translat sutskev et new record icdar chines handwrit recognit benchmark class wa set desktop machin ensembl section almost human perform ciresan schmidhub compar yin wang zhang liu miccai grand challeng mitosi detect veta viergev pluim stathoniko van diest also wa ensembl ciresan et data set wa even larger challeng one icpr section data set includ mani ambigu case frequent encount problem imperfect slide stain three instead mp section observ three orthogon project imag outperform tradit full method task segment tibial cartilag low field knee mri scan prasoon et deep section also help achiev new best result import benchmark comput vision commun imagenet classif szegedi et zeiler fergu conjunct tradit approach pascal object detect girshick donahu darrel malik also learn predict bound box coordin object imagenet databas obtain result task local detect sermanet et also help recogn number googl street view imag goodfellow bulatov ibarz arnoud shet part nn wa train count visibl digit compar earlier work detect numeros dbn stoianov zorzi thi system also excel recogn distort synthet text recaptcha puzzl success cnn applic includ scene pars farabet coupri najman lecun object detect szegedi toshev erhan shadow detect khan bennamoun sohel togneri video classif karpathi et alzheim diseas neuroimag li et addit contest mention web page swiss ai lab idsia univers toronto ny univers univers montreal current success techniqu lstm rnn benchmark deep learner actual use one two supervis techniqu recurr lstm train ctc section b feedforward section base cnn section mp section train bp section except includ two contest goodfellow courvil bengio mesnil et special transfer learn one data set anoth caruana pan yang schmidhub howev deep allow pure transfer ciresan meier schmidhub one train set greatli improv perform quit differ set also recent studi donahu et oquab bottou laptev sivic fact deep mpcnn sl extract use featur quit divers imag yield better result tradit wide use featur sift low mani vision task razavian azizpour sullivan carlsson deal chang data set slowli learn deep nn also combin rapidli adapt surfac nn kak chen wang remark trend went partial vise rnn stack section pure supervis lstm rnn section like trend went partial supervis fnn stack section pure supervis mpcnn section nevertheless mani applic still advantag combin best learn unsupervis section recent trick improv sl deep nn compar section dbn train section improv ent enhanc automat learn rate adjust dure stochast gradient descent cho cho raiko ilin tikhonov arsenin john ular rbm cho ilin raiko contract ae fai vincent muller glorot bengio discourag hidden unit perturb respons input perturb similar fm section lococod ae section discourag output perturb respons weight perturb hierarch cnn neural abstract pyramid behnk train reconstruct imag corrupt structur nois behnk thu enforc increasingli abstract imag represent deeper deeper layer denois ae later use similar procedur vincent et dropout ba frey hinton srivastava krizhevski sutskev salakhutdinov remov unit nn dure train improv gener view ensembl method train multipl data model simultan baldi sadowski certain circumst could also view form train set augment effect inform complex featur remov train data compar dropout rnn pachitariu sahani pascanu gulcehr cho bengio pham kermorv louradour determinist approxim coin fast dropout wang man lead faster learn evalu wa adapt rnn bayer osendorf chen urban van der smagt dropout close relat older biolog plausibl techniqu ad nois neuron synaps dure train hanson jim gile horn murray edward nadal parga schuster turn close relat find nn fm section stochast variat method grave also relat fm use rnn classic regular weight decay section repres bia toward limit memori capac pascanu mikolov et compar recent work variat recurr ae bayer osendorf activ function f rectifi linear unit relu f x x x f x old concept rectifi unit malik perona relu nn use rbm maa hannun ng nair hinton outperform sigmoid activ function deep nn glorot bord bengio help obtain best result sever benchmark problem across multipl domain dahl sainath hinton krizhevski et nn compet linear unit tend outperform nonlinear unit avoid catastroph forget bp train set chang time srivastava masci kazerounian gomez schmidhub thi context choos learn algorithm may import choos activ function goodfellow mirza da courvil bengio maxout nn goodfellow mirza courvil bengio combin competit interact dropout see abov achiev excel result certain schmidhub neural network benchmark compar earli rnn compet unit sl rl schmidhub address overfit instead depend regular bishop hertz krogh palmer rnn slim nn compet unit schmidhub principl learn select runtim number effect free paramet thu learn comput regular section becom fast slim necessari one may penal total length connect clune mouret lipson legenstein maass schmidhub commun cost slim nn implement hardwar expect futur rmsprop schaul zhang lecun tieleman hinton speed first order gradient descent method section compar neuneier zimmermann adagrad duchi hazan singer adadelta zeiler dl nn also improv transform hidden unit activ zero output slope averag raiko valpola lecun mani addit older trick section also applic today deep nn compar montavon et orr müller consequ neurosci iron artifici nn ann help better stand biolog nn bnn isbi result mention section ciresan giusti et segment ronal structur em stack challeng featur detector learn visual ann similar found earli visual process stage bnn section likewis featur detector learn deep layer visual ann highli predict neuroscientist find deep layer bnn visual cortex bnn may use quit differ learn algorithm object function minim may quit similar one visual ann fact result obtain rel deep artifici dbn lee ekanadham ng cnn yamin hong cadieu dicarlo seem compat insight visual pathway primat cerebr cortex ha studi mani decad bichot rossi desimon connor brincat pasupathi desimon albright gross bruce dicarlo zoccolan rust felleman van essen hubel wiesel hung kreiman poggio dicarlo kobatak tanaka kriegeskort et lenni movshon logotheti paul poggio perrett hietanen oram benson roll perrett roll caan compar comput survey kruger et dl spike neuron mani recent dl result profit tradit deep nn section current gpu howev littl oven much hungrier energi biolog brain whose neuron effici commun brief spike fitzhugh hodgkin huxley nagumo arimoto yoshizawa often remain quiet mani comput model spike neuron propos analyz amit brunel boht kok la poutr brea senn pfister brett et brunel deco roll gerstner kistler gerstner van hemmen hoerzer legenstein maass izhikevich et kasabov kempter gerstner van hemmen kistler gerstner van hemmen maass maex orban nessler pfeiffer bues maass rezend gerstner seung song miller abbott stemmler stoop schindler bunimovich tsodyk pawelzik markram tsodyk skagg sejnowski mcnaughton zipser keho littlewort fuster futur hardwar dl nn may implement aspect model fier schemmel meier glackin mcginniti maguir wu belatrech indiveri et jin et khan et liu et merolla et neil liu roggen hofmann thoma floreano schemmel grubl meier mueller gotarredona et simul spike variant neftci da pedroni cauwenbergh rbm section wa train variant contrast diverg algorithm hinton spike net evolv achiev reason perform small face recognit data set wysoski benuskova kasabov control simpl robot floreano mattiussi hagra colley callaghan clark spike dbn neuron part larger nn eliasmith eliasmith et achiev error rate mnist compar similar result spike dbn variant depth use neuromorph sensor connor neil liu delbruck pfeiffer practic applic howev current artifici network spike neuron yet compet best tradit deep nn compar mnist result section dl fnn rnn reinforc learn rl far focus deep learn dl supervis unsupervis nn nn learn classifi pattern pattern sequenc learn act gener sens reinforc learn rl unknown environ see survey kaelbl et sutton barto wier van otterlo add discuss dl fnn rnn rl shorter discuss fnn rnn sl ul section reflect current size variou field without teacher sole occasion pain pleasur signal rl agent must discov interact dynam initi unknown environ maxim pect cumul reward signal section may bitrari priori unknown delay action perceiv consequ problem hard ani problem comput scienc sinc ani task comput descript mulat rl framework hutter exampl answer famou question whether p np cook levin would also set limit achiev eral rl compar specif limit blondel li madani hank condon vlassi littman barber follow subsect mostli focu tain obviou intersect dl serv gener rl survey rl nn world model yield rnn deep cap special case rl fnn control c interact determinist predict environ separ fnn call learn becom c world model system identif predict c input previou action input chocki unbehauen ge hang lee zhang gomi kawato jordan jordan rumelhart levin narendra ljung miller werbo sutton munro narendra parthasarathi prokhorov riu feldkamp robinson fallsid schmidhub werbo assum ha schmidhub neural network learn produc accur predict use tute environ c form rnn put becom input c whose output action turn becom input bp rnn section use achiev desir input event high reward signal weight remain fix gradient inform c weight propag back c back etc certain extent approach also applic probabilist uncertain environ long inner product base gradient estim true gradient tend itiv gener thi approach impli deep cap c unlik tradit rl section decad ago method wa use learn back model truck nguyen widrow rl activ vision system use learn sequenti shift saccad fovea detect target visual scene schmidhub huber thu learn control select attent compar attent learn without nn whitehead allow memori previou event partial observ world section gener variant thi techniqu use rnn instead fnn implement c feldkamp puskoriu schmidhub thi may caus deep cap onli c also also use optim expect reward plan futur action sequenc schmidhub fact winner robocup world championship fast leagu egorova et train nn predict effect steer signal fast robot motor differ wheel dure play nn model use achiev desir subgoal optim action sequenc quickli plan ahead approach also wa use creat robot abl compens faulti motor whose effect longer match predict nn model gloy wiesel tenchio simon schmidhub typic given advanc essenti question experi c conduct quickli improv formal theori fun creativ schmidhub formal drive forc valu function behind curiou exploratori behavior measur learn progress becom intrins reward c schmidhub compar oudey baran kaplan singh barto chentanez thi motiv c creat action sequenc experi make quick progress deep fnn tradit rl markov decis process mdp classic approach rl bertseka tsitsikli samuel make simplifi assumpt markov sion process mdp current input rl agent convey inform necessari comput optim next output event decis thi allow greatli reduc cap depth rl nn section use dynam program dp trick man latter often explain probabilist work sutton barto basic idea alreadi convey determinist set simplic use tion section let input event xt encod entir current state environ includ reward rt need introduc addit notat sinc real valu encod arbitrari vector real valu origin rl goal find weight maxim sum reward episod place equival set altern goal set valu function v defin input event consid ani two quent input event xt xk recurs defin v xt rt v xk v xk rk xk last input event search weight maxim v input event caus propriat output event action due markov assumpt fnn suffic implement polici map input output event relev cap deeper thi fnn v often model separ fnn also yield typic short cap learn approxim v xt onli local inform rt v xk mani variant tradit rl exist abounadi bertseka borkar baird baird moor barto sutton anderson bertseka bradtk barto kaelbl brafman tennenholtz kaelbl littman cassandra lagoudaki parr maei sutton mahadevan meuleau peshkin kim kaelbl moor atkeson morimoto doya peng william prokhorov wunsch rummeri niranjan santamaría sutton ram schwartz singh sutton barto sutton szepesvári maei tsitsikli van roy van hasselt watkin watkin dayan wier schmidhub formul probabilist framework evalu pair input output action event instead input event onli facilit certain mathemat deriv discount delay reward distort origin rl problem problemat perhap rl nn rl backgammon player tesauro achiev level human world champion play nonlinear rather shallow fnn map larg finit number discret board state valu recent rather deep wa use tradit rl framework play sever atari comput game directli pixel hz video input mnih et use experi replay lin extend previou work neural fit nfq riedmil even better result achiev use slow mont carlo tree plan train compar fast deep nn guo singh lee lewi wang compar rl sallan hinton input elfw otsuka uchib doya earlier rl atari player grüttner sehnk schaul schmidhub earlier raw rl nn comput game koutník cuccu schmidhub gomez train indirect polici search section deep rl rnn partial observ mdp pomdp markov assumpt section often unrealist directli perceiv behind back let alon current state entir univers howev memori previou event help deal partial observ markov decis problem pomdp boutili pool jaakkola singh jordan kaelbl et kimura miyazaki kobayashi lin littman cassandra kaelbl mccallum otsuka yoshimoto doya ring schmidhub teller wier schmidhub william naiv way implement memori without leav mdp framework section would simpli consid possibl huge state space name set possibl observ histori prefix realist way use function approxim rnn produc compact state featur function entir histori seen far gener speak pomdp rl often use dl rnn learn event memor ignor three basic altern use rnn valu function map arbitrari event histori valu bakker lin schmidhub exampl deep lstm rnn use thi way rl robot bakker zhumatiy gruener schmidhub schmidhub neural network use rnn control conjunct second rnn predict world model obtain combin rnn deep section use rnn rl direct search section indirect search section weight space gener howev pomdp may impli greatli increas cap depth rl facilit deep ul fnn rnn rl machin may profit ul input preprocess jodogn piater particular ul nn learn compactli encod environment input imag video section compact code instead raw data fed rl machin whose job thu may becom much easier cuccu luciw schmidhub gomez legenstein wilbert wiskott like sl may profit ul section exampl nfq riedmil wa appli control task lang riedmil riedmil lang voigtlaend pure visual input compactli encod deep coder section rl combin ul base slow featur analysi kompella luciw schmidhub wiskott sejnowski enabl real humanoid robot learn skill raw video stream luciw kompella rounian schmidhub deal pomdp section involv input rl wa use suka raam pollack section wa ploy deep unsupervis sequenc encod rl gisslen et certain type rl ul also combin biolog plausibl rnn spike neuron section klampfl maass rezend gerstner yin et deep hierarch rl hrl subgoal learn fnn rnn multipl learnabl level abstract bengio et deng yu fu lenat brown ring seem import rl sl work archic rl hrl ha publish sinc earli particular subgoal discoveri fnn rnn decompos rl task subtask rl submodul huber schmidhub wahnsiedl numer altern hrl techniqu propos bakker schmidhub barto mahadevan dietterich doya samejima katagiri kawato ghavamzadeh mahadevan jameson menach mannor shimkin moor atkeson precup sutton singh ring samejima doya kawato simsek barto tenenberg karlsson head weiss whiteson kohl miikkulainen stone hrl framework feudal rl dayan hinton option barto singh chentanez singh et sutton precup singh rectli address problem automat subgoal discoveri wier schmidhub automat decompos pomdp section sequenc simpler task solv memoryless polici learnabl tive recent hrl organ potenti deep rl motor control map ring schaul schmidhub inspir iolog find graziano deep rl direct nn quit univers method section yet practic gener tradit rl algorithm section method direct polici search ds without need valu function markovian assumpt section weight fnn rnn directli evalu given rl problem result success trial inform search better weight unlik rl support bp section cap depth section crucial issu ds may solv credit assign problem without backtrack deep causal chain modifi neither care exist tri exploit import class ds method nn polici gradient method aberdeen baxter bartlett ghavamzadeh mahadevan grondman busoniu lope babuska grüttner et heess silver teh kohl stone peter peter schaal rückstieß felder schmidhub sehnk et sutton mcallest singh mansour wierstra foerster peter schmidhub wierstra schaul peter schmidhub william gradient total reward respect polici nn weight estim exploit repeat nn evalu rl nn also evolv evolutionari algorithm ea fogel owen walsh goldberg holland rechenberg schwefel seri trial sever polici repres popul nn improv mutat repeat recombin tion fittest individu fogel fogel porto happel murr maniezzo montana davi nolfi parisi elman compar genet program gp cramer see also smith use evolv comput program variabl size dickmann schmidhub hofer koza cartesian gp miller hard miller thomson evolv program ing nn khan khan miller topolog turner miller relat method includ probabl base ea baluja larraanaga lozano sałustowicz schmidhub saravanan fogel covari matrix estim evolut strategi hansen müller koumoutsako hansen ostermei meisner igel igel neuroevolut ment topolog neat stanley miikkulainen hybrid method combin tradit rl section ea whiteson stone sinc rnn gener comput rnn evolut like gp sens evolv gener program unlik sequenti program learn tradit gp howev rnn mix quential parallel inform process natur cient way alreadi mention section mani rnn evolv propos cliff husband harvey juang miglino lund nolfi miller todd hedg moriarti nolfi floreano miglino mondada mann steinmetz dieckman sim whiteson wieland yamauchi beer yao one ularli effect famili method coevolv neuron combin network select neuron reproduct particip network gomez gomez miikkulainen moriarti miikkulainen thi help solv deep pomdp gomez schmidhub cosyn doe someth similar level synaps weight gomez schmidhub lainen benefit thi shown difficult nonlinear pomdp benchmark schmidhub neural network natur evolut strategi ne glasmach schaul sun wierstra schmidhub sun gomez schaul huber sun wierstra schaul schmidhub stra et link polici gradient method evolutionari approach concept natur gradient amari rnn evolut may also help improv sl deep rnn evolino schmidhub et section deep rl indirect polici nn search ds method section evolv nn hundr thousand weight million search larg deep nn sl rl method mention far somehow search space weight wi profit reduct search space share wi get reus cnn section rnn sl section rl section may possibl howev exploit addit space solut indirect search weight space instead evolv larg nn directli tion one sometim greatli reduc search space evolv compact encod nn lindenmey system jacob lindenmay rozenberg lindenmay graph rewrit kitano cellular encod gruau whitley pyeatt hyperneat clune stanley pennock ofria ambrosio stanley stanley ambrosio gauci van den berg whiteson extend neat section extens thereof risi stanley thi help avoid overfit compar section close relat topic regular mdl section gener approach schmidhub sl rl seek compactli encod weight larg nn schmidhub program written univers program languag church gödel post ture often much effici systemat search space program bia toward short fast program levin schmidhub instead directli search huge space possibl nn weight matric previou univers languag encod nn wa schmidhub recent work use practic languag base coeffici popular transform fourier wavelet particular rnn weight matric may compress like imag encod coeffici discret cosin transform dct koutník et koutník gomez schmidhub compact descript evolv ne cosyn section rnn million weight learn without teacher drive simul car torc drive game loiacono cardamon lanzi loiacono et base like visual input stream koutník et rnn learn control visual process scratch without aid ul cours ul might help gener compact imag code section fed smaller rnn reduc overal comput effort univers rl gener purpos learn algorithm may improv themselv fashion way lifelong learn context schmidhub schmidhub zhao schraudolph schmidhub zhao wier gener type rl constrain onli fundament limit comput identifi founder theoret comput scienc church gödel post ture remark exist blueprint univers problem solver univers rl machin unlimit problem depth variou theoret sens hutter schmidhub particular gödel machin implement gener comput rnn may improv ani part softwar includ learn algorithm way provabl certain sens schmidhub initi asymptot optim hutter also applic rnn solv ani problem quickli unknown fastest way solv save addit constant overhead becom neglig problem size grow note problem larg onli small ai dl research still busi becaus mani interest problem small worth tri reduc overhead less gener method includ heurist discuss univers rl method go beyond usual call dl conclus outlook deep learn dl neural network nn relev supervis learn sl section unsupervis learn ul section reinforc learn rl section allevi problem deep credit assign path cap section ul section onli facilit sl sequenc section stationari pattern section also rl section dynam program dp section import deep sl section tradit rl deep nn section search comput section nn describ bit inform section reduc overfit improv deep sl ul section well rl section also case partial observ environ section deep sl ul rl often creat hierarchi abstract represent stationari data section sequenti data section rl polici section ul facilit sl pure sl feedforward nn fnn section recurr nn rnn section onli win earli contest section also recent one section especi dl fnn profit gpu implement section particular base section section convolut nn section competit onli pattern recognit section also imag segment section object detect section unlik system human learn activ perceiv pattern sequenti direct attent relev part avail data near futur deep nn extend previou work sinc nn learn select attent rl motor action saccad control section b intern action control spotlight attent within rnn thu close gener sensorimotor loop extern intern feedback section mani futur deep nn also take account cost energi activ neuron send signal brain seem minim comput cost dure lem solv least two way given time onli small fraction neuron activ becaus local competit mechan shut mani ing neuron onli winner activ neuron outgo connect compar slim nn section merou neuron spars connect compact ume mani connect much like microchip tradit supercomput often neighbor neuron alloc solv singl task thu reduc munic cost physic seem dictat ani effici putat hardwar futur also schmidhub neural network keep two constraint success rent deep rnn howev unlik certain spike nn tion usual activ unit least slightli tend strongli connect ignor natur constraint ware possibl improv adopt minim energi cation cost direct search program weight space section rnn alloc bore rnn part relat behavior distant rnn part less relat one thu way gener tradit map fnn section also implement occam razor section energi minim find simpl highli eral problem solut requir activ neuron mostli short connect distant futur may belong gener purpos learn algorithm improv themselv provabl optim way section yet practic commerci relev acknowledg sinc april draft thi paper undergon sive open onlin peer review public mail list includ connectionist neuro list imageworld machin learn forum thank numer expert valuabl comment thank snf dfg european commiss partial fund dl research group past content thi paper may use educ commerci purpos includ articl wikipedia similar site refer aberdeen algorithm partial observ markov decis process thesi australian nation univers abounadi bertseka borkar learn algorithm markov decis process averag cost siam journal control optim akaik statist predictor identif annal institut statist mathemat akaik inform theori extens maximum likelihood principl second intl symposium inform theori pp akademinai kiado akaik new look statist model identif ieee transact automat control allend applic kolmogorov complex plexiti theori watanab ed eatc monograph theoret puter scienc kolmogorov complex comput complex pp springer almeida b learn rule asynchron perceptron feedback combinatori environ ieee intern confer neural network vol pp almeida almeida langloi amar redol step size adapt technic report inesc rua alv redol amari theori adapt pattern classifi ieee transact electron comput amari natur gradient work effici learn neural comput amari cichocki yang new learn algorithm blind signal separ touretzki mozer hasselmo ed advanc neural inform process system nip vol mit press amari murata statist theori learn curv entrop loss criterion neural comput amit brunel dynam recurr network spike neuron befor follow learn network comput neural system effect ad nois dure backpropag train gener perform neural comput andrad chacon merelo moran evalu secondari structur protein uv circular dichroism spectra use unsupervis learn neural network protein engin andrew diederich tickl b survey critiqu techniqu extract rule train artifici neural network system anguita gome mix format ral network learn neuroprocessor microprocess ming anguita parodi zunino effici implement bp workstat neurocomput arel rose karnowski deep machin new frontier artifici intellig research ieee comput intellig magazin ash dynam node creation backpropag neural network connect scienc atick li redlich understand retin color code first principl neural comput atiya parlo new result recurr network train unifi algorithm acceler converg ieee transact neural network ba frey b adapt dropout train deep neural network advanc neural inform process system nip pp baird document imag defect model proced iapr workshop syntact structur pattern recognit baird residu algorithm reinforc learn function approxim intern confer machin learn pp baird moor gradient descent gener reinforc learn advanc neural inform process system vol nip pp mit press bakker b reinforc learn long memori dietterich becker ghahramani ed advanc neural inform process system vol pp cambridg mit press bakker schmidhub j hierarch reinforc learn base subgoal discoveri subpolici special groen et al ed proc confer intellig autonom system pp amsterdam nl io press bakker zhumatiy gruener schmidhub j robot identifi memor import previou observ proceed intern confer intellig robot system pp baldi gradient descent learn algorithm overview gener dynam system perspect ieee transact neural network baldi autoencod unsupervis learn deep architectur journal machin learn research proc icml workshop unsupervis transfer learn baldi brunak frasconi pollastri soda exploit past futur protein secondari structur predict bioinformat baldi chauvin neural network fingerprint recognit neural comput baldi chauvin hybrid model architectur protein applic neural comput baldi hornik neural network princip compon analysi learn exampl without local minima neural network baldi hornik learn linear network survey ieee transact neural network baldi pollastri principl design recurs neural network protein structur predict problem journal machin learn research baldi sadowski dropout learn algorithm artifici intellig ballard modular learn neural network proc aaai pp baluja increment learn method integr genet search base function optim competit learn technic report carnegi mellon univers balzer year perspect automat program ieee transact softwar engin barlow b unsupervis learn neural comput barlow kaushal mitchison j find minimum entropi code neural comput barrow learn recept field proceed ieee annual confer neural network vol iv pp ieee barto mahadevan recent advanc hierarch reinforc learn discret event dynam system barto singh chentanez intrins motiv learn hierarch collect skill proceed intern confer development learn pp cambridg mit press barto sutton anderson neuronlik adapt element solv difficult learn control problem ieee transact system man cybernet battiti acceler backpropag learn two optim method complex system battiti method learn steepest descent newton method neural comput baum haussler size net give valid gener neural comput baum petri statist infer probabilist function finit state markov chain annal mathemat statist baxter bartlett estim journal artifici intellig research schmidhub neural network bayer osendorf variat infer latent state sequenc use recurr network arxiv preprint bayer osendorf chen urban van der smagt fast dropout applic recurr network arxiv preprint bayer wierstra togeliu schmidhub j evolv memori cell structur sequenc learn proc icann pp bay essay toward solv problem doctrin chanc philosoph transact royal societi london commun price letter canton becker unsupervis learn procedur neural network intern journal neural system becker le cun improv converg learn second order method touretzki hinton sejnowski ed proc connectionist model summer school pp san mateo morgan kaufmann behnk hebbian learn competit neural abstract pyramid proceed intern joint confer neural network vol pp behnk learn iter imag reconstruct neural abstract pyramid intern journal comput intellig applic behnk learn face local use hierarch recurr network proceed intern confer artifici neural network pp behnk discov hierarch speech featur use convolut matrix factor proceed intern joint confer neural network vol pp behnk lnc lectur note comput scienc vol hierarch neural network imag interpret springer behnk face local track neural abstract pyramid neural comput applic behnk roja neural abstract pyramid hierarch imag understand architectur proceed intern joint confer neural network vol pp bell sejnowski j approach blind separ blind deconvolut neural comput bellman dynam program ed princeton nj usa princeton univers press belouchrani cardoso moulin blind sourc separ techniqu use statist ieee transact signal process bengio artifici neural network applic sequenc recognit thesi montreal qc canada mcgill univers comput scienc bengio foundat trend machin learn vol learn deep architectur ai publish bengio courvil vincent represent learn review new perspect ieee transact pattern analysi machin intellig bengio lamblin popovici larochel greedi train deep network cowan tesauro alspector ed advanc neural inform process system vol nip pp mit press bengio simard frasconi learn depend gradient descent difficult ieee transact neural network bering grave schiel schmidhub j classifi unprompt speech retrain lstm net duch kacprzyk oja zadrozni ed lnc vol artifici neural network biolog pp berlin heidelberg bertseka dynam program optim control athena scientif bertseka tsitsikli program belmont athena scientif bichot rossi desimon parallel serial neural mechan visual search macaqu area scienc bärmann learn algorithm multilay neural network base linear least squar problem neural network bishop smooth learn algorithm forward network ieee transact neural network bishop pattern recognit machin learn springer blair pollack b analysi dynam recogn neural comput blondel tsitsikli survey comput complex result system control automatica bluch louradour knibb moysset benzeghiba kermorv arab handwritten text recognit system evalu intern workshop document analysi system blum rivest train neural network neural network blumer ehrenfeucht haussler warmuth occam razor inform process letter bobrowski learn process multilay threshold net biolog cybernet bodén wile j dynam recurr neural network connect scienc bodenhausen waibel tempo algorithm adjust delay supervis learn lippman moodi touretzki ed advanc neural inform process system vol pp morgan kaufmann boht kok la poutr tempor encod network spike neuron neurocomput boltzmann hasenöhrl ed wissenschaftlich abhandlungen leipzig barth collect boltzmann articl scientif journal bottou une approch théoriqu de l apprentissag connexionist tion à la reconnaiss de la parol thesi université de pari xi bourlard morgan connnectionist speech recognit hybrid approach kluwer academ publish boutili pool comput optim polici partial observ markov decis process use compact represent proceed aaai bradtk barto kaelbl linear algorithm tempor differ learn machin learn brafman tennenholtz gener polynomi time algorithm reinforc learn journal machin learn research brea senn pfister match recal storag sequenc learn spike neural network journal neurosci breiman bag predictor machin learn brett rudolph carneval hine beeman bower et al simul network spike neuron review tool strategi journal comput neurosci breuel shafait ocr print english fraktur use lstm network intern confer document analysi recognit pp ieee bromley bentz bottou guyon lecun moor et al signatur verif use siames time delay neural network intern journal pattern recognit artifici intellig broyden et al class method solv nonlinear simultan equat mathemat comput brueckner schulter b social signal classif use deep blstm recurr neural network proceed ieee intern confer acoust speech signal process pp brunel dynam spars connect network excitatori inhibitori spike neuron journal comput neurosci bryson gradient method optim alloc process proc harvard univ symposium digit comput applic bryson denham method solv optimum program problem technic report raytheon compani missl space divis bryson ho appli optim control optim estim control blaisdel pub buhler j effici sequenc comparison hash bioinformat buntin weigend bayesian complex system burgess construct algorithm converg input pattern intern journal neural system cardoso perform orthogon sourc separ algorithm proc eusipco pp continu latent variabl model dimension reduct sequenti data reconstruct thesi uk univers sheffield carter rudolph nucci j oper fault toler cmac network touretzki ed advanc neural inform process system nip vol pp san mateo ca morgan kaufmann caruana multitask learn machin learn casey dynam comput applic recurr neural network finit state machin extract neural comput cauwenbergh fast stochast algorithm supervis learn optim lippman moodi touretzki ed advanc neural inform process system vol morgan kaufmann chaitin j length program comput finit binari sequenc journal acm chalup blair increment train first order recurr neural network predict languag neural network chellapilla puri simard high perform convolut neural network document process intern workshop frontier handwrit recognit chen salman learn characterist deep neural architectur ieee transact neural network cho foundat advanc deep learn thesi aalto univers school scienc cho ilin raiko regular restrict boltzmann machin intl conf artifici neural network pp springer cho raiko ilin enhanc gradient train restrict boltzmann machin neural comput schmidhub neural network church unsolv problem elementari number theori american journal mathemat ciresan giusti gambardella schmidhub j deep neural network segment neuron membran electron microscopi imag advanc neural inform process system nip pp ciresan giusti gambardella schmidhub j mitosi detect breast cancer histolog imag deep neural network proc miccai vol pp ciresan meier gambardella schmidhub j deep big simpl neural net handwritten digit recognt neural comput ciresan meier masci gambardella schmidhub j flexibl high perform convolut neural network imag classif intl joint confer artifici intellig pp ciresan meier masci schmidhub j committe neural network traffic sign classif intern joint confer neural network pp ciresan meier masci schmidhub j deep neural network traffic sign classif neural network ciresan meier schmidhub j deep neural network imag classif ieee confer comput vision pattern recognit long preprint ciresan meier schmidhub j transfer learn latin chines charact deep neural network intern joint confer neural network pp ciresan schmidhub j deep neural network offlin handwritten chines charact classif technic report idsia cliff husband harvey evolv recurr dynam network robot control artifici neural net genet algorithm pp springer clune mouret lipson evolutionari origin modular proceed royal societi b biolog scienc clune stanley pennock ofria perform indirect encod across continuum regular ieee transact evolutionari comput coat huval wang wu ng catanzaro b deep learn cot hpc system proc intern confer machin learn cochocki unbehauen neural network optim signal process john wiley son collobert weston j unifi architectur natur languag process deep neural network multitask learn proceed intern confer machin learn pp acm comon independ compon new concept signal process connor brincat pasupathi transform shape inform ventral pathway current opinion neurobiolog connor martin atla recurr neural network robust time seri predict ieee transact neural network cook complex procedur proceed annual acm symposium theori comput pp new york acm cramer represent adapt gener simpl quential program grefenstett ed proceed intern ferenc genet algorithm applic univers hillsdal nj lawrenc erlbaum associ craven wahba smooth noisi data spline function estim correct degre smooth method gener valid numerisch mathematik cuccu luciw schmidhub gomez intrins motiv evolutionari search reinforc learn proceed ieee confer develop learn epigenet robot vol pp ieee dahl sainath hinton improv deep neural network lvcsr use rectifi linear unit dropout ieee intern confer acoust speech signal process pp ieee dahl yu deng acero deep neural network speech recognit ieee transact audio speech languag process ambrosio stanley novel gener encod exploit neural network sensor output geometri proceed confer genet evolutionari comput pp datar immorlica indyk mirrokni hash scheme base distribut proceed annual symposium comput geometri pp acm dayan hinton feudal reinforc learn lippman moodi touretzki ed advanc neural inform process system nip vol pp morgan kaufmann dayan hinton varieti helmholtz machin neural network dayan hinton neal zemel helmholtz machin neural comput dayan zemel competit multipl caus model neural comput deco parra featur extract redund reduct unsupervis stochast neural network neural network deco roll neurodynam bias competit ation attent model spike neuron journal neurophysiolog de freita bayesian method neural network thesi univers cambridg dejong mooney learn altern view machin learn demer cottrel dimension reduct hanson cowan gile ed advanc neural inform process system nip vol pp morgan kaufmann dempster laird rubin b maximum likelihood incomplet data via em algorithm journal royal statist societi b deng yu deep learn method applic publish desimon albright gross bruce properti inferior tempor neuron macaqu journal neurosci de souto souto oliveira load problem pyramid neural network electron journal mathemat comput de valoi albrecht thorel spatial frequenc select cell macaqu visual cortex vision research devil lau logic program synthesi journal logic program de vri princip theori neural network time delay lippmann moodi touretzki ed advanc neural inform process system nip vol pp morgan kaufmann dicarlo zoccolan rust doe brain solv visual object recognit neuron dickmann behring dickmann hildebrandt maurer thomanek et al see passeng car proc int symp intellig vehicl pp dickmann schmidhub winklhof der genetisch algorithmu ein implementierung prolog technic report inst informat tech univ munich http dietterich ensembl method machin learn multipl classifi system pp springer dietterich hierarch reinforc learn maxq valu function decomposit journal artifici intellig research jair di lena nagata baldi deep architectur protein contact map predict bioinformat director rohrer autom network domain case ieee transact circuit theori dittenbach merkl rauber grow hierarch organ map intern joint confer neural network vol ieee comput societi donahu jia vinyal hoffman zhang tzeng et al decaf deep convolut activ featur gener visual recognit arxiv preprint dorffner neural network time seri process neural network world doya samejima ichi katagiri kawato multipl reinforc learn neural comput dreyfu numer solut variat problem journal mathemat analysi applic dreyfu comput solut optim control problem time lag ieee transact automat control duchi hazan singer adapt subgradi method onlin learn stochast optim journal machin learn egorova gloy göktekin lier luft roja et al fighter small size team descript robocup symposium paper team descript paper cd edit elfw otsuka uchib doya base reinforc learn navig sensori input neural inform process theori algorithm iconip vol pp springer eliasmith build brain neural architectur biolog cognit new york ny oxford univers press eliasmith stewart choo bekolay dewolf tang et al model function brain scienc elman find structur time cognit scienc erhan bengio courvil manzagol vincent bengio whi doe unsupervis help deep learn journal machin learn research wiskott solv classif regress problem data supervis extens slow featur analysi journal machin learn research eubank spline smooth nonparametr regress farlow ed method model new york marcel dekker euler methodu inveniendi eyben wening squartini schuller b voic activ detect lstm recurr neural network applic hollywood movi proc ieee intern confer acoust speech signal process pp faggin neural network hardwar intern joint confer neural network vol schmidhub neural network fahlman empir studi learn speed network technic report univ fahlman recurr learn algorithm lippmann moodi touretzki ed advanc neural inform process system nip vol pp morgan kaufmann falconbridg stamp badcock simpl hebbian network learn spars independ compon natur imag neural comput fan qian xie soong tt synthesi bidirect lstm base recurr neural network proc interspeech farabet coupri najman lecun learn hierarch featur scene label ieee transact pattern analysi machin intellig farlow j method model gmdh type algorithm vol crc press feldkamp prokhorov eagen yuan enhanc stream kalman filter train recurr network nonlinear model pp springer feldkamp prokhorov feldkamp simpl condit adapt behavior kalman filter train recurr network neural network feldkamp puskoriu signal process framework base dynam neural network applic problem adapt filter classif proceed ieee felleman van essen distribut hierarch process primat cerebr cortex cerebr cortex fernández grave schmidhub j applic recurr neural network discrimin keyword spot proc icann pp fernandez grave schmidhub j sequenc label structur domain hierarch recurr neural network proceed intern joint confer artifici intellig fernandez rendel ramabhadran hoori prosodi contour predict long memori deep recurr neural network proc interspeech field j relat statist natur imag respons properti cortic cell journal optic societi america field j goal sensori code neural comput fier schemmel meier realiz biolog spike network model configur hardwar system ieee intern joint confer neural network pp fine singer tishbi hierarch hidden markov model analysi applic machin learn fischer igel train restrict boltzmann machin introduct pattern recognit fitzhugh impuls physiolog state theoret model nerv membran biophys journal fletcher powel j rapidli converg descent method minim comput journal floreano mattiussi evolut spike neural control autonom robot evolutionari robot intellig robot artifici life pp springer fogel fogel porto evolv neural network biolog cybernet fogel owen walsh artifici intellig simul evolut new york wiley földiák form spars represent local learn biolog cybernet földiák young spars code primat cortex arbib ed handbook brain theori neural network pp mit press förster grave schmidhub j learn compact map effici robot local european symposium artifici neural network pp franziu sprekel wiskott slow spars lead place cell plo comput biolog friedman hasti tibshirani springer seri statist vol element statist learn new york frinken fischer bunk term memori neural network languag model handwrit recognit intern confer pattern recognit pp ieee fritzk b grow neural ga network learn topolog tesauro touretzki leen ed nip pp mit press fu syntact pattern recognit applic berlin springer fukada schuster sagisaka phonem boundari estim use bidirect recurr neural network applic system comput japan fukushima neural network model mechan pattern recognit unaffect shift transact iec fukushima neocognitron neural network mechan pattern recognit unaffect shift posit biolog cybernet fukushima increas robust background nois visual pattern recognit neocognitron neural network fukushima artifici vision neural network neocognitron advanc neural network fukushima train neural network neocognitron neural network gabor theori commun part analysi inform electr iii journal institut radio commun engin gallant connectionist expert system commun acm gauss theoria motu corporum coelestium sectionibu conici solem ambientium gauss theoria combinationi observationum erroribu minimi obnoxia theori combin observ least subject error ge hang lee zhang stabl adapt neural network control springer geiger zhang wening schuller rigol robust speech recognit use long memori recurr neural network hybrid acoust model proc interspeech geman bienenstock doursat neural network dilemma neural comput ger schmidhub j recurr net time count proceed intern joint confer neural network vol pp ieee ger schmidhub j lstm recurr network learn simpl context free context sensit languag ieee transact neural network ger schmidhub cummin learn forget continu predict lstm neural comput ger schraudolph schmidhub j learn precis time lstm recurr network journal machin learn research gerstner kistler spike neuron model cambridg univers press gerstner van hemmen associ memori network spike neuron network comput neural system ghavamzadeh mahadevan hierarch polici gradient algorithm proceed twentieth confer machin learn pp gherriti learn algorithm analog fulli recurr neural network intern joint confer neural network san diego vol pp girshick donahu darrel malik j rich featur hierarchi accur object detect semant segment technic report uc berkeley icsi gisslen luciw graziano schmidhub j sequenti constant size compressor reinforc learn proc fourth confer artifici gener intellig pp springer giusti ciresan masci gambardella schmidhub j fast imag scan deep convolut neural network proc icip glackin mcginniti maguir wu belatrech novel approach implement larg scale spike neural network fpga hardwar comput intellig bioinspir system pp springer glasmach schaul sun wierstra schmidhub j exponenti natur evolut strategi proceed genet evolutionari comput confer pp acm glorot bord bengio deep spars rectifi network aistat vol pp gloy wiesel tenchio simon reinforc drive qualiti soccer play robot anticip technolog gödel über formal unentscheidbar sätze der principia mathematica und verwandt system monatsheft für mathematik und physik goldberg genet algorithm search optim machin learn read goldfarb famili method deriv variat mean mathemat comput golub heath wahba gener method choos good ridg paramet technometr gomez j robust nonlinear control neuroevolut thesi depart comput scienc univers texa austin gomez miikkulainen activ guidanc finless rocket use neuroevolut proc gecco gomez schmidhub j recurr neuron learn deep memori pomdp proc confer genet evolutionari comput new york ny usa acm press gomez schmidhub miikkulainen acceler neural evolut cooper coevolv synaps journal machin learn research may gomi kawato neural network control system use neural network sak moreno j automat languag identif use long memori recurr neural network proc interspeech goodfellow bulatov ibarz arnoud shet number recognit street view imageri use deep convolut neural network arxiv preprint schmidhub neural network goodfellow courvil bengio spars code unsupervis featur discoveri nip workshop challeng learn hierarch model goodfellow courvil bengio featur learn spars code proceed intern confer machin learn goodfellow mirza da courvil bengio empir investig catastroph forget neural network tr goodfellow mirza courvil bengio maxout network intern confer machin learn grave practic variat infer neural network advanc neural inform process system nip pp grave eck bering schmidhub j isol digit recognit lstm recurr network first intern workshop biolog inspir approach advanc inform technolog grave fernandez gomez schmidhub j connectionist tempor classif label unseg sequenc data recurr neural net icml proceed intern confer machin learn pp grave fernandez liwicki bunk schmidhub j strain handwrit recognit recurr neural network platt koller singer rowei ed advanc neural inform process system nip vol pp cambridg mit press grave jaitli toward speech recognit recurr neural network proc intern confer machin learn pp grave liwicki fernandez bertolami bunk schmidhub j novel connectionist system improv unconstrain handwrit recognit ieee transact pattern analysi machin intellig grave moham hinton speech recognit deep recurr neural network ieee intern confer acoust speech signal process pp ieee grave schmidhub j framewis phonem classif bidirect lstm neural network architectur neural network grave schmidhub j offlin handwrit recognit multidimension recurr neural network advanc neural inform process system nip vol pp cambridg mit press graziano intellig movement machin etholog perspect primat motor system usa oxford univers press griewank documenta volum ismp pp grondman busoniu lope babuska survey reinforc learn standard natur polici gradient ieee transact system man cybernet part c applic review grossberg network learn rememb reproduc ani number complic pattern journal mathemat mechan grossberg adapt pattern classif univers recod parallel develop code neural featur detector biolog cybernet grossberg adapt pattern classif univers recod feedback expect olfact illus biolog cybernet gruau whitley pyeatt comparison cellular encod direct encod genet neural network neurocolt technic report esprit work group neural comput learn neurocolt grünwald myung pitt advanc minimum descript length theori applic mit press grüttner sehnk schaul schmidhub j deep memori player paramet explor polici gradient proceed intern confer artifici neural network icann pp springer guo singh lee lewi wang x deep learn atari game play use offlin tree search plan advanc neural inform process system vol nip guyon vapnik boser bottou solla structur risk minim charact recognit lippman moodi touretzki ed advanc neural inform process system nip vol pp morgan kaufmann hadamard j mémoir sur le problèm analys relatif à l équilibr de plaqu élastiqu encastré mémoir présenté par diver savant à l académi de scienc de l institut de franc éxtrait imprimeri national hadsel chopra lecun dimension reduct learn invari map proc comput vision pattern recognit confer ieee press hagra colley callaghan clark evolv spike neural network control autonom robot ieee intern confer robot autom vol pp hansen müller koumoutsako reduc time complex derandom evolut strategi covari matrix adapt evolutionari comput hansen ostermei complet derandom evolut strategi evolutionari comput hanson j stochast version delta rule physica nonlinear phenomena hanson pratt compar bias minim network construct touretzki ed advanc neural inform process system nip vol pp san mateo ca morgan kaufmann happel murr design evolut modular neural network architectur neural network hashem schmeiser b improv model accuraci use optim linear combin train neural network ieee transact neural network hassibi stork second order deriv network prune optim brain surgeon lippman moodi touretzki ed advanc neural inform process system vol pp morgan kaufmann hasti tibshirani j monograph statis appli probabl vol gener addit model hasti tibshirani friedman j springer seri statist element statist learn hawkin georg hierarch tempor theori terminolog numenta haykin kalman filter neural network wiley onlin librari hebb organ behavior new york wiley theori backpropag neural network intern joint confer neural network pp ieee heemskerk overview neural hardwar neurocomput style process design implement applic heess silver teh reinforc learn polici proc european workshop reinforc learn pp igel neuroevolut strategi episod reinforc learn journal algorithm herrero valencia dopazo j hierarch unsupervis grow neural network cluster gene express pattern bioinformat hertz krogh palmer introduct theori neural comput redwood citi hesten stiefel method conjug gradient solv linear system journal research nation bureau standard hihi bengio hierarch recurr neural network depend touretzki mozer hasselmo ed advanc neural inform process system vol pp mit press hinton connectionist learn procedur artifici intellig hinton train product expert minim contrast diverg neural comput hinton dayan frey neal algorithm unsupervis neural network scienc hinton deng yu dahl moham jaitli et al deep neural network acoust model speech recognit share view four research group ieee signal process magazin hinton ghahramani z gener model discov spars distribut represent philosoph transact royal societi b hinton osindero teh fast learn algorithm deep belief net neural comput hinton salakhutdinov reduc dimension data neural network scienc hinton sejnowski learn relearn boltzmann machin parallel distribut process vol pp mit press hinton srivastava krizhevski sutskev salakhutdinov improv neural network prevent featur detector technic report hinton van camp keep neural network simpl proceed intern confer artifici neural network amsterdam pp springer hochreit untersuchungen zu dynamischen neuronalen netzen diploma thesi institut für informatik lehrstuhl brauer technisch universität münchen advisor schmidhub hochreit bengio frasconi schmidhub j gradient flow recurr net difficulti learn depend kremer kolen ed field guid dynam recurr neural network ieee press hochreit obermay sequenc classif protein analysi snowbird workshop snowbird utah comput biolog learn societi hochreit schmidhub j bridg long time lag weight guess long memori silva princip almeida ed frontier artifici intellig applic vol spatiotempor model biolog artifici system pp amsterdam netherland io press hochreit schmidhub j flat minima neural comput hochreit schmidhub j long memori neural comput base tr tum hochreit schmidhub j featur extract lococod neural comput hochreit younger conwel learn learn use gradient descent lectur note comp sci vol proc intl conf artifici neural network pp berlin heidelberg springer schmidhub neural network hodgkin huxley quantit descript membran current applic conduct excit nerv journal physiolog hoerzer legenstein maass emerg complex comput structur chaotic neural network modul hebbian learn cerebr cortex holden b theori gener linearli weight connectionist network thesi cambridg univers engin depart holland adapt natur artifici system ann arbor univers michigan press honavar uhr network unit learn perceiv gener well reweight link touretzki hinton sejnowski ed proc connectionist model summer school pp san mateo morgan kaufman honavar uhr gener learn structur process gener connectionist network inform scienc hopfield j neural network physic system emerg collect comput abil proceed nation academi scienc hornik stinchcomb white multilay feedforward network univers approxim neural network hubel wiesel recept field binocular interact function architectur cat visual cortex journal physiolog london hubel wiesel recept field function architectur monkey striat cortex journal physiolog huffman method construct code proceed ire hung kreiman poggio dicarlo j fast readout object ident macaqu inferior tempor cortex scienc hutter fastest shortest algorithm problem intern journal foundat comput scienc schmidhub snf grant hutter univers artifici intellig sequenti decis base algorithm probabl berlin springer schmidhub snf grant hyvärinen hoyer oja spars code shrinkag denois maximum likelihood estim kearn solla cohn ed advanc neural inform process system nip vol mit press hyvärinen karhunen oja independ compon analysi john wiley son icpr contest mitosi detect breast cancer histolog imag ipal laboratori tribvn compani hospit cialab ohio state univ http igel neuroevolut reinforc learn use evolut strategi reynold abbass tan mckay essam gedeon ed congress evolutionari comput vol pp ieee igel hüsken empir evalu improv rprop learn algorithm neurocomput c ikeda ochiai sawaragi sequenti gmdh algorithm applic river flow predict ieee transact system man cybernet indermuhl frinken bunk mode detect onlin handwritten document use blstm neural network frontier handwrit recognit icfhr intern confer pp ieee indermuhl frinken fischer bunk keyword spot onlin handwritten document contain text use blstm neural network document analysi recognit icdar intern confer pp ieee indiveri hamilton van schaik delbruck et al neuromorph silicon neuron circuit frontier neurosci ivakhnenko group method data rival method stochast approxim soviet automat control ivakhnenko polynomi theori complex system ieee transact system man cybernet ivakhnenko review problem solvabl algorithm group method data handl gmdh pattern recognit imag obrazov analiz izobrazhenii ivakhnenko lapa cybernet predict devic ccm inform corpor ivakhnenko lapa mcdonough cybernet forecast techniqu ny american elsevi izhikevich et al simpl model spike neuron ieee transact neural network jaakkola singh jordan reinforc learn algorithm partial observ markov decis problem tesauro touretzki leen ed advanc neural inform process system vol pp mit press jackel boser graf denker lecun henderson et al vlsi implement electron neural network exampl charact recognit ieee ed ieee intern confer system man cybernet pp jacob lindenmay rozenberg genet program lectur note comput scienc parallel problem solv natur iii jacob increas rate converg learn rate adapt neural network jaeger echo state approach analys train recurr neural network technic report gmd report german nation research center inform technolog jaeger har nonlinear predict chaotic system save energi wireless commun scienc jain seung natur imag denois convolut network koller schuurman bengio bottou ed advanc neural inform process system nip vol pp curran associ jameson j delay reinforc learn multipl time scale hierarch backpropag adapt critic neural network control ji xu yang yu convolut neural network human action recognit ieee transact pattern analysi machin intellig jim gile horn effect nois converg gener recurr network tesauro touretzki leen ed advanc neural inform process system nip vol san mateo ca morgan kaufmann jin lujan plana davi templ furber b model spike neural network spinnak comput scienc engin jodogn piater learn visual control polici journal artifici intellig research jone palmer evalu gabor filter model simpl recept field cat striat cortex journal neurophysiolog jordan serial order parallel distribut process approach technic report ic report san diego institut cognit scienc univers california jordan supervis learn system excess degre freedom technic report coin tr massachusett institut technolog jordan serial order parallel distribut process approach advanc psycholog jordan rumelhart supervis learn distal teacher technic report occasion paper center cog massachusett institut technolog jordan sejnowski j graphic model foundat neural comput mit press joseph contribut perceptron theori thesi cornel univ juang hybrid genet algorithm particl swarm optim recurr network design ieee transact system man cybernet part b cybernet judd neural network model connection neural network design complex learn mit press jutten herault j blind separ sourc part adapt algorithm base neuromimet architectur signal process kaelbl littman cassandra plan act partial observ stochast domain technic report provid ri brown univers kaelbl littman moor reinforc learn survey journal ai research kak chen wang data mine use surfac deep agent base neural network amci proceed kalink lehmann comput recurr neural network counter iter function system antoni slaney ed lnai vol advanc topic artifici intellig proceed australian joint confer artifici intellig berlin heidelberg springer kalman new approach linear filter predict problem journal basic engin karhunen joutsensalo j gener princip compon analysi optim problem neural network neural network karpathi toderici shetti leung sukthankar video classif convolut neural network ieee confer comput vision pattern recognit kasabov neucub spike neural network architectur map learn understand brain data neural network kelley j gradient theori optim flight path ar journal kempter gerstner van hemmen hebbian learn spike neuron physic review e kerlirzin vallet robust multilay perceptron neural comput khan bennamoun sohel togneri automat featur learn robust shadow detect ieee confer comput vision pattern recognit khan khan miller evolut neural network use cartesian genet program ieee congress evolutionari comput pp khan lester plana rast jin painkra et al spinnak map neural network onto chip multiprocessor intern joint confer neural network pp ieee kimura miyazaki kobayashi reinforc learn pomdp function approxim icml vol pp schmidhub neural network kistler gerstner van hemmen reduct equat threshold model neural comput kitano design neural network use genet algorithm graph gener system complex system klampfl maass emerg dynam memori trace cortic microcircuit model stdp journal neurosci schraudolph schmidhub j unsupervis learn lstm recurr neural network lectur note comp sci vol proc intl conf artifici neural network pp berlin heidelberg springer kobatak tanaka neuron select complex object featur ventral visual pathway macaqu cerebr cortex journal neurophysiolog kohl stone polici gradient reinforc learn fast quadruped locomot robot autom proceed icra ieee intern confer vol pp ieee kohonen correl matrix memori ieee transact comput kohonen format topolog correct featur map biolog cybernet kohonen associ memori ed springer koikkalainen oja hierarch featur map intern joint confer neural network pp ieee kolmogorov represent continu function sever variabl superposit continu function one variabl addit dokladi akademii nauk sssr kolmogorov three approach quantit definit inform problem inform transmiss kompella luciw schmidhub j increment slow featur analysi adapt slow featur updat dimension input stream neural comput kondo gmdh neural network algorithm use heurist organ method applic pattern identif problem proceed sice annual confer pp ieee kondo ueno j neural network select optimum neural network architectur applic dimension medic imag recognit blood vessel intern journal innov comput inform control kordík náplava snorek modifi gmdh method model qualiti evalu visual control system comput korkin de gari ger hemmi cbm machin hardwar tool evolv neural net modul fraction second run million neuron artifici brain real time kosko b unsupervis learn nois ieee transact neural network koutník cuccu schmidhub gomez evolv neural network reinforc learn proceed genet evolutionari comput confer pp amsterdam acm koutník gomez schmidhub j evolv neural network compress weight space proceed annual confer genet evolutionari comput pp koutník greff gomez schmidhub j clockwork rnn proceed intern confer machin learn vol pp koza genet program comput mean natur select mit press kramer nonlinear princip compon analysi use autoassoci neural network aich journal kremer kolen field guid dynam recurr network ieee press kriegeskort mur ruff kiani bodurka esteki et al match categor object represent inferior tempor cortex man monkey neuron krizhevski sutskev hinton imagenet classif deep convolut neural network advanc neural inform process system krogh hertz simpl weight decay improv gener lippman moodi touretzki ed advanc neural inform process system vol pp morgan kaufmann kruger janssen kalkan lapp leonardi piater et al deep hierarchi primat visual cortex learn comput vision ieee transact pattern analysi machin intellig kullback leibler inform suffici annal mathemat statist kurzweil creat mind secret human thought reveal lagoudaki parr polici iter journal machin learn research lampinen oja cluster properti hierarch map journal mathemat imag vision lang waibel hinton neural network architectur isol word recognit neural network lang riedmil deep neural network reinforc learn neural network intern joint confer pp laped farber nonsymmetr neural net content address memori pattern recognit physica laplac mémoir sur la probabilité de caus par le évènement mémoir de l academi royal de scienc presenté par diver savan larraanaga lozano estim distribut algorithm new tool evolutionari comput norwel usa kluwer academ publish le ranzato monga devin corrado chen et al build featur use larg scale unsupervis learn proc icml lecun une procédur apprentissag pour réseau à seuil asymétriqu proceed cognitiva pp lecun theoret framework touretzki hinton sejnowski ed proceed connectionist model summer school pp cmu pittsburgh pa morgan kaufmann lecun boser denker henderson howard hubbard et al appli handwritten zip code recognit neural comput lecun boser denker henderson howard hubbard et al handwritten digit recognit network touretzki ed advanc neural inform process system vol pp morgan kaufmann lecun bottou bengio haffner learn appli document recognit proceed ieee lecun denker solla optim brain damag touretzki ed advanc neural inform process system vol pp morgan kaufmann lecun muller cosatto flepp b obstacl avoid learn advanc neural inform process system nip lecun simard pearlmutt b automat learn rate tion estim hessian eigenvector hanson cowan gile ed advanc neural inform process system vol nip san mateo ca morgan kaufmann publish lee learn languag survey literatur technic report cambridg massachusett center research comput technolog harvard univers lee battl raina ng effici spars code algorithm advanc neural inform process system nip vol pp lee ekanadham ng spars deep belief net model visual area advanc neural inform process system nip vol pp lee gross ranganath ng convolut deep belief network scalabl unsupervis learn hierarch represent proceed intern confer machin learn pp lee kil gaussian potenti function network hierarch learn neural network lee pham largman ng unsupervis featur learn audio classif use convolut deep belief network proc nip vol pp legendr nouvel méthode pour la détermin de orbit de comet didot legenstein maass neural circuit pattern recognit small total wire length theoret comput scienc legenstein wilbert wiskott reinforc learn slow featur input stream plo comput biolog leibniz memoir use chain rule cite tmme leibniz nova methodu pro maximi et minimi itemqu tangentibu quae nec fracta nec irrational quantit moratur et singular pro illi calculi genu acta eruditorum lenat b theori format heurist search machin learn lenat brown whi eurisko appear work artifici intellig lenni movshon code color form geniculostri visual pathway journal optic societi america levenberg method solut certain problem least squar quarterli appli mathemat levin notion random sequenc soviet mathemat dokladi levin univers sequenti search problem problem inform transmiss levin leen moodi fast prune use princip compon advanc neural inform process system nip vol morgan kaufmann levin narendra control nonlinear dynam system use neural network ii observ identif control ieee transact neural network lewicki olshausen infer spars overcomplet imag code use effici code framework jordan kearn solla ed advanc neural inform process system nip vol pp schmidhub neural network l hôpital analys de infini petit pour l intellig de lign courb pari l imprimeri royal li vitányi b introduct kolmogorov complex applic springer li zhang suk wang li shen et al deep learn base imag data complet improv brain diseas diagnosi proc miccai springer lin reinforc learn robot use neural network thesi pittsburgh carnegi mellon univers lin horn tino gile learn depend narx recurr neural network ieee transact neural network lindenmay mathemat model cellular interact ment journal theoret biolog lindstädt comparison two unsupervis neural network model redund reduct mozer smolenski touretzki elman weigend ed proc connectionist model summer school pp hillsdal nj erlbaum associ linnainmaa represent cumul round error algorithm taylor expans local round error master thesi univ helsinki linnainmaa taylor expans accumul round error bit numer mathemat linsker perceptu network ieee comput littman cassandra kaelbl learn polici partial observ environ scale priediti russel ed machin learn proceed twelfth intern confer pp san francisco ca morgan kaufmann publish liu kramer indiveri delbrück burg dougla et al avlsi spike neuron neural network ljung system identif springer logotheti paul poggio shape represent inferior tempor cortex monkey current biolog loiacono cardamon lanzi simul car race championship competit softwar manual technic report itali dipartimento di elettronica e informazion politecnico di milano loiacono lanzi togeliu onieva pelta butz et al simul car race championship low object recognit local featur proceed seventh ieee intern confer comput vision vol pp low distinct imag featur intern journal comput vision luciw kompella kazerounian schmidhub j intrins valu system develop multipl invari represent increment slow learn frontier neurorobot lusci pollastri baldi deep architectur deep learn chemoinformat predict aqueou solubl molecul journal chemic inform model maa hannun ng rectifi nonlinear improv neural network acoust model intern confer machin learn maass lower bound comput power network spike neuron neural comput maass network spike neuron third gener neural network model neural network maass comput power neural comput maass natschläger markram comput without stabl state new framework neural comput base perturb neural comput mackay practic bayesian framework backprop network neural comput mackay miller analysi linsker simul hebbian rule neural comput maclin shavlik use neural network improv algorithm refin algorithm protein fold machin learn maclin shavlik combin predict multipl classifi use competit learn initi neural network proc ijcai pp madala ivakhnenko induct learn algorithm complex system model boca raton crc press madani hank condon undecid probabilist plan relat stochast optim problem artifici intellig maei sutton gq λ gener gradient algorithm differ predict learn elig trace proceed third confer artifici gener intellig vol pp maex orban model circuit spike neuron gener direct select simpl cell journal neurophysiolog mahadevan averag reward reinforc learn foundat algorithm empir result machin learn malik perona preattent textur discrimin earli vision mechan journal optic societi america maniezzo genet evolut topolog weight distribut neural network ieee transact neural network manolio fanelli recurr neural network determinist finit state automata neural comput marchi ferroni eyben gabrielli squartini schuller b linear predict base featur audio onset detect bidirect lstm neural network proc ieee intern confer acoust speech signal process pp markram human brain project scientif american marquardt algorithm estim nonlinear paramet journal societi industri appli mathemat marten j deep learn via optim fürnkranz joachim ed proceed intern confer machin learn pp haifa israel omnipress marten sutskev learn recurr neural network free optim proceed intern confer machin learn pp martinetz ritter schulten j neural net learn visuomotor coordin robot arm ieee transact neural network masci giusti ciresan fricout schmidhub j fast learn algorithm imag segment convolut network intern confer imag process pp matsuoka nois inject input learn ieee transact system man cybernet mayer gomez wierstra nagi knoll schmidhub j system robot heart surgeri learn tie knot use recurr neural network advanc robot mccallum learn use select attent memori sequenti task mae matar meyer pollack wilson ed anim animat proceed fourth intern confer simul adapt behavior pp mit press bradford book mcculloch pitt logic calculu idea imman nervou activ bulletin mathemat biophys melnik levi pollack b raam infinit languag proc ijcnn pp memisev hinton learn repres spatial transform factor boltzmann machin neural comput menach mannor shimkin q discoveri goal reinforc learn proc ecml pp merolla arthur cassidi sawada akopyan et al million integr circuit scalabl commun network interfac scienc mesnil dauphin glorot rifai bengio goodfellow et al unsupervis transfer learn challeng deep learn approach jmlr w cp proc unsupervis transfer learn vol meuleau peshkin kim kaelbl learn finit state control partial observ environ intern confer uncertainti ai pp miglino lund nolfi evolv mobil robot simul real environ artifici life miller model develop simpl cell recept field order arrang orient column competit input journal neurosci miller hard cartesian genet program proceed annual confer companion genet evolutionari comput confer late break paper pp acm miller thomson cartesian genet program genet program pp springer miller todd hedg design neural network use genet algorithm proceed intern confer genet algorithm pp morgan kauffman miller werbo sutton neural network control mit press minai william perturb respons feedforward network neural network minski step toward artifici intellig feigenbaum feldman ed comput thought pp new york minski papert perceptron cambridg mit press minton carbonel knoblock kuokka etzioni gil learn problem solv perspect artifici intellig mitchel machin learn mcgraw hill mitchel keller gener unifi view machin learn mnih kavukcuoglu silver grave antonogl wierstra et al play atari deep reinforc learn technic report deepmind technolog moham hinton phone recognit use restrict boltzmann machin ieee intern confer acoust speech signal process pp molgedey schuster separ independ signal use correl physic review letter møller exact calcul product hessian matrix forward network error function vector n time technic report denmark comput scienc depart aarhu univers schmidhub neural network montana davi train feedforward neural network use genet algorithm proceed intern joint confer artifici pp san francisco ca usa morgan kaufmann publish montavon orr müller lectur note comput scienc seri lnc vol neural network trick trade springer verlag moodi fast learn hierarchi touretzki ed advanc neural inform process system nip vol pp morgan kaufmann moodi effect number paramet analysi gener regular nonlinear learn system lippman moodi touretzki ed advanc neural inform process system nip vol pp morgan kaufmann moodi utan j architectur select strategi neural network applic corpor bond rate predict refen ed neural network capit market john wiley son moor atkeson priorit sweep reinforc learn less data less time machin learn moor atkeson algorithm variabl resolut reinforc learn multidimension machin learn moriarti symbiot evolut neural network sequenti decis task thesi depart comput scienc univers texa austin moriarti miikkulainen effici reinforc learn symbiot evolut machin learn morimoto doya robust reinforc learn leen dietterich tresp ed advanc neural inform process system nip vol pp mit press mostel tukey data analysi includ statist lindzey aronson ed handbook social psycholog vol mozer focus algorithm tempor sequenc recognit complex system mozer discov discret distribut represent iter competit learn lippmann moodi touretzki ed advanc neural inform process system vol pp morgan kaufmann mozer induct multiscal tempor structur lippman moodi touretzki ed advanc neural inform process system nip vol pp morgan kaufmann mozer smolenski skeleton techniqu trim fat network via relev assess touretzki ed advanc neural inform process system nip vol pp morgan kaufmann muller gunzing guggenbühl fast neural net simul dsp processor array ieee transact neural network munro dual scheme scalar reinforc learn proceed ninth annual confer cognit scienc societi pp murray edward j synapt weight nois dure mlp learn enhanc generalis learn trajectori hanson cowan gile ed advanc neural inform process system nip vol pp san mateo ca morgan kaufmann nadal parga neuron low nois limit factori code maximis inform transfer network nagumo arimoto yoshizawa activ puls transmiss line simul nerv axon proceed ire nair hinton rectifi linear unit improv restrict boltzmann machin intern confer machin learn narendra parthasarathi identif control dynam system use neural network ieee transact neural network narendra thathatchar learn survey ieee transact system man cybernet neal bayesian learn neural network thesi univers toronto neal classif bayesian neural network candela magnini dagan ed lectur note comput scienc vol machin learn challeng evalu predict uncertainti visual object classif recognis textual entail pp springer neal zhang j high dimension classif bayesian neural network dirichlet diffus tree guyon gunn nikravesh zadeh ed studi fuzzi soft comput featur extract foundat applic pp springer neftci da pedroni cauwenbergh driven contrast diverg spike neuromorph system frontier neurosci neil liu minitaur spike network acceler ieee transact veri larg scale integr vlsi system pp nessler pfeiffer bues maass bayesian comput emerg gener cortic microcircuit plastic plo comput biolog neti schneider young maxim fault toler neural network ieee transact neural network neuneier zimmermann train neural network orr müller ed lectur note comput scienc vol neural network trick trade pp springer newton philosophia naturali principia mathematica london william dawson son nguyen widrow b truck exampl self learn neural network proceed intern joint confer neural network pp ieee press nilsson j principl artifici intellig san francisco ca usa morgan kaufmann nolfi floreano miglino mondada evolv autonom robot differ approach evolutionari robot brook mae ed fourth intern workshop synthesi simul live system artifici life iv pp mit nolfi parisi elman learn evolut neural network adapt behavior nowak juri trigg b sampl strategi imag classif proc eccv pp springer nowlan hinton simplifi neural network soft weight share neural comput connor neil liu delbruck pfeiffer classif sensor fusion spike deep belief network frontier neurosci oh jung gpu implement neural network pattern recognit oja neural network princip compon subspac intern journal neural system oja data compress featur extract autoassoci feedforward neural network kohonen mäkisara simula kanga ed artifici neural network vol pp elsevi scienc publish bv olshausen field j emerg recept field properti learn spars code natur imag natur omlin gile extract rule recurr neural network neural network oquab bottou laptev sivic j learn transfer imag represent use convolut neural network technic report reilli biolog plausibl learn use local tion differ gener recircul algorithm neural comput reilli make work memori work comput model learn prefront cortex basal ganglia technic report ic reilli wyatt herd mingu jilk j recurr process dure object recognit frontier psycholog orr müller lectur note comput scienc seri lnc vol neural network trick trade springer verlag ostrovskii volin borisov über die berechnung von ableitungen wissenschaftlich zeitschrift der technischen hochschul für chemi otsuka represent extern world base approach thesi nara institut scienc technolog otsuka yoshimoto doya reinforc learn partial observ environ proc esann ott krechel liwicki dengel local featur base onlin mode detect recurr neural network proceed intern confer frontier handwrit recognit pp ieee comput societi oudey baran kaplan intrins motiv learn real world sensorimotor skill development constraint baldassarr mirolli ed intrins motiv learn natur artifici system springer pachitariu sahani regular nonlinear neural languag model need arxiv preprint palm associ memori biolog cybernet palm inform storag capac local learn rule neural comput pan yang q survey transfer learn ieee transact knowledg data engin parekh yang honavar construct neural network learn algorithm pattern classif ieee transact neural network parker b technic report center comp research econom manag mit pascanu gulcehr cho bengio construct deep recurr neural network arxiv preprint pascanu mikolov bengio difficulti train recurr neural network icml jmlr w cp vol pasemann steinmetz dieckman u evolv structur function neurocontrol angelin michalewicz schoenauer yao zalzala ed proceed congress evolutionari comput vol pp mayflow hotel washington dc usa ieee press pearlmutt learn state space trajectori recurr neural network neural comput pearlmutt fast exact multipl hessian neural comput pearlmutt gradient calcul dynam recurr neural network survey ieee transact neural network schmidhub neural network pearlmutt hinton unsupervis learn procedur discov regular denker ed neural network comput american institut physic confer proceed vol pp peng william j increment machin learn ger eck schmidhub j kalman filter improv lstm network perform problem unsolv tradit recurr net neural network perrett hietanen oram benson roll organ function cell respons face tempor cortex discuss philosoph transact royal societi london seri b biolog scienc perrett roll caan visual neuron respons face monkey tempor cortex experiment brain research peter j polici gradient method scholarpedia peter schaal natur neurocomput peter schaal reinforc learn motor skill polici gradient neural network pham kermorv louradour j dropout improv recurr neural network handwrit recognit arxiv preprint pineda j gener recurr neural network physic review letter plate holograph recurr network hanson cowan gile ed advanc neural inform process system nip vol pp morgan kaufmann plumbley inform theori unsupervis neural network dissert publish technic report engin depart cambridg univers pollack b implic recurs distribut represent proc nip pp pollack b recurs distribut represent artifici intellig pontryagin boltyanskii gamrelidz mishchenko mathemat theori optim process poon domingo network new deep architectur ieee intern confer comput vision workshop pp ieee post finit combinatori journal symbol logic prasoon petersen igel lauz dam nielsen voxel classif base triplanar convolut neural network appli cartilag segment knee mri lnc vol medic imag comput comput assist intervent miccai pp springer precup sutton singh model tempor abstract plan advanc neural inform process system nip pp morgan kaufmann prokhorov convolut learn system object classif lidar data ieee transact neural network prokhorov feldkamp tyukin adapt behavior fix weight rnn overview proceed ieee intern joint confer neural network pp prokhorov puskoriu feldkamp dynam neural network control kolen kremer ed field guid dynam recurr network pp ieee press prokhorov wunsch adapt critic design ieee transact neural network puskoriu feldkamp neurocontrol nonlinear dynam system kalman filter train recurr network ieee transact neural network raiko valpola lecun deep learn made easier linear transform perceptron intern confer artifici intellig statist pp raina madhavan ng deep unsupervis learn use graphic processor proceed annual intern confer machin learn pp acm ramach raab anlauf hachmann beichter bruel et al multiprocessor memori architectur neurocomput intern journal neural system ranzato huang boureau lecun unsupervis learn invari featur hierarchi applic object recognit proc comput vision pattern recognit confer pp ieee press ranzato poultney chopra lecun effici learn spars represent model platt et al ed advanc neural inform process system nip mit press rauber merkl dittenbach grow hierarch map exploratori analysi data ieee transact neural network razavian azizpour sullivan carlsson cnn featur shelf astound baselin recognit arxiv preprint rechenberg technisch system nach prinzipien der biologischen evolut dissert publish redlich redund reduct strategi unsupervis learn neural comput refen zaprani franci stock perform model use neural network compar studi regress model neural network rezend gerstner stochast variat learn recurr spike network frontier comput neurosci riedmil neural fit q experi data effici neural reinforc learn method proc pp berlin heidelberg riedmil braun direct adapt method faster backpropag learn rprop algorithm proc ijcnn pp ieee press riedmil lang voigtlaend autonom reinforc learn raw visual input data real world applic intern joint confer neural network pp riesenhub poggio hierarch model object recognit cortex natur neurosci rifai vincent muller glorot bengio contract encod explicit invari dure featur extract proceed intern confer machin learn pp ring b increment develop complex behavior automat construct hierarchi birnbaum collin ed machin learn proceed eighth intern workshop pp morgan kaufmann ring b learn sequenti task increment ad higher order hanson cowan gile ed advanc neural inform process system vol pp morgan kaufmann ring b continu learn reinforc environ thesi austin texa univers texa austin ring schaul schmidhub j organ behavior proceed first joint confer develop learn epigenet robot risi stanley unifi approach evolv plastic neural geometri intern joint confer neural network pp ieee rissanen j stochast complex model annal statist ritter kohonen semant map biolog cybernet robinson fallsid util driven dynam error propag work technic report cambridg univers engin depart robinson fallsid dynam reinforc driven error propag network applic game play proceed confer cognit scienc societi pp rodriguez wile j recurr neural network learn implement count advanc neural inform process system nip vol pp mit press rodriguez wile elman j recurr neural network learn count connect scienc roggen hofmann thoma floreano hardwar spike neural network reconfigur connect autonom robot proc confer evolv hardwar pp ieee rohwer move target train method kindermann linden ed proceed distribut adapt neural inform process oldenbourg rosenblatt perceptron probabilist model inform storag organ brain psycholog review rosenblatt principl neurodynam new york spartan roux racoceanu lomeni kulikova irshad klossa et al mitosi detect breast cancer histolog icpr contest journal patholog informat rubner schulten develop featur detector organ network model biolog cybernet rückstieß felder schmidhub j explor polici gradient method daeleman et al ed lnai vol european confer machin learn ecml principl practic knowledg discoveri databas part ii pp rumelhart hinton william j learn intern represent error propag rumelhart mcclelland ed parallel distribut process vol pp mit press rumelhart zipser featur discoveri competit learn parallel distribut process pp mit press rummeri niranjan use connectionist sytem technic report uk cambridg univers russel norvig canni malik edward artifici intellig modern approach vol englewood cliff prentic hall saito nakano partial bfg updat effici calcul neural network neural comput sak senior beaufay long memori recurr neural network architectur larg scale acoust model proc interspeech sak vinyal heigold senior mcdermott monga et al sequenc discrimin distribut train long memori recurr neural network proc interspeech salakhutdinov hinton semant hash intern journal approxim reason sallan hinton reinforc learn factor state action journal machin learn research sałustowicz schmidhub j probabilist increment program evolut evolutionari comput samejima doya kawato credit assign modular reinforc learn neural network schmidhub neural network samuel studi machin learn use game checker ibm journal research develop sanger optim principl unsupervis learn touretzki ed advanc neural inform process system nip vol pp morgan kaufmann santamaría sutton ram experi reinforc learn problem continu state action space adapt behavior saravanan fogel b evolv neural control system ieee expert saund unsupervis learn mixtur multipl caus binari data cowan tesauro alspector ed advanc neural inform process system nip vol pp morgan kaufmann schaback werner numerisch mathematik vol springer schäfer udluft zimmermann learn long term depend recurr neural network kollia stafylopati duch oja ed lectur note comput scienc vol icann pp springer schapir strength weak learnabl machin learn schaul schmidhub j metalearn scholarpedia schaul zhang lecun peski learn rate proc intern confer machin learn schemmel grubl meier mueller implement synapt plastic vlsi spike neural network model intern joint confer neural network pp ieee scherer müller behnk evalu pool oper convolut architectur object recognit proc intern confer artifici neural network pp schmidhub j evolutionari principl learn learn learn hook diploma thesi inst f tech univ munich http schmidhub j acceler learn net pfeifer schreter fogelman steel ed connection perspect pp amsterdam elsevi schmidhub j local learn algorithm dynam feedforward recurr network connect scienc schmidhub j dynamisch neuronal netz und da fundamental raumzeitlich lernproblem dynam neural net fundament tempor credit assign problem dissert inst f tech univ munich schmidhub j learn algorithm network intern extern feedback touretzki elman sejnowski hinton ed proc connectionist model summer school pp morgan kaufmann schmidhub j neural heat exchang talk tu munich univers colorado boulder li nip workshop unsupervis learn also publish intl confer neural inform process vol pp schmidhub j algorithm dynam reinforc learn plan reactiv environ proc intern joint confer neural network vol pp schmidhub j curiou control system proceed intern joint confer neural network vol pp ieee press schmidhub j learn gener action sequenc kohonen mäkisara simula kanga ed artifici neural network pp elsevi scienc publish bv schmidhub j reinforc learn markovian environ lippman moodi touretzki ed advanc neural inform process system vol nip pp morgan kaufmann schmidhub j fix size storag time complex learn gorithm fulli recurr continu run network neural comput schmidhub j learn complex extend sequenc use principl histori compress neural comput base tr tum schmidhub j learn factori code predict minim neural comput schmidhub j introspect network learn run weight chang algorithm proc intl conf artifici neural network brighton pp iee schmidhub j netzwerkarchitekturen zielfunktionen und kettenregel network architectur object function chain rule habilit thesi inst f tech univ munich schmidhub j discov neural net low kolmogorov complex high gener capabl neural network schmidhub j speed prior new simplic measur yield optim comput predict kivinen sloan ed lectur note artifici intellig proceed annual confer comput learn theori pp sydney australia springer schmidhub j optim order problem solver machin learn schmidhub j development robot optim artifici curios creativ music fine art connect scienc schmidhub j gödel machin fulli optim univers goertzel pennachin ed artifici gener intellig pp springer verlag variant avail arxiv schmidhub j prototyp resili robot scienc schmidhub j neural network technic report swiss ai lab idsia schmidhub j first deep learn system deep learn timelin technic report swiss ai lab idsia schmidhub j powerplay train increasingli gener problem solver continu search simplest still unsolv problem frontier psycholog schmidhub ciresan meier masci grave fast deep net agi vision proc fourth confer artifici gener intellig pp schmidhub eldrach foltin b semilinear predict minim produc featur detector neural comput schmidhub huber learn gener artifici fovea trajectori target detect intern journal neural system schmidhub mozer preling continu histori compress hüning neuhaus rau ritschel ed proc intl workshop neural network pp augustinu rwth aachen schmidhub preling discov predict classif technic report dept comp univers colorado boulder publish neural comput schmidhub wahnsiedl plan simpl trajectori use neural subgoal gener meyer roitblat wilson ed proc intern confer simul adapt behavior pp mit press schmidhub wierstra gagliolo gomez j train recurr network evolino neural comput schmidhub zhao schraudolph reinforc learn polici thrun pratt ed learn learn pp kluwer schmidhub zhao wier shift induct bia stori algorithm adapt levin search increment machin learn schölkopf burg smola j ed advanc kernel support vector learn cambridg mit press schraudolph fast curvatur product gradient descent neural comput schraudolph sejnowski j unsupervis discrimin cluster data via optim binari inform gain hanson cowan gile ed advanc neural inform process system vol pp san mateo morgan kaufmann schraudolph sejnowski j temper backpropag network weight creat equal touretzki mozer hasselmo ed advanc neural inform process system nip vol pp cambridg mit press schrauwen verstraeten van campenhout j overview reservoir comput theori applic implement proceed european symposium artifici neural network pp schuster learn maxim inform transfer nonlinear noisi neuron nois breakdown physic review schuster supervis learn sequenti data applic speech recognit thesi kyoto japan nara institut scienc technolog schuster paliw bidirect recurr neural network ieee transact signal process schwartz reinforc learn method maxim count reward proc icml pp schwefel numerisch optimierung von tion publish birkhäus basel segment neuron structur em stack challeng ieee intern symposium biomed imag http sehnk osendorf rückstieß grave peter schmidhub j polici gradient neural network sermanet eigen zhang mathieu fergu lecun feat integr recognit local detect use convolut work arxiv preprint sermanet lecun traffic sign recognit convolut network proceed intern joint confer neural network pp oster lichtstein et al caviar k neuron synaps g aer hardwar system visual object recognit track ieee transact neural network serr riesenhub louie poggio role featur real world object recognit biolog vision biolog motiv comput vision pp seung learn spike neural network reinforc stochast synapt transmiss neuron schmidhub neural network cottrel effici visual code retina proc intern confer learn represent arxiv preprint zhang cottrel recurs ica advanc neural inform process system nip vol shanno condit method function minim mathemat comput shannon mathemat theori commun part ii bell system technic journal xxvii shao wu li x learn deep wide spectral method learn deep network ieee transact neural network learn system shavlik combin symbol neural learn machin learn shavlik towel combin neural learn algorithm empir result connect scienc siegelmann theoret foundat recurr neural network thesi new brunswick rutger state new jersey rutger siegelmann sontag ture comput neural net appli mathemat letter silva almeida b speed eckmil ed advanc neural comput pp amsterdam elsevi síma j load deep network hard neural comput síma j train singl sigmoid neuron hard neural comput simard steinkrau platt j best practic convolut neural network appli visual document analysi seventh intern confer document analysi recognit pp sim evolv virtual creatur glassner ed acm siggraph proceed siggraph comput graphic proceed annual confer pp acm press isbn simsek barto skill character base nip pp singh reinforc learn algorithm vian decis process nation confer artifici intellig pp singh barto chentanez intrins motiv reinforc learn advanc neural inform process system vol nip cambridg mit press smith learn system base genet adapt algorithm thesi univ pittsburgh smolenski parallel distribut process explor crostructur cognit inform process dynam system dation harmoni theori vol pp cambridg usa mit press chapter solla acceler learn layer neural network complex system solomonoff j formal theori induct infer part inform control solomonoff j induct system ieee transact inform theori soloway learn program learn construct mechan explan commun acm song miller abbott competit hebbian learn synapt plastic natur neurosci speelpen b compil fast partial deriv function given algorithm thesi depart comput scienc univers illinoi srivastava masci kazerounian gomez schmidhub j compet comput advanc neural inform process system nip pp stallkamp schlips salmen igel german traffic sign recognit benchmark classif competit tional joint confer neural network pp ieee press stallkamp schlips salmen igel man comput benchmark machin learn algorithm traffic sign recognit neural network stanley ambrosio gauci j encod evolv neural network artifici life stanley miikkulainen evolv neural network augment topolog evolutionari comput steijver grunwald recurr network perform contextsensit predict task proceed annual confer cognit scienc societi erlbaum steil j onlin reservoir adapt intrins plastic echo state learn neural network stemmler singl spike suffic simplest form stochast reson model neuron network comput neural system stoianov zorzi emerg visual number sens hierarch gener model natur neurosci stone choic assess statist predict journal royal statist societi b stoop schindler bunimovich pyramid neuron lock respond chaotic like synchron neurosci research stratonovich condit markov process theori probabl applic sun chen lee time warp invari neural network hanson cowan gile ed advanc neural inform process system nip vol pp morgan kaufmann sun gile chen lee neural network pushdown automaton model stack learn simul technic report univers maryland colleg park sun gomez schaul schmidhub j linear time natur evolut strategi function proceed genet evolutionari comput confer amsterdam nl acm sun wierstra schaul schmidhub j effici natur evolut strategi proc genet evolutionari comput confer pp sutskev hinton taylor recurr tempor restrict boltzmann machin nip vol sutskev vinyal le sequenc sequenc learn neural network technic report googl nip sutton barto reinforc learn introduct cambridg mit press sutton mcallest singh mansour polici gradient method reinforc learn function approxim advanc neural inform process system nip vol pp sutton precup singh mdp framework tempor abstract reinforc learn artifici intellig sutton szepesvári maei converg n algorithm learn linear function approxim advanc neural inform process system nip vol pp szabó póczo lőrincz optim independ process analysi independ compon analysi blind signal separ pp springer szegedi liu jia sermanet reed anguelov et al go deeper convolut technic report googl szegedi toshev erhan deep neural network object detect pp taylor spiro bregler fergu learn invari imit confer comput vision pattern recognit pp ieee tegg wang eickholt cheng j nncon improv protein contact map predict use neural network nucleic acid research suppl teichmann wiltschut hamker learn invari natur imag inspir observ primari visual cortex neural comput teller evolut mental model kenneth kinnear ed advanc genet program pp mit press tenenberg karlsson whitehead learn via task decomposit meyer roitblat wilson ed anim animat proceed second intern confer simul adapt behavior pp mit press tesauro backgammon program achiev play neural comput tieleman hinton lectur divid gradient run averag recent magnitud coursera neural network machin learn tikhonov arsenin john solut problem winston ting witten stack gener doe work proc intern joint confer artifici intellig tiňo hammer b architectur bia recurr neural network fractal analysi neural comput tonk wile j learn task recurr neural network analysi stabil proceed fourth biennial confer australasian cognit scienc societi towel shavlik artifici neural network artifici intellig tsitsikli van roy b method larg scale dynam program machin learn tsodyk pawelzik markram neural network dynam synaps neural comput tsodyk skagg sejnowski mcnaughton popul dynam theta rhythm phase precess hippocamp place cell fire spike neuron model hippocampu turaga murray jain roth helmstaedt briggman et al convolut network learn gener affin graph imag segment neural comput ture comput number applic entscheidungsproblem proceed london mathemat societi seri turner miller cartesian genet program encod artifici neural network comparison use three benchmark proceed confer genet evolutionari comput gecco pp ueda optim linear combin neural network improv classif perform ieee transact pattern analysi machin intellig schmidhub neural network urlb digit neural network thesi universidad del vall utgoff stracuzzi j learn neural comput vahe omlin machin learn method extract symbol knowledg recurr neural network neural comput vaillant monrocq lecun origin approach localis object imag iee proceed vision imag signal process van den berg whiteson critic factor perform hyperneat gecco proceed genet evolutionari comput confer pp van hasselt reinforc learn continu state action space wier van otterlo ed reinforc learn pp springer vapnik principl risk minim learn theori lippman moodi touretzki ed advanc neural inform process system nip vol pp morgan kaufmann vapnik natur statist learn theori new york springer versino gambardella learn fine motion use hierarch extend kohonen map proc intl conf artifici neural network pp springer veta viergev pluim stathoniko van diest j miccai grand challeng mitosi detect vieira barrada train algorithm classif dimension data neurocomput viglion applic pattern recognit technolog mendel fu ed adapt learn pattern recognit system academ press vincent hugo bengio manzagol extract compos robust featur denois autoencod proceed intern confer machin learn pp new york ny usa acm vlassi littman barber comput complex stochast control optim pomdp acm transact comput theori vogl mangi rigler zink alkon acceler converg method biolog cybernet von der malsburg orient sensit cell striat cortex kybernetik walding lee prow step toward automat program write walker norton ed proceed intern joint confer artifici intellig pp morgan kaufmann wallac boulton inform theoret measur classif comput journal wan time seri predict use connectionist network intern delay line weigend gershenfeld ed time seri predict forecast futur understand past pp wang man fast dropout train proceed intern confer machin learn pp wang venkatesh judd optim stop effect machin complex learn advanc neural inform process system nip pp morgan kaufmann watanab pattern recognit human mechan new york wiley watanab kolmogorov complex comput complex eatc monograph theoret comput scienc springer watkin learn delay reward thesi oxford king colleg watkin dayan machin learn watrou kuhn induct automata use recurr network moodi hanson lippman ed advanc neural inform process system vol pp morgan kaufmann waydo koch unsupervis learn individu categori imag neural comput weigend gershenfeld result time seri predict competit santa fe institut neural network ieee intern confer pp ieee weigend rumelhart huberman gener applic forecast lippmann moodi touretzki ed advanc neural inform process system nip vol pp san mateo ca morgan kaufmann weiss hierarch chunk classifi system proceed nation confer artifici intellig vol pp aaai mit press weng ahuja huang cresceptron neural network grow adapt intern joint confer neural network vol pp ieee weng ahuja huang learn recognit segment use cresceptron intern journal comput vision werbo j beyond regress new tool predict analysi behavior scienc thesi harvard univers werbo j applic advanc nonlinear sensit analysi proceed ifip confer nyc pp werbo j build understand adapt system approach factori autom brain research ieee action system man cybernet werbo j gener backpropag applic recurr ga market model neural network werbo j backpropag neurocontrol review prospectu intern joint confer neural network vol pp werbo j neural network control system identif proceed tampa werbo j neural network system identif control chemic industri white sofg ed handbook intellig control neural fuzzi adapt approach pp thomson learn werbo j backward differenti ad neural net past link new opportun automat differenti applic theori implement pp springer west saad adapt learn multilay network touretzki mozer hasselmo ed nip pp mit press white learn artifici neural network statist perspect neural comput whitehead reinforc learn adapt control percept action thesi univers rochest whiteson evolutionari comput reinforc learn wier van otterlo ed reinforc learn pp berlin germani springer whiteson kohl miikkulainen stone evolv keepaway soccer player task decomposit machin learn whiteson stone evolutionari function approxim reinforc learn journal machin learn research widrow hoff associ storag retriev digit inform network adapt neuron biolog prototyp synthet system widrow rumelhart lehr neural network applic industri busi scienc commun acm wieland evolv neural network control unstabl system intern joint confer neural network vol pp ieee wier schmidhub j solv pomdp levin search eira saitta ed machin learn proceed thirteenth intern confer pp san francisco ca morgan kaufmann publish wier schmidhub j adapt behavior wier schmidhub j fast onlin q λ machin learn wier van otterlo reinforc learn springer wierstra foerster peter schmidhub j recurr polici gradient logic journal igpl wierstra schaul peter schmidhub j natur evolut strategi congress evolutionari comput wiesel hubel recept field singl neuron cat striat cortex journal physiolog wile elman j learn count without counter case studi dynam activ landscap recurr network proceed seventeenth annual confer cognit scienc societi pp mit press cambridg wilkinson h ed algebra eigenvalu problem new york ny usa oxford univers press william j connectionist network ical analysi technic report san diego institut cognit scienc univers california william j toward theori connectionist system technic report boston colleg comp northeastern univers william j complex exact gradient comput algorithm recurr neural network technic report boston northeastern univers colleg comput scienc william j simpl statist algorithm connectionist reinforc learn machin learn william j train recurr network use extend kalman filter intern joint confer neural network vol pp ieee william peng j effici algorithm train recurr network trajectori neural comput william zipser learn algorithm continu run fulli recurr network technic report ic report san diego la jolla univ california william zipser experiment analysi recurr learn algorithm connect scienc william zipser learn algorithm continu run fulli recurr network neural comput willshaw von der malsburg pattern neural connect set proceed royal societi london seri b windisch load deep network hard pyramid case neural comput wiskott sejnowski slow featur analysi unsupervis learn invari neural comput schmidhub neural network witczak korbicz mrugalski patton j gmdh neural approach robust fault diagnosi applic damad benchmark problem control engin practic wöllmer blaschk schindl schuller färber mayer et al driver distract detect use long memori ieee transact intellig transport system tit wöllmer schuller rigol keyword spot exploit long memori speech commun wolpert stack gener neural network wolpert bayesian backpropag function rather weight cowan tesauro alspector ed advanc neural inform process system nip vol pp morgan kaufmann wu baldi learn play go use recurs neural network neural network wu shao leverag hierarch parametr network skelet joint base action segment recognit proc confer comput vision pattern recognit wyatt curran reilli limit feedforward vision recurr process promot robust object recognit object degrad journal cognit neurosci wysoski benuskova kasabov evolv spike neural network audiovisu inform process neural network yamauchi beer sequenti behavior learn evolv dynam neural network adapt behavior yamin hong cadieu dicarlo j hierarch modular optim convolut network achiev represent similar macaqu human ventral stream advanc neural inform process system nip pp yang ji xu wang lv yu et al detect human action surveil video trec video retriev evalu workshop yao x review evolutionari artifici neural network intern journal intellig system yin meng jin development approach structur organ reservoir comput ieee transact autonom mental develop yin wang zhang liu icdar chines handwrit recognit competit intern confer document analysi recognit pp young davi mishtal arel hierarch spatiotempor featur extract use recurr onlin cluster pattern recognit letter yu chen cheng dynam learn rate optim backpropag algorithm ieee transact neural network frinken fischer bunk neural network languag model handwrit recognit pattern recognit zeiler adadelta adapt learn rate method corr zeiler fergu visual understand convolut network technic report nyu zemel minimum descript length framework unsupervis learn thesi univers toronto zemel hinton develop popul code minim descript length cowan tesauro alspector ed advanc neural inform process system vol pp morgan kaufmann zeng goodman smyth discret recurr neural network grammat infer ieee transact neural network zimmermann tietz grothmann forecast recurr neural network trick montavon orr müller ed lectur note comput scienc vol neural network trick trade ed pp springer zipser keho littlewort fuster j spike network model activ memori journal neurosci