neural network 61 2015 content list available sciencedirect neural network journal homepage review deep learning neural network overview jürgen schmidhuber swiss ai lab idsia istituto dalle molle di studi sull intelligenza artificiale university lugano supsi galleria 2 6928 switzerland r c l e n f article history received 2 may 2014 received revised form 12 september 2014 accepted 14 september 2014 available online 13 october 2014 keywords deep learning supervised learning unsupervised learning reinforcement learning evolutionary computation b r c recent year deep artificial neural network including recurrent one numerous contest pattern recognition machine learning historical survey compactly summarizes relevant work much previous millennium shallow deep learner distinguished depth credit assignment path chain possibly learnable causal link action effect review deep supervised learning also recapitulating history backpropagation unsupervised learning reinforcement learning evolutionary computation indirect search short program encoding deep large network 2014 published elsevier content introduction deep learning dl neural network nns 86 notation activation spreading nns 87 depth credit assignment path cap problem 88 recurring theme deep learning 88 dynamic programming learning 88 unsupervised learning ul facilitating sl rl 89 learning hierarchical representation deep sl ul rl 89 occam razor compression minimum description length mdl 89 fast graphic processing unit gpus dl nns 89 supervised nns some helped unsupervised nns 89 early nns since 90 around 1960 visual cortex provides inspiration dl section 90 1965 deep network based group method data handling 90 1979 convolution weight replication subsampling neocognitron 90 beyond development backpropagation bp nns 90 bp feedforward nns fnns recurrent nns rnns 91 late beyond numerous improvement nns 91 idea dealing long time lag deep cap 91 better bp advanced gradient descent compare section 92 searching simple nns section 92 potential benefit ul sl compare section 92 1987 ul autoencoder ae hierarchy compare section 93 1989 bp convolutional nns cnns section 93 1991 fundamental deep learning problem gradient descent 93 1991 history compression deep stack rnns 94 1992 mp towards mpcnns compare section 94 address juergen 2014 published elsevier 86 schmidhuber neural network 61 2015 1994 early nns 95 1995 supervised recurrent deep learner lstm rnn 95 2003 nns successful deep nns 96 ul deep belief stack bp 96 improved stack 96 2009 first official competition rnns mpcnns 97 2010 plain backprop distortion gpu break mnist record 97 2011 mpcnns gpu achieve superhuman vision performance 97 2011 optimization rnns 98 2012 first contest imagenet object detection segmentation 98 contest benchmark record 98 currently successful technique lstm rnns 99 recent trick improving sl deep nns compare section 99 consequence neuroscience 100 dl spiking neuron 100 dl fnns rnns reinforcement learning rl 100 rl nn world model yield rnns deep cap 100 deep fnns traditional rl markov decision process mdps 101 deep rl rnns partially observable mdps pomdps 101 rl facilitated deep ul fnns rnns 102 deep hierarchical rl hrl subgoal learning fnns rnns 102 deep rl direct nn 102 deep rl indirect policy nn search 103 universal rl 103 conclusion outlook 103 acknowledgment 104 reference 104 preface preprint invited deep learning dl overview one goal assign credit contributed present state art acknowledge limitation ing achieve goal dl research community may viewed continually evolving deep network scientist influenced complex way starting recent dl result tried trace back origin relevant idea past half century beyond sometimes using local search follow citation citation backwards time since not dl publication properly acknowledge earlier relevant work tional global search strategy employed aided consulting numerous neural network expert result present preprint mostly consists reference nevertheless expert lection bias may missed important work related bias wa surely introduced special familiarity work dl research group past son work viewed merely snapshot going credit assignment process help improve please not hesitate send correction suggestion juergen introduction deep learning dl neural network nns modifiable component learning system sible success failure change improve formance ha called fundamental credit assignment problem minsky 1963 general credit assignment od universal problem solver various theoretical sens section present survey however focus narrower commercially important subfield deep learning dl artificial neural network nns standard neural network nn consists many simple nected processor called neuron producing sequence activation input neuron get activated sors perceiving environment neuron get activated weighted connection previously active neuron tail section 2 some neuron may influence environment triggering action learning credit assignment finding weight make nn exhibit desired behavior driving car depending problem neuron nected behavior may require long causal chain tational stage section 3 stage transforms often way aggregate activation network deep learning accurately assigning credit across many stage shallow model stage around many decade not century section model eral successive nonlinear layer neuron date back least section section efficient dient descent method supervised learning sl discrete differentiable network arbitrary depth called propagation bp wa developed plied nns 1981 section training deep nns many layer however found difficult tice late section become explicit research subject early section dl became tically feasible some extent help unsupervised learning ul section 1991 section 2006 also saw many improvement purely vised dl section 5 new millennium deep nns nally attracted attention mainly outperforming alternative machine learning method kernel machine schölkopf burges smola 1998 vapnik 1995 numerous portant application fact since 2009 supervised deep nns many official international pattern recognition competition section achieving first human visual pattern recognition result limited domain tion 2011 deep nns also become relevant general field reinforcement learning rl no pervising teacher section 6 feedforward acyclic nns fnns recurrent cyclic nns rnns contest section sense rnns deepest nns section 3 general computer powerful fnns principle create process memory arbitrary sequence input pattern schmidhuber siegelmann sontag 1991 unlike traditional method automatic sequential gram synthesis balzer 1985 deville lau 1994 soloway schmidhuber neural network 61 2015 87 abbreviation alphabetical order ae autoencoder ai artificial intelligence ann artificial neural network bfgs bnn biological neural network bm boltzmann machine bp backpropagation brnn recurrent neural network cap credit assignment path cec constant error carousel cfl context free language covariance matrix estimation e cnn convolutional neural network cosyne csl context sensitive language ctc connectionist temporal classification dbn deep belief network dct discrete cosine transform dl deep learning dp dynamic programming direct policy search ea evolutionary algorithm em expectation maximization e evolution strategy fm flat minimum search fnn feedforward neural network fsa finite state automaton gmdh group method data handling gofai good ai gp genetic programming gpu graphic processing unit mpcnn hmm hidden markov model hrl hierarchical reinforcement learning htm hierarchical temporal memory hmax hierarchical model x lstm long memory rnn mdl minimum description length mdp markov decision process mnist mixed national institute standard ogy database mp mpcnn cnn ne neuroevolution neat ne augmenting topology ne natural evolution strategy nfq neural fitted nn neural network ocr optical character recognition pcc potential causal connection pdcc potential direct causal connection pm predictability minimization pomdp partially observable mdp raam recursive memory rbm restricted boltzmann machine relu rectified linear unit rl reinforcement learning rnn recurrent neural network resilient backpropagation sl supervised learning slim nn neural network sota tree algorithm svm support vector machine tdnn neural network timit continuous speech corpus ul unsupervised learning wta 1986 waldinger lee 1969 rnns learn program mix sequential parallel information processing natural ficient way exploiting massive parallelism viewed crucial sustaining rapid decline computation cost observed past 75 year rest paper structured follows section 2 duce compact notation simple yet general enough accommodate fnns rnns section 3 introduces concept credit assignment path cap measure whether learning given nn application deep shallow type section 4 list recurring theme dl sl ul rl section 5 cu sl ul ul facilitate sl although pure sl ha become dominant recent competition section section 5 arranged historical timeline format subsection important inspiration technical contribution section 6 deep rl discus traditional dynamic programming dp rl combined search technique sl ul deep nns well general method direct indirect search weight space deep fnns rnns ing successful policy gradient evolutionary method notation activation spreading nns throughout paper let j k p q r denote positive integer variable assuming range implicit given context let n denote positive integer constant nn topology may change time section any given moment described finite subset unit node neuron n finite set h n directed edge connection node fnns acyclic graph rnns cyclic first input layer set input unit subset fnns kth layer k 1 set node u edge path length k no longer path some input unit may shortcut connection distant layer processing fully connected rnns unit connection unit nn behavior program determined set valued possibly modifiable parameter weight wi 1 n focus single finite episode epoch tion processing activation spreading without learning weight change following slightly unconventional notation designed compactly describe happening time system episode partially causal sequence xt 1 real value call event xt either put set environment activation unit may directly depend xk k current nn dependent set int index k representing incoming causal nections link let function v encode topology information map event index pair k weight index ample case may xt ft nett nett xkwv k additive case nett xkwv k multiplicative case ft typically linear activation function tanh many recent nns section also event type xt xk some network type may also use complex polynomial activation function section xt may directly affect certain xk k outgoing connection link represented current set outt index k some event called output event note many xt may refer different activation unit rnns williams 1989 unfolding time also fnns sequentially exposed input pattern large training set encoded input event episode weight 88 schmidhuber neural network 61 2015 may get reused way rnns convolutional nns section call weight sharing across space time weight sharing may greatly reduce nn descriptive complexity number bit information required describe nn section supervised learning sl certain nn output event xt may associated label target dt yielding error et et xt typical goal vised nn training find weight yield episode small total error e sum et hope nn generalize well later episode causing only small error viously unseen sequence input event many alternative error function sl ul possible sl assumes input event independent earlier output event may affect environment action causing subsequent perception assumption doe not hold broader field sequential decision making reinforcement learning rl hutter 2005 kaelbling littman moore 1996 sutton barto 1998 wiering van otterlo 2012 section 6 rl some input event may encode reward signal given environment typical goal find weight yield episode high sum reward signal sequence appropriate output action section use notation compactly describe central algorithm dl namely backpropagation bp supervised fnns rnns fnns may viewed rnns certain fixed zero weight section 6 address general rl case depth credit assignment path cap problem measure whether credit assignment given nn tion deep shallow type introduce concept credit assignment path cap chain possibly causal link event section 2 input hidden output layer fnns transformation time rnns let u first focus sl consider two event xp xq 1 q depending application may potential direct causal connection pdcc expressed boolean cate pdcc p q true only p element list p q defined cap minimal one p learning algorithm may allowed change wv p q prove performance future episode general possibly indirect potential causal connection pcc expressed recursively defined boolean predicate pcc p q sl case true only pdcc p q pcc p k some k pdcc k q latter case appending q any cap p k yield cap p q sive definition set cap may large finite note weight may affect many different pdccs tween successive event listed given cap case rnns fnns suppose cap ha form k q k possibly q first successive element modifiable wv k length suffix list q called cap depth 0 no modifiable link depth limit far backwards credit assignment move causal chain find modifiable suppose episode event sequence xt satisfy computable criterion used decide whether given problem ha solved total error e some threshold set 1 alternative would count only modifiable link measuring depth many typical nn application would not make difference some would section used weight called solution problem depth deepest cap within sequence called solution depth may solution yielding different event sequence different depth given some fixed nn topology smallest depth any solution called problem depth sometimes also speak depth architecture sl fnns fixed topology imply maximal lem depth bounded number layer certain sl rnns fixed weight connection except output unit jaeger 2001 2004 maass natschläger markram 2002 schrauwen verstraeten van campenhout 2007 imal problem depth 1 only final link responding cap modifiable general however rnns may learn solve problem potentially unlimited depth note definition solely based depth causal chain agnostic temporal distance event example shallow fnns perceiving large time dows input event may correctly classify long input sequence appropriate output event thus solve shallow lem involving long time lag relevant event problem depth doe shallow learning end deep learning begin discussion dl expert not yet yielded conclusive response question instead committing precise answer let define purpose overview problem depth 10 require deep learning difficulty problem may little depth some nns quickly learn solve certain deep problem random weight guessing section type direct search section indirect search section weight space training nn first shallow problem whose solution may generalize deep problem collapsing sequence non linear operation single non linear operation see analysis aspect deep linear network baldi hornik 1995 section b general however finding nn precisely model given training set problem blum rivest 1992 judd 1990 also case deep nns de souto souto oliveira 1999 síma 1994 windisch 2005 compare survey negative result síma 2002 section 1 focused sl general case rl unknown environment pcc p q also true xp output event xq any later input action may affect vironment thus any later perception real world environment may even influence event computed physical hardware entangled entire universe ignored possible model replace ifiable environmental pccs part nn ha ready learned predict some unit input event including reward signal former input event action section weight frozen help assign credit still modifiable weight used compute action tion approach may lead deep cap though some dl research automatically rephrasing problem depth reduced section 4 particular time ul used make sl problem le deep section often dynamic programming section used facilitate tain traditional rl problem section section 5 focus cap sl section 6 complex case rl recurring theme deep learning dynamic programming learning one recurring theme dl dynamic programming dp man 1957 help facilitate credit assignment schmidhuber neural network 61 2015 89 certain assumption example sl nns backpropagation self viewed method section ditional rl based strong markovian assumption method help greatly reduce problem depth section dp algorithm also essential system combine cepts nns graphical model hidden markov model hmms baum petrie 1966 stratonovich 1960 tion maximization em dempster laird rubin 1977 friedman hastie tibshirani 2001 baldi chauvin 1996 bengio 1991 bishop 2006 bottou 1991 bourlard morgan 1994 dahl yu deng acero 2012 hastie tibshirani friedman 2009 hinton deng et al 2012 jordan sejnowski 2001 poon domingo 2011 wu shao 2014 unsupervised learning ul facilitating sl rl another recurring theme ul facilitate sl tion 5 rl section 6 ul section normally used encode raw incoming data video speech stream form convenient subsequent ing particular code describe original data le dundant compact way fed sl section rl machine section whose search space may thus become smaller whose cap shallower necessary dealing raw data ul closely connected topic regularization compression section learning hierarchical representation deep sl ul rl many method good artificial intelligence fai nilsson 1980 well recent approach ai sell norvig canny malik edward 1995 machine ing mitchell 1997 learn hierarchy abstract data representation example certain method syntactic pattern recognition fu 1977 grammar induction discover hierarchy formal rule model observation partially un supervised automated lenat 1983 lenat brown 1984 continually learns concept combining previously learnt concept hierarchical representation ing bengio courville vincent 2013 deng yu 2014 ring 1994 also recurring theme dl nns sl section 5 sl section hierarchical rl tion often abstract hierarchical representation natural data compression section section occam razor compression minimum description length mdl occam razor favor simple solution complex one given some programming language principle minimum tion length mdl used measure complexity solution candidate length shortest program putes blumer ehrenfeucht haussler warmuth 1987 chaitin 1966 grünwald myung pitt 2005 kolmogorov levin li vitányi 1997 rissanen 1986 solomonoff 1964 1978 wallace boulton 1968 some method explicitly take account program runtime allender 1992 schmidhuber 1997 2002 watanabe 1992 many consider only program stant runtime written programming language hinton van camp 1993 rissanen 1986 nn case mdl principle suggests low nn weight complexity sponds high nn probability bayesian view buntine weigend 1991 de freitas 2003 mackay 1992 neal 1995 high generalization performance baum haussler 1989 without overfitting training data many method proposed regularizing nns searching computing simple sl nns section rl nns section closely related certain ul method section fast graphic processing unit gpus dl nns previous millennium saw several attempt ating fast hardware faggin 1992 heemskerk 1995 jackel et 1990 korkin de gari gers hemmi 1997 ramacher et 1993 urlbe 1999 widrow rumelhart lehr 1994 exploiting standard hardware anguita gomes 1996 anguita parodi zunino 1994 muller gunzinger guggenbühl 1995 new millennium brought dl form cheap graphic card gpus gpus widely used video game huge competitive market ha driven hardware price gpus excel fast matrix vector multiplication required not only vincing virtual reality also nn training speed learning factor 50 some based fnn implementation section greatly tributed recent success contest pattern recognition section image segmentation section ject detection section supervised nns some helped unsupervised nns main focus current practical application supervised learning sl ha dominated recent pattern recognition contest section several method however use additional unsupervised learning ul facilitate sl section doe make sense treat sl ul section often method bp section used optimize objective function ul sl boundary sl ul may blur example come time series prediction sequence classification section historical timeline format help arrange subsection important inspiration technical contribution although subsection may span time interval many year section briefly mention early shallow nn model since section additional early neurobiological inspiration relevant modern deep learning dl section gmdh network since 1965 knowledge first feedforward dl system section relatively deep neocognitron nn 1979 similar certain modern deep fnn architecture combine convolutional nns cnns weight pattern replication subsampling mechanism section us notation section 2 compactly describe central algorithm dl namely backpropagation bp supervised fnns rnns also summarizes history bp beyond section describes problem encountered late bp deep nns mention several idea previous millennium overcome section discus first hierarchical stack 1987 coupled autoencoders aes concept resurfaced new millennium section section applying bp cnns 1989 important today dl application section explains bp fundamental dl problem gradient discovered section explains deep rnn stack 1991 history compressor ul helped solve previously unlearnable dl benchmark requiring credit assignment path cap section 3 depth 1000 section discus particular wta method called mp 1992 widely used today deep fnns section mention first important contest sl nns section describes purely supervised dl rnn long memory lstm 1995 problem depth 1000 section mention early contest 2003 ensemble shallow fnns well good pattern recognition result cnns deep fnns lstm rnns 90 schmidhuber neural network 61 2015 2003 section mostly deep belief network dbns 2006 related stack autoencoders aes section ul facilitate subsequent sl compare section section mention first 2006 mpcnns 2007 lstm stack 2007 section focus official competition secret test set mostly purely supervised deep nns since 2009 sequence recognition image classification image segmentation object detection many rnn result depended lstm section many fnn result depended fnn code developed since 2004 section particular section section mention recent trick improving dl nns many closely related earlier trick previous millennium section section discus artificial nns help understand biological nns section address possibility dl nns spiking neuron early nns since early nn architecture mcculloch pitt 1943 not learn first idea ul published year later hebb 1949 following decade brought simple nns trained sl narendra thathatchar 1974 rosenblatt 1958 1962 widrow hoff 1962 ul grossberg 1969 kohonen 1972 von der malsburg 1973 willshaw von der malsburg 1976 well closely related associative memory field 1982 palm 1980 sense nns around even longer since early pervised nns essentially variant linear regression od going back least early gauss 1809 1821 legendre 1805 gauss also refers work early nns maximal cap depth 1 section 3 around 1960 visual cortex provides inspiration dl section simple cell complex cell found cat visual cortex hubel wiesel 1962 wiesel hubel 1959 cell fire response certain property visual sensory input orientation edge complex cell exhibit spatial invariance simple cell inspired later deep nn architecture section used certain modern winning deep learner section 1965 deep network based group method data handling network trained group method data handling gmdh ivakhnenko 1968 1971 ivakhnenko lapa 1965 ivakhnenko lapa mcdonough 1967 perhaps first dl system feedforward multilayer perceptron type although wa earlier work nns single hidden layer joseph 1961 viglione 1970 unit gmdh net may polynomial activation function implementing polynomial general widely used nn activation function section 2 given training set er incrementally grown trained regression analysis gauss 1809 1821 legendre 1805 section pruned help separate validation set using today ogy decision regularization used weed superfluous unit compare section number layer unit per layer learned fashion edge wa first example hierarchical resentation learning nns section paper 1971 already described deep gmdh network 8 layer ivakhnenko 1971 numerous application net farlow 1984 ikeda ochiai sawaragi 1976 ivakhnenko 1995 kondo 1998 kondo ueno 2008 kordík náplava snorek 2003 madala ivakhnenko 1994 witczak korbicz mrugalski patton 2006 1979 convolution weight replication subsampling nitron apart deep gmdh network section tron fukushima 1979 1980 wa perhaps first artificial nn deserved attribute deep first incorporate neurophysiological insight section introduced volutional nns today often called cnns convnets typically rectangular receptive field convolutional unit given weight vector filter shifted step step across dimensional array input value pixel image usually several filter resulting array subsequent activation event unit provide input unit due massive weight replication section 2 relatively parameter section may sary describe behavior convolutional layer subsampling downsampling layer consist unit whose connection originate physical neighbor convolutional layer subsampling unit become active least one input active response insensitive certain small image shift compare section neocognitron similar architecture modern purely supervised feedforward deep learner alternating convolutional downsampling layer section fukushima however not set weight supervised backpropagation section local unsupervised learning rule fukushima sense not care dl problem section although architecture wa atively deep indeed downsampling purpose used spatial averaging fukushima 1980 2011 instead mp section currently particularly convenient popular wta mechanism today dl combination cnns mp bp also profit lot later work section beyond development backpropagation bp nns minimization error gradient descent hadamard 1908 parameter space complex nonlinear differentiable leibniz 1684 system ha cussed least since early amari 1967 bryson 1961 bryson denham 1961 bryson ho 1969 director rohrer 1969 dreyfus 1962 kelley 1960 pontryagin skii gamrelidze mishchenko 1961 wilkinson 1965 initially within framework equation calculus variation euler 1744 steepest descent weight space system formed bryson 1961 bryson ho 1969 kelley 1960 ating chain rule leibniz 1676 l hôpital 1696 à la dynamic programming dp bellman 1957 simplified derivation backpropagation method us chain rule only dreyfus 1962 system already efficient dp sense however backpropagated derivative information standard jacobian matrix calculation one layer vious one without explicitly addressing either direct link across several layer potential additional efficiency gain due work sparsity perhaps enhancement seemed obvious author given prior work learning multilayer system see also section deep nonlinear net since schmidhuber neural network 61 2015 91 1965 seems surprising hindsight book minsky pert 1969 limitation simple linear perceptrons single layer section discouraged some researcher ther studying nns explicit efficient error backpropagation bp arbitrary crete possibly sparsely connected network apparently wa first described 1970 master thesis linnainmaa 1970 1976 albeit without reference nns bp also known verse mode automatic differentiation griewank 2012 cost forward activation spreading essentially equal cost backward derivative calculation see early fortran code linnainmaa 1970 closely related work ostrovskii volin borisov 1971 efficient bp wa soon explicitly used minimize cost function adapting control parameter weight dreyfus 1973 pare some preliminary discussion werbos 1974 tion method multilayer threshold nns bobrowski 1978 computer program automatically deriving implementing bp given differentiable system speelpenning 1980 knowledge first application efficient bp wa described 1981 werbos 1981 2006 lated work wa published several year later lecun 1985 1988 parker 1985 paper 1986 significantly contributed ularization bp nns rumelhart hinton williams 1986 experimentally demonstrating emergence useful internal representation hidden layer see generalization processing recurrent nns atiya parlos 2000 baldi 1995 gherrity 1989 kremer kolen 2001 pearlmutter 1989 1995 robinson fallside 1987 rohwer 1989 schmidhuber werbos 1988 williams 1989 williams peng 1990 williams zipser 1988 also equilibrium rnns almeida 1987 pineda 1987 stationary input bp feedforward nns fnns recurrent nns rnns using notation section 2 fnns rnns episode activation spreading tiable ft single iteration gradient descent bp computes change wi proportion rithm additive case weight wi ated variable initialized algorithm one iteration bp fnns rnns 1 compute initialize error signal variable δt 0 xt input event continue next iteration error et δt xt add δt value wv k δk elegant efficient recursive chain rule application collecting impact nett future event multiply δt f nett k add k value xkδt end change wi proportion small learning rate computational cost backward bp pas essentially forward pas section 2 forward backward pass sufficient performance reached 2014 simple bp method still central learning algorithm fnns rnns notably nns 2014 section not augment supervised bp some sort unsupervised learning discussed section late beyond numerous improvement nns late seemed clear bp section wa no panacea fnn application focused fnns hidden layer additional hidden layer often not seem offer empirical benefit many practitioner found solace theorem 1989 hornik stinchcombe white 1989 kolmogorov stating nn single layer enough hidden unit approximate any multivariate continuous function arbitrary accuracy likewise rnn application not require ing error far many researcher helped rnns first ing shallow problem section 3 whose solution generalized deeper problem fact some popular rnn rithms restricted credit assignment single step backwards man 1990 jordan 1986 1997 also recent study jaeger 2001 2004 maass et 2002 generally speaking although bp allows deep problem principle seemed work only shallow problem late early saw idea potential come problem wa fully understood only 1991 tion idea dealing long time lag deep cap deal long time lag relevant event eral sequence processing method proposed including cused bp based decay factor activation unit rnns mozer 1989 1992 neural network tdnns lang waibel hinton 1990 adaptive extension hausen waibel 1991 nonlinear autoregressive exogenous input narx rnns lin horne tino giles 1996 certain archical rnns hihi bengio 1996 compare section 1991 rl economy rnns wta unit local learning rule schmidhuber method bengio simard frasconi 1994 de vries principe 1991 plate 1993 ring 1993 1994 sun chen lee 1993 however algorithm either worked shallow cap only could not generalize unseen cap depth problem greatly varying time lag vant event needed external fine tuning delay constant fered problem fact turned certain simple deep benchmark problem used evaluate method quickly solved randomly guessing rnn weight lution found hochreiter schmidhuber 1996 rnn method designed dl poral sequence neural heat exchanger schmidhuber consists two parallel deep fnns opposite flow direction input pattern enter first fnn propagated sired output target enter opposite fnn gated using local learning rule layer net try similar information content preceding layer adjacent layer net input entering first net slowly heat become target target ing opposite net slowly cool become input helmholtz machine dayan hinton 1996 dayan hinton neal zemel 1995 may viewed unsupervised section variant thereof peter dayan personal communication 1994 hybrid approach shavlik towell 1989 towell shavlik 1994 initializes potentially deep fnn domain theory propositional logic may acquired based learning dejong mooney 1986 minton et 1989 mitchell keller 1986 nn bp section nn depth reflects longest chain 92 schmidhuber neural network 61 2015 reasoning original set logical rule extension approach maclin shavlik 1993 shavlik 1994 initializes rnn domain knowledge expressed finite state automaton fsa ha become important later dl system ul section better bp advanced gradient descent compare tion numerous improvement steepest descent bp section proposed method newton gauss 1809 levenberg 1944 marquardt 1963 newton 1687 schaback werner 1992 method bfgs broyden et 1965 fletcher powell 1963 goldfarb 1970 shanno 1970 computationally expensive large nns partial bfgs battiti 1992 saito nakano 1997 conjugate gradient hestenes stiefel 1952 møller 1993 well method cauwenberghs 1993 schmidhuber solla 1988 provide sometimes useful fast alternative bp treated linear problem bärmann 1993 gradient information passed back preceding layer speed bp momentum wa introduced rumelhart et 1986 constant added slope linearized activation function fahlman 1988 nonlinearity slope wa exaggerated west saad 1995 only sign error derivative taken account successful widely used bp variant riedmiller braun 1993 robust variation igel hüsken 2003 wa also successfully applied rnns local gradient normalized based nn ture schraudolph sejnowski 1996 diagonalized sian approach becker le cun 1989 related efficient method schraudolph 2002 some algorithm controlling bp step size adapt global learning rate battiti 1989 lapedes farber 1986 lecun simard pearlmutter 1993 vogl mangis rigler zink alkon 1988 yu chen cheng 1995 others compute individual learning rate weight jacob 1988 silva almeida 1990 line learning bp applied pattern presentation algorithm neuneier zimmermann 1996 set weight learning rate inversely proportional empirical dard deviation local gradient thus normalizing tic weight fluctuation compare local online step size adaptation method nonlinear nns almeida almeida langlois amaral redol 1997 many additional trick improving nns described montavon orr müller 2012 orr müller 1998 compare section recent development mentioned section searching simple nns section many researcher used method search simple nns section high generalization bility approach address dilemma geman bienenstock doursat 1992 strong prior assumption example weight decay hanson pratt 1989 krogh hertz 1992 weigend rumelhart huberman 1991 encourages zero weight penalizing large weight bayesian work bayes 1763 weight decay derived hinton van camp 1993 gaussian laplacian weight prior gauss 1809 laplace 1774 see also murray edward 1993 tension approach postulate distribution network many similar weight generated gaussian mixture ter priori nowlan hinton 1992 often weight prior implicit additional penalty term mackay 1992 method based validation set craven wahba 1979 eubank 1988 golub heath wahba 1979 hastie tibshirani 1990 mosteller tukey 1968 stone 1974 akaike information criterion final prediction error akaike 1970 1973 1974 generalized prediction error moody 1992 moody utans 1994 see also amari murata 1993 guyon vapnik boser bottou solla 1992 holden 1994 vapnik 1992 wang venkatesh judd 1994 wolpert 1994 similar prior bias towards simplicity implicit constructive pruning algorithm sequential network construction ash 1989 burgess 1994 fahlman 1991 fritzke 1994 gallant 1988 honavar uhr 1988 1993 ivakhnenko 1968 1971 moody 1989 parekh yang honavar 2000 ring 1991 utgoff stracuzzi 2002 weng ahuja huang 1992 see also section input pruning moody 1992 refenes zapranis francis 1994 unit pruning ivakhnenko 1968 1971 levin leen moody 1994 mozer smolensky 1989 white 1989 weight pruning optimal brain damage lecun denker solla 1990 optimal brain surgeon hassibi stork 1993 general not always practical approach ering sl nns rl nns search among weight program written universal programming language bias towards fast short program ber 1997 section flat minimum search fm hochreiter schmidhuber 1999 search flat minimum error function large connected region weight space error low remains approximately constant bit information required describe weight high variance compare perturbation tolerance condition bishop 1993 carter rudolph nucci 1990 hanson 1990 kerlirzin vallet 1993 matsuoka 1992 minai williams 1994 murray edward 1993 neti schneider young 1992 bayesian argument suggests flat minimum correspond simple nns low expected overfitting compare section recent development mentioned section potential benefit ul sl compare section notation section 2 introduced label dt many paper previous millennium however unsupervised learning ul without teacher atick li redlich 1992 baldi hornik 1989 barlow kaushal mitchison 1989 barrow 1987 deco parra 1997 field 1987 földiák 1990 földiák young 1995 grossberg hebb 1949 kohonen 1972 1982 1988 kosko 1990 martinetz ritter schulten 1990 miller 1994 mozer 1991 oja 1989 palm 1992 pearlmutter hinton 1986 ritter kohonen 1989 rubner schulten 1990 sanger 1989 saund 1994 von der malsburg 1973 watanabe 1985 willshaw von der malsburg 1976 see also work 2001 franzius sprekeler wiskott 2007 waydo koch 2008 wiskott sejnowski 2002 many ul method designed maximize boltzmann 1909 kullback leibler 1951 shannon 1948 objective amari cichocki yang 1996 barlow et 1989 dayan zemel 1995 deco parra 1997 field 1994 hinton dayan frey neal 1995 linsker 1988 mackay miller 1990 plumbley 1991 redlich 1993 schmidhuber schraudolph sejnowski 1993 zemel 1993 zemel hinton 1994 many uncover disentangle hidden ing source signal andrade chacon merelo moran 1993 bell sejnowski 1995 belouchrani doso moulines 1997 cardoso 1994 comon 1994 hyvärinen schmidhuber neural network 61 2015 93 karhunen oja 2001 jutten herault 1991 karhunen sensalo 1995 molgedey schuster 1994 schuster 1992 cottrell 2014 zhang cottrell 2007 szabó póczos lőrincz 2006 many ul method automatically robustly generate tributed sparse representation input pattern falconbridge stamp badcock 2006 földiák 1990 hinton ghahramani 1997 hochreiter schmidhuber 1999 hyvärinen hoyer oja 1999 lewicki olshausen 1998 feature tectors olshausen field 1996 schmidhuber eldracher foltin 1996 structure well orientation sensitive edge detector gabor filter gabor 1946 extract simple feature related observed early visual stage biological system de valois albrecht thorell 1982 jones palmer 1987 ul also serve extract invariant feature different data item becker 1991 coupled nns observing two different input schmidhuber prelinger 1992 also called siamese nns bromley et 1993 chen salman 2011 hadsell chopra lecun 2006 taylor spiro bregler fergus 2011 ul help encode input data form advantageous processing context dl one important goal ul redundancy reduction ideally given ensemble input pattern redundancy reduction deep nn create factorial code code statistically independent component ensemble barlow 1989 barlow et 1989 disentangle unknown factor variation compare bengio et 2013 code may sparse advantageous 1 data compression 2 speeding subsequent bp becker 1991 3 trivializing task subsequent naive yet optimal bayes classifier schmidhuber et 1996 early ul fnns single layer method deeper ul fnns include hierarchical section kohonen map dittenbach merkl rauber 2000 koikkalainen oja 1990 lampinen oja 1992 rauber merkl dittenbach 2002 versino gambardella 1996 hierarchical gaussian potential function network lee kil 1991 ul feature hierarchy fed sl classifier behnke 1999 organizing tree algorithm sota herrero valencia dopazo 2001 nonlinear autoencoders aes 3 5 layer demers cottrell 1993 kramer 1991 oja 1991 ae nns rumelhart et 1986 trained map input pattern example compactly encoding activation unit narrow bottleneck hidden layer certain nonlinear aes suffer certain limitation baldi 2012 lococode hochreiter schmidhuber 1999 us fm tion find aes weight describable bit information often producing sparse factorial code predictability minimization pm schmidhuber search factorial code nonlinear feature tectors fight nonlinear predictor trying become informative unpredictable possible ul wa applied not only fnns also rnns lindstädt 1993 schmidhuber compare section rnn stack 1991 well later ul rnns schraudolph schmidhuber 2001 steil 2007 1987 ul autoencoder ae hierarchy compare section perhaps first work study potential benefit wa published proposed unsupervised ae hierarchy ballard 1987 closely related certain feedforward deep learner based ul section level ae nn single hidden layer trained map input pattern hidden layer code fed ae type hope code hidden ae layer property facilitate subsequent learning one experiment particular learning algorithm different traditional bp section wa used learn mapping ae stack type ul ballard 1987 wa faster learning equivalent mapping bp single deeper ae without hand task not really require deep ae benefit ul not obvious experiment compare early survey hinton 1989 somewhat related recursive memory raam melnik levy pollack 2000 pollack 1988 1990 originally used encode sequential linguistic structure arbitrary size fixed number hidden unit recently raams also used unsupervised facilitate deep credit assignment rl gisslen luciw graziano schmidhuber 2011 section principle many ul method section could stacked like aes rnns section restricted boltzmann machine rbms section hierarchical kohonen net section facilitate subsequent sl compare stacked generalization ting witten 1997 wolpert 1992 fnns profit competitive ul rumelhart zipser 1986 prior maclin shavlik 1995 see also recent method using ul improve subsequent sl behnke 1999 wiskott 2013 1989 bp convolutional nns cnns section 1989 backpropagation section wa applied lecun et 1989 lecun boser et 1990 lecun bottou bengio haffner 1998 tional neural layer section adaptive connection combination augmented mp section sped graphic card section ha become sential ingredient many modern ward visual deep learner section work also introduced mnist data set handwritten digit lecun et 1989 time ha become perhaps famous benchmark machine learning cnns helped achieve good formance mnist lecun boser et 1990 cap depth 5 fingerprint recognition baldi chauvin 1993 similar cnns used commercially 1991 fundamental deep learning problem gradient descent diploma thesis hochreiter 1991 represented milestone explicit dl research mentioned section late experiment indicated traditional deep feedforward recurrent network hard train backpropagation bp section hochreiter work formally identified major reason typical deep nns suffer famous problem vanishing exploding gradient standard activation function section 1 cumulative backpropagated error signal section either shrink rapidly grow bound fact decay exponentially number layer cap depth section 3 explode also known long time lag problem much subsequent dl research wa motivated insight later work bengio et 1994 also studied basin attraction stability noise dynamical system point view either dynamic not robust noise gradient vanish see also hochreiter bengio frasconi schmidhuber 2001 tiňo hammer 2004 year several way partially overcoming fundamental deep learning problem explored 94 schmidhuber neural network 61 2015 deep learner 1991 history compressor section alleviates problem unsupervised hierarchy rnns greatly facilitates sequent supervised credit assignment bp section fnn case similar effect achieved ceptually related ae stack section deep belief network dbns section ii network section viate problem special architecture unaffected iii today computer million time putational power desktop machine early allows propagating error layer within reasonable time even traditional nns section basically winning many image recognition petition section although doe not really overcome problem fundamental way iv optimization section alleviate problem fnns marten 2010 møller 1993 pearlmutter 1994 schraudolph 2002 section rnns marten sutskever 2011 section space nn weight matrix also searched without relying error gradient thus avoiding fundamental deep learning problem altogether random weight guessing time work better sophisticated method iter schmidhuber 1996 certain complex problem better solved using universal search levin weight program written universal gramming language schmidhuber 1997 some better solved using linear method obtain optimal weight connection output event section 2 evolving weight connection called evolino huber wierstra gagliolo gomez 2007 compare also lated rnns certain ul rule steil 2007 also case spiking neuron klampfl maass 2013 yin meng jin 2012 section direct search method relevant not only sl also general rl discussed detail section 1991 history compression deep stack rnns working deep learner section 3 1991 schmidhuber could perform credit assignment across hundred nonlinear operator neural layer using unsupervised training hierarchy rnns basic idea still relevant today rnn trained unsupervised fashion predict next input martin atlas 1994 dorffner 1996 only expected input error convey new information get fed next higher rnn thus tick slower time scale easily shown no information get lost get compressed much machine learning essentially compression section ual input sequence get series le le redundant coding deeper deeper level history compressor neural sequence chunker compress data space like feedforward nns time another good example hierarchical representation learning section also continuous variant history compressor schmidhuber mozer prelinger 1993 rnn stack essentially deep generative model data reconstructed compressed form adding another rnn stack improves bound data tion length equivalent negative logarithm ity huffman 1952 shannon 1948 long remaining local learnable predictability data representation responding level hierarchy compare similar observation feedforward deep belief network dbns 2006 section system wa able learn many previously unlearnable dl task one ancient illustrative dl experiment schmidhuber required cap section 3 depth top level code initially unsupervised rnn stack however got compact previously infeasible sequence classification tional sl became possible essentially system used ul greatly reduce problem depth compare earlier tuning nns initialized rule propositional logic shavlik towell 1989 section way compressing higher level lower level thus fully partially collapsing rnn stack trick retrain rnn continually imitate predict hidden unit already trained slower rnn conscious chunker additional predictive output ron schmidhuber help lower rnn atizer develop appropriate rarely changing memory may bridge long time lag procedure greatly reduce required depth bp process 1991 system wa working deep learner modern sense also first neural hierarchical temporal ory htm conceptually similar earlier ae hierarchy 1987 section later deep belief network 2006 section general sense us rnns instead fnns unchanging input recently known entrepreneur hawkins george 2006 kurzweil 2012 also got interested htms compare also hierarchical hmms fine singer tishby 1998 well later rent system klampfl maass 2013 et 2001 steil 2007 young davis mishtal arel 2014 clockwork rnns koutník greff gomez schmidhuber 2014 also consist teracting rnn module different clock rate not use ul set rate stack rnns used later work sl great success section 1992 mp towards mpcnns compare section neocognitron section inspired cresceptron weng et 1992 adapts topology training tion compare incrementally growing shrinking gmdh network 1965 section instead using alternative local subsampling wta od fukushima 1980 maass 2000 schmidhuber cresceptron us mp layer dimensional layer array unit activation partitioned smaller rectangular array replaced downsampling layer activation maximally active unit later complex version cresceptron weng ahuja huang 1997 also included blurring layer improve object location ance neurophysiologically plausible topology feedforward hmax model riesenhuber poggio 1999 similar one 1992 cresceptron thus 1979 neocognitron hmax doe not learn though unit weight biologically plausible learning rule later proposed similar model serre riesenhuber louie poggio 2002 teichmann wiltschut hamker 2012 cnns convnets section combined mp become mpcnns alternating convolutional layer unlike cresceptron hmax however mpcnns trained bp section ranzato huang boureau lecun 2007 advantage pointed subsequently scherer müller behnke 2010 mpcnns become central many modern feedforward visual deep learner section schmidhuber neural network 61 2015 95 1994 early nns back certain nns already certain controlled pattern recognition contest secret test set notably nn internal delay line santa fe competition chaotic intensity pulsation laser wan 1994 weigend gershenfeld 1993 no deep cap section 3 needed though 1995 supervised recurrent deep learner lstm rnn supervised long memory lstm rnns gers schmidhuber cummins 2000 hochreiter schmidhuber gers eck schmidhuber 2003 could eventually perform similar feat deep rnn hierarchy 1991 section overcoming fundamental deep learning problem section without any unsupervised lstm could also learn dl task without local sequence predictability thus unlearnable partially unsupervised 1991 history compressor section dealing deep problem section 3 gers schraudolph schmidhuber 2002 basic lstm idea simple some unit called constant error carousel cecs cec us activation function f identity function ha connection fixed weight due f constant derivative rors backpropagated cec not vanish explode tion stay unless flow cec typically adaptive part nn cecs connected several nonlinear adaptive unit some multiplicative vation function needed learning nonlinear behavior weight change unit often profit error signal propagated far back time cecs cecs main reason lstm net learn discover importance memorize event happened thousand discrete time step ago previous rnns already failed case minimal time lag 10 step many different lstm variant topology allowed possible evolve good topology bayer wierstra togelius schmidhuber 2009 some lstm variant also use modifiable cecs gers schmidhuber 2001 certain extent lstm biologically plausible reilly 2003 lstm learned solve many previously unlearnable dl task involving recognition temporal order widely separated event noisy input stream robust storage precision real number across extended time interval arithmetic operation continuous input stream extraction information conveyed temporal distance event recognition temporally extended pattern noisy input sequence gers et 2000 hochreiter schmidhuber stable generation precisely timed rhythm well smooth periodic trajectory gers schmidhuber 2000 lstm clearly outperformed previous rnns task require learning rule regular language describable deterministic finite state automaton fsas blair pollack 1997 casey 1996 kalinke lehmann 1998 manolios fanelli 1994 omlin giles 1996 siegelmann 1992 vahed omlin 2004 watrous kuhn 1992 zeng goodman smyth 1994 term reliability speed lstm also worked task involving context free language cfls not represented hmms similar fsas discussed rnn literature andrew diederich tickle 1995 rodriguez wile 1998 rodriguez wile elman 1999 steijvers grunwald 1996 sun giles chen lee 1993 tonkes wile 1997 wile elman 1995 cfl recognition lee 1996 requires functional equivalent runtime stack some previous rnns failed learn small cfl training set rodriguez wile 1998 not bodén wile 2000 rodriguez et 1999 failed extract general rule not generalize well substantially larger test set similar language csls chalup blair 2003 lstm generalized well though requiring only 30 shortest exemplar n csl anbncn correctly predict possible continuation sequence prefix n 1000 combination decoupled extended kalman filter feldkamp prokhorov eagen yuan 1998 feldkamp prokhorov feldkamp 2003 haykin 2001 kalman 1960 puskorius feldkamp 1994 williams lstm rnn et 2003 learned deal correctly value n 10 million training network wa able read sequence symbol one symbol time finally detect subtle difference legal string similar illegal string compare also recent rnn algorithm able deal long time lag koutník et 2014 marten sutskever 2011 schäfer udluft zimmermann 2006 zimmermann tietz grothmann 2012 rnns brnns schuster 1999 schuster paliwal 1997 designed input sequence whose start end known advance spoken sentence labeled phoneme compare fukada schuster sagisaka 1999 take past future context sequence element account one rnn process sequence start end backwards end start time step combined output predict corresponding label any brnns successfully applied secondary protein structure prediction baldi brunak frasconi pollastri soda 1999 baldi pollastri 2003 wu baldi 2008 generalize brnns multiple dimension learned predict property small organic molecule lusci pollastri baldi 2013 well protein contact map tegge wang eickholt cheng 2009 also conjunction growing deep fnn di lena nagata baldi 2012 section brnns unfold full potential combined lstm concept graf et 2009 graf schmidhuber 2005 2009 particularly successful recent competition stack tion lstm rnns fernandez graf schmidhuber graf schmidhuber 2009 trained connectionist poral classification ctc graf fernandez gomez ber 2006 method finding rnn weight maximize probability label sequence given typically much longer stream input vector performs simultaneous mentation alignment recognition section early speech recognition wa dominated hmms combined fnns bourlard morgan 1994 nevertheless trained scratch utterance tidigits speech database 2003 lstm already obtained result comparable system beringer graf schiel schmidhuber 2005 graf eck beringer schmidhuber 2003 graf et 2006 2007 lstm outperformed hmms keyword spotting task fernández graf schmidhuber compare recent improvement indermuhle frinken fischer bunke 2011 wöllmer schuller rigoll 2013 2013 lstm also achieved best known result famous timit phoneme recognition benchmark graf mohamed hinton 2013 section recently lstm hybrid obtained best known performance geiger zhang weninger schuller rigoll 2014 speech recognition sak senior beaufays 2014 lstm also applicable robot localization förster graf schmidhuber 2007 robot control mayer et 2008 line driver distraction detection wöllmer et 2011 many 96 schmidhuber neural network 61 2015 task example helped improve state art diverse application protein analysis hochreiter obermayer 2005 handwriting recognition bluche et 2014 graf fernandez liwicki bunke schmidhuber 2008 graf et 2009 graf schmidhuber 2009 voice activity tion eyben weninger squartini schuller 2013 optical acter recognition breuel shafait 2013 language identification sak moreno 2014 prosody contour prediction fernandez rendel ramabhadran hoory 2014 audio onset tection marchi et 2014 synthesis fan qian xie soong 2014 social signal classification brueckner ter 2014 machine translation sutskever vinyals le 2014 others rnns also used metalearning prokhorov feldkamp tyukin 2002 schaul schmidhuber 2010 schmidhuber 1987 principle learn run weight change algorithm schmidhuber successful metalearner hochreiter younger conwell 2001 used lstm rnn quickly learn learning algorithm quadratic function compare section recently lstm rnns several international pattern nition competition set numerous benchmark record large complex data set section based lstm no panacea method sometimes performed least certain task jaeger 2004 koutník et 2014 marten sutskever 2011 pascanu mikolov bengio 2013 schmidhuber et 2007 compare section 2003 nns successful deep nns decade around 2000 many practical commercial tern recognition application dominated chine learning method support vector machine svms schölkopf et 1998 vapnik 1995 nevertheless least tain domain nns outperformed technique bayes nn neal 2006 based ensemble breiman 1996 dietterich hashem schmeiser 1992 schapire 1990 ueda 2000 wolpert 1992 nns nip 2003 feature selection challenge secret test set neal zhang 2006 nn wa not deep two hidden layer thus rather shallow cap section 3 depth important many present pattern recognizers section development cnn department lecun et 1989 cnn section set new mnist record simard steinkraus platt 2003 using training pattern deformation baird 1990 no unsupervised section standard bp net achieved simard et 2003 corresponding cap depth wa low compare improvement section good image interpretation result behnke achieved rather deep nns trained bp variant riedmiller braun 1993 section feedback recurrent connection helped improve image interpretation fnns cap depth 6 used successfully classify data vieira barradas 2003 deep lstm rnns started obtain certain first speech tion result comparable system graf et 2003 compare section ul deep belief stack bp learning network numerous layer date back least 1965 section explicit dl research result published least since 1991 section expression deep learning wa actually coined around 2006 unsupervised deep fnns helped accelerate subsequent sl bp hinton osindero teh 2006 hinton salakhutdinov 2006 compare earlier terminology loading deep network síma 1994 windisch 2005 learning deep memory gomez schmidhuber 2005 compare also section section not deep fnns trained competitive ul maclin shavlik 1995 deep belief network dbn stack restricted boltzmann machine rbms smolensky 1986 turn boltzmann machine bm hinton sejnowski 1986 single layer unit compare also bm memisevic hinton 2010 rbm perceives pattern representation level learns encode unsupervised fashion least theory certain assumption adding layer improves bound data negative log probability hinton et 2006 equivalent data description corresponding observation rnn stack section extension temporal rbms sutskever hinton taylor 2008 without any training pattern deformation section dbn bp achieved error rate hinton salakhutdinov 2006 mnist handwritten digit section result helped arouse interest dbns dbns also achieved good result phoneme recognition error rate timit core test set mohamed hinton 2010 compare improvement fnns deng yu 2014 hinton deng et 2012 lstm rnns section technique called semantic hashing salakhutdinov hinton 2009 map semantically similar document variable size nearby address space document representation outperformed previous searcher similar document locality sensitive hashing buhler 2001 datar immorlica indyk mirrokni 2004 see tutorial fischer igel 2014 autoencoder ae stack ballard 1987 section became popular alternative way deep fnns unsupervised fashion section bp section bengio lamblin popovici larochelle 2007 erhan et 2010 vincent hugo bengio manzagol 2008 sparse coding section wa formulated combination convex optimization problem lee battle raina ng 2007 recent survey stacked rbm ae method focus development arel rose karnowski 2010 bengio 2009 unsupervised dbns ae stack conceptually similar certain sense le general unsupervised rnn history compressor 1991 section process not only stationary input pattern entire pattern sequence improved stack also 2006 lecun et 1989 cnn section set new mnist record ranzato poultney chopra lecun 2006 using training pattern deformation section no unsupervised compare improvement section similar cnns used obstacle avoidance lecun muller cosatto flepp 2006 combination cnns tdnns later learned map representation sentence feature relevant language processing using combination sl ul collobert weston 2008 2006 also saw early cnn implementation chellapilla puri simard 2006 4 time faster cnns compare also earlier gpu implementation standard fnns reported factor 20 oh jung 2004 gpus schmidhuber neural network 61 2015 97 graphic card become important dl subsequent year section 2007 bp section wa applied first time ranzato et 2007 section like mpcnns section alternating convolutional layer mpcnns become essential ingredient many modern winning feedforward visual deep learner section also 2007 hierarchical stack lstm rnns introduced fernandez et trained hierarchical connectionist temporal classification ctc graf et 2006 task sequence labeling every lstm rnn level section predicts sequence label fed next level error signal every level lower level spoken digit recognition lstm stack outperformed hmms despite making fewer assumption domain lstm stack not necessarily require unsupervised like earlier rnn stack schmidhuber section 2009 first official competition rnns mpcnns stack lstm rnns trained ctc section came first rnns win official international pattern tion contest secret test set known only organizer precisely three connected handwriting competition dar 2009 three different language french arab farsi deep lstm rnns without any priori linguistic knowledge performing simultaneous segmentation recognition compare graf jaitly 2014 graf schmidhuber 2005 graf et al 2009 graf et al 2013 schmidhuber ciresan meier masci graf 2011 section detect human action surveillance video cnn jain seung 2009 prokhorov 2010 combined svms wa part larger system yang et 2009 using bag feature approach nowak jurie triggs 2006 extract region interest system three 2009 trecvid competition possibly first official international contest help mp cnns section improved version method wa published later ji xu yang yu 2013 2009 also saw implementation raina madhavan ng 2009 order magnitude faster previous see section see also coates et al 2013 convolutional dbn lee grosse ranganath ng 2009 probabilistic variant mp section combine idea cnns dbns wa successfully applied audio classification lee pham largman ng 2009 2010 plain backprop distortion gpu break mnist record 2010 new mnist section record error rate wa set good old bp section deep otherwise standard nns ciresan meier gambardella schmidhuber 2010 using neither unsupervised section convolution section however training pattern deformation section important generate big training set avoid overfitting success wa made possible mainly gpu implementation bp wa 50 time faster standard cpu version good value wa obtained without distortion except small saccadic eye section since bp wa decade old section pattern deformation 2 decade baird 1990 section result seemed suggest advance exploiting modern computing hardware important advance algorithm 2011 mpcnns gpu achieve superhuman vision performance 2011 flexible ciresan meier masci gambardella schmidhuber 2011 mp cnns convnets wa described building earlier mp work weng et 1992 section cnns fukushima 1979 lecun et 1989 section early based cnns without mp chellapilla et 2006 section compare early oh jung 2004 raina et 2009 section mpcnns alternating convolutional layer section layer mp section topped standard fully connected layer weight trained bp section ranzato et 2007 scherer et 2010 become essential many winning fnns section ciresan meier masci huber 2011 committee breiman 1996 dietterich hashem schmeiser 1992 schapire 1990 ueda 2000 wolpert 1992 simple democratic output averaging several mpcnns see input output vector used assign probability various possible class class average highest probability chosen system fication present input compare earlier sophisticated ensemble method schapire 1990 ble neal 2006 section recent related work shao wu li 2014 ensemble wa first system achieve superhuman visual pattern recognition ciresan meier masci schmidhuber 2011 ciresan meier masci schmidhuber 2012 controlled competition namely ijcnn 2011 traffic sign recognition contest san jose ca stallkamp schlipsing salmen igel 2011 2012 interest fully autonomous driving car traffic dickmanns et 1994 mpcnn ensemble obtained error rate wa twice better human test subject three time better closest artificial nn competitor sermanet lecun 2011 six time better best method month earlier qualifying round wa stage online competition albeit much smaller margin ciresan meier masci schmidhuber 2011 v second place sermanet lecun 2011 deadline organizer revealed human performance test set wa best method already seemed however qualifying wa possible incrementally gain mation test set probing repeated sion illustrated better better result obtained various team time stallkamp et 2012 organizer eventually imposed limit 10 resubmissions final petition wa not possible illustrates general problem benchmark whose test set public least probed some extent competing team tend overfit test set even not directly used training only evaluation 1997 many thought big deal human chess world champion kasparov wa beaten ibm computer back computer could not compete little kid visual pattern recognition seems much harder chess computational perspective course traffic sign domain highly restricted kid still much better general pattern recognizers nevertheless 2011 deep nns could already learn rival important limited visual domain ensemble wa also first method achieve performance around mnist ciresan meier schmidhuber represented dramatic improvement since mnist record hovered around almost decade section 98 schmidhuber neural network 61 2015 given prior work mp cnns section section not breakthrough scientific sense commercially relevant breakthrough efficient coding ha made difference several contest since today feedforward deep nns ensemble section 2011 optimization rnns also 2011 wa shown marten sutskever 2011 optimization møller 1993 pearlmutter 1994 schraudolph 2002 section alleviate fundamental deep learning problem section rnns outperforming standard lstm rnns section several task compare rnn algorithm jaeger 2004 koutník et 2014 pascanu mikolov et 2013 schmidhuber et 2007 also least sometimes yield better result steepest descent lstm rnns 2012 first contest imagenet object detection segmentation 2012 ensemble section achieved best result imagenet classification benchmark krizhevsky sutskever hinton 2012 popular computer vision community relatively large image size 256 256 pixel necessary opposed only 48 48 pixel 2011 traffic sign competition section see improvement section also 2012 biggest nn far 109 free parameter wa trained unsupervised mode section unlabeled data le et 2012 applied imagenet code across top layer used train simple supervised classifier achieved best result far class instead relying efficient gpu programming wa done brute force 1000 standard machine core excellent result achieved deep learner image recognition classification section computer vision community however especially interested object detection large image application search engine biomedical diagnosis goal may automatically detect tumor etc image human tissue object detection present additional challenge one natural approach train deep nn classifier patch big image use feature detector shifted across unknown visual scene using various rotation zoom factor image part yield highly active output unit likely contain object similar nn wa trained 2012 finally saw first dl system ensemble mpcnns section win contest visual object detection ciresan giusti gambardella schmidhuber 2013 large image several million pixel icpr 2012 roux et 2013 biomedical application may turn among important application dl world spends 10 gdp healthcare 6 trillion usd per year much medical diagnosis expensive expert partial automation could not only save lot money also make expert diagnostics accessible many currently not afford gratifying observe today deep nns may actually help improve healthcare perhaps save human life 2012 also saw first pure image segmentation contest dl ciresan giusti gambardella schmidhuber 2012 ensemble segmentation neuronal structure em stack challenge 2012 em stack relevant recently approved huge brain project europe u markram 2012 given electron microscopy image stack thin slice animal brain goal build detailed model brain neuron dendrite human expert need many hour day week annotate image part depict neuronal membrane part irrelevant background need automated turaga et 2010 deep learned solve task experience many training image contest three evaluation metric large margin superhuman performance term pixel error object detection ciresan et 2013 image tation ciresan giusti et 2012 profit fast image scan avoid redundant computation recent mpcnn scanner speed naive implementation three order magnitude giusti ciresan masci gambardella ber 2013 masci giusti ciresan fricout schmidhuber 2013 compare earlier efficient method cnns without mp vaillant monrocq lecun 1994 also 2012 system consisting growing deep fnns di lena et 2012 casp 2012 contest protein contact map prediction benchmark lstm rnns section outperformed method hmms svms online mode detection indermuhle frinken bunke 2012 otte krechel liwicki dengel 2012 keyword spotting indermuhle et 2011 long time lag problem language modeling lstm rnns outperformed statistical approach benchmark frinken et 2012 improved result later obtained combination nns hmms et 2014 compare earlier rnns object recognition iterative image interpretation behnke 2002 behnke rojas 1998 see also recent publication reilly wyatte herd mingus jilk 2013 wyatte curran reilly 2012 extending work biologically plausible learning rule rnns reilly 1996 contest benchmark record stack fernandez et graf schmidhuber 2009 section lstm rnns graf schmidhuber 2005 trained ctc section broke famous timit speech phoneme recognition record achieving test set error rate graf et 2013 despite thousand man year previously spent hidden markov model hmms speech recognition research compare earlier dbn result section also helped score first nist evaluation bluche et 2014 optical character recognition ocr lstm rnns outperformed commercial recognizers historical data breuel et 2013 system also set benchmark record language identification dominguez et 2014 speech recognition geiger et 2014 prosody contour prediction fernandez et 2014 audio onset detection marchi et 2014 synthesis fan et 2014 social signal classification brueckner schulter 2014 lstm rnn wa used estimate state posterior hmm system beat previous state art large vocabulary speech recognition sak senior et 2014 sak vinyals et 2014 another lstm rnn hundred million 2 mentioned however lstm rnns already performed simultaneous segmentation recognition became first recurrent deep learner win official international pattern recognition section schmidhuber neural network 61 2015 99 connection wa used rerank hypothesis statistical machine translation system system beat previous state art english french translation sutskever et 2014 new record icdar chinese handwriting recognition benchmark 3700 class wa set desktop machine ensemble section almost human performance ciresan schmidhuber 2013 compare yin wang zhang liu 2013 miccai 2013 grand challenge mitosis detection veta viergever pluim stathonikos van diest 2013 also wa ensemble ciresan et 2013 data set wa even larger challenging one icpr 2012 section data set including many ambiguous case frequently encountered problem imperfect slide staining three instead mp section observing three orthogonal projection image outperformed traditional full method task segmenting tibial cartilage low field knee mri scan prasoon et 2013 deep section also helped achieve new best result important benchmark computer vision community imagenet classification szegedy et 2014 zeiler fergus 2013 conjunction traditional approach pascal object detection girshick donahue darrell malik 2013 also learned predict bounding box coordinate object imagenet 2013 database obtained result task localization detection sermanet et 2013 also helped recognize number google street view image goodfellow bulatov ibarz arnoud shet 2014 part nn wa trained count visible digit compare earlier work detecting numerosity dbns stoianov zorzi 2012 system also excelled recognizing distorted synthetic text recaptcha puzzle successful cnn application include scene parsing farabet couprie najman lecun 2013 object detection szegedy toshev erhan 2013 shadow detection khan bennamoun sohel togneri 2014 video classification karpathy et 2014 alzheimer disease neuroimaging li et 2014 additional contest mentioned web page swiss ai lab idsia university toronto ny university university montreal currently successful technique lstm rnns benchmark deep learner actually use one two supervised technique recurrent lstm 1997 trained ctc 2006 section b feedforward 2011 section based cnns 1979 section mp 1992 section trained bp section exception include two 2011 contest goodfellow courville bengio 2011 2012 mesnil et 2011 specialized transfer learning one data set another caruana 1997 pan yang 2010 schmidhuber 2004 however deep allow pure transfer ciresan meier schmidhuber one training set greatly improves performance quite different set also recent study donahue et 2013 oquab bottou laptev sivic 2013 fact deep mpcnns sl extract useful feature quite diverse image yielding better result traditional widely used feature sift lowe 1999 2004 many vision task razavian azizpour sullivan carlsson 2014 deal changing data set slowly learning deep nns also combined rapidly adapting surface nns kak chen wang 2010 remarkably trend went partially vised rnn stack section purely supervised lstm rnns section like trend went partially supervised fnn stack section purely supervised mpcnns section nevertheless many application still advantageous combine best learning unsupervised section recent trick improving sl deep nns compare section dbn training section improved ent enhancement automatic learning rate adjustment stochastic gradient descent cho 2014 cho raiko ilin 2013 tikhonov arsenin john 1977 ularization rbms cho ilin raiko 2012 contractive aes fai vincent muller glorot bengio 2011 discourage hidden unit perturbation response input perturbation similar fm section lococode aes section discourages output perturbation response weight perturbation hierarchical cnns neural abstraction pyramid behnke 2005 trained reconstruct image corrupted structured noise behnke 2001 thus enforcing increasingly abstract image representation deeper deeper layer denoising aes later used similar procedure vincent et 2008 dropout ba frey 2013 hinton srivastava krizhevsky sutskever salakhutdinov 2012 remove unit nns training improve generalization some view ensemble method train multiple data model simultaneously baldi sadowski 2014 certain circumstance could also viewed form training set augmentation effectively informative complex feature removed training data compare dropout rnns pachitariu sahani 2013 pascanu gulcehre cho bengio 2013 pham kermorvant louradour 2013 deterministic approximation coined fast dropout wang manning 2013 lead faster learning evaluation wa adapted rnns bayer osendorfer chen urban van der smagt 2013 dropout closely related older biologically plausible technique adding noise neuron synapsis training 1996 hanson 1990 jim giles horne 1995 murray edward 1993 nadal parga 1994 schuster 1992 turn closely related finding nns fm section stochastic variational method graf 2011 also related fm useful rnns classic regularizers weight decay section represent bias towards limited memory capacity pascanu mikolov et 2013 compare recent work variational recurrent aes bayer osendorfer 2014 activation function f rectified linear unit relus f x x x 0 f x 0 old concept rectified unit malik perona 1990 relu nns useful rbms maas hannun ng 2013 nair hinton 2010 outperformed sigmoidal activation function deep nns glorot bordes bengio 2011 helped obtain best result several benchmark problem across multiple domain dahl sainath hinton 2013 krizhevsky et 2012 nns competing linear unit tend outperform nonlinear unit avoid catastrophic forgetting bp training set change time srivastava masci kazerounian gomez schmidhuber 2013 context choosing learning algorithm may important choosing activation function goodfellow mirza da courville bengio 2014 maxout nns goodfellow mirza courville bengio 2013 combine competitive interaction dropout see achieve excellent result certain 100 schmidhuber neural network 61 2015 benchmark compare early rnns competing unit sl rl schmidhuber address overfitting instead depending regularizers bishop 2006 hertz krogh palmer 1991 rnns slim nns competing unit schmidhuber 2012 principle learn select runtime number effective free parameter thus learning computable regularizers section becoming fast slim necessary one may penalize total length connection clune mouret lipson 2013 legenstein maass 2002 schmidhuber 2012 communication cost slim nns implemented hardware expected future rmsprop schaul zhang lecun 2013 tieleman hinton 2012 speed first order gradient descent method section compare neuneier zimmermann 1996 adagrad duchi hazan singer 2011 adadelta zeiler 2012 dl nns also improved transforming hidden unit activation zero output slope average raiko valpola lecun 2012 many additional older trick section also applicable today deep nns compare montavon et 2012 orr müller 1998 consequence neuroscience ironic artificial nns anns help better stand biological nns bnns isbi 2012 result mentioned section ciresan giusti et 2012 segmentation ronal structure em stack challenge 2012 feature detector learned visual anns similar found early visual processing stage bnns section likewise feature detector learned deep layer visual anns highly predictive neuroscientist find deep layer bnns visual cortex bnns may use quite different learning algorithm objective function minimized may quite similar one visual anns fact result obtained relatively deep artificial dbns lee ekanadham ng 2007 cnns yamins hong cadieu dicarlo 2013 seem compatible insight visual pathway primate cerebral cortex ha studied many decade bichot rossi desimone 2005 connor brincat pasupathy 2007 desimone albright gross bruce 1984 dicarlo zoccolan rust 2012 felleman van essen 1991 hubel wiesel 1968 hung kreiman poggio dicarlo 2005 kobatake tanaka 1994 kriegeskorte et 2008 lennie movshon 2005 logothetis paul poggio 1995 perrett hietanen oram benson roll 1992 perrett roll caan 1982 compare computer survey kruger et 2013 dl spiking neuron many recent dl result profit traditional deep nns section current gpus however little oven much hungrier energy biological brain whose neuron efficiently communicate brief spike fitzhugh 1961 hodgkin huxley 1952 nagumo arimoto yoshizawa 1962 often remain quiet many computational model spiking neuron proposed analyzed amit brunel 1997 bohte kok la poutre 2002 brea senn pfister 2013 brette et 2007 brunel 2000 deco roll 2005 gerstner kistler 2002 gerstner van hemmen 1992 hoerzer legenstein maass 2014 izhikevich et 2003 kasabov 2014 kempter gerstner van hemmen 1999 kistler gerstner van hemmen 1997 maass 1996 1997 maex orban 1996 nessler pfeiffer buesing maass 2013 rezende gerstner 2014 seung 2003 song miller abbott 2000 stemmler 1996 stoop schindler bunimovich 2000 tsodyks pawelzik markram 1998 tsodyks skaggs sejnowski mcnaughton 1996 zipser kehoe littlewort fuster 1993 future hardware dl nns may implement aspect model fieres schemmel meier 2008 glackin mcginnity maguire wu belatreche 2005 indiveri et 2011 jin et 2010 khan et 2008 liu et 2001 merolla et 2014 neil liu 2014 roggen hofmann thoma floreano 2003 schemmel grubl meier mueller 2006 gotarredona et 2009 simulated spiking variant neftci da pedroni cauwenberghs 2014 rbm section wa trained variant contrastive divergence algorithm hinton 2002 spiking net evolved achieve reasonable performance small face recognition data set wysoski benuskova kasabov 2010 control simple robot floreano mattiussi 2001 hagras colley callaghan clarke 2004 spiking dbn neuron part larger nn eliasmith 2013 eliasmith et 2012 achieved 6 error rate mnist compare similar result spiking dbn variant depth 3 using neuromorphic sensor connor neil liu delbruck pfeiffer 2013 practical application however current artificial network spiking neuron not yet compete best traditional deep nns compare mnist result section dl fnns rnns reinforcement learning rl far focused deep learning dl supervised unsupervised nns nns learn classify pattern pattern sequence not learn act general sense reinforcement learning rl unknown environment see survey kaelbling et 1996 sutton barto 1998 wiering van otterlo 2012 add discussion dl fnns rnns rl shorter discussion fnns rnns sl ul section 5 reflecting current size various field without teacher solely occasional pain pleasure signal rl agent must discover interact dynamic initially unknown environment maximize pected cumulative reward signal section 2 may bitrary priori unknown delay action perceivable consequence problem hard any problem computer science since any task computable description mulated rl framework hutter 2005 example answer famous question whether p np cook 1971 levin would also set limit achievable eral rl compare specific limitation blondel li 2000 madani hank condon 2003 vlassis littman barber 2012 following subsection mostly focus tain obvious intersection dl not serve general rl survey rl nn world model yield rnns deep cap special case rl fnn controller c interacting deterministic predictable environment separate fnn called learn become c world model system identification predicting c input previous action input chocki unbehauen 1993 ge hang lee zhang 2010 gomi kawato 1993 jordan 1988 jordan rumelhart 1990 levin narendra 1995 ljung 1998 miller werbos sutton 1995 munro 1987 narendra parthasarathy 1990 prokhorov rius feldkamp 2001 robinson fallside 1989 schmidhuber werbos 1981 1987 1992 assume ha schmidhuber neural network 61 2015 101 learned produce accurate prediction use tute environment c form rnn put become input c whose output action turn become input bp rnns section used achieve desired input event high reward signal weight remain fixed gradient information c weight propagated back c back etc certain extent approach also applicable probabilistic uncertain environment long inner product based gradient estimate true gradient tend itive general approach implies deep cap c unlike traditional rl section decade ago method wa used learn back model truck nguyen widrow 1989 rl active vision system used learn sequential shift saccade fovea detect target visual scene schmidhuber huber 1991 thus learning control selective attention compare attention learning without nns whitehead 1992 allow memory previous event partially observable world section general variant technique us rnns instead fnns implement c feldkamp puskorius 1998 schmidhuber may cause deep cap not only c also also used optimize expected reward planning future action sequence schmidhuber fact winner 2004 robocup world championship fast league egorova et 2004 trained nns predict effect steering signal fast robot 4 motor 4 different wheel play nn model used achieve desirable subgoals optimizing action sequence quickly planning ahead approach also wa used create robot able compensate faulty motor whose effect not longer match prediction nn model gloye wiesel tenchio simon 2005 schmidhuber 2007 typically not given advance essential question experiment c conduct quickly improve formal theory fun creativity schmidhuber formalizes driving force value function behind curious exploratory behavior measure learning progress becomes intrinsic reward c schmidhuber compare oudeyer baranes kaplan 2013 singh barto chentanez 2005 motivates c create action sequence experiment make quick progress deep fnns traditional rl markov decision process mdps classical approach rl bertsekas tsitsiklis 1996 samuel 1959 make simplifying assumption markov sion process mdps current input rl agent conveys information necessary compute optimal next output event decision allows greatly reducing cap depth rl nns section 3 using dynamic programming dp trick man 1957 latter often explained probabilistic work sutton barto 1998 basic idea already conveyed deterministic setting simplicity using tion section 2 let input event xt encode entire current state environment including reward rt no need introduce additional notation since real value encode arbitrary vector real value original rl goal find weight maximize sum reward episode placed equivalent set alternative goal set value function v defined input event consider any two quent input event xt xk recursively define v xt rt v xk v xk rk xk last input event search weight maximize v input event causing propriate output event action due markov assumption fnn suffices implement policy map input output event relevant cap not deeper fnn v often modeled separate fnn also yielding typically short cap learning approximate v xt only local information rt v xk many variant traditional rl exist abounadi bertsekas borkar 2002 baird 1995 baird moore 1999 barto sutton anderson 1983 bertsekas 2001 bradtke barto kaelbling 1996 brafman tennenholtz 2002 kaelbling littman cassandra 1995 lagoudakis parr 2003 maei sutton 2010 mahadevan 1996 meuleau peshkin kim kaelbling 1999 moore atkeson 1993 morimoto doya 2000 peng williams 1996 prokhorov wunsch 1997 rummery niranjan 1994 santamaría sutton ram 1997 schwartz 1993 singh 1994 sutton barto 1998 sutton szepesvári maei 2008 tsitsiklis van roy 1996 van hasselt 2012 watkins 1989 watkins dayan 1992 wiering schmidhuber formulated probabilistic framework evaluate pair input output action event instead input event only facilitate certain mathematical derivation some discount delayed reward distortion original rl problem problematic perhaps rl nn rl backgammon player tesauro 1994 achieved level human world champion playing nonlinear rather shallow fnn map large finite number discrete board state value recently rather deep wa used traditional rl framework play several atari 2600 computer game directly 84 84 pixel 60 hz video input mnih et 2013 using experience replay lin 1993 extending previous work neural fitted nfq riedmiller 2005 even better result achieved using slow monte carlo tree planning train comparatively fast deep nns guo singh lee lewis wang 2014 compare rl sallans hinton 2004 input elfwing otsuka uchibe doya 2010 earlier rl atari player grüttner sehnke schaul schmidhuber 2010 earlier raw rl nn computer game koutník cuccu schmidhuber gomez 2013 trained indirect policy search section deep rl rnns partially observable mdps pomdps markov assumption section often unrealistic not directly perceive behind back let alone current state entire universe however memory previous event help deal partially observable markov decision problem pomdps boutilier poole 1996 jaakkola singh jordan 1995 kaelbling et 1995 kimura miyazaki kobayashi 1997 lin 1993 littman cassandra kaelbling 1995 mccallum 1996 otsuka yoshimoto doya 2010 ring 1991 1993 1994 schmidhuber teller 1994 wiering schmidhuber 1996 williams naive way implementing memory without leaving mdp framework section would simply consider possibly huge state space namely set possible observation history prefix realistic way use function approximators rnns produce compact state feature function entire history seen far generally speaking pomdp rl often us dl rnns learn event memorize ignore three basic alternative use rnn value function mapping arbitrary event history value bakker 2002 lin 1993 schmidhuber example deep lstm rnns used way rl robot bakker zhumatiy gruener schmidhuber 2003 102 schmidhuber neural network 61 2015 use rnn controller conjunction second rnn predictive world model obtain combined rnn deep section use rnn rl direct search section indirect search section weight space general however pomdps may imply greatly increased cap depth rl facilitated deep ul fnns rnns rl machine may profit ul input preprocessing jodogne piater 2007 particular ul nn learn compactly encode environmental input image video section compact code instead raw data fed rl machine whose job thus may become much easier cuccu luciw schmidhuber gomez 2011 legenstein wilbert wiskott 2010 like sl may profit ul section example nfq riedmiller 2005 wa applied control task lange riedmiller 2010 riedmiller lange voigtlaender 2012 purely visual input compactly encoded deep coder section rl combined ul based slow feature analysis kompella luciw schmidhuber 2012 wiskott sejnowski 2002 enabled real humanoid robot learn skill raw video stream luciw kompella rounian schmidhuber 2013 deal pomdps section involving input rl wa used suka 2010 raam pollack 1988 section wa ployed deep unsupervised sequence encoder rl gisslen et 2011 certain type rl ul also combined biologically plausible rnns spiking neuron section klampfl maass 2013 rezende gerstner 2014 yin et 2012 deep hierarchical rl hrl subgoal learning fnns rnns multiple learnable level abstraction bengio et 2013 deng yu 2014 fu 1977 lenat brown 1984 ring 1994 seem important rl sl work archical rl hrl ha published since early particular subgoal discovery fnns rnns decomposes rl task subtasks rl submodules huber schmidhuber wahnsiedler 1992 numerous alternative hrl technique proposed bakker schmidhuber 2004 barto mahadevan 2003 dietterich doya samejima katagiri kawato 2002 ghavamzadeh mahadevan 2003 jameson 1991 menache mannor shimkin 2002 moore atkeson 1995 precup sutton singh 1998 ring 1991 1994 samejima doya kawato 2003 simsek barto 2008 tenenberg karlsson head 1993 wei 1994 whiteson kohl miikkulainen stone 2005 hrl framework feudal rl dayan hinton 1993 option barto singh chentanez 2004 singh et 2005 sutton precup singh 1999 not rectly address problem automatic subgoal discovery wiering schmidhuber automatically decomposes pomdps section sequence simpler task solved memoryless policy learnable tive recent hrl organizes potentially deep rl motor control map ring schaul schmidhuber 2011 inspired iological finding graziano 2009 deep rl direct nn not quite universal method section yet practical general traditional rl algorithm section method direct policy search without need value function markovian assumption section weight fnn rnn directly evaluated given rl problem result successive trial inform search better weight unlike rl supported bp section cap depth section 3 not crucial issue may solve credit assignment problem without backtracking deep causal chain modifiable neither care existence try exploit important class method nns policy gradient method aberdeen 2003 baxter bartlett 2001 ghavamzadeh mahadevan 2003 grondman busoniu lope babuska 2012 grüttner et 2010 heess silver teh 2012 kohl stone 2004 peter 2010 peter schaal rückstieß felder schmidhuber 2008 sehnke et 2010 sutton mcallester singh mansour 1999 wierstra foerster peter schmidhuber 2010 wierstra schaul peter schmidhuber 2008 williams 1986 1988 gradient total reward respect policy nn weight estimated exploited repeated nn evaluation rl nns also evolved evolutionary algorithm ea fogel owen walsh 1966 goldberg 1989 holland 1975 rechenberg 1971 schwefel 1974 series trial several policy represented population nns improved mutation repeated recombination tion fittest individual fogel fogel porto 1990 happel murre 1994 maniezzo 1994 montana davis 1989 nolfi parisi elman 1994 compare genetic programming gp cramer 1985 see also smith 1980 used evolve computer program variable size dickmanns schmidhuber hofer 1987 koza 1992 cartesian gp miller harding 2009 miller thomson 2000 evolving program ing nns khan khan miller 2010 topology turner miller 2013 related method include probability based ea baluja 1994 larraanaga lozano 2001 sałustowicz schmidhuber 1997 saravanan fogel 1995 covariance matrix estimation evolution strategy hansen müller koumoutsakos 2003 hansen ostermeier 2001 meisner igel 2009 igel 2003 neuroevolution menting topology neat stanley miikkulainen 2002 hybrid method combine traditional rl section ea whiteson stone 2006 since rnns general computer rnn evolution like gp sense evolve general program unlike sequential program learned traditional gp however rnns mix quential parallel information processing natural cient way already mentioned section many rnn evolvers proposed cliff husband harvey 1993 juang 2004 miglino lund nolfi 1995 miller todd hedge 1989 moriarty 1997 nolfi floreano miglino mondada 1994 mann steinmetz dieckman 1999 sims 1994 whiteson 2012 wieland 1991 yamauchi beer 1994 yao 1993 one ularly effective family method coevolves neuron combining network selecting neuron reproduction participated network gomez 2003 gomez miikkulainen 2003 moriarty miikkulainen 1996 help solve deep pomdps gomez schmidhuber 2005 cosyne doe something similar level synapsis weight gomez schmidhuber lainen 2008 benefit shown difficult nonlinear pomdp benchmark schmidhuber neural network 61 2015 103 natural evolution strategy ne glasmachers schaul sun wierstra schmidhuber 2010 sun gomez schaul huber 2013 sun wierstra schaul schmidhuber 2009 stra et 2008 link policy gradient method evolutionary approach concept natural gradient amari 1998 rnn evolution may also help improve sl deep rnns evolino schmidhuber et 2007 section deep rl indirect policy nn search some method section evolve nns hundred thousand weight not million search large deep nns sl rl method mentioned far somehow search space weight wi some profit reduction search space shared wi get reused cnns section rnns sl section rl section may possible however exploit additional space solution indirect search weight space instead evolving large nns directly tion one sometimes greatly reduce search space evolving compact encoding nns lindenmeyer system jacob lindenmayer rozenberg 1994 lindenmayer 1968 graph rewriting kitano 1990 cellular encoding gruau whitley pyeatt 1996 hyperneat clune stanley pennock ofria 2011 ambrosio stanley 2007 stanley ambrosio gauci 2009 van den berg whiteson 2013 extending neat section extension thereof risi stanley 2012 help avoid overfitting compare section closely related topic regularization mdl section general approach schmidhuber 1997 sl rl seek compactly encode weight large nns schmidhuber 1997 program written universal programming language church 1936 gödel 1931 post 1936 turing 1936 often much efficient systematically search space program bias towards short fast program levin schmidhuber 1997 2004 instead directly searching huge space possible nn weight matrix previous universal language encoding nns wa schmidhuber 1997 recent work us practical language based coefficient popular transforms fourier wavelet particular rnn weight matrix may compressed like image encoding coefficient discrete cosine transform dct koutník et 2013 koutník gomez schmidhuber 2010 compact description evolved ne cosyne section rnn million weight learned without teacher drive simulated car torcs driving game loiacono cardamone lanzi 2011 loiacono et 2009 based like visual input stream koutník et 2013 rnn learned control visual processing scratch without aided ul course ul might help generate compact image code section fed smaller rnn reduce overall computational effort universal rl general purpose learning algorithm may improve fashion way lifelong learning context schmidhuber 1987 schmidhuber zhao schraudolph 1997 schmidhuber zhao wiering 1997 general type rl constrained only fundamental limitation computability identified founder theoretical computer science church 1936 gödel 1931 post 1936 turing 1936 remarkably exist blueprint universal problem solver universal rl machine unlimited problem depth various theoretical sens hutter 2002 2005 schmidhuber 2002 particular gödel machine implemented general computer rnns may improve any part software including learning algorithm way provably certain sense schmidhuber initialized asymptotically optimal hutter 2002 also applicable rnns solve any problem quickly unknown fastest way solving save additive constant overhead becomes negligible problem size grows note problem large only small ai dl researcher still business many interested problem small worth trying reduce overhead le general method including heuristic not discus universal rl method go beyond usually called dl conclusion outlook deep learning dl neural network nns relevant supervised learning sl section 5 unsupervised learning ul section 5 reinforcement learning rl section 6 alleviating problem deep credit assignment path cap section 3 ul section not only facilitate sl sequence section stationary pattern section also rl section dynamic programming dp section important deep sl section traditional rl deep nns section search computing section nns describable bit information section reduce overfitting improve deep sl ul section well rl section also case partially observable environment section deep sl ul rl often create hierarchy abstract representation stationary data section sequential data section rl policy section ul facilitate sl pure sl feedforward nns fnns section recurrent nns rnns section not only win early contest section also recent one section especially dl fnns profited gpu implementation section particular based section section convolutional nns section competition not only pattern recognition section also image segmentation section object detection section unlike system human learn actively perceive pattern sequentially directing attention relevant part available data near future deep nns extending previous work since 1990 nns learn selective attention rl motor action saccade control section b internal action controlling spotlight attention within rnns thus closing general sensorimotor loop external internal feedback section 2 many future deep nns also take account cost energy activate neuron send signal brain seem minimize computational cost lem solving least two way 1 given time only small fraction neuron active local competition mechanism shuts many ing neuron only winner activate neuron outgoing connection compare slim nns section 2 merous neuron sparsely connected compact ume many connection much like microchip traditional supercomputer often neighboring neuron allocated solve single task thus reducing munication cost physic seems dictate any efficient putational hardware future also 104 schmidhuber neural network 61 2015 keeping two constraint successful rent deep rnns however not unlike certain spiking nns tion usually activate unit least slightly tend strongly connected ignoring natural constraint ware possible improve adopting 1 2 minimizing energy cation cost direct search program weight space section rnns allocate boring rnn part related behavior distant rnn part le related one thus way general traditional map fnns section also implement occam razor section energy minimization finding simple highly eralizing problem solution require active neuron mostly short connection distant future may belong general purpose learning algorithm improve provably optimal way section not yet practical commercially relevant acknowledgment since 16 april 2014 draft paper undergone sive open online peer review public mailing list including connectionists neuro list imageworld machine learning forum thanks numerous expert valuable comment thanks snf dfg european commission partially funding dl research group past content paper may used educational commercial purpose including article wikipedia similar site reference aberdeen 2003 algorithm partially observable markov decision process thesis australian national university abounadi bertsekas borkar 2002 learning algorithm markov decision process average cost siam journal control optimization 40 3 akaike 1970 statistical predictor identification annals institute statistical mathematics 22 akaike 1973 information theory extension maximum likelihood principle second intl symposium information theory pp akademinai kiado akaike 1974 new look statistical model identification ieee transaction automatic control 19 6 allender 1992 application kolmogorov complexity plexity theory watanabe ed eatcs monograph theoretical puter science kolmogorov complexity computational complexity pp springer almeida b 1987 learning rule asynchronous perceptrons feedback combinatorial environment ieee international conference neural network vol 2 pp almeida almeida langlois amaral redol 1997 step size adaptation technical report inesc 9 rua alves redol amari 1967 theory adaptive pattern classifier ieee transaction electronic computer 16 3 amari 1998 natural gradient work efficiently learning neural computation 10 2 amari cichocki yang 1996 new learning algorithm blind signal separation touretzky mozer hasselmo ed advance neural information processing system nip vol mit press amari murata 1993 statistical theory learning curve entropic loss criterion neural computation 5 1 amit brunel 1997 dynamic recurrent network spiking neuron following learning network computation neural system 8 4 1996 effect adding noise backpropagation training generalization performance neural computation 8 3 andrade chacon merelo moran 1993 evaluation secondary structure protein uv circular dichroism spectrum using unsupervised learning neural network protein engineering 6 4 andrew diederich tickle b 1995 survey critique technique extracting rule trained artificial neural network system 8 6 anguita gomes 1996 mixing format ral network learning neuroprocessors microprocessing ming 41 10 anguita parodi zunino 1994 efficient implementation bp workstation neurocomputing 6 1 arel rose karnowski 2010 deep machine new frontier artificial intelligence research ieee computational intelligence magazine 5 4 ash 1989 dynamic node creation backpropagation neural network connection science 1 4 atick li redlich 1992 understanding retinal color coding first principle neural computation 4 atiya parlos 2000 new result recurrent network training unifying algorithm accelerating convergence ieee transaction neural network 11 3 ba frey b 2013 adaptive dropout training deep neural network advance neural information processing system nip pp baird 1990 document image defect model proceddings iapr workshop syntactic structural pattern recognition baird 1995 residual algorithm reinforcement learning function approximation international conference machine learning pp baird moore 1999 gradient descent general reinforcement learning advance neural information processing system vol 12 nip pp mit press bakker b 2002 reinforcement learning long memory dietterich becker ghahramani ed advance neural information processing system vol 14 pp cambridge mit press bakker schmidhuber j 2004 hierarchical reinforcement learning based subgoal discovery subpolicy specialization groen et al ed proc conference intelligent autonomous system pp amsterdam nl io press bakker zhumatiy gruener schmidhuber j 2003 robot identify memorize important previous observation proceeding 2003 international conference intelligent robot system pp baldi 1995 gradient descent learning algorithm overview general dynamical system perspective ieee transaction neural network 6 1 baldi 2012 autoencoders unsupervised learning deep architecture journal machine learning research 27 proc 2011 icml workshop unsupervised transfer learning baldi brunak frasconi pollastri soda 1999 exploiting past future protein secondary structure prediction bioinformatics 15 baldi chauvin 1993 neural network fingerprint recognition neural computation 5 3 baldi chauvin 1996 hybrid modeling architecture protein application neural computation 8 7 baldi hornik 1989 neural network principal component analysis learning example without local minimum neural network 2 baldi hornik 1995 learning linear network survey ieee transaction neural network 6 4 baldi pollastri 2003 principled design recursive neural network protein structure prediction problem journal machine learning research 4 baldi sadowski 2014 dropout learning algorithm artificial intelligence ballard 1987 modular learning neural network proc aaai pp baluja 1994 incremental learning method integrating genetic search based function optimization competitive learning technical report carnegie mellon university balzer 1985 15 year perspective automatic programming ieee transaction software engineering 11 11 barlow b 1989 unsupervised learning neural computation 1 3 barlow kaushal mitchison j 1989 finding minimum entropy code neural computation 1 3 barrow 1987 learning receptive field proceeding ieee annual conference neural network vol iv pp ieee barto mahadevan 2003 recent advance hierarchical reinforcement learning discrete event dynamic system 13 4 barto singh chentanez 2004 intrinsically motivated learning hierarchical collection skill proceeding international conference developmental learning pp cambridge mit press barto sutton anderson 1983 neuronlike adaptive element solve difficult learning control problem ieee transaction system man cybernetics battiti 1989 accelerated backpropagation learning two optimization method complex system 3 4 battiti 1992 method learning steepest descent newton method neural computation 4 2 baum haussler 1989 size net give valid generalization neural computation 1 1 baum petrie 1966 statistical inference probabilistic function finite state markov chain annals mathematical statistic baxter bartlett 2001 estimation journal artificial intelligence research 15 1 schmidhuber neural network 61 2015 105 bayer osendorfer 2014 variational inference latent state sequence using recurrent network arxiv preprint bayer osendorfer chen urban van der smagt 2013 fast dropout applicability recurrent network arxiv preprint bayer wierstra togelius schmidhuber j 2009 evolving memory cell structure sequence learning proc icann 2 pp bayes 1763 essay toward solving problem doctrine chance philosophical transaction royal society london 53 communicated price letter canton becker 1991 unsupervised learning procedure neural network international journal neural system 2 becker le cun 1989 improving convergence learning second order method touretzky hinton sejnowski ed proc 1988 connectionist model summer school 1988 pp san mateo morgan kaufmann behnke 1999 hebbian learning competition neural abstraction pyramid proceeding international joint conference neural network vol 2 pp behnke 2001 learning iterative image reconstruction neural abstraction pyramid international journal computational intelligence application 1 4 behnke 2002 learning face localization using hierarchical recurrent network proceeding international conference artificial neural network pp behnke discovering hierarchical speech feature using convolutional matrix factorization proceeding international joint conference neural network vol 4 pp behnke lncs lecture note computer science vol hierarchical neural network image interpretation springer behnke 2005 face localization tracking neural abstraction pyramid neural computing application 14 2 behnke rojas 1998 neural abstraction pyramid hierarchical image understanding architecture proceeding international joint conference neural network vol 2 pp bell sejnowski j 1995 approach blind separation blind deconvolution neural computation 7 6 bellman 1957 dynamic programming ed princeton nj usa princeton university press belouchrani cardoso moulines 1997 blind source separation technique using statistic ieee transaction signal processing 45 2 bengio 1991 artificial neural network application sequence recognition thesis montreal qc canada mcgill university computer science bengio 2009 foundation trend machine learning vol 2 1 learning deep architecture ai publisher bengio courville vincent 2013 representation learning review new perspective ieee transaction pattern analysis machine intelligence 35 8 bengio lamblin popovici larochelle 2007 greedy training deep network cowan tesauro alspector ed advance neural information processing system vol 19 nip pp mit press bengio simard frasconi 1994 learning dependency gradient descent difficult ieee transaction neural network 5 2 beringer graf schiel schmidhuber j 2005 classifying unprompted speech retraining lstm net duch kacprzyk oja zadrozny ed lncs vol artificial neural network biological 2005 pp berlin heidelberg bertsekas 2001 dynamic programming optimal control athena scientific bertsekas tsitsiklis 1996 programming belmont athena scientific bichot rossi desimone 2005 parallel serial neural mechanism visual search macaque area science 308 bärmann 1993 learning algorithm multilayered neural network based linear least square problem neural network 6 1 bishop 1993 smoothing learning algorithm forward network ieee transaction neural network 4 5 bishop 2006 pattern recognition machine learning springer blair pollack b 1997 analysis dynamical recognizers neural computation 9 5 blondel tsitsiklis 2000 survey computational complexity result system control automatica 36 9 bluche louradour knibbe moysset benzeghiba kermorvant 2014 arabic handwritten text recognition system evaluation international workshop document analysis system blum rivest 1992 training neural network neural network 5 1 blumer ehrenfeucht haussler warmuth 1987 occam razor information processing letter 24 bobrowski 1978 learning process multilayer threshold net biological cybernetics 31 bodén wile j 2000 dynamic recurrent neural network connection science 12 bodenhausen waibel 1991 tempo 2 algorithm adjusting delay supervised learning lippman moody touretzky ed advance neural information processing system vol 3 pp morgan kaufmann bohte kok la poutre 2002 temporally encoded network spiking neuron neurocomputing 48 1 boltzmann 1909 hasenöhrl ed wissenschaftliche abhandlungen leipzig barth collection boltzmann article scientific journal bottou 1991 une approche théorique de l apprentissage connexioniste tions à la reconnaissance de la parole thesis université de paris xi bourlard morgan 1994 connnectionist speech recognition hybrid approach kluwer academic publisher boutilier poole 1996 computing optimal policy partially observable markov decision process using compact representation proceeding aaai bradtke barto kaelbling 1996 linear algorithm temporal difference learning machine learning brafman tennenholtz 2002 general polynomial time algorithm reinforcement learning journal machine learning research 3 brea senn pfister 2013 matching recall storage sequence learning spiking neural network journal neuroscience 33 23 breiman 1996 bagging predictor machine learning 24 brette rudolph carnevale hines beeman bower et al 2007 simulation network spiking neuron review tool strategy journal computational neuroscience 23 3 breuel shafait 2013 ocr printed english fraktur using lstm network international conference document analysis recognition pp ieee bromley bentz bottou guyon lecun moore et al 1993 signature verification using siamese time delay neural network international journal pattern recognition artificial intelligence 7 4 broyden et al 1965 class method solving nonlinear simultaneous equation mathematics computation 19 92 brueckner schulter b 2014 social signal classification using deep blstm recurrent neural network proceeding ieee international conference acoustic speech signal processing pp brunel 2000 dynamic sparsely connected network excitatory inhibitory spiking neuron journal computational neuroscience 8 3 bryson 1961 gradient method optimizing allocation process proc harvard univ symposium digital computer application bryson denham 1961 method solving optimum programming problem technical report raytheon company missle space division bryson ho 1969 applied optimal control optimization estimation control blaisdell pub buhler j 2001 efficient sequence comparison hashing bioinformatics 17 5 buntine weigend 1991 bayesian complex system 5 burgess 1994 constructive algorithm converges input pattern international journal neural system 5 1 cardoso 1994 performance orthogonal source separation algorithm proc eusipco pp 2001 continuous latent variable model dimensionality reduction sequential data reconstruction thesis uk university sheffield carter rudolph nucci j 1990 operational fault tolerance cmac network touretzky ed advance neural information processing system nip vol 2 pp san mateo ca morgan kaufmann caruana 1997 multitask learning machine learning 28 1 casey 1996 dynamic computation application recurrent neural network finite state machine extraction neural computation 8 6 cauwenberghs 1993 fast stochastic algorithm supervised learning optimization lippman moody touretzky ed advance neural information processing system vol 5 244 morgan kaufmann chaitin j 1966 length program computing finite binary sequence journal acm 13 chalup blair 2003 incremental training first order recurrent neural network predict language neural network 16 7 chellapilla puri simard 2006 high performance convolutional neural network document processing international workshop frontier handwriting recognition chen salman 2011 learning characteristic deep neural architecture ieee transaction neural network 22 11 cho 2014 foundation advance deep learning thesis aalto university school science cho ilin raiko 2012 regularization restricted boltzmann machine intl conf artificial neural network 2012 pp springer cho raiko ilin 2013 enhanced gradient training restricted boltzmann machine neural computation 25 3 106 schmidhuber neural network 61 2015 church 1936 unsolvable problem elementary number theory american journal mathematics 58 ciresan giusti gambardella schmidhuber j 2012 deep neural network segment neuronal membrane electron microscopy image advance neural information processing system nip pp ciresan giusti gambardella schmidhuber j 2013 mitosis detection breast cancer histology image deep neural network proc miccai vol 2 pp ciresan meier gambardella schmidhuber j 2010 deep big simple neural net handwritten digit recogntion neural computation 22 12 ciresan meier masci gambardella schmidhuber j 2011 flexible high performance convolutional neural network image classification intl joint conference artificial intelligence pp ciresan meier masci schmidhuber j 2011 committee neural network traffic sign classification international joint conference neural network pp ciresan meier masci schmidhuber j 2012 deep neural network traffic sign classification neural network 32 ciresan meier schmidhuber j deep neural network image classification ieee conference computer vision pattern recognition long preprint ciresan meier schmidhuber j transfer learning latin chinese character deep neural network international joint conference neural network pp ciresan schmidhuber j 2013 deep neural network offline handwritten chinese character classification technical report idsia cliff husband harvey 1993 evolving recurrent dynamical network robot control artificial neural net genetic algorithm pp springer clune mouret lipson 2013 evolutionary origin modularity proceeding royal society b biological science 280 1755 clune stanley pennock ofria 2011 performance indirect encoding across continuum regularity ieee transaction evolutionary computation 15 3 coates huval wang wu ng catanzaro b 2013 deep learning cot hpc system proc international conference machine learning cochocki unbehauen 1993 neural network optimization signal processing john wiley son collobert weston j 2008 unified architecture natural language processing deep neural network multitask learning proceeding international conference machine learning pp acm comon 1994 independent component new concept signal processing 36 3 connor brincat pasupathy 2007 transformation shape information ventral pathway current opinion neurobiology 17 2 connor martin atlas 1994 recurrent neural network robust time series prediction ieee transaction neural network 5 2 cook 1971 complexity procedure proceeding annual acm symposium theory computing pp new york acm cramer 1985 representation adaptive generation simple quential program grefenstette ed proceeding international ference genetic algorithm application university hillsdale nj lawrence erlbaum associate craven wahba 1979 smoothing noisy data spline function estimating correct degree smoothing method generalized validation numerische mathematik 31 cuccu luciw schmidhuber gomez 2011 intrinsically motivated evolutionary search reinforcement learning proceeding 2011 ieee conference development learning epigenetic robotics vol 2 pp ieee dahl sainath hinton 2013 improving deep neural network lvcsr using rectified linear unit dropout ieee international conference acoustic speech signal processing pp ieee dahl yu deng acero 2012 deep neural network speech recognition ieee transaction audio speech language processing 20 1 ambrosio stanley 2007 novel generative encoding exploiting neural network sensor output geometry proceeding conference genetic evolutionary computation pp datar immorlica indyk mirrokni 2004 hashing scheme based distribution proceeding annual symposium computational geometry pp acm dayan hinton 1993 feudal reinforcement learning lippman moody touretzky ed advance neural information processing system nip vol 5 pp morgan kaufmann dayan hinton 1996 variety helmholtz machine neural network 9 8 dayan hinton neal zemel 1995 helmholtz machine neural computation 7 dayan zemel 1995 competition multiple cause model neural computation 7 deco parra 1997 feature extraction redundancy reduction unsupervised stochastic neural network neural network 10 4 deco roll 2005 neurodynamics biased competition ation attention model spiking neuron journal neurophysiology 94 1 de freitas 2003 bayesian method neural network thesis university cambridge dejong mooney 1986 learning alternative view machine learning 1 2 demers cottrell 1993 dimensionality reduction hanson cowan giles ed advance neural information processing system nip vol 5 pp morgan kaufmann dempster laird rubin b 1977 maximum likelihood incomplete data via em algorithm journal royal statistical society b deng yu 2014 deep learning method application publisher desimone albright gross bruce 1984 property inferior temporal neuron macaque journal neuroscience 4 8 de souto souto oliveira 1999 loading problem pyramidal neural network electronic journal mathematics computation de valois albrecht thorell 1982 spatial frequency selectivity cell macaque visual cortex vision research 22 5 deville lau 1994 logic program synthesis journal logic programming 19 20 de vries principe 1991 theory neural network time delay lippmann moody touretzky ed advance neural information processing system nip vol 3 pp morgan kaufmann dicarlo zoccolan rust 2012 doe brain solve visual object recognition neuron 73 3 dickmanns behringer dickmanns hildebrandt maurer thomanek et al 1994 seeing passenger car proc int symp intelligent vehicle pp dickmanns schmidhuber winklhofer 1987 der genetische algorithmus eine implementierung prolog technical report inst informatics tech univ munich dietterich ensemble method machine learning multiple classifier system pp springer dietterich hierarchical reinforcement learning maxq value function decomposition journal artificial intelligence research jair 13 di lena nagata baldi 2012 deep architecture protein contact map prediction bioinformatics 28 director rohrer 1969 automated network domain case ieee transaction circuit theory dittenbach merkl rauber 2000 growing hierarchical organizing map international joint conference neural network vol 6 6015 ieee computer society donahue jia vinyals hoffman zhang tzeng et al 2013 decaf deep convolutional activation feature generic visual recognition arxiv preprint dorffner 1996 neural network time series processing neural network world doya samejima ichi katagiri kawato 2002 multiple reinforcement learning neural computation 14 6 dreyfus 1962 numerical solution variational problem journal mathematical analysis application 5 1 dreyfus 1973 computational solution optimal control problem time lag ieee transaction automatic control 18 4 duchi hazan singer 2011 adaptive subgradient method online learning stochastic optimization journal machine learning 12 egorova gloye göktekin liers luft rojas et al 2004 fighter small size 2004 team description robocup 2004 symposium paper team description paper cd edition elfwing otsuka uchibe doya 2010 based reinforcement learning navigation sensory input neural information processing theory algorithm iconip vol 1 pp springer eliasmith 2013 build brain neural architecture biological cognition new york ny oxford university press eliasmith stewart choo bekolay dewolf tang et al 2012 model functioning brain science 338 6111 elman 1990 finding structure time cognitive science 14 2 erhan bengio courville manzagol vincent bengio 2010 doe unsupervised help deep learning journal machine learning research 11 wiskott 2013 solve classification regression problem data supervised extension slow feature analysis journal machine learning research 14 eubank 1988 spline smoothing nonparametric regression farlow ed method modeling new york marcel dekker euler 1744 methodus inveniendi eyben weninger squartini schuller b 2013 voice activity detection lstm recurrent neural network application hollywood movie proc ieee international conference acoustic speech signal processing pp faggin 1992 neural network hardware international joint conference neural network vol 1 153 schmidhuber neural network 61 2015 107 fahlman 1988 empirical study learning speed network technical report univ fahlman 1991 recurrent learning algorithm lippmann moody touretzky ed advance neural information processing system nip vol 3 pp morgan kaufmann falconbridge stamp badcock 2006 simple hebbian network learns sparse independent component natural image neural computation 18 2 fan qian xie soong 2014 tt synthesis bidirectional lstm based recurrent neural network proc interspeech farabet couprie najman lecun 2013 learning hierarchical feature scene labeling ieee transaction pattern analysis machine intelligence 35 8 farlow j 1984 method modeling gmdh type algorithm vol crc press feldkamp prokhorov eagen yuan 1998 enhanced stream kalman filter training recurrent network nonlinear modeling pp springer feldkamp prokhorov feldkamp 2003 simple conditioned adaptive behavior kalman filter trained recurrent network neural network 16 5 feldkamp puskorius 1998 signal processing framework based dynamic neural network application problem adaptation filtering classification proceeding ieee 86 11 felleman van essen 1991 distributed hierarchical processing primate cerebral cortex cerebral cortex 1 1 fernández graf schmidhuber j application recurrent neural network discriminative keyword spotting proc icann 2 pp fernandez graf schmidhuber j sequence labelling structured domain hierarchical recurrent neural network proceeding international joint conference artificial intelligence fernandez rendel ramabhadran hoory 2014 prosody contour prediction long memory deep recurrent neural network proc interspeech field j 1987 relation statistic natural image response property cortical cell journal optical society america 4 field j 1994 goal sensory coding neural computation 6 fieres schemmel meier 2008 realizing biological spiking network model configurable hardware system ieee international joint conference neural network pp fine singer tishby 1998 hierarchical hidden markov model analysis application machine learning 32 1 fischer igel 2014 training restricted boltzmann machine introduction pattern recognition 47 fitzhugh 1961 impulse physiological state theoretical model nerve membrane biophysical journal 1 6 fletcher powell j 1963 rapidly convergent descent method minimization computer journal 6 2 floreano mattiussi 2001 evolution spiking neural controller autonomous robot evolutionary robotics intelligent robotics artificial life pp springer fogel fogel porto 1990 evolving neural network biological cybernetics 63 6 fogel owen walsh 1966 artificial intelligence simulated evolution new york wiley földiák 1990 forming sparse representation local learning biological cybernetics 64 földiák young 1995 sparse coding primate cortex arbib ed handbook brain theory neural network pp mit press förster graf schmidhuber j 2007 learning compact map efficient robot localization european symposium artificial neural network pp franzius sprekeler wiskott 2007 slowness sparseness lead place cell plo computational biology 3 8 friedman hastie tibshirani 2001 springer series statistic vol element statistical learning new york frinken fischer bunke 2012 term memory neural network language modeling handwriting recognition 2012 international conference pattern recognition pp ieee fritzke b 1994 growing neural gas network learns topology tesauro touretzky leen ed nip pp mit press fu 1977 syntactic pattern recognition application berlin springer fukada schuster sagisaka 1999 phoneme boundary estimation using bidirectional recurrent neural network application system computer japan 30 4 fukushima 1979 neural network model mechanism pattern recognition unaffected shift transaction iece 10 fukushima 1980 neocognitron neural network mechanism pattern recognition unaffected shift position biological cybernetics 36 4 fukushima 2011 increasing robustness background noise visual pattern recognition neocognitron neural network 24 7 fukushima artificial vision neural network neocognitron advance neural network 37 fukushima training neural network neocognitron neural network 40 gabor 1946 theory communication part 1 analysis information electrical iii journal institution radio communication engineering 93 26 gallant 1988 connectionist expert system communication acm 31 2 gauss 1809 theoria motus corporum coelestium sectionibus conicis solem ambientium gauss 1821 theoria combinationis observationum erroribus minimis obnoxiae theory combination observation least subject error ge hang lee zhang 2010 stable adaptive neural network control springer geiger zhang weninger schuller rigoll 2014 robust speech recognition using long memory recurrent neural network hybrid acoustic modelling proc interspeech geman bienenstock doursat 1992 neural network dilemma neural computation 4 gers schmidhuber j 2000 recurrent net time count proceeding international joint conference neural network 2000 vol 3 pp ieee gers schmidhuber j 2001 lstm recurrent network learn simple context free context sensitive language ieee transaction neural network 12 6 gers schmidhuber cummins 2000 learning forget continual prediction lstm neural computation 12 10 gers schraudolph schmidhuber j 2002 learning precise timing lstm recurrent network journal machine learning research 3 gerstner kistler 2002 spiking neuron model cambridge university press gerstner van hemmen 1992 associative memory network spiking neuron network computation neural system 3 2 ghavamzadeh mahadevan 2003 hierarchical policy gradient algorithm proceeding twentieth conference machine learning pp gherrity 1989 learning algorithm analog fully recurrent neural network international joint conference neural network san diego vol 1 pp girshick donahue darrell malik j 2013 rich feature hierarchy accurate object detection semantic segmentation technical report uc berkeley icsi gisslen luciw graziano schmidhuber j 2011 sequential constant size compressor reinforcement learning proc fourth conference artificial general intelligence pp springer giusti ciresan masci gambardella schmidhuber j 2013 fast image scanning deep convolutional neural network proc icip glackin mcginnity maguire wu belatreche 2005 novel approach implementation large scale spiking neural network fpga hardware computational intelligence bioinspired system pp springer glasmachers schaul sun wierstra schmidhuber j 2010 exponential natural evolution strategy proceeding genetic evolutionary computation conference pp acm glorot bordes bengio 2011 deep sparse rectifier network aistats vol 15 pp gloye wiesel tenchio simon 2005 reinforcing driving quality soccer playing robot anticipation technology 47 5 gödel 1931 über formal unentscheidbare sätze der principia mathematica und verwandter systeme monatshefte für mathematik und physik 38 goldberg 1989 genetic algorithm search optimization machine learning reading goldfarb 1970 family method derived variational mean mathematics computation 24 109 golub heath wahba 1979 generalized method choosing good ridge parameter technometrics 21 gomez j 2003 robust nonlinear control neuroevolution thesis department computer science university texas austin gomez miikkulainen 2003 active guidance finless rocket using neuroevolution proc gecco gomez schmidhuber j 2005 recurrent neuron learn deep memory pomdps proc 2005 conference genetic evolutionary computation new york ny usa acm press gomez schmidhuber miikkulainen 2008 accelerated neural evolution cooperatively coevolved synapsis journal machine learning research 9 may gomi kawato 1993 neural network control system using neural network 6 7 sak moreno j 2014 automatic language identification using long memory recurrent neural network proc interspeech goodfellow bulatov ibarz arnoud shet 2014 number recognition street view imagery using deep convolutional neural network arxiv preprint 108 schmidhuber neural network 61 2015 goodfellow courville bengio 2011 sparse coding unsupervised feature discovery nip workshop challenge learning hierarchical model goodfellow courville bengio 2012 feature learning sparse coding proceeding international conference machine learning goodfellow mirza da courville bengio 2014 empirical investigation catastrophic forgetting neural network tr goodfellow mirza courville bengio 2013 maxout network international conference machine learning graf 2011 practical variational inference neural network advance neural information processing system nip pp graf eck beringer schmidhuber j 2003 isolated digit recognition lstm recurrent network first international workshop biologically inspired approach advanced information technology graf fernandez gomez schmidhuber j 2006 connectionist temporal classification labelling unsegmented sequence data recurrent neural net icml 06 proceeding international conference machine learning pp graf fernandez liwicki bunke schmidhuber j 2008 strained handwriting recognition recurrent neural network platt koller singer roweis ed advance neural information processing system nip vol 20 pp cambridge mit press graf jaitly 2014 towards speech recognition recurrent neural network proc international conference machine learning pp graf liwicki fernandez bertolami bunke schmidhuber j 2009 novel connectionist system improved unconstrained handwriting recognition ieee transaction pattern analysis machine intelligence 31 5 graf mohamed hinton 2013 speech recognition deep recurrent neural network ieee international conference acoustic speech signal processing pp ieee graf schmidhuber j 2005 framewise phoneme classification bidirectional lstm neural network architecture neural network 18 graf schmidhuber j 2009 offline handwriting recognition multidimensional recurrent neural network advance neural information processing system nip vol 21 pp cambridge mit press graziano 2009 intelligent movement machine ethological perspective primate motor system usa oxford university press griewank 2012 documenta volume ismp pp grondman busoniu lope babuska 2012 survey reinforcement learning standard natural policy gradient ieee transaction system man cybernetics part c application review 42 6 grossberg 1969 some network learn remember reproduce any number complicated pattern journal mathematics mechanic 19 grossberg adaptive pattern classification universal recoding 1 parallel development coding neural feature detector biological cybernetics 23 grossberg adaptive pattern classification universal recoding 2 feedback expectation olfaction illusion biological cybernetics gruau whitley pyeatt 1996 comparison cellular encoding direct encoding genetic neural network neurocolt technical report esprit working group neural computational learning neurocolt grünwald myung pitt 2005 advance minimum description length theory application mit press grüttner sehnke schaul schmidhuber j 2010 deep memory player parameter exploring policy gradient proceeding international conference artificial neural network icann pp springer guo singh lee lewis wang x 2014 deep learning atari game play using offline tree search planning advance neural information processing system vol 27 nip guyon vapnik boser bottou solla 1992 structural risk minimization character recognition lippman moody touretzky ed advance neural information processing system nip vol 4 pp morgan kaufmann hadamard j 1908 mémoire sur le problème analyse relatif à l équilibre de plaque élastiques encastrées mémoires présentés par diver savant à l académie de science de l institut de france éxtrait imprimerie nationale hadsell chopra lecun 2006 dimensionality reduction learning invariant mapping proc computer vision pattern recognition conference ieee press hagras colley callaghan clarke 2004 evolving spiking neural network controller autonomous robot ieee international conference robotics automation vol 5 pp hansen müller koumoutsakos 2003 reducing time complexity derandomized evolution strategy covariance matrix adaptation evolutionary computation 11 1 hansen ostermeier 2001 completely derandomized evolution strategy evolutionary computation 9 2 hanson j 1990 stochastic version delta rule physica nonlinear phenomenon 42 1 hanson pratt 1989 comparing bias minimal network construction touretzky ed advance neural information processing system nip vol 1 pp san mateo ca morgan kaufmann happel murre 1994 design evolution modular neural network architecture neural network 7 6 hashem schmeiser b 1992 improving model accuracy using optimal linear combination trained neural network ieee transaction neural network 6 hassibi stork 1993 second order derivative network pruning optimal brain surgeon lippman moody touretzky ed advance neural information processing system vol 5 pp morgan kaufmann hastie tibshirani j 1990 monograph statisics applied probability vol generalized additive model hastie tibshirani friedman j 2009 springer series statistic element statistical learning hawkins george 2006 hierarchical temporal theory terminology numenta haykin 2001 kalman filtering neural network wiley online library hebb 1949 organization behavior new york wiley 1989 theory backpropagation neural network international joint conference neural network pp ieee heemskerk 1995 overview neural hardware neurocomputers style processing design implementation application heess silver teh 2012 reinforcement learning policy proc european workshop reinforcement learning pp igel 2009 neuroevolution strategy episodic reinforcement learning journal algorithm 64 4 herrero valencia dopazo j 2001 hierarchical unsupervised growing neural network clustering gene expression pattern bioinformatics 17 2 hertz krogh palmer 1991 introduction theory neural computation redwood city hestenes stiefel 1952 method conjugate gradient solving linear system journal research national bureau standard 49 hihi bengio 1996 hierarchical recurrent neural network dependency touretzky mozer hasselmo ed advance neural information processing system vol 8 pp mit press hinton 1989 connectionist learning procedure artificial intelligence 40 1 hinton 2002 training product expert minimizing contrastive divergence neural computation 14 8 hinton dayan frey neal 1995 algorithm unsupervised neural network science 268 hinton deng yu dahl mohamed jaitly et al 2012 deep neural network acoustic modeling speech recognition shared view four research group ieee signal processing magazine 29 6 hinton ghahramani z 1997 generative model discovering sparse distributed representation philosophical transaction royal society b 352 hinton osindero teh 2006 fast learning algorithm deep belief net neural computation 18 7 hinton salakhutdinov 2006 reducing dimensionality data neural network science 313 5786 hinton sejnowski 1986 learning relearning boltzmann machine parallel distributed processing vol 1 pp mit press hinton srivastava krizhevsky sutskever salakhutdinov 2012 improving neural network preventing feature detector technical report hinton van camp 1993 keeping neural network simple proceeding international conference artificial neural network amsterdam pp springer hochreiter 1991 untersuchungen zu dynamischen neuronalen netzen diploma thesis institut für informatik lehrstuhl brauer technische universität münchen advisor schmidhuber hochreiter bengio frasconi schmidhuber j 2001 gradient flow recurrent net difficulty learning dependency kremer kolen ed field guide dynamical recurrent neural network ieee press hochreiter obermayer 2005 sequence classification protein analysis snowbird workshop snowbird utah computational biological learning society hochreiter schmidhuber j 1996 bridging long time lag weight guessing long memory silva principe almeida ed frontier artificial intelligence application vol spatiotemporal model biological artificial system pp amsterdam netherlands io press hochreiter schmidhuber j flat minimum neural computation 9 1 hochreiter schmidhuber j long memory neural computation 9 8 based tr tum 1995 hochreiter schmidhuber j 1999 feature extraction lococode neural computation 11 3 hochreiter younger conwell 2001 learning learn using gradient descent lecture note comp sci vol proc intl conf artificial neural network pp berlin heidelberg springer schmidhuber neural network 61 2015 109 hodgkin huxley 1952 quantitative description membrane current application conduction excitation nerve journal physiology 117 4 hoerzer legenstein maass 2014 emergence complex computational structure chaotic neural network modulated hebbian learning cerebral cortex 24 holden b 1994 theory generalization linearly weighted connectionist network thesis cambridge university engineering department holland 1975 adaptation natural artificial system ann arbor university michigan press honavar uhr 1988 network unit learns perceive generation well reweighting link touretzky hinton sejnowski ed proc 1988 connectionist model summer school pp san mateo morgan kaufman honavar uhr 1993 generative learning structure process generalized connectionist network information science 70 1 hopfield j 1982 neural network physical system emergent collective computational ability proceeding national academy science 79 hornik stinchcombe white 1989 multilayer feedforward network universal approximators neural network 2 5 hubel wiesel 1962 receptive field binocular interaction functional architecture cat visual cortex journal physiology london 160 hubel wiesel 1968 receptive field functional architecture monkey striate cortex journal physiology 195 1 huffman 1952 method construction code proceeding ire 40 hung kreiman poggio dicarlo j 2005 fast readout object identity macaque inferior temporal cortex science 310 5749 hutter 2002 fastest shortest algorithm problem international journal foundation computer science 13 3 schmidhuber snf grant hutter 2005 universal artificial intelligence sequential decision based algorithmic probability berlin springer schmidhuber snf grant 61847 hyvärinen hoyer oja 1999 sparse code shrinkage denoising maximum likelihood estimation kearns solla cohn ed advance neural information processing system nip vol mit press hyvärinen karhunen oja 2001 independent component analysis john wiley son icpr 2012 contest mitosis detection breast cancer histological image 2012 ipal laboratory tribvn company hospital cialab ohio state univ igel 2003 neuroevolution reinforcement learning using evolution strategy reynolds abbass tan mckay essam gedeon ed congress evolutionary computation vol 4 pp ieee igel hüsken 2003 empirical evaluation improved rprop learning algorithm neurocomputing 50 c ikeda ochiai sawaragi 1976 sequential gmdh algorithm application river flow prediction ieee transaction system man cybernetics 7 indermuhle frinken bunke 2012 mode detection online handwritten document using blstm neural network frontier handwriting recognition icfhr 2012 international conference pp ieee indermuhle frinken fischer bunke 2011 keyword spotting online handwritten document containing text using blstm neural network document analysis recognition icdar 2011 international conference pp ieee indiveri hamilton van schaik delbruck et al 2011 neuromorphic silicon neuron circuit frontier neuroscience 5 73 ivakhnenko 1968 group method data rival method stochastic approximation soviet automatic control 13 3 ivakhnenko 1971 polynomial theory complex system ieee transaction system man cybernetics 4 ivakhnenko 1995 review problem solvable algorithm group method data handling gmdh pattern recognition image obrazov analiz izobrazhenii 5 ivakhnenko lapa 1965 cybernetic predicting device ccm information corporation ivakhnenko lapa mcdonough 1967 cybernetics forecasting technique ny american elsevier izhikevich et al 2003 simple model spiking neuron ieee transaction neural network 14 6 jaakkola singh jordan 1995 reinforcement learning algorithm partially observable markov decision problem tesauro touretzky leen ed advance neural information processing system vol 7 pp mit press jackel boser graf denker lecun henderson et al 1990 vlsi implementation electronic neural network example character recognition ieee ed ieee international conference system man cybernetics pp jacob lindenmayer rozenberg 1994 genetic programming lecture note computer science parallel problem solving nature iii jacob 1988 increased rate convergence learning rate adaptation neural network 1 4 jaeger 2001 echo state approach analysing training recurrent neural network technical report gmd report german national research center information technology jaeger 2004 harnessing nonlinearity predicting chaotic system saving energy wireless communication science 304 jain seung 2009 natural image denoising convolutional network koller schuurmans bengio bottou ed advance neural information processing system nip vol 21 pp curran associate jameson j 1991 delayed reinforcement learning multiple time scale hierarchical backpropagated adaptive critic neural network control ji xu yang yu 2013 convolutional neural network human action recognition ieee transaction pattern analysis machine intelligence 35 1 jim giles horne 1995 effect noise convergence generalization recurrent network tesauro touretzky leen ed advance neural information processing system nip vol 7 649 san mateo ca morgan kaufmann jin lujan plana davy temple furber b 2010 modeling spiking neural network spinnaker computing science engineering 12 5 jodogne piater 2007 learning visual control policy journal artificial intelligence research 28 jones palmer 1987 evaluation gabor filter model simple receptive field cat striate cortex journal neurophysiology 58 6 jordan 1986 serial order parallel distributed processing approach technical report ic report san diego institute cognitive science university california jordan 1988 supervised learning system excess degree freedom technical report coin tr massachusetts institute technology jordan 1997 serial order parallel distributed processing approach advance psychology 121 jordan rumelhart 1990 supervised learning distal teacher technical report occasional paper center cog massachusetts institute technology jordan sejnowski j 2001 graphical model foundation neural computation mit press joseph 1961 contribution perceptron theory thesis cornell univ juang 2004 hybrid genetic algorithm particle swarm optimization recurrent network design ieee transaction system man cybernetics part b cybernetics 34 2 judd 1990 neural network modeling connectionism neural network design complexity learning mit press jutten herault j 1991 blind separation source part adaptive algorithm based neuromimetic architecture signal processing 24 1 kaelbling littman cassandra 1995 planning acting partially observable stochastic domain technical report providence ri brown university kaelbling littman moore 1996 reinforcement learning survey journal ai research 4 kak chen wang 2010 data mining using surface deep agent based neural network amcis 2010 proceeding kalinke lehmann 1998 computation recurrent neural network counter iterated function system antoniou slaney ed lnai vol advanced topic artificial intelligence proceeding australian joint conference artificial intelligence berlin heidelberg springer kalman 1960 new approach linear filtering prediction problem journal basic engineering 82 1 karhunen joutsensalo j 1995 generalization principal component analysis optimization problem neural network neural network 8 4 karpathy toderici shetty leung sukthankar 2014 video classification convolutional neural network ieee conference computer vision pattern recognition kasabov 2014 neucube spiking neural network architecture mapping learning understanding brain data neural network kelley j 1960 gradient theory optimal flight path ar journal 30 10 kempter gerstner van hemmen 1999 hebbian learning spiking neuron physical review e 59 4 kerlirzin vallet 1993 robustness multilayer perceptrons neural computation 5 1 khan bennamoun sohel togneri 2014 automatic feature learning robust shadow detection ieee conference computer vision pattern recognition khan khan miller 2010 evolution neural network using cartesian genetic programming ieee congress evolutionary computation pp khan lester plana rast jin painkras et al 2008 spinnaker mapping neural network onto chip multiprocessor international joint conference neural network pp ieee kimura miyazaki kobayashi 1997 reinforcement learning pomdps function approximation icml vol 97 pp 110 schmidhuber neural network 61 2015 kistler gerstner van hemmen 1997 reduction equation threshold model neural computation 9 5 kitano 1990 designing neural network using genetic algorithm graph generation system complex system 4 klampfl maass 2013 emergence dynamic memory trace cortical microcircuit model stdp journal neuroscience 33 28 schraudolph schmidhuber j 2001 unsupervised learning lstm recurrent neural network lecture note comp sci vol proc intl conf artificial neural network pp berlin heidelberg springer kobatake tanaka 1994 neuronal selectivity complex object feature ventral visual pathway macaque cerebral cortex journal neurophysiology 71 kohl stone 2004 policy gradient reinforcement learning fast quadrupedal locomotion robotics automation proceeding icra 04 2004 ieee international conference vol 3 pp ieee kohonen 1972 correlation matrix memory ieee transaction computer 100 4 kohonen 1982 formation topologically correct feature map biological cybernetics 43 1 kohonen 1988 associative memory ed springer koikkalainen oja 1990 hierarchical feature map international joint conference neural network pp ieee kolmogorov representation continuous function several variable superposition continuous function one variable addition doklady akademii nauk sssr 114 kolmogorov three approach quantitative definition information problem information transmission 1 kompella luciw schmidhuber j 2012 incremental slow feature analysis adaptive slow feature updating dimensional input stream neural computation 24 11 kondo 1998 gmdh neural network algorithm using heuristic organization method application pattern identification problem proceeding sice annual conference pp ieee kondo ueno j 2008 neural network selecting optimum neural network architecture application dimensional medical image recognition blood vessel international journal innovative computing information control 4 1 kordík náplava snorek 2003 modified gmdh method model quality evaluation visualization control system computer 2 korkin de gari gers hemmi 1997 cbm machine hardware tool evolves neural net module fraction second run million neuron artificial brain real time kosko b 1990 unsupervised learning noise ieee transaction neural network 1 1 koutník cuccu schmidhuber gomez 2013 evolving neural network reinforcement learning proceeding genetic evolutionary computation conference pp amsterdam acm koutník gomez schmidhuber j 2010 evolving neural network compressed weight space proceeding annual conference genetic evolutionary computation pp koutník greff gomez schmidhuber j 2014 clockwork rnn proceeding international conference machine learning vol 32 pp koza 1992 genetic programming computer mean natural selection mit press kramer 1991 nonlinear principal component analysis using autoassociative neural network aiche journal 37 kremer kolen 2001 field guide dynamical recurrent network ieee press kriegeskorte mur ruff kiani bodurka esteky et al 2008 matching categorical object representation inferior temporal cortex man monkey neuron 60 6 krizhevsky sutskever hinton 2012 imagenet classification deep convolutional neural network advance neural information processing system 4 krogh hertz 1992 simple weight decay improve generalization lippman moody touretzky ed advance neural information processing system vol 4 pp morgan kaufmann kruger janssen kalkan lappe leonardis piater et al 2013 deep hierarchy primate visual cortex learn computer vision ieee transaction pattern analysis machine intelligence 35 8 kullback leibler 1951 information sufficiency annals mathematical statistic kurzweil 2012 create mind secret human thought revealed lagoudakis parr 2003 policy iteration journal machine learning research 4 lampinen oja 1992 clustering property hierarchical map journal mathematical imaging vision 2 lang waibel hinton 1990 neural network architecture isolated word recognition neural network 3 lange riedmiller 2010 deep neural network reinforcement learning neural network 2010 international joint conference pp lapedes farber 1986 nonsymmetrical neural net content addressable memory pattern recognition physica 22 laplace 1774 mémoire sur la probabilité de cause par le évènements mémoires de l academie royale de science presentés par diver savan 6 larraanaga lozano 2001 estimation distribution algorithm new tool evolutionary computation norwell usa kluwer academic publisher le ranzato monga devin corrado chen et al 2012 building feature using large scale unsupervised learning proc icml 12 lecun 1985 une procédure apprentissage pour réseau à seuil asymétrique proceeding cognitiva 85 pp lecun 1988 theoretical framework touretzky hinton sejnowski ed proceeding 1988 connectionist model summer school pp cmu pittsburgh pa morgan kaufmann lecun boser denker henderson howard hubbard et al 1989 applied handwritten zip code recognition neural computation 1 4 lecun boser denker henderson howard hubbard et al 1990 handwritten digit recognition network touretzky ed advance neural information processing system vol 2 pp morgan kaufmann lecun bottou bengio haffner 1998 learning applied document recognition proceeding ieee 86 11 lecun denker solla 1990 optimal brain damage touretzky ed advance neural information processing system vol 2 pp morgan kaufmann lecun muller cosatto flepp b 2006 obstacle avoidance learning advance neural information processing system nip 2005 lecun simard pearlmutter b 1993 automatic learning rate tion estimation hessian eigenvectors hanson cowan giles ed advance neural information processing system vol 5 nip 1992 san mateo ca morgan kaufmann publisher lee 1996 learning language survey literature technical report cambridge massachusetts center research computing technology harvard university lee battle raina ng 2007 efficient sparse coding algorithm advance neural information processing system nip vol 19 pp lee ekanadham ng 2007 sparse deep belief net model visual area advance neural information processing system nip vol 7 pp lee grosse ranganath ng 2009 convolutional deep belief network scalable unsupervised learning hierarchical representation proceeding international conference machine learning pp lee kil 1991 gaussian potential function network hierarchically learning neural network 4 2 lee pham largman ng 2009 unsupervised feature learning audio classification using convolutional deep belief network proc nip vol 9 pp legendre 1805 nouvelles méthodes pour la détermination de orbites de cometes didot legenstein maass 2002 neural circuit pattern recognition small total wire length theoretical computer science 287 1 legenstein wilbert wiskott 2010 reinforcement learning slow feature input stream plo computational biology 6 8 leibniz 1676 memoir using chain rule cited tmme 3 2010 leibniz 1684 nova methodus pro maximis et minimis itemque tangentibus quae nec fractas nec irrationales quantitates moratur et singulare pro illis calculus genus acta eruditorum lenat b 1983 theory formation heuristic search machine learning lenat brown 1984 eurisko appear work artificial intelligence 23 3 lennie movshon 2005 coding color form geniculostriate visual pathway journal optical society america 22 10 levenberg 1944 method solution certain problem least square quarterly applied mathematics 2 levin notion random sequence soviet mathematics doklady 14 5 levin universal sequential search problem problem information transmission 9 3 levin leen moody 1994 fast pruning using principal component advance neural information processing system nip vol 6 35 morgan kaufmann levin narendra 1995 control nonlinear dynamical system using neural network ii observability identification control ieee transaction neural network 7 1 lewicki olshausen 1998 inferring sparse overcomplete image code using efficient coding framework jordan kearns solla ed advance neural information processing system nip vol 10 pp schmidhuber neural network 61 2015 111 l hôpital 1696 analyse de infiniment petits pour l intelligence de ligne courbes paris l imprimerie royale li vitányi b 1997 introduction kolmogorov complexity application springer li zhang suk wang li shen et al 2014 deep learning based imaging data completion improved brain disease diagnosis proc miccai springer lin 1993 reinforcement learning robot using neural network thesis pittsburgh carnegie mellon university lin horne tino giles 1996 learning dependency narx recurrent neural network ieee transaction neural network 7 6 lindenmayer 1968 mathematical model cellular interaction ment journal theoretical biology 18 lindstädt 1993 comparison two unsupervised neural network model redundancy reduction mozer smolensky touretzky elman weigend ed proc 1993 connectionist model summer school pp hillsdale nj erlbaum associate linnainmaa 1970 representation cumulative rounding error algorithm taylor expansion local rounding error master thesis univ helsinki linnainmaa 1976 taylor expansion accumulated rounding error bit numerical mathematics 16 2 linsker 1988 perceptual network ieee computer 21 littman cassandra kaelbling 1995 learning policy partially observable environment scaling prieditis russell ed machine learning proceeding twelfth international conference pp san francisco ca morgan kaufmann publisher liu kramer indiveri delbrück burg douglas et al 2001 avlsi spiking neuron neural network 14 ljung 1998 system identification springer logothetis paul poggio 1995 shape representation inferior temporal cortex monkey current biology 5 5 loiacono cardamone lanzi 2011 simulated car racing championship competition software manual technical report italy dipartimento di elettronica e informazione politecnico di milano loiacono lanzi togelius onieva pelta butz et al 2009 2009 simulated car racing championship lowe 1999 object recognition local feature proceeding seventh ieee international conference computer vision vol 2 pp lowe 2004 distinctive image feature international journal computer vision 60 luciw kompella kazerounian schmidhuber j 2013 intrinsic value system developing multiple invariant representation incremental slowness learning frontier neurorobotics 7 9 lusci pollastri baldi 2013 deep architecture deep learning chemoinformatics prediction aqueous solubility molecule journal chemical information modeling 53 7 maas hannun ng 2013 rectifier nonlinearities improve neural network acoustic model international conference machine learning maass 1996 lower bound computational power network spiking neuron neural computation 8 1 maass 1997 network spiking neuron third generation neural network model neural network 10 9 maass 2000 computational power neural computation 12 maass natschläger markram 2002 computing without stable state new framework neural computation based perturbation neural computation 14 11 mackay 1992 practical bayesian framework backprop network neural computation 4 mackay miller 1990 analysis linsker simulation hebbian rule neural computation 2 maclin shavlik 1993 using neural network improve algorithm refining algorithm protein folding machine learning 11 maclin shavlik 1995 combining prediction multiple classifier using competitive learning initialize neural network proc ijcai pp madala ivakhnenko 1994 inductive learning algorithm complex system modeling boca raton crc press madani hank condon 2003 undecidability probabilistic planning related stochastic optimization problem artificial intelligence 147 1 maei sutton 2010 gq λ general gradient algorithm difference prediction learning eligibility trace proceeding third conference artificial general intelligence vol 1 pp maex orban 1996 model circuit spiking neuron generating directional selectivity simple cell journal neurophysiology 75 4 mahadevan 1996 average reward reinforcement learning foundation algorithm empirical result machine learning 22 malik perona 1990 preattentive texture discrimination early vision mechanism journal optical society america 7 5 maniezzo 1994 genetic evolution topology weight distribution neural network ieee transaction neural network 5 1 manolios fanelli 1994 recurrent neural network deterministic finite state automaton neural computation 6 marchi ferroni eyben gabrielli squartini schuller b 2014 linear prediction based feature audio onset detection bidirectional lstm neural network proc ieee international conference acoustic speech signal processing pp markram 2012 human brain project scientific american 306 6 marquardt 1963 algorithm estimation nonlinear parameter journal society industrial applied mathematics 11 2 marten j 2010 deep learning via optimization fürnkranz joachim ed proceeding international conference machine learning pp haifa israel omnipress marten sutskever 2011 learning recurrent neural network free optimization proceeding international conference machine learning pp martinetz ritter schulten j 1990 neural net learning visuomotor coordination robot arm ieee transaction neural network 1 1 masci giusti ciresan fricout schmidhuber j 2013 fast learning algorithm image segmentation convolutional network international conference image processing pp matsuoka 1992 noise injection input learning ieee transaction system man cybernetics 22 3 mayer gomez wierstra nagy knoll schmidhuber j 2008 system robotic heart surgery learns tie knot using recurrent neural network advanced robotics 22 mccallum 1996 learning use selective attention memory sequential task maes mataric meyer pollack wilson ed animal animats 4 proceeding fourth international conference simulation adaptive behavior pp mit press bradford book mcculloch pitt 1943 logical calculus idea immanent nervous activity bulletin mathematical biophysics 7 melnik levy pollack b 2000 raam infinite language proc ijcnn 5 pp memisevic hinton 2010 learning represent spatial transformation factored boltzmann machine neural computation 22 6 menache mannor shimkin 2002 q discovery goal reinforcement learning proc ecml 02 pp merolla arthur cassidy sawada akopyan et al 2014 million integrated circuit scalable communication network interface science 345 6197 mesnil dauphin glorot rifai bengio goodfellow et al 2011 unsupervised transfer learning challenge deep learning approach jmlr w cp proc unsupervised transfer learning vol meuleau peshkin kim kaelbling 1999 learning finite state controller partially observable environment international conference uncertainty ai pp miglino lund nolfi 1995 evolving mobile robot simulated real environment artificial life 2 4 miller 1994 model development simple cell receptive field ordered arrangement orientation column competition input journal neuroscience 14 1 miller harding 2009 cartesian genetic programming proceeding annual conference companion genetic evolutionary computation conference late breaking paper pp acm miller thomson 2000 cartesian genetic programming genetic programming pp springer miller todd hedge 1989 designing neural network using genetic algorithm proceeding international conference genetic algorithm pp morgan kauffman miller werbos sutton 1995 neural network control mit press minai williams 1994 perturbation response feedforward network neural network 7 5 minsky 1963 step toward artificial intelligence feigenbaum feldman ed computer thought pp new york minsky papert 1969 perceptrons cambridge mit press minton carbonell knoblock kuokka etzioni gil 1989 learning problem solving perspective artificial intelligence 40 1 mitchell 1997 machine learning mcgraw hill mitchell keller 1986 generalization unifying view machine learning 1 1 mnih kavukcuoglu silver graf antonoglou wierstra et al 2013 playing atari deep reinforcement learning technical report deepmind technology mohamed hinton 2010 phone recognition using restricted boltzmann machine ieee international conference acoustic speech signal processing pp molgedey schuster 1994 separation independent signal using correlation physical review letter 72 23 møller 1993 exact calculation product hessian matrix forward network error function vector n time technical report denmark computer science department aarhus university 112 schmidhuber neural network 61 2015 montana davis 1989 training feedforward neural network using genetic algorithm proceeding international joint conference artificial 1 pp san francisco ca usa morgan kaufmann publisher montavon orr müller 2012 lecture note computer science series lncs vol neural network trick trade springer verlag moody 1989 fast learning hierarchy touretzky ed advance neural information processing system nip vol 1 pp morgan kaufmann moody 1992 effective number parameter analysis generalization regularization nonlinear learning system lippman moody touretzky ed advance neural information processing system nip vol 4 pp morgan kaufmann moody utans j 1994 architecture selection strategy neural network application corporate bond rating prediction refenes ed neural network capital market john wiley son moore atkeson 1993 prioritized sweeping reinforcement learning le data le time machine learning 13 moore atkeson 1995 algorithm variable resolution reinforcement learning multidimensional machine learning 21 3 moriarty 1997 symbiotic evolution neural network sequential decision task thesis department computer science university texas austin moriarty miikkulainen 1996 efficient reinforcement learning symbiotic evolution machine learning 22 morimoto doya 2000 robust reinforcement learning leen dietterich tresp ed advance neural information processing system nip vol 13 pp mit press mosteller tukey 1968 data analysis including statistic lindzey aronson ed handbook social psychology vol mozer 1989 focused algorithm temporal sequence recognition complex system 3 mozer 1991 discovering discrete distributed representation iterative competitive learning lippmann moody touretzky ed advance neural information processing system vol 3 pp morgan kaufmann mozer 1992 induction multiscale temporal structure lippman moody touretzky ed advance neural information processing system nip vol 4 pp morgan kaufmann mozer smolensky 1989 skeletonization technique trimming fat network via relevance assessment touretzky ed advance neural information processing system nip vol 1 pp morgan kaufmann muller gunzinger guggenbühl 1995 fast neural net simulation dsp processor array ieee transaction neural network 6 1 munro 1987 dual scheme scalar reinforcement learning proceeding ninth annual conference cognitive science society pp murray edward j 1993 synaptic weight noise mlp learning enhances generalisation learning trajectory hanson cowan giles ed advance neural information processing system nip vol 5 pp san mateo ca morgan kaufmann nadal parga 1994 neuron low noise limit factorial code maximises information transfer network 5 nagumo arimoto yoshizawa 1962 active pulse transmission line simulating nerve axon proceeding ire 50 10 nair hinton 2010 rectified linear unit improve restricted boltzmann machine international conference machine learning narendra parthasarathy 1990 identification control dynamical system using neural network ieee transaction neural network 1 1 narendra thathatchar 1974 learning survey ieee transaction system man cybernetics 4 neal 1995 bayesian learning neural network thesis university toronto neal 2006 classification bayesian neural network candela magnini dagan ed lecture note computer science vol machine learning challenge evaluating predictive uncertainty visual object classification recognising textual entailment pp springer neal zhang j 2006 high dimensional classification bayesian neural network dirichlet diffusion tree guyon gunn nikravesh zadeh ed study fuzziness soft computing feature extraction foundation application pp springer neftci da pedroni cauwenberghs 2014 driven contrastive divergence spiking neuromorphic system frontier neuroscience 7 272 neil liu 2014 minitaur spiking network accelerator ieee transaction large scale integration vlsi system pp 99 nessler pfeiffer buesing maass 2013 bayesian computation emerges generic cortical microcircuit plasticity plo computational biology 9 4 neti schneider young 1992 maximally fault tolerant neural network ieee transaction neural network 3 neuneier zimmermann 1996 train neural network orr müller ed lecture note computer science vol neural network trick trade pp springer newton 1687 philosophiae naturalis principia mathematica london william dawson son nguyen widrow b 1989 truck example self learning neural network proceeding international joint conference neural network pp ieee press nilsson j 1980 principle artificial intelligence san francisco ca usa morgan kaufmann nolfi floreano miglino mondada 1994 evolve autonomous robot different approach evolutionary robotics brook maes ed fourth international workshop synthesis simulation living system artificial life iv pp mit nolfi parisi elman 1994 learning evolution neural network adaptive behavior 3 1 nowak jurie triggs b 2006 sampling strategy image classification proc eccv 2006 pp springer nowlan hinton 1992 simplifying neural network soft weight sharing neural computation 4 connor neil liu delbruck pfeiffer 2013 classification sensor fusion spiking deep belief network frontier neuroscience 7 178 oh jung 2004 gpu implementation neural network pattern recognition 37 6 oja 1989 neural network principal component subspace international journal neural system 1 1 oja 1991 data compression feature extraction autoassociation feedforward neural network kohonen mäkisara simula kangas ed artificial neural network vol 1 pp elsevier science publisher bv olshausen field j 1996 emergence receptive field property learning sparse code natural image nature 381 6583 omlin giles 1996 extraction rule recurrent neural network neural network 9 1 oquab bottou laptev sivic j 2013 learning transferring image representation using convolutional neural network technical report reilly 1996 biologically plausible learning using local tion difference generalized recirculation algorithm neural computation 8 5 reilly 2003 making working memory work computational model learning prefrontal cortex basal ganglion technical report ic reilly wyatte herd mingus jilk j 2013 recurrent processing object recognition frontier psychology 4 orr müller 1998 lecture note computer science series lncs vol neural network trick trade springer verlag ostrovskii volin borisov 1971 über die berechnung von ableitungen wissenschaftliche zeitschrift der technischen hochschule für chemie 13 otsuka 2010 representation external world based approach thesis nara institute science technology otsuka yoshimoto doya 2010 reinforcement learning partially observable environment proc esann otte krechel liwicki dengel 2012 local feature based online mode detection recurrent neural network proceeding 2012 international conference frontier handwriting recognition pp ieee computer society oudeyer baranes kaplan 2013 intrinsically motivated learning real world sensorimotor skill developmental constraint baldassarre mirolli ed intrinsically motivated learning natural artificial system springer pachitariu sahani 2013 regularization nonlinearities neural language model needed arxiv preprint palm 1980 associative memory biological cybernetics palm 1992 information storage capacity local learning rule neural computation 4 2 pan yang q 2010 survey transfer learning ieee transaction knowledge data engineering 22 10 parekh yang honavar 2000 constructive neural network learning algorithm pattern classification ieee transaction neural network 11 2 parker b 1985 technical report center comp research economics management mit pascanu gulcehre cho bengio 2013 construct deep recurrent neural network arxiv preprint pascanu mikolov bengio 2013 difficulty training recurrent neural network icml 13 jmlr w cp vol pasemann steinmetz dieckman u 1999 evolving structure function neurocontrollers angeline michalewicz schoenauer yao zalzala ed proceeding congress evolutionary computation vol 3 pp mayflower hotel washington dc usa ieee press pearlmutter 1989 learning state space trajectory recurrent neural network neural computation 1 2 pearlmutter 1994 fast exact multiplication hessian neural computation 6 1 pearlmutter 1995 gradient calculation dynamic recurrent neural network survey ieee transaction neural network 6 5 schmidhuber neural network 61 2015 113 pearlmutter hinton 1986 unsupervised learning procedure discovering regularity denker ed neural network computing american institute physic conference proceeding 151 vol 2 pp peng williams j 1996 incremental machine learning 22 gers eck schmidhuber j 2003 kalman filter improve lstm network performance problem unsolvable traditional recurrent net neural network 16 perrett hietanen oram benson roll 1992 organization function cell responsive face temporal cortex discussion philosophical transaction royal society london series b biological science 335 1273 perrett roll caan 1982 visual neurones responsive face monkey temporal cortex experimental brain research 47 3 peter j 2010 policy gradient method scholarpedia 5 11 peter schaal natural neurocomputing 71 peter schaal reinforcement learning motor skill policy gradient neural network 21 4 pham kermorvant louradour j 2013 dropout improves recurrent neural network handwriting recognition arxiv preprint pineda j 1987 generalization recurrent neural network physical review letter 19 59 plate 1993 holographic recurrent network hanson cowan giles ed advance neural information processing system nip vol 5 pp morgan kaufmann plumbley 1991 information theory unsupervised neural network dissertation published technical report engineering department cambridge university pollack b 1988 implication recursive distributed representation proc nip pp pollack b 1990 recursive distributed representation artificial intelligence 46 pontryagin boltyanskii gamrelidze mishchenko 1961 mathematical theory optimal process poon domingo 2011 network new deep architecture ieee international conference computer vision workshop pp ieee post 1936 finite combinatory journal symbolic logic 1 3 prasoon petersen igel lauze dam nielsen 2013 voxel classification based triplanar convolutional neural network applied cartilage segmentation knee mri lncs vol medical image computing computer assisted intervention miccai pp springer precup sutton singh 1998 model temporally abstract planning advance neural information processing system nip pp morgan kaufmann prokhorov 2010 convolutional learning system object classification lidar data ieee transaction neural network 21 5 prokhorov feldkamp tyukin 2002 adaptive behavior fixed weight rnn overview proceeding ieee international joint conference neural network pp prokhorov puskorius feldkamp 2001 dynamical neural network control kolen kremer ed field guide dynamical recurrent network pp ieee press prokhorov wunsch 1997 adaptive critic design ieee transaction neural network 8 5 puskorius feldkamp 1994 neurocontrol nonlinear dynamical system kalman filter trained recurrent network ieee transaction neural network 5 2 raiko valpola lecun 2012 deep learning made easier linear transformation perceptrons international conference artificial intelligence statistic pp raina madhavan ng 2009 deep unsupervised learning using graphic processor proceeding annual international conference machine learning pp acm ramacher raab anlauf hachmann beichter bruels et al 1993 multiprocessor memory architecture neurocomputer international journal neural system 4 4 ranzato huang boureau lecun 2007 unsupervised learning invariant feature hierarchy application object recognition proc computer vision pattern recognition conference pp ieee press ranzato poultney chopra lecun 2006 efficient learning sparse representation model platt et al ed advance neural information processing system nip 2006 mit press rauber merkl dittenbach 2002 growing hierarchical map exploratory analysis data ieee transaction neural network 13 6 razavian azizpour sullivan carlsson 2014 cnn feature shelf astounding baseline recognition arxiv preprint rechenberg 1971 technischer systeme nach prinzipien der biologischen evolution dissertation published 1973 redlich 1993 redundancy reduction strategy unsupervised learning neural computation 5 refenes zapranis francis 1994 stock performance modeling using neural network comparative study regression model neural network 7 2 rezende gerstner 2014 stochastic variational learning recurrent spiking network frontier computational neuroscience 8 riedmiller 2005 neural fitted q experience data efficient neural reinforcement learning method proc pp berlin heidelberg riedmiller braun 1993 direct adaptive method faster backpropagation learning rprop algorithm proc ijcnn pp ieee press riedmiller lange voigtlaender 2012 autonomous reinforcement learning raw visual input data real world application international joint conference neural network pp riesenhuber poggio 1999 hierarchical model object recognition cortex nature neuroscience 2 11 rifai vincent muller glorot bengio 2011 contractive encoders explicit invariance feature extraction proceeding international conference machine learning pp ring b 1991 incremental development complex behavior automatic construction hierarchy birnbaum collins ed machine learning proceeding eighth international workshop pp morgan kaufmann ring b 1993 learning sequential task incrementally adding higher order hanson cowan giles ed advance neural information processing system vol 5 pp morgan kaufmann ring b 1994 continual learning reinforcement environment thesis austin texas 78712 university texas austin ring schaul schmidhuber j 2011 organization behavior proceeding first joint conference development learning epigenetic robotics risi stanley 2012 unified approach evolving plasticity neural geometry international joint conference neural network pp ieee rissanen j 1986 stochastic complexity modeling annals statistic 14 3 ritter kohonen 1989 semantic map biological cybernetics 61 4 robinson fallside 1987 utility driven dynamic error propagation work technical report cambridge university engineering department robinson fallside 1989 dynamic reinforcement driven error propagation network application game playing proceeding conference cognitive science society pp rodriguez wile j 1998 recurrent neural network learn implement counting advance neural information processing system nip vol 10 pp mit press rodriguez wile elman j 1999 recurrent neural network learns count connection science 11 1 roggen hofmann thoma floreano 2003 hardware spiking neural network reconfigurable connectivity autonomous robot proc conference evolvable hardware pp ieee rohwer 1989 moving target training method kindermann linden ed proceeding distributed adaptive neural information processing oldenbourg rosenblatt 1958 perceptron probabilistic model information storage organization brain psychological review 65 6 rosenblatt 1962 principle neurodynamics new york spartan roux racoceanu lomenie kulikova irshad klossa et al 2013 mitosis detection breast cancer histological icpr 2012 contest journal pathology informatics 4 rubner schulten 1990 development feature detector organization network model biological cybernetics 62 rückstieß felder schmidhuber j 2008 exploration policy gradient method daelemans et al ed lnai vol european conference machine learning ecml principle practice knowledge discovery database 2008 part ii pp rumelhart hinton williams j 1986 learning internal representation error propagation rumelhart mcclelland ed parallel distributed processing vol 1 pp mit press rumelhart zipser 1986 feature discovery competitive learning parallel distributed processing pp mit press rummery niranjan 1994 using connectionist sytems technical report uk cambridge university russell norvig canny malik edward 1995 artificial intelligence modern approach vol englewood cliff prentice hall saito nakano 1997 partial bfgs update efficient calculation neural network neural computation 9 1 sak senior beaufays 2014 long memory recurrent neural network architecture large scale acoustic modeling proc interspeech sak vinyals heigold senior mcdermott monga et al 2014 sequence discriminative distributed training long memory recurrent neural network proc interspeech salakhutdinov hinton 2009 semantic hashing international journal approximate reasoning 50 7 sallans hinton 2004 reinforcement learning factored state action journal machine learning research 5 sałustowicz schmidhuber j 1997 probabilistic incremental program evolution evolutionary computation 5 2 samejima doya kawato 2003 credit assignment modular reinforcement learning neural network 16 7 114 schmidhuber neural network 61 2015 samuel 1959 some study machine learning using game checker ibm journal research development 3 sanger 1989 optimality principle unsupervised learning touretzky ed advance neural information processing system nip vol 1 pp morgan kaufmann santamaría sutton ram 1997 experiment reinforcement learning problem continuous state action space adaptive behavior 6 2 saravanan fogel b 1995 evolving neural control system ieee expert saund 1994 unsupervised learning mixture multiple cause binary data cowan tesauro alspector ed advance neural information processing system nip vol 6 pp morgan kaufmann schaback werner 1992 numerische mathematik vol springer schäfer udluft zimmermann 2006 learning long term dependency recurrent neural network kollias stafylopatis duch oja ed lecture note computer science vol icann 1 pp springer schapire 1990 strength weak learnability machine learning 5 schaul schmidhuber j 2010 metalearning scholarpedia 6 5 schaul zhang lecun 2013 no pesky learning rate proc international conference machine learning schemmel grubl meier mueller 2006 implementing synaptic plasticity vlsi spiking neural network model international joint conference neural network pp ieee scherer müller behnke 2010 evaluation pooling operation convolutional architecture object recognition proc international conference artificial neural network pp schmidhuber j 1987 evolutionary principle learning learning learn hook diploma thesis inst f tech univ munich schmidhuber j accelerated learning net pfeifer schreter fogelman steel ed connectionism perspective pp amsterdam elsevier schmidhuber j local learning algorithm dynamic feedforward recurrent network connection science 1 4 schmidhuber j dynamische neuronale netze und da fundamentale raumzeitliche lernproblem dynamic neural net fundamental temporal credit assignment problem dissertation inst f tech univ munich schmidhuber j learning algorithm network internal external feedback touretzky elman sejnowski hinton ed proc 1990 connectionist model summer school pp morgan kaufmann schmidhuber j neural heat exchanger talk tu munich 1990 university colorado boulder 1992 li nip 94 workshop unsupervised learning also published intl conference neural information processing vol 1 pp schmidhuber j algorithm dynamic reinforcement learning planning reactive environment proc international joint conference neural network vol 2 pp schmidhuber j curious control system proceeding international joint conference neural network vol 2 pp ieee press schmidhuber j learning generate action sequence kohonen mäkisara simula kangas ed artificial neural network pp elsevier science publisher bv schmidhuber j reinforcement learning markovian environment lippman moody touretzky ed advance neural information processing system vol 3 nip 3 pp morgan kaufmann schmidhuber j fixed size storage time complexity learning gorithm fully recurrent continually running network neural computation 4 2 schmidhuber j learning complex extended sequence using principle history compression neural computation 4 2 based tr tum schmidhuber j learning factorial code predictability minimization neural computation 4 6 schmidhuber j introspective network learn run weight change algorithm proc intl conf artificial neural network brighton pp iee schmidhuber j netzwerkarchitekturen zielfunktionen und kettenregel network architecture objective function chain rule habilitation thesis inst f tech univ munich schmidhuber j 1997 discovering neural net low kolmogorov complexity high generalization capability neural network 10 5 schmidhuber j 2002 speed prior new simplicity measure yielding optimal computable prediction kivinen sloan ed lecture note artificial intelligence proceeding annual conference computational learning theory pp sydney australia springer schmidhuber j 2004 optimal ordered problem solver machine learning 54 schmidhuber j developmental robotics optimal artificial curiosity creativity music fine art connection science 18 2 schmidhuber j gödel machine fully optimal universal goertzel pennachin ed artificial general intelligence pp springer verlag variant available arxiv schmidhuber j 2007 prototype resilient robot science 316 5825 schmidhuber j 2012 neural network technical report swiss ai lab idsia schmidhuber j first deep learning system 1991 deep learning timeline technical report swiss ai lab idsia schmidhuber j powerplay training increasingly general problem solver continually searching simplest still unsolvable problem frontier psychology schmidhuber ciresan meier masci graf 2011 fast deep net agi vision proc fourth conference artificial general intelligence pp schmidhuber eldracher foltin b 1996 semilinear predictability minimization produce feature detector neural computation 8 4 schmidhuber huber 1991 learning generate artificial fovea trajectory target detection international journal neural system 2 1 2 schmidhuber mozer prelinger 1993 continuous history compression hüning neuhauser raus ritschel ed proc intl workshop neural network pp augustinus rwth aachen schmidhuber prelinger 1992 discovering predictable classification technical report dept comp university colorado boulder published neural computation 5 4 1993 schmidhuber wahnsiedler 1992 planning simple trajectory using neural subgoal generator meyer roitblat wilson ed proc international conference simulation adaptive behavior pp mit press schmidhuber wierstra gagliolo gomez j 2007 training recurrent network evolino neural computation 19 3 schmidhuber zhao schraudolph 1997 reinforcement learning policy thrun pratt ed learning learn pp kluwer schmidhuber zhao wiering 1997 shifting inductive bias story algorithm adaptive levin search incremental machine learning 28 schölkopf burges smola j ed 1998 advance kernel support vector learning cambridge mit press schraudolph 2002 fast curvature product gradient descent neural computation 14 7 schraudolph sejnowski j 1993 unsupervised discrimination clustered data via optimization binary information gain hanson cowan giles ed advance neural information processing system vol 5 pp san mateo morgan kaufmann schraudolph sejnowski j 1996 tempering backpropagation network not weight created equal touretzky mozer hasselmo ed advance neural information processing system nip vol 8 pp cambridge mit press schrauwen verstraeten van campenhout j 2007 overview reservoir computing theory application implementation proceeding european symposium artificial neural network pp schuster 1992 learning maximization information transfer nonlinear noisy neuron noise breakdown physical review 46 4 schuster 1999 supervised learning sequential data application speech recognition thesis kyoto japan nara institute science technolog schuster paliwal 1997 bidirectional recurrent neural network ieee transaction signal processing 45 schwartz 1993 reinforcement learning method maximizing counted reward proc icml pp schwefel 1974 numerische optimierung von tion published 1977 birkhäuser basel segmentation neuronal structure em stack challenge 2012 ieee international symposium biomedical imaging sehnke osendorfer rückstieß graf peter schmidhuber j 2010 policy gradient neural network 23 4 sermanet eigen zhang mathieu fergus lecun 2013 feat integrated recognition localization detection using convolutional work arxiv preprint sermanet lecun 2011 traffic sign recognition convolutional network proceeding international joint conference neural network pp oster lichtsteiner et al 2009 caviar 45 k neuron 5 synapse 12 g aer hardware system visual object recognition tracking ieee transaction neural network 20 9 serre riesenhuber louie poggio 2002 role feature real world object recognition biological vision biologically motivated computer vision pp seung 2003 learning spiking neural network reinforcement stochastic synaptic transmission neuron 40 6 schmidhuber neural network 61 2015 115 cottrell 2014 efficient visual coding retina proc international conference learning representation arxiv preprint zhang cottrell 2007 recursive ica advance neural information processing system nip vol 19 1273 shanno 1970 conditioning method function minimization mathematics computation 24 111 shannon 1948 mathematical theory communication part ii bell system technical journal xxvii shao wu li x 2014 learning deep wide spectral method learning deep network ieee transaction neural network learning system shavlik 1994 combining symbolic neural learning machine learning 14 3 shavlik towell 1989 combining neural learning algorithm empirical result connection science 1 3 siegelmann 1992 theoretical foundation recurrent neural network thesis new brunswick rutgers state new jersey rutgers siegelmann sontag 1991 turing computability neural net applied mathematics letter 4 6 silva almeida b 1990 speeding eckmiller ed advanced neural computer pp amsterdam elsevier síma j 1994 loading deep network hard neural computation 6 5 síma j 2002 training single sigmoidal neuron hard neural computation 14 11 simard steinkraus platt j 2003 best practice convolutional neural network applied visual document analysis seventh international conference document analysis recognition pp sims 1994 evolving virtual creature glassner ed acm siggraph proceeding siggraph 94 computer graphic proceeding annual conference pp acm press isbn simsek barto 2008 skill characterization based betweenness nip 08 pp singh 1994 reinforcement learning algorithm vian decision process national conference artificial intelligence pp singh barto chentanez 2005 intrinsically motivated reinforcement learning advance neural information processing system vol 17 nip cambridge mit press smith 1980 learning system based genetic adaptive algorithm thesis univ pittsburgh smolensky 1986 parallel distributed processing exploration crostructure cognition information processing dynamical system dations harmony theory vol 1 pp cambridge usa mit press chapter solla 1988 accelerated learning layered neural network complex system 2 solomonoff j 1964 formal theory inductive inference part information control 7 solomonoff j 1978 induction system ieee transaction information theory 5 soloway 1986 learning program learning construct mechanism explanation communication acm 29 9 song miller abbott 2000 competitive hebbian learning synaptic plasticity nature neuroscience 3 9 speelpenning b 1980 compiling fast partial derivative function given algorithm thesis department computer science university illinois srivastava masci kazerounian gomez schmidhuber j 2013 compete compute advance neural information processing system nip pp stallkamp schlipsing salmen igel 2011 german traffic sign recognition benchmark classification competition tional joint conference neural network pp ieee press stallkamp schlipsing salmen igel 2012 man computer benchmarking machine learning algorithm traffic sign recognition neural network 32 stanley ambrosio gauci j 2009 encoding evolving neural network artificial life 15 2 stanley miikkulainen 2002 evolving neural network augmenting topology evolutionary computation 10 steijvers grunwald 1996 recurrent network performs contextsensitive prediction task proceeding annual conference cognitive science society erlbaum steil j 2007 online reservoir adaptation intrinsic plasticity echo state learning neural network 20 3 stemmler 1996 single spike suffices simplest form stochastic resonance model neuron network computation neural system 7 4 stoianov zorzi 2012 emergence visual number sense hierarchical generative model nature neuroscience 15 2 stone 1974 choice assessment statistical prediction journal royal statistical society b 36 stoop schindler bunimovich 2000 pyramidal neuron lock respond chaotically like synchronize neuroscience research 36 1 stratonovich 1960 conditional markov process theory probability application 5 2 sun chen lee 1993 time warping invariant neural network hanson cowan giles ed advance neural information processing system nip vol 5 pp morgan kaufmann sun giles chen lee 1993 neural network pushdown automaton model stack learning simulation technical report university maryland college park sun gomez schaul schmidhuber j 2013 linear time natural evolution strategy function proceeding genetic evolutionary computation conference 61 amsterdam nl acm sun wierstra schaul schmidhuber j 2009 efficient natural evolution strategy proc genetic evolutionary computation conference pp sutskever hinton taylor 2008 recurrent temporal restricted boltzmann machine nip vol 21 2008 sutskever vinyals le 2014 sequence sequence learning neural network technical report google nip 2014 sutton barto 1998 reinforcement learning introduction cambridge mit press sutton mcallester singh mansour 1999 policy gradient method reinforcement learning function approximation advance neural information processing system nip vol 12 pp sutton precup singh 1999 mdps framework temporal abstraction reinforcement learning artificial intelligence 112 sutton szepesvári maei 2008 convergent n algorithm learning linear function approximation advance neural information processing system nip 08 vol 21 pp szabó póczos lőrincz 2006 optimization independent process analysis independent component analysis blind signal separation pp springer szegedy liu jia sermanet reed anguelov et al 2014 going deeper convolution technical report google szegedy toshev erhan 2013 deep neural network object detection pp taylor spiro bregler fergus 2011 learning invariance imitation conference computer vision pattern recognition pp ieee tegge wang eickholt cheng j 2009 nncon improved protein contact map prediction using neural network nucleic acid research 37 suppl 2 teichmann wiltschut hamker 2012 learning invariance natural image inspired observation primary visual cortex neural computation 24 5 teller 1994 evolution mental model kenneth kinnear ed advance genetic programming pp mit press tenenberg karlsson whitehead 1993 learning via task decomposition meyer roitblat wilson ed animal animats 2 proceeding second international conference simulation adaptive behavior pp mit press tesauro 1994 backgammon program achieves play neural computation 6 2 tieleman hinton 2012 lecture divide gradient running average recent magnitude coursera neural network machine learning tikhonov arsenin john 1977 solution problem winston ting witten 1997 stacked generalization doe work proc international joint conference artificial intelligence tiňo hammer b 2004 architectural bias recurrent neural network fractal analysis neural computation 15 8 tonkes wile j 1997 learning task recurrent neural network analysis stability proceeding fourth biennial conference australasian cognitive science society towell shavlik 1994 artificial neural network artificial intelligence 70 1 tsitsiklis van roy b 1996 method large scale dynamic programming machine learning 22 tsodyks pawelzik markram 1998 neural network dynamic synapsis neural computation 10 4 tsodyks skaggs sejnowski mcnaughton 1996 population dynamic theta rhythm phase precession hippocampal place cell firing spiking neuron model hippocampus 6 3 turaga murray jain roth helmstaedter briggman et al 2010 convolutional network learn generate affinity graph image segmentation neural computation 22 2 turing 1936 computable number application entscheidungsproblem proceeding london mathematical society series 2 41 turner miller 2013 cartesian genetic programming encoded artificial neural network comparison using three benchmark proceeding conference genetic evolutionary computation gecco pp ueda 2000 optimal linear combination neural network improving classification performance ieee transaction pattern analysis machine intelligence 22 2 116 schmidhuber neural network 61 2015 urlbe 1999 digital neural network thesis universidad del valle utgoff stracuzzi j 2002 learning neural computation 14 10 vahed omlin 2004 machine learning method extracting symbolic knowledge recurrent neural network neural computation 16 1 vaillant monrocq lecun 1994 original approach localisation object image iee proceeding vision image signal processing 141 4 van den berg whiteson 2013 critical factor performance hyperneat gecco 2013 proceeding genetic evolutionary computation conference pp van hasselt 2012 reinforcement learning continuous state action space wiering van otterlo ed reinforcement learning pp springer vapnik 1992 principle risk minimization learning theory lippman moody touretzky ed advance neural information processing system nip vol 4 pp morgan kaufmann vapnik 1995 nature statistical learning theory new york springer versino gambardella 1996 learning fine motion using hierarchical extended kohonen map proc intl conf artificial neural network pp springer veta viergever pluim stathonikos van diest j 2013 miccai 2013 grand challenge mitosis detection vieira barradas 2003 training algorithm classification dimensional data neurocomputing 50 viglione 1970 application pattern recognition technology mendel fu ed adaptive learning pattern recognition system academic press vincent hugo bengio manzagol 2008 extracting composing robust feature denoising autoencoders proceeding international conference machine learning pp new york ny usa acm vlassis littman barber 2012 computational complexity stochastic controller optimization pomdps acm transaction computation theory 4 4 vogl mangis rigler zink alkon 1988 accelerating convergence method biological cybernetics 59 von der malsburg 1973 orientation sensitive cell striate cortex kybernetik 14 2 waldinger lee 1969 prow step toward automatic program writing walker norton ed proceeding international joint conference artificial intelligence pp morgan kaufmann wallace boulton 1968 information theoretic measure classification computer journal 11 2 wan 1994 time series prediction using connectionist network internal delay line weigend gershenfeld ed time series prediction forecasting future understanding past pp wang manning 2013 fast dropout training proceeding international conference machine learning pp wang venkatesh judd 1994 optimal stopping effective machine complexity learning advance neural information processing system nip 6 pp morgan kaufmann watanabe 1985 pattern recognition human mechanical new york wiley watanabe 1992 kolmogorov complexity computational complexity eatcs monograph theoretical computer science springer watkins 1989 learning delayed reward thesis oxford king college watkins dayan 1992 machine learning 8 watrous kuhn 1992 induction automaton using recurrent network moody hanson lippman ed advance neural information processing system vol 4 pp morgan kaufmann waydo koch 2008 unsupervised learning individual category image neural computation 20 5 weigend gershenfeld 1993 result time series prediction competition santa fe institute neural network ieee international conference pp ieee weigend rumelhart huberman 1991 generalization application forecasting lippmann moody touretzky ed advance neural information processing system nip vol 3 pp san mateo ca morgan kaufmann wei 1994 hierarchical chunking classifier system proceeding national conference artificial intelligence vol 2 pp aaai mit press weng ahuja huang 1992 cresceptron neural network grows adaptively international joint conference neural network vol 1 pp ieee weng ahuja huang 1997 learning recognition segmentation using cresceptron international journal computer vision 25 2 werbos j 1974 beyond regression new tool prediction analysis behavioral science thesis harvard university werbos j 1981 application advance nonlinear sensitivity analysis proceeding ifip conference nyc pp werbos j 1987 building understanding adaptive system approach factory automation brain research ieee action system man cybernetics werbos j 1988 generalization backpropagation application recurrent gas market model neural network werbos j backpropagation neurocontrol review prospectus international joint conference neural network vol 1 pp werbos j neural network control system identification proceeding tampa werbos j 1992 neural network system identification control chemical industry white sofge ed handbook intelligent control neural fuzzy adaptive approach pp thomson learning werbos j 2006 backwards differentiation ad neural net past link new opportunity automatic differentiation application theory implementation pp springer west saad 1995 adaptive learning multilayer network touretzky mozer hasselmo ed nip pp mit press white 1989 learning artificial neural network statistical perspective neural computation 1 4 whitehead 1992 reinforcement learning adaptive control perception action thesis university rochester whiteson 2012 evolutionary computation reinforcement learning wiering van otterlo ed reinforcement learning pp berlin germany springer whiteson kohl miikkulainen stone 2005 evolving keepaway soccer player task decomposition machine learning 59 1 whiteson stone 2006 evolutionary function approximation reinforcement learning journal machine learning research 7 widrow hoff 1962 associative storage retrieval digital information network adaptive neuron biological prototype synthetic system 1 widrow rumelhart lehr 1994 neural network application industry business science communication acm 37 3 wieland 1991 evolving neural network controller unstable system international joint conference neural network vol 2 pp ieee wiering schmidhuber j 1996 solving pomdps levin search eira saitta ed machine learning proceeding thirteenth international conference pp san francisco ca morgan kaufmann publisher wiering schmidhuber j adaptive behavior 6 2 wiering schmidhuber j fast online q λ machine learning 33 1 wiering van otterlo 2012 reinforcement learning springer wierstra foerster peter schmidhuber j 2010 recurrent policy gradient logic journal igpl 18 2 wierstra schaul peter schmidhuber j 2008 natural evolution strategy congress evolutionary computation wiesel hubel 1959 receptive field single neurones cat striate cortex journal physiology 148 wile elman j 1995 learning count without counter case study dynamic activation landscape recurrent network proceeding seventeenth annual conference cognitive science society pp mit press cambridge wilkinson h ed 1965 algebraic eigenvalue problem new york ny usa oxford university press williams j 1986 connectionist network ical analysis technical report san diego institute cognitive science university california williams j 1988 toward theory connectionist system technical report boston college comp northeastern university williams j 1989 complexity exact gradient computation algorithm recurrent neural network technical report boston northeastern university college computer science williams j simple statistical algorithm connectionist reinforcement learning machine learning 8 williams j training recurrent network using extended kalman filter international joint conference neural network vol 4 pp ieee williams peng j 1990 efficient algorithm training recurrent network trajectory neural computation 4 williams zipser 1988 learning algorithm continually running fully recurrent network technical report ic report san diego la jolla univ california williams zipser experimental analysis recurrent learning algorithm connection science 1 1 williams zipser learning algorithm continually running fully recurrent network neural computation 1 2 willshaw von der malsburg 1976 patterned neural connection set proceeding royal society london series b 194 windisch 2005 loading deep network hard pyramidal case neural computation 17 2 wiskott sejnowski 2002 slow feature analysis unsupervised learning invariance neural computation 14 4 schmidhuber neural network 61 2015 117 witczak korbicz mrugalski patton j 2006 gmdh neural approach robust fault diagnosis application damadics benchmark problem control engineering practice 14 6 wöllmer blaschke schindl schuller färber mayer et al 2011 driver distraction detection using long memory ieee transaction intelligent transportation system tit 12 2 wöllmer schuller rigoll 2013 keyword spotting exploiting long memory speech communication 55 2 wolpert 1992 stacked generalization neural network 5 2 wolpert 1994 bayesian backpropagation function rather weight cowan tesauro alspector ed advance neural information processing system nip vol 6 pp morgan kaufmann wu baldi 2008 learning play go using recursive neural network neural network 21 9 wu shao 2014 leveraging hierarchical parametric network skeletal joint based action segmentation recognition proc conference computer vision pattern recognition wyatte curran reilly 2012 limit feedforward vision recurrent processing promotes robust object recognition object degraded journal cognitive neuroscience 24 11 wysoski benuskova kasabov 2010 evolving spiking neural network audiovisual information processing neural network 23 7 yamauchi beer 1994 sequential behavior learning evolved dynamical neural network adaptive behavior 2 3 yamins hong cadieu dicarlo j 2013 hierarchical modular optimization convolutional network achieves representation similar macaque human ventral stream advance neural information processing system nip pp yang ji xu wang lv yu et al 2009 detecting human action surveillance video trec video retrieval evaluation workshop yao x 1993 review evolutionary artificial neural network international journal intelligent system 4 yin meng jin 2012 developmental approach structural organization reservoir computing ieee transaction autonomous mental development 4 4 yin wang zhang liu 2013 icdar 2013 chinese handwriting recognition competition international conference document analysis recognition pp young davis mishtal arel 2014 hierarchical spatiotemporal feature extraction using recurrent online clustering pattern recognition letter 37 yu chen cheng 1995 dynamic learning rate optimization backpropagation algorithm ieee transaction neural network 6 3 frinken fischer bunke 2014 neural network language model handwriting recognition pattern recognition 47 4 zeiler 2012 adadelta adaptive learning rate method corr zeiler fergus 2013 visualizing understanding convolutional network technical report nyu zemel 1993 minimum description length framework unsupervised learning thesis university toronto zemel hinton 1994 developing population code minimizing description length cowan tesauro alspector ed advance neural information processing system vol 6 pp morgan kaufmann zeng goodman smyth 1994 discrete recurrent neural network grammatical inference ieee transaction neural network 5 2 zimmermann tietz grothmann 2012 forecasting recurrent neural network 12 trick montavon orr müller ed lecture note computer science vol neural network trick trade ed pp springer zipser kehoe littlewort fuster j 1993 spiking network model active memory journal neuroscience 13 8