computer science review 40 2021 100379 content list available sciencedirect computer science review journal homepage review article survey deep learning application shi dong b ping wang khushnood abbas school computer science technology zhoukou normal university henan 466000 china b state key laboratory networking switching technology beijing university post telecommunication beijing 100876 china r c l e n f article history received 5 july 2020 received revised form 27 january 2021 accepted 3 february 2021 available online 15 march 2021 keywords deep learning stacked auto encoder deep belief network deep boltzmann machine convolutional neural network b r c deep learning branch machine learning frontier artificial intelligence aiming closer primary intelligence paper mainly adopts summary induction method deep learning firstly introduces global development current situation deep learning secondly describes structural principle characteristic some kind classic model deep learning stacked auto encoder deep belief network deep boltzmann machine convolutional neural network thirdly present latest development application deep learning many field speech processing computer vision natural language processing medical application finally put forward problem future research direction deep learning 2021 elsevier right reserved content introduction 2 history deep neural network 2 activation function 3 parameter learning 3 deep learning performance 3 limitation key issue 3 optimization 3 architecture 4 generalization regularization 4 stability robustness 4 deep learning model 4 stacking automatic encoders 4 convolution neural network 6 deep learning graph 7 deep probabilistic neural network 9 deep fuzzy neural network 9 generative adversarial network gans 9 application deep learning 10 natural language processing 10 speech recognition 10 medical application 12 computer vision 12 deep learning graph 15 intelligent transportation system 15 use deep learning 16 challenge deep learning 17 lack innovation model structure 17 update training method 17 author school computer science technology zhoukou normal university henan 466000 china address njbsok dong 2021 elsevier right reserved dong wang abbas computer science review 40 2021 100379 challenge parameter learning 17 reduce training time 17 online learning 17 overcome adversarial sample 18 conclusion 18 declaration competing interest 18 acknowledgment 18 reference 18 introduction deep learning nothing many classifier working gether based linear regression followed some activation function basis traditional tistical linear regression w x b approach only difference many neural node deep learning instead only one node called linear regression ditional statistical learning neural node also known neural network one classifier node known neural unit perception another contrasting point need noticed deep learning many layer input output layer many hundred even thousand neural unit layer input output known hidden layer node known hidden node traditional machine learning classifier need write complex hypothesis deep neural network generated network make powerful tool learning nonlinear relationship effectively machine learning divided two development process including shallow learning deep learning 2006 deep learning wa introduced research trend research direction mainly focus shallow learning structure data processing compared deep learning shallow learning limited not exceed two layer feature conversion layer common shallow structure include logistic regression support vector chine gaussian mixture model far shallow learning only quickly efficiently solve lem multiple restriction not handle complex problem real world human voice natural picture visual scene shallow learning ha limitation never handled like human brain information 2006 hinton et al 11 put forward deep belief network dbn deep belief network wa stacked number restricted boltzmann machine rbm restricted boltzmann machine put forward unsupervised ing algorithm greedy unsupervised learning training put data learning initial value supervised learning deep learning structure could solve problem shallow learning could not solve deep learning started development scientific technological personnel began focus application deep learning research nificantly promoted development human intelligence study deep learning mainly embodied convening various artificial intelligence conference tablishment world elite research group establishment enterprise research team continuous application deep learning artificial intelligence deep learning rithms proposed continuously new record created continuously many data set example test process image classification 1000 kind image five year continuous improvement deep learning model image classification error rate dropped higher accuracy ordinary people fact wa success using deep learning enable machine learn successfully identify categorize image development science technology constantly refreshing human cognition deep learning model constantly updated core technology model artificial intelligence big data environment reflecting latest research progress current science technology history deep neural network initial move towards neural network occurred 1943 warren mcculloch neurophysiologist youthful mathematician walter pitt composed paper ron may function proposed basic neural network electrical circuit 1949 donald hebb theorized neural pathway strengthened time used 12 nathanial rochester ibm research simulated abstract neural network ibm 704 computer 13 1956 four scientist worked together summer project known mouth summer research project artificial intelligence four scientist john mccarthy marvin minsky nathaniel rochester claude shannon provided insightful leap ai research 14 following dartmouth project 1957 john von neumann proposed telegraph relay vacuum tube used imitate simple neuron function 1958 frank rosenblatt cornell began work perceptron wa charmed activity eye fly significant part preparing advises fly escape done eye perceptron came research wa built hardware established neural network still used today single layer perceptron wa discovered helpful classifying single valued set input one two class perceptron figure weighted amount data source take away limit pass one two potential quality outcome 1959 bernard widrow marcian hoff stanford created model called adaline madaline model named utilization multiple adaptive linear element madaline wa primary neural network applied problem adaptive channel eliminate echo telephone line neural organization still business use shockingly prior triumph made individual overstate capability neural network especially considering restriction hardware accessible extreme hype streamed academic technical field tainted overall writing time disillusionment set guarantee unfilled likewise dread set essayist contemplated impact figuring machine would man asimov arrangement robot uncovered impact man ethic quality machine equipped entirety humankind work 1982 interest field wa reestablished john hopfield caltech introduced 1 html 2 dong wang abbas computer science review 40 2021 100379 paper national academy science methodology wa make valuable machine utilizing bidirectional line beforehand association neuron wa single way additionally 1982 wa joint conference neural network japan declared another fifth generation exertion neural network u paper created stress u could abandoned field fifth era processing includes computerized reasoning original utilized switch wire second era utilized transistor third state utilized strong state innovation like porated circuit significant level programming dialect fourth era code subsequently wa additionally subsidizing manner exploration field 1985 american institute physic started ha become yearly gathering neural network computing 1987 institute electrical electronic engineer ieee first international conference neural network drew ce participant 1997 recurrent neural network structure long memory lstm wa proposed schmidhuber hochreiter long momentary memory lstm artificial recurrent neural network rnn architecture 1 utilized field deep learning not like standard feedforward neural network lstm ha feedback connection not cycle single information focus example picture yet tionally whole sequence data example speech video 1998 yann lecun published learning applied document recognition wa major step learning data 15 activation function another important factor neural network activation function inspired human neural firing either fire not activation function used generate nonlinear relationship input output nonlinearity combined many neural node many layer mimic human brain like structure called neural network many activation function some presented fig 1 b fig 1 plotted different activation function commonly used sigmoid hyperbolic tangent relu role activation function transform abstract data classifiable plane generally data tightly clustered job tivation function transforms data different plane help observing effect different dimension given problem best classic example activation function sigmoid activation used logistic regression fact logistic regression considered one neural unit see fig 1 job sigmoid function take any input give output 0 1 used classification problem fig 1 c plotted one hidden layer neural network ha three hidden neural unit hidden layer one output layer hidden unit similar logistic regression model difference next layer input come layer fig 1 plotted description one hidden layer one neural unit every layer fig 1 easily noticed neural network consist many layer every layer any number neural unit 2 parameter learning traditional machine learning classifier deep ing classifier also need learn parameter help some mathematical tool gradient descent gradient scent algorithm useful learning parameter convex function function convex ha one absolute ima function convex parameter learning easy otherwise need some mathematical trick change convex function convex function problem also known convex optimization problem however technicality neural network optimization optimization mean ha many optimum point learning done minimizing error predicted value actual value deep learning performance figure fig 2 show size neural network affect prediction accuracy small data small size neural network perform regression svm svm support vector machine classifier big data small neural network better classical classifier however bigger neural network improves performance trained big data performance bigger size neural work grows data compared classical model medium smaller neural network wange et al 16 found deep neural network better perceived using knowledge received visualization output image acquired layer study ha done improve visualization idea neural network method strategy obscuring technique may useful neural network performance limitation key issue deep learning estimation large number parameter go input space output vector space learning continuous geometric morphing input vector space output vector space training point point deep neural network best interpolate point near training point mean best learn need point point training possible outcome expensive real world complex problem autonomous driving etc training huge number possible outcome reduce chance testing error deep learning fails abstract information example algorithm based logic applied verity unknown data deep learning only applied data model wa trained although known deep neural network approximate any degree polynomial function given time main challenge optimizing error error optimization difficult no one generic function achieve task optimization understood many real world problem ral optimization 17 optimization high dimensional data open challenge researcher 18 nouiehed razaviyayn 17 proposed solution convex optimization problem neural network yun et al 19 found even input data random label created according planted model loss surface nonlinear network contains spurious local minimum furthermore yun et al 19 observed intrusion minor nonlinearities activation function caused bad local minimum loss surface neural network haeffele vidal came solution find optimum minimum deep neural network studying effect training neural network 20 3 dong wang abbas computer science review 40 2021 100379 fig basic deep neural network show classic logistic regression b show different activation function plot c show one hidden layer shallow network depicts two hidden layer neural network architecture learn complex relationship data neural unit stacked together either serial allel found arrangement neuron also affect neural network performance haeffele et al 21 observed network specific structure path initialization global minimum vation help find long lasting questing finding optimum minimum neural network generalization regularization dropout widely used prevent switching some neural unit randomly us predefined dropout probability found strong regularizer neural work 22 mianjy et al 23 investigated effect dropout regularization linear autoencoders laes found induced regularizer indeed nuclear norm 24 another recent dropout technique put graphical ising model top neural network order identify le useful neuron drop ising dropout model dropout method switch offs neural unit based activation value dense layer neural network stability robustness sengupta et al 25 investigated prediction time increase rnn model loses robustness different ments task obtained different dynamical behavior suggests policy selection mizes expected complexity computational cost sengupta friston zheng et al 26 propose attach stability term objective function power model parable yield test training set perturbed form propose improve robustness neural organization little perturbation enter picture 27 plored performance neural network different random weight haber et al 28 interpreted deep learning eter estimation problem nonlinear dynamical system given formulation analyze stability deep neural network use new understanding develop new network architecture introduce concept stable network arbitrarily long malladi et al 29 proposed fast normalization technique converges cost le computation cycle exploit property weight update predicts norm without explicitly culating capacity revert neural network mitigates need store activation value backpropagation decreasing memory impression calculation plication motivates utilization reversible neural network emerging hyperbolic system 30 additionally open likelihood build various network utilizing distinctive discretizations ode midpoint network paper structured follows section 6 give introduction overview deep learning model tion 7 discus application deep learning technology section 8 give example use deep learning solving problem challenge deep learning discussed section 9 section 10 concludes paper point focus future work deep learning model section survey basic deep model discus architecture feature present deep learning mainly includes stacked automatic encoder deep belief network deep boltzmann machine convolution ral network following brief introduction basic model stacking automatic encoders automatic encoder auto encoder ae auto encoder 31 mainly consists encoder decoder hidden layer working process shown fig automatic encoder firstly encodes input signal us coded signal reconstruct initial signal coded signal minimize error initial signal reconstructed signal process encoding reconstruction encoder map input data specific feature space characteristic encoded 4 dong wang abbas computer science review 40 2021 100379 fig figure show performance deep neural network improves data size fig schematic diagram automatic encoder signal mapped back data space decoder initial data reconstructed automatic encoder mapping often concerned input encoded difference forced coding data input data system restore initial signal different form thus feature extracted achieve automatic learning stacking automatic encoders 2006 hinton et al graded structure encoder improve posed automatic encoder dae denoising auto encoder 32 researcher gradually put forward shrink automatic encoder contractive auto encoder spare auto encoder convolutional auto encoder automatic encoders stacked automatic encoders stacked automatic encoders deep network tures formed superposition simple coding structure realization stacked automatic encoder presented shown fig shown fig 4 let n automatic encoders trained bottom top firstly first autoencoder trained initial reconstruction error minimized secondly output first autoencoder trained input second encoder operated last layer output last layer used input data classifier parameter finally based standard supervision only top layer appropriate restricted boltzmann machine bipartite graph no link first layer second layer first layer consider input layer visual layer second layer consider den layer 33 let u suppose node random binary variable moreover full probability distribution p v h subjected boltzmann distribution called boltzmann machine 34 specific model shown fig according characteristic restricted boltzmann machine activation condition hidden layer independent given state visible layer input data state given hidden layer activation condition visible layer independent though distribution restricted boltzmann machine not calculated effectively random sample obtained gibbs sampling random sample subjected constrained boltzmann machine long number hidden layer sufficient restricted boltzmann machine fit any discrete distribution term application stricted boltzmann model ha successfully used fig stacked automatic encoders 5 dong wang abbas computer science review 40 2021 100379 solve various machine learning problem sion classification dimensionality reduction time series modeling image feature extraction deep belief network deep belief network dbn 35 36 composed superposition multiple constrained boltzmann model hidden explanatory factor neural network multiple layer typical neural network shown fig network layer remains connection layer connection layer doe not exist data dependency exhibited visual layer unit captured training hidden layer shown fig 7 process deep belief network training firstly carried supervised greedy method obtain eigenvalue model layer layer greedy layer method called contrastive divergence ha proved valid method training process visual layer generates vector pass data hidden layer vector conversely input visual layer randomly selected attempt reconstruct original input signal neuron activation unit new visual unit continue forwarding transfer input order reconstruct hidden layer activation unit obtains tor repetitive process also known gibbs sampling correlation input visual layer activation unit hidden layer important basis measuring weight update restricted boltzmann machine trained layer bottom top access concatenated put bottom layer provide association top layer associate memory content ultimately discriminate performance obtained completed neural network get good initial data however not optimal solution deep belief network us tagged data adjust discriminated performance error back tion bp meanwhile adding label set top layer repetitive learning identification weight get classification network stronger single error back propagation algorithm training time shorter neural work important turning point deep learning emergence deep belief network utilized various area voice recognition image processing deep boltzmann machine deep boltzmann machine 37 dbm deep boltzmann machine also formed restricted boltzmann machine stack similar deep belief network difference deep boltzmann machine deep belief network former layer current layer connection no feedback rameters top bottom deep boltzmann machine training method first us unsupervised get desired initial authority us averaging algorithm finally supervised carried deep boltzmann machine different model firstly deep boltzmann machine ha ability learn complex intrinsic representation new way speech recognition object nition deep learning significantly improve performance field voice recognition secondly deep boltzmann machine build higher tation large number data achieve fig restricted boltzmann machine binary map desired value deep boltzmann us known artificial tagged data model also deep boltzmann machine robust deal vague input data information spread better reduces error process dissemination convolution neural network overview convolution neural network convolution neural network 38 cnn convolutional neural network wa first proposed inspired cat cortex 39 system wa classical model convolution neural network error rate wa only mnist widely used identify handwritten check bank not recognize large image development graphic processing unit gpu technology krizhevsky et al 40 used efficient gpu supported program solve imagenet problem 2012 made convolution neural network popular fact one bottleneck deep neural net wa took long time training many hidden node network gpus become faster parallel computing bottleneck wa overcome present convolution neural network hot topic field voice data analysis image recognition convolution neural network ha network structure share permission make closer biological neural network network structure convolution neural network effectively reduce complexity network model also reduce number weight mainly efficient deal dimensional image directly consider image input entire network effectively avoid complex feature extraction reconstruction traditional algorithm process image recognition convolution neural network ha high degree variance scaling tilting translating form image deformation structure convolution neural network layer neural network layer convolution ral network structure composed number dimensional plane plane ha independent ron sparse connection used layer 6 dong wang abbas computer science review 40 2021 100379 fig structure deep belief network mean neuron feature map only connects neuron small area upper feature map rather traditional neural network typical model convolution neural network shown fig convolution neural network structure mainly depends shared weight local experience field ensure invariance input data factor explained follows local experience field first hidden layer contains six feature map see fig 8 feature map corresponding small box input layer local experience field called sliding window convolution activation value aj l jth feature map convolution layer l expressed following equation aj l f bj l ai l 1 f function convolution operator bj l jth offset lth layer weight cumulative input feature map layer index vector mjl feature map l layer feature map need accumulated lth layer weight share convolution layer usually composed several feature map weight feature fig 8 reduce number parameter translation convolution layer also translate output time however characteristic remain unchanged long feature detected exact position no longer considered only relative position feature preserved convolution layer ha layer performs local averaging reduce sensitivity associated deformation translation output feature mapping subsampling layer denoted eq 2 follows aj l ai nl 2 nl boundary size subsurface required l layer pling function factor formula 2 mean operation localized part size neuron output layer c class identified output layer output characterization front connection feature mapping expressed eq 2 follows output f w 3 partial value vector w 0 weight trix fv eigenvector kij l bj l w 0 model parameter convolution neural network structure mainly alternately composed convoluted layer sampling layer reduction spatial resolution number feature map also increasing training process convolution neural network follows first stage forward training phase consists following three step select sample according given sample set randomly put sample initial data network calculate corresponding output data second stage backward propagation phase consists following two step calculate difference ideal data mation output data information adjust weight matrix according tion error method reverse transmission deep learning graph deep learning traditional machine learning take data form vector considers euclidian plane graph data set dissimilar data set image audio etc graph following characteristic explain failure traditional machine learning approach 41 irregular domain previously mentioned graph represents irregular domain data data set image audio easily resented euclidean plane grid like structure explained reason many mathematical operation not directly applied graph data 42 structure graph tool represent complex system therefore might different shape structure homogeneous signed unsigned graph graph may also different node centric link prediction node ranking 7 dong wang abbas computer science review 40 2021 100379 fig training process deep belief network etc graph centric graph generation graph tion etc utilised graph tion method using adjacency matrix change shape addition deletion node reason ml model not take adjacency matrix directly scalability parallelization computational tool abundance era first problem big data problem consequence generated graph might million node billion edge example google web page link data graph cause hindrance training machine learning model especially many node hidden layer second problem parallelize algorithm since every node graph carry some information node graph node some relation node not missed since losing information might lose vital information domain specific knowledge learning graph might also need aware domain specific knowledge interaction prediction task drug chemical molecular structure may help better tion extra information might helpful prediction side effect feature graph neural network graph neural network kind neural network take input graph data not vector learns represent feature every node generated feature used any problem node classification graph fig picture show convolutional operator work grid like structure generates output grid classification clustering node tion problem every node characterized feature xi ha some associated label lv graph tion problem set node associated label li learning feature node graph neural network ha predict label unknown node learns represent node dimensional vector vi vector vi contains information neighbor node node presented following equation 4 43 vi f xi xco vne xne 4 xco represents feature edge adjacent node embedded neighboring node node f transition function neural network output dimensional vector formula solved using neighborhood aggregation theorem method rewritten iterative form follows v f v x 5 output transition function oi applied get final low dimensional vector follows oi g vi xi 6 hidden parameter learned applying loss function predicted output oi true label li based graph neural network many derived deep learning model developed graph lutional neural network gcn 44 gaphsage 45 etc model state art model based graph neural network graph convolutional network graph convolutional neural network operates three step fig convolutional neural network model 8 dong wang abbas computer science review 40 2021 100379 function act like scanner ha limitation number pixel cell adjacency matrix need considered time scanner function work small part grid fig show kernel function applied input grid pooling similar kernel scanner function ing function give output value scanned scanner time output value based max pooling important element average value also known mean pooling seen fig 9 noticed many value input grid only one output cell generated applying kernel followed pooling function flattening flattening function truncates grid structure vector used feed forward neural network three method common graph lutional neural network big difference come only different kernel function across graph neural work however researcher classified graph convolutional neural network two type follows spatial method kind convolutional ations not need eigenvalue graph spectral method kind method based eigenvalue popular spatial method consider aspect whole graph structure well individual graph component subfield known graph nal processing gps based signal cessing technique fourier transformation etc some work field chebnets 46 kipf welling 44 deep probabilistic neural network although probabilistic neural network pnn quite long time 47 based shallow neural network architecture pnn network ha four basic layer input layer grab distributes input vector pattern layer applies kernel input summation layer get average output pattern unit class decision layer declares class assigned input vector based unit maximum output summation layer 47 recently deep probabilistic neural network dpnn ha introduced 48 advantage pnns neural network perform better even le training data 47 therefore found useful many case usually lack lot training data ical imaging signal processing etc pnns better adversarial attack make promising choice dnns fails even minor random error gast et al 48 provide deep probabilistic neural network altering little current neural network applied following two approach achieve first simplest consists replacing output layer network probabilistic one see fig 10 b second alteration go considering activation tainties also within network mean deep tainty propagation see fig 10 c deep fuzzy neural network although current neural network outperforms even human benchmark some problem image recognition still lack many aspect one aspect pretability expansibility model one not know explain going inside neural network eter estimation therefore use black box machine some network deep convolution network stack hundred thousand layer together solve image recognition problem consist million billion nal parameter although performance good not explain layer parameter logic reason even slight adversarial attack image predicts completely different response even random label cause deep neural network affect test formance bad 49 neural network lack logical reasoning therefore prone adversarial attack example alpha go lost one game south korean go player move wa not decisive move led sequence move neural network give unpredictable result example despite advance sensor camera neural network failed detect pedestrian shadowed street 50 considering logical reasoning aspect fuzzy logic some research proposed deep fuzzy neural network 51 fuzzing two system together proposed hierarchical approach fuse fuzzy logic neural network taneously leaned feature representation altogether robust data classification zhou et al first transform input vector latent vector using neural network fuzzifies representation output layer pattern classification 52 generative adversarial network gans class generative model based game ory not explicitly model data distribution rather model sample sampling performed using deep neural network neural network take input random noise transform model distribution suppose example sample data pdata x xi 1 need find model approximate given data pmodel x pdata x pmodel x not parametric model accomplished using deep neural network generative adversarial work consists two neural network one called generator another one called discriminator model called adversarial generator constantly trying fool discriminator believing input training data real data discriminator always distinguishes two generator neural network take input random noise vector transform model distribution discriminator neural network distinguishes output data point fake training data ples real act like classifier input real fake two neural network trying work setting weight generator learns convert random noise vector model bution fig 11 generator g take random noise vector latent space put some sample discriminator take input training data real check generated fake sample generator g training data image similar kind task say painting face etc upon taking input error function output probability ticular sample real fake output used train weight generator well discriminator another important part formulation error function cost function gans problem formulated minimax zero sum game 9 dong wang abbas computer science review 40 2021 100379 fig 10 show deterministic neural network b show final layer change probabilistic put c show intermediate layer activation value change distribution source image 48 fig generative adversarial network work application deep learning section covered application deep learning various area summarized one table following various application deep learning natural language processing natural language deep learning applied many area voice translation machine translation computer semantic understanding fact success deep learning only two field image processing natural language processing 2012 schwenk et al 55 proposed statistical machine translation system based deep neural network dnn wa able learn meaningful translation probability unseen phrase not sented training set 2014 dong et al 56 proposed novel adaptive adamc layer recursive neural network model introduced one composition function wa adaptively selected based input feature 2014 tang et al 57 presented dnn twitter data sentiment analysis 2015 google duced word lens recognition technology based deep learning used word lens call translation video translation technology not only could read word time also word could translated desired target language also translation work could done phone without networking current technology could applied visual translation 20 language addition google proposed automatic mail reply function gmail used deep learning model extracting mail content analyzing semantically finally reply generated based analysis semantic technique fundamentally different traditional functionality speech recognition order realize interaction searcher made great effort 1952 bell institute davis others successfully developed world first experimental tem identify 10 english digital pronunciation research speech recognition technology ha decade history voice recognition wa dictator used certain area wa mentioned u press one top ten event computer development last two decade speech recognition technology ha made significant progress continuous improvement deep learning model large number speech recognition device application gun move laboratory market 2014 baidu launched deep speech voice recognition system deep learning technology achieve 8 accuracy noisy 10 dong wang abbas computer science review 40 2021 100379 table 1 application deep learning application field reference method task natural language processing schwenk et al 2012 55 translation dong et al 2014 56 adammc based sentiment analysis semantic composition tang et al 2014 et al 57 coool sentiment classification speech recognition et al 2015 58 speech recognition maas et al 2017 59 speech recognition medical application li et al 2014 60 lung disease identification li et al 2015 61 alzheimer disease classification sirinukunwattana et al 2016 62 nep cancer disease classification dou et al 2016 63 cnn cerebral microbleeds identification semantic segmentation scene labeling face recognition krizhevsky et al 2012 40 image detection malik et al 2012 64 bayesian network ontology learning behnke et al 2014 65 sensor based deep learning technology semantic scene segmentation sun et al 2014 66 deepid face recognition pinheiro p et al 2014 67 scene labeling taigman et al 2014 68 deepface face recognition long et al 2015 69 fcn semantic segmentation schroff et al 2015 70 facenet face recognition clustering wang et al 2015 71 semantic segmentation zheng 2015 72 semantic segmentation ronneberger et al 2015 73 biomedical image segmentation badrinarayana et al 2015 74 segnet convolutional semantic labeling liu et al 2015 75 dpn cnn semantic segmentation byeon w et al 2015 76 segmentation scene labeling lin et al 2016 77 semantic segmentation shen et al 2016 78 semantic image segmentation chandra et al 2016 79 gcrf based contextual relation part image luc et al 2016 80 semantic segmentation hoffman j et al 2016 81 semantic segmentation shuai b et al 2016 82 scene labeling et al 2016 83 resnet image recognition chen et al 2017 84 semantic segmentation koziski et al 2017 85 gan based semantic segmentation chen et al 2017 86 semantic segmentation souly et al 2017 87 gan based semantic segmentation yu et 88 semantic segmentation marvin et al 2018 89 semantic segmentation object detection karen et al 2015 90 oject detection pierre et al 2014 91 object localization detection russakovsky et al 2015 92 object detection chatfield et al 2015 93 object detection pi et al 2020 94 object detection aerial imagery gu et al 2020 95 object detection image video object segmentation caellas et al 2017 96 osvos moving object detection video shin et al 2017 97 moving object detection video jang et al 2017 98 convolutional trident network moving object detection video hu et al 2017 99 maskrnn instance level video object segmentation sasikumar et al 2018 100 mask moving object detection video li et al 2018 101 video foreground target extraction xiao et al 2018 102 monet moving object detection video goel et al 2018 103 morel moving object detection video separation schofield et al 1996 104 object detection video tavakkoli et al 2005 105 dnn foreground background separation video culibrk et al 2006 106 background modeling maddalena et al 2007 107 self background modeling ramiraz et al 2013 108 resom background separation video guo et al 2013 109 background modeling xu et al 2014 110 network background modeling xu et al 2014 111 network background modeling ramirez et al 2015 112 map som cellular neural network cnns dynamic object detection qu et al 2016 113 background modeling minematsu et al 2018 114 background modeling ammar et al 2019 115 deepsphere foreground modeling sultana et al 2020 116 background modeling continued next page environment february 2016 baidu deep speech 2 error rate phrase recognition reduced 2015 et al 58 proposed node pruning method reconstruct dnn gave novel bottleneck feature 2017 11 dong wang abbas computer science review 40 2021 100379 table 1 continued application field reference method task graph based application duvenaud et al 2015 117 molecular property generation kearness et al 2016 118 molecular graph convolution molecular property prediction berg et al 2017 119 graph auto link prediction monti et al 2017 120 cnn matrix completion gilmer et al 2017 121 mpnn molecular property prediction coley et al 2017 122 molecular graph embedding generation ktena et al 2017 123 graph similarity prediction parisont et al 2017 124 brain disease prediction parisont et 125 brain disease prediction qui et al 2018 126 deepinf gnn based social influence prediction ying et al 2018 127 pinsage recommendation et al 2018 128 gcpn molecular graph generation cao et al 2018 129 molgan molecular graph generation zitnic et al 2018 130 dacagon side effect prediction xie et al 2018 131 cgcnn material property prediction intelligent transportation system moreira et al 2013 132 time varying poisson arima destination prediction de et al 2015 133 destination prediction vinyals et al 2015 134 demand serving li et al 2015 135 dl routing problem bello et al 2016 136 demand serving zhang et al 2016 137 traffic flow prediction chen et al 2016 138 stacked predicting traffic accident severity endo et al 2017 139 destination prediction ke et al 2017 140 demand prediction yao et al 2017 141 deep network demand prediction khalil et al 2017 142 dl routing problem et al 2017 143 traffic flow prediction jiang et al 2017 144 transportation mode yao et al 2017 145 trajectory clustering yang et al 2018 146 dl navigation jindal et al 2018 147 neural reinforcement learning travel time estimation li et al 2018 148 dl travel time estimation kool et al 2018 149 attention model routing problem lv et al 2018 150 destination prediction yuan et al 2019 151 seen recognition li et al 2019 152 traffic route planning maas et al 59 analyzed different architecture parameter dnn training large speech data found simple architecture simple optimization method gave strong performance complicated model medical application forecast function deep learning automatic ture identification make popular technique disease nosis also application deep learning medical field either use frequency use specie stantly upgrading 2014 li et al 60 proposed customized cnn classify lung image patch model used dropout method structure avoid overfitting 2015 li et al 61 proposed framework differentiate identity stage alzheimer disease ad mri pet scan data 2016 srinukunwattana et al 62 proposed spatially constrained convolutional neural network analyze histopathology image identify nucleus cancerous cell method better performance classical feature classification method 2016 google developed vision system identifying ocular eas worked moorfields eye hospital diabetic retinopathy macular degeneration provide early prevention method month later google used deep learning technique design head neck cancer therapy method effective control patient radiotherapy time could minimize radiotherapy patient injury continuous development deep learning technology deep learning field precision medical care lead prominent contribution computer vision computer vision essential application artificial telligence interdisciplinary field deal computer gain understanding digital image video use computer camera replace human eye target object recognition tracking measurement visual problem deal graphic computer achieve image processing capability even beyond eye 2015 baidu announced refresh performance imagenet image classification tion test error rate image recognition wa le 5 wa beyond human level error first time computer performance computer vision broad term give birth many research direction following some well known direction come umbrella computer vision image segmentation throughout previous thirty year one difficult issue computer vision ha image segmentation image segmentation not quite image classification item recognition not important understand visual idea article explicit object classification characterize object ha explicit label example horse auto house dog ideal image segmentation calculation likewise fragment unknown object object new unknown various application image tation could utilized improve existing algorithm social legacy conservation picture duplicate fication satellite symbolism examination 12 dong wang abbas computer science review 40 2021 100379 visual hunt interaction application approaching division would permit sue drawn closer semantic level instance image recovery picture could portioned added information base point question prepared well may fragmented permit client inquiry ative portion information base discover entirety cruiser data set interaction aspect every video casing would segmented client could connect better level different people item climate regard air terminal instance security group ordinarily keen any unattended thing some could hold risky material useful make inquiry article given human image segmentation problem stated given image algorithm identify two pixel closely related see work pavlidis et al 1979 154 ideal algorithm cluster pixel 155 together according two object two car image algorithm separate pixel car pixel many work ha done specially utilizing deep learning tool conventional algorithm segment image based clustering information contour edge markov process proposed geman et al 156 1984 advancement algorithm deep learning technique image segmentation digital medium becoming popular challenging survey image segmentation basis strength weakness major challenge using deep learning technique found various application area review presented 1996 zhang et al 157 2013 narkhedo 158 2014 kaur et al 159 2016 kuruvilla et al 160 face recognition face recognition biometric fication technology based feature human face firstly camera used collect video image data containing face collected video image data used detect image face automatically 2012 huang et al 161 presented convolutional deep belief network image tation problem order exploit global structural feature model used local convolutional restricted machine 2014 taigman et al 68 applied face modeling apply piecewise affine transformation erating feature achieved 27 error reduction respect model 2014 sun et al 66 proposed deep hidden identity feature deepid representation generation face data could easily used classifier 2015 schroff et al 70 proposed facenet based convolutional network considered face image euclidian space generated low dimensional feature face recognition accuracy latest deep learning algorithm facenet wa eye recognition generally deep learning obtain essential characteristic manual expression doe not deep learning moderate sparse ha strong selectivity face attribute identification also ha good robustness local block face recognition feature obtained based deep learning display straint added model main reason deep learning widely used field face recognition main technology deep learning face recognition includes convolution neural network technology robustness modeling deep learning face pose deep face recognition technology face recognition technology constraint vironment face recognition technology based deep learning etc object detection object detection one tal challenging problem computer vision ha active research area since last decade 166 goal object detection find given object category image video human face eye mal etc foundational task image understanding computer vision object detection basis ing many complex problem scene understanding image captioning event detection activity recognition etc object detection ha plethora application consumer electronics robot vision security autonomous driving interaction automated lance image dimension high traditional algorithm not effective learn tern recently 2006 hinton et al 167 found deep neural net effective automated feature learning high dimensional image fact credit success deep learning go computer vision community see work 2015 russakovsky et al 92 2015 lecun et al 168 2012 krizhevsky et al 40 object detection problem categorize two type first one aim find particular object face football player ronaldo eiffel tower etc task find generic object some category probably unseen object example cat dog car etc farmer problem bit harder later research work ha done later problem many review article field mostly specific problem interest example pedestrian detection see work dollar et al 169 2012 enzweiler et al 170 2009 geronimo et al 171 2009 vehicle detection see work sun et al 172 2006 sakhare et al 2020 173 yuan et al 174 2020 face detection 2019 zhao et al 178 object detection method using semantic segmentation deep learning method surveyed application semantic segmentation field maritime surveillance presented 2018 cane et al 179 year 2014 girshick et al 180 ha proposed simple scalable object detection scheme experimental result give 30 percent improves mean average precision previous state art method 2015 girshick et al 181 ha designed fast convolutional network method fast object detection performance fast higher existing architecture image semantic segmentation image contains large amount information semantic segmentation image process dividing image specific region extracting relevant target key image processing image analysis lie called image semantic segmentation image segmentation task segregate pixel cluster according some semantic relevance not classify pixel mantic segmentation pixel classified 2014 liu et al 182 reviewed some work probabilistic ical model pgm image segmentation analysis 2014 hoft et al 65 solved image mentation problem getting depth information greatly improved image semantic segmentation 13 dong wang abbas computer science review 40 2021 100379 2014 pinheiro collobert 67 proposed end end framework based recurrent convolutional neural work scene labeling 2015 long et al 69 proposed end end fully convolution network fcn tic segmentation however fcn method not quately consider relationship different pixel resulted insufficient segmentation 2015 yon et al 76 proposed long short term memory lstm recurrent neural network based end end framework segmentation classification later 2015 zheng et al 72 combined convolutional neural network conditional random graphical model image segment analysis order reach segmentation classification task 2015 yu koltun 88 proposed dilated cnn crf label modeling image segmentation 2015 ronneberger et al 73 proposed biomedical image segmentation relied data augmentation 2015 badrinarayana et al 74 proposed segnet using convolutional encoder framework image analysis 2015 liu et al 75 posed deep parsing network dpn solved age segmentation problem incorporating markov dom field mrf 2016 lin et al 77 proposed fully connected crf linear binary clique helped identifying similar image segment 2016 shen zeng 78 proposed framework considered order feature along discriminative feature 2016 chandra kokkinos 79 solved image segmentation problem using gaussian conditional random field deep neural net 2016 shuai et al 82 proposed directed acyclic graph rent neural network wa able model semantic dependency among image unit wa also able learn rare class 2016 luc et al 80 proposed generative adversarial network image semantic segmentation could work different kind image trained 2016 hoffman et al 81 proposed unsupervised adversarial generative model returned model domain adaptive model 2017 chen et al 84 combined atrous arable convolutional pyramid pooling semantic image segmentation problem 2017 chen et al 86 proposed deeplab image segmentation problem ered multiscale feature via parallel filter different dilation factor 2017 kozinski et al 85 proposed generative adversarial framework age segmentation 2017 souly et al 87 gave gan generative adversarial network based generative model annotation motivation behind model wa adding large amount fake visual data forced real sample closed feature space consequence clustering approach helped multiclass classification task some extent reduced problem brought cnn fcn network process image semantic segmentation continuous development deep learning image semantic segmentation ues develop precise faster direction 2018 wang et al 71 solved problem dense sampling convolutional framework 2018 mann 89 proposed convolutional crfs based framework considering conditional independence helped implementing crfs gpus video object segmentation due rapid development online social medium video data overwhelmed internet environment finding object interest inside video really beneficial demanding task object tation pixel divided two subset ground target background region generates object segmentation mask core problem behavior recognition video retrieval object tracking used locate location object insider video helpful intelligent surveillance object tracking object segmentation tary accurate object segmentation help object tracking vice versa instance level ject segmentation popular video processing object identification video editing video compression achieved interesting research direction recently user get object first frame using line video object segmentation algorithm presented 2017 jang et al 98 also known ctn maskrnn 2017 hu et al 99 novel recurrent neural net method instance level segmentation video method using recurrent neural net bination segmentation localization net idea take benefit temporal information location prior improve result some state art method 2018 xiao et al 102 presented novel trainable network monet introduces two motion exploitation component feature ment distance transform layer refine segmentation result separation segmentation task algorithm split background foreground area image currently hot topic ha wide application intelligent surveillance public space traffic monitoring industrial machine sion recently neural network based model also applied background separation task see work 1996 schofield et al 104 2013 ramirez et el 108 2015 ramirez et al 112 1996 schofield et al 104 first use ral network background foreground separation task proposed random access neural network need background information correctly represented tavakkoli 2005 105 proposed nn approach approaching novelty detection ground divided block block associated radial basis function neural network 2006 culibrk et al 106 proposed feed forward neural network background separation task based bayesian model although work wa supervised work unsupervised also 2007 maddalena petrosino 107 came self organizing background subtraction sob model based organizing neural network architecture preserving pixel spatial relation weight vector neural network number pixel setting ground modeled using neuron weight network model used simple neural network deep neural network along convolutional neural network used showed promising accuracy see work guo et al 109 2013 xu et al 110 2014 xu et al 111 2019 xu et al 188 2015 qu et al 113 study area suggest read paper ammar et al 115 2019 bouwmans et al 189 2019 minematsu et al 114 2018 sultana et al 116 2020 14 dong wang abbas computer science review 40 2021 100379 deep learning graph recent year researcher trying develop new technique effectively learn pattern graph structured data wide verity problem solved using deep learning graph example 2018 qiu et al 126 presented end end deep learning framework influential user prediction took input user local graph structure 2018 ying et al 127 proposed recommendation framework wa based random walk graph convolutional neural network framework wa suitable largescale graph 2017 berge et al 119 proposed graph framework based differential message passing mechanism helped interaction bipartite graph completion also 2017 monti et al 120 presented geometric deep ing framework wa based convolutional neural work recurrent neural network model helped matrix completion problem predicting accurate rating ommendation system researcher also solved deep learning graph problem chemistry 2015 naud et al 117 presented deep learning model generating molecular feature based convolutional neural network 2017 gilmer et al 121 developed deep learning framework based message passing neural network molecular property prediction 2016 kearnes et al 118 developed molecular graph convolutional neural network worked undirected molecular graph 2018 et al 128 proposed ment learning based graph convolutional policy network gcpn wa goal directed graph generation model model wa highly applied chemistry drug discovery need find new molecule within given molecular property synthetic accessibility 2018 cao kipf 129 proposed generative adversarial network gan based likelihood free generative model model wa also able generate molecule desired molecular property 2017 coley et al 122 solved molecular graph representation problem applying graph convolutional network undirected molecular graph along molecular graph structural tribute also considered factor atom bond attribute atom neighbor radius 2018 xie et al 131 proposed crystal graph convolutional neural network framework wa able learn material property crystal atomic link structure could helpful new material design 2017 ktena et al 123 used graph tional neural network graph similarity prediction identity brain disorder wa common treat complex disease giving many drug time targeted complex diseased teins however sometimes presence another drug effect changing one drug usually not observed clinical train solve problem 2018 zitnik et al 130 posed decagon graph convolutional framework decagon could predict side effect patient could caused two drug 2017 2018 parisot et al used graph convolutional network brain disease prediction also 2018 assouel et al 190 proposed conditional graph generative model intelligent transportation system intelligent transportation system heart smart city research focus century 151 191 transportation system back bone any nation age found 40 world population spent 1 hour road everyday see paper zhang et al 191 population world growing vehicle becoming hard manage without help machine 2019 alone u citizen used public transportation vehicle took billion trip resulted billion km traveling suggest smart transportation demand big city around world portation data vary letter digit sound image video example automatic passenger counter lead revenue generation prediction need image recognition video surveillance along automatic passenger counter also need analyze route people took time need gps road map information also sometimes require generated data weather heterogeneous data come various sensor would installed various location traffic signal car etc main problem focus destination prediction traffic signal control demand prediction traffic flow prediction transportation mode combinatorial optimization deep learning ha applied following problem see work veras et al 192 2019 destination prediction destination prediction one task dict person vehicle end journey currently hot research area popularity cause deep learning model improve performance abundance data transportation system duce tera byte data every day see work moreira et al 132 two approach found literature destination prediction predicting destination trajectory prefix brébisson lv et al proposed deep neural network method based basis trajectory path information 2015 2018 respectively achieve combined idea fixed length trajectory possible destination information predicting destination via next step 2017 endo et al 139 solved destination prediction atively predicting next point trajectory demand prediction oppose destination aim predict journey end demand aim predict journey start used allocate resource example making available taxi any tourist spot closing time 2017 ke et al 140 proposed based short term taxi demand prediction model 2018 yao et al 141 only considered local information along temporal information demand prediction traffic flow prediction traffic flow prediction one important task predict much time take flow traffic advance many interesting work ha done area using deep learning 2016 zhang et al 137 made inflow outflow tive model considering city grid 2017 et al 143 considered speed vehicle sensed via gps along highway solves problem considering highway single column vector stacking travel time estimation travel time estimation tte also interesting problem predict mated time ahead starting journey researcher 2016 siripanpornchana et al 193 2018 zhang et al 194 calculate time already predicting path start end achieve stacking known already estimated time trajectory section along path some work like 2018 jindal et al 147 2018 li et al 148 consider route origin destination unknown achieve considering every possible path two node 15 dong wang abbas computer science review 40 2021 100379 predicting traffic accident severity no doubt road dent one leading cause death injury around world canada wa found people got injured road accident 2016 chen et al 138 predicted bad traffic accident could achieve categorize injury four class predicting mode transportation task aim predict people moving rather moving 2017 jiang et al proposed 144 jectorynet solved prediction problem considering gps information gps easily available smart phone model based rnn trajectory clustering trajectory clustering also diction task task interested cluster similar route probably minimizing euclidian distance unsupervised fashion 2017 yao et al 145 2002 longest common subsequence lcss 195 vlachose et al some work direction navigation intelligent transportation system navigation one challenging task affected not only environment also personal choice road traffic namics factor make hard predict accurately tamar et al 2016 tamar et al 196 proposed planning module known value iteration work vin 2016 yang et al utilized time traverse map 146 following work yang et al 2019 li et al 152 proposed network based prediction traffic demand serving demand prediction task passenger need vehicle travel demand serving task efficiently serve demand routing vehicle car pooling one approach demand serving mechanism see work jindal et al 147 2018 traffic signal control demand serving come mand prediction anomalously traffic signal control come traffic flow prediction intelligent traffic signaling huge impact traffic jam reinforcement learning found effective field see work yau et al 197 2017 combinatorial optimization combinatorial optimization problem well researched area intelligent port system problem searched since long time famous travel sale man problem some case environment cle work may somewhat referred ple vehicle routing issue stochastic demand vrpsd vehicle routing problem stochastic tomers vrpsc real setting ideal course may likewise exist intricate capacity point essentially separation time augmentation incorporate factor example kind product sort street sort vehicle even quantity driver street impact sort intricacy calculation used play steering many approach followed till example 2015 vinyals et al 134 followed pointer network based approach 2016 bello et al 136 introduced idea training pointer network using reinforcement learning likewise specific methodology influence diagram structure see example work li et al 135 2015 take care comparative issue ongoing work khalil et al 2017 142 kool et al 149 2018 applied graph ding design activity research issue including tsp vrp among others use deep learning many library available also open source tensorflow one best tool available give some guidance use tensorflow need follow following step use tensorflow problem installing tensorflow python environment need follow step import tensorflow library use import tensorflow tf load prepare mnist dataset convert sample integer number tensorflow ha already many datasets learning purpose use mnist handwritten digit classification dataset divide datasets two part training testing dividing 255 normalize pixel intensity value varies dividing 255 value lie 0 1 mnist mnist build model stacking layer building neural network thing careful creating neural network need choosing input output dimension according dataset problem like giving input shape 28 28 single image size 28 pixel value sion final layer equal number class case 10 regression problem final layer binary classification either 1 intermediate hidden layer many want no restriction number node long best fit problem using relu activation function intermediate layer want switch node whose activation value using dropout threshold value model 28 28 128 relu 10 example model return vector logits score one class prediction model function convert logits bilities class using softmax tion final layer activation problem classification problem prediction loss take tor logits true index return scalar loss example loss function change according problem regression could route mean square many loss function available one problem tensorflow one use give best accuracy sparsecategoricalcrossentropy 16 dong wang abbas computer science review 40 2021 100379 loss equal negative log probability true class zero model sure correct class untrained model give probability close random class initial loss close prediction need compile model see correctly build also mentioning optimizer rithm adam many optimizer available gradient descent momentum base gradient descent adam accuracy method adjusts model parameter imize loss method check model performance usually want model return probability wrap trained model attach softmax model challenge deep learning lack innovation model structure since deep learning 2006 deep learning model wa mainly introduced several sical model last introduction deep learning model traditional model based evolution decade past model stacked simple model due stacking becoming difficult increase efficiency data processing however depth advantage learning technology still not fully implemented need realize development new depth learning model either current depth learning model appropriate method effective integration need solve problem update training method supervised unsupervised learning two training method current deep learning model use vised training method restricted boltzmann machine automatic encoder core model main use large number training method way unsupervised learning time combined supervised learning training learn no real sense complete unsupervised training achieve complete unsupervised training direction future study deep learning model challenge parameter learning many challenge parameter learning deep neural network listed learning rate small learning rate take long time find optimum point stick local minimum hand large learning rate may skip optimum point may never converge local optimum local minimum major problem many parameter learning objective gradient descent algorithm work taking slop current point accordingly update parameter ideal convex problem only one minimum maximum point absolute minimum found case local minimum many minimum maximum point however parameter updating using gradient descent reach local minimum point gradient value becomes zero slope any local minimum point zero therefore never update parameter called local minimum problem fig 12 present local global minimum problem saddle point saddle point minimax point graph derivative function almost zero consequence gradient descent stop updating rameters also saddle point neither minimum maximum problem generally happened many dimension present hessian matrix used determine saddle point hessian matrix square matrix second order partial derivative describes local curvature graph many dimension given point hessian indefinite point saddle point however due complexity hessian matrix not suitable neural network vanishing exploding gradient one crucial problem faced training large neural work deep neural network contain two hidden layer feature propagated final layer applying many affine transformation followed activation function consequence sometimes value gradient may become large some time becomes small former known ploding gradient latter known vanishing gradient literature reduce training time present detection various type deep learning model mostly carried ideal environment complex reality environment current technology still able achieve desired result also deep learning model composed either simple model several model complexity problem higher amount information processed significant mean need training time deep learning model change deep learning model without any flexibility hardware improve accuracy speed data processing future research deep learning technology online learning unsupervised supervised main training method today deep learning technique however online learning training requires global fine tuning cause output fallen local minimum therefore current training not conducive realization online learning improvement online learning ability based innovative deep learning model need faced 17 dong wang abbas computer science review 40 2021 100379 fig pictorial description local global minimum overcome adversarial sample input sample deliberately added subtle ence data set cause model throw wrong output high confidence however adversarial sample big problem current deep learning adding input sample not only effectively avoid potential security problem exploring overcome problem adversarial sample also help improving deep learning model solve problem precision sense fundamental contradiction creating linear model easy training creating model resist sample however development deep learning creation powerful optimization method model training direction future research field conclusion deep learning technology widely applied many field research area speech recognition image processing graph medicine computer vision one fastest developing adaptive technology ever tie presence big complex data effectively solve problem using deep learning tual process application challenging build appropriate model deep learning although current deep learning not fully matured many problem need solved deep learning ha shown strong learning ability still hot research area field future artificial intelligence paper ha discussed some classic advance deep learning application plethora field finally application deep learning presented many scientific problem solved day day sometimes unexpected better performance achieved deep learning many area image processing diabetic retinopathy diagnosis difficult diagnosed human expert fact diabetic retinopathy diagnosis nothing application image processing therefore one advance one field might breakthrough solution another field deep learning getting attention fast every day some new application invention happening according limited knowledge following active research area also keep getting attention near future 1 generative model using deep neural network 198 example generative adversarial network 2 deep learning data deep learning graph geometric deep learning 199 hyperbolic neural network 200 3 deep learning data mining 201 4 improve structure algorithm deep neural network model 202 etc declaration competing interest author declare no known competing cial interest personal relationship could appeared influence work reported paper acknowledgment author would like thank anonymous reviewer helpful comment improve technical quality paper paper supported science ogy plan project henan province grant no 192102210125 202102210379 zhoukou normal university super scientific project grant open foundation state key laboratory networking switching technology beijing university post telecommunication grant no reference 1 freedman statistical model theory practice cambridge university press 2009 2 mood logistic regression not think eur sociol rev 26 1 2010 3 kleinbaum klein analysis matched data using logistic sion logistic regression text springer 2002 pp 4 hosmer jr lemeshow sturdivant applied logistic regression vol 398 john wiley son 2013 5 soentpiet advance kernel method support vector learning mit press 1999 6 hearst dumais osuna platt scholkopf support vector machine ieee intell syst appl 13 4 1998 7 steinwart christmann support vector machine springer science business medium 2008 8 schraudolph fast curvature product gradient descent neural comput 14 7 2002 9 li encyclopedia biometrics vol 2 springer science business medium 2009 10 verbeek vlassis kröse efficient greedy learning gaussian mixture model neural comput 15 2 2003 11 hinton osindero teh fast learning algorithm deep belief net neural comput 18 7 2006 12 hebb organization behavior neuropsycholocigal theory wiley book clinical psychology 62 1949 78 13 crevier ai tumultuous history search artificial intelligence basic book 1993 14 mccarthy minsky rochester shannon proposal dartmouth summer research project artificial intelligence august 31 1955 ai mag 27 4 2006 12 18 dong wang abbas computer science review 40 2021 100379 15 lecun bottou bengio haffner learning applied document recognition proc ieee 86 11 1998 16 wang liu cheng visualizing deep neural network alternately image blurring deblurring neural netw 97 2018 17 nouiehed razaviyayn learning deep model critical point local openness 2018 arxiv preprint 18 diakonikolas kane stewart robust learning bayesian network 2016 corr 19 yun sra jadbabaie critical view global optimality deep learning 2018 arxiv preprint 20 haeffele vidal global optimality tensor factorization deep learning beyond 2015 arxiv 21 haeffele vidal global optimality neural network training proceeding ieee conference computer vision pattern recognition 2017 pp 22 srivastava hinton krizhevsky sutskever salakhutdinov dropout simple way prevent neural network overfitting mach learn 15 2014 23 mianjy arora vidal implicit bias dropout icml 2018 24 salehinejad valaee regularization method training compression deep neural network icassp 2019 2019 ieee international conference acoustic speech signal processing icassp 2019 pp 25 sengupta friston robust deep neural network 2018 arxiv 26 zheng song leung goodfellow improving robustness deep neural network via stability training 2016 ieee conference computer vision pattern recognition cvpr 2016 pp 27 giryes sapiro bronstein stability deep network 2015 corr 28 haber ruthotto stable architecture deep neural network inverse problem 34 1 2017 014004 29 malladi sharapov fastnorm improving numerical stability deep network training efficient normalization 2018 30 chang meng haber ruthotto begert holtham versible architecture arbitrarily deep residual neural network proceeding aaai conference artificial intelligence 32 1 2018 31 bengio learning deep architecture ai publisher inc 2009 32 vincent larochelle lajoie bengio manzagol bottou stacked denoising autoencoders learning useful representation deep network local denoising criterion mach learn 11 12 2010 33 fiore palmieri castiglione de santis network anomaly detection restricted boltzmann machine neurocomputing 122 2013 34 ackley hinton sejnowski learning algorithm boltzmann machine cognitive science 9 1 1985 35 ranzato susskind mnih hinton deep generative el application recognition cvpr 2011 ieee 2011 pp 36 rifai bengio courville vincent mirza disentangling factor variation facial expression recognition european conference computer vision springer 2012 pp 37 salakhutdinov hinton deep boltzmann machine artificial intelligence statistic 2009 pp 38 gu wang kuen shahroudy shuai liu wang wang cai et recent advance convolutional neural network pattern recognit 77 2018 39 hubel wiesel receptive field binocular interaction tional architecture cat visual cortex physiol 160 1 1962 106 40 krizhevsky sutskever hinton imagenet classification deep convolutional neural network nip 2012 41 zhang cui zhu deep learning graph survey ieee trans knowl data eng 2020 42 shuman narang frossard ortega vandergheynst emerging field signal processing graph extending dimensional data analysis network irregular domain ieee signal process mag 30 3 2013 43 zhou cui zhang yang liu wang li sun graph neural network review method application 2018 arxiv preprint 44 kipf welling classification graph convolutional network 2016 arxiv preprint 45 hamilton ying leskovec inductive representation learning large graph advance neural information processing system 2017 pp 46 defferrard bresson vandergheynst convolutional neural work graph fast localized spectral filtering advance neural information processing system 2016 pp 47 mohebali tahmassebi gandomi tic neural network brief overview theory implementation application handbook probabilistic model elsevier 2020 pp 48 gast roth lightweight probabilistic deep network proceeding ieee conference computer vision pattern recognition 2018 pp 49 zhang bengio hardt recht vinyals understanding deep learning requires rethinking generalization 2016 arxiv preprint arxiv 50 fan revisit fuzzy neural network bridging gap fuzzy logic deep learning tech 2017 51 deng ren kong bao dai hierarchical fused fuzzy deep neural network data classification ieee trans fuzzy syst 25 4 2016 52 zhou chen wang fuzzy deep belief network sentiment classification neurocomputing 131 2014 53 goodfellow mirza xu ozair courville bengio generative adversarial net advance neural information processing system 2014 pp 54 salimans goodfellow zaremba cheung radford chen improved technique training gans advance neural information processing system 2016 pp 55 schwenk continuous space translation model tical machine translation proceeding coling 2012 poster 2012 pp 56 dong wei zhou xu adaptive recursive neural model application sentiment analysis proceeding national conference artificial intelligence vol 2 2014 pp 57 tang wei qin liu zhou coooolll deep learning system twitter sentiment classification proceeding international workshop semantic evaluation semeval 2014 2014 pp 58 qian yu investigation bottleneck feature based robust speech recognition 2015 ieee china summit international conference signal information processing chinasip ieee 2015 pp 59 maas qi xie hannun lengerich jurafsky ng building dnn acoustic model large vocabulary speech recognition comput speech lang 41 2017 60 li cai wang zhou feng chen medical image classification convolutional neural network 2014 national conference control automation robotics vision icarcv ieee 2014 pp 61 li tran thung ji shen li robust deep model improved classification patient ieee biomed health inf 19 5 2015 62 sirinukunwattana raza tsang snead cree rajpoot locality sensitive deep learning detection classification nucleus routine colon cancer histology image ieee trans med imaging 35 5 2016 63 dou chen yu zhao qin wang mok shi heng automatic detection cerebral microbleeds mr image via convolutional neural network ieee trans med imaging 35 5 2016 64 mallik chaudhury acquisition multimedia ontology tion preservation cultural heritage int multimedia inf retr 1 4 2012 65 höft schulz behnke fast semantic segmentation scene deep neural network joint conference artificial intelligence künstliche intelligenz springer 2014 pp 66 sun wang tang deep learning face representation ing 10 000 class proceeding ieee conference computer vision pattern recognition 2014 pp 67 pinheiro collobert recurrent convolutional neural network scene labeling international conference machine learning 2014 pp 68 taigman yang ranzato wolf deepface closing gap performance face verification proceeding ieee conference computer vision pattern recognition 2014 pp 69 long shelhamer darrell fully convolutional network semantic segmentation proceeding ieee conference computer vision pattern recognition 2015 pp 19 dong wang abbas computer science review 40 2021 100379 70 schroff kalenichenko philbin facenet unified embedding face recognition clustering proceeding ieee conference computer vision pattern recognition 2015 pp 71 wang chen yuan liu huang hou cottrell standing convolution semantic segmentation 2018 ieee winter conference application computer vision wacv ieee 2018 pp 72 zheng jayasumana vineet su du huang torr conditional random field recurrent neural network proceeding ieee international conference computer vision 2015 pp 73 ronneberger fischer brox convolutional network biomedical image segmentation international conference medical image computing intervention springer 2015 pp 74 badrinarayanan handa cipolla segnet deep convolutional architecture robust semantic labelling 2015 arxiv preprint 75 liu li luo loy tang semantic image segmentation via deep parsing network proceeding ieee international conference computer vision 2015 pp 76 byeon breuel raue liwicki scene labeling lstm recurrent neural network proceeding ieee conference computer vision pattern recognition 2015 pp 77 lin shen van den hengel reid efficient piecewise training deep structured model semantic segmentation proceeding ieee conference computer vision pattern recognition 2016 pp 78 shen zeng fast semantic image segmentation high order context guided filtering 2016 arxiv preprint 79 chandra kokkinos fast exact inference semantic image segmentation deep gaussian crfs european conference computer vision springer 2016 pp 80 luc couprie chintala verbeek semantic segmentation using adversarial network 2016 arxiv preprint 81 hoffman wang yu darrell fcns wild adversarial adaptation 2016 arxiv preprint arxiv 82 shuai zuo wang wang neural network scene labeling proceeding ieee conference computer vision pattern recognition 2016 pp 83 zhang ren sun deep residual learning image recognition proceeding ieee conference computer vision pattern recognition 2016 pp 84 chen papandreou schroff adam rethinking atrous volution semantic image segmentation 2017 arxiv preprint arxiv 85 koziński simon jurie adversarial regularisation supervised training structured output neural network 2017 arxiv preprint 86 chen papandreou kokkinos murphy yuille deeplab semantic image segmentation deep convolutional net atrous volution fully connected crfs ieee trans pattern anal mach intell 40 4 2017 87 souly concetto mubarak semi weakly supervised semantic segmentation using generative adversarial network 2017 arxiv preprint 88 yu koltun context aggregation dilated convolution 2015 arxiv preprint 89 teichmann cipolla convolutional crfs semantic tion 2018 arxiv preprint 90 simonyan zisserman deep convolutional network scale image recognition bengio lecun ed international conference learning representation iclr 2015 san diego ca usa may 2015 conference track proceeding 2015 91 sermanet eigen zhang mathieu fergus lecun overfeat integrated recognition localization detection using convolutional network bengio lecun ed international conference learning representation iclr 2014 banff ab canada april 2014 conference track proceeding 2014 92 russakovsky deng su krause satheesh huang karpathy khosla bernstein et imagenet large scale visual recognition challenge int comput vi 115 3 2015 93 chatfield arandjelović parkhi zisserman ing visual search image video datasets int multimedia inf retr 4 2 2015 94 pi nath behzadan convolutional neural network object detection aerial imagery disaster response recovery adv eng inform 43 2020 101009 95 gu ge chen luo coatrieux automatic robust object detection baggage inspection using deep convolutional neural network ieee transaction industrial electronics 2020 http 96 caelles maninis cremers van gool video object segmentation proceeding ieee conference computer vision pattern recognition 2017 pp 97 shin yoon rameau kim lee shin kweon matching video object segmentation using convolutional neural work proceeding ieee international conference computer vision 2017 pp 98 jang kim online video object segmentation via convolutional trident network proceeding ieee conference computer vision pattern recognition 2017 pp 99 hu huang schwing maskrnn instance level video object segmentation advance neural information processing system 2017 pp 100 sasikumar investigating application deep convolutional neural network video object segmentation technological university dublin 2018 101 li jiang fang huang zhao deep video foreground target extraction complex scene 2018 international conference sensor network signal processing snsp ieee 2018 pp 102 xiao feng lin liu zhang monet deep motion exploitation video object segmentation proceeding ieee conference computer vision pattern recognition 2018 pp 103 goel weng poupart unsupervised video object segmentation deep reinforcement learning advance neural information processing system 2018 pp 104 schofield mehta stonham system counting people video image using neural network identify background scene pattern recognit 29 8 1996 105 tavakkoli segmentation video sequence using neural network intelligent system neural network application 2005 106 culibrk marque socek kalva furht neural network proach bayesian background modeling video object segmentation visapp 1 2006 pp 107 maddalena petrosino approach detection moving pattern application international symposium brain vision artificial intelligence springer 2007 pp 108 retinotopic map applied background modeling dynamic object segmentation video sequence 2013 international joint conference neural network ijcnn ieee 2013 pp 109 guo qi restricted boltzmann machine ground modeling subtraction 2013 international conference machine learning application vol 1 ieee 2013 pp 110 xu ye li liu yang ding dynamic background learning deep network proceeding acm international conference multimedia 2014 pp 111 xu ye liu li pei ding motion detection via couple network 2014 ieee international conference multimedia expo icme ieee 2014 pp 112 ral system dynamic object detection normal complex scenario pattern recognit 48 4 2015 113 qu yu fu motion background modeling based encoder 2016 third international conference artificial intelligence pattern recognition aipr ieee 2016 pp 114 minematsu shimada uchiyama taniguchi analytics deep neural background subtraction imaging 4 6 2018 78 115 ammar bouwmans zaghden neji moving object mentation based deepsphere video surveillance international symposium visual computing springer 2019 pp 116 sultana mahmood bouwmans jung unsupervised versarial learning dynamic background modeling international workshop frontier computer vision springer 2020 pp 117 duvenaud maclaurin iparraguirre bombarell hirzel adam convolutional network graph learning molecular fingerprint advance neural information processing system 2015 pp 118 kearnes mccloskey berndl pande riley molecular graph convolution moving beyond fingerprint comput aided mol de 30 8 2016 119 berg kipf welling graph convolutional matrix completion 2017 arxiv preprint 120 monti bronstein bresson geometric matrix completion current neural network advance neural information processing system 2017 pp 20 dong wang abbas computer science review 40 2021 100379 121 gilmer schoenholz riley vinyals dahl neural message passing quantum chemistry 2017 arxiv preprint 122 coley barzilay green jaakkola jensen tional embedding attributed molecular graph physical property prediction chem inf model 57 8 2017 123 ktena parisot ferrante rajchl lee glocker rueckert distance metric learning using graph convolutional network application functional brain network international conference medical image computing intervention springer 2017 pp 124 parisot ktena ferrante lee moreno glocker rueckert spectral graph convolution disease prediction international conference medical image computing intervention springer 2017 pp 125 parisot ktena ferrante lee guerrero glocker ert disease prediction using graph convolutional network application autism spectrum disorder alzheimer disease med image anal 48 2018 126 qiu tang dong wang tang deepinf social influence prediction deep learning proceeding acm sigkdd international conference knowledge discovery data mining 2018 pp 127 ying chen eksombatchai hamilton leskovec graph convolutional neural network recommender tems proceeding acm sigkdd international conference knowledge discovery data mining 2018 pp 128 liu ying pande leskovec graph convolutional policy network molecular graph generation advance neural information processing system 2018 pp 129 de cao kipf molgan implicit generative model small molecular graph 2018 arxiv preprint 130 zitnik agrawal leskovec modeling polypharmacy side fects graph convolutional network bioinformatics 34 13 2018 131 xie grossman crystal graph convolutional neural network accurate interpretable prediction material property phys rev lett 120 14 2018 145301 132 gama ferreira dama predicting demand using streaming data ieee trans intell transp syst 14 3 2013 133 de brébisson simon auvolat vincent bengio artificial neural network applied taxi destination prediction 2015 arxiv preprint 134 vinyals fortunato jaitly pointer network advance neural information processing system 2015 pp 135 li tarlow brockschmidt zemel gated graph sequence neural network 2015 arxiv preprint 136 bello pham le norouzi bengio neural combinatorial optimization reinforcement learning 2016 arxiv preprint arxiv 137 zhang zheng qi deep residual network citywide crowd flow prediction 2016 arxiv preprint 138 chen song yamada shibasaki learning deep representation big heterogeneous data traffic accident inference thirtieth aaai conference artificial intelligence 2016 139 endo nishida toda sawada predicting destination partial trajectory using recurrent neural network ference knowledge discovery data mining springer 2017 pp 140 ke zheng yang chen forecasting passenger demand ride service deep learning approach transp c 85 2017 141 yao wu ke tang jia lu gong ye li deep network taxi demand prediction 2018 arxiv preprint 142 khalil dai zhang dilkina song learning combinatorial optimization algorithm graph advance neural information processing system 2017 pp 143 dai wang wang learning traffic image deep convolutional neural network transportation network speed prediction sensor 17 4 2017 818 144 jiang de souza pesaranghader hu silver matwin trajectorynet embedded gps trajectory representation classification using recurrent neural network 2017 arxiv preprint arxiv 145 yao zhang zhu huang bi trajectory clustering via deep representation learning 2017 international joint conference neural network ijcnn ieee 2017 pp 146 yang li wang liu yang learning urban navigation via value iteration network 2018 ieee intelligent vehicle symposium iv ieee 2018 pp 147 jindal qin chen nokleby ye optimizing taxi carpool policy via reinforcement learning mining 2018 ieee international conference big data big data ieee 2018 pp 148 li fu wang shahabi ye liu representation learning travel time estimation proceeding acm sigkdd international conference knowledge discovery data mining 2018 pp 149 kool van hoof welling attention learn solve routing problem 2018 arxiv preprint 150 lv li sun wang convolutional neural network taxi trajectory prediction 2018 ieee international conference big data smart computing bigcomp ieee 2018 pp 151 yuan xiong wang acm adaptive graph tional neural network scene recognition proceeding aaai conference artificial intelligence vol 33 2019 pp 152 li fu yuan zhang chen yang yang traffic prediction enabled double rewarded value iteration network route planning ieee trans veh technol 68 5 2019 153 tu chen yuille zhu image parsing unifying segmentation detection recognition int comput vi 63 2 2005 154 pavlidis fundamental picture segmentation structural pattern recognition springer 1977 pp 155 coleman andrew image segmentation clustering proc ieee 67 5 1979 156 geman geman stochastic relaxation gibbs distribution bayesian restoration image ieee trans pattern anal mach intell 6 1984 157 zhang survey evaluation method image segmentation pattern recogn 29 8 1996 158 narkhede review image segmentation technique int sci modern eng 1 8 2013 159 kaur kaur various image segmentation technique review int comput sci mobile comput 3 5 2014 160 kuruvilla sukumaran sankar joy review image processing image segmentation 2016 international conference data mining advanced computing sapience 2016 pp 161 huang lee learning hierarchical tions face verification convolutional deep belief network 2012 ieee conference computer vision pattern recognition ieee 2012 pp 162 fischler elschlager representation matching pictorial structure ieee trans comput 100 1 1973 163 liu ouyang wang fieguth chen liu pietikäinen deep learning generic object detection survey int comput vi 128 2 2020 164 sultana sufian dutta review object detection model based convolutional neural network intelligent computing image processing based application springer 2020 pp 165 ren girshick sun faster towards object detection region proposal network advance neural information processing system 2015 pp 166 dai li sun object detection via fully convolutional network advance neural information processing system 2016 pp 167 hinton salakhutdinov reducing dimensionality data neural network science 313 5786 2006 168 lecun bengio hinton deep learning nature 521 7553 2015 169 dollar wojek schiele perona pedestrian detection evaluation state art ieee trans pattern anal mach intell 34 4 2011 170 enzweiler gavrila monocular pedestrian detection survey experiment ieee trans pattern anal mach intell 31 12 2008 171 geronimo lopez sappa graf survey pedestrian detection advanced driver assistance system ieee trans pattern anal mach intell 32 7 2009 172 sun bebis miller vehicle detection review ieee trans pattern anal mach intell 28 5 2006 173 sakhare tewari vyas review vehicle detection system advanced driver assistant system arch comput method eng 27 2 2020 174 yuan zhang liu vehicle detection based area proportion prior sensor network signal processing springer 2020 pp 175 zafeiriou zhang zhang survey face detection wild past present future comput vi image underst 138 2015 21 dong wang abbas computer science review 40 2021 100379 176 masi wu hassner natarajan deep face recognition survey 2018 sibgrapi conference graphic pattern image sibgrapi ieee 2018 pp 177 zeng veldhuis spreeuwers survey face recognition technique occlusion 2020 arxiv preprint 178 zhao zheng xu wu object detection deep learning review ieee trans neural netw learn syst 30 11 2019 179 cane ferryman evaluating deep semantic segmentation network object detection maritime surveillance 2018 ieee international conference advanced video signal based surveillance av ieee 2018 pp 180 girshick donahue darrell malik rich feature hierarchy accurate object detection semantic segmentation proceeding ieee conference computer vision pattern recognition 2014 pp 181 girshick fast proceeding ieee international conference computer vision 2015 pp 182 learning technique tic graphical model review acta automat sinica 40 6 2014 183 bouwmans traditional recent approach background modeling foreground detection overview comput sci rev 11 2014 184 bouwmans sobral javed jung zahzah decomposition plus additive matrix tion review comparative evaluation dataset comp sci rev 23 2017 185 bouwmans silva background subtraction real application challenge current model future direction comp sci rev 35 2020 100204 186 bouwmans zahzah robust pca via principal component pursuit review comparative evaluation video surveillance comput vi image underst 122 2014 187 javed oh sobral bouwmans jung mrf robust foreground detection highly dynamic background asian conference computer vision springer 2014 pp 188 xu li wang chen temporally adaptive restricted boltzmann machine background modeling aaai conference artificial intelligence 2015 189 bouwmans javed sultana jung deep neural network cepts background subtraction systematic review comparative evaluation neural netw 117 2019 190 assouel ahmed segler saffari bengio defactor ferentiable edge probabilistic graph generation 2018 arxiv preprint 191 zhang wang wang lin xu chen intelligent transportation system survey ieee trans intell transp syst 12 4 2011 192 veres moussa deep learning intelligent transportation tems survey emerging trend ieee transaction intelligent transportation system 21 8 2019 193 siripanpornchana panichpapiboon chaovalit tion deep learning 2016 ieee region 10 conference tencon ieee 2016 pp 194 zhang wu sun zheng deeptravel neural network based travel time estimation model auxiliary supervision 2018 arxiv preprint 195 vlachos kollios gunopulos discovering similar sional trajectory proceeding international conference data engineering ieee 2002 pp 196 tamar wu thomas levine abbeel value iteration network advance neural information processing system 2016 pp 197 yau qadir khoo ling komisarczuk survey reinforcement learning model algorithm traffic signal control acm comput surv 50 3 2017 198 salakhutdinov learning deep generative model annu rev stat appl 2 2015 199 masci rodolà boscaini bronstein li geometric deep learning siggraph asia 2016 course 2016 pp 200 ganea bécigneul hofmann hyperbolic neural network arxiv preprint 201 wang cao yu deep learning data mining survey ieee transaction knowledge data engineering 2020 202 leskovec xie graph structure neural network iii singh ed proceeding international conference machine learning proceeding machine learning research 119 pmlr 2020 pp html 22