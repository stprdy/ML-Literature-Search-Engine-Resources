sip vol page author onlin version thi articl publish within open access environ subject condit creativ common attribut licenc http overview paper tutori survey architectur algorithm applic deep learn li deng thi invit paper overview materi topic present plenari overview session tutori materi present confer expand updat includ recent develop deep learn previou updat materi cover theori applic analyz futur direct goal thi tutori survey introduc emerg area deep learn hierarch learn apsipa commun deep learn refer class machin learn techniqu develop larg sinc mani stage inform process hierarch architectur exploit pattern classif featur learn recent literatur also connect represent learn involv hierarchi featur concept level concept defin one concept help defin one thi tutori survey brief histori deep learn research discuss first classificatori scheme develop analyz summar major work report recent deep learn literatur use thi scheme provid survey exist deep architectur algorithm literatur categor three class gener discrimin hybrid three repres deep architectur deep autoencod deep stack network gener tempor domain recurr network deep neural network pretrain deep belief network one three class present detail next select applic deep learn review broad area signal inform process includ multimod languag model natur languag process inform retriev final futur direct deep learn discuss analyz keyword deep learn algorithm inform process receiv februari revis decemb n r u c n research nowaday ha significantli widen scope compar year ago ha encompass mani broad area inform ing signal semant inform sinc deep learn recent refer represent learn ha emerg new area machin learn research within past year techniqu develop deep learn research alreadi impact wide rang work within tradit new widen scope includ machin learn artifici intellig see recent new york time media coverag thi progress seri workshop tutori special issu confer special session devot sive deep learn applic variou sical expand area includ microsoft research redmond wa usa phone correspond author deng email deng intern confer learn tation icassp special session new type deep neural network learn speech recognit relat applic icml workshop audio speech languag process nip workshop deep learn unsupervis featur learn icml workshop tion learn challeng intern conf learn represent icml workshop represent learn icml workshop learn architectur represent optim speech visual inform process icml workshop ing featur hierarchi nip workshop deep learn speech recognit relat applic icassp deep learn tutori special section deep learn speech languag process ieee tran audio speech languag process januari special issu learn deep architectur ieee tran pattern analysi machin intellig author ha directli involv research organ sever event editori abov ha seen emerg natur field henc need provid tutori survey articl http publish onlin cambridg univers press li deng deep learn refer class machin learn techniqu mani layer stage hierarch architectur exploit tern classif featur represent ing intersect among research area neural network graphic model optim pattern recognit signal process three import son popular deep learn today drastic increas chip process abil gpu unit nificantli lower cost comput hardwar recent advanc machin learn process research activ research thi area includ univers toronto new york univers versiti montreal microsoft research googl ibm research baidu facebook stanford univers siti michigan mit univers washington numer place research strate success deep learn divers applic comput vision phonet recognit voic search convers speech recognit speech imag ture code semant utter classif recognit audio process visual object recognit inform retriev even analysi molecul may lead discov new drug report recent thi paper expand recent overview materi topic present plenari overview session well tutori materi present confer aim introduc apsipa transact reader emerg technolog enabl deep learn attempt provid tutori review research work conduct thi excit area sinc birth deep learn ha direct vanc signal inform process futur research direct discuss attract interest apsipa research student practition ing signal technolog core mission apsipa commun remaind thi paper organ follow section ii brief histor account deep learn provid perspect signal inform process section iii classif scheme larg bodi work deep learn develop grow number deep architectur classifi gener discrimin hybrid egori descript provid categori section three categori rial exampl chosen provid detail ment exampl chosen deep autoencod gener categori section iv dnn train dbn hybrid categori section v deep stack network dsn relat cial version recurr neural network rnn discrimin categori section vi section vii set typic success applic deep learn divers area signal inform process review section viii summari futur direct given b r e f h r c l c c u n f e e p l e r n n g recent machin learn techniqu exploit architectur architectur typic contain singl layer linear featur transform lack multipl layer adapt featur exampl low architectur convent commonli use sian mixtur model gmm hidden markov model hmm linear dynam system tional random field crf maximum entropi maxent model support vector machin svm logist sion kernel regress perceptron mlp neural network singl hidden layer includ extrem learn machin properti common shallow learn model rel simpl architectur consist onli one layer respons transform raw input signal featur featur space may unobserv take exampl svm convent kernel method use shallow linear pattern separ model one zero featur transform layer kernel trick use otherwis notabl except recent kernel od inspir integr deep learn shallow architectur shown effect solv mani simpl problem limit model represent power caus difficulti deal plicat applic involv natur signal human speech natur sound languag natur imag visual scene human mechan vision speech howev suggest need deep architectur extract complex structur ing intern represent rich sensori input exampl human speech product percept tem equip clearli layer hierarch structur transform inform form level linguist level similar vein human visual system also hierarch natur percept side interestingli also gener side natur believ art advanc process type natur signal effici effect deep learn algorithm develop learn tem deep architectur compos mani layer process stage lower layer output fed immedi higher layer input success deep learn techniqu develop far share two addit key properti gener natur model typic requir ad addit http publish onlin cambridg univers press tutori survey architectur algorithm applic deep learn top layer perform discrimin task vise pretrain step make effect use larg amount unlabel train data extract structur regular input featur histor concept deep learn wa origin artifici neural network research henc one may occasion hear discuss ral network neural network mlp mani hidden layer inde good exampl el deep architectur backpropag popular ha algorithm learn weight network unfortun backpropag alon work well practic learn network small number hidden layer see review analysi pervas presenc local optima object function deep network main sourc difficulti ing backpropag base local gradient descent start usual random initi point often get trap poor local optima sever increas significantli depth network increas thi ficulti partial respons steer away machin learn research ral network shallow model convex loss tion svm crf maxent model global optimum effici obtain cost less power model optim difficulti associ deep model wa empir allevi reason cient unsupervis learn algorithm wa introduc two paper paper class deep gener model wa introduc call deep belief work dbn compos stack restrict boltzmann machin rbm core compon dbn greedi learn algorithm optim dbn weight time complex linear size depth network separ surpris initi weight mlp spondingli configur dbn often produc much better result random weight mlp mani hidden layer deep neural network dnn learn unsupervis dbn pretrain low backpropag sometim also call dbn literatur recent research care distinguish dnn dbn dbn use ize train dnn result network call addit suppli good initi point dbn come addit attract featur first learn algorithm make effect use unlabel data second interpret bayesian probabilist er model third valu hidden variabl deepest layer effici comput fourth overfit problem often observ model million paramet dbn fit problem occur often deep network effect address gener pretrain step insight analysi speech inform dbn captur provid procedur onli one make effect train dnn possibl sinc licat semin work number research improv appli deep learn techniqu success exampl one altern pretrain dnn layer layer consid pair layer denois autoencod regular set subset input zero also tractiv autoencod use purpos regular via penal gradient activ hidden unit respect input ranzato et al develop spars encod ric machin sesm ha veri similar architectur rbm build block dbn principl sesm may also use effect initi dnn train histor use gener model dbn facilit train dnn play import role ignit interest deep learn speech featur ing speech recognit thi effect wa demonstr research show mani altern simpler way pretrain larg amount train data know learn dnn start shallow neural network one hidden layer thi shallow network ha train discrimin new hidden layer insert previou hidden layer softmax output layer full network discrimin train one continu thi process desir number hidden layer reach dnn final full backpropag carri complet dnn train train data ful weight initi abov process discrimin pretrain remov also effect dnn train next section overview provid iou architectur deep learn includ beyond origin dbn publish h r e e b r c l e f e e p r c h e c u r e n v e r v e w describ earlier deep learn refer rather wide class machin learn techniqu ture hallmark use mani layer linear stage hierarch natur depend architectur techniqu intend use one broadli categor work thi area three main class gener deep architectur intend character correl properti observ visibl data pattern analysi synthesi purpos character joint statist bution visibl data associ class http publish onlin cambridg univers press li deng latter case use bay rule turn thi type architectur discrimin one discrimin deep architectur intend directli provid discrimin power pattern sific often character posterior tion class condit visibl data hybrid deep architectur goal nation assist often signific way outcom gener architectur via better mizat regular discrimin criteria use learn paramet ani deep gener model categori abov note use hybrid abov differ use sometim literatur refer hybrid pipelin system speech recognit feed output probabl neural network hmm machin learn tradit may ural use classif scheme accord discrimin learn neural network versu deep probabilist gener learn dbn dbm thi classif scheme howev miss key insight gain deep learn research gener model greatli improv learn dnn deep discrimin model via better optim izat also deep gener model may necessarili need probabilist deep autoencod ertheless classif point import differ dnn deep probabilist model former usual effici train ing flexibl construct less constrain normal difficult partit function replac sparsiti suitabl learn complex system approxim infer learn latter hand easier interpret emb domain knowledg ier compos handl uncertainti typic intract infer learn complex system thi distinct howev retain also propos classif adopt throughout thi paper briefli review repres work abov three class sever basic definit use summar tabl applic deep architectur defer section vii gener architectur associ thi gener categori often see unsupervis featur learn sinc label data concern appli gener architectur pattern recognit supervis learn key cept unsupervis pretrain thi concept aris need learn deep network learn lower level network difficult especi train data limit therefor desir learn lower layer without reli layer abov learn layer greedi manner bottom thi gist pretrain befor subsequ learn layer togeth among variou subclass gener deep tectur deep model includ coder common origin form deep autoencod give detail section iv typic exampl gener model categori form deep autoencod also gener natur quit differ properti implement exampl transform autoencod predict spars coder stack version denois autoencod stack version specif denois autoencod input tor first corrupt random percentag input set zero one design hidden encod node reconstruct origin rupt input data use criteria kl distanc origin input reconstruct input rupt encod represent use input next level stack denois autoencod anoth promin type gener model deep boltzmann machin dbm dbm contain mani layer hidden variabl ha connect variabl within layer thi cial case gener boltzmann machin bm network symmetr connect unit make stochast decis whether veri simpl learn algorithm gener bm veri complex studi veri slow comput learn dbm layer captur complic correl activ hidden featur layer dbm potenti learn intern represent becom increasingli complex highli desir solv object speech recognit lem furthermor represent built larg suppli unlabel sensori input veri limit label data use onli slightli model specif task hand number hidden layer dbm reduc one rbm like dbm connect main virtu rbm via compos mani rbm mani hidden layer learn effici use featur activ one rbm train data next composit lead dbn describ detail togeth rbm section standard dbn ha extend factor bm bottom layer strong result phone recognit obtain thi model call rbm mcrbm recogn tion standard rbm abil repres covari structur data howev veri cult train mcrbm use higher level deep architectur furthermor strong result lish easi reproduc architectur mcrbm paramet full dbn easi use discrimin inform http publish onlin cambridg univers press tutori survey architectur algorithm applic deep learn tabl basic deep learn terminolog deep learn class machin learn techniqu mani layer process stage hierarch architectur exploit unsupervis featur learn pattern essenc deep learn comput hierarch featur represent observ data featur factor defin one deep belief network dbn probabilist gener model compos multipl layer stochast hidden variabl top two layer undirect symmetr connect lower layer receiv direct connect layer abov boltzmann machin bm network symmetr connect unit make stochast decis whether restrict boltzmann machin rbm special bm consist layer visibl unit layer hidden unit connect deep boltzmann machin dbm special bm hidden unit organ deep layer manner onli adjac layer connect hidden connect within layer deep neural network dnn multilay network mani hidden layer whose weight fulli connect often initi pretrain use stack rbm dbn literatur dbn sometim use mean dnn deep dnn whose output target data input often pretrain dbn use distort train data regular learn distribut represent represent observ data way ele gener interact mani hidden factor particular factor learn configur factor often gener well distribut represent form basi deep learn regular rbm higher layer howev recent work show better featur use cepstral speech featur subject linear discrimin analysi fmllr transform mcrbm need covari transform data alreadi model anoth repres deep gener architectur network spn spn direct acycl graph data leav sum product oper intern node deep architectur sum node give mixtur model product node build featur hierarchi erti complet consist constrain spn desir way learn spn carri use em algorithm togeth tion learn procedur start dens spn find spn structur learn weight zero weight remov connect main ficulti learn found common one learn signal gradient quickli dilut propag deep layer empir solut found mitig thi difficulti report wa point despit mani desir tive properti spn difficult fine tune weight use discrimin inform limit effect classif task thi difficulti ha overcom subsequ work report effici discrimin train algorithm spn wa present wa point standard gradient descent comput deriv condit likelihood suffer dient diffus problem well known regular deep network margin infer replac ring probabl state hidden variabl hard gradient descent reliabl estim deep spn weight excel result imag tion task report rnn regard class deep gener tectur use model gener tial data depth rnn larg length input data sequenc rnn veri er model sequenc data speech text recent wide use partli becaus extrem difficult train properli due known vanish gradient problem recent advanc optim partial overcom thi difficulti use inform stochast vatur estim recent work rnn train optim use tive deep architectur languag ele lm task gate connect introduc allow current input charact predict tion one latent state vector next gener rnn model demonstr well capabl ate sequenti text charact recent bengio et al sutskev explor new optim method train gener rnn modifi tic gradient descent show modif perform optim method mikolov et al report excel result use rnn lm recent mesnil et al report success rnn spoken languag understand exampl differ type gener deep el ha long histori speech recognit research human speech product mechan exploit construct dynam deep structur abilist gener model comprehens review see book specif earli work describ gener extend convent shallow http publish onlin cambridg univers press li deng condit independ hmm structur impos dynam constraint form polynomi trajectori hmm paramet variant thi approach ha recent develop use differ learn niqu hmm paramet applic extend speech recognit robust similar trajectori hmm also form basi metric speech synthesi subsequ work ad new hidden layer dynam model itli account erti human speech gener effici implement thi deep architectur den dynam achiev fir filter recent studi abov gener model speech shown special case gener dynam bayesian network model even gener dynam graphic model graphic model compris mani hidden layer acter complex relationship variabl speech gener arm power graphic ing tool deep architectur speech ha recent success appli solv veri difficult problem speech recognit mix speech visibl variabl mix speech becom repres new hidden layer deep gener architectur deep tive graphic model inde power tool mani applic due capabl embed domain knowledg howev addit weak use represent classif gori also often implement ate approxim infer learn predict topolog design aris inher intract task applic thi problem ha partli address recent work provid interest direct make deep tive graphic model potenti use practic futur standard statist method use speech recognit understand combin shallow hmm speech acoust higher layer structur repres differ level natur languag hierarchi thi combin hierarch model suitabl regard deep gener architectur whose motiv technic detail may found chapter recent book hierarch hmm hhmm relat model greater technic depth ical treatment found hhmm layer hmm earli deep model formul direct graphic model miss key aspect tribut represent embodi recent deep gener architectur dbn dbm discuss earlier thi section final tempor recurs deep gener el found human motion model natur languag natur scene pars latter model particularli interest becaus ing algorithm capabl automat determin optim model structur thi contrast deep architectur dbn onli paramet learn architectur need predefin specif report recurs structur monli found natur scene imag natur guag sentenc discov use structur predict architectur onli unit tain imag sentenc identifi way unit interact form whole b discrimin architectur mani discrimin techniqu signal mation process appli shallow architectur hmm crf sinc crf defin condit probabl input data well output label intrins shallow discrimin architectur interest lenc crf discrimin train gaussian model hmm found recent crf develop stack output lower layer crf togeth origin input data onto higher layer ou version crf use appli phone recognit spoken languag tion natur languag process howev least phone recognit task perform crf pure discrimin ha abl match hybrid approach involv dbn take shortli recent articl give excel review major exist discrimin model speech nition base mainli tradit neural network mlp architectur use backpropag learn dom initi argu import increas width layer neural network increas depth particular class dnn model form basi popular tandem approach discrimin learn neural network develop context comput discrimin emiss iti hmm repres recent work thi area see tandem approach gener discrimin featur hmm use tie one hidden layer neural network variou way inform combin regard form discrimin deep architectur recent work new deep learn architectur sometim call dsn togeth tensor variant kernel version develop focu discrimin scalabl paralleliz learn reli littl gener compon describ thi type discrimin deep architectur detail section rnn success use gener model output taken predict input data http publish onlin cambridg univers press tutori survey architectur algorithm applic deep learn futur discuss preced subsect see also neural predict model mechan also use discrimin model output explicit label sequenc associ input data sequenc note discrimin rnn appli speech long time ago limit success train rnn discrimin preseg train data typic requir also need transform output label sequenc highli desir remov requir especi costli presegment train data often separ hmm use automat segment sequenc ing train transform rnn classif result label sequenc howev use hmm purpos doe take advantag full potenti rnn interest method wa propos enabl rnn themselv perform sequenc ficat remov need preseg train data output underli thi method idea interpret rnn output condit distribut possibl label sequenc given input sequenc differenti tive function deriv optim condit distribut correct label sequenc segment data requir anoth type discrimin deep architectur volut neural network cnn modul sist convolut layer pool layer modul often stack one top anoth dnn top form deep model convolut layer share mani weight pool layer subsampl output convolut layer reduc data rate layer weight share convolut layer togeth ate chosen pool scheme endow cnn invari properti translat invari ha argu limit invari adequ complex pattern recognit task principl way handl wider rang anc need nevertheless cnn ha found highli effect commonli use comput vision imag recognit recent appropri chang cnn design imag analysi take account tie cnn also found effect speech recognit discuss applic detail section vii use point neural network tdnn develop earli speech recognit special case cnn weight share limit one two dimens time dimens wa recent research discov time wrong dimens impos invari frequenc dimens effect share weight ing output analysi underli reason provid togeth new egi design cnn pool layer demonstr effect nearli previou cnn phone recognit also use point model chical tempor memori htm anoth variant extens cnn extens includ follow aspect time tempor dimens introduc serv supervis inform crimin even static imag inform flow use instead cnn bayesian probabilist formal use fuse inform decis make final learn architectur develop speech recognit propos develop sinc notabl use techniqu also categor discrimin deep architectur categori intent mechan thi architectur ize joint probabl data recognit target speech attribut phone word current implement thi approach base multipl layer neural network use tion learn one intermedi neural network layer implement thi framework explicitli repres speech attribut plifi entiti atom unit speech develop earli work simplif lie remov tempor overlap properti speech attribut featur ding realist properti futur work expect improv accuraci speech recognit c hybrid architectur term hybrid thi third categori refer deep architectur either compris make use gener discrimin model compon mani exist hybrid architectur publish atur gener compon exploit help discrimin final goal hybrid architectur whi gener ing help discrimin examin two viewpoint optim viewpoint gener model provid excel initi point highli linear paramet estim problem commonli use term pretrain deep learn ha introduc thi reason regular perspect gener el effect control complex overal model studi report provid insight analysi experiment evid support point abov http publish onlin cambridg univers press li deng gener deep architectur dbn discuss section subject discrimin train use backprop commonli call eratur obtain equival architectur dnn weight dnn pretrain stack rbm dbn instead usual random tion see detail explan equival relationship use often confus ogi review detail dnn context pretrain well interfac commonli use shallow gener architectur hmm section iv anoth exampl hybrid deep architectur develop gener dbn use initi dnn weight fine tune carri use discrimin mation error criterion level one thi combin static dnn shallow discrimin architectur crf overal architectur learn use discrimin criterion condit probabl full label sequenc given input sequenc data shown equival hybrid deep architectur dnn hmm whose eter learn jointli use imum mutual inform mmi entir label sequenc input vector sequenc close relat train method carri success shallow neural network deep one use point connect abov hybrid discrimin train highli lar minimum phone error mpe train techniqu hmm iter mpe train procedur use extend initi hmm paramet arbitrari one commonli use initi eter set train gener use algorithm maximum likelihood furthermor polat term take valu gener train hmm paramet need extend updat formula may analog fine tune dnn train discuss earlier ha similar spirit dbn pretrain hybrid dnn learn along line use discrimin criteria train paramet gener model abov hmm train exampl briefli discuss method appli learn gener architectur gener model rbm learn use nativ criterion posterior probabl label vector concaten input data tor form overal visibl layer rbm thi way rbm consid solut classif problem author deriv nativ learn algorithm rbm shallow gener model recent work deep er model dbn gate mrf lowest level learn featur extract nition difficult imag class includ occlus gener abil dbn model facilit eri inform captur lost level represent deep model demonstr relat work use discrimin rion empir risk train deep graphic model found exampl hybrid deep architectur use gener model dbn train deep convolut neural network deep dnn like dnn cuss earlier dbn pretrain also shown improv discrimin deep cnn random initi final exampl given hybrid deep tectur base idea work one task discrimin speech recognit produc output text serv input second task discrimin machin translat overal system give function speech translat translat speech one languag text anoth languag deep architectur consist gener discrimin element model speech nition hmm machin translat phrasal map align gener natur paramet learn tion framework describ enabl perform optim overal deep architectur use unifi learn framework initi publish thi hybrid deep learn approach appli onli speech translat also possibl task speech inform retriev speech understand lingual understand retriev etc briefli survey wide rang work three class deep architectur abov follow three section elabor three promin el deep learn one three class ideal repres influenti architectur give state art perform chosen three familiar respons develop may serv tutori purpos well simplic tural mathemat descript three ture describ follow three section may interpret repres influenti work three class exampl categori gener architectur highli complex deep tectur gener train method develop describ beyond scope thi rial perform quit well imag recognit likewis categori discrimin architectur even complex architectur learn describ kingsburi et al seid et al yan et al gave state art perform speech recognit http publish onlin cambridg univers press tutori survey architectur algorithm applic deep learn v g e n e r v e r c h e c u r e e e p u e n c e r introduct deep autoencod special type dnn whose output data input use learn effici encod dimension reduct set data specif featur extract method involv class label henc gener autoencod use three layer neural network input layer data effici code pixel imag spectra speech one consider smaller hidden layer form encod output layer neuron ha mean input layer number hidden layer greater one autoencod consid deep autoencod often train use one mani backpropag variant conjug gradient method steepest descent etc though often reason effect fundament problem use gation train network mani hidden layer onc error get backpropag first layer becom minuscul quit ineffect thi caus network almost alway learn reconstruct averag train data though advanc agat method conjug gradient method help thi degre still result veri slow learn poor solut thi problem remedi use tial weight approxim final solut process find initi weight often call pretrain success pretrain techniqu develop train deep autoencod involv treat bore set two layer rbm pretrain approxim good solut use tion techniqu minim code error thi train techniqu appli construct deep autoencod map imag short binari code fast imag retriev also appli ing document call semant hash code speech featur review b use deep autoencod extract speech featur review recent work develop similar type autoencod extract bottleneck speech instead imag featur discoveri effici binari code relat featur also use speech mation retriev importantli potenti benefit use discret represent speech construct thi type deep autoencod deriv almost ite suppli unlabel data speech recognit retriev system fig architectur deep autoencod use extract speech featur spectrogram deep gener model patch spectrogram contain frequenc bin frame trate fig undirect graphic model call rbm built ha one visibl layer linear variabl gaussian nois one hidden layer binari latent variabl learn rbm activ probabl hidden unit treat data train anoth rbm two rbm pose form dbn easi infer state second layer binari hidden unit input singl forward pass dbn use thi work illustr left side fig two rbm shown separ box see detail discuss rbm dbn next section deep autoencod three hidden layer form unrol dbn use weight matric lower layer thi deep autoencod use matric encod input upper layer use matric revers order decod input thi deep autoencod use backpropag make output similar possibl input shown right side fig learn complet ani spectrogram encod struct follow first overlap frame log power spectra normal mean provid input deep autoencod first hidden layer use logist function comput activ real ue fed next code layer comput code activ hidden unit code layer quantiz either zero one old binari code use reconstruct origin spectrogram individu patch http publish onlin cambridg univers press li deng fig top bottom origin spectrogram reconstruct use input window size n forc code unit zero one binari code valu indic fft bin number fft use construct spectrogram reconstruct first use two upper layer work weight final techniqu use reconstruct speech spectrogram output produc appli deep autoencod everi possibl window n consecut frame show illustr encod reconstruct exampl c illustr exampl top fig origin speech follow reconstruct speech utter forc binari valu zero one unit code layer encod window length n respect lower code error n clearli seen encod accuraci deep autoencod tive examin compar tradit code via vector quantiz vq figur variou aspect encod accuraci top origin speech utter spectrogram next two spectrogram blurri reconstruct vq much faith reconstruct deep coder code error coder plot function time shown spectrogram ing autoencod red curv produc lower error vq coder blue curv throughout entir span utter final two spectrogram show detail code error distribut time frequenc bin transform autoencod deep autoencod describ abov extract pact code featur vector due mani layer extract code would chang dictabl input featur vector transform desir abl code chang predict reflect underli transform invari ceiv content thi goal transform autoencod propos imag recognit build block transform autoencod capsul independ subnetwork extract singl parameter featur repres singl entiti visual audio transform autoencod receiv input vector target output vector relat input vector simpl global tion translat whole imag frequenc shift due vocal tract length differ speech explicit represent global transform known also bottleneck code layer transform coder consist output sever capsul dure train phase differ capsul learn extract differ entiti order minim error final output target addit deep autoencod architectur describ thi section mani type er architectur literatur character use data alon free classif label automat deriv featur although complex architectur produc state http publish onlin cambridg univers press tutori survey architectur algorithm applic deep learn fig top bottom origin spectrogram test set reconstruct vq coder reconstruct autoencod code error function time vq coder blue autoencod red spectrogram vq coder residu spectrogram deep autoencod residu art result complex doe permit detail treatment thi tutori paper rather brief vey broader rang gener deep architectur wa includ section v h b r r c h e c u r e n n p r e r n e w h b n basic thi section present wide studi hybrid deep architectur dnn consist pretrain use gener dbn stage eter learn part thi review base recent public gener compon dbn abilist model compos multipl layer stochast latent variabl unobserv variabl binari valu often call hidden unit featur detector top two layer undirect symmetr tion form associ memori lower layer receiv direct connect layer abov state unit lowest layer visibl unit repres input data vector effici procedur ing gener weight determin variabl one layer depend variabl layer abov learn valu latent variabl everi layer infer singl pass start observ data vector bottom layer use gener weight revers direct dbn learn one layer time treat ue latent variabl one layer infer data data train next layer thi effici greedi learn follow bine learn procedur weight improv gener discrimin formanc full network thi latter learn procedur constitut discrimin compon dbn hybrid architectur discrimin perform ad final layer variabl repres desir put backpropag error deriv network mani hidden layer appli highli structur input data speech imag backpropag work much better featur detector hidden er initi learn dbn model structur input data origin propos dbn view composit simpl ing modul via stack thi simpl learn ule call rbm introduc next b restrict bm rbm special type markov random field ha one layer typic bernoulli stochast hidden unit one layer typic bernoulli gaussian tic visibl observ unit rbm repres bipartit graph visibl unit connect http publish onlin cambridg univers press li deng hidden unit hidden connect rbm joint distribut p v h θ ble unit v hidden unit h given model paramet θ defin term energi function e v h θ p v h θ exp v h θ z z v h exp v h θ normal factor partit function margin probabl model assign visibl vector v p v θ h exp v h θ z bernoulli visibl hidden rbm energi function defin e v h θ j wi jvihj bivi j jhj wi j repres symmetr interact term visibl unit vi hidden unit hj bi j bia term j number visibl den unit condit probabl effici calcul p hj θ σ wijvi j p vi θ σ j wijhj bi σ x exp x similarli gaussian visibl hidden rbm energi e v h θ j wi jvihj vi j jhj correspond condit probabl becom p hj θ σ wi jvi j p θ n j wi jhj bi vi take real valu follow gaussian tribut mean wi jhj bi varianc one rbm use convert valu stochast variabl binari stochast variabl fig pictori view sampl rbm dure neg learn phase rbm courtesi hinton process use bernoulli rbm abov discuss use two common tional distribut visibl data rbm sian data binomi binari data gener type distribut rbm also use see use gener famili distribut thi purpos take gradient log likelihood log p v θ deriv updat rule rbm weight j edata vihj vihj edata vihj expect observ ing set emodel vihj expect distribut defin model unfortun emodel vihj intract comput contrast diverg cd approxim gradient use emodel vihj replac run gibb pler initi data one full step step approxim emodel vihj follow initi data sampl sampl sampl sampl model veri rough estim emodel vihj true sampl model use approxim emodel vihj give rise algorithm sampl process pictori depict fig care train rbm essenti success appli rbm relat deep learn techniqu solv practic problem see technic report veri use practic guid train rbm rbm discuss abov gener model character input data distribut use hidden abl label inform involv howev label inform avail use togeth data form joint data set cd learn appli optim imat gener object function relat data lihood interestingli discrimin object function defin term condit likelihood label thi discrimin rbm use fine tune rbm classif task note sesm architectur ranzato et al vey section iii quit similar rbm describ abov symmetr encod http publish onlin cambridg univers press tutori survey architectur algorithm applic deep learn fig illustr architectur decod logist top encod main differ rbm train use approxim maximum likelihood sesm train simpli minim averag energi plu addit code sparsiti term sesm reli sparsiti term vent flat energi surfac rbm reli explicit contrast term loss approxim log tition function anoth differ code strategi code unit noisi binari rbm spars sesm c stack rbm form architectur stack number rbm learn layer layer bottom give rise dbn exampl shown fig stack procedur follow learn rbm applic continu featur speech rbm applic nomin binari featur imag code text treat tion probabl hidden unit data train rbm one layer activ probabl rbm use visibl data input rbm theoret tific thi effici greedi learn strategi given shown stack procedur abov improv variat lower bound likelihood train data composit model greedi procedur abov achiev approxim learn note thi learn cedur unsupervis requir class label appli classif task gener train follow combin calli discrimin learn procedur weight jointli improv perform network thi discrimin perform ad final layer variabl repres desir output label provid train data backpropag algorithm use adjust tune dbn weight use final set weight way standard feedforward neural work goe top label layer thi dnn depend applic speech recognit applic top layer denot j fig resent either syllabl phone subphon phone state speech unit use speech nition system gener pretrain describ abov ha duce excel phone speech recognit result wide varieti task survey section vii research ha also shown effect pretrain strategi exampl greedi train may carri addit nativ term gener cost function level without gener pretrain pure discrimin train dnn random initi weight use dition stochast gradient decent method ha shown work veri well scale initi weight set care size trade noisi gradient converg speed use stochast ent decent adapt prudent increas size train epoch also random order creat need judici determin importantli wa found effect learn dnn ing shallow neural net singl hidden layer onc thi ha train discrimin use earli stop avoid overfit second hidden layer insert first hidden layer label softmax put unit expand deeper network train discrimin thi continu desir number hidden layer reach full propag fine tune appli thi discrimin pretrain procedur found work well practic thi type discrimin pretrain procedur close relat learn algorithm develop deep architectur call deep network describ section vi interleav linear layer use build deep ture modular manner origin input vector concaten output vector modul consist shallow neural net discrimin ing use posit subset weight modul reason space use paralleliz convex optim follow fine tune cedur also paralleliz due constraint two subset weight modul pure discrimin train full dnn random initi weight known work much http publish onlin cambridg univers press li deng fig interfac hmm form thi architectur ha success use speech recognit experi report better thought earli day provid scale initi weight set care larg amount label train data avail size train epoch set appropri less gener pretrain still improv test perform sometim signific amount especi small task gener pretrain wa origin done use rbm variou type autoencod one hidden layer also use interfac dnn hmm discuss abov static classifi input vector fix dimension howev mani tical pattern recognit lem includ speech recognit machin translat natur languag understand video process inform process requir sequenc recognit sequenc recognit sometim call classif structur dimension input output variabl hmm base dynam program oper conveni tool help port strength static sifier handl dynam sequenti pattern thu natur combin hmm bridg gap static sequenc pattern recognit tectur show interfac dnn hmm provid fig thi architectur ha success use speech recognit experi report import note uniqu elast poral dynam speech elabor would requir model better hmm http publish onlin cambridg univers press tutori survey architectur algorithm applic deep learn ultim success speech recognit integr dynam model realist properti dnn possibl deep learn model form coher dynam deep architectur ing new research v c r n v e r c h e c u r e n n r e c u r r e n n e w r k introduct dnn review ha shown extrem power connect perform nition classif task includ speech recognit imag classif train dbn ha proven difficult comput particular tional techniqu train dnn fine tune phase involv util stochast gradient descent ing algorithm extrem difficult parallel thi make learn larg scale calli imposs exampl ha possibl use one singl veri power gpu machin train speech recogn dozen hundr hour speech train data remark result veri difficult howev scale thi success thousand hour train data describ new deep learn architectur dsn attack learn scalabl problem thi section base part recent public expand discuss central idea dsn design relat concept stack propos origin simpl ule function classifi compos first stack top order learn complex function classifi variou way ment stack oper develop past typic make use supervis inform simpl modul new featur stack classifi higher level stack architectur often come concaten classifi output lower modul raw input featur simpl modul use stack wa crf thi type deep architectur wa develop hidden state ad success natur languag speech recognit applic segment inform unknown train data convolut neural network also consid stack architectur supervis inform typic use final stack modul dsn architectur wa origin present also use name deep convex network dcn emphas convex natur main learn algorithm use learn network dsn discuss thi section make use supervis inform ing basic modul take simplifi form perceptron basic modul fig dsn architectur stack onli four modul illustr distinct color dash line denot copi layer output unit linear hidden unit sigmoid linear output unit permit highli effici paralleliz estim result convex optim output network weight given hidden unit activ owe form constraint input output weight input weight also elegantli estim effici paralleliz manner name convex use accentu role convex optim learn output network weight given hidden unit activ basic ule also point import constraint deriv convex input output weight constraint make learn remain network paramet input network weight much easier otherwis enabl learn dsn distribut cpu cluster recent public dsn wa use key oper stack emphas b architectur overview dsn dsn shown fig includ variabl number layer modul wherein modul special ral network consist singl hidden layer two trainabl set weight fig onli four modul http publish onlin cambridg univers press li deng illustr modul shown rate color practic hundr modul effici train use imag speech classif experi lowest modul dsn compris first linear layer set linear input unit layer set hidden unit second linear layer set linear output unit hidden layer lowest modul dsn prise set unit map input unit way first weight matrix denot instanc weight matrix may compris plural randomli gener valu zero one weight rbm train separ linear unit may sigmoid unit configur perform oper weight output input unit weight accord first weight matrix w second linear layer ani modul dsn includ set output unit repres get classif unit modul dsn may map set linear put unit way second weight matrix denot thi second weight matrix learn way batch learn process ing undertaken parallel convex optim employ connect learn instanc u learn base least part upon first weight matrix w valu code classif target valu input unit indic abov dsn includ set serial connect overlap layer modul wherein modul includ aforement three layer first linear layer includ set linear input unit whose number equal dimension input featur hidden layer compris set unit whose number tunabl second linear layer compris plural linear output unit whose number equal target classif class modul refer herein layer becaus output unit lower modul subset input unit adjac higher modul dsn specif second modul directli abov lowest ule dsn input unit includ output unit hidden unit lower modul input unit addit includ raw train data word output unit lowest modul append input unit second modul input unit second modul also includ output unit lowest modul pattern discuss abov includ output unit lower modul portion input unit cent higher modul dbn thereaft learn weight matrix describ connect weight hidden unit linear output unit via convex tion continu mani modul result learn dsn may deploy connect matic classif task speech phone state classif connect dsn output hmm ani dynam program devic enabl continu speech recognit form sequenti pattern recognit c learn dsn weight technic detail provid use linear output unit dsn facilit learn dsn weight singl modul use illustr advantag simplic reason first clear upper layer weight matrix u effici learn onc activ matrix h train sampl den layer known let us denot train vector x xi xn vector denot xi x ji xdi dimens input vector function block n total number train sampl denot l number hidden unit c dimens put vector output dsn block yi uthi hi σ wtxi vector sampl u l c weight matrix upper layer block w l weight matrix lower layer block σ sigmoid function bia term implicitli sent abov formul xi hi augment one given target vector full train set total n sampl ti tn vector ti tji tci paramet u w learn minim averag total squar error e n output network yn uthn utσ wtxn g n u w depend weight matric standard neural net assum h hi hn known equival w known set error tive respect u zero give u hht f w hn σ wtxn thi provid explicit constraint u w treat independ popular backprop algorithm given equal constraint u f w let us use lagrangian multipli method solv optim problem learn optim lagrangian e n n u w w deriv gradient descent learn algorithm gradient take follow form ht htt http publish onlin cambridg univers press tutori survey architectur algorithm applic deep learn fig comparison one singl modul dsn left tdsn two equival form tdsn modul shown right ht hht h bol multipl compar backprop abov method ha less nois gradient comput due exploit explicit constraint u f w wa found iment unlik backprop batch train effect aid parallel learn dsn tensor dsn dsn architectur discuss far ha recent gener tensor version call tdsn ha scalabl dsn term alleliz learn gener dsn provid featur interact miss dsn architectur tdsn similar dsn way stack oper carri modul tdsn stack similar way form deep architectur differ tdsn dsn lie mainli modul construct dsn one set hidden unit form hidden layer denot left panel fig contrast modul tdsd tain two independ hidden layer denot hidden hidden middl right panel fig result thi differ weight denot u fig chang matrix array dsn tensor array tdsn shown cube label u middl panel tensor u ha connect one predict layer remain two separ den layer equival form thi tdsn modul shown right panel fig implicit hidden layer form expand two separ hidden layer outer product result larg vector contain possibl product two set layer vector thi turn tensor u matrix whose dimens size predict layer uct two hidden layer size equival enabl convex optim learn u develop dsn appli learn tensor importantli order hidden featur interact enabl tdsn via outer product construct larg implicit hidden layer stack tdsn modul form deep architectur pursu similar way dsn concaten variou fig stack tdsn modul concaten predict vector input vector vector two exampl shown fig note stack concaten hidden layer input fig would difficult dsn sinc hidden layer tend larg practic purpos e recurr neural network consid increasingli higher modul dsn version shallow neural network turn dsn stack instead stack discret time index correspond depth dsn constraint dsn among weight matric similarli appli thi type rnn learn http publish onlin cambridg univers press li deng fig stack tdsn modul concaten two tor input vector weight paramet provid output unit linear fact concept rnn dsn bine form recurr version dsn lentli stack version simpl rnn discuss thi paper one way learn rnn linear output adopt approach shown effect dsn learn outlin section abov thi would captur short memori one time step increas memori length appli tradit method backprop time bptt exploit relationship among variou weight matric turn recurs procedur simpler analyt form howev thi difficult formul deriv dsn case discuss section use gener bptt ha advantag dling output unit shown speed learn substanti compar use linear output unit rnn commonli discuss problem vanish explod gradient bptt mitig appli constraint regard rnn recurr matric dure optim process remaind thi section let us formul rnn term state space model monli use signal process compar formul dynam tem use gener model speech acoust contrast discrimin rnn use mathemat model gener mode allow us shed light onto whi one approach work better anoth rnn state dynam nois free express ht f wxhxt observ predict label target vector lt vector code class label observ equat formul becom yt whyht yt g whyht defin error function sum squar differ yt lt time bptt unfold rnn time ing gradient respect whi wxh whh stochast gradient descent appli updat weight matric use similar formul rnn model abov gener mode known den dynam model briefli discuss section speech recognit research built mani type speech recogn past year see vey section particular correspond state observ equat er ht g state nois xt h ht obsnois rewritten equat sistent rnn variabl system matrix drive state dynam depend label lt time henc model also call switch dynam system system matrix paramet analog whh rnn paramet set govern map hidden state speech product acoust featur speech one implement took form shallow mlp weight anoth implement took form set matric mixtur linear expert state equat mani exist implement hidden dynam model speech doe take linear form rather follow linear form wa use ht whh lt lt lt state nois exhibit properti dynam paramet whh function phonet label lt particular time lt map symbol quantiti lt target vector surfac base mathemat tion strike similar tive rnn gener hidden dynam model howev essenc well represent substanc two model veri differ summar first rnn adopt strategi use distribut represent supervis inform label wherea hidden dynam model label local repres use index separ set paramet lead switch dynam consider complic decod comput http publish onlin cambridg univers press tutori survey architectur algorithm applic deep learn second rnn run directli produc posterior probabl class contrast hidden dynam model run top gener likelihood ue class individu thi differ clear compar two observ equat one give label predict anoth give input featur predict state equat rnn model ha input drive system dynam wherea gener model ha label index drive dynam via intermedi represent articulatori vocal tract reson get third learn algorithm bptt rnn directli minim label predict error contrast kalman filter e step em algorithm use learn gener model doe inat explicitli given known difficulti bptt rnn one obviou direct adopt hybrid deep architectur use gener hidden dynam model pretrain discrimin rnn analog use gener dbn pretrain dnn discuss preced subsect v p p l c n f e e p l e r n n g g n l n n f r n p r c e n g expand technic scope signal process signal endow onli tradit type audio speech imag video also text languag document convey semant tion human consumpt addit scope process ha extend convent ing enhanc analysi recognit includ task interpret understand retriev mine user interfac signal process research work one signal process area defin matrix construct two axe signal process discuss deep learn techniqu discuss thi paper recent appli larg number tradit extend area recent interest cation predict protein structur cover provid brief survey thi bodi work four main categori pertain close signal inform process speech audio tradit neural network mlp ha use speech recognit mani year use alon perform typic lower hmm system observ probabl approxim gmm recent deep learn techniqu wa success appli phone recognit larg vocabulari speech recognit task integr power discrimin train abil dnn sequenti model abil hmm speech recognit ha long domin method underli shallow ativ model neural network onc popular approach competit gener model deep hidden dynam likewis competit either deep learn dnn start make impact speech recognit close collabor academ industri research see review collabor work start small ulari task demonstr power hybrid deep architectur work also show import raw speech featur spectrogram back mfcc featur yet reach raw level collabor continu larg vocabulari task ing highli posit result thi success larg part attribut use veri larg dnn put layer structur way speech unit senon motiv initi speech industri desir keep chang alreadi highli effici decod softwar infrastructur minimum meantim thi bodi work also demonstr possibl reduc need ing effect learn dnn larg amount label data avail combin three tor quickli spread success deep learn speech recognit entir speech industri academia minim requir decod chang new base speech recogn deploy condit enabl use senon dnn output significantli ere error compar hmm system train simplic empow big data train timefram least major speech recognit group worldwid confirm experiment success dnn veri larg task use raw speech spectral featur away mfcc notabl group includ major trial speech lab worldwid microsoft ibm googl baidu result repres new speech tion wide deploy compani voic product servic extens media coverag discuss section concept convolut time wa origin tdnn shallow neural net develop earli speech recognit onli recent deep architectur deep cnn use ha found weight share effect phone recognit time domain previou tdnn studi also show design pool deep cnn properli invari vocal tract length discrimin speech sound togeth regular techniqu dropout lead even better phone recognit perform thi set work also point direct trajectori discrimin invari express whole dynam pattern speech defin mix time http publish onlin cambridg univers press li deng frequenc domain use convolut pool recent work show cnn also use larg vocabulari continu speech tion demonstr multipl convolut layer provid even improv tional layer use larg number convolut kernel featur map addit rbm dbn cnn dsn deep model also develop report literatur speech audio process relat applic exampl crf stack mani layer crf success use task languag identif phone tion sequenti label natur languag ing confid calibr speech recognit furthermor rnn ha earli success phone recognit wa easi duplic due intricaci train let alon scale larger speech recognit task learn algorithm rnn dramat improv sinc better result obtain recent use rnn especi structur long memori lstm embed rnn sever layer train rnn also recent appli process applic use rectifi linear hidden unit instead logist tanh explor rnn rectifi linear unit relu comput max x lead sparser gradient less diffus credit blame rnn faster train addit speech recognit impact deep learn ha recent spread speech synthesi aim overcom limit convent approach statist parametr synthesi base hmm model cluster ferenc icassp may four differ deep learn approach report improv tradit base speech synthesi system ling et al rbm dbn gener model use replac tradit gaussian model achiev signific qualiti improv subject object measur synthes voic approach develop dbn gener model use repres joint distribut linguist acoust featur decis tree gaussian model replac dbn hand studi report make use discrimin model dnn repres condit distribut acoust featur given linguist featur joint distribut model final discrimin model dnn use featur extractor summar structur raw acoust featur dnn featur use input second stage system predict prosod contour target contextu ture fill speech synthesi system applic deep learn speech synthesi infanc much work expect commun near futur likewis audio music process deep ing ha also becom intens interest onli recent impact area includ mainli music signal process music inform retriev deep learn present uniqu set challeng area music audio signal time seri event organ music time rather real time chang function rhythm express measur signal typic combin multipl voic synchron time overlap frequenc mix tempor depend influenc tor includ music tradit style compos pretat high complex varieti give rise signal represent problem high level abstract afford perceptu biolog motiv process techniqu deep learn much work expect music audio signal process commun near futur b imag video multimod origin dbn deep autoencod ope demonstr success simpl imag recognit dimension reduct code task mnist interest note gain code effici use autoencod imag data convent method princip ponent analysi demonstr veri similar gain report speech data tradit techniqu vq modifi dbn develop layer model use bm thi type dbn appli norb databas object recognit task error rate close best publish result thi task report particular shown dbn substanti outperform shallow model svm deep architectur convolut structur found highli effect commonli use puter vision imag recognit notabl advanc wa recent achiev imagenet lsvrc contest differ imag class target million imag train set test set consist imag deep cnn approach describ achiev error rate consider lower ou veri larg deep cnn use sist million weight neuron five convolut layer togeth er addit three layer dnn describ previous use top deep cnn layer although abov structur develop separ earlier work best combin account part success addit factor contribut final success power regular techniqu call dropout see detail use satur neuron relu comput max x significantli speed train process especi http publish onlin cambridg univers press tutori survey architectur algorithm applic deep learn veri effici gpu implement recent similar deep cnn approach stochast pool also report excel result four imag dataset deep network shown power comput vision imag recognit task becaus extract appropri featur jointli perform discrimin anoth type deep architectur ha creat stantial impact imag recognit le et al report excel result use gener model base spars autoencod larg framework thi type extrem larg network billion paramet wa train use thousand cpu core recent work along thi direct report size network altern train use cluster onli gpu server machin use tempor condit dbn video sequenc human motion synthesi report condit dbn make dbn weight associ fix time window condit data previou time step comput tool offer thi type tempor dbn relat recurr network may provid opportun improv toward effici integr human speech product mechan speech product model interest studi appear author propos evalu novel applic deep network learn featur audio video modal similar deep autoencod architectur describ section iv use consid gener singl modal two modal featur learn ha demonstr better featur video learn audio video inform sourc abl featur learn time author show learn share audio video represent evalu fix task classifi train data test data vice versa work conclud deep ing architectur gener effect learn modal featur unlabel data improv gle modal featur learn one except set use cuav dataset result present show improv learn video featur video audio compar learn featur onli video data howev paper also show model sophist signal ing techniqu extract visual featur togeth method develop nalli robust speech recognit give best classif accuraci learn task beat featur deriv gener deep tectur design thi task deep gener architectur multimod learn describ base autoencod neural net probabilist version base dbm ha appear recent multimod applic dbm use extract unifi represent integr separ modal use classif inform retriev task rather use bottleneck layer deep autoencod repres multimod input probabl densiti defin joint space multimod input state suitabl defin latent variabl use represent advantag thi probabilist lation lack deep autoencod miss modal inform fill natur pling condit distribut bimod data consist imag text multimod dbm shown outperform deep multimod autoencod well multimod dbn classif inform retriev task c languag model research languag document text process ha seen increas popular recent signal ing commun ha design one main focu area societi audio speech guag process technic committe ha long histori use shallow neural work lm import compon speech nition machin translat text inform retriev natur languag process recent dnn attract attent statist lm lm function captur salient statist characterist distribut sequenc word natur languag allow one make probabilist diction next word given preced one neural network lm one exploit neural network abil learn distribut represent reduc impact curs dimension distribut represent symbol vector featur character mean bol neural network lm one reli learn algorithm discov meaning ture basic idea learn associ word dictionari vector tion word correspond point featur space one imagin dimens space correspond semant grammat characterist word hope function similar word get closer space least along tion sequenc word thu transform sequenc learn featur vector neural work learn map sequenc featur vector probabl distribut next word sequenc distribut represent approach lm ha advantag allow model gener well sequenc set train word sequenc similar term featur distribut represent becaus neural network tend map nearbi input nearbi output predict correspond word sequenc similar featur map similar predict http publish onlin cambridg univers press li deng abov idea neural network lm ment variou studi involv deep tectur tempor factor rbm wa use lm unlik tradit model factor rbm use distribut represent onli text word also word predict thi approach gener deeper structur report recent work neural network lm deep architectur found particular work describ make use rnn build larg scale languag model achiev stabil fast converg train help cap grow dient train rnn also develop adapt scheme lm sort train data respect relev train model ing process test data empir comparison lm show much better perform rnn especi perplex measur separ work appli rnn lm unit ter instead word found veri interest properti predict depend make open close quot paragraph strate use practic applic ha clear becaus word power represent natur languag chang word charact lm limit practic applic scenario furthermor use hierarch bayesian prior build deep recurs structur lm appear specif process exploit bayesian prior deep four layer tic gener model built offer principl approach lm smooth incorpor tion natur languag discuss section iii thi type prior knowledg embed readili abl probabilist model setup neural network one natur languag process sometim debat work ral languag process collobert weston ope employ convolut dbn common model simultan solv number classic lem includ tag chunk name entiti tag semant role identif similar word identif recent work report develop fast pure discrimin approach pars base deep recurr convolut tectur call graph transform network collobert et al provid comprehens review thi line work specif way appli unifi neural work architectur relat deep learn algorithm solv natur languag process problem scratch theme thi line work avoid featur engin provid versatil unifi featur construct automat deep learn applic natur task system describ automat learn nal represent vast amount mostli unlabel train data one import aspect work describ transform raw word tion term spars vector veri high sion vocabulari size squar even cubic vector process sequent neural network layer thi known word embed wide use natur languag process lm nowaday unsupervis learn use text word use learn signal neural network excel tutori wa recent given explain neural network train form word embed origin propos recent work propos new way word embed better captur semant word incorpor local global document context better account homonymi polysemi learn multipl ding per word also evid use rnn also provid empir good perform word embed concept word embed wa veri recent extend singl languag two produc gual word embed machin translat applic good perform wa shown zou et al chines semant similar bilingu train embed use embed comput semant similar phrase pair wa shown improv bleu score slightli machin translat hand gao et al made use word embed sourc target languag raw input ture dnn extract semant featur translat score comput measur distanc semant featur new featur space dnn weight learn directli mize qualiti bleu score machin translat anoth area appli deep learn natur languag process appear recurs neural network use build deep tectur network shown capabl success merg natur languag word base learn semant transform origin featur thi deep learn approach provid excel perform natur languag pars approach also demonstr author success pars natur scene imag relat studi similar recurs deep architectur use paraphras tion predict sentiment distribut text recent work socher et al extend recurs neural network tensor similar way dnn wa extend tensor sion appli semant composition thi recurs neural tensor network result semant word space capabl express mean longer phrase drastic improv predict accuraci sentiment label http publish onlin cambridg univers press tutori survey architectur algorithm applic deep learn e inform retriev discuss applic dbn relat deep autoencod advanc deep learn method develop recent document index mation retriev salakhutdinov hinton show hidden variabl final layer dbn onli easi infer also give better represent ument base featur wide use latent semant analysi tradit approach inform retriev use pact code produc deep autoencod document map memori address way calli similar text document locat nearbi address facilit rapid document retriev map vector compact code highli effici requir onli matrix multipl subsequ sigmoid function evalu hidden layer encod part network briefli lowest layer dbn repres count vector document top layer repres lean binari code document top two layer dbn form undirect associ memori remain layer form bayesian also call belief network direct connect thi dbn compos set stack rbm review section v produc feedforward encod network convert vector compact code ing rbm opposit order decod network construct map compact code vector struct vector combin encod decod one obtain deep autoencod subject ther discuss section iv document code subsequ retriev deep model train retriev process start map queri document binari code perform forward pass model threshold similar ming distanc queri binari code document binari code comput effici semant hash method describ abov intend extract hierarch semant structur ded queri document nevertheless adopt unsupervis learn approach dbn deep autoencod paramet optim struction document rather real goal inform retriev differenti relev document irrelev one given queri result fail significantli outperform line retriev model base keyword match semant hash model also face scalabl challeng regard matrix multipl problem veri recent address huang et al weakli supervis approach taken specif seri deep structur semant model dssm develop thi work determinist word ing construct document queri first produc vector rel low iti feed dnn extract semant featur pair learn dnn instead use optim criterion dssm construct novel object function directli target goal document rank enabl abil data supervis tion thi object function defin basi cosin similar measur semant featur pair extract dnn lent result base ndcg perform measur report web search task use semant featur produc dssm discrimin manner instead use deep net produc semant featur aid inform retriev deng et al appli dsn describ section directli perform task inform retriev base rich set tradit featur queri length text match translat probabl queri ment applic deep learn inform retriev infanc expect work thi area emerg come year includ open constrain ad document search aim predict document relev input queri v u r n c u n thi paper present brief histori deep learn develop categor scheme analyz exist deep architectur literatur gener hybrid class deep autoencod dsn includ gener rnn architectur one three class discuss analyz detail appear popular promis approach author person research experi applic deep learn five broad area inform process review literatur deep learn vast mostli ing machin learn commun process commun embrac deep learn onli within past year momentum grow fast thi overview paper written mainli process perspect beyond survey exist deep learn work classificatori scheme base ture natur learn algorithm develop analysi concret exampl conduct thi hope provid insight reader better stand capabl variou deep learn system discuss paper connect among differ similar deep learn method way design proper deep learn algorithm differ circumst throughout thi review import messag vey deep architectur archi featur highli desir discuss difficulti learn paramet layer onc http publish onlin cambridg univers press li deng due pervas local optima diminish explod gradient gener pretrain method hybrid architectur review detail section v appear offer use albeit cal solut poor local optima optim especi label train data limit deep learn emerg technolog despit empir promis result report far much need develop importantli ha experi deep learn research singl deep learn techniqu success classif task exampl popular learn strategi gener pretrain follow discrimin seem work well empir mani task fail work task review success deep learn number perceptu task speech languag vision task requir trivial intern represent tion retriev natur languag process task artifici intellig causal infer decis make would like benefit deep learn approach deep learn branch machin learn graphic model kernel method enhanc issu remain explor recent publish work show vast room improv current optim techniqu learn deep architectur extent train import learn full set paramet deep architectur ha current investig especi veri larg amount label train data avail reduc even obliter need model regular experiment result discuss thi paper effect scalabl parallel algorithm critic train deep model veri larg data mani mon applic speech recognit machin translat inform retriev web scale popular stochast ent techniqu known parallel comput recent advanc develop chronou stochast gradient learn show promis use cpu cluster gpu cluster make deep learn techniqu scalabl veri larg train data theoret sound parallel ing algorithm effect architectur ing one need develop one major barrier applic dnn relat deep model current requir erabl skill experi choos sensibl valu learn rate schedul strength regular number layer number unit per layer etc sensibl valu one paramet may depend valu chosen tune dnn especi expens interest method ing problem develop recent includ random sampl bayesian optim cedur research need thi tant area final solid theoret foundat deep learn need establish myriad aspect exampl success deep learn vise mode ha demonstr much supervis learn yet essenc major tion deep learn lie right unsupervis learn aim automat discoveri data represent appropri object learn effect sentat may deep learn architectur algorithm use distribut represent effect entangl hidden explanatori factor variat data comput neurosci model hierarch brain structur learn style help improv engin deep learn architectur rithm import question need intens research order push frontier deep learn r e f e r e n c e deng overview learn inform process proc signal inform process annu summit conf octob deng expand scope signal process ieee signal process hinton osindero teh fast learn algorithm deep belief net neural bengio learn deep architectur ai found trend mach bengio courvil vincent represent learn review new perspect ieee tran pattern anal mach hinton et al deep neural network acoust model speech recognit ieee signal process yu deng deep learn applic signal inform process ieee signal process arel rose karnowski deep machin learn new frontier artifici intellig ieee comput intellig markoff scientist see promis program new york time novemb cho saul kernel method deep learn nip deng tur use kernel deep vex network learn spoken languag stand proc ieee workshop spoken languag technolog decemb vinyal jia deng darrel learn recurs perceptu represent proc nip baker et al research develop direct speech recognit understand ieee signal process baker et al updat min report speech recognit understand ieee signal process http publish onlin cambridg univers press tutori survey architectur algorithm applic deep learn deng comput model speech product putat model speech pattern process verlag berlin heidelberg deng switch dynam system model speech tion acoust mathemat foundat speech languag process springer new york georg brain might work hierarch poral model learn recognit thesi stanford univers bouvri hierarch learn theori applic speech vision thesi mit poggio brain might work role inform learn understand replic intellig tion scienc technolog new centuri jacovitt pettorossi consolo senni ed lateran univers press amsterdam netherland glorot bengio understand difficulti train deep feedforward neural network proc aistat hinton salakhutdinov reduc dimension data neural network scienc dahl yu deng acero hmm larg vocabulari continu speech recognit proc icassp moham yu deng investig ing deep belief network speech recognit proc speech septemb moham dahl hinton acoust model use deep belief network ieee tran audio speech lang dahl yu deng acero hmm larg vocabulari continu speech recognit ieee tran audio speech lang moham hinton penn understand deep belief network perform acoust model proc icassp vincent larochel lajoi bengio manzagol stack denois autoencod lean use represent deep network local denois criterion mach learn rifai vincent muller glorot bengio contract autoencod explicit invari dure featur extract proc icml ranzato boureau lecun spars featur learn deep belief network proc nip deng seltzer yu acero moham hinton binari code speech spectrogram use deep proc interspeech bengio de mori flammia komp global tion neural network hidden markov model hybrid proc proc eurospeech bourlard morgan connectionist speech recognit hybrid approach kluwer norwel morgan deep wide multipl layer automat speech recognit ieee tran audio speech lang deng li machin learn paradigm speech recognit overview ieee tran audio speech lecun chopra ranzato huang model document recognit comput vision proc int conf document analysi recognit icdar ranzato poultney chopra lecun effici learn spars represent model proc nip ngiam khosla kim nam lee ng multimod deep learn proc icml ngiam chen koh ng learn deep energi model proc icml hinton krizhevski wang transform proc int conf artifici neural network salakhutdinov hinton deep boltzmann machin proc aistat salakhutdinov hinton better way pretrain deep mann machin proc nip srivastava salakhutdinov multimod learn deep boltzmann machin proc nip dahl ranzato moham hinton phone tion restrict boltzmann machin proc nip poon domingo network new deep tectur proc conf uncertainti artifici ligenc barcelona spain gen domingo discrimin learn network proc nip sutskev marten hinton gener text recurr neural network proc icml marten deep learn optim proc icml marten sutskev learn recurr neural network optim proc icml bengio boulang pascanu advanc optim recurr network proc icassp sutskev train recurr neural network thesi univers toronto mikolov karafiat burget cernocki khudanpur recurr neural network base languag model proc icassp mesnil deng bengio investig architectur learn method spoken guag understand proc interspeech deng dynam speech model theori algorithm applic morgan claypool decemb deng gener hidden markov model condit trend function time speech signal signal deng stochast model speech incorpor hierarch nonstationar ieee tran speech audio deng aksmanov sun wu speech recognit use hidden markov model polynomi regress function nonstationari state ieee tran speech audio ostendorf digalaki kimbal hmm segment model unifi view stochast model speech recognit ieee tran speech audio deng sameti transit speech unit tion regress markov state applic speech recognit ieee tran speech audio http publish onlin cambridg univers press li deng deng aksmanov phonet sific use hidden markov model mixtur trend function ieee tran speech audio yu deng solv nonlinear estim problem use spline ieee signal process yu deng gong acero novel framework train algorithm hidden markov el ieee tran audio speech lang zen nankaku tokuda continu stochast featur map base trajectori hmm ieee tran audio speech lang zen gale nankaku tokuda product expert statist parametr speech synthesi ieee tran audio speech lang ling richmond yamagishi articulatori control base parametr speech synthesi use multipl regress ieee tran audio speech lang ling deng yu model spectral envelop use restrict boltzmann machin statist parametr speech synthesi icassp shannon zen byrn autoregress model tical parametr speech synthesi ieee tran audio speech lang deng ramsay sun product model structur basi automat speech recognit speech bridl et al investig segment hidden dynam el speech coarticul automat speech recognit final report workshop languag engin clsp john hopkin picon et al initi evalu hidden dynam model convers speech proc icassp minami mcdermott nakamura katagiri nition method parametr trajectori synthes use direct relat static dynam featur vector time seri proc icassp deng huang challeng adopt speech recognit commun acm deng effici decod strategi tional speech recognit use constrain nonlinear space model ieee tran speech audio deng mixtur dynam model taneou speech recognit ieee tran speech audio deng yu acero structur speech model ieee tran audio speech lang deng yu acero bidirect target filter model speech coarticul implement phonet recognit ieee tran audio speech deng yu use differenti cepstra acoust featur hidden trajectori model phonet recognit proc icassp april bilm bartel graphic model architectur speech recognit ieee signal process bilm dynam graphic model ieee signal process renni hershey olsen multitalk speech recognit graphic model approach ieee signal process wohlmayr stark pernkopf probabilist interact model multipitch track factori hidden markov model ieee tran audio speech lang stoyanov ropson eisner empir risk minim graphic model paramet given approxim infer decod model structur proc aistat kurzweil creat mind vike book decemb fine singer tishbi hierarch hidden markov model analysi applic mach oliv garg horvitz layer represent ing infer offic activ multipl sensori channel comput vi imag taylor hinton rowei model human motion use binari latent variabl proc nip socher lin ng man learn continu phrase represent syntact pars recurs neural network proc icml juang chou lee minimum classif error rate method speech recognit ieee tran speech audio chengalvarayan deng speech trajectori discrimin use minimum classif error learn ieee tran speech audio povey woodland minimum phone error improv discrimin train proc icassp deng chou discrimin learn sequenti pattern recognit unifi review speech recognit ieee signal process jiang li paramet estim statist model use convex optim advanc method discrimin ing speech languag process ieee signal process yu deng acero minimum sific error train speech recognit task proc icassp xiao deng geometr perspect train gaussian model ieee signal process gibson hain error approxim minimum phone error acoust model estim ieee tran audio speech lang yang furui combin crf model joint sourc channel model machin transliter proc acl uppsala sweden yu wang deng sequenti label use condit random field sel top signal hifni renal speech recognit use augment tional random field ieee tran audio speech lang heintz brew discrimin input stream combin condit random field phone recognit ieee tran audio speech lang http publish onlin cambridg univers press tutori survey architectur algorithm applic deep learn zweig nguyen segment crf approach larg lari continu speech recognit proc asru peng bo xu condit neural field proc nip heigold ney lehnen gass schluter equival gener model ieee tran audio speech lang yu deng hidden condit random field phonet recognit proc interspeech septemb yu wang karam deng languag recognit use condit random field proc icassp pinto garimella hermanski bourlard analysi hierarch phone posterior probabl estim ieee tran audio speech lang ketabdar bourlard enhanc phone posterior ing speech recognit system ieee tran audio speech lang morgan et al push envelop asid speech recognit ieee signal process deng yu deep convex network scalabl architectur speech pattern classif proc interspeech deng yu platt scalabl stack learn build deep architectur proc icassp tur deng toward deep stand deep convex network semant utter tion proc icassp lena nagata baldi deep spatiotempor architectur learn protein structur predict proc nip hutchinson deng yu deep architectur ear model hidden represent applic phonet recognit proc icassp hutchinson deng yu tensor deep stack work ieee tran pattern anal mach deng hassanein elmasri analysi correl ture neural predict model applic speech nition neural robinson applic recurr net phone probabl estim ieee tran neural grave fernandez gomez schmidhub ist tempor classif label unseg sequenc data recurr neural network proc icml grave maham hinton speech recognit deep recurr neural network proc icassp grave sequenc transduct recurr neural network represent learn worksop icml lecun bottou bengio haffner ing appli document recognit proc ieee ciresan giusti gambardella schidhub deep ral network segment neuron membran electron microscopi imag proc nip dean et al larg scale distribut deep network proc nip krizhevski sutskev hinton imagenet classif deep convolut neural network proc nip moham jiang penn appli volut neural network concept hybrid model speech recognit icassp deng yu explor convolut neural network structur optim speech recognit proc interspeech deng yu jiang deep segment neural network speech recognit proc interspeech sainath moham kingsburi ramabhadran lution neural network lvcsr proc icassp deng yu deep convolut neural work use heterogen pool trade acoust invari phonet confus proc icassp lang waibel hinton neural network tectur isol word recognit neural hawkin blakesle intellig new stand brain lead creation truli intellig machin time book new york waibel hanazawa hinton shikano lang phonem recognit use neural network ieee tran assp hawkin ahmad dubinski hierarch tempor memori includ htm cortic learn algorithm numenta technic report decemb lee ing new speech research paradigm automat speech recognit proc icslp yu siniscalchi deng lee boost attribut phone estim accuraci deep neural network base speech recognit proc icassp siniscalchi yu deng lee exploit deep neural network speech recognit neurocomput siniscalchi svendsen lee modular search approach larg vocabulari continu speech recognit ieee tran audio speech lang yu seid li deng exploit spars deep neural network larg vocabulari speech recognit proc icassp deng sun statist approach automat speech nition use atom speech unit construct overlap articulatori featur acoust soc sun deng base phonolog model incorpor linguist constraint applic speech tion acoust soc sainath kingsburi ramabhadran improv train time deep belief network hybrid larger batch size proc nip workshop model decemb erhan bengio courvel manzagol vencent bengio whi doe unsupervis help deep learn mach learn kingsburi optim sequenc tion criteria acoust model proc icassp kingsburi sainath soltau scalabl minimum bay risk train deep neural network acoust model use distribut optim proc interspeech http publish onlin cambridg univers press li deng larochel bengio classif use discrimin restrict boltzmann machin proc icml lee gross ranganath ng unsupervis ing hierarch represent convolut deep belief network commun acm vol octob pp lee gross ranganath ng convolut deep belief network scalabl unsupervis learn hierarch represent proc icml lee largman pham ng unsupervis featur ing audio classif use convolut deep belief network proc nip ranzato susskind mnih hinton deep gener model applic recognit proc cvpr ney speech translat coupl recognit translat proc icassp deng speech recognit machin translat speech translat unifi discrimin framework ieee signal process yamin deng wang acero integr techniqu spoken utter classif ieee tran audio speech lang deng optim inform cess criteria techniqu proc icassp deng inform process approach proc ieee deng gao deep stack network inform retriev proc icassp deng tur adapt train robust spoken languag understand proc icassp le ranzato monga devin corrado chen dean ng build featur use larg scale unsupervis learn proc icml seid li yu convers speech transcript use deep neural network proc interspeech yan huo xu scalabl approach use featur base acoust model lvcsr proc interspeech well hinton exponenti famili nium applic inform retriev proc nip vol hinton practic guid train restrict boltzmann machin utml technic report univers toronto august wolpert stack gener neural cohen de carvalho stack sequenti learn proc ijcai jarrett kavukcuoglu lecun best multistag architectur object recognit proc int conf comput vision pascanu mikolov bengio difficulti train recurr neural network proc icml deng spontan speech recognit use cal coarticulatori model vocal tract reson dynam acoust soc togneri deng joint state paramet estim nonlinear dynam system model ieee tran nal moham dahl hinton deep belief network phone recognit proc nip workshop deep learn speech recognit relat applic sivaram hermanski spars multilay perceptron phonem recognit ieee tran audio speech lang kubo hori nakamura integr deep neural network structur classif approach base weight transduc proc interspeech deng et al recent advanc deep learn speech research microsoft proc icassp deng hinton kingsburi new type deep neural work learn speech recognit relat applic overview proc icassp juang levinson sondhi maximum likelihood estim multivari mixtur observ markov chain ieee tran inf theori deng lennig seitz mermelstein larg lari word recognit use allophon den markov model comput speech deng kenni lennig gupta seitz mermelstein phonem hidden markov model continu mixtur output densiti larg vocabulari word recognit ieee tran signal process sheikhzadeh deng speech recognit use hidden filter model paramet select sensit power normal ieee tran speech audio jaitli hinton learn better represent speech sound wave use restrict boltzmann machin proc icassp sainath kingburi ramabhadran novak moham make deep belief network effect larg vocabulari tinuou speech recognit proc ieee asru jaitli nguyen vanhouck applic deep neural network larg vocabulari speech recognit proc interspeech hinton srivastava krizhevski sutskev nov improv neural network prevent featur detector arxiv yu deng dahl role speech recognit proc nip workshop yu li deng calibr confid measur speech recognit ieee tran audio speech maa le neil vinyal nguyen ng rent neural network nois reduct robust asr proc interspeech ling deng yu model spectral envelop use restrict boltzmann machin deep belief network tical parametr speech synthesi ieee tran audio speech lang kang qian meng deep belief network speech synthesi proc icassp http publish onlin cambridg univers press tutori survey architectur algorithm applic deep learn zen senior schuster statist parametr speech thesi use deep neural network proc icassp fernandez rendel ramabhadran hoori tour predict deep belief process hybrid model proc icassp humphrey bello lecun move beyond featur design deep architectur automat featur learn music matic proc ismir batternberg wessel analyz drum pattern use tional deep belief network proc ismir schmidt kim learn acoust featur deep belief network proc ieee applic signal process audio acoust nair hinton object recognit deep belief net proc nip lecun bengio convolut network imag speech time seri handbook brain theori neural network arbib ed mit press cambridg sachusett kavukcuoglu sermanet boureau gregor mathieu lecun learn convolut featur hierarchi visual recognit proc nip zeiler fergu stochast pool regular deep convolut neural network proc iclr lecun learn invari featur hierarchi proc eccv coat huval wang wu ng catanzaro deep learn cot hpc proc icml papandr katsamani pitsikali marago adapt multimod fusion uncertainti compens applic audiovisu speech recognit ieee tran audio speech lang deng wu droppo acero dynam compens hmm varianc use featur enhanc uncertainti pute parametr model speech distort ieee tran speech audio bengio ducharm vincent jauvin neural bilist languag model proc nip fast evalu connectionist languag model int conf artifici neural network mnih hinton three new graphic model statist languag model proc icml mnih hinton scalabl hierarch distribut languag model proc nip le allauzen wisniewski yvon train continu space languag model practic issu le oparin allauzen gauvain yvon structur output layer neural network languag model proc icassp mikolov deora povey burget cernocki gie train larg scale neural network languag model proc ieee asru mikolov statist languag model base neural network thesi brno univers technolog huang renal hierarch bayesian languag model convers speech recognit ieee tran audio speech lang collobert weston unifi architectur natur languag process deep neural network multitask learn proc icml collobert deep learn effici discrimin pars proc nip workshop deep learn unsupervis featur learn collobert weston bottou karlen kavukcuoglu kuksa natur languag process almost scratch mach learn socher bengio man deep learn nlp rial acl http huang socher man ng improv word sentat via global context multipl word prototyp proc acl zou socher cer man bilingu word ding machin translat proc emnlp gao yih deng learn semant tion phrase translat model septemb socher pennington huang ng man supervis recurs autoencod predict sentiment bution proc emnlp socher pennington huang ng man dynam pool unfold recurs autoencod paraphras tion proc nip socher perelygin wu chuang man ng pott recurs deep model semant composition sentiment treebank proc emnlp yu deng seid deep tensor neural network applic larg vocabulari speech recognit ieee tran audio speech lang salakhutdinov hinton semant hash proc sigir workshop inform retriev applic graphic model hinton salakhutdinov discov binari code ment learn deep gener model top cognit huang gao deng acero heck learn deep structur semant model web search use clickthrough data acm int conf inform knowledg manag cikm le ngiam coat lahiri prochnow ng optim method deep learn proc icml bottou lecun larg scale onlin learn proc nip bergstra bengio random search mizat mach learn snoek larochel adam practic bayesian optim machin learn algorithm proc nip http publish onlin cambridg univers press