review deep learning concept cnn architecture challenge application future direction laith jinglan amjad ayad ye omran mohammed muthana laith abstract last year deep learning dl computing paradigm ha deemed gold standard machine learning ml community moreover ha gradually become widely used computational approach field ml thus ing outstanding result several complex cognitive task matching even beating provided human performance one benefit dl ability learn massive amount data dl field ha grown fast last year ha extensively used successfully address wide range traditional application importantly dl ha outperformed ml technique many domain cybersecurity natural language processing bioinformatics robotics control medical information processing among many others despite ha uted several work reviewing dl only tackled one aspect dl lead overall lack knowledge therefore contribution propose using holistic approach order provide suitable starting point develop full understanding dl specifically review attempt provide comprehensive survey tant aspect dl including enhancement recently added field particular paper outline importance dl present type dl niques network present convolutional neural network cnns utilized dl network type describes development cnns architecture together main feature starting alexnet network closing network finally present challenge suggested solution help researcher understand existing research gap followed list major dl application computational tool including fpga gpu cpu summarized along description influence dl paper end evolution matrix benchmark datasets summary conclusion keywords deep learning machine learning convolution neural network cnn deep neural network architecture deep learning application image classification transfer learning medical image analysis supervised learning fpga gpu open access author article licensed creative common attribution international license permit use sharing adaptation distribution reproduction any medium format long give appropriate credit original author source provide link creative common licence indicate change made image third party material article included article creative common licence unless indicated otherwise credit line material material not included article creative common licence intended use not permitted statutory regulation exceeds ted use need obtain permission directly copyright holder view copy licence visit survey paper alzubaidi et al j big data 2021 correspondence 1 school computer science queensland university technology brisbane qld 4000 australia full list author information available end article page 2 74 alzubaidi et al j big data 2021 introduction recently machine learning ml ha become widespread research ha incorporated variety application including text mining spam detection video recommendation image classification multimedia concept retrieval among different ml algorithm deep learning dl commonly employed application another name dl representation learning rl continuing appearance novel study field deep distributed learning due unpredictable growth ability obtain data amazing progress made hardware technology high performance computing hpc 10 dl derived conventional neural network considerably outperforms predecessor moreover dl employ transformation graph technology neously order build learning model recently developed dl technique obtained good outstanding performance across variety tions including audio speech processing visual data processing natural language processing nlp among others usually effectiveness ml algorithm highly dependent integrity representation ha shown suitable data representation vides improved performance compared poor data representation thus significant research trend ml many year ha feature engineering ha informed numerous research study approach aim constructing feature raw data addition extremely frequently requires sizable human effort instance several type feature introduced compared computer vision context histogram oriented gradient hog 15 invariant feature transform sift 16 bag word bow 17 soon novel feature introduced found perform well becomes new research direction pursued multiple decade relatively speaking feature extraction achieved automatic way throughout dl algorithm encourages researcher extract discriminative feature using smallest possible amount human effort field knowledge 18 algorithm data representation architecture first layer extract feature last layer extract feature note artificial intelligence ai originally inspired type architecture simulates process occurs core sensorial region within human brain using different scene human brain automatically extract data representation specifically output process classified object received scene information represents input process simulates working methodology human brain thus emphasizes main benefit dl field ml dl due considerable success currently one prominent research trend paper overview dl presented adopts various perspective main concept architecture challenge application computational tool evolution matrix convolutional neural network cnn one popular used dl network 19 20 cnn dl ular nowadays main advantage cnn compared predecessor matically detects significant feature without any human supervision made used therefore dug deep cnn presenting main page 3 74 alzubaidi et al j big data 2021 component furthermore elaborated detail common cnn architecture starting alexnet network ending network several published dl review paper presented last year ever only addressed one side focusing one application topic review cnn architecture 21 dl classification plant disease 22 dl object detection 23 dl application medical image analysis 24 etc although review present good topic not provide full understanding dl topic concept detailed research gap computational tool dl tions first required understand dl aspect including concept challenge application going deep application achieve requires extensive time large number research paper learn dl including research gap application therefore propose deep review dl provide able starting point develop full understanding dl one review paper motivation behind review wa cover important aspect dl including open challenge application computational tool perspective review first step towards dl topic main aim review present important aspect dl make easy researcher student clear image dl single review paper review advance dl research helping people discover recent development field researcher would allowed decide able direction work taken order provide accurate alternative field contribution outlined follows first review almost provides deep survey important aspect deep learning review help researcher student good understanding one paper explain cnn deep popular deep learning algorithm describing concept theory architecture review current challenge limitation deep learning including lack ing data imbalanced data interpretability data uncertainty scaling catastrophic forgetting model compression overfitting vanishing gradient problem exploding gradient problem underspecification additionally discus proposed solution tackling issue provide exhaustive list medical imaging application deep learning categorizing based task starting classification ending registration discus computational approach cpu gpu fpga comparing influence tool deep learning algorithm rest paper organized follows survey methodology section describes survey methodology background section present background classification dl approach section defines classification dl approach type dl work section display type dl network cnn architecture section show cnn architecture challenge limitation deep learning alternate solution section page 4 74 alzubaidi et al j big data 2021 detail challenge dl alternate solution application deep learning tion outline application dl computational approach section explains influence computational approach cpu gpu fpga dl evaluation rics section present evaluation metric framework datasets section list framework datasets summary conclusion section present summary conclusion survey methodology reviewed significant research paper field published 2020 mainly year 2020 2019 some paper main focus wa paper reputed publisher ieee elsevier mdpi nature acm springer some paper selected arxiv reviewed 300 paper various dl topic 108 paper year 2020 76 paper year 2019 48 paper year cates review focused latest publication field dl selected paper analyzed reviewed 1 list define dl approach network type 2 list explain cnn architecture 3 present challenge dl gest alternate solution 4 ass application dl 5 ass computational approach keywords used search criterion review paper deep learning machine learning convolution neural network deep learning architecture deep learning image detection fication segmentation localization deep learning detection classification segmentation localization deep learning cpu gpu fpga deep learning transfer learning deep ing imbalanced data deep learning interpretability data deep learning overfitting deep learning underspecification figure 1 show search structure survey paper table 1 present detail some journal cited review paper background section present background dl begin quick introduction dl followed difference dl ml show situation require dl finally present reason applying dl dl subset ml fig 2 inspired information processing pattern found human brain dl doe not require any rule operate rather us large amount data map given input specific label dl designed using numerous layer algorithm artificial neural network anns provides different interpretation data ha fed 18 25 achieving classification task using conventional ml technique requires several sequential step specifically feature extraction wise feature selection learning classification furthermore feature selection ha great impact performance ml technique biased feature selection may lead incorrect nation class conversely dl ha ability automate learning ture set several task unlike conventional ml method 18 26 dl enables learning classification achieved single shot 3 dl ha become incredibly page 5 74 alzubaidi et al j big data 2021 popular type ml algorithm recent year due huge growth evolution field big data 27 28 still continuous development regarding novel formance several ml task 22 ha simplified improvement many learning field 32 33 image 34 object detection 35 36 image recognition 30 37 recently dl performance ha come exceed human performance task image classification 4 nearly scientific field felt impact technology industry business already disrupted transformed use dl leading technology company around world race improve dl even performance capability not exceed performance dl many area predicting time taken make car erie decision certify loan request predicting movie rating 38 winner 2019 nobel prize computing also known turing award three pioneer field dl yann lecun geoffrey hinton yoshua bengio 39 although large number goal achieved progress made dl context fact dl ha ability enhance human life ing additional accuracy diagnosis including estimating natural disaster 40 covery new drug 41 cancer diagnosis esteva et al 45 found dl network ha ability diagnose disease dermatologist using image 2032 disease furthermore grading prostate fig 1 search framework page 6 74 alzubaidi et al j big data 2021 cancer u general pathologist achieved average accuracy 61 google ai 44 outperformed specialist achieving average racy 70 2020 dl playing increasingly vital role early diagnosis novel coronavirus 29 dl ha become main tool many pitals around world automatic classification detection using chest table 1 some journal cited review paper journal 2019 citescore 2019 publisher journal homepage pattern recognition elsevier nition pattern recognition letter elsevier r artificial intelligence review springer 10462 com expert system application 11 elsevier n neurocomputing elsevier ting nature medicine nature nature 51 nature journal big data springer multimedia tool application springer 11042 computer method program biomedicine elsevier dicine machine learning springer 10994 machine vision application springer 138 medical image analysis elsevier si ieee access ieee jsp 39 ieee transaction knowledge data engineering ieee jsp 69 nature communication nature ieee transaction intelligent transportation system ieee jsp 6979 method elsevier acm journal emerging nologies computing system acm jetc acm computing survey acm csur applied soft computing elsevier ting electronics mdpi ronics applied science mdpi ci ieee transaction industrial informatics ieee jsp 9424 page 7 74 alzubaidi et al j big data 2021 fig 2 deep learning family fig 3 difference deep learning traditional machine learning page 8 74 alzubaidi et al j big data 2021 image type image end section saying ai pioneer geoffrey hinton deep learning going able everything apply deep learning machine intelligence useful many situation equal better human expert some case meaning dl solution following problem case human expert not available case human unable explain decision made using expertise guage understanding medical decision speech recognition case problem solution update time price prediction stock ence weather prediction tracking case solution require adaptation based specific case personalization biometrics case size problem extremely large exceeds inadequate soning ability sentiment analysis matching ad facebook calculation webpage rank deep learning several performance feature may answer question universal learning approach dl ha ability perform mately application domain sometimes referred universal learning robustness general precisely designed feature not required dl niques instead optimized feature learned automated fashion related fig 4 deep learning performance compared human page 9 74 alzubaidi et al j big data 2021 task consideration thus robustness usual change input data attained generalization different data type different application use dl technique approach frequently referred transfer learning tl explained latter section furthermore useful approach problem data insufficient scalability dl highly scalable resnet 37 wa invented microsoft comprises 1202 layer frequently applied supercomputing scale rence livermore national laboratory llnl large enterprise working evolving framework network adopted similar approach thousand node implemented 53 classification dl approach dl technique classified three major category unsupervised partially vised supervised furthermore deep reinforcement learning drl also known rl another type learning technique mostly sidered fall category partially supervised occasionally unsupervised learning technique deep supervised learning technique deal labeled data considering technique environs collection input resultant output xt yt instance smart agent guess input xt obtain loss value next network parameter repeatedly updated agent obtain improved mate preferred output following positive training outcome agent acquires ability obtain right solution query environs dl several supervised learning technique recurrent neural network rnns convolutional neural network cnns deep neural network dnns addition rnn category includes gated recurrent unit grus long memory lstm approach main advantage technique ability collect data generate data output prior knowledge however disadvantage technique decision boundary might overstrained training set sample class overall technique simpler niques way learning high performance deep learning technique learning process based datasets occasionally generative adversarial network gans drl employed way technique addition rnns include grus lstms also employed partially supervised learning one advantage technique minimize amount labeled data needed hand one disadvantage technique irrelevant input feature present training data could furnish incorrect sion text document classifier one popular example application page 10 74 alzubaidi et al j big data 2021 learning due difficulty obtaining large amount labeled text document learning ideal text document classification task deep unsupervised learning technique make possible implement learning process absence available labeled data no label required agent learns nificant feature interior representation required discover unidentified structure relationship input data technique generative network dimensionality reduction clustering frequently counted within category unsupervised learning several member dl family performed well dimensionality reduction clustering task include restricted boltzmann machine gans recently developed niques moreover rnns include grus lstm approach also employed unsupervised learning wide range application main disadvantage unsupervised learning unable provide accurate information concerning data sorting computationally complex one popular unsupervised learning approach clustering 54 deep reinforcement learning reinforcement learning operates interacting environment vised learning operates provided sample data technique wa developed 2013 google deep mind 55 subsequently many enhanced technique ent reinforcement learning constructed example input ment sample xt agent predict received cost agent p unknown probability distribution ment asks question agent answer give noisy score method sometimes referred learning based concept several supervised unsupervised technique developed comparison tional supervised technique performing learning much difficult no straightforward loss function available reinforcement learning technique addition two essential difference supervised learning forcement learning first no complete access function requires optimization meaning queried via interaction second state interacted founded environment input xt based preceding action 9 56 solving task selection type reinforcement learning need performed based space scope problem example drl best way problem involving many parameter optimized contrast reinforcement learning technique performs well problem limited parameter some application reinforcement learning ness strategy planning robotics industrial automation main drawback reinforcement learning parameter may influence speed learning main motivation utilizing reinforcement learning page 11 74 alzubaidi et al j big data 2021 assist identify action produce highest reward longer period assist discover situation requires action also enables figure best approach reaching large reward reinforcement learning also give learning agent reward function reinforcement learning utilize situation case sufficient data resolve issue supervised learning niques reinforcement learning specially workspace large type dl network famous type deep learning network discussed section include recursive neural network rvnns rnns cnns rvnns rnns briefly explained section cnns explained deep due tance type furthermore used several application among network recursive neural network rvnn achieve prediction hierarchical structure also classify output lizing compositional vector 57 recursive memory raam 58 primary inspiration rvnn development rvnn architecture ated processing object randomly shaped structure like graph tree approach generates distributed representation structure network trained using introduced structure bts learning system 58 bts system track nique propagation algorithm ha ability support treelike structure train network regenerate pattern output layer rvnn highly effective nlp context socher et al 59 duced rvnn architecture designed process input variety modality author demonstrate two application classifying natural language sentence case sentence split word nature image case image separated various segment interest rvnn computes likely pair score merging construct syntactic tree furthermore rvnn calculates score related merge plausibility every pair unit next pair largest score merged within composition vector following every merge rvnn generates larger area numerous unit b compositional vector area c label class instance noun phrase become class label new area two unit noun word compositional vector entire area root rvnn tree structure example rvnn tree shown 5 rvnn ha employed several application page 12 74 alzubaidi et al j big data 2021 recurrent neural network rnns commonly employed familiar algorithm discipline dl 65 rnn mainly applied area speech processing nlp context 66 67 unlike conventional network rnn us sequential data network since embedded structure sequence data delivers valuable information feature fundamental range different application instance tant understand context sentence order determine meaning specific word thus possible consider rnn unit memory x represents input layer output layer represents state hidden layer given input sequence typical unfolded rnn diagram illustrated 6 pascanu et al 68 introduced three different type deep rnn technique namely deep rnn introduced lessens learning difficulty deep network brings benefit deeper rnn based three technique however rnn sensitivity exploding gradient vanishing problem resent one main issue approach 69 specifically training process reduplication several large small derivative may cause gradient exponentially explode decay entrance new input work stop thinking initial one therefore sensitivity decay time furthermore issue handled using lstm 70 approach offer rent connection memory block network every memory block contains number memory cell ability store temporal state network addition contains gated unit controlling flow information deep network 37 residual connection also ability considerably reduce impact vanishing gradient issue explained later section fig 5 example rvnn tree page 13 74 alzubaidi et al j big data 2021 cnn considered powerful rnn rnn includes le feature patibility compared cnn convolutional neural network field dl cnn famous commonly employed algorithm 30 main benefit cnn compared predecessor automatically identifies relevant feature without any human supervision 76 cnns extensively applied range different field including computer vision 77 speech processing 78 face recognition 79 etc structure cnns wa inspired ron human animal brain similar conventional neural network ically cat brain complex sequence cell form visual cortex sequence simulated cnn 80 goodfellow et al 28 identified three key benefit cnn equivalent representation sparse interaction parameter sharing unlike conventional fully connected fc network shared weight local connection cnn employed make full use structure like image signal operation utilizes extremely small number parameter simplifies training process speed network visual tex cell notably only small region scene sensed cell rather whole scene cell spatially extract local correlation available input like local filter input commonly used type cnn similar perceptron mlp consists numerous convolution layer preceding pooling layer ending layer fc layer example cnn architecture image classification illustrated 7 input x layer cnn model organized three dimension height width depth r height equal width depth also referred channel number example rgb image depth r equal three several kernel filter available convolutional layer denoted k also three dimension n n q similar input image fig 6 typical unfolded rnn diagram page 14 74 alzubaidi et al j big data 2021 however n must smaller q either equal smaller tion kernel basis local connection share similar parameter bias bk weight w k generating k feature map hk size convolved input mentioned convolution layer calculates dot product input weight eq 1 similar nlp input undersized area initial image size next applying nonlinearity activation function output obtain following next step every feature map layer lead reduction network parameter accelerates training process turn enables handling overfitting issue feature map pooling tion max average applied adjacent area size p p p kernel size finally fc layer receive feature create abstraction represents layer typical neural work classification score generated using ending layer support vector machine svms softmax given instance every score represents ity specific class benefit employing cnns benefit using cnns traditional neural network computer vision environment listed follows main reason consider cnn weight sharing feature reduces number trainable network parameter turn help network enhance generalization avoid overfitting concurrently learning feature extraction layer classification layer cause model output highly organized highly reliant extracted tures 1 hk f w k bk fig 7 example cnn architecture image classification page 15 74 alzubaidi et al j big data 2021 network implementation much easier cnn ral network cnn layer cnn architecture consists number layer block layer cnn architecture including function described detail convolutional layer cnn architecture significant component convolutional layer consists collection convolutional filter nels input image expressed metric convolved filter generate output feature map kernel definition grid discrete number value describes kernel value called kernel weight random number assigned act weight kernel beginning cnn training process tion several different method used initialize weight next weight adjusted training era thus kernel learns extract cant feature convolutional operation initially cnn input format described vector format input traditional neural network channeled image input cnn instance format image rgb image format understand convolutional operation let u take example 4 4 image 2 2 random kernel first nel slide whole image horizontally vertically addition dot product input image kernel determined corresponding value multiplied summed create single lar value calculated concurrently whole process repeated no sliding possible note calculated dot product value represent feature map output figure 8 graphically illustrates primary culations executed step figure light green color represents 2 2 kernel light blue color represents similar size area input image multiplied end result summing resulting product value marked light orange color represents entry value output feature map however padding input image not applied previous example stride one denoted selected vertical zontal location applied kernel note also possible use another stride value addition feature map lower dimension obtained result increasing stride value hand padding highly significant determining border size information related input image contrast border move carried away fast applying padding size input image page 16 74 alzubaidi et al j big data 2021 increase turn size output feature map also increase core benefit convolutional layer sparse connectivity neuron layer fc neural network link neuron following layer contrast cnns only weight available two adjacent layer thus number required weight connection small memory required store weight also fig 8 primary calculation executed step convolutional layer page 17 74 alzubaidi et al j big data 2021 small hence approach addition matrix operation computationally much costly dot operation cnn weight sharing no allocated weight any two neuron neighboring layer cnn whole weight operate one pixel input matrix learning single group weight whole input significantly decrease required training time various cost not necessary learn additional weight neuron pooling layer main task pooling layer feature map map generated following convolutional operation word approach shrink feature map create smaller feature map concurrently maintains majority dominant information feature every step pooling stage similar manner convolutional operation stride kernel initially pooling operation executed several type pooling method available utilization various pooling layer method include tree pooling gated pooling average pooling min pooling max pooling global average pooling gap global max pooling familiar frequently utilized pooling method max min gap pooling figure 9 illustrates three pooling operation sometimes overall cnn performance decreased result represents main shortfall pooling layer layer help cnn determine whether not certain feature available particular input image focus exclusively ascertaining correct location feature thus cnn model miss relevant information activation function mapping input output core tion type activation function type neural network input value determined computing weighted summation neuron input along bias present mean activation function make decision whether not fire neuron reference particular input creating corresponding output fig 9 three type pooling operation page 18 74 alzubaidi et al j big data 2021 activation layer employed layer weight learnable layer fc layer convolutional layer cnn architecture performance activation layer mean mapping input output moreover layer give cnn ability learn thing activation function must also ability entiate extremely significant feature allows error used train network following type activation function commonly used cnn deep neural network sigmoid input activation function real number output restricted zero one sigmoid function curve represented mathematically 2 tanh similar sigmoid function input real number put restricted 1 mathematical representation 3 relu mostly commonly used function cnn context convert whole value input positive number lower computational load main benefit relu others mathematical representation 4 occasionally significant issue may occur use relu instance consider error algorithm larger gradient flowing passing gradient within relu function update weight way make neuron certainly not activated issue referred dying relu some relu alternative exist solve issue following discus some leaky relu instead relu negative input activation function ensures input never ignored employed solve dying relu problem leaky relu represented mathematically 5 note leak factor denoted commonly set small value noisy relu function employ gaussian distribution make relu noisy represented mathematically 6 2 f x sigm 1 1 3 f x tanh ex ex 4 f x relu max 0 x 5 f x leakyrelu x x 0 mx x 6 f x noisyrelu max x 0 σ x page 19 74 alzubaidi et al j big data 2021 parametric linear unit mostly leaky relu main ence leak factor function updated model training process parametric linear unit represented mathematically 7 note learnable weight denoted fully connected layer commonly layer located end cnn architecture inside layer neuron connected neuron vious layer fully connected fc approach utilized cnn classifier follows basic method conventional perceptron neural network type ann input fc layer come last pooling convolutional layer input form vector created feature map flattening output fc layer represents final cnn output illustrated 10 loss function previous section ha presented various cnn architecture addition final classification achieved output layer represents last layer cnn architecture some loss function lized output layer calculate predicted error created across training sample cnn model error reveals difference actual put predicted one next optimized cnn learning ce however two parameter used loss function calculate error cnn estimated output referred prediction first parameter actual output referred label second parameter several type loss function employed various problem type following concisely explains some loss function type 7 f x parametriclinear x x 0 ax x fig 10 fully connected layer page 20 74 alzubaidi et al j big data 2021 softmax loss function function commonly employed measuring cnn model performance also referred log loss function output probability p 0 1 addition usually employed substitution square error loss function sification problem output layer employ softmax activation generate output within probability distribution mathematical sentation output class probability 8 eai represents output preceding layer n represents number neuron output layer finally ical representation loss function 9 b euclidean loss function function widely used regression problem addition also mean square error mathematical sion estimated euclidean loss 10 c hinge loss function function commonly employed problem related binary classification problem relates sification mostly important svms use hinge loss function wherein optimizer attempt maximize margin around dual objective class mathematical formula 11 margin commonly set moreover predicted output denoted pi desired output denoted yi regularization cnn cnn model represents central issue associated obtaining generalization model entitled case model executes especially well training data doe not succeed test data unseen data explained latter section model opposite case occurs model doe not learn sufficient amount training data model referred executes well training testing data three type illustrated 11 various intuitive concept used help regularization avoid detail fitting discussed latter section 8 pi eai n ea k 9 h p yi log pi 1 n 10 h p 1 n pi 2 11 h p n max 0 pi page 21 74 alzubaidi et al j big data 2021 dropout widely utilized technique generalization training epoch neuron randomly dropped feature selection power distributed equally across whole group neuron well forcing model learn different independent feature training process dropped ron not part contrast network utilized perform prediction testing process method highly similar dropout training epoch connection neuron weight dropped rather dropping ron represents only difference dropout data augmentation training model sizeable amount data easiest way avoid achieve data augmentation used several niques utilized artificially expand size training dataset detail found latter section describes data augmentation technique batch normalization method ensures performance output activation 81 performance follows unit gaussian distribution subtracting mean dividing standard deviation normalize output layer possible consider task layer network also possible differentiate integrate network addition employed reduce internal covariance shift activation layer layer variation activation distribution defines internal covariance shift shift becomes high due continuous weight updating ing may occur sample training data gathered ous dissimilar source example day night image thus model sume extra time convergence turn time required training also increase resolve issue layer representing operation batch tion applied cnn architecture advantage utilizing batch normalization follows prevents problem vanishing gradient arising effectively control poor weight initialization significantly reduces time required network convergence datasets extremely useful struggle decrease training dependency across fig 11 issue page 22 74 alzubaidi et al j big data 2021 chance reduced since ha minor influence tion optimizer selection section discus cnn learning process two major issue included learning process first issue learning algorithm selection optimizer second issue use many enhancement adadelta adagrad tum along learning algorithm enhance output loss function founded numerous learnable parameter bias weight etc minimizing error variation actual predicted output core purpose supervised learning algorithm technique based learning cnn network appear usual selection network ters always update though training epoch network also look locally optimized answer training epoch order minimize error learning rate defined step size parameter updating ing epoch represents complete repetition parameter update involves complete training dataset one time note need select learning rate wisely doe not influence learning process imperfectly although gradient descent learning algorithm minimize ing error algorithm repetitively update network parameter every training epoch specifically update parameter correctly need pute objective function gradient slope applying derivative respect network parameter next parameter updated reverse direction gradient reduce error parameter updating process formed though network gradient every neuron neuron preceding layer mathematical tion operation 12 final weight current training epoch denoted wijt weight preceding training epoch denoted learning rate η tion error different alternative learning algorithm able commonly employed include following batch gradient descent execution technique 82 network parameter updated merely one time behind considering training datasets via network depth calculates gradient whole training set subsequently us gradient update parameter dataset cnn model converges faster creates gradient using bgd since parameter changed only every training epoch requires substantial amount resource contrast large training dataset additional 12 wijt wijt wijt η page 23 74 alzubaidi et al j big data 2021 time required converging could converge local optimum convex instance stochastic gradient descent parameter updated training sample technique 83 preferred arbitrarily sample training sample every epoch advance training training dataset technique much faster bgd however frequently updated take extremely noisy step direction answer turn cause convergence behavior become highly unstable gradient descent approach training sample partitioned several every considered sized collection sample no overlap 84 next parameter updating performed following gradient computation every advantage method come combining advantage bgd sgd technique thus ha steady convergence computational efficiency extra memory effectiveness following describes several enhancement niques learning algorithm usually sgd fully enhance cnn training process momentum neural network technique employed objective tion enhances accuracy training speed summing puted gradient preceding training step weighted via factor known momentum factor however therefore simply becomes stuck local minimum rather global minimum represents main disadvantage learning algorithm issue kind frequently occur issue ha no convex surface solution space together learning algorithm momentum used solve issue expressed mathematically 13 weight increment current training epoch denoted wijt η learning rate weight increment preceding training epoch momentum factor value maintained within range 0 1 turn step size weight updating increase direction bare minimum minimize error value momentum factor becomes low model loses ability avoid local bare minimum contrast tum factor value becomes high model develops ability converge much rapidly high value momentum factor used together lr model could miss global bare minimum crossing however gradient varies direction continually throughout training process suitable value momentum factor ter cause smoothening weight updating variation adaptive moment estimation adam another optimization technique ing algorithm widely used adam 85 represents latest trend deep 13 wijt η page 24 74 alzubaidi et al j big data 2021 learning optimization represented hessian matrix employ derivative adam learning strategy ha designed cally training deep neural network memory efficient le tional power two advantage adam mechanism adam calculate adaptive lr parameter model integrates pro tum rmsprop utilizes squared gradient scale learning rate rmsprop similar momentum using moving average dient equation adam represented 14 design algorithm backpropagation let start notation refers weight network unambiguously denote wh ij weight connection ith input neuron h th jt neuron hth layer fig 12 show weight connection neuron first layer another neuron next layer network 11 ha represented weight first neuron first layer first neuron second layer based second weight neuron 21 mean weight come second neuron previous layer first layer next layer second net regarding bias since bias not connection neuron layer easily handled neuron must bias some network layer ha certain bias seen net layer ha bias network ha parameter no layer net number neuron layer no weight connection layer no connection easily determined based no neuron layer example ten input fully connect two neuron next layer 14 wijt η e e fig 12 mlp structure page 25 74 alzubaidi et al j big data 2021 number connection 10 20 connection weight error defined weight updated imagine two layer neural network label induvial input ith output individual input backpropagation understanding change weight bias work based change cost function error ultimately mean ing partial derivative ij j compute local variable introduced j called local error jth neuron hth layer based local error backpropagation give procedure compute ij j error defined weight updated imagine two layer neural network shown 13 output error j 1 1 l l no neuron output e k error epoch k shown eq 2 vj k derivate activation function vj output backpropagate error rest layer except output j k output error jl k represented weight layer error need obtain finding error neuron layer update weight layer based eq 16 17 improving performance cnn based experiment different dl application conclude active solution may improve performance cnn 15 error di 2 16 j k e k vj k 17 δh j k vj k l j jl k fig 13 neuron activation function page 26 74 alzubaidi et al j big data 2021 expand dataset data augmentation use transfer learning explained ter section increase training time increase depth width model add regularization increase hyperparameters tuning cnn architecture last 10 year several cnn architecture presented 21 26 model architecture critical factor improving performance different application various modification achieved cnn architecture 1989 today modification include structural reformulation regularization parameter zations etc conversely noted key upgrade cnn performance occurred largely due reorganization well development novel block particular novel development cnn architecture performed use network depth section review popular cnn architecture beginning alexnet model 2012 ending resolution hr model studying architecture feature input size depth robustness key help researcher choose suitable architecture target task table 2 present brief overview cnn architecture alexnet history deep cnns began appearance lenet 89 14 time cnns restricted handwritten digit recognition task not scaled image class deep cnn architecture alexnet highly respected 30 achieved innovative result field image recognition classification krizhevesky et al 30 first proposed alexnet consequently improved cnn learning ability increasing depth implementing several parameter tion strategy figure 15 illustrates basic design alexnet architecture learning ability deep cnn wa limited time due hardware restriction overcome hardware limitation two gpus nvidia gtx 580 used parallel train alexnet moreover order enhance ity cnn different image category number feature extraction stage wa increased five lenet seven alexnet regardless fact depth enhances generalization several image resolution wa fact overfitting resented main drawback related depth krizhevesky et al used hinton idea address problem 90 91 ensure feature learned algorithm extra robust krizhevesky et al algorithm randomly pass several tional unit throughout training stage moreover reducing vanishing gradient problem relu 92 could utilized activation function enhance rate convergence 93 local response normalization overlapping subsampling also performed enhance generalization decreasing overfitting improve performance previous network modification made using filter 5 5 11 11 earlier layer alexnet ha considerable page 27 74 alzubaidi et al j big data 2021 table 2 brief overview cnn architecture model main finding depth dataset error rate input size year alexnet utilizes dropout relu 8 imagenet 227 227 3 2012 nin new layer called mlpconv utilizes gap 3 100 mnist 32 32 3 2013 zfnet visualization idea middle layer 8 imagenet 224 224 3 2014 vgg increased depth small filter size 16 19 imagenet 224 224 3 2014 googlenet increased depth block concept ent filter size concatenation concept 22 imagenet 224 224 3 2015 utilizes small filtersize better feature tation 48 imagenet 229 229 3 2015 highway presented tipath concept 19 32 32 32 3 2015 divided transform integration concept 70 imagenet 229 229 3 2016 resnet robust overfitting due symmetry skip link 152 imagenet 224 224 3 2016 introduced concept residual link 164 imagenet 229 229 3 2016 fractalnet introduced concept regularization 32 32 3 2016 wideresnet decreased depth increased width 28 32 32 3 2016 xception depthwise volutionfollowed pointwise convolution 71 imagenet 229 229 3 2017 residual attention neural network presented attention nique 452 100 40 40 3 2017 tation network modeled dependency nels 152 imagenet 229 229 3 2017 224 224 3 320 320 3 densenet block layer layer connected 201 100 imagenet 224 224 3 2017 competitive squeeze excitation work residual identity ping utilized rescale channel 152 32 32 3 2018 page 28 74 alzubaidi et al j big data 2021 significance recent cnn generation well beginning innovative research era cnn application network model ha some slight difference preceding model introduced two innovative concept 94 first wa employing multiple layer table 2 continued model main finding depth dataset error rate input size year inverted residual structure 53 imagenet 224 224 3 2018 capsulenet pay attention special ship feature 3 mnist 28 28 1 2018 representation imagenet 224 224 3 2020 fig 14 architecture lenet fig 15 architecture alexnet page 29 74 alzubaidi et al j big data 2021 perception convolution convolution executed using filter port addition extra nonlinearity network moreover support ing network depth may later regularized using dropout dl model idea frequently employed bottleneck layer substitution fc layer gap also employed represents second novel concept enables nificant reduction number model parameter addition gap considerably update network architecture generating final feature vector no reduction feature map dimension possible gap used large feature map 95 96 figure 16 show structure network zefnet 2013 cnn learning mechanism wa basically constructed error basis precluded understanding precise purpose following enhancement issue restricted deep cnn performance convoluted image response zeiler fergus introduced deconvnet multilayer ral network 2013 97 method later became known zefnet wa oped order quantitively visualize network monitoring cnn performance via understanding neuron activation wa purpose network activity zation however erhan et al utilized exact concept optimize deep belief network dbn performance visualizing feature hidden layer 98 moreover addition issue le et al assessed deep unsupervised ae formance visualizing created class image using output neuron 99 reversing operation order convolutional pooling layer denconvnet operates like cnn reverse mapping kind launch tional layer output backward create visually observable image shape accordingly give neural interpretation internal feature representation learned layer 100 monitoring learning schematic training stage wa key cept underlying zefnet addition utilized outcome recognize ability issue coupled model concept wa experimentally proven alexnet ing deconvnet indicated only certain neuron working others action first two layer network furthermore indicated feature extracted via second layer contained aliasing object thus zeiler fergus changed cnn topology due existence outcome addition executed parameter optimization also exploited cnn learning ing stride filter size order retain feature initial two lutional layer improvement performance wa accordingly achieved due fig 16 architecture page 30 74 alzubaidi et al j big data 2021 rearrangement cnn topology rearrangement proposed visualization feature could employed identify design weakness conduct appropriate parameter alteration figure 17 show structure network visual geometry group vgg cnn wa determined effective field image recognition easy efficient design principle cnn wa proposed simonyan zisserman vative design wa called visual geometry group vgg multilayer model 101 tured nineteen layer zefnet 97 alexnet 30 simulate relation network representational capacity depth conversely petition zefnet wa frontier network proposed filter small size could enhance cnn performance reference result vgg inserted layer heap 3 3 filter rather 5 5 11 11 filter zefnet showed experimentally parallel assignment filter could duce influence filter word filter made receptive field similarly efficient filter 7 7 5 5 decreasing number parameter extra advantage reducing computational complication wa achieved using filter outcome established novel research trend working filter cnn addition inserting 1 1 convolution middle convolutional layer vgg regulates network complexity learns linear grouping subsequent feature map respect network tuning max pooling layer 102 inserted following convolutional layer padding implemented maintain spatial resolution general vgg obtained significant result localization problem image classification not achieve first place competition acquired reputation due enlarged depth homogenous topology simplicity however vgg tional cost wa excessive due utilization around 140 million parameter represented main shortcoming figure 18 show structure network googlenet competition googlenet also called emerged winner 103 achieving accuracy decreased computational cost core aim googlenet architecture proposed novel inception block module concept cnn context since combine convolutional mations employing merge transform split function feature extraction ure 19 illustrates inception block architecture architecture incorporates filter fig 17 architecture zefnet page 31 74 alzubaidi et al j big data 2021 different size 5 5 3 3 1 1 capture channel information together spatial information diverse range spatial resolution common convolutional layer googlenet substituted small block using concept nin architecture 94 replaced layer work googlenet concept merge transform split utilized supported attending issue correlated different learning type variant existing similar class several image motivation googlenet wa improve ciency cnn parameter well enhance learning capacity addition regulates computation inserting 1 1 convolutional filter bottleneck layer ahead using kernel googlenet employed sparse connection overcome redundant information problem decreased cost neglecting irrelevant nels noted only some input channel connected some output channel employing gap layer end layer rather utilizing fc layer density connection wa decreased number parameter wa also significantly decreased 40 5 million parameter due parameter tuning additional regularity factor used included employment rmsprop mizer batch normalization 104 furthermore googlenet proposed idea iliary learner speed rate convergence conversely main shortcoming googlenet wa heterogeneous topology shortcoming requires adaptation one module another shortcoming googlenet include representation fig 18 architecture vgg fig 19 basic structure google block page 32 74 alzubaidi et al j big data 2021 jam substantially decreased feature space following layer turn occasionally lead valuable information loss highway network increasing network depth enhances performance mainly complicated task contrast network training becomes difficult presence several layer deeper network may result small gradient value error lower layer 2015 srivastava et al 105 suggested novel cnn architecture called highway network overcome issue approach based tivity concept unhindered information flow highway network empowered instructing two gating unit inside layer gate mechanism concept wa motivated rnn 106 107 information aggregation wa conducted ing information ıth layer next ıth layer generate tion impact make training deeper network simple empowers training network 100 layer deeper network 900 layer sgd algorithm highway network depth fifty layer presented improved rate convergence better thin deep architecture time 108 contrast 69 empirically demonstrated plain net performance decline ten hidden layer inserted noted even highway network 900 layer depth converges much rapidly plain network resnet et al 37 developed resnet residual network wa winner vrc objective wa design network free vanishing gradient issue compared previous network several type resnet developed based number layer starting 34 layer going 1202 layer common type wa comprised 49 convolutional er plus single fc layer overall number network weight wa overall number mac wa novel idea resnet use bypass pathway concept shown fig 20 wa employed highway net address problem training deeper network illustrated fig 20 contains fundamental resnet block diagram conventional forward network plus residual connection residual layer output fied l th output delivered preceding layer xl executing different operation convolution using filter batch normalization applying activation function like relu xl output f xl ending residual output xl mathematically represented 18 numerous basic residual block included residual network based type residual network architecture operation residual block also changed 37 18 xl f xl xl page 33 74 alzubaidi et al j big data 2021 comparison highway network resnet presented shortcut connection inside layer enable connectivity independent note layer characterize function gated shortcut closed highway network contrast individuality shortcut never closed residual information permanently passed resnet thermore resnet ha potential prevent problem gradient diminishing shortcut connection residual link accelerate deep network convergence resnet wa winner championship 152 layer depth represents 8 time depth vgg 20 time depth alexnet parison vgg ha lower computational complexity even enlarged depth inception resnet szegedy et al 103 109 110 proposed upgraded type concept behind wa minimize computational cost no effect deeper network generalization thus szegedy et al used asymmetric filter 1 5 1 7 rather size filter 7 7 5 5 moreover utilized bottleneck 1 1 convolution prior filter 110 change make operation traditional fig 20 block diagram resnet page 34 74 alzubaidi et al j big data 2021 convolution similar correlation previously lin et al utilized 1 1 filter potential nin architecture 94 subsequently 110 utilized idea intelligent manner using 1 1 convolutional operation input data mapped three four isolated space smaller initial input space next correlation mapped smaller space common 5 5 3 3 convolution contrast szegedy et al bring together inception block residual learning power replacing filter concatenation residual connection 111 szegedy et al empirically onstrated residual connection achieve similar generalization power enlarged width depth residual connection thus clearly illustrated using residual connection training significantly accelerate inception network training figure 21 show basic block diagram inception residual unit densenet solve problem vanishing gradient densenet wa presented following direction resnet highway network 105 111 112 one back resnet clearly conserve information mean preservative viduality transformation several layer contribute extremely little no information addition resnet ha large number weight since layer ha isolated group weight densenet employed connectivity improved approach address problem connected layer layer network using approach therefore feature map previous layer employed input following layer traditional cnns l nections previous layer current layer densenet l 2 direct connection densenet demonstrates influence depth fig 21 basic block diagram inception residual unit page 35 74 alzubaidi et al j big data 2021 thus network gain ability discriminate clearly added preserved information since densenet concatenates feature preceding layer rather adding however due narrow layer ture densenet becomes parametrically addition increased number feature map direct admission layer gradient via loss function enhances information flow across network addition includes larizing impact minimizes overfitting task alongside minor training set ure 22 show architecture densenet network resnext resnext enhanced version inception network 115 also known aggregated residual transform network cardinality new term presented 115 utilized split transform merge topology easy effective way denotes size transformation set extra dimension however inception network manages network resource efficiently well enhancing learning ability conventional cnn transformation branch different spatial embeddings employing 5 5 3 3 1 1 used thus customizing layer required separately contrast resnext derives characteristic feature resnet vgg inception employed vgg deep homogenous topology basic architecture googlenet setting 3 3 filter spatial resolution inside block split transform merge figure 23 show resnext building block resnext utilized inside block split transform merge well outlining transformation cardinality term performance nificantly improved increasing cardinality xie et al showed complexity fig 22 architecture densenet network adopted 112 page 36 74 alzubaidi et al j big data 2021 resnext wa regulated employing 1 1 filter low embeddings ahead 3 3 convolution contrast skipping connection used optimized training 115 wideresnet feature reuse problem core shortcoming related deep residual network since certain feature block transformation contribute small amount ing zagoruyko komodakis 119 accordingly proposed wideresnet address problem author advised depth ha supplemental influence residual unit convey core learning ability deep residual network wideresnet utilized residual block power via making resnet wider instead deeper 37 enlarged width presenting extra factor k handle network width word indicated layer widening highly successful method formance enhancement compared deepening residual network enhanced representational capacity achieved deep residual network network also certain drawback exploding vanishing gradient problem feature reuse problem inactivation several feature map nature training et al 37 tackled feature reuse problem including dropout residual block regularize network efficient manner similar manner utilizing dropout huang et al 120 presented stochastic depth concept solve slow learning gradient vanishing problem earlier research wa focused ing depth thus any small enhancement performance required addition several new layer comparing number parameter wideresnet ha twice resnet experimental study showed contrast wideresnet present improved method training relative deep network 119 note tures prior residual network including highly effective vgg inception wider resnet thus wider residual network established wa mined however inserting dropout convolutional layer opposed within residual block made learning effective wideresnet 121 122 pyramidal net depth feature map increase succeeding layer due deep ing layer shown previous deep cnn architecture fig 23 basic block diagram resnext building block page 37 74 alzubaidi et al j big data 2021 resnet vgg alexnet contrast spatial dimension reduces since pling follows convolutional layer thus augmented feature representation pensed decreasing size feature map extreme expansion depth feature map alongside spatial information loss interferes learning ability deep cnns resnet obtained notable outcome issue image sification conversely deleting convolutional number channel spatial dimension vary channel depth enlarges spatial dimension reduces result decreased classifier performance accordingly chastic resnet enhanced performance decreasing information loss panying residual unit drop han et al 123 proposed pyramidal net address resnet learning interference problem address depth enlargement extreme reduction spatial width via resnet pyramidal net slowly enlarges residual unit width cover feasible place rather saving spatial dimension inside residual block appearance wa referred pyramidal net due slow enlargement feature map depth based method factor l wa determined eq 19 regulates depth feature map dimension lth residual unit indicated dl moreover n indicates overall number residual unit step factor indicated depth increase regulated factor n uniformly distributes weight increase across dimension feature map identity mapping used insert ual connection among layer comparison shortcut nections identity mapping requires fewer parameter turn lead enhanced generalization 124 widening two different approach used pyramidal net network widening specifically first approach multiplication enlarges geometrically second one tion enlarges linearly 92 main problem associated width enlargement growth time space required related quadratic time xception extreme inception architecture main characteristic xception main idea behind xception depthwise separable convolution 125 xception model adjusted original inception block making wider exchanging single dimension 3 3 followed 1 1 convolution reduce computational ity figure 24 show xception block architecture xception network becomes extra computationally effective use decoupling channel spatial correspondence moreover first performs mapping convolved output embedding short dimension applying 1 1 convolution performs k spatial transformation note k represents cardinality obtained via transformation number xception however computation made simpler xception distinctly convolving channel around spatial ax 19 dl 16 l 1 n 2 1 page 38 74 alzubaidi et al j big data 2021 ax subsequently used 1 1 convolution pointwise convolution performing correspondence 1 1 convolution utilized xception regularize depth channel traditional convolutional operation tion utilizes number transformation segment equivalent number channel inception moreover utilizes three transformation segment traditional cnn architecture utilizes only single transformation segment conversely suggested xception transformation approach achieves extra learning efficiency better mance doe not minimize number parameter 126 127 residual attention neural network improve network feature representation wang et al 128 proposed residual attention network ran enabling network learn aware feature object main purpose incorporating attention cnn ran consists stacked residual block addition attention module hence cnn however attention module divided two branch namely mask branch trunk branch branch adopt learning strategy respectively encapsulating two different strategy attention model support attention feedback fast processing only one particular forward process specifically architecture generates dense feature make inference every aspect moreover feedforward ture generates feature map addition robust semantic information restricted boltzmann machine employed strategy ously proposed study 129 training reconstruction phase goh et al 130 used mechanism attention deep boltzmann machine dbms regularizing factor note network globally optimized using learning strategy similar manner map progressively output input throughout learning process incorporating attention concept convolutional block easy way wa used transformation network obtained previous study 133 unfortunately inflexible represents main problem along inability fig 24 basic block diagram xception block architecture page 39 74 alzubaidi et al j big data 2021 used varying surroundings contrast stacking module ha made ran effective recognizing noisy complex cluttered image ran chical organization give capability adaptively allocate weight every feature map depending importance within layer furthermore incorporating three distinct level attention spatial channel mixed enables model use ability capture feature distinct level convolutional block attention module importance feature map utilization attention mechanism certified via ran 128 134 135 convolutional block attention cbam module novel cnn wa first developed woo et al 136 module similar simple design disregard object spatial locality image considers only channel contribution image classification regarding object detection object spatial location play nificant role convolutional block attention module sequentially infers attention map specifically applies channel attention preceding spatial attention obtain refined feature map spatial attention performed using 1 1 convolution pooling function literature generating effective feature descriptor achieved using spatial axis along pooling feature addition erating robust spatial attention map possible cbam concatenates max ing average pooling operation similar manner collection gap max pooling operation used model feature map statistic woo et al 136 strated utilizing gap return inference channel attention whereas max pooling provides indication distinguishing object feature thus utilization max pooling average pooling enhances network tional power feature map improve representational power well facilitating focus significant portion chosen feature expression attention map serial learning procedure assist decreasing computational cost number parameter woo et al 136 experimentally proved note any cnn architecture simply integrated cbam concurrent spatial channel excitation mechanism make work valid segmentation task roy et al 137 138 expanded hu et al 134 effort adding influence spatial information channel information roy et al 137 138 presented three type module 1 channel squeeze tion concurrent channel scse 2 exciting spatially squeezing sse 3 exciting squeezing spatially cse segmentation pose employed cnns addition suggested inserting module following encoder decoder layer specifically highlight specific feature map allocated attention every channel expressing scaling factor channel spatial information first module scse second module sse feature map information ha lower importance spatial locality spatial information play significant role segmentation ce therefore several channel collection spatially divided developed page 40 74 alzubaidi et al j big data 2021 employed segmentation final module cse similar cept used furthermore scaling factor derived founded contribution feature map within object detection 137 138 capsulenet cnn efficient technique detecting object feature achieving recognition performance comparison innovative handcrafted feature detector number restriction related cnn present meaning cnn doe not consider certain relation orientation size perspective feature instance considering face image cnn doe not count various face component mouth eye nose etc position incorrectly activate cnn ron recognize face without taking specific relation size orientation etc account point consider neuron ha probability addition feature property size orientation perspective etc specific type ha ability effectively detect face along different type mation thus many layer capsule node used construct capsule network encoding unit contains three layer capsule node form capsulenet capsnet initial version capsule network example mnist architecture comprises 28 28 image applying 256 filter size 9 9 stride 28 1 20 output plus 256 feature map next output input first capsule layer producing vector rather scalar fact modified convolution layer note stride 2 9 9 filter employed first convolution layer thus dimension output 20 1 6 initial capsule employ 8 32 filter generate 32 8 6 6 32 group 8 neuron 6 6 neuron size figure 25 represents complete capsnet encoding decoding process cnn context layer frequently employed handle translation change detect feature move event feature still within window approach ha ability detect overlapped feature fig 25 complete capsnet encoding decoding process page 41 74 alzubaidi et al j big data 2021 highly significant detection segmentation operation since capsule involves weighted feature sum preceding layer conventional cnns particular cost function employed evaluate global error grows toward back throughout training process conversely case activation neuron not grow weight two neuron turn zero instead single size provided plete cost function repetitive dynamic routing alongside agreement signal directed based feature parameter sabour et al 139 provides detail architecture using mnist recognize handwritten digit innovative cnn architecture give superior accuracy application tive architecture ha extra suitability segmentation detection approach compared classification approach network hrnet representation necessary vision task semantic segmentation object detection human pose estimation present framework input image encoded sentation using subnetwork constructed connected series resolution convolution vggnet resnet tion recovered become one alternatively tion representation maintained entire process using novel network referred network hrnet 143 144 network ha two principal feature first convolution series resolution nected parallel second information across resolution repeatedly exchanged advantage achieved includes getting representation accurate spatial domain semantic domain moreover hrnet ha several application field object detection semantic tion human pose prediction computer vision problem hrnet sent robust backbone figure 26 illustrates general architecture hrnet challenge limitation deep learning alternate solution employing dl several difficulty often taken consideration challenging listed next several possible alternative accordingly provided fig 26 general architecture hrnet page 42 74 alzubaidi et al j big data 2021 training data dl extremely considering also involves representation learning 145 146 dl demand extensively large amount data achieve formance model data increase extra performance model achieved 27 case available data sufficient obtain good performance model however sometimes shortage data using dl directly 87 properly address issue three suggested method able first involves employment concept data collected similar task note transferred data not directly ment actual data help term enhancing original input sentation data mapping function 147 way model performance boosted another technique involves employing model similar task ending two layer even one layer based limited original data refer 148 149 review different technique applied dl approach second method data augmentation performed 150 task helpful use augmenting image data since image translation mirroring rotation commonly not change image label versely important take care applying technique some case bioinformatics data instance mirroring enzyme sequence output data may not represent actual enzyme sequence third method simulated data considered increasing volume training set occasionally possible create simulator based physical process issue well understood therefore result involve simulation much data needed processing data requirement simulation obtained example ref 151 transfer learning recent research ha revealed widespread use deep cnns offer breaking support answering many classification problem generally speaking deep cnn model require sizable volume data obtain good performance fig 27 performance dl regarding amount data page 43 74 alzubaidi et al j big data 2021 common challenge associated using model concern lack training data indeed gathering large volume data exhausting job no successful solution available time undersized dataset problem therefore rently solved using tl technique 148 149 highly efficient ing lack training data issue mechanism tl involves training cnn model large volume data next step model ing small request dataset relationship suitable approach clarifying tl gathering detailed knowledge subject first step 152 next teacher provides course conveying information within lecture series time put simply teacher transfer information student detail expert teacher transfer knowledge information learner student similarly dl network trained using vast volume data also learns bias weight training process weight transferred different network retraining testing similar novel model thus novel model enabled weight rather requiring training scratch figure 28 illustrates conceptual diagram tl technique model many cnn model alexnet 30 googlenet 103 resnet 37 trained large datasets imagenet image ognition purpose model employed recognize different task without need train scratch furthermore weight remain apart learned feature case data sample lacking model useful many reason employing model first training large model sizeable datasets requires computational power second training large model taking multiple week finally model assist network generalization speed convergence fig 28 conceptual diagram tl technique page 44 74 alzubaidi et al j big data 2021 research problem using model training dl approach requires massive number image thus obtaining good performance challenge circumstance achieving excellent outcome image classification ognition application performance occasionally superior human becomes possible use deep convolutional neural network dcnns including several layer huge amount data available 37 148 153 ever avoiding overfitting problem application requires sizable datasets properly generalizing dcnn model training dcnn model dataset size ha no lower limit however accuracy model becomes insufficient case utilized model ha fewer layer small dataset used training due problem due no ability lize hierarchical feature sizable datasets model fewer layer poor accuracy difficult acquire sufficient training data dl model ple medical imaging environmental science gathering labelled datasets costly 148 moreover majority crowdsourcing worker unable make accurate note medical biological image due lack medical biological knowledge thus ml researcher often rely field expert label image however process costly time consuming therefore producing large volume label required develop flourishing deep network turn unfeasible recently tl ha widely employed address later issue nevertheless although tl enhances accuracy several task field tern recognition computer vision 154 155 essential issue related source data type used tl compared target dataset instance enhancing medical image classification performance cnn model achieved training model using imagenet dataset contains natural image 153 however natural image completely dissimilar raw medical image meaning model performance not enhanced ha proven tl different domain doe not significantly affect performance medical imaging task lightweight model trained scratch perform nearly well standard model 156 therefore exists narios using model not become affordable solution 2020 some researcher utilized tl achieved excellent result 157 tl approach using image look similar target dataset training example using image different chest eas train model training chest image diagnosis detail tl implement process found 87 data augmentation technique goal increase amount available data avoid overfitting issue data augmentation technique one possible solution 150 158 159 technique solution any problem data augmentation incorporates collection method improve attribute size training datasets thus dl page 45 74 alzubaidi et al j big data 2021 network perform better technique employed next list some data augmentation alternate solution flipping flipping vertical axis le common practice flipping zontal one flipping ha verified valuable datasets like imagenet moreover highly simple implement addition not conserving transformation datasets involve text recognition svhn mnist color space encoding digital image data commonly used dimension tensor height width colorchannels accomplishing augmentation color space channel alternative technique extremely workable mentation easy color augmentation involves separating channel ticular color red green blue simple way rapidly convert image using channel achieved separating matrix inserting tional double zero remaining two color channel furthermore ing decreasing image brightness achieved using straightforward matrix operation easily manipulate rgb value deriving color histogram describes image additional improved color augmentation obtained lighting alteration also made possible adjusting intensity value gram similar employed application cropping cropping dominant patch every single image technique employed combined dimension height width specific processing step image data furthermore random cropping may employed produce impact similar translation difference translation random cropping translation conserve spatial dimension image random ping reduces input size example 256 256 224 224 according selected reduction threshold cropping transformation may not addressed rotation rotating image left right within 0 360 degree around axis rotation augmentation obtained rotation degree parameter greatly determines suitability rotation augmentation digit recognition task small rotation 0 20 degree helpful contrast data label not preserved rotation degree increase translation avoid positional bias within image data useful tion shift image left right instance common whole dataset image centered moreover tested dataset entirely made centered image test model note translating initial image particular direction residual space filled gaussian random noise constant value 255 0 spatial dimension image preserved using padding noise injection approach involves injecting matrix arbitrary value matrix commonly obtained gaussian distribution et al 160 employed nine datasets test noise injection datasets taken uci repository 161 injecting noise within image enables cnn learn additional robust feature page 46 74 alzubaidi et al j big data 2021 however highly solution positional bias available within training data achieved mean geometric transformation separate distribution testing data training data several prospective source bias exist instance face completely centered within frame facial recognition datasets problem positional bias emerges thus geometric translation best solution geometric translation helpful due simplicity implementation well effective capability able positional bias several library image processing available enables beginning simple operation rotation horizontal flipping additional training time higher computational cost additional memory some shortcoming geometric transformation furthermore number metric transformation arbitrary cropping translation ally observed ensure not change image label finally bias separate test data training data complicated tional positional change hence not trivial answering geometric transformation suitable applied imbalanced data commonly biological data tend imbalanced negative sample much numerous positive one example compared image volume normal image large noted undesirable result may produced training dl model using imbalanced data following technique used solve issue first necessary employ correct criterion evaluating loss well prediction result considering imbalanced data model perform well small class well larger one thus model employ area curve auc resultant loss well criterion 165 second employ weighted loss ensures model perform well small class still prefers employ loss simultaneously model training possible either large class small class finally make data balanced ref 166 possible construct model every hierarchical level biological system frequently ha hierarchical label space however effect imbalanced data performance dl model ha comprehensively tigated addition lessen problem frequently used technique also compared nevertheless note technique not specified biological problem interpretability data occasionally dl technique analyzed act black box fact pretable need method interpreting dl used obtain able motif pattern recognized network common many field bioinformatics 167 task disease diagnosis not only required know disease diagnosis prediction result trained dl model also enhance surety prediction outcome model make decision based verification 168 achieve possible give score importance every page 47 74 alzubaidi et al j big data 2021 portion particular example within solution niques approach used 169 approach portion input changed effect change model output observed concept ha high computational complexity simple understand hand check score importance ous input portion signal output propagates back input layer technique technique proven valuable 174 different scenario various meaning represent model interpretability uncertainty scaling commonly final prediction label not only label required employing dl technique achieve prediction score confidence every inquiry model also desired score confidence defined confident model prediction 175 since score confidence prevents belief unreliable misleading prediction significant attribute regardless application scenario biology confidence score reduces resource time expended proving outcome misleading prediction generally speaking healthcare similar application uncertainty scaling frequently significant help evaluating automated clinical decision reliability machine nosis 176 177 overconfident prediction output different dl model score probability achieved softmax output often not correct scale 178 note softmax output requires achieve reliable probability score outputting probability score rect scale several technique introduced including bayesian binning quantiles bbq 179 isotonic regression 180 histogram binning 181 endary platt scaling 182 specifically dl technique temperature scaling wa recently introduced achieves superior performance compared technique catastrophic forgetting defined incorporating new information plain dl model made possible interfering learned information instance consider case 1000 type flower model trained classify flower new type flower introduced model only new class performance become unsuccessful older class 183 184 logical data continually collected renewed fact highly typical scenario many field biology address issue direct solution involves employing old new data train entirely new model scratch solution computationally intensive furthermore lead unstable state learned representation initial data time three different type ml technique not catastrophic forgetting made available solve human brain problem founded neurophysiological theory 185 186 niques first type founded regularization ewc 183 technique second type employ rehearsal training technique dynamic neural network page 48 74 alzubaidi et al j big data 2021 architecture like icarl 187 188 finally technique third type founded learning system 189 refer order gain detail model compression obtain model still employed productively dl model intensive memory computational requirement due huge complexity large number parameter 193 194 one field characterized intensive field healthcare environmental science need reduce deployment dl limited machine mainly healthcare field numerous method assessing human health data heterogeneity become far complicated vastly larger size 195 thus issue requires additional computation 196 furthermore novel parallel processing solution fpgas gpus developed solve putation issue associated dl recently numerous technique compressing dl model designed decrease computational issue model starting point also introduced technique classified four class first class redundant parameter no significant impact model performance reduced class includes famous deep sion method called parameter pruning 200 second class larger model us distilled knowledge train compact model thus called knowledge distillation 201 202 third class compact convolution filter used reduce number parameter 203 final class information parameter mated preservation using factorization 204 model compression class represent representative technique 193 ha provided comprehensive discussion topic overfitting dl model excessively high possibility resulting data overfitting ing stage due vast number parameter involved correlated plex manner situation reduce model ability achieve good performance tested data 90 205 problem not only limited specific field involves different task therefore proposing dl technique problem fully considered accurately handled dl implied bias training process enables model overcome crucial overfitting problem recent study suggest even still necessary develop technique handle ting problem investigation available dl algorithm ease overfitting problem categorize three class first class act model architecture model parameter includes familiar approach weight decay 209 batch normalization 210 dropout 90 dl default nique weight decay 209 used extensively almost ml algorithm universal regularizer second class work model input data corruption data augmentation 150 211 one reason overfitting problem lack training data make learned distribution not mirror real distribution data augmentation enlarges training data contrast marginalized data tion improves solution exclusive augmenting data final class work page 49 74 alzubaidi et al j big data 2021 model output recently proposed technique penalizes output regularizing model 178 technique ha demonstrated ability regularize rnns cnns vanishing gradient problem general using backpropagation learning technique along anns largely training stage problem called vanishing gradient lem arises specifically training iteration every weight neural network updated based current weight proportionally relative partial derivative error function however weight updating may not occur some case due vanishingly small gradient worst case mean no extra training possible neural network stop completely conversely larly activation function sigmoid function shrink large input space tiny input space thus derivative sigmoid function small due large variation input produce small variation output shallow network only some layer use activation not significant issue using layer lead gradient become small training stage case network work efficiently technique used determine dients neural network initially technique determines network tives layer reverse direction starting last layer progressing back first layer next step involves multiplying derivative layer network similar manner first step instance multiplying n small derivative together n hidden layer employ activation function sigmoid function hence gradient decline exponentially propagating back first layer specifically bias weight first layer not updated efficiently training stage gradient small moreover condition decrease overall network accuracy first layer frequently critical recognizing essential element input data however problem avoided employing activation function function lack ing property ability squish input space within small space mapping x max relu 91 popular selection doe not yield small tive employed field another solution involves employing batch malization layer 81 mentioned earlier problem occurs large input space squashed small space leading vanishing derivative employing batch malization degrades issue simply normalizing input expression doe not accomplish exterior boundary sigmoid function normalization process make largest part come green area ensures derivative large enough action furthermore faster hardware tackle previous issue provided gpus make standard possible many deeper layer network compared time required nize vanishing gradient problem 215 page 50 74 alzubaidi et al j big data 2021 exploding gradient problem opposite vanishing problem one related gradient specifically large error gradient accumulated latter lead extremely significant update weight network meaning system becomes unsteady thus model lose ability learn effectively grosso modo moving backward network gradient grows tially repetitively multiplying gradient weight value could thus become ibly large may overflow become nan value some potential solution include using different weight regularization technique redesigning architecture network model underspecification 2020 team computer scientist google ha identified new challenge called underspecification 219 ml model including dl model often show surprisingly poor behavior tested application computer vision cal imaging natural language processing medical genomics reason behind weak performance due underspecification ha shown small tions force model towards completely different solution well lead ferent prediction deployment domain different technique addressing underspecification issue one design stress test examine good model work data find possible issue nevertheless demand reliable understanding process model work inaccurately team stated designing stress test applied ments provide good coverage potential failure mode major challenge underspecification put major constraint credibility ml prediction may require some reconsidering certain application since ml linked human serving several application medical imaging car require proper attention issue application deep learning presently various dl application widespread around world tions include healthcare social network analysis audio speech processing like ognition enhancement visual data processing method multimedia data analysis computer vision nlp translation sentence classification among others fig 29 application classified five category classification localization detection segmentation registration although task ha target fundamental overlap pipeline tion application shown 30 classification concept categorizes set data class detection used locate interesting object image consideration given background detection multiple object could dissimilar class surrounded bounding box localization concept page 51 74 alzubaidi et al j big data 2021 used locate object surrounded single bounding box tion semantic segmentation target object edge surrounded outline also label moreover fitting single image could onto another refers registration one important dl application healthcare area research critical due relation human life moreover dl ha shown tremendous performance healthcare therefore take dl application medical image analysis field example describe dl application fig 29 example dl application fig 30 workflow deep learning task page 52 74 alzubaidi et al j big data 2021 classification diagnosis cadx another title sometimes used tion bharati et al 231 used chest dataset detecting lung disease based cnn another study attempted read image employing cnn 232 modality comparative accessibility image ha likely enhanced progress dl 233 used improved googlenet cnn containing image training testing process dataset wa augmented 1850 chest creator reorganized image orientation lateral frontal view achieved approximately 100 accuracy work orientation sification ha clinically limited use part ultimately fully automated si workflow obtained data augmentation efficiency learning metadata relevant image chest infection commonly referred nia extremely treatable commonly occurring health problem worldwide conversely rajpurkar et al 234 utilized chexnet improved version densenet 112 121 convolution layer classifying fourteen type disease author used dataset 235 comprises image network achieved excellent performance recognizing fourteen different disease particular pneumonia classification accomplished auc score using receiver operating characteristic roc analysis addition network obtained better equal performance panel four individual ologists zuo et al 236 adopted cnn candidate classification lung nodule shen et al 237 employed random forest rf svm classifier cnns classify lung nodule employed two convolutional layer three parallel cnns lung image database consortium dataset tained ct lung scan wa used classify two type lung nodule malignant benign different scale image patch used every cnn extract feature output feature vector wa constructed using learned feature next vector classified malignant benign using either rf classifier svm radial basis function rbf filter model wa robust various noisy input level achieved accuracy 86 nodule classification conversely model 238 interpolates image data missing pet mri image using cnns alzheimer disease neuroimaging initiative adni database containing 830 pet mri patient scan wa utilized work pet mri image used train cnns first input output furthermore patient no pet image cnns utilized trained image rebuild pet image rebuilt image approximately fitted actual disease recognition outcome however approach not address overfitting issue turn restricted technique term possible capacity eralization diagnosing normal versus alzheimer disease patient ha achieved several cnn model 239 240 et al 241 attained 99 accuracy outcome diagnosing normal versus alzheimer disease patient author applied architecture using cnns generic brain tures caddementia dataset subsequently outcome learned feature became input higher layer differentiate patient scan alzheimer disease mild cognitive impairment normal brain based page 53 74 alzubaidi et al j big data 2021 adni dataset using deep supervision technique architecture vggnet rnns order basis voxcnn resnet model developed korolev et al 242 also discriminated alzheimer disease normal patient using adni database accuracy wa 79 voxnet 80 resnet compared work model achieved lower accuracy conversely implementation algorithm wa simpler not require ture korolev declared 2020 mehmood et al 240 trained oped network called scnn mri image task classification alzheimer disease achieved result obtaining accuracy recently cnn ha taken some medical imaging classification task different level traditional diagnosis automated diagnosis tremendous performance ples task diabetic foot ulcer dfu normal abnormal dfu class 87 sickle cell anemia sca normal abnormal sca blood component 86 247 breast cancer classify breast biopsy image four class invasive carcinoma carcinoma benign tumor normal tissue 42 88 skin cancer classification 2020 cnns playing vital role early diagnosis novel coronavirus cnn ha become primary tool automatic diagnosis many hospital around world using chest image detail classification medical imaging application found 226 localization although application anatomy education could increase practicing clinician likely interested localization normal anatomy radiological image independently examined described outside human intervention zation could applied completely automatic application zhao et al 269 introduced new deep approach localize pancreatic tumor projection image radiation therapy without need fiducials roth et al 270 constructed trained cnn using five convolutional layer classify around 4000 ct image author used five egories classification leg pelvis liver lung neck data augmentation niques applied achieved auc score classification error rate model wa detecting position spleen kidney heart liver shin et al 271 employed stacked 78 mri scan stomach area containing kidney liver temporal spatial domain used learn hierarchal feature based organ approach achieved detection accuracy sirazitdinov et al 268 presented gate two convolutional neural network namely retinanet mask pneumonia detection localization detection detection cade another method used detection clinician patient overlooking lesion scan may dire consequence page 54 74 alzubaidi et al j big data 2021 thus detection field study requiring accuracy sensitivity chouhan et al 275 introduced innovative deep learning framework tion pneumonia adopting idea transfer learning approach obtained accuracy recall unseen data area 19 pulmonary disease several convolutional neural network approach proposed automatic detection image showed excellent mance 46 area skin cancer several application introduced detection task et al 283 introduced deep learning approach skin cancer detection five convolutional neural network el addressed issue lack training data adopting idea transfer learning data augmentation technique network ha shown superior result compared model another interesting area histopathological image progressively digitized several paper published field human gist read image laboriously search malignancy marker high index cell proliferation using molecular marker cellular necrosis sign abnormal cellular architecture enlarged number mitotic figure denoting augmented cell replication enlarged ratio note cal slide may contain huge number cell thousand thus risk disregarding abnormal neoplastic region high wading cell excessive level magnification ciresan et al 291 employed cnns layer identifying mitotic figure fifty breast histology image mitos dataset used technique attained recall precision score tively sirinukunwattana et al 292 utilized 100 histology image colorectal cinoma detect cell nucleus using cnns roughly nucleus training purpose novelty approach wa use spatially constrained cnn cnn detects center nucleus using surrounding spatial context spatial regression instead cnn xu et al 293 employed stacked sparse encoder ssae identify nucleus histological slide breast cancer achieving recall precision score respectively field showed pervised learning technique also effectively utilized medical image albarquoni et al 294 investigated problem insufficient labeling actual mitoses labeling histology image breast cancer amateur online solving recurrent issue inadequate labeling analysis medical image achieved feeding input label cnn method signifies remarkable effort 2020 lei et al 285 introduced employment deep convolutional neural network automatic identification mitotic candidate histological section mitosis screening obtained detection result dataset international pattern recognition conference icpr 2012 mitosis detection competition segmentation although mri ct image segmentation research includes different organ knee cartilage prostate liver research work ha concentrated brain page 55 74 alzubaidi et al j big data 2021 segmentation particularly tumor issue highly significant surgical preparation obtain precise tumor limit shortest surgical resection ing surgery excessive sacrificing key brain region may lead neurological shortfall including cognitive damage emotionlessness limb difficulty conventionally cal anatomical segmentation wa done hand specifically clinician draw line within complete stack ct mri volume slice slice thus perfect implementing solution computerizes painstaking work wadhwa et al 301 presented brief overview brain tumor segmentation mri image akkus et al 302 wrote brilliant review brain mri segmentation addressed different metric cnn architecture employed moreover explain several competition detail well datasets included ischemic stroke lesion segmentation isle mild traumatic brain injury outcome prediction mtop brain tumor segmentation brat chen et al 299 proposed convolutional neural network precise brain tumor segmentation approach employed involves several approach better feature learning including deepmedic model novel training scheme label loss function cessing conducted method two modern brain tumor tion datasets brat 2017 brat 2015 datasets hu et al 300 introduced brain tumor segmentation method adopting convolutional neural network mccnn fully connected conditional random field crfs achieved result excellent compared method moeskops et al 303 employed three cnns input patch dissimilar size segmenting classifying mri brain image image include 35 adult 22 infant classified various sue category cerebrospinal fluid grey matter white matter every patch concentrate capturing various image aspect benefit employing three dissimilar size input patch bigger size incorporated spatial feature lowest patch size concentrated local texture general algorithm ha dice coefficient range achieved satisfactory accuracy although image slice employed majority segmentation research letrate et al 304 implemented cnn segmenting mri prostate image used challenge dataset fifty mri scan used training thirty testing architecture ronnerberger et al 305 inspired model attained dice coefficient score winning team competition reduce overfitting create model deeper layer cnn pereira et al 306 applied intentionally sized filter model used mri scan 274 glioma type brain tumor training achieved first place 2013 brat challenge well second place brat challenge havaei et al 307 also considered glioma using 2013 brat dataset investigated different cnn architecture compared winner brat 2013 algorithm worked better required only 3 min execute rather 100 min concept cascaded architecture formed basis model thus referred inputcascadecnn employing fc tional random field crfs atrous spatial pyramid pooling filter page 56 74 alzubaidi et al j big data 2021 technique introduced chen et al 308 author aimed enhance accuracy localization enlarge field view every filter model deeplab attained miou mean intersection union cal image segmentation model obtained excellent performance recently automatic segmentation lung infection ct image help detect development infection employing several deep learning technique registration usually given two input image four main stage canonical procedure image registration task 313 314 target selection illustrates determined input image second part input image need remain accurately superimposed feature extraction computes set feature extracted input image feature matching allows finding similarity previously obtained tures pose optimization aimed minimize distance input image result registration procedure suitable geometric transformation translation rotation scaling etc provides input image within coordinate system way distance minimal level optimal scope work provide extensive review topic nevertheless short summary accordingly introduced next commonly input image registration approach could ous form point cloud voxel grid mesh additionally some technique allow input result feature extraction matching step canonical scheme specifically outcome could some data particular form well result step classical pipeline feature vector matching vector formation nevertheless newest method novel conceptual type ecosystem issue contains acquired characteristic target material behavior registered input data conceptual ecosystem formed neural network training manner could counted input registration approach nevertheless not input one might adopt every registration situation since corresponds interior data representation dl interpretation conceptual design enables tiating input data registration approach defined model particular illustrated phase model depict particular spatial data one generalization data set created learning system yumer et al 315 developed framework model acquires characteristic object meaning ready identify sporty car seems like comfy chair also adjusting model fit characteristic maintaining main characteristic primary data likewise fundamental perspective vised learning method introduced ding et al 316 no target page 57 74 alzubaidi et al j big data 2021 registration approach instance network able placing input point cloud global space solving slam issue many point cloud istered rigidly hand mahadevan 317 proposed combination two conceptual model utilizing growth imagination machine give flexible cial intelligence system relationship learned phase training scheme not inspired label classification another practical application dl especially cnns image registration reconstruction object wang et al 318 applied adversarial way using cnns rebuild model object image network learns many object orally accomplishes tration image conceptual model similarly hermoza et al 319 also utilize gan network prognosticating absent geometry damaged logical object providing reconstructed object based voxel grid format label selecting class dl medical image registration ha numerous application listed some review paper yang et al 323 implemented stacked convolutional er approach predict morphing input pixel last formation using mri brain scan oasis dataset employed tration model known large deformation diffeomorphic metric mapping lddmm attained remarkable enhancement computation time miao et al 324 used thetic image train cnn register model probe hand implant knee implant onto image pose estimation determined model achieved execution time representing important enhancement conventional registration technique based intensity moreover achieved effective registration time li et al 325 introduced neural approach registration lateral cephalogram volumetric ct cbct image computational approach computationally exhaustive application complex ml dl approach idly identified significant technique widely used different field development enhancement algorithm aggregated capability computational performance large datasets make possible tively execute several application earlier application either not possible ficult take consideration currently several standard dnn configuration available interconnection pattern layer total number layer represent main difference configuration table 2 illustrates growth rate overall ber layer time seems far faster moore law growth rate normal dnn number layer grew around year period 2012 recent investigation future resnet version reveal number layer extended however sgd technique employed fit weight parameter different optimization technique employed obtain parameter updating dnn training process repetitive update required enhance network accuracy addition minorly augmented rate enhancement example training process using imagenet large dataset contains page 58 74 alzubaidi et al j big data 2021 14 million image along resnet network model take around repetition converge steady solution addition overall computational load prediction may exceed 1020 flop training set size dnn complexity increase prior 2008 boosting training satisfactory extent wa achieved using gpus usually day week needed training session even gpu port contrast several optimization strategy developed reduce extensive learning time computational requirement believed increase dnns continuously enlarge complexity size addition computational load cost memory bandwidth capacity significant effect entire training performance lesser extent deduction specifically parameter distributed every layer input data sizeable amount reused data computation several network layer exhibit excessive ratio contrast no uted parameter amount reused data extremely small additional fc layer extremely small ratio table 3 present parison different aspect related device addition table lished facilitate familiarity tradeoff obtaining optimal approach configuring system based either fpga gpu cpu device noted ha corresponding weakness strength accordingly no clear solution although gpu processing ha enhanced ability address computational challenge related network maximum gpu cpu performance not achieved several technique model turned strongly linked bandwidth worst case gpu efficiency 15 20 mum theoretical performance issue required enlarge memory bandwidth using stacked memory next different approach based fpga gpu cpu accordingly detailed table 3 comparison different aspect related device feature assessment leader development cpu easiest program gpu fpga cpu size fpga cpu smaller volume solution due lower power consumption customization broader flexibility provided fpga fpga ease change easier way vary application functionality provided gpu cpu backward compatibility transferring rtl novel fpga requires additional work furthermore gpu ha le stable architecture cpu cpu interface several variety interface implemented using fpga fpga fpga configurability assist utilization wider acceleration space due considerable processing ability gpu win customized design optimized fpga timing latency implemented fpga algorithm offer deterministic timing turn much faster gpu fpga large data analysis fpga performs well inline processing cpu support storage capability largest memory dcnn inference fpga ha lower latency customized fpga dcnn training greater capability provided gpu gpu page 59 74 alzubaidi et al j big data 2021 approach performance cpu node usually assist robust network nectivity storage ability large memory although cpu node purpose fpga gpu lack ability match unprocessed computation facility since requires increased network ability larger memory capacity approach gpus extremely effective several basic dl primitive include greatly operation activation function matrix multiplication convolution incorporating memory gpu model significantly enhances bandwidth enhancement allows numerous primitive efficiently utilize computational resource available gpus improvement gpu performance cpu performance usually related dense linear algebra operation maximizing parallel processing base initial gpu programming model example gpu model may involve computational unit four simd engine per computational layer simd ha sixteen computation lane peak performance 25 tflops 10 tflops percentage employment approach 100 additional gpu performance may achieved addition multiply function vector combine inner production instruction matching primitive related matrix operation dnn training gpu usually considered optimized design inference operation may also offer considerable performance improvement approach fpga wildly utilized various task including deep learning 199 247 inference accelerator commonly implemented utilizing fpga fpga effectively configured reduce unnecessary overhead function involved gpu system compared gpu fpga restricted performance integer inference main fpga aspect capability cally reconfigure array characteristic well capability figure array mean effective design little no overhead mentioned earlier fpga offer performance latency every watt gain gpu cpu dl inference operation implementation custom performance hardware pruned network reduced arithmetic precision three factor enable fpga implement dl algorithm achieve fpga level efficiency addition fpga may employed implement cnn lay engine 80 efficiency accuracy 15 top peak mance used conventional cnns xillinx partner demonstrated recently contrast pruning technique mostly employed lstm context size model efficiently minimized provides tant benefit implementation optimal solution mlp neural ing demonstrated recent study field implementing precision page 60 74 alzubaidi et al j big data 2021 custom ha revealed lowering extremely promising aid supplying additional advancement implementing peak performance fpga related dnn model evaluation metric evaluation metric adopted within dl task play crucial role achieving mized classifier 335 utilized within usual data classification procedure two main stage training testing utilized optimize classification algorithm training stage mean evaluation metric utilized discriminate select optimized solution discriminator erate forecast upcoming evaluation related specific classifier time evaluation metric utilized measure efficiency ated classifier evaluator within model testing stage using hidden data given eq 20 tn tp defined number negative positive instance respectively successfully classified addition fn fp defined number misclassified positive negative instance respectively next some evaluation metric listed accuracy calculates ratio correct predicted class total number sample evaluated 20 sensitivity recall utilized calculate fraction positive pattern correctly classified 21 specificity utilized calculate fraction negative pattern correctly classified 22 precision utilized calculate positive pattern correctly predicted predicted pattern positive class 23 calculates harmonic average recall precision rate 24 j score metric also called youdens j statistic eq 25 represents metric 20 accuracy tp tn tp tn fp fn 21 sensitivity tp tp fn 22 speciﬁcity tn fp tn 23 precision tp tp fp 24 2 precision recall precision recall page 61 74 alzubaidi et al j big data 2021 false positive rate fpr metric refers possibility false alarm ratio calculated eq 26 area roc curve auc common ranking type metric utilized conduct comparison learning algorithm well construct optimal learning model 339 340 contrast probability threshold rics auc value expose entire classifier ranking performance following formula used calculate auc value problem 341 eq 27 sp represents sum positive ranked sample number negative positive sample denoted nn np respectively compared racy metric auc value wa verified empirically theoretically making helpful identifying optimized solution evaluating classifier mance classification training considering discrimination evaluation process auc mance wa brilliant however multiclass issue auc computation ily discriminating large number created solution tion time complexity computing auc n log n respect hand till auc model 341 n log n according provost domingo auc model 336 framework datasets several dl framework datasets developed last year ous framework library also used order expedite work good result use training process ha become easier table 4 list utilized framework library based star rating github well background field tensorflow deemed effective easy use ha ability work several platform github one biggest software hosting site github star refer project site moreover several benchmark datasets employed different dl task some listed table summary conclusion finally mandatory inclusion brief discussion gathering relevant data provided along extensive research next itemized analysis presented order conclude review exhibit future direction 25 jscore sensitivity speciﬁcity 26 fpr 1 27 auc sp nn 1 npnn page 62 74 alzubaidi et al j big data 2021 dl already experience difficulty simultaneously modeling modality data recent dl development another common approach multimodal dl dl requires sizeable datasets labeled data preferred predict unseen data train model challenge turn particularly difficult data processing required provided datasets limited table 5 benchmark datasets dataset num class application link dataset imagenet 1000 image classification object localization object detection etc image classification html mnist 10 classification handwritten digit pascal voc 20 image classification tion object detection microsoft coco 80 object detection semantic segmentation home video image understanding 4716 video classification 101 human action detection php kinetics 400 human action detection ic google open image 350 image classification tion object detection html 101 classification labeled face wild face recognition scene dataset 67 indoor scene recognition htm table 4 list common framework library framework license core language year release homepage tensorflow apache python 2015 kera mit python 2015 caffe bsd 2015 matconvnet oxford matlab 2014 mxnet apache 2015 mxnet cntk mit 2016 cntk theano bsd python 2008 torch bsd c lua 2002 apache java 2014 gluon aws microsoft 2017 opendeep mit python 2017 page 63 74 alzubaidi et al j big data 2021 case healthcare data alleviate issue tl data augmentation researched last year although ml slowly transition unsupervised learning manage practical data without need manual human labeling many rent model utilize supervised learning cnn performance greatly influenced selection any small change value affect general cnn performance therefore careful parameter selection extremely significant issue considered optimization scheme development impressive robust hardware resource like gpus required effective cnn training moreover also required exploring efficiency using cnn smart embedded system cnn context ensemble learning 342 343 represents prospective research area collection different multiple architecture support model improving generalizability across different image category extracting several level semantic image representation similarly idea new tion function dropout batch normalization also merit investigation exploitation depth different structural adaptation significantly improved cnn learning capacity substituting traditional layer tion block result significant advance cnn performance ha shown recent literature currently developing novel efficient block tectures main trend new research model cnn architecture hrnet only one example show always way improve architecture expected platform play essential role future development computational dl application utilizing cloud computing offer solution handling enormous amount data also help increase ciency reduce cost furthermore offer flexibility train dl tures recent development computational tool including chip neural work mobile gpu see dl application mobile device easier user use dl regarding issue lack training data expected various technique transfer learning considered training dl model large unlabeled image datasets next transferring knowledge train dl model small number labeled image task last overview provides starting point community dl ested field dl furthermore researcher would allowed decide suitable direction work taken order provide accurate native field acknowledgement would like thank professor queensland university technology university information technology communication gave feedback paper author contribution conceptualization la jz methodology la jz j software la maf validation la jz lf formal analysis la jz yd j investigation la jz resource la jz maf data curation la oa nal draft preparation la oa editing la jz ajh aa yd oa j maf lf visualization page 64 74 alzubaidi et al j big data 2021 la maf supervision jz yd project administration jz yd j funding acquisition la ajh aa yd author read approved final manuscript funding research received no external funding availability data material not applicable declaration ethic approval consent participate not applicable consent publication not applicable competing interest author declare no competing interest author detail 1 school computer science queensland university technology brisbane qld 4000 australia 2 control tems engineering department university technology baghdad 10001 iraq 3 electrical engineering technical college middle technical university baghdad 10001 iraq 4 faculty electrical engineering computer science university missouri columbia mo 65211 usa 5 alnidhal campus university information technology communication baghdad 10001 iraq 6 department computer science university jaén 23071 jaén spain 7 college computer science information technology university sumer thi qar 64005 iraq 8 school engineering manchester metropolitan university manchester uk received 21 january 2021 accepted 22 march 2021 reference rozenwald mb galitsyna aa sapunov gv khrameeva ee gelfand machine learning framework prediction chromatin folding drosophila using epigenetic feature peerj comput sci 2020 amrit c paauw aly r lavric identifying child abuse text mining machine learning expert syst appl 2017 hossain e khan f sikander sunny msh application big data machine learning smart grid associated security concern review ieee access 2019 crawford khoshgoftaar tm prusa jd richter al najada survey review spam detection using machine learning technique j big data 2015 2 1 deldjoo elahi cremonesi p garzotto f piazzolla p quadrana video recommendation system based stylistic visual feature j data semant 2016 5 2 k chandran v nguyen k bank j benchmarking specimen cell tion using linear discriminant analysis higher order spectrum feature cell shape pattern recogn lett 2019 liu w wang z liu x zeng n liu alsaadi fe survey deep neural network architecture tions neurocomputing 2017 pouyanfar sadiq yan tian h tao reyes mp shyu ml chen sc iyengar survey deep learning rithms technique application acm comput surv csur 2018 51 5 alom mz taha tm yakopcic c westberg sidike p nasrin hasan van essen bc awwal aa asari vk survey deep learning theory architecture electronics 2019 8 3 potok te schuman c young patton r spedalieri f liu j yao kt rose g chakma study complex deep learning network neuromorphic quantum computer acm j emerg technol comput syst jetc 2018 14 2 adeel gogate hussain contextual deep switching speech enhancement environment inf fusion 2020 tian h chen sc shyu ml evolutionary programming based deep learning feature selection network struction visual data classification inf syst front 2020 22 5 young hazarika poria cambria recent trend deep learning based natural language processing ieee comput intell mag 2018 13 3 koppe g durstewitz deep learning small big data psychiatry harmacology 2021 46 1 dalal n triggs histogram oriented gradient human detection 2005 ieee computer society ence computer vision pattern recognition cvpr 05 vol ieee lowe dg object recognition local feature proceeding seventh ieee international conference computer vision vol ieee wu l hoi sc yu model application ieee trans image process 2010 19 7 page 65 74 alzubaidi et al j big data 2021 lecun bengio hinton deep learning nature 2015 521 7553 yao g lei zhong review action recognition pattern recogn lett 2019 dhillon verma gk convolutional neural network review model methodology application object detection prog artif intell 2020 9 2 khan sohail zahoora u qureshi survey recent architecture deep convolutional neural work artif intell rev 2020 53 8 hasan ri yusuf sm alzubaidi review state art deep learning plant disease broad analysis discussion plant 2020 9 10 xiao tian z yu j zhang liu du lan review object detection based deep learning multimed tool appl 2020 79 33 ker j wang l rao j lim deep learning application medical image analysis ieee access 2017 zhang z cui p zhu deep learning graph survey ieee trans knowl data eng 33 shrestha mahmood review deep learning algorithm architecture ieee access 2019 najafabadi mm villanustre f khoshgoftaar tm seliya n wald r muharemagic deep learning application challenge big data analytics j big data 2015 2 1 goodfellow bengio courville bengio deep learning vol cambridge mit press 2016 shorten c khoshgoftaar tm furht deep learning application j big data 2021 8 1 krizhevsky sutskever hinton ge imagenet classification deep convolutional neural network commun acm 2017 60 6 bhowmick nagarajaiah veeraraghavan vision deep algorithm detect quantify crack concrete surface uav video sensor 2020 20 21 goh gb hodas no vishnu deep learning computational chemistry j comput chem 2017 38 16 li zhang sun gao accelerating flash calculation deep learning method j comput phys 2019 yang w zhang x tian wang w xue jh liao deep learning single image brief review ieee trans multimed 2019 21 12 tang j li liu review lane detection method based deep learning pattern recogn 2020 zhao zq zheng p xu st wu object detection deep learning review ieee trans neural netw learn syst 2019 30 11 k zhang x ren sun deep residual learning image recognition proceeding ieee conference computer vision pattern recognition ng machine learning yearning technical strategy ai engineer era deep learning org metz turing award 3 pioneer artificial intelligence new york time 2019 27 nevo anisimov v elidan g r giencke p gigi hassid moshe z schlesinger shalev g et al ml flood forecasting scale arxiv preprint arxiv 09583 chen h engkvist wang olivecrona blaschke rise deep learning drug discovery drug discov today 2018 23 6 benhammou achchab b herrera f tabik breakhis based breast cancer automatic diagnosis using deep ing taxonomy survey insight neurocomputing 2020 wulczyn e steiner df xu z sadhwani wang h mermel ch chen phc liu stumpe mc deep survival prediction multiple cancer type using histopathology image plo one 2020 15 6 nagpal k foote liu chen phc wulczyn e tan f olson n smith jl mohtashamian wren jh et al ment validation deep learning algorithm improving gleason scoring prostate cancer npj digit med 2019 2 1 esteva kuprel b novoa ra ko j swetter sm blau hm thrun classification skin cancer deep neural network nature 2017 542 7639 brunese l mercaldo f reginelli santone explainable deep learning pulmonary disease coronavirus detection comput method program biomed 2020 196 105 jamshidi lalbakhsh talla j peroutka z hadjilooei f lalbakhsh p jamshidi la spada l mirmozafari dehghani et al artificial intelligence deep learning approach diagnosis treatment ieee access 2020 shorfuzzaman hossain metacovid siamese neural network framework contrastive loss diagnosis patient pattern recogn 2020 carvelli l olesen leary eb peppard pe mignot e sørensen hb jennum design deep learning model automatic scoring periodic leg movement sleep validated multiple human expert sleep med 2020 de fauw j ledsam jr b nikolov tomasev n blackwell askham h glorot x donoghue b visentin et al clinically applicable deep learning diagnosis referral retinal disease nat med 2018 24 9 topol ej medicine convergence human artificial intelligence nat med 2019 25 1 kermany goldbaum cai w valentim cc liang h baxter sl mckeown yang g wu x yan f et al ing medical diagnosis treatable disease deep learning cell 2018 172 5 van essen b kim h pearce r boakye k chen lbann livermore big artificial neural network hpc toolkit proceeding workshop machine learning computing environment saeed mm al aghbari z alsharidah big data clustering technique based spark literature review peerj comput sci 2020 page 66 74 alzubaidi et al j big data 2021 mnih v kavukcuoglu k silver rusu aa veness j bellemare mg graf riedmiller fidjeland ak ostrovski g et al control deep reinforcement learning nature 2015 518 7540 arulkumaran k deisenroth mp brundage bharath aa deep reinforcement learning brief survey ieee signal process mag 2017 34 6 socher r perelygin wu j chuang j manning cd ng ay potts recursive deep model semantic sitionality sentiment treebank proceeding 2013 conference empirical method natural language processing goller c kuchler learning distributed representation backpropagation structure proceeding international conference neural network icnn 96 vol ieee socher r lin ccy ng ay manning cd parsing natural scene natural language recursive neural work icml 2011 louppe g cho k becot c cranmer recursive neural network jet physic j high energy phys 2019 2019 1 sadr h pedram mm teshnehlab robust sentiment analysis method based sequential combination convolutional recursive neural network neural process lett 2019 50 3 urban g subrahmanya n baldi inner outer recursive neural network chemoinformatics application j chem inf model 2018 58 2 hewamalage h bergmeir c bandara recurrent neural network time series forecasting current status future direction int j forecast 2020 37 1 jiang kim h asnani h kannan oh viswanath learn code inventing code via recurrent neural network ieee j sel area inf theory 2020 1 1 john ra acharya j zhu c surendran bose sk chaturvedi tiwari n gao zhang kk et al optogenetics inspired transition metal dichalcogenide neuristors deep recurrent neural network nat commun 2020 11 1 batur dinler ö aydin optimal feature parameter set based gated recurrent unit recurrent neural work speech segment detection appl sci 2020 10 4 jagannatha yu structured prediction model rnn based sequence labeling clinical text ings conference empirical method natural language processing conference empirical method natural language processing vol 2016 nih public access 856 pascanu r gulcehre c cho k bengio construct deep recurrent neural network proceeding second international conference learning representation iclr 2014 2014 glorot x bengio understanding difficulty training deep feedforward neural network proceeding thirteenth international conference artificial intelligence statistic gao c yan j zhou varshney pk liu long deep recurrent neural network target tracking inf sci 2019 zhou dx theory deep convolutional neural network downsampling neural netw 2020 jhong sy tseng py siriphockpirom n hsia ch huang hua kl chen yy automated biometric tion system using palm vein recognition 2020 international conference advanced robotics intelligent system aris ieee ouadou max h duan tanner jj cheng deepcryopicker fully automated deep neural network single protein particle picking bmc bioinform 2020 21 1 wang lu c yang hong f liu hybrid method heartbeat classification via convolutional neural work multilayer perceptrons focal loss peerj comput sci 2020 li g zhang li j lv f tong efficient densely connected convolutional neural network pattern recogn 2021 gu j wang z kuen j l shahroudy shuai b liu wang x wang g cai j et al recent advance tional neural network pattern recogn 2018 fang w love pe luo h ding computer vision safety construction review future direction adv eng inform 2020 palaz collobert acoustic modeling using convolutional neural network automatic speech recognition speech commun 2019 li hc deng zy chiang hh lightweight learning network face recognition performance optimization sensor 2020 20 21 hubel dh wiesel tn receptive field binocular interaction functional architecture cat visual cortex j physiol 1962 160 1 ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift arxiv preprint arxiv 03167 ruder overview gradient descent optimization algorithm arxiv preprint arxiv 04747 bottou machine learning stochastic gradient descent proceeding compstat 2010 springer hinton g srivastava n swersky neural network machine learning lecture overview ent descent cited 2012 14 8 zhang improved adam optimizer deep neural network 2018 international symposium quality service iwqos ieee alzubaidi l fadhel zhang j duan deep learning model classification red blood cell microscopy image aid sickle cell anemia diagnosis electronics 2020 9 3 alzubaidi l fadhel zhang j santamaría j duan oleiwi towards better understanding transfer learning medical imaging case study appl sci 2020 10 13 alzubaidi l fadhel farhan l zhang j duan optimizing performance breast cancer classification employing domain transfer learning hybrid deep convolutional neural network model electronics 2020 9 3 page 67 74 alzubaidi et al j big data 2021 lecun jackel ld bottou l cortes c denker j drucker h guyon muller ua sackinger e simard p et al ing algorithm classification comparison handwritten digit recognition neural netw stat mech perspect 1995 srivastava n hinton g krizhevsky sutskever salakhutdinov dropout simple way prevent neural work overfitting j mach learn 2014 15 1 dahl ge sainath tn hinton ge improving deep neural network lvcsr using rectified linear unit 2013 ieee international conference acoustic speech signal processing ieee xu b wang n chen li empirical evaluation rectified activation convolutional network arxiv preprint arxiv 00853 hochreiter vanishing gradient problem learning recurrent neural net problem solution int j uncertain fuzziness knowl based syst 1998 6 02 lin chen q yan network network arxiv preprint arxiv 4400 hsiao ty chang yc chou hh chiu global average pooling tional network j syst arch 2019 li z wang sh fan rr cao g zhang yd guo teeth category classification via deep convolutional neural network max pooling global average pooling int j imaging syst technol 2019 29 4 zeiler md fergus visualizing understanding convolutional network european conference computer vision springer erhan bengio courville vincent visualizing feature deep network univ montreal 2009 1341 3 le qv building feature using large scale unsupervised learning 2013 ieee international conference acoustic speech signal processing ieee grün f rupprecht c navab n tombari taxonomy library visualizing learned feature convolutional neural network arxiv preprint arxiv 07757 simonyan k zisserman deep convolutional network image recognition arxiv print arxiv 1556 ranzato huang fj boureau yl lecun unsupervised learning invariant feature hierarchy application object recognition 2007 ieee conference computer vision pattern recognition ieee szegedy c liu w jia sermanet p reed anguelov erhan vanhoucke v rabinovich going deeper convolution proceeding ieee conference computer vision pattern recognition bengio et al rmsprop equilibrated adaptive learning rate nonconvex optimization arxiv srivastava rk greff k schmidhuber highway network arxiv preprint arxiv 00387 kong w dong zy jia hill dj xu zhang residential load forecasting based lstm recurrent neural network ieee trans smart grid 2017 10 1 ordóñez fj roggen deep convolutional lstm recurrent neural network multimodal wearable activity recognition sensor 2016 16 1 cireşan meier u masci j schmidhuber deep neural network traffic sign classification neural netw 2012 szegedy c ioffe vanhoucke v alemi impact residual connection learning arxiv preprint arxiv 07261 szegedy c vanhoucke v ioffe shlens j wojna rethinking inception architecture computer vision proceeding ieee conference computer vision pattern recognition wu zhong liu deep residual learning image steganalysis multimed tool appl 2018 77 9 huang g liu z van der maaten l weinberger kq densely connected convolutional network proceeding ieee conference computer vision pattern recognition rubin j parvaneh rahman conroy b babaeizadeh densely connected convolutional network tion atrial fibrillation short ecg recording j electrocardiol 2018 51 6 kuang p chen z li image densely connected convolutional network appl intell 2019 49 1 xie girshick r dollár p tu z aggregated residual transformation deep neural network ings ieee conference computer vision pattern recognition su x zhao jpeg steganalysis based resnext gauss partial derivative filter multimed tool appl 2020 80 3 yadav jalal garlapati hossain k goyal pant deep resnext model phycological future algal 2020 han w feng r wang l gao adaptive deep convolutional neural network tion remote sensing imagery scene classification igarss ieee international geoscience remote sensing symposium ieee zagoruyko komodakis wide residual network arxiv preprint arxiv 07146 huang g sun liu z sedra weinberger kq deep network stochastic depth european conference computer vision springer huynh ht nguyen joint age estimation gender classification asian face using wide resnet sn comput sci 2020 1 5 takahashi r matsubara uehara data augmentation using random image cropping patching deep cnns ieee trans circuit syst video technol 2019 30 9 han kim j kim deep pyramidal residual network proceeding ieee conference computer vision pattern recognition wang wang l wang h li image via deep shallow convolutional network ieee access 2019 page 68 74 alzubaidi et al j big data 2021 chollet xception deep learning depthwise separable convolution proceeding ieee conference computer vision pattern recognition lo ww yang x wang xception convolutional neural network malware classification transfer ing 2019 ifip international conference new technology mobility security ntms ieee rahimzadeh attar modified deep convolutional neural network detecting nia chest image based concatenation xception inform med unlocked 2020 wang f jiang qian c yang li c zhang h wang x tang residual attention network image classification proceeding ieee conference computer vision pattern recognition salakhutdinov r larochelle efficient learning deep boltzmann machine proceeding thirteenth international conference artificial intelligence statistic goh h thome n cord lim jh regularization deep belief network adv neural inf process syst 2013 guan j lai r xiong liu z gu fixed pattern noise reduction infrared image based cascade residual attention cnn neurocomputing 2020 bi q qin k zhang h li z xu residual attention based convolution network aerial scene sification neurocomputing 2020 jaderberg simonyan k zisserman et al spatial transformer network advance neural information processing system san mateo morgan kaufmann publisher hu j shen l sun network proceeding ieee conference computer vision pattern recognition mou l zhu xx learning pay attention spectral domain spectral attention convolutional network hyperspectral image classification ieee trans geosci remote sen 2019 58 1 woo park j lee jy kweon cbam convolutional block attention module proceeding european conference computer vision eccv roy ag navab n wachinger concurrent spatial channel squeeze excitation fully convolutional work international conference medical image computing intervention springer roy ag navab n wachinger recalibrating fully convolutional network spatial channel squeeze excitation block ieee trans med imaging 2018 38 2 sabour frosst n hinton ge dynamic routing capsule advance neural information processing system san mateo morgan kaufmann publisher arun p buddhiraju km porwal classifier hyperspectral image ieee j sel topic appl earth ob remote sen 2019 12 6 xinwei l lianghao x yi compact video fingerprinting via improved capsule net syst sci control eng 2020 b li x xia zhang autonomous deep learning genetic dcnn designer image classification computing 2020 wang j sun k cheng jiang b deng c zhao liu mu tan wang x et al deep sentation learning visual recognition ieee trans pattern anal mach intell 86 cheng b xiao b wang j shi h huang zhang higherhrnet representation learning human pose estimation cvpr 2020 karimi h derr tang characterizing decision boundary deep neural network arxiv preprint arxiv 11460 li ding l gao decision boundary deep neural network arxiv preprint arxiv 05385 yosinski j clune j bengio lipson transferable feature deep neural network advance neural information processing system san mateo morgan kaufmann publisher tan c sun f kong zhang w yang c liu survey deep transfer learning international conference artificial neural network springer wei k khoshgoftaar tm wang survey transfer learning j big data 2016 3 1 shorten c khoshgoftaar tm survey image data augmentation deep learning j big data 2019 6 1 wang f wang h wang h li g situ learning simulation approach putational ghost imaging opt express 2019 27 18 pan survey transfer learning collaborative recommendation auxiliary data neurocomputing 2016 deng j dong w socher r li lj li k imagenet hierarchical image database 2009 ieee conference computer vision pattern recognition ieee cook feuz kd krishnan nc transfer learning activity recognition survey knowl inf syst 2013 36 3 cao x wang z yan p li transfer learning pedestrian detection neurocomputing 2013 raghu zhang c kleinberg j bengio transfusion understanding transfer learning medical imaging advance neural information processing system san mateo morgan kaufmann publisher pham tn van tran l dao svt early disease classification mango leaf using neural network hybrid metaheuristic feature selection ieee access 2020 saleh hamoud analysis best parameter selection person recognition based gait model using cnn algorithm image augmentation j big data 2021 8 1 hirahara takaya e takahara ueda effect data count image scaling deep learning training peerj comput sci 2020 page 69 74 alzubaidi et al j big data 2021 fj strazzera f jerez jm urda franco forward noise adjustment scheme data tion 2018 ieee symposium series computational intelligence ssci ieee dua karra taniskidou uci machine learning repository irvine university california school information computer science ml johnson jm khoshgoftaar tm survey deep learning class imbalance j big data 2019 6 1 yang p zhang z zhou bb zomaya ay sample subset optimization classifying imbalanced biological data conference knowledge discovery data mining springer yang p yoo pd fernando j zhou bb zhang z zomaya ay sample subset optimization technique imbalanced ensemble learning problem bioinformatics application ieee trans cybern 2013 44 3 wang sun xu deep convolutional neural field sequence labeling arxiv preprint arxiv 05265 li wang umarov r xie b fan li l gao deepre enzyme ec number prediction deep learning bioinformatics 2018 34 5 li huang c ding l li z pan gao deep learning bioinformatics introduction application perspective big data era method 2019 choi e bahadori mt sun j kulas j schuetz stewart retain interpretable predictive model healthcare using reverse time attention mechanism advance neural information processing system san mateo morgan kaufmann publisher ching himmelstein bk kalinin aa bt way gp ferrero e agapow pm zietz man mm et al opportunity obstacle deep learning biology medicine j r soc interface 2018 15 141 zhou j troyanskaya og predicting effect noncoding variant deep sequence model nat method 2015 12 10 pokuri bs ghosal kokate sarkar ganapathysubramanian interpretable deep learning guided exploration photovoltaics npj comput mater 2019 5 1 ribeiro mt singh guestrin trust explaining prediction any classifier ings acm sigkdd international conference knowledge discovery data mining wang l nie r yu z xin r zheng c zhang z zhang j cai interpretable architecture capsule network identifying gene expression program data nat mach intell 2020 2 11 sundararajan taly yan axiomatic attribution deep network arxiv preprint arxiv 01365 platt j et al probabilistic output support vector machine comparison regularized likelihood od adv large margin classif 1999 10 3 nair precup arnold dl arbel exploring uncertainty measure deep network multiple sclerosis lesion detection segmentation med image anal 2020 herzog l murina e dürr wegener sick integrating uncertainty deep neural network mri based stroke analysis med image anal 2020 pereyra g tucker g chorowski j kaiser ł hinton regularizing neural network penalizing confident output distribution arxiv preprint arxiv 06548 naeini mp cooper gf hauskrecht obtaining well calibrated probability using bayesian binning ings aaai conference artificial intelligence aaai conference artificial intelligence vol nih public access 2901 li sethi ik classifier design pattern recogn 2006 39 7 zadrozny b elkan obtaining calibrated probability estimate decision tree naive bayesian classifier icml vol 1 citeseer steinwart consistency support vector machine regularized kernel classifier ieee trans inf theory 2005 51 1 lee k lee k shin j lee overcoming catastrophic forgetting unlabeled data wild proceeding ieee international conference computer vision shmelkov k schmid c alahari incremental learning object detector without catastrophic forgetting proceeding ieee international conference computer vision zenke f gerstner w ganguli temporal paradox hebbian learning homeostatic plasticity curr opin neurobiol 2017 andersen n krauth n nabavi hebbian plasticity vivo relevance induction curr opin neurobiol 2017 zheng r chakraborti phase ii nonparametric adaptive exponentially weighted moving average control chart qual eng 2016 28 4 rebuffi sa kolesnikov sperl g lampert ch icarl incremental classifier representation learning ceedings ieee conference computer vision pattern recognition hinton ge plaut dc using fast weight deblur old memory proceeding ninth annual conference cognitive science society parisi gi kemker r part jl kanan c wermter continual lifelong learning neural network review neural netw 2019 soltoggio stanley ko risi born learn inspiration progress future evolved plastic artificial neural network neural netw 2018 parisi gi tani j weber c wermter lifelong learning human action deep neural network tion neural netw 2017 cheng wang zhou p zhang model compression acceleration deep neural network principle progress challenge ieee signal process mag 2018 35 1 page 70 74 alzubaidi et al j big data 2021 wiedemann kirchhoffer h matlage haase p marban marinč neumann nguyen schwarz h wiegand et al deepcabac universal compression algorithm deep neural network ieee j sel topic signal process 2020 14 4 mehta n pandit concurrence big data analytics healthcare systematic review int j med inform 2018 esteva robicquet ramsundar b kuleshov v depristo chou k cui c corrado g thrun dean guide deep learning healthcare nat med 2019 25 1 shawahna sait sm accelerator deep learning network learning tion review ieee access 2018 min public welfare organization management system based fpga deep learning microprocess microsyst 2020 fadhel hameed ra alzubaidi l zhang boosting convolutional neural network performance based fpga accelerator international conference intelligent system design application springer han mao h dally wj deep compression compressing deep neural network pruning trained quantization huffman coding arxiv preprint arxiv 00149 chen z zhang l cao z guo distilling knowledge handcrafted feature human activity recognition ieee trans ind inform 2018 14 10 hinton g vinyals dean distilling knowledge neural network arxiv preprint arxiv 02531 lenssen je fey libuschewski group equivariant capsule network advance neural information cessing system san mateo morgan kaufmann publisher denton el zaremba w bruna j lecun fergus exploiting linear structure within convolutional network efficient evaluation advance neural information processing system san mateo morgan kaufmann lishers xu q zhang gu z pan overfitting remedy sparsifying regularization layer cnns neurocomputing 2019 zhang c bengio hardt recht b vinyals understanding deep learning requires rethinking generalization commun acm 2018 64 3 xu x jiang x c du p li x lv yu l ni q chen su j et al deep learning system screen novel ru disease 2019 pneumonia engineering 2020 6 10 sharma k alsadoon prasad p nguyen tqv pham dth novel solution using deep learning left ventricle detection enhanced feature extraction comput method program biomed 2020 zhang g wang c xu b grosse three mechanism weight decay regularization arxiv preprint arxiv 12281 laurent c pereyra g brakel p zhang bengio batch normalized recurrent neural network 2016 ieee tional conference acoustic speech signal processing icassp ieee salamon j bello jp deep convolutional neural network data augmentation environmental sound sification ieee signal process lett 2017 24 3 wang x qin wang xiang chen reltanh activation function vanishing gradient resistance dnns application rotating machinery fault diagnosis neurocomputing 2019 tan hh lim kh vanishing gradient mitigation deep learning neural network optimization 2019 international conference smart computing communication icscc ieee macdonald g godbout gillcash b cairn neural network solution vanishing gradient problem arxiv preprint arxiv 09576 mittal vaishay survey technique optimizing deep learning gpus j syst arch 2019 kanai fujiwara iwamura preventing gradient explosion gated recurrent unit advance neural information processing system san mateo morgan kaufmann publisher hanin neural net architecture give rise exploding vanishing gradient advance neural information processing system san mateo morgan kaufmann publisher ribeiro ah tiels k aguirre la schön beyond exploding vanishing gradient analysing rnn training using attractor smoothness international conference artificial intelligence statistic pmlr amour heller k moldovan adlam b alipanahi b beutel chen c deaton j eisenstein j hoffman md et al underspecification present challenge credibility modern machine learning arxiv preprint arxiv 03395 chea p mandell jc current application future direction deep learning musculoskeletal radiology skelet radiol 2020 49 2 wu x sahoo hoi sc recent advance deep learning object detection neurocomputing 2020 kuutti bowden r jin barber p fallah survey deep learning application autonomous vehicle control ieee trans intell transp syst 2020 yolcu g oztel kazan oz c bunyak deep face analysis system monitoring customer est j ambient intell humaniz comput 2020 11 1 jiao l zhang f liu f yang li l feng z qu survey deep object detection ieee access 2019 muhammad k khan del ser j de albuquerque vhc deep learning multigrade brain tumor classification smart healthcare system prospective survey ieee trans neural netw learn syst 2020 litjens g kooi bejnordi setio aaa ciompi f ghafoorian van der laak ja van ginneken b sánchez ci survey deep learning medical image analysis med image anal 2017 mukherjee mondal r singh pk sarkar r bhattacharjee ensemconvnet deep learning approach human activity recognition using smartphone sensor healthcare application multimed tool appl 2020 79 41 page 71 74 alzubaidi et al j big data 2021 zeleznik r foldyna b eslami p wei j alexander taron j parmar c alvi rm banerji uno et al deep convolutional neural network predict cardiovascular risk computed tomography nature commun 2021 12 1 wang j liu q xie h yang z zhou boosted efficientnet detection lymph node metastasis breast cancer using convolutional neural network cancer 2021 13 4 yu h yang lt zhang q armstrong deen mj convolutional neural network medical image analysis comparison improvement perspective neurocomputing 157 bharati podder p mondal mrh hybrid deep learning detecting lung disease image inform med unlocked 2020 dong pan zhang j xu learning read chest image example using cnn 2017 international conference connected health application system engineering technology chase ieee rajkomar lingam taylor ag blum mongan classification radiograph using deep convolutional neural network j digit imaging 2017 30 1 rajpurkar p irvin j zhu k yang b mehta h duan ding bagul langlotz c shpanskaya k et al chexnet pneumonia detection chest deep learning arxiv preprint arxiv 05225 wang x peng lu l lu z bagheri summer rm chest database mark classification localization common thorax disease proceeding ieee conference computer vision pattern recognition zuo w zhou f li z wang cnn knowledge transfer candidate classification lung nodule detection ieee access 2019 shen w zhou yang f yang c tian convolutional neural network lung nodule classification international conference information processing medical imaging springer li r zhang w suk hi wang l li j shen ji deep learning based imaging data completion improved brain disease diagnosis international conference medical image computing intervention springer wen j e j routier bottani dormont durrleman burgos n colliot et al convolutional neural network classification alzheimer disease overview reproducible evaluation med image anal 2020 mehmood maqsood bashir shuyuan deep siamese convolution neural network sification alzheimer disease brain sci 2020 10 2 e ghazal mahmoud aslantas shalaby casanova barnes g gimel farb g keynton r alzheimer disease diagnostics deeply supervised adaptable convolutional network front biosci 2018 korolev safiullin belyaev dodonova residual plain convolutional neural network brain mri classification 2017 ieee international symposium biomedical imaging isbi 2017 ieee alzubaidi l fadhel oleiwi sr zhang diabetic foot ulcer classification using novel deep convolutional neural network multimed tool appl 2020 79 21 goyal reef nd davison ak rajbhandari spragg j yap mh dfunet convolutional neural network diabetic foot ulcer classification ieee trans emerg topic comput intell 2018 4 5 yap hachiuma r alavi brungel r goyal zhu h cassidy b ruckert j olshansky huang x et al deep learning diabetic foot ulcer detection comprehensive evaluation arxiv preprint arxiv 03341 tulloch j zamani r akrami machine learning prevention diagnosis management diabetic foot ulcer systematic review ieee access 2020 fadhel alzubaidi l oleiwi sickle cell anemia diagnosis based hardware tor international conference new trend information communication technology application springer debelee tg kebede sr schwenker f shewarega zm deep learning selected cancer image survey j imaging 2020 6 11 khan islam n jan z din iu rodrigues jjc novel deep learning based framework detection sification breast cancer using transfer learning pattern recogn lett 2019 alzubaidi l hasan ri awad fh fadhel alshamma zhang breast cancer classification novel deep convolutional neural network architecture 2019 international conference development esystems engineering dese ieee roy k banik bhattacharjee nasipuri system classification breast histology image using deep learning comput med imaging gr 2019 hameed z zahia b javier aguirre j maría vanegas breast cancer histopathology image sification using ensemble deep learning model sensor 2020 20 16 hosny km kassem foaud mm skin cancer classification using deep learning transfer learning 2018 cairo international biomedical engineering conference cibec ieee dorj uo lee kk choi jy lee skin cancer classification using deep convolutional neural network multimed tool appl 2018 77 8 kassem hosny km fouad mm skin lesion classification eight class isic 2019 using deep tional neural network transfer learning ieee access 2020 heidari mirniaharikandehei khuzani az danala g qiu zheng improving performance cnn predict likelihood using chest image preprocessing algorithm int j med inform 2020 ah khushaba rn mosa zm escudero efficient mixture deep machine learning model tuberculosis detection using image resource limited setting arxiv preprint arxiv page 72 74 alzubaidi et al j big data 2021 abraham b nair detection image using bayesnet classifier biocybern biomed eng 2020 40 4 nour cömert z polat novel medical diagnosis model infection detection based deep feature bayesian optimization appl soft comput 2020 mallio ca napolitano castiello g giordano fm alessio p iozzino sun angeletti russano santini et al deep learning algorithm trained pneumonia also identifies immune checkpoint inhibitor pneumonitis cancer 2021 13 4 fourcade khonsari deep learning medical image analysis third eye doctor j stomatol oral maxillofac surg 2019 120 4 guo z li x huang h guo n li deep image segmentation multimodal medical imaging ieee trans radiat plasma med sci 2019 3 2 thakur n yoon h chong current trend artificial intelligence colorectal cancer pathology image analysis systematic review cancer 2020 12 7 lundervold lundervold overview deep learning medical imaging focusing mri zeitschrift für medizinische physik 2019 29 2 yadav jadhav sm deep convolutional neural network based medical image classification disease diagnosis j big data 2019 6 1 nehme e freedman gordon r ferdman b wei le alalouf naor orange r michaeli shechtman dense localization microscopy psf design deep learning nat method 2020 17 7 zulkifley abdani sr zulkifley nh deep learning approach pterygium detection localization multimed tool appl 2019 78 24 sirazitdinov kholiavchenko mustafaev yixuan kuleev r ibragimov deep neural network ensemble pneumonia localization chest database comput electr eng 2019 zhao w shen l han b yang cheng k toesca da koong ac chang dt xing markerless pancreatic tumor target localization enabled deep learning int j radiat oncol biol phys 2019 105 2 roth hr lee ct shin hc seff kim l yao j lu l summer rm classification medical image using deep convolutional net 2015 ieee international symposium biomedical imaging isbi ieee shin hc orton mr collins dj doran sj leach mo stacked autoencoders unsupervised feature ing multiple organ detection pilot study using patient data ieee trans pattern anal mach intell 2012 35 8 li z dong wen hu x zhou p zeng object detection medical image neurocomputing 2019 gao j jiang q zhou b chen convolutional neural network detection diagnosis medical image analysis overview math biosci eng 2019 16 6 lumini nanni review fair comparison skin detection approach publicly available datasets expert syst appl 113677 chouhan v singh sk khamparia gupta tiwari p moreira c damaševičius r de albuquerque vhc novel transfer learning based approach pneumonia detection chest image appl sci 2020 10 2 apostolopoulos id mpesiana ta automatic detection image utilizing transfer learning convolutional neural network phys eng sci med 2020 43 2 mahmud rahman fattah sa covxnet convolutional neural network automatic 19 pneumonia detection chest image transferable feature tion comput biol med 2020 mh application artificial intelligence battling literature review chaos ton fractal 2020 toraman alakus tb turkoglu convolutional capsnet novel artificial neural network approach detect disease image using capsule network chaos soliton fractal 2020 dascalu david skin cancer detection deep learning sound analysis algorithm prospective clinical study elementary dermoscope ebiomedicine 2019 adegun viriri deep learning technique skin lesion analysis melanoma cancer detection survey artif intell rev 2020 zhang n cai yx wang yy tian yt wang xl badami skin cancer diagnosis based optimized convolutional neural network artif intell med 2020 k domínguez convolutional neural network framework accurate skin cancer detection neural process lett jain massoud tf predicting tumour mutational burden histopathological image using multiscale deep learning nat mach intell 2020 2 6 lei h liu elazab lei convolutional neural network mitosis detection histopathological image ieee j biomed health inform 2020 25 2 celik talo yildirim karabatak acharya ur automated invasive ductal carcinoma detection based using deep transfer learning image pattern recogn lett 2020 sebai wang x wang maskmitosis deep learning framework fully supervised weakly supervised unsupervised mitosis detection histopathology image med biol eng comput 2020 sebai wang sa partmitosis partially supervised deep learning framework mitosis detection breast cancer histopathology image ieee access 2020 mahmood arsalan owais lee mb park kr artificial mitosis detection breast cancer histopathology image using faster deep cnns j clin med 2020 9 3 srinidhi cl ciga martel al deep neural network model computational histopathology survey med image anal 2020 page 73 74 alzubaidi et al j big data 2021 cireşan dc giusti gambardella lm schmidhuber mitosis detection breast cancer histology image deep neural network international conference medical image computing vention springer sirinukunwattana k raza sea tsang yw snead dr cree ia rajpoot nm locality sensitive deep learning detection classification nucleus routine colon cancer histology image ieee trans med imaging 2016 35 5 xu j xiang l liu q gilmore h wu j tang j madabhushi stacked sparse autoencoder ssae nucleus tion breast cancer histopathology image ieee trans med imaging 2015 35 1 albarqouni baur c achilles f belagiannis v demirci navab aggnet deep learning crowd mitosis detection breast cancer histology image ieee trans med imaging 2016 35 5 mk awad ai khalaf aa hamed hf automatic brain tumour diagnosis system magnetic resonance image using convolutional neural network eurasip j image video process 2018 2018 1 thaha mm kumar kpm murugan b dhanasekeran vijayakarthick p selvi brain tumor segmentation using convolutional neural network mri image j med syst 2019 43 9 talo yildirim baloglu ub aydin g acharya ur convolutional neural network brain disease detection using mri image comput med imaging gr 2019 gabr coronado robinson sujit sj datta sun x allen wj lublin fd wolinsky j narayana brain lesion segmentation multiple sclerosis using fully convolutional neural network study mult scler j 2020 26 10 chen ding c liu convolutional neural network accurate brain tumor segmentation pattern recogn 2019 hu k gan q zhang deng xiao f huang w cao c gao brain tumor segmentation using convolutional neural network conditional random field ieee access 2019 wadhwa bhardwaj verma review brain tumor segmentation mri image magn reson imaging 2019 akkus z galimzianova hoogi rubin dl erickson bj deep learning brain mri segmentation state art future direction j digit imaging 2017 30 4 moeskops p viergever mendrik de vries l bender mj išgum automatic segmentation mr brain image convolutional neural network ieee trans med imaging 2016 35 5 milletari f navab n ahmadi sa fully convolutional neural network volumetric medical image tation 2016 fourth international conference vision ieee ronneberger fischer p brox convolutional network biomedical image segmentation tional conference medical image computing intervention springer pereira pinto alves v silva ca brain tumor segmentation using convolutional neural network mri image ieee trans med imaging 2016 35 5 havaei davy biard courville bengio pal c jodoin pm larochelle brain tumor mentation deep neural network med image anal 2017 chen lc papandreou g kokkinos murphy k yuille al deeplab semantic image segmentation deep convolutional net atrous convolution fully connected crfs ieee trans pattern anal mach intell 2017 40 4 yan q wang b gong luo c zhao w shen j shi q jin zhang l chest ct image deep convolutional neural network solution arxiv preprint arxiv 10987 wang g liu x li c xu z ruan j zhu h meng li k huang n zhang framework automatic segmentation pneumonia lesion ct image ieee trans med imaging 2020 39 8 khan sh sohail khan lee classification region analysis infection using lung ct image deep convolutional neural network arxiv preprint arxiv 08864 shi f wang j shi j wu z wang q tang z k shi shen review artificial intelligence technique ing data acquisition segmentation diagnosis ieee rev biomed eng 2020 santamaría j roca overview latest image registration algorithm appl sci 2020 10 6 santamaría j cordón dama comparative study evolutionary image registration od modeling comput vision image underst 2011 115 9 yumer mitra nj learning semantic deformation flow convolutional network european ence computer vision springer ding l feng deepmapping unsupervised map estimation multiple point cloud proceeding ieee conference computer vision pattern recognition mahadevan imagination machine new challenge artificial intelligence aaai 2018 wang l fang unsupervised reconstruction single image via adversarial learning arxiv preprint arxiv 09312 hermoza r sipiran reconstruction incomplete archaeological object using generative adversarial network proceeding computer graphic international association computing machinery fu lei wang curran wj liu yang deep learning medical image registration review phys med biol 2020 65 20 haskins g kruger u yan deep learning medical image registration survey mach vision appl 2020 31 1 de vos bd berendsen ff viergever sokooti h staring išgum deep learning framework unsupervised affine deformable image registration med image anal 2019 yang x kwitt r styner niethammer quicksilver fast predictive image deep learning approach neuroimage 2017 page 74 74 alzubaidi et al j big data 2021 miao wang zj liao cnn regression approach registration ieee trans med imaging 2016 35 5 li p pei guo g xu zha registration using convolutional autoencoders 2020 ieee international symposium biomedical imaging isbi ieee zhang j yeung sh shu b wang efficient memory management deep learning system arxiv preprint arxiv 06631 zhao h han z yang z zhang q yang f zhou l yang lau fc wang xiong et al hived sharing gpu cluster deep learning guarantee usenix symposium operating system design mentation osdi 20 lin jiang z gu j li w dhar ren h khailany b pan dz dreamplace deep learning gpu eration modern vlsi placement ieee trans comput aided de integr circuit syst 2020 hossain lee dj deep detection tracking aerial imagery via flying robot embedded device sensor 2019 19 15 castro fm guil n mj j ujaldón tuning convolutional neural network concurr comput pract exp 2019 31 21 gschwend zynqnet embedded convolutional neural network arxiv preprint arxiv 06892 zhang n wei x chen h liu fpga implementation optical remote sensing object detection electronics 2021 10 3 zhao hu c wei f wang k wang c jiang underwater image recognition fpga embedded system convolutional neural network sensor 2019 19 2 liu x yang j zou c chen q yan x chen cai collaborative edge computing cnn tor face tracking system ieee trans comput soc syst 18 hossin sulaiman review evaluation metric data classification evaluation int j data min knowl manag process 2015 5 2 provost f domingo tree induction ranking mach learn 2003 52 3 rakotomamonyj optimizing area roc svms proceeding european conference artificial intelligence workshop roc curve artificial intelligence rocai 2004 mingote v miguel ortega lleida optimization area roc curve using neural network vector speaker verification comput speech lang 2020 fawcett introduction roc analysis pattern recogn lett 2006 27 8 huang j ling cx using auc accuracy evaluating learning algorithm ieee trans knowl data eng 2005 17 3 hand dj till rj simple generalisation area roc curve multiple class classification problem mach learn 2001 45 2 masoudnia mersa araabi bn vahabie ah sadeghi ahmadabadi mn learning offline signature verification using snapshot ensemble cnns expert syst appl 2019 coupé p mansencal b clément giraud r de senneville bd ta vt lepetit v manjon jv assemblynet large ensemble cnns whole brain mri segmentation neuroimage 2020 publisher note springer nature remains neutral regard jurisdictional claim published map institutional affiliation