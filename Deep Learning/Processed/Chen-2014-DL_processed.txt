receiv april accept may date public may date current version may digit object identifi big data deep learn challeng perspect senior member ieee xiaotong comput scienc wayn state univers detroit mi usa comput scienc engin oakland univers rochest mi usa correspond author chen abstract deep learn current extrem activ research area machin learn pattern recognit societi ha gain huge success broad area applic speech recognit comput vision natur languag process sheer size data avail today big data bring big opportun transform potenti variou sector hand also present unpreced challeng har data inform data keep get bigger deep learn come play key role provid big data predict analyt solut thi paper provid brief overview deep learn highlight current research effort challeng big data well futur trend index term classiﬁ design evalu featur represent machin learn neural net model parallel process introduct deep learn big data two hottest trend rapidli grow digit world big data ha deﬁn differ way herein refer nential growth wide avail digit data difﬁcult even imposs manag analyz use convent softwar tool technolog digit data shape size grow astonish rate ple accord nation secur agenc internet process petabyt data per day digit inform ha grown nine time volum ﬁve year amount world reach trillion gigabyt thi explos digit data bring big opportun transform potenti variou sector enterpris healthcar industri manufactur educ servic also lead dramat paradigm shift scientiﬁc research toward discoveri big data offer great potenti ing aspect societi harvest valuabl edg big data ordinari task larg rapidli grow bodi inform hidden dent volum data requir develop advanc technolog interdisciplinari team work close collabor today machin ing techniqu togeth advanc avail tation power come play vital role big data analyt knowledg discoveri see employ wide leverag predict power big data ﬁeld like search engin medicin astronomi extrem activ subﬁeld machin learn deep learn consid togeth big data big deal base american innov econom revolut contrast convent learn method consid use learn ture deep learn refer machin learn techniqu use supervis unsupervis strategi ical learn hierarch represent deep architectur classiﬁc inspir biolog tion human brain mechan process natur signal deep learn ha attract much attent academ commun recent year due perform mani research domain speech nition collabor fulter comput vision deep learn ha also success appli industri product take advantag larg volum digit data compani like googl appl facebook collect analyz massiv amount data daili basi aggress push forward deep learn relat project exampl appl siri virtual person assist iphon offer wide varieti servic includ weather report sport news answer user question remind etc util deep learn data collect appl servic googl appli deep learn algorithm massiv chunk messi data obtain internet googl translat ieee translat content mine permit academ research onli person use also permit requir ieee permiss see http inform volum chen lin big data deep learn android voic recognit googl street view imag search engin industri giant far behind either exampl microsoft languag tion bing voic search ibm comput use techniqu like deep learn leverag big data competit advantag data keep get bigger deep learn come play key role provid big data predict analyt solut particularli increas process power advanc graphic processor thi paper goal present comprehens survey relat work deep learn mainli discuss import issu relat learn massiv amount data highlight current research effort challeng big data well futur trend rest paper organ follow section present brief review two commonli use deep learn architectur section cuss strategi deep learn massiv amount data final discuss challeng perspect deep learn big data section ii overview deep learn deep learn refer set machin learn techniqu learn multipl level represent deep tectur thi section present brief overview two deep architectur deep belief work dbn convolut neural network cnn deep belief network convent neural network prone get trap local optima object function often lead poor perform furthermor take advantag unlabel data often abund cheap collect big data allevi problem deep belief network dbn use deep architectur capabl learn featur represent label unlabel data present incorpor unsupervis supervis strategi construct model unsupervis stage intend learn data distribut without use label inform supervis stage perform local search ﬁne tune fig show typic dbn architectur pose stack restrict boltzmann machin rbm one addit layer discrimin task rbm probabilist gener model learn joint probabl distribut observ train data without use data label effect util larg amount unlabel data exploit complex data ture onc structur dbn determin goal train learn weight bias layer thi conduct ﬁrstli unsupervis learn rbm typic rbm consist two layer node one layer fulli connect node layer connect node layer see exampl input layer ﬁrst hidden layer form rbm consequ node independ node layer given node layer thi characterist allow us train gener weight w rbm use gibb sampl figur illustr deep belief network architectur thi particular dbn consist three hidden layer three neuron one input later five neuron one output layer also five neuron ani two adjac layer form rbm train unlabel data output current rbm h first rbm mark red input next rbm h second rbm mark green weight w label data befor rbm perform output rbm fed input next rbm process repeat rbm train thi unsupervis learn critic dbn train practic help avoid local optima allevi problem observ million paramet use furthermor algorithm veri efﬁcient term time complex linear number size rbm featur differ layer contain differ inform data structur featur construct featur note number stack rbm paramet determin user requir onli unlabel data good gener simpl rbm bernoulli distribut visibl hidden layer sampl probabl follow p v w σ x wijvi aj p vi h w σ j x wijhj bi v h repres visibl unit vector j hidden unit vector respect w matrix weight wij connect visibl hidden layer aj bi bia term σ sigmoid function case volum chen lin big data deep learn visibl unit condit probabl bution slightli differ typic distribut assum p vi h w gaussian weight wij updat base approxim method call contrast diverg cd approxim exampl weight wij updat follow α α learn rate c momentum factor expect distribut deﬁn data model respect expect may calcul run gibb sampl inﬁnit mani time practic cd often use becaus perform well model paramet bias updat similarli gener mode rbm train includ gibb sampler sampl hidden unit base visibl unit vice versa eq weight two layer updat use cd rule eq thi process repeat converg rbm model data tion use hidden unit without employ label inform thi veri use featur big data analysi dbn potenti leverag much data without know label improv perform inform input data store weight everi adjac layer dbn add ﬁnal layer repres desir output overal network ﬁne tune use label data back propag strategi better discrimin implement top stack rbm anoth layer call associ memori determin supervis learn method variat instead use rbm exampl stack denois stack predict spars code also pose unsupervis featur learn furthermor recent result show larg number train data abl fulli supervis train use random initi weight instead weight without use rbm practic work well exampl discrimin model start network one singl hidden layer shallow neural network train back propag method upon converg new hidden layer insert thi shallow nn ﬁrst hidden layer desir output layer full network discrimin train thi process continu predetermin criterion met number hidden neuron summari dbn use greedi efﬁcient layer approach learn latent variabl weight hidden layer back propag method tune thi hybrid train strategi thu improv gener perform discrimin power network convolut neural network typic cnn compos mani layer hierarchi layer featur represent featur map type convent neural network classiﬁc often start two alter type layer call convolut subsampl layer lution layer perform convolut oper sever ﬁlter map equal size subsampl layer reduc size proceed layer averag pixel within small neighborhood fig show typic architectur cnn input ﬁrst convolut set ﬁlter c layer fig ﬁltere data call featur map nonlinear transform subsampl perform reduc dimension layer fig sequenc repeat mani time determin user figur illustr typic convolut neural network architectur input imag convolv four differ filter h follow nonlinear activ form four featur map second layer featur map factor creat featur map layer sequenc repeat mani time thi exampl form featur map layer use eight differ filter h first third fourth sixth featur map layer defin one correspond featur map layer convolut differ filter second fifth map layer form two map convolut two differ filter last layer output layer form fulli connect neural network output last subsampl later concaten one long input vector neuron fulli connect neuron next layer hidden layer thi figur illustr fig lowest level thi architectur input layer n n imag input local recept ﬁeld upper layer neuron extract elementari complex visual featur convolut layer label cx fig compos multipl featur map construct convolv input ferent ﬁlter weight vector word valu unit featur map result depend local recept ﬁeld previou layer ﬁlter thi volum chen lin big data deep learn follow nonlinear activ l j f x kij x bj l j output convolut layer cl f nonlinear function recent implement use scale hyperbol tangent function nonlinear activ function f x tanh kij trainabl ﬁlter kernel ﬁlter bank convolv featur map x previou layer produc new featur map current layer symbol n sent discret convolut oper bj bia note ﬁlter kij connect portion featur map previou layer fig show partial connect featur map layer label sx fig reduc spatial resolut ture map thu provid level distort invari gener unit layer construct averag area featur map max pool small region key paramet decid weight layer normal train standard tion procedur gradient descent algorithm mean loss function altern train deep cnn architectur unsupervis herein review particular method unsupervis train cnn tive spars decomposit psd idea imat input xwith linear combin basic spars function arg λ α tanh kx w matrix linear basi set z spars coefﬁcient matrix diagon gain matrix k ﬁlter bank predictor paramet goal ﬁnd optim basi function set w ﬁlter bank kthat minim reconstruct error ﬁrst term eq spars represent second term code predict error simultan third term eq sure differ predict code actual code preserv invari certain distort psd train encod learn ﬁlter bank also pool togeth summari inspir biolog process cnn algorithm learn hierarch featur represent lize strategi like local recept ﬁeld size ﬁlter normal small share weight use weight construct featur map level signiﬁcantli reduc number paramet pling reduc dimension ﬁlter bank train either supervis unsupervis od cnn capabl learn good featur hierarchi automat provid degre translat distort invari iii deep learn massiv amount data deep learn ha shown impress result mani applic train trivial task big data learn due fact iter comput inher deep learn algorithm often extrem difﬁcult parallel thu unpreced growth commerci academ data set recent year surg interest effect scalabl parallel algorithm train deep model contrast shallow architectur paramet prefer avoid overﬁt problem deep learn algorithm enjoy success larg number den neuron often result million free paramet thu deep learn often involv larg ume data larg model algorithm approach explor learn exampl local connect network improv optim new structur implement parallel recent deng et al propos modiﬁ deep architectur call deep stack network dsn effect parallel dsn consist sever special neural network call modul singl hidden layer stack modul input compos raw data vector put previou modul form dsn recent new deep architectur call tensor deep stack network base dsn implement use cpu cluster scalabl parallel comput use great comput power speed train process ha shown signiﬁc potenti big data deep learn exampl one way scale dbn use multipl cpu core core deal subset train data scheme vanhouck et al discuss aspect technic detail includ fulli design data layout batch comput use instruct leverag instruct implement mentat enhanc perform modern cpu deep learn anoth recent work aim parallel gibb sampl hidden visibl unit split hidden unit visibl unit n machin respons unit order make work data transfer machin requir sampl hidden unit machin data visibl unit vice vers thi method efﬁcient hidden visibl unit binari also sampl size modest commun cost howev rise quickli data set use method deep learn also explor implement custom architectur control unit implement cpu grid multipl process tile fast memori thi survey focu recent develop deep learn framework take advantag great pute power avail today take graphic processor unit volum chen lin big data deep learn gpu exampl august nvidia singl precis gpu exceed memori bandwidth near particularli suit massiv parallel comput transistor devot data proceed need newli develop deep learn framework shown signiﬁc advanc make deep learn practic fig show schemat typic gpu four mp consist sever stream multiprocessor sm form build block fig show two sm block sm ha multipl stream processor sp share control logic memori furthermor gpu ha global memori veri high bandwidth high latenc access cpu host thi architectur allow two level parallel instruct memori level mp thread level sp thi simt singl instruct multipl thread architectur allow thousand ten thousand thread run concurr best suit oper larg number arithmet oper small access time memori level parallel also effect util special attent data ﬂow develop gpu parallel comput applic one consider exampl reduc data transfer ram gpu global memori transfer data larg chunk thi achiev upload larg set unlabel data possibl store free paramet well intermedi comput global memori addit data parallel learn updat implement leverag two level parallel input exampl assign across mp individu node treat thread sp deep belief network raina et al propos framework sive parallel unsupervis learn model includ dbn thi paper refer algorithm stack rbm spars code previou model tend use one four million free paramet hinton salakhutdinov use million paramet free imag ranzato szummer use three million ter text process propos approach train million free paramet million unlabel train data becaus transfer data host gpu global memori time consum one need minim devic transfer take advantag share memori achiev thi one strategi store paramet larg chunk train exampl global memori dure train thi reduc data transfer time host globa memori also allow paramet updat carri fulli insid gpu addit util level parallel unlabel train data global memori select time pute updat concurr across block data parallel figur illustr architectur gpu highli thread stream processor sp thi exampl gpu ha stream processor sp organ four multiprocessor mp two stream multiprocessor sm sm ha eight sp share control unit instruct cach four mp build block also share global memori graphic doubl data rate dram often function memori memori bandwidth data exchang rate global memori typic ha high latenc access cpu host typic process flow includ input data first copi host memori gpu memori follow load execut gpu program result sent back gpu memori host memori practic one need pay care consider data transfer host gpu memori may take consider amount time fig meanwhil compon input exampl handl sp implement dbn learn gibb sampl repeat use eq thi ment ﬁrst gener two sampl matric p p j element p iti hidden node given input exampl p respect sampl matric implement parallel gpu block take exampl thread work element exampl similarli weight updat oper eq perform parallel use linear algebra packag gpu new exampl gener experiment result show million paramet rbm one million exampl mentat increas speed dbn learn factor compar cpu implement around minut implement versu one day implement convolut neural network cnn type local connect deep learn method cnn learn often implement gpu sever hundr parallel process core cnn ing involv forward backward propag parallel forward propag one block assign featur map depend size map thread block devot singl neuron volum chen lin big data deep learn map consequ comput neuron includ convolut share weight kernel neuron previou layer activ summat perform sp output store global memori weight updat error δk error signal δ k neuron k previou layer l depend error signal δ l j neuron local ﬁeld current layer parallel backward propag implement either pull push pull error signal refer process ing delta signal neuron previou layer pull error signal current layer thi straightforward becaus subsampl convolut oper exampl neuron previou layer may connect differ number neuron previou layer due border effect illustr plot dimension convolut subsampl fig seen ﬁrst six unit differ number connect need ﬁrst identifi list neuron current layer contribut error signal neuron previou layer contrari unit current layer exactli number incom connect consequ push error signal current layer previou layer efﬁcient unit current layer updat relat unit previou layer figur illustr oper involv convolut subsampl convolut filter size six consequ unit convolut layer defin six input unit subsampl involv averag two adjac unit convolut layer implement data parallel one need consid size global memori featur map size typic ani given stage limit number train exampl process parallel furthermor within block comvolut oper perform onli portion featur map maintain ani given time due extrem limit amount share memori convolut oper scherer et al suggest use limit share memori circular buffer onli hold small portion featur map load global memori time convolut perform thread parallel result written back global memori overcom gpu memori limit author ment modiﬁ architectur convolut subsampl oper combin one step thi modiﬁc allow store activ error valu reduc memori usag run backpropag speedup krizhevski et al propos use two gpu train cnn ﬁve convolut layer three fulli connect classiﬁc layer cnn use rectiﬁ linear unit relu nonlinear function f x max x ha shown run sever time faster commonli use function layer half network comput singl gpu portion calcul gpu two gpu commun layer thi tectur take full advantag parallel allow two gpu commun transfer data without use host memori combin scheme distbelief softwar framework recent design tribut train learn deep network veri larg model billion paramet data set leverag cluster machin manag data model parallel via multithread messag pass synchron well commun machin data high dimension deep ing often involv mani dens connect layer larg number free paramet larg model deal larg model learn distbelief ﬁrst implement model parallel allow user partit larg network tectur sever smaller structur call block whose node assign calcul sever machin collect call partit model block assign one machin see fig boundari node node whose edg belong one partit requir data transfer machin appar connect network boundari node often demand higher commun cost structur thu less perform beneﬁt nevertheless mani partit report larg model distbelief lead signiﬁc improv train speed distbelief also implement data parallel employ two separ distribut optim procedur downpour stochast gradient descent sgd sandblast perform onlin batch optim respect herein discuss downpour detail mation sandblast found refer first multipl replica partit model creat train infer like deep learn model larg data set partit mani subset lief run multipl replica partit model comput gradient descent via downpour sgd differ subset train data speciﬁc distbelief employ central paramet server store appli updat volum chen lin big data deep learn figur distbelief model partit four block consequ assign four machin inform node belong two partit transfer machin line mark yellow color thi model effect less dens connect network paramet model paramet group server shard ani given time machin tion model need onli commun paramet server shard hold relev paramet thi munic asynchron machin partit model run independ paramet server shard act independ well one advantag use chronou commun standard synchron sgd fault toler event failur one machin model copi model replica continu nicat central paramet server process data updat share weight practic adagrad adapt learn rate dure integr downpour sgd ter perform distbelief implement two deep learn model fulli connect network million model paramet billion exampl connect convolut neural network million imag pixel categori mani billion paramet experiment result show local connect learn model beneﬁt distbelief inde machin billion paramet method faster use singl machin demonstr signiﬁc advantag distbelief abil scale singl machin thousand machin key big data analysi recent distbelief framework wa use train deep architectur spars deep autoencod local tive ﬁeld pool local contrast normal deep learn architectur consist three stack layer sublay local ﬁltere local pool local contrast normal ﬁltere sublay lution ﬁlter weight optim thi architectur involv overal object function summat object function three layer aim minim reconstruct error maintain sparsiti connect sublay distbelief framework abl scale dataset model resourc togeth model partit machin cpu core multipl core allow anoth level parallel subset core perform differ task asynchron sgd implement sever replica core model train exampl framework wa abl train mani million imag size pixel thousand categori three day cluster machin core model capabl learn featur detect object without use label data cot hpc system distbelief learn veri larg model one billion paramet train requir cpu core commonli avail research recent coat et al present nativ approach train compar deep network el billion free paramet use three machin commod high perform comput cot hpc system compris cluster gpu server inﬁniband adapt interconnect mpi data exchang cluster server equip four nvidia gpu memori number gpu cpu cot hpc capabl run veri deep learn implement includ care design cuda kernel effect usag memori efﬁcient tation exampl efﬁcient comput matrix tiplic wx w ﬁlter matrix x input matrix coat et al fulli take advantag matrix spars local recept ﬁeld extract zero column w neuron share ident recept ﬁeld multipli correspond row thi strategi success avoid situat request memori larger share memori gpu addit matrix oper perform use highli optim tool call magma bla multipli kernel furthermor gpu util implement model parallel scheme gpu onli use differ part model optim input exampl collect commun occur mpi thi veri larg scale deep learn system capabl train billion paramet largest model report far much less machin tabl summar current progress deep learn ha observ sever group see singl cpu impract deep learn larg model multipl machin run time may big concern ani see howev volum chen lin big data deep learn signiﬁc comput resourc need achiev goal consequ major research effort toward experi gpu tabl summari recent research progress deep learn iv remain challeng perspect deep learn big data recent year big data ha taken center stage govern societi larg obama administr announc big data research develop tive help solv nation press challeng consequ six feder depart agenc nsf dod doe darpa usg commit million support project transform abil har novel way huge volum digit data may year state massachusett announc massachusett big data initi fund varieti research institut april presid barack obama announc anoth feder project new brain map initi call brain brain research advanc innov neurotechnolog aim develop new tool help map human brain function understand complex link function behavior treat cure brain order thi initi might test extend current limit technolog big data collect analysi nih director franci collin state collect storag process yottabyt billion petabyt data would eventu requir thi initi potenti big data undoubtedli signiﬁc fulli achiev thi potenti requir new way think novel algorithm address mani technic challeng exampl tradit machin learn algorithm design data would complet load memori arriv big data age howev thi assumpt doe hold ani therefor algorithm learn massiv amount data need spite recent achiev deep learn discuss section thi ﬁeld still infanc much need done address mani niﬁcant challeng post big data often character three v model volum varieti veloc refer larg scale data differ type data speed stream data respect deep learn high volum data first foremost high volum data present great challeng issu deep learn big data often possess larg number exampl input larg varieti class type output veri high dimension attribut properti directli lead complex model complex sheer volum data make often imposs train deep learn algorithm central processor storag instead distribut framework parallel machin prefer recent impress progress made mitig challeng relat high volum novel model util cluster cpu gpu increas train speed without scarifi accuraci deep learn algorithm strategi data allel model parallel develop exampl data model divid block ﬁt data forward backward propag implement effect parallel although deep learn algorithm trivial parallel recent deep learn framework handl signiﬁcantli larg number sampl paramet also possibl scale gpu use less clear howev deep learn system continu scale signiﬁcantli beyond current framework expect continu growth comput memori comput power mainli parallel tribut comput environ research effort address issu associ comput munic manag copi data paramet gradient valu differ machin need veri larg data set ultim build futur deep learn system scalabl big data one need develop high perform comput system togeth theoret sound parallel learn algorithm novel architectur anoth challeng associ high volum data incomplet noisi label unlik tional dataset use machin learn highli curat nois free big data often incomplet ing dispar origin make thing even complic major data may label label exist noisi label take million tini volum chen lin big data deep learn imag databas exampl ha million resolut color imag search term thi imag databas wa creat search web everi english noun wordnet sever search engin googl flickr use collect data span six month manual curat wa conduct remov duplic imag still imag label extrem unreli becaus search technolog one uniqu characterist deep learn algorithm possess abil util unlabel data dure ing learn data distribut without use label tion thu avail larg unlabel data present ampl opportun deep learn method data incomplet noisi label part big data packag believ use vastli data prefer use smaller number exact clean care curat data advanc deep learn method requir deal noisi data abl toler messi exampl efﬁcient cost function novel train strategi may need allevi effect noisi label strategi use learn may also help allevi problem relat noisi label deep learn high varieti data second dimens big data varieti data today come type format varieti sourc probabl differ distribut exampl rapidli grow multimedia data come web mobil devic includ huge collect still imag video audio stream graphic anim unstructur text differ characterist key deal high varieti data integr clearli one uniqu advantag deep learn abil represent learn either supervis unsupervis method combin deep learn use learn good featur represent classiﬁc abl discov mediat abstract represent carri use unsupervis learn hierarchi fashion one level time featur deﬁn featur thu natur solut address data integr lem learn data represent individu data sourc use deep learn method integr learn featur differ level deep learn ha shown veri effect integr data differ sourc exampl ngiam et al develop novel applic deep learn algorithm learn represent integr audio video data demonstr deep learn eral effect learn singl modal tion multipl modal unlabel data learn share represent capabl captur relat across multipl modal recent tava salakhutdinov develop multimod deep boltzmann machin dbm fuse two veri differ data modal dens imag data text data spars word frequenc togeth learn uniﬁ sentat dbm gener model without ﬁrst build multipl modal form multimod dbm addit layer binari hidden unit ad top rbm joint represent learn joint distribut multimod input space allow learn even miss modal current experi demonstr deep learn abl util heterogen sourc niﬁcant gain system perform numer question remain open exampl given differ sourc may offer conﬂict inform resolv ﬂict fuse data differ sourc effect efﬁcient current deep learn method mainli test upon data two sourc system perform beneﬁt signiﬁcantli enlarg modal furthermor level deep learn architectur appropri featur fusion geneou data deep learn seem well suit gration heterogen data multipl modal due capabl learn abstract represent underli factor data variat deep learn high veloc data emerg challeng big data learn also aros high veloc data gener extrem high speed need process time manner one solut learn high veloc data onlin learn approach onlin learn learn one instanc time true label instanc soon avail use reﬁn model thi sequenti learn strategi particularli work big data current machin hold entir dataset memori convent neural network explor onlin learn onli limit progress onlin deep learn ha made recent year interestingli deep learn often train stochast gradient descent approach one train exampl known label use time updat model paramet thi strategi may adapt onlin learn well speed learn instead proceed sequenti one exampl time updat perform batch basi practic exampl independ possibl provid good balanc comput memori run time anoth challeng problem associ high veloc data often data tribut chang time practic data normal separ chunk data small time interv assumpt data close time stationari may character signiﬁc degre correl therefor follow distribut thu import featur deep learn algorithm big data abil learn data stream one area need explor deep onlin learn onlin learn often scale natur volum chen lin big data deep learn memori bound readili paralleliz theoret guarante algorithm capabl learn data crucial big data learn deep learn also leverag high varieti veloc big data transfer learn domain adapt train test data may sampl differ distribut recent glorot et al implement stack denois base deep architectur domain adapt one train unsupervis sentat larg number unlabel data set domain appli train classiﬁ label exampl onli one domain empir result demonstr deep learn abl extract ful represent share across differ domain intermedi abstract gener enough uncov underli factor domain tion transfer across domain recent bengio also appli deep learn multipl level sentat transfer learn train exampl may well repres test data show abstract featur discov deep learn approach like gener train test data thu deep learn top candid transfer learn becaus abil identifi share factor present input although preliminari experi shown much potenti deep learn transfer learn appli deep learn thi ﬁeld rel new much need done improv perform cours big question whether beneﬁt big data deep architectur transfer learn conclus big data present signiﬁc challeng deep learn includ larg scale heterogen noisi label distribut among mani order realiz full potenti big data need address technic challeng new way think transform solut believ research challeng pose big data onli time also bring ampl opportun deep learn togeth provid major advanc scienc medicin busi refer nation secur agenc nation secur agenc mission author oversight partnership onlin avail http gantz reinsel extract valu chao hopkinton usa emc jun gantz reinsel digit univers readi hopkinton usa emc may may big data next frontier innov competit product mckinsey global institut onlin avail http lin kolcz machin learn twitter proc acm sigmod scottsdal arizona usa pp smola narayanamurthi architectur parallel topic model proc vldb endow vol pp ng et machin learn multicor proc adv neural inf proce vol pp panda herbach basu bayardo mapreduc applic massiv parallel learn decis tree ensembl scale machin learn parallel distribut approach cambridg cambridg univ press crego munoz islam big data deep learn big deal big delus busi onlin avail http bengio bengio model discret data neural network proc adv neural inf process vol pp marc aurelio ranzato boureau lecun spars featur learn deep belief network proc adv neural inf process vol pp dahl yu deng acero train deep neural network speech recognit ieee tran audio speech lang vol pp hinton et deep neural network acoust model speech recognit share view four research group ieee signal process vol pp salakhutdinov mnih hinton restrict boltzmann machin collabor ﬁltere proc int conf mach pp cireşan meler cambardella schmidhub deep big simpl neural net handwritten digit recognit neural vol pp zeiler taylor fergu adapt deconvolut network mid high level featur learn proc ieee int conf comput pp efrati deep learn work appl beyond inform onlin avail http jone comput scienc learn machin natur vol pp wang yu ju acero voic search languag stand system extract semant inform speech tur de mori ed new york ny usa wiley ch kirk univers ibm join forc build comput pcworld onlin avail http hinton salakhutdinov reduc dimension data neural network scienc vol pp bengio learn deep architectur ai found trend mach vol pp nair hinton object recongit deep belief net proc adv nip vol pp lecun bottou bengio haffner ing appli document recognit proc ieee vol pp collobert weston bottou karlen kavukcuoglu kuksa natur languag process almost scratch mach learn vol pp le callet barba convolut neural network approach object video qualiti assess ieee tran neural vol pp rumelhart hinton william learn represent error natur vol pp hinton practic guid train restrict boltzmann machin dept comput univ toronto toronto canada tech utml tr hinton osindero teh fast learn algorithm deep belief net neural vol pp bengio lamblin popovici larochel greedi wise train deep network proc neural inf process pp hinton train product expert minim contrast genc neural vol pp vincent larochel bengio manzagol extract compos robust featur denois autoencod proc int conf mach pp volum chen lin big data deep learn larochel bengio louradour lamblin explor strategi train deep neural network mach learn vol pp lee battl raina ng efﬁcient spars code rithm proc neural inf proce pp seid li yu convers speech transcript use deep neural network proc interspeech pp cireşan meier masci gambardella schmidhub flexibl high perform convolut neural network imag classiﬁc proc int conf artif pp scherer müller behnk evalu pool oper convolut architectur object recognit proc int conf artif neural pp lecun bottou orr muller efﬁcient backprop ral network trick trade orr muller ed new york ny usa kavukcuoglu ranzato fergu lecun learn invari featur topograph ﬁlter map proc int conf cvpr pp hubel wiesel recept ﬁeld function architectur monkey striat cortex j vol pp mar raina madhavan ng deep unsupervis learn use graphic processor proc int conf mach montreal qc canada pp marten deep learn via optim proc int conf mach zhang chen deep belief net mapreduc ieee access vol pp apr deng yu platt scalabl stack learn build deep architectur proc ieee icassp mar pp hutchinson deng yu tensor deep stack network ieee tran pattern anal mach vol pp vanhouck senior mao improv speed neural network cpu proc deep learn unsupervis featur learn workshop krizhevski learn multipl layer featur tini imag dept comput univ toronto toronto canada tech farabet et convolut network machin learn veri larg data set bekkerman bilenko langford ed cambridg cambridg univ press cuda c program guid nvidia corpor santa clara ca usa jul le et build featur use larg scale unsupervis learn proc int conf mach ranzato szummer learn compact document represent deep network proc int conf mach pp geman geman stochast relax gibb distribut bayesian restor imag ieee tran pattern anal mach vol pp casella georg explain gibb sampler amer vol pp simard steinkrau platt best practic convolut neural network appli visual document analysi proc icdar pp krizhevski sutskev hinton imagenet classiﬁc deep convolut neural network proc adv nip pp dean et larg scale distribut deep network proc adv nip pp duchi hazan singer adapt subgradi method onlin learn stochast optim mach learn vol pp jul coat huval wng wu wu deep ing cot hp system mach learn vol pp tomov nath du dongarra magma user guid icl univ tennesse knoxvil tn usa onlin avail http obama administr unveil big data initi announc million new r invest ofﬁc scienc nolog polici execut ofﬁc presid washington dc usa onlin avail http haberlin mcgilpin ouellett governor patrick announc new initi strengthen massachusett posit world leader big data commonwealth massachusett onlin avail http fact sheet brain initi ofﬁc press secretari white hous washington dc usa laney import big data deﬁnit stamford ct usa gartner torralba fergu freeman million tini imag larg data set nonparametr object scene recognit ieee tran softw vol pp wang shen larg margin learn mach learn vol pp weston ratl collobert deep learn via embed proc int conf mach helsinki finland sinha belkin learn use spars function base proc adv nip pp fergu weiss torralba learn gigant imag collect proc adv nip pp ngiam khosla kim nam lee ng multimod deep learn proc int conf mach bellevu wa usa srivastava salakhutdinov multimod learn deep boltzmann machin proc adv nip bottou onlin algorithm stochast approxim learn neural network saad ed cambridg cambridg univ press blum burch learn metric task system problem proc annu conf comput learn theori pp freund helmbold warmuth predict convers strategi proc conf comput learn theori eurocolt vol oxford pp freund schapir game theori predict ing proc annu conf comput learn theori pp littleston long warmuth ing linear function proc symp theori pp onlin learn onlin convex optim found trend mach vol pp hesk kappen learn process artiﬁci neural network math librari vol pp marti multilay neural network experiment evalu train method comput operat vol pp lim harrison onlin pattern classiﬁc multipl neural network system experiment studi ieee tran man cybern c appl vol pp may rattray saad global optim learn rule neural network phi math gener vol pp riegler biehl backpropag neural network phi vol pp saad solla exact solut learn multilay neural network phi rev vol pp west saad learn adapt network phi rev e vol pp campolucci uncini piazza rao learn algorithm local recurr neural network ieee tran neural vol pp mar liang huang saratchandran sundararajan fast accur onlin sequenti learn algorithm feedforward network ieee tran neural vol pp ruiz de angulo torra learn minim dation feedforward network ieee tran neural vol pp may volum chen lin big data deep learn choy srinivasan cheu neural network continu onlin learn control ieee tran neural vol pp bottou bousequet stochast gradient learn neural network proc singer srebro pegaso primal estim solver svm proc int conf mach chien hsieh nonstationari sourc separ use sequenti variat bayesian learn ieee tran neural netw learn vol pp may sugiyama kawanab machin learn environ introduct covari shift adapt cambridg usa mit press mar elwel polikar increment learn nonstationari ment control forget proc int joint conf neural pp elwel polikar increment learn concept drift stationari environ ieee tran neural vol pp alippi roveru adapt detect nonstationari chang ieee tran neural vol pp jul alippi roveru adapt ii design classiﬁ ieee tran neural vol pp rutkowski adapt probabilist neural network pattern ﬁcation environ ieee tran neural vol pp jul de oliveira rosenblatt bayesian algorithm learn stationari environ ieee tran neural vol pp mar bartlett optim onlin predict adversari environ proc int conf ds bengio deep learn represent unsupervis fer learn mach learn vol pp glorot bord bengio domain adapt sentiment classiﬁc deep learn approach proc int conf mach bellevu wa usa mesnil et unsupervis transfer learn challeng deep learn approach mach learn vol pp pan yang survey transfer learn ieee tran knowl data vol pp gutstein fuent freudenth knowledg transfer deep convolut neural net int artif intel tool vol pp blum mitchel combin label unlabel data proc annu conf comput learn theori pp raina battl lee packer ng learn transfer learn unlabel data proc icml pan tsang kwok yang domain adapt via transfer compon analysi ieee tran neural vol pp mesnil rifai bord glorot bengio vincent unsupervis transfer learn uncertainti object detect scene categor proc icpram pp chen current professor chair depart comput scienc wayn state univers detroit mi usa receiv degre carnegi mellon univers pittsburgh pa usa current serv ciat editor editori board member eral intern journal includ ieee access bmc system biolog ieee transact inform technolog biomedicin serv confer chair program chair number confer acm confer inform knowledg agement ieee intern confer machin learn applic senior member ieee comput societi xiaotong lin current visit assist professor ment comput scienc engin oakland univers rochest mi usa receiv degre univers kansa lawrenc ks usa degre univers pittsburgh pittsburgh pa usa research interest includ larg scale machin learn data mine comput bioinformat volum