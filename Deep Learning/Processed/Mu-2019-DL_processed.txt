ksii transact internet inform system vol apr review deep learn research ruihui xiaoqin comput inform hohai univers nanj muruihui xzeng comput inform engin xinxiang univers xinxiang correspond author ruihui mu receiv juli revis septemb accept octob publish april abstract advent big data deep learn technolog ha becom import research direct field machin learn ha wide appli imag process natur languag process speech recognit onlin advertis thi paper introduc deep learn techniqu variou aspect includ common model deep learn optim method commonli use open sourc framework exist problem futur research direct firstli introduc applic deep learn secondli introduc sever common model deep learn optim method thirdli describ sever common framework platform deep learn final introduc latest acceler technolog deep learn highlight futur work deep learn keyword deep learn machin learn artifici intellig learn network optim method http issn ksii transact internet inform system vol apr introduct rapid develop technolog cloud comput big data internet thing emerg variou applic internet space ha led explos growth data scale accord report intern data corpor idc global total data expect time reach big data contain rich valu great potenti bring transform develop human societi also bring seriou problem inform overload quickli effici obtain valuabl inform variou complex data ha becom key challeng recent year deep learn ha made breakthrough imag process natur languag understand speech recognit view rapid develop deep learn technolog deep learn map differ data hidden space perform automat featur learn heterogen data obtain unifi data represent thi paper review deep learn techniqu variou aspect includ common model deep learn optim method commonli use framework exist problem futur research direct remaind thi paper organ follow section review relat applic deep learn section address common model deep learn section describ optim method section introduc commonli use open sourc framework platform deep learn section present acceler technolog deep learn section describ problem prospect deep learn section conclud work outlin potenti futur work relat applic imag process imag recognit earliest applic deep learn dong et al first propos use deep convolut neural network learn map relationship imag imag imag recognit denker use convolut neural network recogn handwritten digit achiev best result time ren et al propos faster object detect method use deep convolut neural network wang et al introduc spars prior deep convolut neural network imag recognit yu et al use autoencod classifi imag train support vector machin imag classif accuraci deep learn exceed imagenet ruihui mu et al review deep learn research competit xia et al propos supervis depth hash algorithm call cnnh convolut neural network hash imag recognit speech recognit recent year deep learn technolog ha appli field speech recognit field speech recognit baidu hkust sogou announc accuraci chines speech recognit base deep learn exceed end microsoft research speech recognit base deep neural network ha complet chang origin technic framework speech recognit deep neural network model ha brought tremend innov improv accuraci speech recognit present speech recognit algorithm use internet compani baidu hkust sogou adopt deep neural network model appli convolut neural network cnn speech featur extract et al propos speech synthesi model base multilay perceptron speech recognit use lstm method extract speech featur greatli improv effici featur hinton replac gaussian mixtur model gmm tradit model dbn result show error rate drop timit core test set wa significantli improv recent speech recognit system feedforward sequenti memori network propos googl use larg number convolut layer directli model entir sentenc speech signal better express relev speech baidu appli deep convolut neural network speech recognit research use visual geometri group network combin deep convolut neural network recognit error rate ha drop greatli natur languag process natur languag process also applic deep learn cho et al propos vector constant length represent model base recurr neural network rnn machin translat artifici neural network attract attent natur languag process rousseau et al use similar model statist machin translat task evalu bilingu evalu understudi rate mechan karlen adopt embed convolut structur typic natur languag process issu semant role label deora et al studi neural network model found perform improv ad multipl recurs layer vincent et al propos use embed method map word vector represent space use nonlinear neural network repres languag model bahdanau et al put forward rnn search model machin translat ksii transact internet inform system vol apr deep learn techniqu wide use variou task field natur languag process includ tag depend grammar analysi name bodi recognit semant role tag twitter sentiment analysi chines microblog sentiment analysi machin translat question answer dialogu system etc common model deep learn autoencod autoencod unsupervis learn algorithm appli backpropag set target valu equal input rumelhart et al propos concept autoencod ae use complex data process autoencod extract latent featur reconstruct input data form output data basic structur autoencod view neural network input layer x hidden layer h output layer output layer input layer ha scale fig schemat diagram autoencod fig schemat diagram autoencod model autoencod three layer input layer output layer neuron middl layer output layer reconstruct input data train network similar input data output data possibl train error repres similar assum input signal x input layer reach hidden layer signal chang z express follow formula ùëî ùëè g nonlinear function commonli use sigmoid function rectifi linear unit function also call activ function w link weight input layer hidden layer b bia hidden layer signal decod ruihui mu et al review deep learn research decod layer output output layer signal becom z follow ùëì f nonlinear function sigmoid function link weight hidden layer output layer b bia output layer treat predict gener weight matrix limit transpos weight matrix w ùëäùëá accord differ form data reconstruct error usual defin mean squar error cross entropi research introduc idea spars represent propos spars autoencod averag valu output signal penalti encourag small mean valu approxim gaussian distribut autoencod wa train complet task locat key point face order enhanc gener autoencod mikolov et al propos denois autoencod ad artifici nois train sampl allow network reconstruct origin clean input noisi signal deep autoencod use obtain compact imag descript imag retriev mani research success appli deep autoencod imag featur represent autoencod reconstruct input data train network tune paramet cascad form deep neural network restrict boltzmann machin hinton sejnowski propos boltzmann machin bm boltzmann machin random neural network belong type feedback neural network boltzmann machin consist visibl unit visibl variabl data sampl hidden unit hidden variabl visibl unit connect hidden unit visibl variabl hidden variabl binari variabl state repres neuron suppress state repres neuron activ state sejnowski et al propos restrict boltzmann machin rbm fig schemat diagram rbm includ visibl layer v hidden layer input train data visibl layer hidden layer detect featur input data neuron disconnect layer fulli connect two layer equat repres joint probabl distribut two layer ùëÉ ùë£ ‚Ñé z repres normal function b denot bias visibl layer hidden layer respect w denot connect weight two layer equat goal optim arg ùëÉ ùë£ ‚Ñé ksii transact internet inform system vol apr fig restrict boltzmann machin train restrict boltzmann machin faster autoencod propos effici optim algorithm base stochast gradient descent method tradit train method rbm requir larg number sampl step make train effici rbm still high contrast diverg propos hinton solv thi problem taylor et al construct deep deconvolut network concaten multipl convolut spars autoencod maximum pool layer learn hierarch structur featur bottom layer top layer directli global imag research put forward mani expans model base restrict boltzmann machin propos discrimin learn incorpor rbm gener learn algorithm better appli discrimin task classif rbm model directli cascad structur propos deep boltzmann machin hinton et al propos deep spars autoencod model learn latent featur imag pixel block restrict boltzmann machin cascad form deep neural network use train method optim hinton et al extend deep belief network convolut oper model learn potenti featur represent directli origin imag except deep structur hierarch gener model direct sigmoid belief network wa cascad rbm construct deep belief network introduc gaussian kernel restrict boltzmann machin support continu variabl input signal restrict boltzmann machin extend solv complex problem modifi structur rbm probabl distribut model usual defin complex energi function reduc effici learn infer convolut neural network convolut neural network cnn convolut layer pool layer layer except input layer output layer cnn reduc complex paramet share weight promot gener abil neural network reduc neuron pool oper robust recent year ruihui mu et al review deep learn research cnn appli process data imag understand fig show schemat diagram cnn fig convolut neural network input imag cnn imag directli input network therebi avoid complex featur extract data reconstruct process tradit imag process algorithm convolut neural network necessari learn set filter templat convolut oper featur map x obtain featur map convolut layer featur map previou layer convolut convolut kernel output convolut result activ function form neuron next layer featur map therebi form next layer correspond certain featur map featur convolut layer contain three oper convolut nonlinear activ function maximum pool convolut use differ convolut kernel extract differ featur previou layer featur map subsampl layer pool layer usual follow convolut layer featur map accord certain subsampl rule two main function layer dimension reduct featur map b maintain scale invari characterist featur certain extent thi oper featur map first divid number spatial region use uniform grid region may overlap portion averag maximum valu imag region taken output exist research work prove perform maximum pool oper better averag pool imag featur extract multipl convolut subsampl layer altern transmit convolut neural network reli fulli connect network classifi extract featur obtain probabl distribut train goal convolut neural network minim loss function network common loss function ha mean squar error mse function neg log likelihood nll function etc ksii transact internet inform system vol apr research propos mani simpl effect techniqu momentum method weight decay data enhanc hinton et al extend thi idea train full connect layer subset set iter select randomli connect weight network network updat differ network structur thi enhanc gener model due larg number paramet convolut neural network easi overfit affect final test perform glorot et al propos dropout optim techniqu prevent overfit randomli ignor half featur point train iter howev dropout perform improv convolut neural network obviou main reason convolut neural network greatli reduc number train paramet compar fulli connect network due weight share characterist convolut kernel avoid seriou therefor dropout method act fulli connect layer ideal effect convolut neural network whole recurr neural network neuron fulli connect network cnn disconnect layer fulli connect differ layer layer process signal independ propag next layer thi type network work well sequenti data rnn produc output compris new input latent vector rnn three layer input layer hidden layer output layer theori rnn work well sequenti data complex network simpl becaus current data onli depend previou data fig show schemat diagram rnn neuron connect hidden layer rnn remain former inform fig rnn expand time x repres output inform input inform respect h denot hidden unit w u v repres weight denot time input time previou state time determin output hidden unit time accord differ domain rnn differ variant lstm long ruihui mu et al review deep learn research memori network exampl bidirect rnn echo state network long memori model lstm lstm network variant rnn lstm work well data rnn onli memori due disappear gradient rnn design process entir time seri inform deepli rememb signal last input signal intens signal affect previou signal get lower lower lstm avoid disappear gradient extent control gate long memori short memori lstm differ rnn becaus lstm determin inform use cell cell includ forget gate except input gate output gate input messag lstm determin inform retain forget accord whether match certif algorithm shown fig network structur lstm input previou layer act output path introduct gate make network focus effect lstm gener includ input gate forget gate output gate input gate supplement latest input current input state forgotten part output gate base latest state ct previou moment output current input xt determin output ht thi moment forget gate make recurr neural network forget inform wa use befor lstm natur rememb input long befor long time storag unit cell special unit act like accumul gate leaki neuron thi unit ha direct connect previou state next state replic current state accumul extern signal due presenc forget gate lstm learn decid clear content memori unit fig schemat diagram lstm cell bidirect rnn bidirect rnn rel simpl rnn compos two rnn superimpos top state two hidden layer determin output brnn structur shown fig rnn current time usual take account inform previou moment without ksii transact internet inform system vol apr consid follow inform bidirect rnn overcom thi shortcom introduc follow consider previou state follow state determin current output fig bidirect rnn structur echo state network esn esn also type rnn veri differ tradit rnn core structur make reserv pool gener ani time ha constant control reserv pool randomli gener spars connect loop structur reserv pool output layer weight matrix onli part need adjust core algorithm use recurr network random connect replac hidden layer thu simplifi process train deep neural network deep neural network dnn includ input output layer multipl hidden layer dnn deal linear problem comput probabl output layer layer appropri activ function dnn usual appli imag understand speech recognit dnn essenti neural network deep neural network sometim call perceptron mlp input featur vector transform hidden layer reach output layer final get classif result wa linear classif model two categori mainli use linear classif classif abil wa limit earli discret transfer function shackl multiplay perceptron use continu function avoid thi problem tanh function sigmoid function dnn construct ad number neuron hidden layer ruihui mu et al review deep learn research fig deep neural network structur fig show structur fulli connect deep neural network deep neural network divid three layer input layer hidden layer output layer first layer input layer last layer output layer middl layer hidden layer process deep neural network train firstli accord requir initi perform set structur dnn forward perform layer transmit layer obtain error back propag perform accord principl minim error stochast gradient descent method use deduct paramet determin direct descent updat paramet dnn mani variant appli differ field success becaus data set usual differ perform differ dnn usual ca compar deep belief network deep belief network dbn wa propos geoffrey hinton gener model make whole neural network gener train data high probabl train weight neuron multipl restrict boltzmann machin stack construct deep belief network visibl layer restrict boltzmann machin use hidden layer previou restrict boltzmann machin train restrict boltzmann machin simpl becaus output previou restrict boltzmann machin use train next restrict boltzmann machin structur shown fig gener model compos multipl layer nonlinear variabl connect deep belief network part close visibl layer multipl bayesian belief network part farthest visibl layer rbm time thi train method dbn also obtain deep featur represent unlabel data identifi featur categor data gener data ksii transact internet inform system vol apr fig deep belief network dbn consist multipl layer neuron subdivid visibl neuron hidden neuron visibl element use accept input hidden element use extract featur therefor hidden element also individu name call featur detector top two layer dbn undirect connect thi connect constitut associ memori dbn construct rbm one dimens data vector repres neuron bottom layer bottom layer denot data vector layer dbn connect layer layer process train dbn done layer layer layer data vector use infer hidden layer thi hidden layer treat data vector next layer higher layer gener adversari network goodfellow et al propos gener adversari network idea come game differ previou gener model guid train gener model discrimin model network consist two part gener g discrimin discrimin equival judg whether input data real data gener data gener use captur distribut train data discrimin equival judg whether input data real data gener data result network output repres probabl input data come real data gener gener exampl similar sampl train dataset synthet sampl transform random nois discrimin discrimin data sampl synthet sampl train discrimin predict accur train gener produc synthet sampl make discrimin believ real data sampl structur shown fig ruihui mu et al review deep learn research fig gener adversari network use framework probabl describ gan better understand defin two differ margin distribut one q x repres data margin distribut ùëù x z ùëù ùëëz denot model margin distribut equat defin function gener g discrimin min ùê∫ max ùê∑ ùëâ ùê∑ ùê∫ ùê∏ùëû x x ùê∏ùëù z ùê∫ z ‡∂±ùëû x x x repres real sampl z repres nois input gener g g z repres sampl gener gener g repres probabl discrimin determin whether sampl true train process goal gener g gener real sampl much possibl deceiv discrimin goal discrimin separ sampl real sampl thu gener g discrimin constitut dynam adversari game minim ùê∑ x ùëû x ùëû x ùëù x equival diverg data margin distribut model margin distribut also minim train continu gener g gener sampl similar real sampl fusion neural network fusion train multipl model integr model accord certain method develop deep learn research shown singl deep learn model alway bring best result howev method increas depth model often ha certain limit gradient disappear comput complex model parallel model fusion base deep learn may bring better result excel neural network model appear recent gener spike neural network deep residu network echo state network esn attent neural network etc karpathi et al combin cnn rnn automat gener imag descript enabl combin model gener textual descript base featur imag ksii transact internet inform system vol apr gener correspond imag base text accord relationship individu learner divid two categori strong depend individu learner serial method must gener serial repres boost method strong depend individu learner parallel gener simultan method repres bag random forest optim method deep learn model two way parallel deep neural network model parallel data parallel thi section mainli introduc commonli use optim method data parallel data parallel refer segment train data multipl model use train multipl piec data simultan data parallel distribut train store backup model worker node process differ part data set machin data parallel train method need combin result work node synchron model paramet node basic architectur data parallel shown fig paramet server respons updat latest model paramet fig basic architectur data parallel data parallel divid synchron mode asynchron mode synchron mode one batch train data train time train program synchron exchang paramet time paramet exchang complet train program common new model start point next batch train synchron updat process data parallel distribut gradient descent advantag converg model ruihui mu et al review deep learn research rel stabl becaus thi mode use larg batch larger batch closer effect batch gradient overal gradient howev disadvantag synchron mode inform transmiss overhead larg ha short board effect asynchron mode train program complet batch train data immedi exchang paramet paramet server regardless state train program latest result train program asynchron mode immedi reflect train program perform next paramet exchang advantag data parallel easier implement easier extend onli w commun requir data model doe need commun disadvantag model larg gpu memori meet storag requir complet calcul model parallel model parallel also good method larg model ca accommod memori fig show basic framework model parallel differ data parallel model parallel split model sever shard held sever train unit work togeth complet train commun overhead occur input one neuron come output neuron anoth train unit case commun overhead synchron consumpt brought model parallel exce data parallel acceler less data parallel fig basic architectur model parallel data parallel model parallel extend indefinit mani data parallel train program learn rate ha reduc ensur smooth train process mani slice parallel mode exchang amount neuron output valu increas sharpli effici drop drastic therefor simultan model parallel data parallel also ksii transact internet inform system vol apr common solut common framework platform deep learn common framework industri academia introduc relat open sourc framework use similar method parallel learn deep learn model librari use cpu train network use data parallel model parallel dnn librari support gpu speed network implement distribut parallel deep learn model data parallel model parallel use commonli use softwar tool follow theano theano born montreal ha spawn number deep learn python packag notabl block theano python librari defin optim comput mathemat express effici comput multidimension array advantag integr numpi use gpu acceler calcul effici symbol differenti speed stabil optim dynam gener c code extens unit test disadvantag transfer limit bad paramet scan immut mechan caus function compil take long theano lack flexibl polymorph mechan defin function difficult debug method torch core comput unit well optim use c common model built use lua core featur power array mani implement index slice transpos routin via lua linear algebra routin neural network model numer optim routin effici gpu support embedd portabl io android fpga backend mxnet mxnet combin advantag imper declar program optim system facilit debug resourc comput schedul memori alloc resourc manag data represent comput optim etc worth learn nativ support distribut train mxnet provid imper tensor calcul bridg main languag symbol express hand nd array seamlessli interfac symbol express mxnet provid distribut store data exchang mainli ha two function push push pair devic store pull pull valu key store cntk cntk unifi deep learn tool describ neural network seri comput step direct graph thi direct graph leaf node repres input layer network paramet node repres matrix oper input layer easi implement combin today popular model cntk feedforward neural network dnn convolut neural network cnn recurr neural network gradient automat calcul ruihui mu et al review deep learn research implement stochast gradient descent learn parallel comput implement multipl gpu server biggest advantag cntk parallel multipl gpu server anoth advantag cntk support microsoft window thi open sourc tool written caff caff learn framework deep convolut neural network conveni train test cnn model use caff caff pure architectur support command line python matlab interfac seamlessli switch cpu gpu caff veri suitabl rapid develop engin applic caff offici provid larg number exampl accord exampl caff onli ask write proto txt train process gradient descent algorithm packag understand syntax proto txt basic construct neural network tensorflow tensorflow googl artifici intellig learn system base distbelief tensor mean array flow mean calcul base data flow graph tensorflow process tensor flow one end imag tensorflow system transmit complex data structur artifici intellig neural network analysi process tabl show sever commonli use open resourc framework seen mani open resourc framework base deep learn also mani correspond program languag tabl show perform comparison common open resourc framework dimens futur believ effici program languag platform may also emerg tabl comparison common framework framework support hardwar oper languag parallel mode theano cpu gpu data parallel torch cpu gpu pga data parallel mxnet cpu gpu data parallel model parallel cntk cpu gpu data parallel caff cpu gpu data parallel tensorflow cpu gpu data parallel model parallel ksii transact internet inform system vol apr tabl perform comparison common framework industri platform mariana tencent deep learn platform ha evolv three framework includ dnn gpu data parallel framework cnn gpu model parallel data parallel framework dnn cpu cluster model parallel data parallel framework data parallel model parallel mariana solv problem deep learn becom effect help deep learn research mariana effect support larg model model parallel mariana ha done lot work enhanc eas use simplifi deep learn experi greatli save algorithm develop time mariana dnn gpu data parallel framework wechat speech recognit applic complet dnn model train billion train sampl day model error rate reduc mariana cnn model parallel data parallel framework imagenet imag classif issu achiev accuraci imagenet dataset present tencent deep learn platform mariana ha support voic input method wechat voic recognit voic open platform long press voic messag text product began appli wechat imag recognit addit field advertis recommend person recommend also activ explor experi xdl ali mama deep learn framework base alibaba massiv scale busi scenario practic new gener distribut deep learn framework design develop xdl ha creat four key paradigm new data paradigm new model paradigm new capabl paradigm new architectur paradigm make truli xdl achiev approxim linear comput speedup thousand node concurr accommod hundr billion spars paramet train onlin stream train abil asynchron pipelin maxim hardwar satur present xdl ha wide use mani busi scenario ali mama creat revenu billion scale time thank xdl new architectur capabl seri model algorithm framework model design interfac deploy perform architectur design overal rate tensorflow caff torch mxnet cntk theano ruihui mu et al review deep learn research effect train xdl platform includ ani deep learn search model base tree structur user behavior imag paddl paddl open sourc deep learn framework provid baidu new gener deep learn framework base deep learn program languag ha greatli improv abil framework express model describ ani potenti possibl friendli comput lot comput busi baidu paddl excel distribut comput technolog save lot comput resourc also support spars model train provid visual deep learn visual dl help develop easili observ overal trend train data sampl qualiti intermedi result paramet distribut trend structur model help develop complet program process conveni acceler technolog deep learn present deep learn acceler technolog acceler dedic acceler fpga ha advantag low power consumpt high speed main deep learn acceler techniqu follow cpu acceler technolog design dedic cpu processor processor design algorithm algorithm famili ha characterist low power consumpt high speed high develop cost order design dedic processor make analysi algorithm get featur base featur algorithm design dedic circuit howev due larg amount comput high degre parallel deep learn cpu increasingli difficult adapt comput need onli util cpu research optim use regist base window movement featur convolut pool algorithm final acceler convolut neural network algorithm acceler convolut pool calcul research design hardwar neuron neuron structur map connect neuron latch memori final acceler neural network algorithm map neuron neural network hardwar neuron design differ storag unit accord featur algorithm memori alloc type data differ storag unit differ final util comput unit improv pipelin lower gener processor gpu acceler technolog gpu graphic process unit concept rel cpu wa propos rapid growth multimedia comput eventu becam processor independ ksii transact internet inform system vol apr tradit cpu today gpu provid ten time even hundr time perform cpu term comput oper parallel comput compar cpu gpu ha follow advantag power comput power gpu ha power comput power period theoret peak capac gpu calcul one order magnitud higher cpu price rel cheap individu small organ user obtain comput tradit cpu comput system use expens requir site power suppli also high use gpu train deep neural network achiev highli effici parallel comput capabl massiv amount train data time spent drastic reduc fewer server occupi asic acceler technolog asic applic specif integr circuit effici hardwar acceler method gpu one hand high degre custom increas comput power limit mobil hand asic expens difficult individu small organ combin two aspect research deep learn current rare consid use asic comput acceler asic integr circuit design specif purpos refer integr circuit design manufactur meet need specif user need specif electron system asic character need specif user asic divid full custom highlight dedic tailor execut speed faster faster fpga process fpga save unus logic implement fpga cost lower fpga product characterist asic need specif user asic ha advantag smaller size lower power consumpt higher reliabl improv perform enhanc confidenti lower cost compar integr circuit mass product asic need longer develop cycl fpga fpga solut greater flexibl shorten develop cycl asic high risk becaus fpga flexibl system use programm devic easili upgrad correct field onc asic ha problem piec discard cost design support tool fpga solut also save lot money design support tool asic use larg project fpga suitabl small project need quickli place market support remot upgrad fpga acceler technolog fpga field programm gate array product develop base ruihui mu et al review deep learn research programm devic pal gal cpld one field field applic specif integr circuit asic fpga circuit field asic onli solv shortcom custom circuit also overcom origin programm devic gate highlight programm bring great conveni design implement also provid viabl solut reduc design cost slower rate asic process emerg circuit onli solv shortcom custom circuit also overcom shortcom limit number origin programm devic gate fpga achiev gpu asic good balanc process speed control one hand fpga programm reconfigur hardwar ha power control gpu hand ha design space increas gate resourc memori bandwidth conveni fpga also elimin process requir asic solut one disadvantag fpga requir user program hardwar descript languag howev technolog compani research institut develop languag easier use compil develop impuls acceler technolog make fpga yale develop lua script languag tool shorten user develop time limit certain extent make research easier problem prospect deep learn bring dawn research artifici intellig design complex neural network model use huge train data consum larg amount comput resourc train final learn abil extract abstract featur data due complex process deep neural network algorithm number iter high comput complex challeng bottleneck parallel deep neural network propos direct worth explor futur parallel develop deep neural network algorithm optim origin deep learn algorithm design process main focu algorithm accuraci without consid parallel key factor therefor current deep learn algorithm fulli util comput capabl order better util potenti platform design algorithm addit train precis algorithm parallel algorithm taken consider model compress techniqu model compress divid four categori one call prune neural network mainli connect node one layer one layer side ha weight prune mean find weight side small edg may matter edg ksii transact internet inform system vol apr remov train larg model see edg smaller weight remov edg retrain model reserv edg anoth way compress model weight share assum full connect two adjac layer layer ha thousand node one thousand one thousand two layer one million weight paramet dual learn dual learn attempt appli dual attribut thi structur deep learn thi dual process obtain feedback inform unlabel data know whether model work well train updat accord feedback inform revers model achiev goal learn unlabel data broadband limit comput power embed system deep learn requir lot data use ddr transfer layer weight convolut full connect data come ddr data transmiss extrem larg case also use float point precis mani case network use process multipl roi larg electr machin perform task embed platform set strict limit achiev low power minim scale embed solut use small amount data limit memori size usual run integ precis opposit float point untag data featur learn present featur learn tag data still domin massiv unmark data real world obvious unrealist add artifici tag unmark data one one therefor develop data set storag technolog attent paid featur learn unmark data research automat label unmark data super paramet optim hyper paramet neural network paramet train learn process paramet huge impact neural network differ thousand mile hyper parametr tune import task deep learn perform network like depend nuanc hyper paramet experi deep learn neural network unexplain natur deep learn deep learn know paramet model input data connect network still know implement thi process deep learn like black box nerv ha veri magic function know work interpret deep learn hinder abstract summar thi technolog limit research develop cognit intellig time oper neural network abstract human ca realli verifi whether work process reason unsupervis featur learn supervis method learn classif model train data appli classif model classif unknown data test ruihui mu et al review deep learn research data therefor qualiti classif model strongli depend quantiti qualiti train data set good train data set obtain good classif model better classif result obtain unknown data current still rel train train data set avail may limit develop deep learn therefor necessari obtain larger label data set make possibl deepen deep learn conclus import research direct machin learn deep learn bring dawn research artifici intellig view rapid chang deep learn systemat introduc latest progress deep learn firstli introduc sever commonli use neural network model deep learn analyz two commonli use parallel train model base deep learn compar advantag disadvantag train method two model analyz commonli use deep learn open sourc framework compar applic featur sever industri research platform final focus current research neural network hardwar acceler futur develop deep learn technolog still full differ opportun challeng highli promis acknowledg thi research wa partial support nation key research develop plan key project china grant nation natur scienc foundat china grant natur scienc foundat colleg univers jiangsu provinc china thank anonym refere help comment suggest initi version thi paper refer marz n warren j big data principl best practic scalabl real time data system greenwich usa man public co gantz j reinsel digit univers big data bigger digit shadow biggest growth far east idc iview idc analyz futur articl crossref link lecun bengio hinton g deep learn natur articl crossref link ksii transact internet inform system vol apr peng zhu w zhao analysi reason advanc direct frontier inform technolog electron engin articl crossref link dong loy tang x imag use deep convolut network ieee transact pattern analysi machin intellig pp articl crossref link denker j boser b lecun backpropag appli handwritten zip code recognit neural comput articl crossref link ren girshick sun j faster toward object detect region propos network proc advanc neural inform process system montreal canada articl crossref link wang liu yang han huang deep network imag spars prior proc intern confer comput vision santiago chile articl crossref link yu k yang j gong linear spatial pyramid match use spars code imag classif proc ieee comput vision pattern recognit cvpr miami fl usa pp articl crossref link hinton g e salakhutdinov r r reduc dimension data neural network scienc articl crossref link xia pan lai liu yan supervis hash imag retriev via imag represent learn proc associ advanc artifici intellig qu√©bec citi canada articl crossref link mao q dong huang z learn salient featur speech emot recognit use convolut neural network ieee transact multimedia articl crossref link zen h senior schuster statist parametr speech synthesi use deep neural network proc ieee intern confer acoust speech signal process nj ieee may articl crossref link felix w j√ºrgen g martin w featur enhanc deep lstm network asr reverber multisourc environ comput speech languag articl crossref link moham r dahl g e hinton g acoust model use deep belief network ieee transact audio speech languag process articl crossref link cho k van merrienbo b gulcehr c learn phrase represent use rnn encod decod statist machin translat corr articl crossref link rousseau attik schwenk h larg prune continu space languag model gpu statist machin translat proc workshop ruihui mu et al review deep learn research ever realli replac model futur languag model hlt associ comput linguist june articl crossref link karlen collobert r weston j natur languag process almost scratch journal machin learn research articl crossref link deora kombrink smikolov empir evalu combin advanc languag model techniqu proc confer intern speech commun associ vincent p janvin c bengio neural probabilist languag model journal machin learn research bahdanau cho k bengio neural machin translat jointli learn align translat corr articl crossref link kumar irsoy ondruska p ask anyth dynam memori network natur languag process proc intern confer machin learn new york nj usa acm articl crossref link weiss alberti c collin structur train neural network pars eprint arxiv articl crossref link lampl g ballestero subramanian neural architectur name entiti recognit eprint arxiv articl crossref link l lee k lewi deep semant role label work next proc annual meet associ comput linguist new york nj usa acm articl crossref link severyn moschitti twitter sentiment analysi deep convolut neural network proc research develop inform retriev new york nj usa acm articl crossref link x sun niu f f deep learn model enhanc emot semant microblog sentiment analysi chines journal comput gehr j auli grangier convolut sequenc sequenc learn eprint arxiv articl crossref link vinyal fortunato jaitli n pointer network proc neural inform process system cambridg usa mit press articl crossref link zhou x dong wu h respons select convers proc empir method natur languag process new york nj usa acm articl crossref link rumelhart william r hinton g learn represent error natur pp articl crossref link bourlard h kamp perceptron singular valu decomposit biolog cybernet articl crossref link ksii transact internet inform system vol apr mikolov le q v distribut represent sentenc document eprint crossref link kim convolut neural network sentenc classif eprint articl crossref link vincent p larochel h bengio extract compos robust featur denois autoencod proc intern confer articl crossref link sejnowski j hinton g e learn relearn boltzmann machin parallel distribut process explor microstructur cognit mit press articl crossref link le q v ngiam j coat optim method deep learn proc intern confer machin learn taylor g w fergu r zeiler adapt deconvolut network mid featur learn proc ieee intern confer comput vision articl crossref link rifai vincent p muller x contract explicit invari dure featur extract proc intern confer machin learn articl crossref link zhang j kan chen x network face align proc european confer comput vision springer intern publish articl crossref link hinton g e train product expert minim contrast diverg neural comput articl crossref link hinton g e practic guid train restrict boltzmann machin momentum articl crossref link krizhevski hinton g e use veri deep autoencod imag retrieva proc esann articl crossref link zou w ng yu k unsupervis learn visual invari tempor coher proc workshop deep learn unsupervis featur learn articl crossref link krizhevski sutskev hinton g e imagenet classif deep convolut neural network commun acm articl crossref link hinton g e ranzato model pixel mean covari use factor boltzmann machin proc comput vision pattern recognit confer articl crossref link glorot x bengio understand difficulti train deep feedforward neural network journal machin learn research jan articl crossref link ruihui mu et al review deep learn research courvil bergstra j bengio spike slab restrict boltzmann machin proc intern confer artifici intellig statist articl crossref link memisev r hinton g unsupervis learn imag transform proc ieee confer comput vision pattern recognit articl crossref link hinton g e osindero teh w fast learn algorithm deep belief net neural comput articl crossref link wang x wang improv hybrid music recommend use deep learn proc acm intern confer multimedia orlando usa articl crossref link bengio learn deep architectur ai foundat machin learn articl crossref link goodfellow ian j jean mirza mehdi gener adversari network eprint articl crossref link karpathi li f f deep align gener imag descript ieee tran pattern analysi machin intellig articl crossref link ruihui mu receiv hi degre comput technolog huazhong univers scienc technolog current student hohai univers associ professor henan xinxiang univers hi research interest includ deep learn neural network person recommend system xiaoqin zeng receiv hi degre comput softwar nanj univers degre comput applic southeast univers china hi degre comput scienc hong kong polytechn univers professor hohai univers doctor supervisor director institut intellig scienc technolog hi research interest focu machin learn comput intellig neural network