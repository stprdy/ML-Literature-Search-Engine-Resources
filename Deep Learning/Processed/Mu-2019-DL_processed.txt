ksii transaction internet information system vol 13 no 4 apr 2019 1738 review deep learning research ruihui xiaoqin computer information hohai university nanjing 210098 muruihui xzeng computer information engineering xinxiang university xinxiang 453000 corresponding author ruihui mu received july 5 2018 revised september 15 2018 accepted october 19 2018 published april 30 2019 abstract advent big data deep learning technology ha become important research direction field machine learning ha widely applied image processing natural language processing speech recognition online advertising paper introduces deep learning technique various aspect including common model deep learning optimization method commonly used open source framework existing problem future research direction firstly introduce application deep learning secondly introduce several common model deep learning optimization method thirdly describe several common framework platform deep learning finally introduce latest acceleration technology deep learning highlight future work deep learning keywords deep learning machine learning artificial intelligence learning network optimization method issn ksii transaction internet information system vol 13 no 4 apr 2019 1739 introduction rapid development technology cloud computing big data internet thing emergence various application internet space ha led explosive growth data scale 1 according report international data corporation idc 2012 global total data expected 22 time 2011 reaching 2020 2 big data contains rich value great potential bring transformation development human society also brings serious problem information overload quickly efficiently obtain valuable information various complex data ha become key challenge recent year deep learning ha made breakthrough image processing natural language understanding speech recognition 3 view rapid development deep learning technology deep learning map different data hidden space performing automatic feature learning heterogeneous data obtains unified data representation 4 paper review deep learning technique various aspect including common model deep learning optimization method commonly used framework existing problem future research direction remainder paper organized follows section 2 review related application deep learning section 3 address common model deep learning section 4 describe optimization method section 5 introduce commonly used open source framework platform deep learning section 6 present acceleration technology deep learning section 7 describes problem prospect deep learning section 8 concludes work outline potential future work related application image processing image recognition earliest application deep learning 2014 dong et al 5 first proposed use deep convolutional neural network learn mapping relationship image image image recognition denker 6 used convolutional neural network recognize handwritten digital achieved best result time ren et al 7 proposed faster object detection method using deep convolutional neural network 2015 wang et al 8 introduced sparse prior deep convolutional neural network image recognition yu et al 9 used autoencoder classify image trained support vector machine image classification accuracy deep learning exceeded 97 2016 imagenet 1740 ruihui mu et al review deep learning research competition 10 xia et al 11 proposed supervised depth hashing algorithm called cnnh convolutional neural network hashing image recognition speech recognition recent year deep learning technology ha applied field speech recognition field speech recognition baidu hkust sogou announced accuracy chinese speech recognition based deep learning exceeded 97 end microsoft research speech recognition based deep neural network ha completely changed original technical framework speech recognition deep neural network model ha brought tremendous innovation improve accuracy speech recognition present speech recognition algorithm used internet company baidu hkust sogou adopt deep neural network model 12 applied convolutional neural network cnn speech feature extraction et al 13 proposed speech synthesis model based multilayer perceptron speech recognition 14 used lstm method extract speech feature greatly improves efficiency feature 2012 hinton replaced gaussian mixture model gmm traditional model dbn result showed error rate dropped timit core test set wa significantly improved 15 recently speech recognition system feedforward sequential memory network proposed google us large number convolutional layer directly model entire sentence speech signal better express relevance speech baidu applied deep convolutional neural network speech recognition research using visual geometry group network combination deep convolutional neural network recognition error rate ha dropped greatly natural language processing natural language processing also application deep learning cho et al 16 proposed vector constant length representation model based recurrent neural network rnn machine translation artificial neural network attracted attention natural language processing rousseau et al 17 used similar model statistical machine translation task evaluated bilingual evaluation understudy rating mechanism karlen 18 adopted embedding convolutional structure typical natural language processing issue semantic role labeling deoras et al 19 studied neural network model found performance improved adding multiple recursive layer vincent et al 20 proposed use embedding method map word vector representation space use nonlinear neural network represent language model bahdanau et al 21 put forward rnn search model machine translation ksii transaction internet information system vol 13 no 4 apr 2019 1741 deep learning technique widely used various task field natural language processing including tagging 22 dependency grammar analysis 23 naming body recognition 24 semantic role tagging 25 twitter sentiment analysis 26 chinese microblog sentiment analysis 27 machine translation 28 question answering 29 dialogue system 30 etc common model deep learning autoencoder autoencoder unsupervised learning algorithm applies backpropagation setting target value equal input 1986 rumelhart et al 31 proposed concept autoencoder ae used complex data processing autoencoder extract latent feature reconstructing input data form output data basic structure autoencoder viewed neural network input layer x hidden layer h output layer output layer input layer ha scale fig 1 schematic diagram autoencoder fig schematic diagram autoencoder model autoencoders three layer input layer output layer neuron middle layer output layer reconstructs input data training network similarity input data output data possible training error represents similarity assuming input signal x input layer reach hidden layer signal change z expressed following formula ùëî ùëè 2 g nonlinear function commonly used sigmoid function rectified linear unit function also called active function w link weight input layer hidden layer b bias hidden layer signal decoded 1742 ruihui mu et al review deep learning research decoding layer output output layer signal becomes z follows ùëì 2 f nonlinear function sigmoid function link weight hidden layer output layer b bias output layer treated prediction general weight matrix limited transpose weight matrix w ùëäùëá according different form data reconstruction error usually defined mean square error cross entropy some researcher introduced idea sparse representation proposed sparse autoencoder average value output signal penalty encouragement small mean value approximated gaussian distribution 32 autoencoder wa trained complete task locating key point face order enhance generalization autoencoder mikolov et al 33 proposed denoising autoencoder added artificial noise training sample allow network reconstruct original clean input noisy signal 34 deep autoencoder used obtain compact image description image retrieval many researcher successfully applied deep autoencoder image feature representation autoencoders reconstruct input data training network tuning parameter cascading form deep neural network restricted boltzmann machine hinton sejnowski 35 proposed boltzmann machine bm boltzmann machine random neural network belonging type feedback neural network boltzmann machine consists some visible unit visible variable data sample some hidden unit hidden variable visible unit connected hidden unit visible variable hidden variable binary variable state 0 1 0 represents neuron suppressed state 1 represents neuron active state sejnowski et al 36 proposed restricted boltzmann machine rbm fig 2 schematic diagram rbm includes visible layer v hidden layer input training data visible layer hidden layer detects feature input data neuron disconnected layer fully connected two layer equation 3 represents joint probability distribution two layer ùëÉ ùë£ ‚Ñé 1 ‡µØ 3 z represents normalized function b denote bias visible layer hidden layer respectively w denotes connection weight two layer equation 4 goal optimization arg ùëÉ ùë£ ‚Ñé 4 ksii transaction internet information system vol 13 no 4 apr 2019 1743 fig restricted boltzmann machine training restricted boltzmann machine faster autoencoder 37 proposed efficient optimization algorithm based stochastic gradient descent method traditional training method rbm requires large number sampling step make training efficiency rbm still not high contrastive divergence proposed hinton solved problem taylor et al 38 construct deep deconvolution network concatenating multiple convolutional sparse autoencoder maximum pooling layer learn hierarchical structure feature bottom layer top layer directly global image some researcher put forward many expansion model based restricted boltzmann machine 39 proposed discriminative learning incorporated rbm generative learning algorithm better applied discriminative task classification 40 rbm model directly cascaded structure proposed deep boltzmann machine hinton et al 41 proposed deep sparse autoencoder model learning latent feature image pixel block restricted boltzmann machine cascaded form deep neural network use training method optimization hinton et al 42 extended deep belief network convolution operation model learn potential feature representation directly original image except deep structure hierarchical generation model 43 directed sigmoid belief network wa cascaded rbm construct deep belief network 44 introducing gaussian kernel restricted boltzmann machine support continuous variable input signal restricted boltzmann machine extended solve complex problem modifying structure rbm probability distribution model usually defined complex energy function reduced efficiency learning inference convolutional neural network convolutional neural network cnns convolutional layer pooling layer layer except input layer output layer cnns reduce complexity parameter sharing weight promoted generalization ability neural network reduce neuron pooling operation robust recent year 1744 ruihui mu et al review deep learning research cnns 45 applied processing data image understanding fig 3 show schematic diagram cnn fig convolutional neural network inputting image cnn image directly input network thereby avoiding complex feature extraction data reconstruction process traditional image processing algorithm convolutional neural network necessary learn set filtering template convolution operation feature map x obtain feature map convolutional layer feature map previous layer convoluted convolution kernel output convolution result activation function form neuron next layer feature map thereby forming next layer corresponding certain feature map feature convolutional layer contains three operation convolution nonlinear activation function maximum pooling convolution using different convolution kernel extract different feature previous layer feature map subsampling layer pooling layer usually follows convolutional layer feature map according certain subsampling rule two main function layer dimensionality reduction feature map b maintaining scale invariant characteristic feature certain extent operation feature map first divided number spatial region using uniform grid region may overlapping portion average maximum value image region taken output existing research work proved performance maximum pooling operation better average pooling image feature extraction multiple convolutional subsampling layer alternately transmitted convolutional neural network relies fully connected network classify extracted feature obtain probability distribution training goal convolutional neural network minimize loss function network common loss function ha mean squared error mse function negative log likelihood nll function etc ksii transaction internet information system vol 13 no 4 apr 2019 1745 researcher proposed many some simple effective technique momentum method weight decay data enhancement hinton et al 46 extended idea training full connection layer subset set 0 iteration selected randomly connection weight network network update different network structure enhances generalization model due large number parameter convolutional neural network easy overfit affect final test performance glorot et al 47 proposed dropout optimization technique prevent overfitting randomly ignoring half feature point training iteration however dropout performance improvement convolutional neural network not obvious main reason convolutional neural network greatly reduce number training parameter compared fully connected network due weight sharing characteristic convolution kernel avoid serious therefore dropout method acting fully connected layer not ideal effect convolutional neural network whole recurrent neural network neuron fully connected network cnn disconnected layer fully connected different layer layer processed signal independent propagated next layer type network not work well sequential data rnns produced output comprised new input latent vector rnns three layer input layer hidden layer output layer theory rnns work well sequential data complexity network simple current data only dependent previous data fig 4 show schematic diagram rnn neuron connected hidden layer rnns remain former information fig rnn expands time x represent output information input information respectively h denotes hidden unit w u v represent weight denotes time input time previous state time determine output hidden unit time according different domain rnns different variant lstm long 1746 ruihui mu et al review deep learning research memory network example bidirectional rnns echo state network 1 long memory model lstm 48 lstm network variant rnn lstm work well data rnns only memory due disappearance gradient rnn designed process entire time series information deeply remembered signal last input signal intensity signal affected previous signal getting lower lower lstm avoid disappearance gradient some extent controlling gate long memory short memory lstm different rnn lstm determine information useful cell cell includes forget gate except input gate output gate input message lstm determine information retain forget according whether not match certification algorithm shown fig 5 network structure lstm input previous layer act output path introduction gate make network focusing effect lstm generally included input gate forget gate output gate input gate supplement latest input current input state forgotten part output gate based latest state ct previous moment output current input xt determine output ht moment forget gate make recurrent neural network forget information wa not used lstm naturally remember input long long time storage unit cell special unit act like accumulator gated leaky neuron unit ha direct connection previous state next state replicate current state accumulate external signal due presence forget gate lstm learn decide clear content memory unit fig schematic diagram lstm cell 2 bidirectional rnns bidirectional rnns 49 relatively simple rnns composed two rnns superimposed top state two hidden layer determine output brnns structure shown fig 6 rnn current time usually take account information previous moment without ksii transaction internet information system vol 13 no 4 apr 2019 1747 considering following information bidirectional rnns overcome shortcoming introduces following consideration previous state following state determine current output fig bidirectional rnn structure 3 echo state network esns esns also type rnns different traditional rnns core structure make reserve pool generated any time ha constant control reserve pool randomly generated sparsely connected loop structure reserve pool output layer weight matrix only part need adjusted core algorithm using recurrent network random connection replace hidden layer thus simplifying process training deep neural network deep neural network dnn include input output layer multiple hidden layer dnns deal linear problem computing probability output layer layer appropriate activation function dnns usually applied image understanding speech recognition dnns essentially neural network deep neural network sometimes called perceptron mlp input feature vector transformed hidden layer reach output layer finally get classification result wa linear classification model two category mainly used linear classification classification ability wa limited early discrete transfer function some shackle multiplayer perceptron use some continuous function avoid problem tanh function sigmoid function dnns constructed adding number neuron hidden layer 1748 ruihui mu et al review deep learning research fig deep neural network structure fig 7 show structure fully connected deep neural network deep neural network divided three layer input layer hidden layer output layer first layer input layer last layer output layer middle layer hidden layer process deep neural network training firstly according requirement initialization performed set structure dnn forward performed layer transmitted layer obtain error back propagation performed according principle minimization error stochastic gradient descent method used deduct parameter determine direction descent update parameter dnns many variant applied different field successfully data set usually different performance different dnns usually ca compared deep belief network deep belief network dbn wa proposed geoffrey hinton 2006 50 generation model make whole neural network generate training data high probability training weight neuron multiple restricted boltzmann machine stacked construct deep belief network visible layer restricted boltzmann machine used hidden layer previous restricted boltzmann machine training restricted boltzmann machine simple output previous restricted boltzmann machine used training next restricted boltzmann machine structure shown fig generative model composed multiple layer nonlinear variable connection deep belief network part close visible layer multiple bayesian belief network part farthest visible layer rbm time training method dbn also obtain deep feature representation unlabeled data identify feature categorize data generate data 51 ksii transaction internet information system vol 13 no 4 apr 2019 1749 fig deep belief network dbn consists multiple layer neuron subdivided visible neuron hidden neuron visible element used accept input hidden element used extract feature therefore hidden element also individual name called feature detector top two layer dbn undirected connection connection constitutes associative memory dbn constructed rbm 52 one dimension data vector represented neuron bottom layer bottom layer denotes data vector layer dbn connected layer layer process training dbn done layer layer layer data vector used infer hidden layer hidden layer treated data vector next layer higher layer generative adversarial network goodfellow et al 53 proposed generative adversarial network idea come game different previous generation model guide training generated model discriminant model network consists two part generator g discriminator discriminator equivalent judging whether input data real data generated data generator used capture distribution training data discriminator equivalent judging whether input data real data generated data result network output represents probability input data come real data generator generate example similar sample training dataset synthetic sample transforming random noise discriminator discriminate data sample synthetic sample train discriminator predicted accurately train generator produce synthetic sample make discriminator believe real data sample structure shown fig 9 1750 ruihui mu et al review deep learning research fig generative adversarial network use framework probability describe gans better understanding defined two different marginal distribution one q x represents data marginal distribution ùëù x z ùëù ùëëz denotes model marginal distribution equation 5 defines function generator g discriminator min ùê∫ max ùê∑ ùëâ ùê∑ ùê∫ ùê∏ùëû x x ùê∏ùëù z ùê∫ z ‡µØ‡µß ‡∂±ùëû x x x represents real sample z represents noise input generator g g z represents sample generated generator g represents probability discriminator determines whether sample true training process goal generator g generate real sample much possible deceive discriminator goal discriminator separate sample real sample thus generator g discriminator constitute dynamic adversarial game minimize ùê∑ x ùëû x ùëû x ùëù x equivalent divergence data marginal distribution model marginal distribution also minimized training continues generator g generates sample similar real sample fusion neural network fusion train multiple model integrate model according certain method development deep learning research shown single deep learning model always bring best result however method increasing depth model often ha certain limitation gradient disappearance computational complexity model parallelism model fusion based deep learning may bring better result excellent neural network model appearing recent generation spiking neural network deep residual network echo state network esn attention neural network etc karpathy et al 54 combined cnn rnn automatic generation image description enabling combined model generate textual description based feature image ksii transaction internet information system vol 13 no 4 apr 2019 1751 generate corresponding image based text according relationship individual learner divided two category strong dependency individual learner serialization method must generated serially representing boosting method no strong dependency individual learner parallelization generated simultaneously method representative bagging random forest optimization method deep learning model two way parallelize deep neural network model parallelism data parallelism section mainly introduces commonly used optimization method data parallelism data parallelism refers segmentation training data multiple model used train multiple piece data simultaneously data parallelized distributed training store backup model worker node processing different part data set machine data parallelization training method need combine result working node synchronize model parameter node basic architecture data parallelism shown fig parameter server responsible updating latest model parameter fig basic architecture data parallelism data parallelism divided synchronous mode asynchronous mode synchronous mode one batch training data trained time training program synchronization exchange parameter time parameter exchange completed training program common new model starting point next batch trained synchronous update process data parallel distributed gradient descent advantage convergence model 1752 ruihui mu et al review deep learning research relatively stable mode us large batch larger batch closer effect batch gradient overall gradient however disadvantage synchronous mode information transmission overhead large ha short board effect asynchronous mode training program completes batch training data immediately exchange parameter parameter server regardless state training program latest result training program asynchronous mode not immediately reflected training program perform next parameter exchange advantage data parallelism easier implement easier extend only w communication required data model doe not need communicate disadvantage model large gpu memory not meet storage requirement not complete calculation model parallelism model parallelism also good method large model ca accommodated memory fig 11 show basic framework model parallelism different data parallelism model parallelism split model several shard held several training unit work together complete training communication overhead occurs input one neuron come output neuron another training unit case communication overhead synchronization consumption brought model parallelism exceeds data parallelism acceleration le data parallelism fig basic architecture model parallelism data parallelism model parallelism not extended indefinitely many data parallel training program learning rate ha reduced ensure smoothness training process many slice parallel mode exchange amount neuron output value increase sharply efficiency drop drastically therefore simultaneous model parallelism data parallelism also ksii transaction internet information system vol 13 no 4 apr 2019 1753 common solution common framework platform deep learning common framework industry academia introduced related open source framework use similar method parallel learning deep learning model library used cpu train network using data parallelism model parallelism dnn library supported gpu speed network implementation distributed parallel deep learning model data parallelism model parallelism used commonly used software tool follows 1 theano theano born montreal 2008 ha spawned number deep learning python package notably block theano python library defining optimizing computing mathematical expression efficient computation multidimensional array advantage integrated numpy use gpu accelerate calculation efficient symbol differentiation speed stability optimization dynamically generate c code extensive unit testing disadvantage transfer limit bad parameter scan immutable mechanism cause function compile take long theano lack flexible polymorphic mechanism defining function difficult debugging method 2 torch core computational unit well optimized using c common model built using lua core feature powerful array many implementation indexing slicing transposing routine via lua linear algebra routine neural network model numerical optimization routine efficient gpu support embeddable portable io android fpga backend 3 mxnet mxnet combine advantage imperative declarative programming optimize system facilitate debugging resource computational scheduling memory allocation resource management data representation computational optimization etc worth learning native support distributed training mxnet provides imperative tensor calculation bridge main language symbolic expression hand nd array seamlessly interface symbolic expression mxnet provides distributed store data exchange mainly ha two function push push pair device store pull pull value key store 4 cntk cntk unified deep learning tool describes neural network series computational step directed graph directed graph leaf node represent input layer network parameter node represented matrix operation input layer easy implement combine today popular model cntk feedforward neural network dnns convolutional neural network cnns recurrent neural network gradient automatically calculated 1754 ruihui mu et al review deep learning research implementing stochastic gradient descent learning parallel computing implemented multiple gpus server biggest advantage cntk parallel multiple gpus server another advantage cntk support microsoft window open source tool written 5 caffe caffe learning framework deep convolutional neural network convenient train test cnn model using caffe caffe pure architecture support command line python matlab interface seamlessly switched cpu gpu caffe suitable rapid development engineering application caffe officially provides large number example according example caffe only asks write proto txt training process gradient descent algorithm packaged understand syntax proto txt basically construct neural network 6 tensorflow tensorflow google artificial intelligence learning system based distbelief tensor mean array flow mean calculation based data flow graph tensorflow process tensor flow one end image tensorflow system transmits complex data structure artificial intelligence neural network analysis processing table 1 show several commonly used open resource framework seen many open resource framework based deep learning also many corresponding programming language table 2 show performance comparison common open resource framework dimension future believe efficient programming language platform may also emerge table comparison common framework framework support hardware operating language parallel mode theano cpu gpu data parallelism torch cpu gpu pga data parallelism mxnet cpu gpu data parallelism model parallelism cntk cpu gpu data parallelism caffe cpu gpu data parallelism tensorflow cpu gpu data parallelism model parallelism ksii transaction internet information system vol 13 no 4 apr 2019 1755 table performance comparison common framework industry platform 1 mariana tencent deep learning platform ha evolved three framework including dnn gpu data parallel framework cnn gpu model parallel data parallel framework dnn cpu cluster model parallel data parallel framework data parallelism model parallelism mariana solves problem deep learning becomes effective help deep learning research mariana effectively support large model model parallelism mariana ha done lot work enhance ease use simplify deep learning experiment greatly save algorithm development time mariana dnn gpu data parallel framework wechat speech recognition application complete dnn model training billion training sample day model error rate reduced 10 mariana cnn model parallel data parallel framework imagenet image classification issue achieved 87 accuracy imagenet 2012 dataset present tencent deep learning platform mariana ha supported voice input method wechat voice recognition voice open platform long press voice message text product began apply wechat image recognition addition field advertising recommendation personalized recommendation also actively exploring experimenting 2 xdl ali mama deep learning framework based alibaba massive scale business scenario practice new generation distributed deep learning framework designed developed xdl ha created four key paradigm new data paradigm new model paradigm new capability paradigm new architectural paradigm make truly xdl achieves approximate linear computational speedup thousand node concurrently accommodate hundred billion sparse parameter training online streaming training ability asynchronous pipeline maximize hardware saturation present xdl ha widely used many business scenario ali mama creating revenue 10 billion scale time thanks xdl new architectural capability series model algorithm framework model design interface deploy performance architecture design overall rating tensorflow 80 80 90 90 100 88 caffe 60 60 90 80 70 72 torch 90 70 60 70 90 76 mxnet 70 100 80 80 90 84 cntk 50 50 70 100 60 66 theano 80 70 40 50 50 58 1756 ruihui mu et al review deep learning research effectively trained xdl platform including any deep learning search model based tree structure user behavior image 3 paddle paddle open source deep learning framework provided baidu new generation deep learning framework based deep learning programming language ha greatly improved ability framework express model describe any potential possibility friendly computing lot computing business baidu paddle excellent distributed computing technology save lot computing resource also support sparse model training provides visual deep learning visual dl help developer easily observe overall trend training data sample quality intermediate result parameter distribution trend structure model helping developer complete programming process conveniently acceleration technology deep learning present deep learning acceleration technology accelerated dedicated accelerator fpga ha advantage low power consumption high speed main deep learning acceleration technique follows cpu acceleration technology designing dedicated cpu processor processor designed algorithm algorithm family ha characteristic low power consumption high speed high development cost order design dedicated processor make analysis algorithm get feature based feature algorithm design dedicated circuit however due large amount computation high degree parallelism deep learning cpu increasingly difficult adapt computing need only utilize cpu researcher optimize use register based window movement feature convolution pooling algorithm finally accelerate convolutional neural network algorithm accelerating convolution pooling calculation researcher design hardware neuron neuron structure map connection neuron latch memory finally accelerate neural network algorithm mapping neuron neural network hardware neuron design different storage unit according feature algorithm memory allocation type data different storage unit different finally utilization computing unit improved pipeline lower general processor gpu acceleration technology gpu graphic processing unit concept relative cpu wa proposed rapid growth multimedia computing eventually became processor independent ksii transaction internet information system vol 13 no 4 apr 2019 1757 traditional cpu today gpus provide ten time even hundred time performance cpu term computation operation parallel computing compared cpu gpu ha following advantage powerful computing power gpu ha powerful computing power period theoretical peak capacity gpu calculation one order magnitude higher cpu price relatively cheap individual small organization user obtain computing traditional cpu computer system used expensive requirement site power supply also high use gpus train deep neural network achieve highly efficient parallel computing capability massive amount training data time spent drastically reduced fewer server occupied asic acceleration technology asic application specific integrated circuit efficient hardware acceleration method gpu one hand high degree customization increase computing power limit mobility hand asics expensive difficult individual small organization combining two aspect researcher deep learning currently rarely consider using asics computer acceleration asic integrated circuit designed specific purpose refers integrated circuit designed manufactured meet need specific user need specific electronic system asic characterized need specific user asic divided full custom highlight dedicated tailored execution speed faster faster fpga process fpga save some unused logic implementation fpga cost lower fpga production characteristic asic need specific user asic ha advantage smaller size lower power consumption higher reliability improved performance enhanced confidentiality lower cost compared integrated circuit mass production asic need longer development cycle fpgas fpga solution greater flexibility shorten development cycle asic high risk fpgas flexibility system using programmable device easily upgraded corrected field asic ha problem piece discarded cost designing supporting tool fpga solution also save lot money design support tool asics used large project fpgas suitable small project need quickly placed market support remote upgrade fpga acceleration technology fpga field programmable gate array product development based 1758 ruihui mu et al review deep learning research programmable device pal gal cpld one field field application specific integrated circuit asic fpga circuit field asic not only solves shortcoming custom circuit also overcomes original programmable device gate highlight programmability brings great convenience design implementation also provides viable solution reducing design cost slower rate asic process emergence circuit not only solves shortcoming custom circuit also overcomes shortcoming limited number original programmable device gate fpgas achieved gpus asics good balance processing speed control one hand fpga programmable reconfigurable hardware ha powerful controllability gpu hand ha design space increasing gate resource memory bandwidth conveniently fpga also eliminates process required asic solution one disadvantage fpgas require user program hardware description language however technology company research institute developed language easier use compiler developed impulse accelerated technology making fpga yale developed lua scripting language tool shortened user development time limit certain extent making research easier problem prospect deep learning brings dawn research artificial intelligence designing complex neural network model using huge training data consuming large amount computing resource train finally learning ability extract abstract feature data due complex process deep neural network algorithm number iteration high computational complexity some challenge bottleneck parallelization deep neural network propose some direction worth exploring future parallel development deep neural network 1 algorithm optimization original deep learning algorithm design process main focus algorithm accuracy without considering parallelism key factor therefore current deep learning algorithm not fully utilize computing capability order better utilize potential platform designing algorithm addition training precision algorithm parallelism algorithm taken consideration 2 model compression technique model compression divided four category one called pruning neural network mainly connected node one layer one layer side ha some weight pruning mean find weight some side small edge may not matter edge ksii transaction internet information system vol 13 no 4 apr 2019 1759 removed training large model see edge smaller weight remove edge retrain model reserved edge another way compress model weight sharing assuming full connection two adjacent layer layer ha thousand node one thousand one thousand two layer one million weight parameter 3 dual learning dual learning attempt apply dual attribute structure deep learning dual process obtain feedback information unlabeled data know whether model work well not train update according feedback information reverse model achieve goal learning unlabeled data 4 broadband limitation computing power embedded system deep learning requires lot data us ddr transfer layer weight convolution full connection data come ddr data transmission extremely large case also use floating point precision many case network used process multiple roi large electrical machine perform task embedded platform set strict limit achieve low power minimal scale embedded solution use small amount data limiting memory size usually running integer precision opposite floating point 5 untagged data feature learning present feature learning tagged data still dominates massive unmarked data real world obviously unrealistic add artificial tag unmarked data one one therefore development data set storage technology attention paid feature learning unmarked data research automatic labeling unmarked data 6 super parameter optimization hyper parameter neural network parameter training learning process parameter huge impact neural network difference thousand mile hyper parametric tuning important task deep learning performance network likely depend nuance hyper parameter experience deep learning neural network 7 unexplainable nature deep learning deep learning know parameter model input data connect network still know implement process deep learning like black box nerve ha magical function know work interpretability deep learning hinders abstraction summarization technology limit research development cognitive intelligence time operation neural network abstract human ca really verify whether working process reasonable 8 unsupervised feature learning supervised method learns classification model training data applies classification model classification unknown data test 1760 ruihui mu et al review deep learning research data therefore quality classification model strongly dependent quantity quality training data set good training data set obtain good classification model better classification result obtained unknown data currently still relatively trained training data set available may limit development deep learning therefore necessary obtain larger labeled data set making possible deepen deep learning conclusion important research direction machine learning deep learning brings dawn research artificial intelligence view rapid change deep learning systematically introduced latest progress deep learning firstly introduced several commonly used neural network model deep learning analyzed two commonly used parallel training model based deep learning compared advantage disadvantage training method two model analyzed commonly used deep learning open source framework compared application feature several industrial research platform finally focused current research neural network hardware accelerator future development deep learning technology still full different opportunity challenge highly promising acknowledgement research wa partially supported national key research development plan key project china grant national natural science foundation china grant no 60971088 60571048 61432008 61375121 natural science foundation college university jiangsu province china thank anonymous referee helpful comment suggestion initial version paper reference 1 marz n warren j big data principle best practice scalable real time data system greenwich usa manning publication co 2015 2 gantz j reinsel digital universe 2020 big data bigger digital shadow biggest growth far east idc iview idc analyze future article crossref link 3 lecun bengio hinton g deep learning nature article crossref link ksii transaction internet information system vol 13 no 4 apr 2019 1761 4 peng zhu w zhao analysis reasoning advance direction frontier information technology electronic engineering article crossref link 5 dong loy tang x image using deep convolutional network ieee transaction pattern analysis machine intelligence pp article crossref link 6 denker j boser b lecun backpropagation applied handwritten zip code recognition neural computation article crossref link 7 ren girshick sun j faster towards object detection region proposal network proc advance neural information processing system montreal canada article crossref link 8 wang liu yang han huang deep network image sparse prior proc international conference computer vision santiago chile article crossref link 9 yu k yang j gong linear spatial pyramid matching using sparse coding image classification proc 2009 ieee computer vision pattern recognition cvpr miami fl usa pp article crossref link 10 hinton g e salakhutdinov r r reducing dimensionality data neural network science 5786 article crossref link 11 xia pan lai liu yan supervised hashing image retrieval via image representation learning proc association advancement artificial intelligence qu√©bec city canada article crossref link 12 mao q dong huang z learning salient feature speech emotion recognition using convolutional neural network ieee transaction multimedia 2213 article crossref link 13 zen h senior schuster statistical parametric speech synthesis using deep neural network proc 2013 ieee international conference acoustic speech signal processing nj ieee may article crossref link 14 felix w j√ºrgen g martin w feature enhancement deep lstm network asr reverberant multisource environment computer speech language article crossref link 15 mohamed r dahl g e hinton g acoustic modeling using deep belief network ieee transaction audio speech language processing article crossref link 16 cho k van merrienboer b gulcehre c learning phrase representation using rnn encoder decoder statistical machine translation corr article crossref link 17 rousseau attik schwenk h large pruned continuous space language model gpu statistical machine translation proc 2012 workshop 1762 ruihui mu et al review deep learning research ever really replace model future language modeling hlt association computational linguistics june article crossref link 18 karlen collobert r weston j natural language processing almost scratch journal machine learning research article crossref link 19 deoras kombrink smikolov empirical evaluation combination advanced language modeling technique proc conference international speech communication association 2011 20 vincent p janvin c bengio neural probabilistic language model journal machine learning research 21 bahdanau cho k bengio neural machine translation jointly learning align translate corr article crossref link 22 kumar irsoy ondruska p ask anything dynamic memory network natural language processing proc international conference machine learning new york nj usa acm article crossref link 23 wei alberti c collins structured training neural network parsing eprint arxiv article crossref link 24 lample g ballesteros subramanian neural architecture named entity recognition eprint arxiv article crossref link 25 l lee k lewis deep semantic role labeling work next proc annual meeting association computational linguistics new york nj usa acm article crossref link 26 severyn moschitti twitter sentiment analysis deep convolutional neural network proc research development information retrieval new york nj usa acm article crossref link 27 x sun niu f f deep learning model enhanced emotion semantics microblog sentiment analysis chinese journal computer 2017 28 gehring j auli grangier convolutional sequence sequence learning eprint arxiv article crossref link 29 vinyals fortunato jaitly n pointer network proc neural information processing system cambridge usa mit press article crossref link 30 zhou x dong wu h response selection conversation proc empirical method natural language processing new york nj usa acm article crossref link 31 rumelhart williams r hinton g learning representation error nature pp article crossref link 32 bourlard h kamp perceptron singular value decomposition biological cybernetics article crossref link ksii transaction internet information system vol 13 no 4 apr 2019 1763 33 mikolov le q v distributed representation sentence document eprint crossref link 34 kim convolutional neural network sentence classification eprint article crossref link 35 vincent p larochelle h bengio extracting composing robust feature denoising autoencoders proc international conference article crossref link 36 sejnowski j hinton g e learning relearning boltzmann machine parallel distributed processing exploration microstructure cognition mit press article crossref link 37 le q v ngiam j coates optimization method deep learning proc international conference machine learning 2011 38 taylor g w fergus r zeiler adaptive deconvolutional network mid feature learning proc ieee international conference computer vision article crossref link 39 rifai vincent p muller x contractive explicit invariance feature extraction proc international conference machine learning article crossref link 40 zhang j kan chen x network face alignment proc european conference computer vision springer international publishing article crossref link 41 hinton g e training product expert minimizing contrastive divergence neural computation article crossref link 42 hinton g e practical guide training restricted boltzmann machine momentum article crossref link 43 krizhevsky hinton g e using deep autoencoders image retrieva proc esann article crossref link 44 zou w ng yu k unsupervised learning visual invariance temporal coherence proc workshop deep learning unsupervised feature learning article crossref link 45 krizhevsky sutskever hinton g e imagenet classification deep convolutional neural network communication acm article crossref link 46 hinton g e ranzato modeling pixel mean covariance using factorized boltzmann machine proc computer vision pattern recognition conference article crossref link 47 glorot x bengio understanding difficulty training deep feedforward neural network journal machine learning research jan article crossref link 1764 ruihui mu et al review deep learning research 48 courville bergstra j bengio spike slab restricted boltzmann machine proc international conference artificial intelligence statistic article crossref link 49 memisevic r hinton g unsupervised learning image transformation proc ieee conference computer vision pattern recognition article crossref link 50 hinton g e osindero teh w fast learning algorithm deep belief net neural computation article crossref link 51 wang x wang improving hybrid music recommendation using deep learning proc acm international conference multimedia orlando usa article crossref link 52 bengio learning deep architecture ai foundation machine learning article crossref link 53 goodfellow ian j jean mirza mehdi generative adversarial network eprint article crossref link 54 karpathy li f f deep alignment generating image description ieee trans pattern analysis machine intelligence article crossref link ruihui mu received degree computer technology huazhong university science technology currently student hohai university associate professor henan xinxiang university research interest include deep learning neural network personalized recommender system xiaoqin zeng received degree computer software nanjing university degree computer application southeast university china degree computer science hong kong polytechnic university professor hohai university doctoral supervisor director institute intelligent science technology research interest focus machine learning computing intelligence neural network