review deep learn comput biolog christof tanel leopold oliv abstract technolog advanc genom imag led explos molecular cellular profil data larg number sampl thi rapid increas biolog data sion acquisit rate challeng convent analysi strategi modern machin learn method deep ing promis leverag veri larg data set find hidden structur within make accur predict thi review discuss applic thi new breed analysi approach regulatori genom cellular imag provid background deep learn set success appli deriv biolog insight addit present specif applic provid tip practic use also highlight possibl pitfal limit guid comput biologist make use thi new technolog keyword cellular imag comput biolog deep learn machin learn regulatori genom doi receiv april revis june accept june mol syst biol introduct machin learn method approach learn function relationship data without need defin priori hasti et al murphi michalski et al comput biolog appeal abil deriv predict model without need strong assumpt underli mechan frequent unknown insuffici defin case point accur predict gene express level current made broad set epigenet featur use spars linear model karlic et al cheng et al random forest li et al select featur determin transcript level remain activ research topic predict genom libbrecht nobl et al proteom swan et al metabolom kell sensit compound eduati et al reli machin learn approach key ingredi applic describ within canon machin learn workflow involv four step data ing featur extract model fit tion fig customari denot one data sampl includ covari featur input x usual vector number label respons variabl output valu usual singl number avail supervis machin learn model aim learn function f x list train pair data record fig one typic applic biolog predict viabil cancer cell line expos chosen drug menden et al eduati et al input featur x would captur somat sequenc variant cell line chemic drug concentr togeth measur viabil output label use train support vector machin random forest classifi relat method function relationship f given new cell line unlabel data sampl x futur learnt function predict surviv output label calcul f x even f resembl black box inner work whi particular mutat combin influenc cell growth easili interpret regress real number classif categor class label view thi way counterpart unsupervis machin learn approach aim discov pattern data sampl x selv without need output label method ing princip compon analysi outlier detect typic exampl unsupervis model appli biolog data input x calcul raw data repres model see world choic highli specif fig deriv inform featur essenti perform process requir domain knowledg thi bottleneck especi limit dimension data even comput featur select method scale assess util vast number possibl input combin major recent advanc machin learn autom thi critic step learn suitabl represent data deep artifici neural network bengio et al lecun et al schmidhub fig briefli deep neural network take raw data lowest input layer transform increasingli abstract featur represent success combin output preced layer driven manner encapsul highli complic function european molecular biolog laboratori european bioinformat institut wellcom trust genom campu hinxton cambridg uk depart comput scienc univers tartu tartu estonia wellcom trust sanger institut wellcom trust genom campu hinxton cambridg uk correspond author tel correspond author tel author contribut equal thi work ª author publish term cc licens molecular system biolog download http march ip process box deep learn one activ field machin learn ha shown improv perform imag speech recognit hinton et al krizhevski et al grave et al zeiler fergu deng togneri natur languag understand bahdanau et al sutskev et al lipton xiong et al recent comput biolog eickholt cheng dahl et al leung et al sønderbi winther alipanahi et al wang et al zhou troyanskaya kelley et al potenti deep learn biolog clear principl allow better exploit avail increasingli larg data set dna sequenc rna measur flow cytometri autom microscopi train complex network multipl layer captur intern structur fig learn network discov featur improv perform tradit model increas interpret provid addit understand structur biolog data thi review discuss recent forthcom applic deep learn focu applic regulatori genom biolog imag analysi goal thi review wa provid comprehens background technic detail found special literatur bengio bengio et al deng schmidhub goodfellow et al instead aim provid practic pointer necessari background get start deep architectur review current softwar solut give recommend appli data applic cover deliber broad illustr differ common approach review focus specif domain found elsewher park kelli gawehn et al leung et al mamoshina et al final discuss potenti possibl pitfal deep learn contrast method tradit machin learn classic statist analysi approach deep learn regulatori genom convent approach regulatori genom relat sequenc variat chang molecular trait one approach leverag variat genet divers individu map quantit trait loci qtl thi principl ha appli identifi regulatori variant affect gene express level montgomeri et al pickrel et al dna methyl gibb et al bell et al histon mark grubert et al waszak et al proteom variat vincent et al albert et al part et al battl et al fig better statist method help increas power detect regulatori qtl kang et al stegl et al part et al rakitsch stegl howev ani map approach intrins limit variat present train popul thu studi effect rare mutat particular requir data set veri larg sampl size altern train model use variat region within genom fig split sequenc window centr trait interest give rise ten sand train exampl molecular trait even use singl individu even larg data set predict ular trait dna sequenc challeng due multipl layer x featur model result clean data featur extract discrimin featur raw data label c intron exon featur extract train evalu supervis unsupervis x linear regress logist regress random forest svm pca factor analysi cluster outlier detect b c g c g c g g c c g g c g g g g g c g c c g g c c g g c g c g c g c c g tc g g c c g ag ca c c acc g g tg g c g c c g c g c g c g c tg c ga c c g g c g c g g c c g cgg ct c g g c c tg c c c g g c g c cca g cc g ct c g c g c c g c g g c c gg c g c gg c g c c g c g c g g c c g c g g g c g c g c g c g c g g c g g c g c c g ac c g tg c g tc c g layer g c g g c c g cgg ct c g g c c tg c c g g c g c c g ac c g tg c g tc c g raw data process raw data layer intron exon tss figur machin learn represent learn classic machin learn workflow broken four step data featur extract model learn model evalu b supervis machin learn method relat input featur x output label wherea unsupervis method learn factor x without observ label c raw input data often relat correspond label complic way challeng mani classic machin learn algorithm left plot altern featur extract use deep model may abl better discrimin class right plot deep network use hierarch structur learn increasingli abstract featur represent raw data molecular system biolog ª author molecular system biolog deep learn comput biolog christof angermuel et al download http march ip abstract effect individu dna variant trait interest well depend molecular trait broad sequenc context interact distal regulatori element valu deep neural network thi context twofold first classic machin learn method oper sequenc directli thu requir featur extract sequenc base prior knowledg presenc absenc variant snv frequenc motif occurr conserv known regulatori variant structur element deep neural network help circumv manual extract featur learn data second becaus represent rich captur nonlinear depend sequenc interact effect span wider sequenc context multipl genom scale attest util deep neural network fulli appli predict splice activ leung et al xiong et al specif protein box artifici neural network artifici neural network initi inspir neural network brain mcculloch pitt farley clark rosenblatt consist layer interconnect comput unit neuron depth neural network correspond number hidden layer width maximum number neuron one layer becam possibl train network larger number hidden layer artifici neural network rebrand deep network canon configur network receiv data input layer transform nonlinear way multipl hidden layer befor final output comput output layer panel neuron hidden output layer connect neuron previou layer neuron comput weight sum input appli nonlinear activ function calcul output f x panel b popular activ function rectifi linear unit relu panel b threshold neg signal pass posit signal thi type activ function allow faster learn compar altern sigmoid tanh unit glorot et al weight w neuron free paramet captur model represent data learn sampl learn minim loss function l w measur fit model output true label sampl panel bottom thi minim challeng sinc loss function similar landscap mani hill valley panel c took sever decad befor backward propag algorithm wa first appli comput loss function gradient via chain rule deriv rumelhart et al ultim enabl effici train neural network use stochast gradient descent dure learn predict label compar true label comput loss current set model weight loss backward propag network comput ent loss function updat panel loss function l w typic optim use descent step current weight vector red dot move along direct steepest descent dw direct arrow learn rate g length vector decay learn rate time allow explor differ domain loss function jump valley begin train left side paramet smaller learn rate later stage model train learn deep neural network remain activ area research exist softwar packag tabl alreadi appli without knowledg mathemat detail involv altern architectur fulli connect feedforward network develop specif applic differ way neuron arrang includ convolut neural network wide use model imag box recurr neural network sequenti data sutskev lipton restrict boltzmann machin salakhutdinov larochel hinton autoencod hinton salakhutdinov alain et al kingma well unsupervis learn choic network architectur paramet made object way assess model perform valid data set input layer hidden layer output layer max weight sum activ function output input relu predict label true label forward propag backward propag loss local optimum global optimum b c w w f x l w l w σ w w ηδw η δw ª author molecular system biolog christof angermuel et al deep learn comput biolog molecular system biolog download http march ip alipanahi et al epigenet mark studi effect dna sequenc alter zhou troyanskaya kelley et al earli applic neural network regulatori genom first success applic neural network regulatori genom replac classic machin learn approach deep model without chang input featur exampl xiong et al consid fulli connect feedforward neural network predict splice activ individu exon model wa train use featur extract candid exon adjac intron despit rel low number train sampl combin model complex thi method achiev substanti higher predict accuraci splice activ compar simpler approach particular wa abl identifi rare mutat implic splice misregul convolut design recent work use convolut neural network cnn allow direct train dna sequenc without need defin featur alipanahi et al zhou troyanskaya angermuel et al kelley et al cnn architectur allow greatli reduc number model paramet compar fulli connect network appli convolut oper onli small region input space share paramet region key advantag result thi approach abil directli train model larger sequenc window box fig alipanahi et al consid convolut network tectur predict specif protein g c c g g ca c c acc g g tg g c g c g g c g c c g ac c g tg c g tc c g pool convolut convolut individu c g c c g c c g g c c g c c g g c g c g g c c g c c g g c c g c c g g c g c c g c c g c c g g c c g c c g g c g c c g c c g c c g individu variat anova eqtl variat locu locu locu fulli connect layer output layer convolut layer convolut layer input sequenc variant score g c g g c c g cgg ct c g g c c tg c activ normal deleteri b c g c g g c c g cgg ct c g g c c tg c e wild type individu individu mutant wild type respons g g c g c g c g c c c g c g c c c c c g c c g g g g g g g g sequenc align motif mutant respons g c g g c c g c c g g c c g c c g c c g peak g c g g c c g cgg ct c g g c c tg c g c c g ag ca c c acc g g tg g c g c g g c g c c g ac c g tg c g tc c g g c g g c c g cgg ct c g g c c tg c g c c g ag ca c c acc g g tg g c g c g g c g c c g ac c g tg c g tc c g cgc g c c g c c g g c c c g c c cgc cgc wt cgc c g figur principl use neural network predict molecular trait dna sequenc dna sequenc molecular respons variabl along genom three individu convent approach regulatori genom consid variat individu wherea deep learn allow exploit variat tile genom sequenc dna window centr individu trait result larg train data set singl sampl b convolut neural network predict molecular trait raw dna sequenc window filter first convolut layer exampl shown edg scan motif input sequenc subsequ pool reduc input dimens addit convolut layer model interact motif previou layer c respons variabl predict neural network shown b mutant sequenc use input addit neural network predict variant score allow discrimin normal deleteri variant visual convolut filter align genet sequenc maxim activ filter creat sequenc motif e mutat map sequenc window row correspond four possibl base pair substitut column sequenc posit predict impact ani sequenc chang letter top denot sequenc height nucleotid denot maximum effect across mutat figur panel adapt alipanahi et al molecular system biolog ª author molecular system biolog deep learn comput biolog christof angermuel et al download http march ip deepbind model outperform exist method wa abl recov known novel sequenc motif could quantifi effect sequenc alter identifi function snv key innov enabl train model directli raw dna sequenc wa applic tional layer intuit neuron convolut layer scan motif sequenc combin thereof similar convent posit weight matric stormo et al learn signal deeper layer inform convolut layer motif relev motif recov model visual heatmap sequenc logo fig box convolut neural network convolut neural network cnn origin inspir cognit neurosci hubel wiesel semin work cat visual cortex wa found simpl neuron respond small motif visual field complex neuron respond larger one hubel wiesel cnn design model input data form multidimension array imag three colour channel lecun et al jarrett et al krizhevski et al zeiler fergu et al szegedi et al genom sequenc one channel per nucleotid alipanahi et al wang et al zhou troyanskaya angermuel et al kelley et al high dimension data million pixel imag render train fulli connect neural network challeng number paramet model would typic exceed number train data fit circumv thi cnn make addit assumpt structur network therebi reduc effect number paramet learn convolut layer consist multipl map neuron featur map filter size equal dimens input imag panel two concept allow reduc number model paramet local connect paramet share first unlik fulli connect network neuron within featur map onli connect local patch neuron previou layer recept field second neuron within given featur map share paramet henc neuron within featur map scan featur previou layer howev differ locat differ featur map might exampl detect edg differ orient imag sequenc motif genom sequenc activ neuron obtain comput discret convolut recept field comput weight sum input neuron appli activ function panel b applic exact posit frequenc featur irrelev final predict recogn object imag use thi assumpt pool layer summar adjac neuron comput exampl maximum averag activ result smoother represent featur activ panel c appli pool oper small imag patch shift one pixel input imag effect therebi reduc number model paramet cnn typic consist multipl convolut pool layer allow learn abstract featur increas scale small edg object part final entir object one fulli connect layer follow last pool layer panel model meter number convolut layer number featur map size recept field strictli select valid data set cytoplasm cell peripheri vacuol convolut layer pool layer input imag max discret convolut max pool fulli connect layer output layer max pool discret convolut n recept field featur map b c ª author molecular system biolog christof angermuel et al deep learn comput biolog molecular system biolog download http march ip silico predict mutat effect import applic deep neural network train raw dna sequenc predict effect mutat silico assess effect sequenc chang complement method base qtl map particular help uncov regulatori effect rare snv like causal gene intuit approach visual predict regulatori effect mutat map alipanahi et al wherebi effect possibl mutat given input sequenc repres matrix view fig author could reliabl identifi deleteri snv train addit neural network predict bind score mutant sequenc fig joint predict multipl trait extens follow initi success convolut architectur extend appli rang task regulatori mic exampl zhou troyanskaya consid architectur predict chromatin mark dna sequenc author observ size input sequenc window major determin model perform larger window kb coupl multipl convolut layer enabl captur sequenc featur differ genom length scale second innov wa use neural network architectur multipl output variabl multitask neural network predict multipl chromatin state parallel multitask ture allow learn share featur output therebi improv gener perform markedli reduc comput cost model train compar learn dent model trait dahl et al similar vein kelley et al develop deep learn framework basset predict dnase hypersensit across multipl cell type quantifi effect snv matin access model improv predict manc compar convent method wa abl retriev known novel sequenc motif associ dnase hypersensit relat architectur ha also consid angermuel et al predict dna methyl state bisulphit sequenc studi angermuel et al thi approach combin convolut architectur detect inform dna sequenc motif addit featur deriv neighbour cpg site therebi account lation context recent koh pierson kundaj appli cnn genomewid chromatin immunoprecipit follow sequenc data order obtain accur preval estim differ chromatin mark koh et al present cnn among wide use architectur extract featur dna sequenc window howev altern architectur could also consid exampl recurr neural network rnn suit model sequenti data lipton appli model natur languag speech hinton et al grave et al sutskev et al che et al deng togneri xiong et al protein sequenc agathocl et al sønderbi winther clinic medic data che et al lipton et al limit extent dna sequenc xu et al lee et al rnn appeal applic regulatori mic becaus allow model sequenc variabl length captur interact within sequenc across multipl output howev present rnn difficult train cnn addit work need better understand set one prefer complementari supervis method unsupervis deep ing architectur learn featur represent unlabel data similarli classic princip compon analysi factor analysi use nonlinear model exampl approach stack autoencod vincent et al restrict boltzmann machin deep belief network hinton et al learnt featur use visual data input classic supervis learn task exampl spars autoencod appli classifi cancer case use gene express profil fakoor et al predict protein backbon lyon et al restrict boltzmann machin also use unsupervis deep network quentli train supervis model protein secondari structur spencer et al disord protein region eickholt cheng amino acid contact eickholt cheng neural network appli learn sentat protein sequenc improv protein classif asgari mofrad gener unsupervis model power approach larg quantiti unlabel data avail complex model onc train model help improv perform classif task smaller number label exampl typic avail deep learn biolog imag analysi histor perhap import success deep neural network imag analysi deep architectur train million photograph famous detect object pictur better human et al current model imag classif object detect imag retriev semant segment make use neural network convolut neural network box common network architectur imag analysi briefli cnn perform pattern match convolut aggreg pool tion box pixel level convolut oper scan imag given pattern calcul strength match everi posit pool determin presenc pattern region exampl calcul maximum pattern match smaller patch therebi aggreg region tion singl number success applic convolut pool oper core network architectur use imag analysi box first applic comput level classif earli applic deep network biolog imag focus task addit model build network output exampl ning et al appli molecular system biolog ª author molecular system biolog deep learn comput biolog christof angermuel et al download http march ip convolut neural network studi predict abnorm develop elegan embryo imag train cnn pixel patch classifi centr pixel cell wall plasm nucleu membran nucleu outsid medium use three convolut pool layer follow fulli connect output layer model predict fed base model analysi cnn outperform standard method exampl markov random field condit random field li raw data analysi task ple restor noisi neural circuitri imag jain et al ad layer allow move clear pixel nois model abstract imag featur ciresan et al use five convolut pool layer follow two fulli connect layer find mitosi breast histolog imag thi model mitosi detect challeng intern confer pattern recognit outperform competitor substanti margin approach wa also use segment neuron structur electron microscopi imag fy pixel membran ciresan et al applic cnn train manner addit wa requir obtain class probabl output new imag success pool oper lose inform local onli summari retain larger larger region avoid thi skip link ad carri inform earli layer forward deeper one current perform classif method neuron structur ronneberg et al employ architectur neuron take input lower layer local featur well overcom arbitrari choic context size analysi whole cell cell popul tissu mani case predict requir exampl xu et al directli classifi colon histopatholog imag ou find supervis featur learn deep network wa superior use handcraft featur xu et al pa part use cnn classifi segment imag patch individu yeast cell carri cent protein differ subcellular local pattern pa part deep network outperform method base tradit featur krau et al combin tion classif task singl architectur learn appli model full resolut yeast microscopi imag krau et al thi approach allow sifi entir imag without perform segment process step cnn even appli count bacteri coloni agar plate ferrari et al sinc earli nois applic pixel level field ha move toward imag analysi pipelin make use larg bioimag data set represent power cnn reus train model train convolut neural network requir larg data set biolog data acquisit expens thi doe mean deep neural network use million imag avail regardless imag sourc lower level network tend captur similar signal edg blob specif train data applic instead recur perceptu task gener thu convolut neural network reus pictur similar domain help learn even data therebi requir fewer imag model task interest inde donahu et al razavian et al show featur learn million imag classifi object success use imag retriev detect classif new domain onli hundr imag label effect approach depend similar train data new domain yosinski et al concept transfer model paramet ha also success bioimag analysi exampl zhang et al show featur learn natur imag transfer biolog data improv predict drosophila ster development stage situ hybrid imag model wa first data imagenet russakovski et al open corpu one million divers imag extract rich featur differ scale xie et al use synthet imag train cnn automat cell count microscopi imag expect network tori host model emerg biolog imag analysi effort alreadi exist gener imag process task see learn section train model could download use featur extractor fig tune adapt particular task data interpret visual convolut network convolut neural network success across mani domain interpret perform use stand featur captur visual input weight one way understand particular neuron repres look input maxim activ mathemat constraint pattern proport incom weight see also box krizhevski et al visual weight first convolut layer krizhevski et al found maxim activ pattern correspond colour blob edg differ orient filter fig gabor filter wide use featur imag analysi neural network rediscov way use compon imag model higher layer weight visual well input pixel weight difficult interpret find imag maxim neuron activ understand deeper layer term input pixel girshick et al retriev simonyan et al gener imag maxim output individu neuron fig thi approach yield explicit represent provid view type featur differenti imag larg neuron activ visual tend show featur combin edg first layer ª author molecular system biolog christof angermuel et al deep learn comput biolog molecular system biolog download http march ip therebi detect corner angl deeper layer neuron activ specif object part nose eye deepest layer detect whole object face car complic engin featur look specif nose eye face neural network learn featur sole exampl hide import imag part understand imag part import determin valu featur zeiler fergu occlud imag smaller grey box part influenti drastic chang featur valu occlud similar vein simonyan et al springenberg et al ize individu pixel make differ featur bach binder colleagu develop pixel relev vidual classif decis gener framework bach et al thi inform also use object tion segment sensit imag pixel usual correctli correspond true object krau et al use thi idea effect local cell larg microscopi imag visual similar input two dimens visual cnn represent help gaug input get map similar featur vector henc understand model ha learn donahu et al project cnn featur two dimens show subsequ layer transform data separ linear classifi gener differ cnn visual method show higher layer featur specif learn task featur tend captur gener aspect imag edg corner tool practic consider deep learn framework deep learn framework develop easili build neural network exist modul high level popular one caff jia et al theano bastien et al collobert et al tensorflow abadi et al rampasek goldenberg tabl differ iti eas use way model defin train caff jia et al develop berkeley vision learn center written network architectur specifi configur file model train use via command line without write code addit python matlab interfac avail caff offer one effici implement cnn provid multipl train model imag recognit make well suit comput vision task downsid custom model need implement difficult addit caff optim recurr architectur theano bastien et al team et al develop maintain univers montreal written python model definit follow declar instead imper program paradigm mean user specifi need done order neural network declar comput graph compil nativ code execut thi design allow theano optim comput step automat deriv main strength consequ theano well suit build custom model offer particularli effici implement rnn softwar wrapper kera http kera lasagn http provid addit abstract allow build network exist compon reus network major back theano frequent long compil time build larger model collobert et al wa initi develop univers new york base script languag luajit network easili built stack exist modul compil henc make suit fast ing theano offer effici cnn implement access rang model possibl downsid need user familiar luajit script languag also luajit less suit build custom recurr network tensorflow abadi et al recent deep learn framework develop googl softwar written offer interfac python similar theano neural network declar comput graph optim dure compil howev shorter compil time make suit prototyp key strength tensorflow nativ support parallel across differ devic includ cpu gpu use multipl comput node cluster accompani tool tensorboard allow conveni visual network web browser monitor train progress exampl learn curv paramet updat present fulli connect pool conv pool conv pool conv vacuol cytoplasm cell peripheri figur convolut pool oper stack therebi creat deep network imag analysi standard applic convolut layer follow pool layer box thi exampl lowest level convolut unit oper patch deeper one use captur inform larger region convolut layer follow one multipl fulli connect layer learn featur inform classif layer learnabl weight three exampl imag maxim neuron output shown molecular system biolog ª author molecular system biolog deep learn comput biolog christof angermuel et al download http march ip tensorflow provid effici implement rnn softwar recent activ develop henc onli model current avail data prepar train data key everi machin learn applic sinc data inform featur usual result better manc effort spent collect label clean normal data requir data set size success applic deep learn supervis learn set suffici label train sampl avail fit complex model rule thumb number train sampl least high number model paramet although special architectur model lariz help avoid overfit train data scarc bengio central problem regulatori genom exampl predict molecular trait genotyp limit number train instanc hundr ten thousand train ple typic strategi consid sequenc window centr trait interest splice site transcript factor bind site epigenet mark see fig wide use approach help increas number pair singl individu imag analysi data abund manual curat label train exampl typic difficult obtain instanc train set augment scale ing crop exist imag approach also enhanc robust krizhevski et al anoth strategi reus network wa larg data set imag tion alexnet krizhevski et al vgg simonyan zisserman googlenet szegedi et al resnet et al paramet data set interest microscopi imag particular segment task approach exploit fact differ data set share import characterist featur edg curv transfer caff lasagn torch limit extend tensorflow provid repositori model partit data train valid test set machin learn model need train select test independ data set avoid overfit assur model gener unseen data holdout valid partit data train valid test set standard deep neural network fig train set use learn model differ assess valid set model best perform exampl predict accuraci error select evalu test set quantifi perform unseen data comparison method typic data set proport train valid model test data set small bootstrap use instead hasti et al normal raw data appropri choic data normal help acceler train identif good local minimum categor featur dna nucleotid first need encod numer typic repres binari vector one entri set zero indic gori code exampl dna nucleotid categori tabl overview exist deep learn framework compar four wide use softwar solut caff theano tensorflow core languag python luajit interfac python matlab python c python wrapper lasagn kera kera pretti tensor scikit flow program paradigm imper declar imper declar well suit cnn reus exist model comput vision custom model rnn custom model cnn reus exist model custom model parallel rnn first layer featur third layer featur top left top right bottom right left right bottom figur network use gener featur extractor feed input first layer left give featur represent term pattern left right present smaller patch everi cell top bottom neuron activ extract deeper layer right give rise abstract featur captur inform larger segment imag ª author molecular system biolog christof angermuel et al deep learn comput biolog molecular system biolog download http march ip commonli encod g c fig dna sequenc sent binari string concaten encod nucleotid treat nucleotid independ input featur feedforward neural network cnn four bit encod base commonli consid analog colour nel imag preserv entiti nucleotid numer featur typic subtract mean valu imag pixel usual individu jointli subtract mean pixel intens per colour nel addit common normal step standard featur unit varianc white use decorrel featur fig comput involv sinc requir comput featur covari matrix hasti et al distribut featur skew due extrem valu log transform similar process step may appropri valid test data need normal tentli train data exampl featur valid data need subtract mean comput train data valid data model build choic model architectur prepar data design choic model ture need made default architectur feedforward neural network fulli connect hidden layer appropri start point mani problem convolut tectur well suit data imag abund genom data recurr neural network captur depend sequenti data vari length text protein dna sequenc sophist model built combin differ architectur describ content imag exampl cnn combin rnn cnn encod imag rnn gener correspond imag descript vinyal et al xu et al deep learn work provid modul differ architectur combin determin number neuron network optim number hidden layer hidden unit depend optim valid set one common heurist maxim number layer unit without overfit data layer unit increas number represent function local optima empir evid show make find good local optimum less sensit weight initi dauphin et al model train goal model train find paramet w minim object function l w measur fit tion model parameter w actual observ scale whiten loss epoch low high good perform epoch point earli stop overfit train set valid set g c g c b c e f learn rate test valid train train model evalu model test final perform repeat select model figur data normal deep neural network dna sequenc encod binari vector use code g c b continu data green orang scale unit varianc blue white purpl c holdout valid partit full data set randomli train valid test set model train differ train set model highest perform valid set select gener perform model assess compar machin learn method test set shape learn curv indic learn rate low red shallow decay high orang steep decay follow satur appropri particular learn task green gradual decay e larg differ model perform train set blue valid set green indic overfit stop train soon valid set perform start drop earli stop prevent overfit f illustr dropout regular shown feedforward neural network randomli drop neuron cross reduc sensit neuron neuron previou layer due input grey edg molecular system biolog ª author molecular system biolog deep learn comput biolog christof angermuel et al download http march ip common object function sific error regress minim l w challeng sinc fig see also box fig stochast gradient descent stochast gradient descent wide use train deep model start initi set paramet gradient dw l respect w comput random batch onli exampl train sampl dw point direct steepest descent toward w updat step size eta learn rate fig step paramet updat direct steepest descent minimum reach gousli ball run hill valley bengio train perform strongli depend paramet initi learn rate batch size paramet initi gener model paramet initi randomli avoid local optima determin fix initi start point model paramet sampl independ normal distribut small varianc commonli normal distribut varianc scale invers number hidden unit input layer glorot bengio et al learn rate batch size learn rate batch size stochast gradient descent need chosen care sinc strongli impact train speed model perform differ learn rate usual explor logarithm scale recommend default valu bengio batch size train sampl suitabl applic batch size increas speed train decreas reduc memori usag import train complex model gpu optimum learn rate batch size connect larger batch size typic requir smaller learn rate learn rate decay learn rate gradual reduc dure train base idea larger step may help earli train stage order overcom possibl local optima wherea smaller step size allow explor narrow paramet region loss function advanc stage train common approach includ linearli reduc learn rate constant factor valid loss stop improv exponenti everi train iter epoch bengio gawehn et al momentum vanilla stochast gradient descent extend tum usual improv train sutskev et al instead updat current paramet vector wt time gradient vector directli fraction previou updat ad current one momentum rate v weight updat momentum vector mt thi approach help take larger step direct gradient point consist therefor speed converg momentum rate v set typic valu nesterov momentum nesterov special form concept sometim provid addit advantag adapt learn rate method reduc sensit specif choic learn rate adapt learn rate method rmsprop adagrad srivastava et al adam kingma ba develop order appropri adapt learn rate per paramet dure train recent method adam ne strength previou method rmsprop adagrad gener recommend mani applic batch normal batch normal ioff szegedi recent describ approach reduc depend train paramet initi speed train reduc overfit easi implement ha margin addit comput cost ha henc becom common practic batch normal zero centr normal data onli input layer also hidden layer befor activ function thi approach allow use higher learn rate henc also acceler train analys learn curv valid learn process loss monitor function number train epoch number time full train set ha travers fig ing curv decreas slowli learn rate may small increas loss decreas steepli begin satur quickli learn rate may high extrem learn rate result increas fluctuat learn curv bengio monitor train valid perform parallel train loss recommend monitor target perform accuraci train valid set dure train fig low decreas tion perform rel train perform indic fit bengio avoid overfit deep neural network notori difficult train ting data major challeng sinc nonlinear mani paramet overfit result complex model rel size train set thu reduc decreas model complex exampl number hidden layer unit increas size train set exampl via data augment follow train guidelin help avoid overfit dropout srivastava et al common tion techniqu often one key ingredi train deep model activ neuron randomli set zero drop dure train forward pass intuit result ensembl differ network whose ª author molecular system biolog christof angermuel et al deep learn comput biolog molecular system biolog download http march ip predict averag fig dropout rate correspond probabl neuron drop sensibl default valu addit drop hidden unit input unit drop howev usual lower rate dropout often combin regular magnitud paramet valu norm less commonli norm anoth popular regular method earli stop train stop soon valid perform start satur deterior paramet best manc valid set chosen layerwis bengio et al salakhutdinov hinton consid model overfit despit mention regular techniqu instead train entir network onc layer first unsupervis use autoencod restrict boltzmann machin afterward entir network use actual supervis learn object optim tabl summar recommend start point common exclud size number filter cnn sinc best configur model differ configur train perform evalu tion set number configur grow exponenti number tri sibl practic bengio therefor recommend optim import ing rate batch size length convolut filter dentli via line search explor differ valu keep constant refin paramet space explor random sampl set best perform valid set chosen framework spearmint snoek et al hyperopt bergstra cox smac hutter et al allow automat explor space use bayesian optim howev although alli power present difficult appli parallel random sampl train gpu train neural network compar shallow model take hour day even week depend size train set model architectur train gpu consider reduc train time commonli tenfold therefor crucial evalu multipl model cientli reason thi speedup learn deep network requir larg number matrix multipl parallel effici gpu deep learn framework provid support train model either cpu gpu without requir ani knowledg gpu program desktop machin local gpu card often use framework support specif brand altern commerci provid provid gpu cloud comput cluster pitfal singl method univers applic choic whether use deep learn approach specif convent analysi approach remain valid advantag data scarc aim assess statist signific current difficult use deep ing method anoth limit deep learn increas train complex appli model design requir comput environ conclus deep learn method power complement classic machin learn tool analysi strategi alreadi approach found use number applic tional biolog includ regulatori genom imag analysi first publicli avail softwar framework help reduc overhead model develop provid rich access toolbox practition expect continu improv softwar infrastructur make deep learn applic grow rang biolog problem acknowledg os ca fund european molecular biolog laboratori tp wa support european region develop fund biomedit project estonian research council lp wa support wellcom trust estonian research council os wa support european research council agreement conflict interest author declar conflict interest tabl central paramet neural network recommend set name rang default valu learn rate batch size momentum rate weight initi normal uniform glorot uniform glorot uniform adapt learn rate method rmsprop adagrad adadelta adam adam batch normal ye ye learn rate decay none linear exponenti linear rate activ function sigmoid tanh relu softmax relu dropout rate regular molecular system biolog ª author molecular system biolog deep learn comput biolog christof angermuel et al download http march ip refer abadi agarw barham p brevdo e chen z citro c corrado gs davi dean j devin ghemawat goodfellow harp irv g isard jia josofowicz r kaiser l kudlur levenberg j et al tensorflow machin learn heterogen distribut system agathocl christodoul g prompona v christodoul c vassiliad v antoni protein secondari structur predict bidirect recurr neural net weight updat residu enhanc perform artifici intellig applic innov papadopoulo h andreou bramer ed vol pp berlin heidelberg springer alain g bengio rifai regular estim local statist proc corr pp albert fw treusch shockley ah bloom js kruglyak l genet protein abund variat larg yeast popul natur alipanahi b delong weirauch mt frey bj predict sequenc specif protein deep learn nat biotechnol angermuel c lee h reik w stegl accur predict cell dna methyl state use deep learn biorxiv doi asgari e mofrad mrk protvec continu distribut represent biolog sequenc plo one bach binder montavon g klauschen f muller kr samek w explan classifi decis relev propag plo one bahdanau cho k bengio neural machin translat jointli learn align translat bastien f lamblin p pascanu r bergstra j goodfellow bergeron bouchard n bengio theano new featur speed improv battl khan z wang sh mitrano ford mj pritchard jk gilad genom variat impact regulatori variat rna protein scienc bell jt pai aa pickrel jk gaffney dj r degner jf gilad pritchard jk dna methyl pattern associ genet gene express variat hapmap cell line genom biol bengio lamblin p popovici larochel h greedi train deep network advanc neural inform process system schölkopf b platt b hofmann ed vol pp cambridg mit press bengio practic recommend train deep architectur neural network trick trade montavon g orr g müller ed pp berlin heidelberg springer bengio courvil vincent p represent learn review new perspect pattern anal mach intel ieee tran bergstra j cox dd hyperparamet optim boost classifi facial express good null model che z purushotham khemani r liu distil knowledg deep network applic healthcar domain cheng c yan kk yip ky rozowski j alexand r shou c gerstein statist framework model gene express use chromatin featur applic modencod dataset genom biol ciresan giusti gambardella lm schmidhub j deep neural network segment neuron membran electron microscopi imag advanc neural inform process system pp cambridg mit press ciresan dc giusti gambardella lm schmidhub j mitosi detect breast cancer histolog imag deep neural network medic imag comput pp berlin heidelberg springer collobert r kavukcuoglu k farabet c environ machin learn biglearn nip workshop dahl ge jaitli n salakhutdinov r neural network qsar predict dauphin yn pascanu r gulcehr c cho k ganguli bengio identifi attack saddl point problem optim advanc neural inform process system pp cambridg mit press deng l deep learn method applic found signal process deng l togneri r deep dynam model learn hidden represent speech featur speech audio process code enhanc recognit ogunfunmi togneri r narasimha ed pp new york springer donahu j jia vinyal hoffman j zhang n tzeng e darrel decaf deep convolut activ featur gener visual recognit eduati f mangravit lm wang tang h bare jc huang r norman kellen menden mp yang j zhan x zhong r xiao g xia abdo n kosyk collabor friend dearri simeonov et al predict human popul respons toxic compound collabor competit nat biotechnol eickholt j cheng j predict protein contact use deep network boost bioinformat eickholt j cheng j dndisord predict protein disord use boost deep network bmc bioinformat fakoor r ladhak f nazi huber use deep learn enhanc cancer diagnosi classif proceed icml workshop role machin learn transform healthcar atlanta ga jmlr w cp farley b clark w simul system digit comput tran ire profess group inf theori ferrari lombardi signoroni bacteri coloni count convolut neural network engin medicin biolog societi embc annual intern confer ieee pp new york ieee gawehn e hiss ja schneider g deep learn drug discoveri mol informat gibb jr van der brug mp hernandez dg traynor bj nall lai arep dillman rafferti ip troncoso j abund quantit trait loci exist dna methyl gene express human brain plo genet girshick r donahu j darrel malik j rich featur hierarchi accur object detect semant segment proceed ieee confer comput vision pattern recognit pp new york ieee glorot x bengio understand difficulti train deep feedforward neural network intern confer artifici intellig statist pp jmlr confer proceed ª author molecular system biolog christof angermuel et al deep learn comput biolog molecular system biolog download http march ip glorot x bord bengio deep spars rectifi neural network intern confer artifici intellig statist pp jmlr confer proceed goodfellow bengio courvil deep learn cambridg mit press prepar grave moham hinton g speech recognit deep recurr neural network ieee intern confer acoust speech signal process icassp pp new york ieee grubert f zaugg jb kasowski ursu spacek dv martin ar greensid p sriva r phanstiel dh pekowska heidari n euskirchen g huber w pritchard jk bustamant cd steinmetz lm kundaj snyder genet control chromatin state human involv local distal chromosom interact cell hasti tibshirani r friedman j franklin j element statist learn data mine infer predict math intel k zhang x ren sun j deep residu learn imag recognit hinton ge salakhutdinov rr reduc dimension data neural network scienc hinton ge osindero teh fast learn algorithm deep belief net neural comput hinton ge practic guid train restrict boltzmann machin neural network trick trade montavon g orr g müller ed pp heidelberg berlin springer hinton g deng l yu dahl ge moham jaitli n senior vanhouck v nguyen p sainath tn deep neural network acoust model speech recognit share view four research group signal process mag ieee hubel wiesel shape arrang column cat striat cortex j physiol hubel dh wiesel tn period suscept physiolog effect unilater eye closur kitten j physiol hutter f hoo hh k sequenti optim gener algorithm configur learn intellig optim coello coello ca ed vol pp heidelberg berlin springer ioff szegedi c batch normal acceler deep network train reduc intern covari shift jain v murray jf roth f turaga zhigulin v briggman kl helmstaedt mn denk w seung hs supervis learn imag restor convolut network ieee intern confer comput vision pp new york ieee jarrett k kavukcuoglu k ranzato lecun best architectur object recognit ieee intern confer comput vision pp new york ieee jia shelham e donahu j karayev long j girshick r guadarrama darrel caff convolut architectur fast featur embed proceed acm intern confer multimedia pp new york acm kang hm ye c eskin e accur discoveri express quantit trait loci confound spuriou genuin regulatori hotspot genet karlic r chung hr lasserr j vlahovicek k vingron histon modif level predict gene express proc natl acad sci usa kell db metabolom machin learn model toward understand languag cell biochem soc tran kelley dr snoek j rinn j basset learn regulatori code access genom deep convolut neural network genom kingma dp well variat bay kingma ba j adam method stochast optim koh pw pierson e kundaj denois histon convolut neural network biorxiv krau oz ba lj frey b classifi segment microscopi imag use convolut multipl instanc learn krizhevski sutskev hinton ge imagenet classif deep convolut neural network advanc neural inform process system pp cambridg mit press lecun boser b denker js henderson howard hubbard w jackel ld backpropag appli handwritten zip code recognit neural comput lecun bengio hinton g deep learn natur lee b lee na b yoon splice junction predict use deep recurr neural network leung mkk xiong hy lee lj frey bj deep learn regul splice code bioinformat leung mkk delong alipanahi b frey bj machin learn genom medicin review comput problem data set proceed ieee vol pp new york ieee li sz markov random field model imag analysi berlin heidelberg springer scienc busi media li j ching huang garmir lx use epigenom data predict gene express lung cancer bmc bioinformat suppl libbrecht mw nobl ws machin learn applic genet genom nat rev genet lipton zc critic review recurr neural network sequenc learn lipton zc kale dc elkan c wetzel r learn diagnos lstm recurr neural network lyon j dehzangi heffernan r sharma paliw k sattar zhou yang predict backbon ca angl dihedr protein sequenc stack spars deep neural network j comput chem mamoshina p vieira putin e zhavoronkov applic deep learn biomedicin mol pharm märten k hallin j warring j liti g part l predict quantit trait genom phenom near perfect accuraci nat commun mcculloch ws pitt w logic calculu idea imman nervou activ bull math biophi menden mp iorio f garnett mcdermott u bene ch ballest pj rodriguez j machin learn predict cancer cell sensit drug base genom chemic properti plo one michalski rs carbonel jg mitchel tm machin learn artifici intellig approach berlin heidelberg springer scienc busi media montgomeri sb sammeth lach rp ingl c nisbett j guigo r dermitzaki et transcriptom genet use second gener sequenc caucasian popul natur molecular system biolog ª author molecular system biolog deep learn comput biolog christof angermuel et al download http march ip murphi kp machin learn probabilist perspect cambridg mit press nesterov method solv convex program problem converg rate soviet math dokladi nesterov introductori lectur convex optim basic cours vol berlin heidelberg springer scienc busi media ning f delhomm lecun piano f bottou l barbano pe toward automat phenotyp develop embryo video imag process ieee tran park kelli deep learn regulatori genom nat biotechnol pärnamaa part l accur classif protein subcellular local high throughput microscopi imag use deep learn biorxiv part l stegl winn j durbin r joint genet analysi gene express data infer cellular phenotyp plo genet part l liu yc tekkedil mm steinmetz lm caudi aa fraser ag boon c andrew bj rosebrock ap herit genet basi protein level variat outbr popul genom pickrel jk marioni jc pai aa degner jf engelhardt nkadori e veyriera jb stephen gilad pritchard jk understand mechan underli human gene express variat rna sequenc natur rakitsch b stegl model local gene network increas power detect genet effect gene express genom biol rampasek l goldenberg tensorflow biolog gateway deep learn cell syst razavian azizpour h sullivan j carlsson cnn featur shelf astound baselin recognit proceed ieee confer comput vision pattern recognit workshop pp new york ieee ronneberg fischer p brox convolut network biomed imag segment medic imag comput pp heidelberg berlin springer rosenblatt f perceptron probabilist model inform storag organ brain psychol rev rumelhart de hinton ge william rj learn represent error cogn model russakovski deng j su h kraus j satheesh huang z karpathi khosla bernstein imagenet larg scale visual recognit challeng int j comput vi salakhutdinov r larochel h effici learn deep boltzmann machin intern confer artifici intellig statist pp jmlr confer proceed salakhutdinov r hinton g effici learn procedur deep boltzmann machin neural comput schmidhub j deep learn neural network overview neural netw simonyan k vedaldi zisserman deep insid convolut network visualis imag classif model salienc map simonyan k zisserman veri deep convolut network scale imag recognit snoek j larochel h adam rp practic bayesian optim machin learn algorithm advanc neural inform process system pp cambridg mit press sønderbi sk winther protein secondari structur predict long short term memori network spencer eickholt j cheng j deep learn network approach ab initio protein secondari structur predict tran comput biol bioinformat springenberg jt dosovitskiy brox riedmil strive simplic convolut net srivastava n hinton g krizhevski sutskev salakhutdinov r dropout simpl way prevent neural network overfit j mac learn stegl part l durbin r winn j bayesian framework account complex factor gene express level greatli increas power eqtl studi plo comput biol stormo gd schneider td gold l ehrenfeucht use perceptron algorithm distinguish translat initi site coli nucleic acid sutskev train recurr neural network phd thesi graduat depart comput scienc univers toronto toronto canada sutskev marten j dahl g hinton g import initi momentum deep learn proceed intern confer machin learn pp jmlr confer proceed sutskev vinyal le qv sequenc sequenc learn neural network advanc neural inform process system pp cambridg mit press swan al mobasheri allaway liddel bacardit j applic machin learn proteom data classif biomark identif postgenom biolog omic szegedi c liu w jia sermanet p reed anguelov erhan vanhouck v rabinovich go deeper convolut proceed ieee confer comput vision pattern recognit pp new york ieee szegedi c vanhouck v ioff shlen j wojna z rethink incept architectur comput vision team ttd r alain g almahairi angermuel c bahdanau balla n bastien f bayer j belikov theano python framework fast comput mathemat express vincent p larochel h lajoi bengio manzagol stack denois autoencod learn use represent deep network local denois criterion j mac learn vinyal toshev bengio erhan show tell neural imag caption gener proceed ieee confer comput vision pattern recognit pp new york ieee wang k cao k hannenh chromatin genom determin altern splice proceed acm confer bioinformat comput biolog health informat pp new york acm waszak sm delaneau gschwind ar kilpinen h raghav sk witwicki rm orioli wiederkehr panousi ni yurovski l planchon bielser padioleau udin g thurnheer hacker hernandez n reymond deplanck b et al popul variat genet control modular chromatin architectur human cell xie xing f kong x su h yang l beyond classif structur regress robust cell detect use convolut neural network medic imag comput pp heidelberg berlin springer ª author molecular system biolog christof angermuel et al deep learn comput biolog molecular system biolog download http march ip xiong hy alipanahi b lee lj bretschneid h merico yuen rkc hua gueroussov najafabadi hs hugh tr morri q barash krainer ar jojic n scherer sw blencow bj frey bj human splice code reveal new insight genet determin diseas scienc xiong c meriti socher r dynam memori network visual textual question answer xu r wunsch ii frank r infer genet regulatori network recurr neural network model use particl swarm optim tran comput biol bioinformat xu mo feng q zhong p lai chang ei deep learn featur represent multipl instanc learn medic imag analysi ieee intern confer acoust speech signal process icassp pp new york ieee xu k ba j kiro r cho k courvil salakhutdinov r zemel r bengio show attend tell neural imag caption gener visual attent yosinski j clune j bengio lipson h transfer featur deep neural network advanc neural inform process system ghahramani z well cort c lawrenc nd weinberg kq ed pp cambridg mit press zeiler md fergu r visual understand convolut network comput pp heidelberg berlin springer zhang w li r zeng sun q kumar ye j ji deep model base transfer learn biolog imag analysi proceed acm sigkdd intern confer knowledg discoveri data mine pp new york acm zhou j troyanskaya og predict effect noncod variant deep sequenc model nat method licens thi open access articl term creativ common attribut licens permit use distribut tion ani medium provid origin work properli cite molecular system biolog ª author molecular system biolog deep learn comput biolog christof angermuel et al download http march ip