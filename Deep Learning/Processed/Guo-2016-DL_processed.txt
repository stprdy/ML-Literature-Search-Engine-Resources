deep learn visual understand review yanm guo c yu liu ard oerleman b songyang lao c song wu michael lew n liac media lab leiden univers niel bohrweg leiden netherland b vdg secur bv zoeterm netherland c colleg inform system manag nation univers defens technolog changsha china r c l e n f articl histori receiv april receiv revis form septemb accept septemb avail onlin novemb keyword deep learn comput vision develop applic trend challeng b r c deep learn algorithm subset machin learn algorithm aim discov multipl level distribut represent recent numer deep learn algorithm propos solv tradit artiﬁci intellig problem thi work aim review art deep learn algorithm comput vision highlight contribut challeng recent research paper ﬁrst give overview variou deep learn approach recent develop brieﬂi describ applic divers vision task imag classiﬁc object detect imag retriev semant segment human pose estim final paper summar futur trend challeng design train deep neural network elsevi right reserv introduct deep learn subﬁeld machin learn attempt learn abstract data util hierarch architectur emerg approach ha wide appli tradit artiﬁci intellig domain semant pars transfer learn natur languag process comput vision mani mainli three import reason boom deep learn today dramat increas chip process abil gpu unit signiﬁcantli lower cost comput hardwar consider advanc machin learn algorithm variou deep learn approach extens review discuss recent year among schmidhub et al emphas import inspir technic contribut histor timelin format bengio examin challeng deep learn research propos research direct deep work shown success comput vision task becaus extract appropri featur jointli form discrimin recent imagenet larg scale visual recognit challeng ilsvrc competit deep learn method wide adopt differ research achiev top accuraci score thi survey intend use gener neural pute comput vision multimedia research interest deep learn comput vision provid overview variou deep learn algorithm applic especi appli comput vision domain remaind thi paper organ follow section divid deep learn algorithm four categori convolut neural network restrict boltzmann machin autoencod spars code model categori well develop ted also describ contribut limit model thi section section describ achiev deep learn scheme variou comput vision applic imag classiﬁc object detect imag retriev semant segment human pose estim result applic shown compar pipelin commonli use dataset section along success deep learn method achiev also face sever challeng design train deep network thi section summar major challeng deep learn togeth inher trend might develop futur section conclud paper content list avail sciencedirect journal homepag neurocomput http elsevi right reserv n correspond author address guo liu oerleman laosongyang lao wu lew neurocomput method recent develop recent year deep learn ha extens studi ﬁeld comput vision consequ larg number relat approach emerg gener method divid four categori accord basic method deriv convolut neural network cnn restrict boltzmann machin rbm autoencod spars code categor deep learn method along repres work shown fig next four part brieﬂi review deep learn method recent develop convolut neural network cnn convolut neural network cnn one notabl deep learn approach multipl layer train robust manner ha found highli effect also commonli use divers comput vision applic pipelin gener cnn architectur shown fig gener cnn consist three main neural layer convolut layer pool layer fulli connect layer differ kind layer play differ role fig gener cnn architectur imag classiﬁc shown layer two stage train network forward stage backward stage first main goal forward stage repres input imag current paramet weight bia layer predict output use comput loss cost ground truth label ond base loss cost backward stage comput gradient paramet chain rule paramet updat base gradient prepar next forward comput sufﬁcient iter forward backward stage network learn stop next ﬁrst introduc function along recent develop layer summar commonli use train strategi network final present eral cnn model deriv model conclud current tendenc use model real applic type layer gener cnn hierarch neural network whose volut layer altern pool layer follow fulli connect layer see fig thi section present function three layer brieﬂi review recent advanc appear research layer convolut layer convolut layer cnn util variou kernel convolv whole imag well intermedi featur map gener variou featur map shown fig three main advantag convolut oper weight share mechan featur map reduc number paramet local connect learn correl among neighbor pixel invari tion object due beneﬁt introduc convolut oper research paper use replac fulli connect layer acceler learn process one interest approach handl convolut layer network network nin method main idea substitut convent convolut layer small multilay perceptron consist multipl fulli connect layer nonlinear activ function therebi replac linear ﬁlter nonlinear neural network thi method achiev good result imag classiﬁc pool layer gener pool layer follow volut layer use reduc dimens featur map network paramet similar convolut layer pool layer also translat invari becaus comput take neighbor pixel account averag pool max pool commonli use strategi fig give exampl max pool process featur map output map reduc dimens max pool oper ha size stride max pool averag pool boureau et al vide detail theoret analysi perform scherer et al conduct comparison two pool oper found lead faster converg select superior invari featur improv eral recent year variou fast gpu implement cnn variant present util strategi pool layer extens studi among three layer three approach relat pool layer differ purpos stochast pool drawback max pool sensit overﬁt train set make hard gener well test sampl aim solv thi problem zeiler et al propos stochast pool approach replac convent determinist pool oper stochast procedur randomli pick activ within pool region accord multinomi distribut equival standard max pool mani copi input imag small local deform thi stochast natur help prevent overﬁt problem spatial pyramid pool spp normal method requir input imag thi restrict may reduc recognit accuraci imag arbitrari size elimin thi limit et al util gener cnn architectur replac last pool layer spatial pyramid pool layer spatial pyramid pool extract represent arbitrari imag region gener ﬂexibl solut handl differ scale size aspect ratio appli ani cnn structur boost perform thi structur deep learn method method method method spars method deep belief network deep boltzmann machin deep energi model spars autoencod denois autoencod contract autoencod spars code spm laplacian spars code local coordin code code alexnet clarifai vgg googlenet spp fig categor deep learn method repres work guo et al neurocomput handl deform fundament challeng comput vision especi object recognit task max pool averag pool use handl deform abl learn deform constraint geometr model object part deal deform efﬁcient ouyang et al introduc new deform constrain pool layer call layer enrich deep model learn deform visual pattern substitut tradit layer ani inform abstract level becaus differ purpos procedur pool strategi design variou pool strategi could combin boost perform cnn layer follow last pool layer network seen fig sever layer convert featur map featur vector featur represent seen fig layer perform like tradit neural work contain paramet cnn enabl us feed forward neural network vector deﬁn length could either feed forward vector tain number categori imag classiﬁc take featur vector process chang structur layer mon howev exampl came transfer learn approach preserv paramet learn genet replac last layer two new layer adapt new visual recognit task drawback layer contain mani meter result larg comput effort train therefor promis commonli appli direct remov layer decreas connect certain method exampl googlenet design deep wide network keep comput budget constant switch fulli connect spars connect architectur train strategi compar shallow learn advantag deep learn build deep architectur learn abstract inform howev larg amount paramet introduc may also lead anoth problem overﬁt recent numer regular method emerg defens overﬁt includ stochast pool mention abov thi section introduc sever regular techniqu may inﬂuenc train perform dropout dropconnect dropout wa propos ton et al explain baldi et al dure train case algorithm randomli omit half featur detector order prevent complex train data enhanc gener abil thi method wa improv speciﬁc research convolut max pool convolut layer pool layer fulli connect layer cat dog person bird fish fox fig pipelin gener cnn architectur kernel input output fig oper convolut layer featur map output map max fig oper max pool layer fig oper layer guo et al neurocomput et al analyz efﬁcaci dropout suggest dropout extrem effect ensembl ing method one gener deriv dropout call dropconnect randomli drop weight rather activ experi show achiev competit even better result varieti standard benchmark although slightli slower fig give comparison dropout dropconnect network data augment cnn appli visual object recognit data augment often util gener addit data without introduc extra label cost known alexnet employ two distinct form data mentat ﬁrst form data augment consist erat imag translat horizont reﬂect second form consist alter intens rgb nel train imag howard et al took alexnet base model ad addit transform improv translat invari color invari extend imag crop extra pixel ad addit color manipul thi data augment method wa wide util recent studi dosovitskiy et al propos unsupervis featur learn approach base data tation ﬁrst randomli sampl set imag patch declar surrog class extend class appli transform correspond translat scale color contrast final train cnn discrimin surrog class featur learnt work show good result varieti classiﬁc task asid classic method scale rotat crop wu et al adopt color cast vignet len distort techniqu produc train exampl broad coverag mean aliz network paramet rather domli set paramet quit popular model base cnn due advantag acceler learn process well improv gener abil erhan et al ha conduct extens simul exist algorithm ﬁnd whi network work better network train tradit way alexnet achiev excel perform releas public numer approach choos net train baselin deep model use paramet accord speciﬁc task nevertheless approach deliv better perform train model clarifai googlenet vgg crucial stage reﬁn model adapt speciﬁc task dataset gener requir class label new train dataset use comput loss function thi case layer new model initi base model alexnet except last output layer depend number class label new dataset therefor randomli initi howev occas veri difﬁcult obtain class label ani new dataset address thi problem similar learn object function wa propos use loss function without class label propag work normal allow model reﬁn layer layer also mani research result describ transfer model efﬁcient new way deﬁn quantifi degre particular layer gener speciﬁc name well featur layer transfer one task anoth conclud initi network transfer featur almost ani number layer give boost gener perform tune new dataset addit regular method describ abov also common method weight decay weight tie mani weight decay work ad extra term cost function penal paramet prevent exactli model train data therefor help gener new exampl weight tie allow model learn good represent input data reduc number paramet convolut neural work anoth interest thing note regular techniqu train mutual exclus combin boost perform cnn architectur recent develop cnn scheme puter vision domain cnn model emerg thi section ﬁrst describ commonli use cnn model summar characterist applic conﬁgur primari contribut sever typic cnn model list tabl alexnet signiﬁc cnn architectur consist ﬁve convolut layer three fulli connect layer input one imag network would repeatedli convolv pool activ forward result layer network wa train imagenet integr variou regular techniqu data augment dropout etc alexnet competit set tone surg interest deep convolut neural network architectur nevertheless two major drawback thi model requir ﬁxed resolut imag clear understand whi perform well zeiler et al introduc novel visual que give insight inner work intermedi featur network dropout network dropconnect network w f wv w f f wv w f w v fig comparison dropout dropconnect network network b dropout network c dropconnect network guo et al neurocomput layer visual enabl ﬁnd architectur outperform alexnet imagenet classiﬁc benchmark result model clarifai receiv top perform competit requir ﬁxed resolut et al propos new pool strategi spatial pyramid pool elimin restrict imag size result could boost accuraci varieti publish cnn tectur despit differ design addit commonli use conﬁgur cnn structur ﬁve convolut layer plu three fulli connect layer also approach tri explor deeper work contrast alexnet vgg increas depth network ad convolut layer take tage veri small convolut ﬁlter layer similarli szegedi et al propos model googlenet also ha quit deep structur layer ha achiev lead formanc competit despit classiﬁc perform achiev variou model model applic limit onli imag classiﬁc base model new framework deriv address challeng task object detect semant segment etc two deriv framework rcnn region cnn featur fcn fulli convolut network mainli design object detect semant mentat respect shown fig core idea rcnn gener multipl object propos extract featur propos use cnn classifi candid window linear svm recognit use region paradigm receiv encourag formanc object detect ha gradual becom eral pipelin recent promis object detect algorithm howev perform rcnn reli much precis object locat may limit robust besid gener process larg ber propos would also decreas efﬁcienc recent opment mainli focus two aspect rcnn take cnn model featur extractor doe make ani chang network contrast fcn propos techniqu recast cnn model fulli convolut net recast techniqu remov restrict imag tion could produc output efﬁcient although fcn propos mainli semant segment techniqu could also util applic imag classiﬁc edg detect etc asid creat variou model usag model also demonstr sever characterist larg network one intuit idea improv formanc cnn increas size includ increas depth number level width number unit level googlenet vgg describ abov adopt quit larg network layer layer respect demonstr increas size beneﬁci imag recognit accuraci jointli train multipl network could lead better manc singl one also mani research design larg network combin differ deep structur cascad mode output former network util latter one shown fig cascad architectur util handl differ task function prior network output may vari task exampl wang et al connect two network object extract ﬁrst network use object local therefor output correspond coordin object sun et al propos care design convolut network detect facial point ﬁrst level provid highli robust initi estim follow two level initi predict similarli ouyang et al adopt train scheme propos zeng et al classiﬁ previou stage jointli work classiﬁ current stage deal misclassiﬁ sampl multipl network anoth tendenc current tion combin result multipl network network execut task independ instead design singl architectur jointli train network insid execut task seen fig miclut et al gave insight erat ﬁnal result receiv set score prior alexnet ciresan et al propos method call dnn mcdnn combin sever dnn umn averag predict thi model achiev tabl cnn model achiev ilsvrc classiﬁc competit method year place conﬁgur contribut alexnet five convolut layersþthre fulli connect layer import cnn architectur set tone mani comput vision research clarifai five convolut layersþthre fulli connect layer insight function intermedi featur layer spp five convolut layersþthre fulli connect layer propos spatial pyramid pool remov requir imag resolut vgg convolut layersþthre fulli nect layer thorough evalu network increas depth googlenet convolut layersþon fulli connect layer increas depth width without rais comput requir basic model deriv model alexnet clarifai vgg googlenet imag classif rcnn fcn object detect semant segment fig cnn basic model deriv model guo et al neurocomput competit result task recognit written digit trafﬁc sign recent ouyang et al also conduct experi evalu perform model combin strategi learnt model differ set combin averag scheme result show model gener thi way high divers plementari improv detect result divers network asid alter cnn structur research also attempt introduc inform sourc combin shallow structur grate contextu inform illustr fig shallow method give addit insight problem literatur exampl found combin shallow method deep learn framework take deep learn method extract featur input featur shallow learn method svm one sent success algorithm rcnn method feed highli distinct cnn featur svm ﬁnal object detect task besid deep cnn fisher vector fv complementari also combin signiﬁcantli improv accuraci imag classiﬁc contextu inform sometim avail object detect task possibl integr global context mation inform bound box genet larg scale visual recognit challeng ilsvrc win team nu concaten raw detect score combin output tradit classiﬁc framework context reﬁnement similarli ouyang et al also took imag classiﬁc score contextu featur object detect restrict boltzmann machin rbm restrict boltzmann machin rbm gener chastic neural network wa propos hinton et al rbm variant boltzmann machin restrict visibl unit hidden unit must form bipartit graph thi restrict allow efﬁcient train algorithm particular contrast diverg algorithm sinc model bipartit graph hidden unit h visibl unit condit independ therefor p ð þ p ð þp ð ð þ equat h satisfi boltzmann distribut given input get h p ð similarli ﬁgure p ð adjust paramet minim differ result h act good featur hinton gave detail explan provid tical way train rbm work discuss main difﬁculti train rbm underli reason pose new algorithm consist adapt learn rate enhanc gradient address difﬁculti develop rbm found model approxim binari unit noisi rectiﬁ linear unit preserv inform rel intens mation travel multipl layer featur detector reﬁnement onli function well thi model also wide employ variou approach util rbm learn modul compos low deep model deep belief network dbn deep mann machin dbm deep energi model dem comparison three model shown fig dbn undirect connect top two layer form rbm direct connect lower layer dbm undirect connect layer network dem determinist hidden unit lower layer stochast hidden unit top hidden layer summari three deep model along relat repres refer provid tabl next section explain model describ applic comput vision task respect deep belief network dbn deep belief network dbn propos hinton wa signiﬁc advanc deep learn probabilist gener model provid joint probabl distribut observ data label dbn ﬁrst take advantag efﬁcient layer greedi learn strategi initi deep network weight jointli desir output greedi learn procedur ha two main advantag gener proper initi network address ﬁculti paramet select may result poor local optima extent learn procedur unsupervis network network estim network k input fig combin deep structur cascad mode network network estim estim estim k network k estim input input input fig combin result multipl network network contextu inform shallow structur estim input fig combin deep network inform sourc guo et al neurocomput requir class label remov necess label data train howev creat dbn model comput expens task involv train sever rbm clear approxim train mize model dbn success focus research attent deep learn consequ mani variant creat nair et al develop modiﬁ dbn model util boltzmann machin object recognit model learn model ural imag use spars rbm ﬁrst layer learn local orient edg ﬁlter second layer captur varieti contour featur well corner junction improv robust occlus random nois lee et al appli two strategi one take advantag spars nection ﬁrst layer dbn regular model develop probabilist algorithm appli comput vision task drawback dbn consid structur input imag address thi problem convolut deep belief network cdbn wa introduc cdbn util spatial inform neighbor pixel introduc convolut rbm erat translat invari gener model scale well high dimension imag algorithm wa extend achiev excel perform face veriﬁc deep boltzmann machin dbm deep boltzmann machin dbm propos khutdinov et al anoth deep learn algorithm unit arrang layer compar dbn whose top two layer form undirect graphic model whose lower layer form direct gener model dbm ha undirect connect across structur like rbm dbm also subset boltzmann famili differ dbm possess multipl layer hidden unit unit layer condit independ layer vice versa given visibl unit calcul posterior distribut hidden unit longer tractabl result interact hidden unit train network dbm would jointli train layer speciﬁc unsupervis model instead maxim likelihood directli dbm use stochast maximum likelihood sml base algorithm maxim lower bound likelihood perform onli one updat use markov chain mont carlo mcmc method paramet updat avoid end poor local minima leav mani hidden unit effect dead greedi train strategi also ad layer fig comparison three model dbn b dbm c dem tabl overview repres method method characterist advantag drawback refer dbn undirect connect top two layer direct connect lower layer properli initi network prevent poor local optima extent train unsupervis remov necess label data train due initi process comput expens creat dbn model lee et al nair et al dbm undirect connect layer network deal robustli ambigu input incorpor feedback joint optim hinton et al cho et al montavon et al goodfellow srivastava et al dem determinist hidden unit lower layer stochast hidden unit top hidden layer produc better gener model allow lower layer adapt train higher layer learnt initi weight may good converg carreira et al elfw et al guo et al neurocomput dbm network much way dbn thi joint learn ha brought promis improv term likelihood classiﬁc perform deep featur learner howev crucial disadvantag dbm time complex approxim infer consider higher dbn make joint optim dbm paramet impract larg dataset increas cienci dbm research introduc approxim infer algorithm util separ tion model initi valu latent variabl layer thu effect acceler infer also mani approach aim improv effect dbm improv either take place stage train stage exampl montavon et al introduc center trick improv stabil dbm made crimin gener train scheme wa util jointli train dbm outperform previou method imag classiﬁc propos deep energi model dem deep energi model dem introduc ngiam et al recent approach train deep architectur unlik dbn dbm share properti multipl stochast hidden layer dem ha singl layer stochast hidden unit efﬁcient train infer model util deep feed forward neural network model energi landscap abl train layer simultan evalu perform natur imag demonstr joint train multipl layer yield qualit tativ improv greedi train ngiam et al use hybrid mont carlo hmc train model also option includ contrast diverg score ing similar work found although rbm suitabl cnn vision tion also good exampl adopt rbm vision task shape boltzmann machin wa propos eslami et al handl task model binari shape imag learn high qualiti probabl distribut object shape realism sampl distribut gener new exampl shape class kae et al combin crf rbm model local global structur face segment ha consist reduc error face label new deep architectur ha present phone recognit combin rbm featur extract modul standard dbn thi approach attack represent inefﬁci issu gmm import limit previou work appli dbn phone recognit autoencod autoencod special type artiﬁci neural network use learn efﬁcient encod instead train network predict target valu given input x encod train reconstruct input x therefor output vector dimension input vector gener process autoencod shown fig dure process autoencod optim ing reconstruct error correspond code learn featur gener singl layer abl get discrimin repres featur raw data research util deep autoencod forward code learnt viou autoencod next accomplish task deep autoencod wa ﬁrst propos hinton et al still extens studi recent paper deep autoencod often train variant conjug gradient method though often reason effect thi model could becom quit ineffect error present ﬁrst layer thi may caus network learn reconstruct averag train data proper approach remov thi problem network initi weight approxim ﬁnal solut also variant autoencod propos make represent constant possibl respect chang input tabl list variant autoencod brieﬂi summar characterist advantag next section describ three import variant spars encod denois autoencod contract autoencod spars autoencod spars autoencod aim extract spars featur raw data sparsiti represent either achiev penal hidden unit bias directli lize output hidden unit activ spars represent sever potenti advantag use represent increas likelihood differ categori easili separ theori svm spars represent provid us ple interpret complex input data term number part biolog vision use spars represent earli visual area quit variant spars autoencod layer local connect spars autoencod pool local contrast normal thi model allow system train face detector without label imag contain face result featur detector robust translat scale rotat denois autoencod order increas robust model vincent pose model call denois autoencod dae recov correct input corrupt version thu cing model captur structur input distribut process dae shown fig contract autoencod contract autoencod cae propos rifai et al follow dae share similar motiv learn robust represent dae make whole ping robust inject nois train set cae achiev robust ad analyt contract penalti reconstruct error function although notabl differ dae cae state bengio et al alain et al suggest dae form cae close relat dae small corrupt nois valu type cae tractiv penalti whole reconstruct function rather encod dae cae success use unsupervis transfer learn challeng encod code decod reconstruct error input fig pipelin autoencod guo et al neurocomput spars code purpos spars code learn set basic function describ input data numer advantag spars code reconstruct descriptor better use multipl base captur lation similar descriptor share base sparsiti allow represent captur salient properti imag line biolog visual system argu spars featur signal use learn imag statist studi show imag patch spars nal pattern spars featur linearli separ solv spars code equat thi subsect brieﬂi describ solv spars code problem get spars represent gener object function spars code min x min hðtþ xðtþ þλ ﬁrst term function reconstruct error dhðtþ reconstruct xðtþ second regular term sparsiti penalti norm regular ha iﬁe lead spars represent eq solv regress method call lasso least absolut shrinkag select oper get analyt solut spars represent therefor solv problem normal result intract comput optim spars code model altern cedur updat weight infer featur activ h input given current set weight weight updat one commonli use algorithm updat weight call project gradient algorithm renorm column weight matrix right updat tradit gradient descent algorithm normal necessari sparsiti penalti ani effect howev gradient descent use iter project often show slow converg lee et al deriv lagrang dual method much efﬁcient od given dictionari paper propos search algorithm learn spars represent tion two algorithm enabl perform signiﬁcantli better previou one howev efﬁcient handl veri larg train set dynam train data chang time thu inher access whole train set iter address thi issu onlin approach wa propos learn dictionari process one element small subset train set time algorithm updat dictionari use coordin descent warm restart doe requir ani learn rate tune gregor et al tri acceler dictionari learn anoth way import idea coordin descent algorithm cod onli updat promis hidden unit therefor lead dramat reduct number iter reach given code predict error activ infer given set weight need infer featur vation popular algorithm spars code infer iter algorithm ista take gradient step optim reconstruct term follow sparsiti term ha close form shrinkag oper although simpl effect algorithm suffer sever problem converg quit slowli problem partli solv fast iter algorithm fista approach preserv comput pliciti ista converg quickli due introduct momentum term dynam converg plexiti chang ista fista infer involv sort iter optim lasso high comput complex contrast cuoglu et al util network approxim spars code dramat acceler infer process furthermor lasso optim stage wa replac margin regress effect scale spars code framework larg dictionari develop brieﬂi state gener spars sentat given object function thi subsect introduc algorithm relat spars code tabl variant autoencod method characterist advantag spars add sparsiti penalti forc represent spars make categori separ make complex data meaning line biolog vision system autoencod denois recov correct input corrupt version robust nois autoencod contract add analyt contract penalti reconstruct error function better captur local direct variat dictat data autoencod satur rais reconstruct error input near data manifold limit abil reconstruct input near data manifold autoencod convolut share weight among locat input preserv spatial local util imag structur autoencod util proper shrinkag function train autoencod without addit regular power learn represent data veri high intrins dimension autoencod input corrupt input reconstruct hidden node reconstruct error fig denois autoencod guo et al neurocomput particular use comput vision task spars code algorithm relat along contribut drawback shown fig one repres algorithm spars code call spars code spm scspm extens spatial pyramid match spm method unlik spm use vector quantiz vq imag represent scspm util spars code sc follow spatial max pool codebook sc basi featur activ small number compar vq sc receiv much lower reconstruct error due less restrict constraint assign coat et al investig reason success sc vq detail drawback scspm deal local featur separ thu ignor mutual depend among make sensit featur varianc spars code may vari lot even similar featur address thi problem gao et al propos laplacian spars code lsc approach similar featur onli assign cluster center also guarante select cluster center similar enc spars code laplacian spars ing shown fig ad local preserv constraint object spars code lsc keep mutual depend spars code procedur gao et al rais graph laplacian spars code hlsc method extend lsc case similar among instanc deﬁn hyper graph lsc hlsc enhanc robust spars code anoth way address sensit problem hierarch spars code method propos yu et al introduc layer spars code model ﬁrst layer encod individu patch second layer jointli encod set patch belong group therefor model leverag spatial borhood structur model depend patch local region imag besid fulli automat method learn featur pixel level rather exampl sift featur hierarch spars code util anoth research learn featur imag unsupervis fashion model improv zeiler et al addit sensit anoth method exist improv scspm algorithm consid local yu et al observ scspm result tend local nonzero cient often assign base nearbi result vation suggest modiﬁc scspm call local dinat code lcc explicitli encourag code local also theoret show local import sparsiti experi shown local enhanc sparsiti spars code help learn onli code local prefer let similar data similar zero dimens code although lcc ha comput advantag classic spars code still need solv norm optim problem acceler learn process practic code method call constrain linear code llc wa introduc seen fast implement lcc replac regular regular scspm spm sc ha less restrict constraint assign vq ignor mutual depend local featur lsc enhanc similar featur keep mutual depend spars code hlsc defin similar among instanc hyper graph lcc enhanc local explicitli encourag code local llc acceler process time consum svc enhanc local adopt smoother code scheme asgd imagenet prior cnn fig spars code algorithm relat contribut drawback spars code laplacian sc featur quantiz visual word fig differ spars code laplacian spars code b spars code c laplacian sc guo et al neurocomput comparison vq scspm llc shown fig besid llc anoth model call code svc also guarante local spars code given x svc activ coordin associ neighborhood x achiev spars represent svc simpl extens vq expand vq local tangent direct thu smoother code scheme remark result shown propos averag stochast gradient descent asgd scheme combin lcc svc algorithm scale imag classiﬁc scale dataset produc result imagenet object recognit task prior rise cnn architectur anoth smooth code method present call smooth spars code ssc method rate neighborhood similar tempor inform spars code lead code repres neighborhood rather individu sampl lower mean squar reconstruct error recent et al propos new unsupervis featur learn framework call deep spars code deepsc extend spars code architectur ha best perform among spars code scheme describ abov discuss order compar understand abov four categori deep learn summar advantag advantag respect divers properti list tabl nine properti total detail refer whether approach ha shown effect divers media text imag audio applic includ speech recognit visual recognit unsupervis learn refer abil learn deep model without supervisori annot featur learn abil automat learn featur base data set train predict refer efﬁcienc learn infer process respect biolog understand theoret justiﬁc repres whether approach ha signiﬁc logic underpin theoret foundat respect invari refer whether approach ha shown robust transform rotat scale translat small train set refer abil learn deep model use small number exampl import note tabl onli repres gener current ﬁnding futur sibil special nich case applic result deep learn ha wide adopt variou direct comput vision imag classiﬁc object detect imag retriev semant segment human pose estim key task imag understand thi part brieﬂi summar develop deep learn result refer origin paper especi cnn base algorithm ﬁve area imag classiﬁc imag classiﬁc task consist label input imag probabl presenc particular visual object class shown fig prior deep learn perhap commonli use method imag classiﬁc method base bag visual word bow ﬁrst describ imag histogram quantiz visual word feed histogram classiﬁ typic svm thi pipelin wa base orderless statist incorpor spatial geometri bow descriptor lazebnik et al integr spatial pyramid approach pipelin count number visual word insid set imag instead whole region thereaft thi pipelin wa improv ing spars code optim problem build book receiv best perform imagenet classiﬁc spars code one basic algorithm deep learn discrimin origin one hog lbp approach base bow concern zero order statist count visual word discard lot valuabl inform imag method introduc ronnin et al overcam thi issu extract higher order statist employ fisher kernel achiev imag classiﬁc result thi phase research tend focu higher order statist core idea deep learn krizhevski et al repres turn point object recognit larg cnn wa train imagenet databas thu prove cnn could addit handwritten digit recognit perform well natur imag classiﬁc propos alexnet ilsvrc vq scspm llc node assign node assign node assign fig comparison vq scspm llc vq b scspm c llc tabl comparison among four categori deep learn properti cnn rbm autoencod spars code gener ye ye ye ye unsupervis learn ye ye ye featur learn ye ye ye train ye ye predict ye ye ye ye biolog understand ye theoret justiﬁc ye ye ye ye invari ye ye small train set ye ye ye ye note ye indic categori doe well properti otherwis mark ye refer preliminari weak abil guo et al neurocomput error rate spark signiﬁc addit activ cnn research fig present result imagenet test dataset sinc along pipelin ilsvrc overfeat propos multiscal slide window approach could ﬁnd optim scale imag fulﬁll differ task simultan classiﬁc local detect speciﬁc algorithm decreas test error zeiler et al introduc novel visual techniqu give insight function intermedi featur layer adjust new model outperform alexnet reach error rate top perform ilsvrc ilsvrc wit steep growth deep learn particip util cnn basi model signiﬁc progress made imag classiﬁc error wa almost halv sinc model elimin restrict ﬁxed input imag size could boost accuraci varieti publish cnn tectur despit differ design multipl reduc error rate rank third imag classiﬁc challeng ilsvrc along improv classic cnn model anoth characterist share model architectur becam deeper shown googlenet rank ilsvrc vgg rank ilsvrc achiev respect despit potenti capac possess larger model also suffer overﬁt underﬁt problem littl train data littl train time avoid thi shortcom wu et al develop new strategi imag data augment usag imag also built larg supercomput deep neural network develop highli optim parallel algorithm classiﬁc result achiev rel improv previou one error rate recent et al propos parametr rectiﬁ linear unit erat tradit rectiﬁ activ unit deriv robust initi method thi scheme led test error surpass perform ﬁrst time similar result achiev ioff et al whose method reach test error util ensembl normal network fig imag classiﬁc exampl alexnet imag ha one ground truth label follow top guess probabl fig imagenet classiﬁc result test dataset guo et al neurocomput object detect object detect differ close relat imag classiﬁc task imag classiﬁc whole imag util input class label object within imag estim object detect besid output inform presenc given class also need estim posit instanc instanc shown fig detect window regard correct output bound box ha sufﬁcient larg overlap ground truth object usual challeng pascal voc dataset wide employ evalu object detect class thi databas dure test phase algorithm predict bound box object belong class test imag thi section describ recent ment deep learn scheme object detect accord achiev voc voc relat advanc shown tabl befor surg deep learn deform part model dpm wa effect method object detect take advantag deform part model detect object across scale locat imag exhaust manner integr techniqu bound box predict context rescor model achiev averag precis voc test set deep learn method especi method achiev top tier perform imag classiﬁc task research start transfer object detect problem earli deep learn approach object detect wa duce szegedi et al paper propos algorithm call detectornet replac last layer alexnet regress layer algorithm captur object locat well achiev competit result test set advanc algorithm time handl tipl instanc object imag deepmultibox also show neural network model gener pattern current success object detect tem gener larg pool candid box classifi use cnn featur repres approach rcnn scheme propos girshick et al util select search gener object propos extract cnn featur propos featur fed svm classiﬁ decid whether relat candid window tain object rcnn improv benchmark larg margin becam base model mani promis algorithm algorithm deriv rcnn mainli divid two categori ﬁrst categori aim acceler train test process although rcnn ha excel object detect accuraci comput intens becaus ﬁrst warp process object propos independ consequ algorithm aim improv efﬁcienc appear frcn rpn yolo etc algorithm detect object faster achiev compar map benchmark second categori mainli intend improv raci rcnn perform recognit use region paradigm highli depend qualiti object hypothes current mani object propos algorithm object select search object propos bing edg box et al fig object detect exampl rcnn red box extract salient object contain green box contain predict score interpret refer color thi ﬁgure legend reader refer web version thi articl tabl object detect result voc voc challeng method train data voc voc map map net map map net dpm detectornet deepmultibox rcnn rcnn þbb rcnn rcnn þbb þ bb frcn frcn rpn rpn rpn rpn fg fg þbb noc noc þbb note train data trainval trainval trainval union trainval trainval test union trainval bb bound box regress approach base alexnet clarifai approach base guo et al neurocomput scheme exhaust evalu although scheme good ﬁnding rough object posit normal could accur local whole object via tight bound box form largest sourc detect error therefor mani approach emerg tri correct poor local one import direct method combin semant segment techniqu ple sd scheme propos hariharan et al util segment background insid detect result improv perform object detect without bound box regress hand ud method uniﬁ object detect semant segment process one framework enforc consist integr context inform model demonstr encourag perform task similar work come segdeepm propos zhu et al also incorpor segment along addit evid boost accuraci object detect also approach attempt precis locat object way instanc fg address lizat problem via two method develop search algorithm iter optim locat train cnn classiﬁ structur svm object balanc classiﬁc local combin method demonstr promis perform two challeng dataset asid effort object local noc framework tri evolv effort object classiﬁc step place commonli use perceptron mlp explor differ noc structur implement object classiﬁ much cheaper easier collect larg amount label collect detect data label precis bound box therefor major challeng scale object detect difﬁculti obtain label imag larg number categori hoffman et al propos deep detect adapt dda algorithm learn differ imag classiﬁc object detect transfer classiﬁ categori detector without bound box annot data method ha potenti enabl detect thousand categori lack bound box annot two promis scalabl approach conceptlean babylearn learn accur concept detector without massiv annot visual concept collect weakli label imag cheap ceptlean develop hard instanc learn algorithm automat discov visual concept noisi label imag collect result ha potenti learn concept directli web hand learn approach simul babi interact physic world achiev compar result base approach onli sampl object categori along larg amount onlin unlabel video tabl also observ sever factor could improv perform addit algorithm larger train set deeper base model bound box regress imag retriev imag retriev aim ﬁnd imag contain similar object scene queri imag illustr fig success alexnet suggest featur emerg upper layer cnn learn classifi imag serv good descriptor imag classiﬁc motiv thi mani recent studi use cnn model imag retriev task studi achiev competit result compar tradit method vlad fisher vector follow paragraph introduc main idea cnn base method inspir spatial pyramid match gong et al pose kind revers spm idea extract patch tipl scale start whole imag pool scale without regard spatial inform aggreg local patch respons ﬁner scale via vlad encod orderless natur vlad help build invari sentat final origin global deep activ caten vlad featur ﬁner scale form new imag represent razavian et al use featur extract overfeat network gener imag represent tackl divers rang vision task includ recognit retriev first augment train set ad crop rotat sampl imag extract multipl differ size differ locat comput cnn present distanc refer queri imag set averag distanc queri refer imag given recent success deep learn techniqu achiev research present attempt evalu deep learn bridg semant gap imag retriev cbir encourag result reveal deep cnn model larg dataset directli use fig imag retriev exampl use cnn featur left imag queri one imag green frame right repres posit retriev candid interpret refer color thi ﬁgure legend reader refer web version thi articl guo et al neurocomput featur extract new cbir task appli featur represent new domain wa found lariti learn boost retriev perform ther retrain deep model classiﬁc lariti learn object new domain accuraci improv signiﬁcantli differ approach shown ﬁrst extract like imag patch gener object detector one cnn featur extract object patch alexnet model mani result experi conclud method achiev signiﬁc accuraci improv space consumpt time cost still obtain higher accuraci final without slide window patch babenko et al focu holist descriptor whole imag map singl vector cnn model found best perform observ veri top network rather layer two level put import result pca affect perform cnn much less perform vlad fisher vector therefor pca compress work better cnn ture tabl show retriev result sever public dataset one interest problem cnn featur layer ha highest impact ﬁnal perform method extract featur second fulli connect layer contrast method use ﬁrst fulli connect layer cnn model imag represent moreov choic may chang differ dataset thu think investig characterist layer still open problem semant segment past larg number studi focu semant segment task yield promis progress main reason success come cnn model capabl tackl predict network dataset differ classiﬁc tion semant segment requir output mask spatial distribut semant segment recent advanc cnn base method summar follow segment approach segment imag base candid window output object detect rcnn sd ﬁrst gener region propos object detect util tradit approach segment region assign pixel class label detect base sd hariharan et al propos hypercolumn pixel vector activ gain larg improv one disadvantag tation larg addit expens object detect without extract region raw imag dai et al design convolut featur mask cfm method extract propos directli featur map efﬁcient convolut featur map onli need comput onc even though error caus propos object detect tend propag segment stage base segment second one fulli volut network fcn replac fulli connect layer convolut layer ha popular strategi baselin semant segment long et al deﬁn novel architectur combin semant inform deep coars layer appear mation shallow ﬁne layer produc accur detail segment deeplab propos similar fcn model also integr strength condit random ﬁeld crf fcn detail boundari recoveri instead use crf step lin et al jointli train fcn crf efﬁcient piecewis train wise work convert crf recurr neural network rnn plug part fcn model weakli supervis annot apart advanc segment model work focus weakli supervis segment papandr et al studi challeng segment weakli annot ing data bound box label wise boxsup method made use bound box annot estim segment mask use updat network iter work show excel perform combin small number annot imag larg number bound box annot imag tabl imag retriev result sever dataset method holiday ukb babenko et al sun et al gong et al razavian et al wan et al tabl semant segment result pascal voc val test set method train val test descript sd voc extra region propos input imag cfm voc extra region propos featur map voc extra one model three stride hypercolumn voc extra region propos input imag deeplab voc extra one model one stride voc extra field view voc extra scale imag model voc extra recurr neural network boxsup voc extraþcoco weakli supervis annot voc extraþcoco weakli supervis annot guo et al neurocomput describ main properti abov method compar result pascal voc val test set list tabl human pose estim human pose estim aim estim local human joint still imag imag sequenc shown fig veri import wide rang potenti tion video surveil human behavior analysi interact hci extens die recent howev thi task also veri challeng becaus great variat human appear complic background well mani nuisanc factor illumin viewpoint scale etc thi part mainli summar deep learn scheme estim human lation still imag although scheme could porat motion featur boost perform video normal human pose estim involv multipl problem recogn peopl imag detect describ human bodi part model spatial conﬁgur prior deep learn best perform human pose estim method base bodi part detector detect describ human bodi part ﬁrst impos textual relat local part one typic approach pictori structur take advantag tree model captur geometr relat adjac part ha develop variou method deep learn algorithm learn featur toler variat nuisanc factor achiev success variou comput vision task recent receiv signiﬁc attent research commun summar perform relat deep learn algorithm two commonli use dataset frame label cinema flic leed sport pose lsp flic sist train imag test imag obtain popular hollywood movi contain peopl divers pose annot joint label lsp extens contain train test imag sport peopl gather flickr full bodi joint annot two wide accept evalu metric evalu centag correct part pcp measur rate correct limb detect percent detect joint pdj measur rate correct limb detect follow tabl illustr pdj comparison iou deep learn method flic dataset normal distanc tabl list pcp comparison lsp dataset gener deep learn scheme human pose estim categor accord handl manner input imag holist process process holist process method tend accomplish task global manner explicitli deﬁn model individu part spatial relationship one typic model call deeppos propos toshev et al thi model mulat human pose estim method joint regress problem doe explicitli deﬁn graphic model part fig human pose estim tabl pdj comparison flic dataset pdj pck head shoulder elbow wrist jain et al deeppos chen et al tompson et al tompson et al tabl pcp comparison lsp dataset torso head mean ouyang et al deeppos chen et al guo et al neurocomput detector human pose estim speciﬁc util architectur ﬁrst layer address ambigu bodi part holist way gener initi pose estim second layer reﬁn joint locat estim thi model achiev advanc sever challeng dataset howev method suffer inaccuraci region sinc difﬁcult learn direct regress complex pose vector imag process method propos detect human bodi part individu follow graphic model incorpor spatial inform instead train work use whole imag chen et al util local part patch background patch train dcnn order learn condit probabl part presenc spatial relationship incorpor graphic model algorithm gain promis perform moreov jain et al train multipl smaller convnet perform independ binari part classiﬁc follow weak spatial model remov strong outlier enforc global pose consist similarli tompson et al design convnet architectur perform likelihood regress bodi part follow implicit graphic model promot joint consist model wa extend argu pool layer cnn would limit spatial local accuraci tri recov precis loss pool process especi improv method ad care design spatial dropout layer present novel network reus tional featur improv precis spatial local also approach suggest combin local part appear holist view part accur human pose estim exampl ouyang et al deriv deep model deep belief net dbn attempt take advantag three inform sourc human articul mixtur type appear score deform combin represent learn holist human bodi articul pattern hand fan et al propos convolut neutral network integr holist partial view cnn framework take part patch bodi che input combin local contextu inform accur pose estim scheme tend design new architectur carreira et al introduc model call iter error feedback ief thi model encompass rich structur input output space incorpor feedback show promis result trend challeng along promis perform deep learn ha achiev research literatur ha indic sever import challeng well inher trend describ next theoret understand although promis result address comput vision task achiev deep learn method lie theori well understood clear stand architectur perform better difﬁcult determin structur mani layer mani node layer proper certain task also need speciﬁc knowledg choos sensibl valu learn rate strength regular etc design architectur ha histor determin basi chu et al propos theoret method determin optim number featur map howev thi theoret method onli work extrem small recept ﬁeld better understand behavior cnn architectur zeiler et al develop visual techniqu gave insight function intermedi featur layer reveal featur interpret pattern brought possibl better architectur design similar visual wa also studi yu et al apart visual featur rcnn attempt discov learn pattern cnn test perform pattern dure train process found convolut layer learn gener featur convey cnn represent capac top layer addit ing cnn featur agraw et al investig effect commonli use strategi cnn perform provid back intuit appli cnn model comput vision problem despit progress achiev theori deep learn signiﬁc room better understand evolv optim cnn architectur toward improv desir properti invari class discrimin vision human vision ha remark proﬁcienc comput vision task even simpl visual represent chang geometr transform background variat occlus vision refer either bridg semant gap term accuraci bring new insight studi human brain integr machin learn tectur compar tradit featur cnn mimic human brain structur build activ featur studi aim evalu much retriev improv achiev develop deep learn techniqu whether deep featur desir key bridg semant gap long term seen fig imag classiﬁc error imagenet test set decreas thi promis improv veriﬁ efﬁcienc cnn particular result ha exceed accuraci human rater howev conclud represent perform cnn rival brain exampl easi produc imag complet unrecogniz human one cnn believ contain recogniz object conﬁdenc thi result highlight differ human vision current cnn model rais question gener cnn comput vision studi found like cortex recent cnn could gener similar featur space categori distinct one imag differ categori thi result indic cnn may provid insight stand primat visual process anoth studi author consid novel approach brain decod fmri data leverag unlabel data tempor cnn learn multipl layer tempor ﬁlter train power brain decod model whether cnn model reli comput mechan similar primat visual system yet determin ha potenti improv mimick incorpor primat visual system guo et al neurocomput train limit data larger model demonstr potenti capac becom tendenc recent develop howev shortag train data may limit size learn abil model especi expens obtain fulli label data overcom need enorm amount train data train larg network effect remain address current two commonli use solut obtain train data ﬁrst solut gener train data exist data base variou data augment scheme scale rotat crop top wu et al adopt color cast vignet len distort que could produc much convert train exampl broad coverag second solut collect train data weak learn algorithm recent ha rang articl learn visual concept imag search engin order scale comput vision recognit system zhou et al propos conceptlearn approach could automat learn thousand visual concept detector weakli label imag collect besid reduc labori bound box annot cost object detect mani supervis approach emerg senc label nevertheless promis develop techniqu gener collect comprehens train data could make network learn better featur robust variou chang geometr transform occlus time complex earli cnn seen method requir lot comput resourc candid applic one trend toward develop new tectur allow run cnn studi conduct seri experi constrain time cost propos model fast applic yet competit exist cnn model addit ﬁxing time complex also help understand impact factor depth number ﬁlter ﬁlter size etc anoth studi elimin redund comput forward backward propag cnn result speedup time ha robust ﬂexibl variou cnn model differ design structur reach high efﬁcienc becaus gpu implement ren et al convert key oper deep cnn vector form high parallel achiev given basic parallel oper provid uniﬁ framework vision applic power model deep learn relat algorithm move forward result variou comput vision task larg margin becom challeng make progress top might sever direct power model ﬁrst direct increas gener abil increas size network larger network could normal bring higher qualiti perform care taken address issu thi may caus overﬁt need lot comput resourc second direct combin inform multipl sourc featur fusion ha long popular appeal thi fusion categor two type combin ture layer network differ layer may learn differ featur promis could develop algorithm make featur layer plementari exampl deepindex propos integr multipl cnn featur multipl invert indic includ differ layer one model sever layer distinct model combin featur differ type obtain comprehens model integr type featur sift improv imag retriev perform deepembed use sift featur build invert index structur extract cnn featur local patch enhanc match strength third direct toward power model design speciﬁc deep network current almost base scheme adopt share network predict may distinct enough promis direct train speciﬁc deep network focu type object interest studi ha veriﬁ annot use annot object detect thi view kind speciﬁc deep network focus object rather whole imag anoth possibl solut train differ network differ categori instanc built intuit class equal difﬁcult distinguish true class label design initi coars classiﬁ cnn well sever ﬁne cnn adopt classiﬁc strategi achiev perform conclus thi paper present comprehens review deep learn develop categor scheme analyz exist deep learn literatur divid deep learn algorithm four categori accord basic model deriv convolut neural network restrict boltzmann machin autoencod spars code approach four class discuss analyz detail applic comput vision domain paper mainli report advanc cnn base scheme extens util suitabl imag notabl recent articl report inspir advanc show algorithm alreadi exceed accuraci human rater despit promis result report far niﬁcant room advanc exampl underli theoret foundat doe yet explain condit perform well outperform approach determin optim structur certain task thi paper describ challeng summar new trend design train deep neural network along sever direct may explor futur acknowledg thi work wa support leiden univers grant nation univers defens technolog grant nwo grant nvidia corpor grant refer bord glorot weston et al joint learn word mean represent semant pars proceed aistat ciresan meier schmidhub transfer learn latin chines charact deep neural network proceed ijcnn ren xu vector deep convolut neural network vision task proceed aaai guo et al neurocomput mikolov sutskev chen et distribut represent word phrase composition proceed nip ciresan meier schmidhub deep neural network imag classiﬁc proceed cvpr krizhevski sutskev hinton imagenet classiﬁc deep convolut neural network proceed nip bengio learn deep architectur ai found trendss mach learn deng tutori survey architectur algorithm applic deep learn apsipa tran signal inf process schmidhub deep learn neural network overview neural netw bengio deep learn represent look forward statist languag speech process springer berlin heidelberg bengio courvil vincent represent learn review new perspect pattern anal mach intel ieee tran lecun learn invari featur hierarchi proceed eccv workshop goroshin lecun satur proceed iclr li zhao wang highli efﬁcient forward backward propag convolut neural network pixelwis classiﬁc arxiv preprint arxiv erhan bengio courvil et whi doe unsupervis help deep learn mach learn lecun bottou bengio et learn appli document recognit proc ieee sun convolut neural network constrain time cost proceed cvpr zeiler hierarch convolut deep learn comput vision thesi new york univers szegedi liu jia et go deeper convolut ceed cvpr min lin qiang chen shuicheng yan network network proceed iclr boureau ponc lecun theoret analysi featur pool visual recognit proceed icml scherer müller behnk evalu pool oper volut architectur object recognit proceed icann cireşan meier masci et neural network visual object classiﬁc proceed ijcai zeiler fergu stochast pool regular deep volut neural network proceed iclr zhang ren et spatial pyramid pool deep convolut network visual recognit proceed eccv ouyang luo zeng et deform deep convolut neural network object detect proceed cvpr gong wang guo et orderless pool deep volut activ featur proceed eccv girshick donahu darrel et rich featur hierarchi accur object detect semant segment proceed cvpr oquab bottou laptev et learn transfer imag represent use convolut neural network proceed cvpr simonyan zisserman veri deep convolut network imag recognit proceed iclr zeng ouyang wang contextu deep learn pedestrian detect proceed iccv sun wang tang deep convolut network cascad facial point detect proceed cvpr miclut committe deep feedforward network train data pattern recognit springer intern publish pp weston ratl mobahi et deep learn via embed neural network trick trade springer berlin berg pp simonyan vedaldi zisserman deep fisher network imag classiﬁc proceed nip chen song huang et contextu object detect classiﬁc proceed cvpr hinton srivastava krizhevski et improv neural network prevent featur detector arxiv preprint arxiv baldi sadowski understand dropout proceed nip ba frey adapt dropout train deep neural network ceed nip mcallest tutori dropout bound arxiv preprint arxiv wager wang liang dropout train adapt regular proceed nip wang man fast dropout train proceed icml srivastava hinton krizhevski et dropout simpl way prevent neural network overﬁt mach learn goodfellow courvil et empir analysi dropout piecewis linear network proceed iclr wan l zeiler zhang et regular neural network use dropconnect proceed icml howard improv deep convolut neural network base imag classiﬁc arxiv preprint arxiv dosovitskiy springenberg brox unsupervis featur learn augment singl imag arxiv preprint arxiv hinton osindero teh fast learn algorithm deep belief net neural comput poultney chopra cun efﬁcient learn spars represent model proceed nip song lee jegelka et discoveri visual pattern conﬁgur proceed nip zeiler fergu visual understand convolut neural network proceed eccv hinton sejnowski learn relearn boltzmann machin mit press cambridg hinton contrast diverg learn proceed tenth intern workshop artiﬁci intellig statist np societi artiﬁci intellig statist pp hinton practic guid train restrict boltzmann machin momentum cho raiko ihler enhanc gradient adapt learn rate train restrict boltzmann machin proceed icml nair hinton rectiﬁ linear unit improv restrict boltzmann machin proceed icml arel rose karnowski deep machin new frontier artiﬁci intellig research research frontier comput intel mag ieee lee ekanadham ng spars deep belief net model visual area proceed nip nair hinton object recognit deep belief net ceed nip lee gross ranganath et convolut deep belief network scalabl unsupervis learn hierarch represent ing icml lee gross ranganath et unsupervis learn hierarch represent convolut deep belief network commun acm tang eliasmith deep network robust visual recognit ceed icml huang lee learn hierarch represent face veriﬁc convolut deep belief network proceed cvpr salakhutdinov hinton deep boltzmann machin proceed aistat salakhutdinov larochel efﬁcient learn deep boltzmann machin proceed aistat salakhutdinov hinton efﬁcient learn procedur deep boltzmann machin neural comput hinton salakhutdinov better way pretrain deep boltzmann machin proceed nip cho raiko ilin et pretrain algorithm deep boltzmann machin proceed icann montavon müller deep boltzmann machin center trick neural network trick trade springer berlin heidelberg pp goodfellow courvil bengio joint train deep boltzmann machin classiﬁc arxiv preprint arxiv goodfellow mirza courvil et deep boltzmann machin proceed nip ngiam chen koh et learn deep energi model ceed icml elfw uchib doya expect restrict boltzmann machin classiﬁc neural netw liou cheng liou et autoencod word comput hinton salakhutdinov reduc dimension data neural network scienc zhang kan et network cfan face align proceed eccv jiang zhang zhang et novel spars deep unsupervis learn proceed icaci zhou arpit nwogu et joint train better deep encod arxiv preprint arxiv goodfellow lee le et measur invari deep network proceed nip ngiam coat lahiri et optim method deep learn proceed icml zou ng yu unsupervis learn visual invari tempor coher proceed nip workshop guo et al neurocomput simoncelli e statist model photograph imag le build featur use larg scale unsupervis learn proceed icassp vincent larochel bengio et extract compos robust featur denois autoencod proceed icml vincent larochel lajoi et stack denois autoencod learn use represent deep network local denois criterion mach learn rifai vincent muller et contract explicit invari dure featur extract proceed icml alain bengio regular learn data gener distribut proceed iclr mesnil dauphin glorot et unsupervis transfer learn challeng deep learn approach proceed icml masci meier cireşan et stack convolut hierarch featur extract proceed icann baccouch mamalet wolf et convolut spars sequenc classiﬁc proceed bmvc leng guo zhang et object retriev stack local volut autoencod signal process memisev konda krueger autoencod beneﬁt featur proceed iclr olshausen field spars code overcomplet basi set strategi employ vi yu zhang gong nonlinear learn use local coordin code proceed nip raina battl lee et learn transfer learn unlabel data proceed icml wang yang yu et linear code imag classiﬁc proceed cvpr yang yu gong et linear spatial pyramid match use spars code imag classiﬁc proceed cvpr donoho larg underdetermin system linear equat minim solut also sparsest solut commun pure appl math censor parallel optim theori algorithm applic oxford univers press oxford unit kingdom rumelhart hinton william learn represent propag error natur lee battl raina et efﬁcient spars code algorithm ceed nip mairal bach ponc et onlin dictionari learn spars code proceed icml mairal bach ponc et onlin learn matrix factor spars code mach learn friedman hasti höﬂing et pathwis coordin optim ann appl stat gregor lecun learn fast approxim spars code ceed icml chambol de vore lee et nonlinear wavelet imag cess variat problem compress nois remov wavelet shrinkag imag process ieee tran beck teboul fast iter algorithm applic imag deblur proceed icassp kavukcuoglu ranzato lecun fast infer spars code algorithm applic object recognit arxiv preprint arxiv balasubramanian yu lebanon smooth spars code via margin regress learn spars represent proceed icml lazebnik schmid ponc beyond bag featur spatial pyramid match recogn natur scene categori proceed cvpr coat ng import encod versu train spars code vector quantiz proceed icml gao tsang chia et local featur spars code imag classiﬁc proceed cvpr gao tsang chia laplacian spars code hypergraph laplacian spars code applic pattern anal mach intel ieee tran yu lin lafferti learn imag represent pixel level via hierarch spars code proceed cvpr zeiler krishnan taylor et deconvolut network proceed cvpr zeil taylor fergu adapt deconvolut network mid high level featur learn proceed iccv zhou yu zhang et imag classiﬁc use code local imag descriptor proceed eccv lin lv zhu et imag classiﬁc fast featur extract svm train proceed cvpr kavukcuoglu wang et unsupervis featur learn deep spars code proceed sdm szegedi toshev erhan deep neural network object detect proceed nip agraw girshick malik analyz perform multilay neural network object recognit proceed eccv cadieu hong yamin et deep neural network rival represent primat cortex core visual object recognit plo comput biol nguyen yosinski clune deep neural network easili fool high conﬁdenc predict unrecogniz imag proceed cvpr firat aksan oztekin et learn deep tempor represent brain decod arxiv preprint arxiv chen shrivastava gupta neil extract visual knowledg web data proceed iccv divvala farhadi guestrin learn everyth anyth visual concept learn proceed cvpr zhou jagadeesh piramuthu conceptlearn discov visual concept weakli label imag collect proceed cvpr master larg scale object detect czech technic univers csurka danc fan et visual categor bag keypoint proceed eccv workshop boser guyon vapnik train algorithm optim margin classiﬁ proceed fifth annual workshop comput learn theori acm dalal trigg histogram orient gradient human detect proceed cvpr wang han yan human detector partial occlus handl proceed iccv perronnin sánchez mensink improv ﬁsher kernel scale imag classiﬁc proceed eccv jaakkola haussler exploit gener model discrimin classiﬁ proceed nip deng dong socher et imagenet hierarch imag databas proceed cvpr noh hong han learn deconvolut network semant mentat proceed iccv hariharan arbeláez girshick et hypercolumn object mentat local proceed cvpr mostajabi yadollahpour shakhnarovich feedforward semant segment featur proceed cvpr chu krzyżak analysi featur map select supervis learn use convolut neural network advanc artiﬁci intellig springer intern publish pp yu yang bai et visual compar convolut neural network arxiv preprint arxiv hoffman guadarrama tzeng et lsda larg scale detect adapt proceed nip hoffman guadarrama tzeng et object classiﬁ object detector adapt approach chen papandr kokkino et semant imag segment deep convolut net fulli connect crf proceed iclr sermanet eigen zhang et overfeat integr recognit local detect use convolut network proceed iclr long shelham darrel fulli convolut network semant segment proceed cvpr erhan szegedi toshev et scalabl object detect use deep neural network proceed cvpr dai sun convolut featur mask joint object stuff segment proceed cvpr liu guo wu et deep index accur efﬁcient imag retriev proceed icmr alex desela ferrari measur object imag window pattern anal mach intel ieee tran uijl van de sand gever et select search object recognit int comput vi endr hoiem categori independ object propos proceed eccv cheng zhang lin et bing binar norm gradient object estim proceed cvpr zitnick dollár edg box locat object propos edg proceed eccv hosang benenson schiel good detect propos realli proceed bmvc liu guo wu lew deepindex accur efﬁcient imag retriev proceed icmr zheng wang tian see big pictur deep embed contextu evid arxiv preprint arxiv yan jagadeesh decost et hierarch deep volut neural network imag classiﬁc proceed iccv wu yan et deep imag scale imag recognit arxiv preprint arxiv ngiam chen chia et tile convolut neural network proceed nip guo et al neurocomput youn converg markovian stochast algorithm rapidli decreas ergod rate stoch int probab stoch process zhang ren et delv deep rectiﬁ surpass level perform imagenet classiﬁc proceed iccv ioff szegedi batch normal acceler deep network train reduc intern covari shift proceed nip hariharan arbeláez girshick et simultan detect segment proceed eccv razavian azizpour sullivan carlsson cnn featur astound baselin recognit proceed cvpr shop wan wang hoi et deep learn imag retriev comprehens studi proceed multimedia yosinski clune bengio et transfer featur deep neural network proceed nip eslami heess winn shape boltzmann machin strong model object shape proceed cvpr kae sohn lee et augment crf boltzmann machin shape prior imag label proceed cvpr dahl ranzato moham et phone recognit restrict boltzmann machin proceed nip sun zhou li et search featur imag retriev proceed icimc babenko slesarev chigorin et neural code imag retriev proceed eccv oquab bottou laptev et object local free supervis learn convolut neural network proceed cvpr srivastava salakhutdinov multimod learn deep boltzmann machin proceed nip wang distribut optim deepli nest system proceed aistat felzenszwalb girshick mcallest et object detect discrimin train model pattern anal mach intel ieee tran girshick fast proceed iccv ren girshick et faster toward object detect region propos network proceed nip redmon divvala girshick et onli look onc uniﬁ object detect arxiv preprint arxiv dai hoiem learn local detect object proceed cvpr hoiem chodpathumwan dai diagnos error object detector proceed eccv dong chen yan et toward uniﬁ object detect semant segment proceed eccv zhu urtasun salakhutdinov et segdeepm exploit tation context deep neural network object detect ceed cvpr gidari komodaki object detect via semant cnn model proceed iccv zhang sohn villega et improv object detect deep convolut network via bayesian optim structur predict proceed cvpr ren girshick et object detect network convolut featur map arxiv preprint arxiv liang liu wei et toward comput babi learn supervis approach object detect proceed iccv xie tu edg detect proceed iccv russakovski deng su et imagenet larg scale visual recognit challeng int comput vi wang zhang lin et deep joint task learn gener object extract proceed nip yoo park lee et pyramid pool deep volut represent proceed cvpr workshop jain tompson lecun et modeep deep learn framework use motion featur human pose estim proceed accv pﬁster simonyan charl et deep convolut neural network efﬁcient pose estim gestur video proceed accv pﬁster charl zisserman flow convnet human pose mation video proceed iccv yu guo tao et human pose recoveri supervis spectral embed neurocomput felzenszwalb huttenloch et pictori structur object recognit int comput vi tian zitnick narasimhan explor spatial hierarchi mixtur model human pose estim proceed eccv wang li beyond physic connect tree model human pose estim proceed cvpr pishchulin andriluka gehler et poselet condit pictori structur proceed cvpr danton gall leistner et human pose estim use bodi part depend joint regressor proceed cvpr sapp taskar modec multimod decompos model human pose estim proceed cvpr johnson everingham cluster pose nonlinear appear model human pose estim proceed bmvc eichner zisserman et articul human pose estim retriev almost unconstrain still imag int comput vi toshev szegedi deeppos human pose estim via deep neural network proceed cvpr chen yuill articul pose estim graphic model imag depend pairwis relat proceed nip jain tompson andriluka et learn human pose estim featur convolut network proceed iclr tompson jain lecun et joint train convolut network graphic model human pose estim proceed nip tompson goroshin jain et efﬁcient object local use convolut network proceed cvpr ouyang chu wang deep learn human pose estim proceed cvpr fan zheng lin et combin local appear holist view deep neural network human pose estim ing cvpr carreira agraw fragkiadaki et human pose estim iter error feedback arxiv preprint arxiv huang boyer ilic robust human bodi shape pose track proceed lin shen reid et efﬁcient piecewis train deep structur model semant segment arxiv preprint arxiv zheng jayasumana et condit random ﬁeld recurr neural network proceed iccv papandr chen murphi et learn dcnn semant imag segment proceed iccv dai sun boxsup exploit bound box supervis convolut network semant segment proceed iccv yanm guo receiv degre inform system engin degre oper research nation univers defens technolog changsha china respect current leiden institut advanc comput scienc liac leiden versiti hi current research interest includ imag classiﬁc object detect imag retriev yu liu receiv degre degre school softwar technolog dalian univers technolog dalian china tive current leiden institut advanc comput scienc liac leiden univers hi current research interest includ semant mentat imag retriev ard oerleman receiv degre degre leiden univers respect ha ever softwar engin senior softwar engin lead softwar engin vdg secur well comput vision engin prime vision current comput vision engin vdg secur hi current research interest includ video analysi video imag retriev interest point detect visual concept detect also interest logic motiv comput vision techniqu optim approach larg scale imag video analysi guo et al neurocomput songyang lao receiv degre inform system engin degre system engin nation univers defens technolog changsha china respect current professor school inform system manag wa ing scholar dublin citi univers irish hi current research interest includ imag process video analysi comput interact song wu receiv degre degre comput scienc southwest univers chongq china respect current leiden institut advanc comput scienc liac leiden univers netherland hi current research interest includ imag match imag retriev classiﬁc michael lew imageri media research cluster liac director liac media lab receiv hi doctor univers illinoi becam postdoctor research leiden univers one year later becam ﬁrst leiden univers fellow wa pilot program tenur track professor becam tenur associ professor leiden univers wa invit serv chair full professor comput scienc tsinghua versiti mit china ha publish peer review paper three best paper citat area comput vision retriev machin learn current septemb ha cite paper histori acm transact multimedia addit ha cite paper acm intern confer multimedia inform retriev mir also acm mir ha serv organ committe dozen acm ieee confer serv found chair acm icmr steer committe serv chair acm mir acm civr steer committe addit intern journal multimedia inform retriev springer member acm sigmm execut board highest inﬂuenti committe sigmm guo et al neurocomput