deep learning visual understanding review yanming guo c yu liu ard oerlemans b songyang lao c song wu michael lew n liacs medium lab leiden university niels bohrweg 1 leiden netherlands b vdg security bv zoetermeer netherlands c college information system management national university defense technology changsha china r c l e n f article history received 30 april 2015 received revised form 18 september 2015 accepted 22 september 2015 available online 26 november 2015 keywords deep learning computer vision development application trend challenge b r c deep learning algorithm subset machine learning algorithm aim discovering multiple level distributed representation recently numerous deep learning algorithm proposed solve traditional artiﬁcial intelligence problem work aim review art deep learning algorithm computer vision highlighting contribution challenge 210 recent research paper ﬁrst give overview various deep learning approach recent development brieﬂy describes application diverse vision task image classiﬁcation object detection image retrieval semantic segmentation human pose estimation finally paper summarizes future trend challenge designing training deep neural network 2015 elsevier right reserved introduction deep learning subﬁeld machine learning attempt learn abstraction data utilizing hierarchical architecture emerging approach ha widely applied traditional artiﬁcial intelligence domain semantic parsing 1 transfer learning natural language processing 4 computer vision many mainly three important reason booming deep learning today dramatically increased chip processing ability gpu unit signiﬁcantly lowered cost computing hardware considerable advance machine learning algorithm 9 various deep learning approach extensively reviewed discussed recent year among schmidhuber et al 10 emphasized important inspiration technical contribution historical timeline format bengio 11 examined challenge deep learning research proposed research direction deep work shown successful computer vision task extract appropriate feature jointly forming discrimination recent imagenet large scale visual recognition challenge ilsvrc competition 189 deep learning method widely adopted different researcher achieved top accuracy score 7 survey intended useful general neural puting computer vision multimedia researcher interested deep learning computer vision provides overview various deep learning algorithm application especially applied computer vision domain remainder paper organized follows section 2 divide deep learning algorithm four category convolutional neural network restricted boltzmann machine autoencoder sparse coding some model category well development ted also describe contribution limitation model section section 3 describe achievement deep learning scheme various computer vision application image classiﬁcation object detection image retrieval semantic segmentation human pose estimation result application shown compared pipeline commonly used datasets section 4 along success deep learning method achieved also face several challenge designing training deep network section summarize some major challenge deep learning together inherent trend might developed future section 5 conclude paper content list available sciencedirect journal homepage neurocomputing 2015 elsevier right reserved n corresponding author address guo liu oerlemans laosongyang lao wu lew neurocomputing 187 2016 method recent development recent year deep learning ha extensively studied ﬁeld computer vision consequence large number related approach emerged generally method divided four category according basic method derived convolutional neural network cnns restricted boltzmann machine rbms autoencoder sparse coding categorization deep learning method along some representative work shown fig next four part brieﬂy review deep learning method recent development convolutional neural network cnns convolutional neural network cnn one notable deep learning approach multiple layer trained robust manner 17 ha found highly effective also commonly used diverse computer vision application pipeline general cnn architecture shown fig generally cnn consists three main neural layer convolutional layer pooling layer fully connected layer different kind layer play different role fig 2 general cnn architecture image classiﬁcation 6 shown layer two stage training network forward stage backward stage first main goal forward stage represent input image current parameter weight bias layer prediction output used compute loss cost ground truth label ond based loss cost backward stage computes gradient parameter chain rule parameter updated based gradient prepared next forward computation sufﬁcient iteration forward backward stage network learning stopped next ﬁrst introduce function along recent development layer summarize commonly used training strategy network finally present eral cnn model derived model conclude current tendency using model real application type layer generally cnn hierarchical neural network whose volutional layer alternate pooling layer followed some fully connected layer see fig 2 section present function three layer brieﬂy review recent advance appeared research layer convolutional layer convolutional layer cnn utilizes various kernel convolve whole image well intermediate feature map generating various feature map shown fig three main advantage convolution operation 19 1 weight sharing mechanism feature map reduces number parameter 2 local connectivity learns correlation among neighboring pixel 3 invariance tion object due beneﬁts introduced convolution operation some research paper use replacement fully connected layer accelerate learning process one interesting approach handling convolutional layer network network nin 21 method main idea substitute conventional convolutional layer small multilayer perceptron consisting multiple fully connected layer nonlinear activation function thereby replacing linear ﬁlters nonlinear neural network method achieves good result image classiﬁcation pooling layer generally pooling layer follows volutional layer used reduce dimension feature map network parameter similar convolutional layer pooling layer also translation invariant computation take neighboring pixel account average pooling max pooling commonly used strategy fig 4 give example max pooling process 8 8 feature map output map reduce 4 4 dimension max pooling operator ha size 2 2 stride max pooling average pooling boureau et al 22 vided detailed theoretical analysis performance scherer et al 23 conducted comparison two pooling operation found lead faster convergence select superior invariant feature improve eralization recent year various fast gpu implementation cnn variant presented utilize strategy pooling layer extensively studied among three layer three approach related pooling layer different purpose stochastic pooling drawback max pooling sensitive overﬁt training set making hard generalize well test sample 19 aiming solve problem zeiler et al 25 proposed stochastic pooling approach replaces conventional deterministic pooling operation stochastic procedure randomly picking activation within pooling region according multinomial distribution equivalent standard max pooling many copy input image small local deformation stochastic nature helpful prevent overﬁtting problem spatial pyramid pooling spp normally method require input image restriction may reduce recognition accuracy image arbitrary size eliminate limitation et al 26 utilized general cnn architecture replaced last pooling layer spatial pyramid pooling layer spatial pyramid pooling extract representation arbitrary image region generating ﬂexible solution handling different scale size aspect ratio applied any cnn structure boost performance structure deep learning method method method method sparse method deep belief network 49 deep boltzmann machine 65 deep energy model 73 sparse autoencoder 50 denoising autoencoder 85 contractive autoencoder 87 sparse coding spm 98 laplacian sparse coding 113 local coordinate coding 95 coding 118 alexnet 6 clarifai 52 vgg 31 googlenet 20 spp 26 fig categorization deep learning method representative work guo et al neurocomputing 187 2016 28 handling deformation fundamental challenge computer vision especially object recognition task max pooling average pooling useful handling deformation not able learn deformation constraint geometric model object part deal deformation efﬁciently ouyang et al 27 introduced new deformation constrained pooling layer called layer enrich deep model learning deformation visual pattern substitute traditional layer any information abstraction level different purpose procedure pooling strategy designed various pooling strategy could combined boost performance cnn layer following last pooling layer network seen fig 2 several layer converting feature map feature vector feature representation seen fig layer perform like traditional neural work contain 90 parameter cnn enables u feed forward neural network vector deﬁned length could either feed forward vector tain number category image classiﬁcation 6 take feature vector processing 29 changing structure layer mon however example came transferred learning approach 30 preserved parameter learned genet 6 replaced last layer two new layer adapt new visual recognition task drawback layer contain many meter result large computational effort training therefore promising commonly applied direction remove layer decrease connection certain method example googlenet 20 designed deep wide network keeping computational budget constant switching fully connected sparsely connected architecture training strategy compared shallow learning advantage deep learning build deep architecture learn abstract information however large amount parameter introduced may also lead another problem overﬁtting recently numerous regularization method emerged defense overﬁtting including stochastic pooling mentioned section introduce several regularization technique may inﬂuence training performance dropout dropconnect dropout wa proposed ton et al 38 explained baldi et al 39 training case algorithm randomly omit half feature detector order prevent complex training data enhance generalization ability method wa improved speciﬁcally research convolution max pooling convolutional layer pooling layer fully connected layer cat dog person bird fish fox fig pipeline general cnn architecture kernel input output fig operation convolutional layer feature map output map max fig operation max pooling layer fig operation layer guo et al neurocomputing 187 2016 29 et al 45 analyzed efﬁcacy dropout suggested dropout extremely effective ensemble ing method one generalization derived dropout called dropconnect 46 randomly drop weight rather activation experiment showed achieve competitive even better result variety standard benchmark although slightly slower fig 6 give comparison dropout dropconnect network 46 data augmentation cnn applied visual object recognition data augmentation often utilized generate additional data without introducing extra labeling cost known alexnet 6 employed two distinct form data mentation ﬁrst form data augmentation consists erating image translation horizontal reﬂections second form consists altering intensity rgb nels training image howard et al 47 took alexnet base model added additional transformation improved translation invariance color invariance extending image crop extra pixel adding additional color manipulation data augmentation method wa widely utilized some recent study dosovitskiy et al 48 proposed unsupervised feature learning approach based data tation ﬁrst randomly sampled set image patch declares surrogate class extended class applying transformation corresponding translation scale color contrast finally trained cnn discriminate surrogate class feature learnt work showed good result variety classiﬁcation task aside classical method scaling rotating cropping wu et al 159 adopted color casting vignetting lens distortion technique produced training example broad coverage mean alize network parameter rather domly set parameter quite popular model based cnns due advantage accelerate learning process well improve generalization ability erhan et al 16 ha conducted extensive simulation existing algorithm ﬁnd network work better network trained traditional way alexnet 6 achieved excellent performance released public numerous approach choose net trained baseline deep model use parameter according speciﬁc task nevertheless approach deliver better performance training model clarifai 52 googlenet 20 vgg 31 crucial stage reﬁning model adapt speciﬁc task datasets general requires class label new training dataset used computing loss function case layer new model initialized based model alexnet 6 except last output layer depends number class label new dataset therefore randomly initialized however some occasion difﬁcult obtain class label any new dataset address problem similarity learning objective function wa proposed used loss function without class label 166 propagation work normally allow model reﬁned layer layer also many research result describing transfer model efﬁciently new way deﬁned quantify degree particular layer general speciﬁc 167 namely well feature layer transfer one task another concluded initializing network transferred feature almost any number layer give boost generalization performance tuning new dataset addition regularization method described also common method weight decay weight tying many 10 weight decay work adding extra term cost function penalize parameter preventing exactly modeling training data therefore helping generalize new example 6 weight tying allows model learn good representation input data reducing number parameter convolutional neural work 160 another interesting thing note regularization technique training not mutually exclusive combined boost performance cnn architecture recent development cnn scheme puter vision domain some cnn model emerged section ﬁrst describe commonly used cnn model summarize characteristic application conﬁgurations primary contribution several typical cnn model listed table alexnet 6 signiﬁcant cnn architecture consists ﬁve convolutional layer three fully connected layer inputting one 224 224 image network would repeatedly convolve pool activation forward result layer network wa trained imagenet integrated various regularization technique data augmentation dropout etc alexnet competition 189 set tone surge interest deep convolutional neural network architecture nevertheless two major drawback model 1 requires ﬁxed resolution image 2 no clear understanding performs well 2013 zeiler et al 52 introduced novel visualization que give insight inner working intermediate feature network dropout network dropconnect network w f wv w f f wv 1 1 0 w f w v fig comparison dropout dropconnect network 46 network b dropout network c dropconnect network guo et al neurocomputing 187 2016 30 layer visualization enabled ﬁnd architecture outperform alexnet 6 imagenet classiﬁcation benchmark resulting model clarifai received top performance competition requirement ﬁxed resolution et al 26 proposed new pooling strategy spatial pyramid pooling eliminate restriction image size resulting could boost accuracy variety published cnn tectures despite different design addition commonly used conﬁguration cnn structure ﬁve convolutional layer plus three fully connected layer also approach trying explore deeper work contrast alexnet vgg 31 increased depth network adding convolutional layer taking tage small convolutional ﬁlters layer similarly szegedy et al 20 proposed model googlenet also ha quite deep structure 22 layer ha achieved leading formance competition 189 despite classiﬁcation performance achieved various model model application not limited only image classiﬁcation based model new framework derived address challenging task object detection semantic segmentation etc two derived framework rcnn region cnn feature 29 fcn fully convolutional network 146 mainly designed object detection semantic mentation respectively shown fig core idea rcnn generate multiple object proposal extract feature proposal using cnn classify candidate window linear svm recognition using region paradigm received encouraging formance object detection ha gradually become eral pipeline recent promising object detection algorithm however performance rcnn relies much precision object location may limit robustness besides generation processing large ber proposal would also decrease efﬁciency recent opments mainly focused two aspect rcnn take cnn model feature extractor doe not make any change network contrast fcn proposes technique recast cnn model fully convolutional net recasting technique remove restriction image tion could produce output efﬁciently although fcn proposed mainly semantic segmentation technique could also utilized application image classiﬁcation 191 edge detection 188 etc aside creating various model usage model also demonstrates several characteristic large network one intuitive idea improve formance cnns increasing size includes increasing depth number level width number unit level 20 googlenet 20 vgg 31 described adopted quite large network 22 layer 19 layer respectively demonstrating increasing size beneﬁcial image recognition accuracy jointly training multiple network could lead better mance single one also many researcher designed large network combining different deep structure cascade mode output former network utilized latter one shown fig cascade architecture utilized handle different task function prior network output may vary task example wang et al 190 connected two network object extraction ﬁrst network used object localization therefore output corresponding coordinate object sun et al 33 proposed carefully designed convolutional network detect facial point ﬁrst level provides highly robust initial estimation following two level initial prediction similarly ouyang et al 27 adopted training scheme proposed zeng et al 32 classiﬁers previous stage jointly work classiﬁers current stage deal misclassiﬁed sample multiple network another tendency current tions combine result multiple network network execute task independently instead designing single architecture jointly training network inside execute task seen fig miclut et al 34 gave some insight erate ﬁnal result received set score prior alexnet 6 ciresan et al 5 proposed method called dnn mcdnn combine several dnn umns average prediction model achieved table 1 cnn model achievement ilsvrc classiﬁcation competition method year place conﬁguration contribution alexnet 6 2012 five convolutional layersþthree fully connected layer important cnn architecture set tone many computer vision researcher clarifai 52 2013 five convolutional layersþthree fully connected layer insight function intermediate feature layer spp 26 2014 five convolutional layersþthree fully connected layer proposed spatial pyramid pooling remove requirement image resolution vgg 31 2014 convolutional layersþthree fully nected layer thorough evaluation network increasing depth googlenet 20 2014 convolutional layersþone fully connected layer increased depth width without raising computational requirement basic model derived model alexnet 6 clarifai 52 26 vgg 31 googlenet 20 image classification rcnn 29 fcn 147 object detection semantic segmentation fig cnn basic model derived model guo et al neurocomputing 187 2016 31 competitive result task recognition written digit trafﬁc sign recently ouyang et al 27 also conducted experiment evaluate performance model combination strategy learnt 10 model different setting combined averaging scheme result show model generated way high diversity plementary improving detection result diverse network aside altering cnn structure some researcher also attempt introduce information source combining shallow structure grating contextual information illustrated fig shallow method give additional insight problem literature example found combining shallow method deep learning framework 35 take deep learning method extract feature input feature shallow learning method svm one sentative successful algorithm rcnn method 29 feed highly distinctive cnn feature svm ﬁnal object detection task besides deep cnns fisher vector fv complementary 36 also combined signiﬁcantly improve accuracy image classiﬁcation contextual information sometimes available object detection task possible integrate global context mation information bounding box genet large scale visual recognition challenge 2014 ilsvrc 2014 winning team nu concatenated raw detection score combined output traditional classiﬁcation framework context reﬁnement 37 similarly ouyang et al 27 also took image classiﬁcation score contextual feature object detection restricted boltzmann machine rbms restricted boltzmann machine rbm generative chastic neural network wa proposed hinton et al 1986 53 rbm variant boltzmann machine restriction visible unit hidden unit must form bipartite graph restriction allows efﬁcient training algorithm particular contrastive divergence algorithm 54 since model bipartite graph hidden unit h visible unit conditionally independent therefore p ð þ p ð þp ð ð þ equation h satisfy boltzmann distribution given input get h p ð similarly ﬁgure p ð adjusting parameter minimize difference resulting h act good feature hinton 55 gave detailed explanation provided tical way train rbms work 56 discussed main difﬁculties training rbms underlying reason posed new algorithm consists adaptive learning rate enhanced gradient address difﬁculties development rbm found 57 model approximates binary unit noisy rectiﬁed linear unit preserve information relative intensity mation travel multiple layer feature detector reﬁnement not only function well model also widely employed various approach utilizing rbms learning module compose lowing deep model deep belief network dbns deep mann machine dbms deep energy model dems comparison three model shown fig dbns undirected connection top two layer form rbm directed connection lower layer dbms undirected connection layer network dems deterministic hidden unit lower layer stochastic hidden unit top hidden layer 73 summary three deep model along related representative reference provided table next section explain model describe application computer vision task respectively deep belief network dbns deep belief network dbn proposed hinton 21 wa signiﬁcant advance deep learning probabilistic generative model provides joint probability distribution observable data label dbn ﬁrst take advantage efﬁcient layer greedy learning strategy initialize deep network weight jointly desired output greedy learning procedure ha two main advantage 58 1 generates proper initialization network addressing ﬁculty parameter selection may result poor local optimum some extent 2 learning procedure unsupervised network 1 network 2 estimation network k input fig combining deep structure cascade mode network 1 network 2 estimation 1 estimation 2 estimation k network k estimation input input input fig combining result multiple network network contextual information shallow structure estimation input fig combining deep network information source guo et al neurocomputing 187 2016 32 requires no class label remove necessity labeled data training however creating dbn model computationally expensive task involves training several rbms not clear approximate training mize model 12 dbns successfully focused researcher attention deep learning consequence many variant created 62 nair et al 60 developed modiﬁed dbn model utilizes boltzmann machine object recognition model 59 learned model ural image using sparse rbms ﬁrst layer learns local oriented edge ﬁlters second layer capture variety contour feature well corner junction improve robustness occlusion random noise lee et al 63 applied two strategy one take advantage sparse nections ﬁrst layer dbn regularize model develop probabilistic algorithm applied computer vision task drawback dbns not consider structure input image address problem convolutional deep belief network cdbn wa introduced 61 cdbn utilized spatial information neighboring pixel introducing convolutional rbms erating translation invariant generative model scale well high dimensional image algorithm wa extended 64 achieved excellent performance face veriﬁcation deep boltzmann machine dbms deep boltzmann machine dbm proposed khutdinov et al 65 another deep learning algorithm unit arranged layer compared dbns whose top two layer form undirected graphical model whose lower layer form directed generative model dbm ha undirected connection across structure like rbm dbm also subset boltzmann family difference dbm posse multiple layer hidden unit unit layer conditionally independent layer vice versa given visible unit calculating posterior distribution hidden unit no longer tractable resulting interaction hidden unit training network dbm would jointly train layer speciﬁc unsupervised model instead maximizing likelihood directly dbm us stochastic maximum likelihood sml 161 based algorithm maximize lower bound likelihood performing only one update using markov chain monte carlo mcmc method parameter update avoid ending poor local minimum leave many hidden unit effectively dead greedy training strategy also added layer fig comparison three model 73 dbn b dbm c dem table 2 overview representative method method characteristic advantage drawback reference dbn 49 undirected connection top two layer directed connection lower layer properly initializes network prevents poor local optimum some extent training unsupervised remove necessity labeled data training due initialization process computationally expensive create dbn model lee et al nair et al 60 dbm 65 undirected connection layer network deal robustly ambiguous input incorporating feedback joint optimization hinton et al 68 cho et al 69 montavon et al 70 goodfellow srivastava et al 174 dem 73 deterministic hidden unit lower layer stochastic hidden unit top hidden layer produce better generative model allowing lower layer adapt training higher layer learnt initial weight may not good convergence carreira et al 175 elfwing et al 74 guo et al neurocomputing 187 2016 33 dbm network much way dbn 12 joint learning ha brought promising improvement term likelihood classiﬁcation performance deep feature learner however crucial disadvantage dbms time complexity approximate inference considerably higher dbns make joint optimization dbm parameter impractical large datasets increase ciency dbms some researcher introduced approximate inference algorithm utilizes separate tion model initialize value latent variable layer thus effectively accelerating inference also many approach aim improve effectiveness dbms improvement either take place stage training stage example montavon et al 70 introduced centering trick improve stability dbm made criminative generative training scheme 72 wa utilized jointly train dbm outperforms previous method image classiﬁcation proposed 71 deep energy model dems deep energy model dem introduced ngiam et al 73 recent approach train deep architecture unlike dbns dbms share property multiple stochastic hidden layer dem ha single layer stochastic hidden unit efﬁcient training inference model utilizes deep feed forward neural network model energy landscape able train layer simultaneously evaluating performance natural image demonstrated joint training multiple layer yield qualitative tative improvement greedy training ngiam et al 73 used hybrid monte carlo hmc train model also option including contrastive divergence score ing others similar work found 74 although rbms not suitable cnns vision tions also some good example adopting rbms vision task shape boltzmann machine wa proposed eslami et al 168 handle task modeling binary shape image learns high quality probability distribution object shape realism sample distribution generalization new example shape class kae et al 169 combined crf rbm model local global structure face segmentation ha consistently reduced error face labeling new deep architecture ha presented phone recognition 170 combine rbm feature extraction module standard dbn approach attack representational inefﬁciency issue gmms important limitation previous work applying dbns phone recognition autoencoder autoencoder special type artiﬁcial neural network used learning efﬁcient encoding 75 instead training network predict some target value given input x encoder trained reconstruct input x therefore output vector dimensionality input vector general process autoencoder shown fig 12 process autoencoder optimized ing reconstruction error corresponding code learned feature generally single layer not able get discriminative representative feature raw data researcher utilize deep autoencoder forward code learnt vious autoencoder next accomplish task deep autoencoder wa ﬁrst proposed hinton et al 76 still extensively studied recent paper deep autoencoder often trained variant conjugate gradient method though often reasonably effective model could become quite ineffective error present ﬁrst layer may cause network learn reconstruct average training data proper approach remove problem network initial weight approximate ﬁnal solution 76 also variant autoencoder proposed make representation constant possible respect change input table 3 list some variant autoencoder brieﬂy summarize characteristic advantage next section describe three important variant sparse encoder denoising autoencoder contractive autoencoder sparse autoencoder sparse autoencoder aim extract sparse feature raw data sparsity representation either achieved penalizing hidden unit bias directly lizing output hidden unit activation sparse representation several potential advantage 50 1 using representation increase likelihood different category easily separable theory svms 2 sparse representation provide u ple interpretation complex input data term number part 3 biological vision us sparse representation early visual area 83 quite variant sparse autoencoder layer locally connected sparse autoencoder pooling local contrast normalization 84 model allows system train face detector without label image containing face not resulting feature detector robust translation scaling rotation denoising autoencoder order increase robustness model vincent posed model called denoising autoencoder dae recover correct input corrupted version thus cing model capture structure input distribution process dae shown fig 13 contractive autoencoder contractive autoencoder cae proposed rifai et al 87 followed dae shared similar motivation learning robust representation 12 dae make whole ping robust injecting noise training set cae achieves robustness adding analytic contractive penalty reconstruction error function although notable difference dae cae stated bengio et al 12 alain et al 88 suggested dae form cae closely related dae small corruption noise valued type cae tractive penalty whole reconstruction function rather encoder dae cae successfully used unsupervised transfer learning challenge 89 encoder code decoder reconstruction error input fig pipeline autoencoder guo et al neurocomputing 187 2016 34 sparse coding purpose sparse coding learn set basic function describe input data 94 numerous advantage sparse coding 1 reconstruct descriptor better using multiple base capturing lations similar descriptor share base 2 sparsity allows representation capture salient property image 3 line biological visual system argues sparse feature signal useful learning 4 image statistic study show image patch sparse nals 5 pattern sparse feature linearly separable solving sparse coding equation subsection brieﬂy describe solve sparse coding problem get sparse representation general objective function sparse coding min 1 x 1 min hðtþ 1 2 xðtþ 2 2 þλ 1 ﬁrst term function reconstruction error dhðtþ reconstruction xðtþ second regularization term sparsity penalty norm regularization ha iﬁed lead sparse representation 99 eq 2 solved regression method called lasso least absolute shrinkage selection operator not get analytic solution sparse representation therefore solving problem normally result intractable computation optimize sparse coding model alternating cedure updating weight inferring feature activation h input given current setting weight weight update one commonly used algorithm updating weight called projected gradient algorithm 100 renormalizes column weight matrix right update traditional gradient descent algorithm 101 normalization necessary sparsity penalty any effect however gradient descent using iterative projection often show slow convergence 2007 lee et al 102 derived lagrange dual method much efﬁcient od given dictionary paper proposed search algorithm learn sparse representation tion two algorithm enabled performance signiﬁcantly better previous one however not efﬁciently handle large training set dynamic training data changing time thus inherently access whole training set iteration address issue online approach wa proposed learning dictionary process one element small subset training set time algorithm update dictionary using coordinate descent 105 warm restarts doe not require any learning rate tuning gregor et al 106 tried accelerate dictionary learning another way import idea coordinate descent algorithm cod only update promising hidden unit therefore lead dramatic reduction number iteration reach given code prediction error activation inference given set weight need infer feature vations popular algorithm sparse coding inference iterative algorithm ista 107 take gradient step optimize reconstruction term followed sparsity term ha closed form shrinkage operation although simple effective algorithm suffers severe problem converges quite slowly problem partly solved fast iterative algorithm fista approach 108 preserve computational plicity ista converges quickly due introduction momentum term dynamic convergence plexity changed ista fista inference involve some sort iterative optimization lasso high computational complexity contrast cuoglu et al 109 utilized network approximate sparse code dramatically accelerated inference process furthermore lasso optimization stage wa replaced marginal regression 110 effectively scaling sparse coding framework large dictionary development brieﬂy stated generate sparse sentation given objective function subsection introduce some algorithm related sparse coding table 3 variant autoencoder method characteristic advantage sparse add sparsity penalty force representation sparse make category separable make complex data meaningful line biological vision system autoencoder denoising recovers correct input corrupted version robust noise autoencoder contractive add analytic contractive penalty reconstruction error function better capture local direction variation dictated data autoencoder 87 saturating raise reconstruction error input not near data manifold limit ability reconstruct input not near data manifold autoencoder 14 convolutional share weight among location input preserving spatial locality utilizes image structure autoencoder utilizes proper shrinkage function train autoencoders without additional regularization powerful learning representation data high intrinsic dimensionality autoencoder 93 input corrupted input reconstruction hidden node reconstruct error fig denoising autoencoder 85 guo et al neurocomputing 187 2016 35 particular used computer vision task sparse coding algorithm relation along contribution drawback shown fig one representative algorithm sparse coding called sparse coding spm scspm 98 extension spatial pyramid matching spm method 111 unlike spm us vector quantization vq image representation scspm utilizes sparse coding sc followed spatial max pooling codebook sc basis feature activate small number compared vq sc receives much lower reconstruction error due le restrictive constraint assignment coates et al 112 investigated reason success sc vq detail drawback scspm deal local feature separately thus ignores mutual dependence among make sensitive feature variance sparse code may vary lot even similar feature address problem gao et al 113 proposed laplacian sparse coding lsc approach similar feature not only assigned cluster center also guarantee selected cluster center similar ence sparse coding laplacian sparse ing shown fig adding locality preserving constraint objective sparse coding lsc keep mutual dependency sparse coding procedure gao et al 114 raised graph laplacian sparse coding hlsc method extends lsc case similarity among instance deﬁned hyper graph lsc hlsc enhance robustness sparse coding another way address sensitivity problem hierarchical sparse coding method proposed yu et al 115 introduced layer sparse coding model ﬁrst layer encodes individual patch second layer jointly encodes set patch belong group therefore model leverage spatial borhood structure modeling dependency patch local region image besides fully automatic method learn feature pixel level rather example sift feature hierarchical sparse coding utilized another research 116 learn feature image unsupervised fashion model improved zeiler et al 117 addition sensitivity another method exists improving scspm algorithm considering locality yu et al 95 observed scspm result tend local nonzero cients often assigned base nearby result vations suggested modiﬁcation scspm called local dinate coding lcc explicitly encourages coding local also theoretically showed locality important sparsity experiment shown locality enhance sparsity sparse coding helpful learning only code local preferred let similar data similar zero dimension code although lcc ha computational advantage classical sparse coding still need solve norm optimization problem accelerate learning process practical coding method called constrained linear coding llc wa introduced 97 seen fast implementation lcc replaces regularization regularization scspm 98 spm 11 sc ha le restrictive constraint assignment vq ignore mutual dependence local feature lsc 113 enhance similar feature keep mutual dependency sparse coding hlsc 114 define similarity among instance hyper graph lcc 95 enhance locality explicitly encouraging coding local llc 97 accelerate process time consuming svc 118 enhance locality adopting smoother coding scheme asgd 119 imagenet prior cnns fig sparse coding algorithm relation contribution drawback sparse coding laplacian sc feature quantized visual word fig difference sparse coding laplacian sparse coding 113 b sparse coding c laplacian sc guo et al neurocomputing 187 2016 36 comparison vq scspm llc 97 shown fig besides llc another model called coding svc 118 also guarantee local sparse coding given x svc activate coordinate associated neighborhood x achieve sparse representation svc simple extension vq expanding vq local tangent direction thus smoother coding scheme remarkable result shown 119 proposed averaging stochastic gradient descent asgd scheme combined lcc svc algorithm scale image classiﬁcation scale dataset produced result imagenet object recognition task prior rise cnn architecture another smooth coding method presented 110 called smooth sparse coding ssc method rate neighborhood similarity temporal information sparse coding leading code represent neighborhood rather individual sample lower mean square reconstruction error recently et al 120 proposed new unsupervised feature learning framework called deep sparse coding deepsc extends sparse coding architecture ha best performance among sparse coding scheme described discussion order compare understand four category deep learning summarize advantage advantage respect diverse property listed table nine property total detail refers whether approach ha shown effective diverse medium text image audio application including speech recognition visual recognition unsupervised learning refers ability learn deep model without supervisory annotation feature learning ability automatically learn feature based data set training prediction refer efﬁciency learning inferring process respectively biological understanding theoretical justiﬁcation represent whether approach ha signiﬁcant logical underpinnings theoretical foundation respectively invariance refers whether approach ha shown robust transformation rotation scale translation small training set refers ability learn deep model using small number example important note table only represents general current ﬁndings not future sibilities specialized niche case application result deep learning ha widely adopted various direction computer vision image classiﬁcation object detection image retrieval semantic segmentation human pose estimation key task image understanding part brieﬂy summarize development deep learning result referred original paper especially cnn based algorithm ﬁve area image classiﬁcation image classiﬁcation task consists labeling input image probability presence particular visual object class 129 shown fig prior deep learning perhaps commonly used method image classiﬁcation method based bag visual word bow 130 ﬁrst describes image histogram quantized visual word feed histogram classiﬁer typically svm 131 pipeline wa based orderless statistic incorporate spatial geometry bow descriptor lazebnik et al 111 integrated spatial pyramid approach pipeline count number visual word inside set image instead whole region thereafter pipeline wa improved ing sparse coding optimization problem building book 119 receives best performance imagenet classiﬁcation sparse coding one basic algorithm deep learning discriminative original one hog 132 lbp 133 approach based bow concern zero order statistic count visual word discarding lot valuable information image 129 method introduced ronnin et al 134 overcame issue extracted higher order statistic employing fisher kernel 135 achieving image classiﬁcation result phase researcher tend focus higher order statistic core idea deep learning krizhevsky et al 6 represented turning point object recognition large cnn wa trained imagenet database 136 thus proving cnn could addition handwritten digit recognition 17 perform well natural image classiﬁcation proposed alexnet ilsvrc 2012 vq scspm llc 1 2 1 2 1 2 node assigned 1 node assigned 2 node assigned 1 2 fig comparison vq scspm llc 97 vq b scspm c llc table 4 comparison among four category deep learning property cnns rbms autoencoder sparse coding generalization yes yes yes yes unsupervised learning no yes yes yes feature learning yes yes yes no training no no yes yes prediction yes yes yes yes biological understanding no no no yes theoretical justiﬁcation yes yes yes yes invariance yes no no yes small training set yes yes yes yes note yes indicates category doe well property otherwise marked no yes refers preliminary weak ability guo et al neurocomputing 187 2016 37 error rate sparked signiﬁcant additional activity cnn research fig 18 present result imagenet test dataset since 2012 along pipeline ilsvrc overfeat 145 proposed multiscale sliding window approach could ﬁnd optimal scale image fulﬁll different task simultaneously classiﬁcation localization detection speciﬁcally algorithm decreased test error zeiler et al 52 introduced novel visualization technique give insight function intermediate feature layer adjusted new model outperformed alexnet reaching error rate top performance ilsvrc ilsvrc 2014 witnessed steep growth deep learning participant utilized cnns basis model signiﬁcant progress made image classiﬁcation error wa almost halved since 26 model eliminated restriction ﬁxed input image size could boost accuracy variety published cnn tectures despite different design multiple reduced error rate ranked third image classiﬁcation challenge ilsvrc along improvement classical cnn model another characteristic shared model architecture became deeper shown googlenet 20 rank 1 ilsvrc 2014 vgg 31 rank 2 ilsvrc 2014 achieved respectively despite potential capacity possessed larger model also suffered overﬁtting underﬁtting problem little training data little training time avoid shortcoming wu et al 159 developed new strategy image data augmentation usage image also built large supercomputer deep neural network developed highly optimized parallel algorithm classiﬁcation result achieved relative 20 improvement previous one error rate recently et al 162 proposed parametric rectiﬁed linear unit erate traditional rectiﬁed activation unit derived robust initialization method scheme led test error surpassed performance ﬁrst time similar result achieved ioffe et al 163 whose method reached test error utilizing ensemble normalized network fig image classiﬁcation example alexnet 6 image ha one ground truth label followed top 5 guess probability fig imagenet classiﬁcation result test dataset guo et al neurocomputing 187 2016 38 object detection object detection different closely related image classiﬁcation task image classiﬁcation whole image utilized input class label object within image estimated object detection besides outputting information presence given class also need estimate position instance instance shown fig detection window regarded correct outputted bounding box ha sufﬁciently large overlap ground truth object usually 50 challenging pascal voc datasets widely employed evaluation object detection 20 class database test phase algorithm predict bounding box object belong class test image section describe recent ments deep learning scheme object detection according achievement voc 2007 voc related advance shown table surge deep learning deformable part model dpm 176 wa effective method object detection take advantage deformable part model detects object across scale location image exhaustive manner integrating some technique bounding box prediction context rescoring model achieved average precision voc 2007 test set deep learning method especially method achieved top tier performance image classiﬁcation task researcher started transfer object detection problem early deep learning approach object detection wa duced szegedy et al 121 paper proposed algorithm called detectornet replaced last layer alexnet 6 regression layer algorithm captured object location well achieved competitive result test set advanced algorithm time handle tiple instance object image deepmultibox 147 also showed neural network model general pattern current successful object detection tems generate large pool candidate box classify using cnn feature representative approach rcnn scheme proposed girshick et al 29 utilizes selective search 151 generate object proposal extract cnn feature proposal feature fed svm classiﬁer decide whether related candidate window tain object not rcnns improved benchmark large margin became base model many promising algorithm algorithm derived rcnns mainly divided two category ﬁrst category aim accelerate training testing process although rcnn ha excellent object detection accuracy computationally intensive ﬁrst warp process object proposal independently consequently some algorithm aim improve efﬁciency appeared 26 frcn 177 rpn 178 yolo 179 etc algorithm detect object faster achieving comparable map benchmark second category mainly intended improve racy rcnns performance recognition using region paradigm highly dependent quality object hypothesis currently many object proposal algorithm objectness 150 selective search 151 object proposal 152 bing 153 edge box 154 et al fig object detection example rcnn 29 red box extract salient object contains green box contains prediction score interpretation reference color ﬁgure legend reader referred web version article table 5 object detection result voc 2007 voc 2012 challenge method training data voc 2007 voc 2012 map map net map map net dpm 176 07 detectornet 121 12 deepmultibox 147 12 rcnn 29 07 rcnn 29 þbb 07 66 rcnn 29 12 rcnn 29 þbb 12 26 07 26 26 þ bb 07 frcn 177 07 frcn 177 rpn 178 07 rpn 178 12 67 rpn 178 rpn 178 184 07 184 12 fgs 185 07 fgs 185 þbb 07 noc 186 noc 186 þbb note training data 07 trainval 12 trainval trainval union trainval trainval test union trainval bb bounding box regression approach based alexnet 6 clarifai 52 approach based 31 guo et al neurocomputing 187 2016 39 scheme exhaustively evaluated 155 although scheme good ﬁnding rough object position normally could not accurately localize whole object via tight bounding box form largest source detection error therefore many approach emerged try correct poor localization one important direction method combine semantic segmentation technique ple sd scheme proposed hariharan et al 164 utilizes segmentation background inside detection resulting improved performance object detection without bounding box regression hand uds method 182 uniﬁed object detection semantic segmentation process one framework enforcing consistency integrating context information model demonstrated encouraging performance task similar work come segdeepm proposed zhu et al 183 184 also incorporate segmentation along additional evidence boost accuracy object detection also approach attempt precisely locate object way instance fgs 185 address lization problem via two method 1 develop search algorithm iteratively optimize location 2 train cnn classiﬁer structured svm objective balance classiﬁcation localization combination method demonstrates promising performance two challenging datasets aside effort object localization noc framework 186 try evolve effort object classiﬁcation step place commonly used perceptron mlp explored different noc structure implement object classiﬁers much cheaper easier collect large amount label collect detection data label precise bounding box therefore major challenge scaling object detection difﬁculty obtaining labeled image large number category hoffman et al 142 proposed deep detection adaption dda algorithm learn difference image classiﬁcation object detection transferring classiﬁers category detector without bounding box annotated data method ha potential enable detection thousand category lack bounding box annotation two promising scalable approach conceptleaner 128 babylearning 187 learn accurate concept detector without massive annotation visual concept collecting weakly labeled image cheap ceptleaner develops hard instance learning algorithm automatically discover visual concept noisy labeled image collection result ha potential learn concept directly web hand learning 187 approach simulates baby interaction physical world achieve comparable result based approach only sample object category along large amount online unlabeled video table 4 also observe several factor could improve performance addition algorithm 1 larger training set 2 deeper base model 3 bounding box regression image retrieval image retrieval aim ﬁnd image containing similar object scene query image illustrated fig success alexnet 6 suggests feature emerging upper layer cnn learned classify image serve good descriptor image classiﬁcation motivated many recent study use cnn model image retrieval task study achieved competitive result compared traditional method vlad fisher vector following paragraph introduce main idea cnn based method inspired spatial pyramid matching gong et al 28 posed kind reverse spm idea extract patch tiple scale starting whole image pool scale without regard spatial information aggregate local patch response ﬁner scale via vlad encoding orderless nature vlad help build invariant sentation finally original global deep activation catenated vlad feature ﬁner scale form new image representation razavian et al 165 used feature extracted overfeat network generic image representation tackle diverse range vision task including recognition retrieval first augments training set adding cropped rotated sample image extract multiple different size different location computed cnn presentation distance reference query image set average distance query reference image given recent success deep learning technique achieved research presented 166 attempt evaluate deep learning bridge semantic gap image retrieval cbir encouraging result reveal deep cnn model large datasets directly used fig image retrieval example using cnn feature left image querying one image green frame right represent positive retrieval candidate interpretation reference color ﬁgure legend reader referred web version article guo et al neurocomputing 187 2016 40 feature extraction new cbir task applied feature representation new domain wa found larity learning boost retrieval performance ther retraining deep model classiﬁcation larity learning objective new domain accuracy improved signiﬁcantly different approach shown 171 ﬁrst extract like image patch general object detector one cnn feature extracted object patch alexnet model many result experiment concluded method achieve signiﬁcant accuracy improvement space consumption time cost still obtains higher accuracy finally without sliding window patch babenko et al 172 focus holistic descriptor whole image mapped single vector cnn model found best performance observed not top network rather layer two level put important result pca affect performance cnn much le performance vlads fisher vector therefore pca compression work better cnn tures table 6 show retrieval result several public datasets one interesting problem cnn feature layer ha highest impact ﬁnal performance some method extract feature second fully connected layer contrast method use ﬁrst fully connected layer cnn model image representation moreover choice may change different datasets 166 thus think investigating characteristic layer still open problem semantic segmentation past large number study focus semantic segmentation task yield promising progress main reason success come cnn model capable tackling prediction network datasets different classiﬁcation tion semantic segmentation requires output mask spatial distribution semantic segmentation recent advanced cnn based method summarized follows 1 segmentation approach segment image based candidate window outputted object detection rcnn 29 sd 164 ﬁrst generated region proposal object detection utilized traditional approach segment region assign pixel class label detection based sd 164 hariharan et al 138 proposed hypercolumn pixel vector activation gained large improvement one disadvantage tation largely additional expense object detection without extracting region raw image dai et al 148 designed convolutional feature masking cfm method extract proposal directly feature map efﬁcient convolutional feature map only need computed even though error caused proposal object detection tend propagated segmentation stage 2 based segmentation second one fully volutional network fcn replacing fully connected layer convolutional layer ha popular strategy baseline semantic segmentation long et al 146 deﬁned novel architecture combined semantic information deep coarse layer appearance mation shallow ﬁne layer produce accurate detailed segmentation deeplab 144 proposed similar fcn model also integrated strength conditional random ﬁelds crfs fcn detailed boundary recovery instead using crfs step lin et al 213 jointly train fcn crfs efﬁcient piecewise training wise work 214 converted crfs recurrent neural network rnn plugged part fcn model 3 weakly supervised annotation apart advancement segmentation model some work focused weakly supervised segmentation papandreou et al 215 studied challenging segmentation weakly annotated ing data bounding box label wise boxsup method 216 made use bounding box annotation estimate segmentation mask used update network iteratively work showed excellent performance combining small number annotated image large number bounding box annotated image table 6 image retrieval result several datasets method holiday ukb babenko et al 172 sun et al 171 gong et al 28 razavian et al 165 wan et al 166 table 7 semantic segmentation result pascal voc 2012 val test set method train val 2012 test 2012 description sd 164 voc extra region proposal input image cfm 148 voc extra region proposal feature map 146 voc extra one model three stride hypercolumn 138 voc extra region proposal input image deeplab 144 voc extra one model one stride 144 voc extra field view 213 voc extra 3 scale image 5 model 215 voc extra recurrent neural network boxsup 215 voc extraþcoco weakly supervised annotation 216 voc extraþcoco weakly supervised annotation guo et al neurocomputing 187 2016 41 describe main property method compare result pascal voc 2012 val test set listed table 7 human pose estimation human pose estimation aim estimate localization human joint still image image sequence shown fig important wide range potential tions video surveillance human behavior analysis interaction hci extensively died recently however task also challenging great variation human appearance complicated background well many nuisance factor illumination viewpoint scale etc part mainly summarize deep learning scheme estimate human lation still image although scheme could porated motion feature boost performance video normally human pose estimation involves multiple problem recognizing people image detecting describing human body part modeling spatial conﬁguration prior deep learning best performing human pose estimation method based body part detector detect describe human body part ﬁrst impose textual relation local part one typical approach pictorial structure 196 take advantage tree model capture geometric relation adjacent part ha developed various method deep learning algorithm learn feature tolerant variation nuisance factor achieved success various computer vision task recently received signiﬁcant attention research community summarized performance related deep learning algorithm two commonly used datasets frame labeled cinema flic 201 leeds sport pose lsp 202 flic sists 3987 training image 1016 test image obtained popular hollywood movie containing people diverse pose annotated joint label lsp extension contains training 1000 testing image sport people gathered flickr 14 full body joint annotated two widely accepted evaluation metric evaluation centage correct part pcp 203 measure rate correct limb detection percent detected joint pdj 201 measure rate correct limb detection following table 8 illustrates pdj comparison iou deep learning method flic dataset normalized distance table 9 list pcp comparison lsp dataset general deep learning scheme human pose estimation categorized according handling manner input image holistic processing processing holistic processing method tend accomplish task global manner not explicitly deﬁne model individual part spatial relationship one typical model called deeppose proposed toshev et al 204 model mulates human pose estimation method joint regression problem doe not explicitly deﬁne graphical model part fig human pose estimation 212 table 8 pdj comparison flic dataset pdj pck head shoulder elbow wrist jain et al 206 deeppose 204 chen et al 205 210 tompson et al 207 tompson et al 208 73 table 9 pcp comparison lsp dataset torso head mean ouyang et al 209 deeppose 204 56 38 77 71 chen et al 205 77 75 210 98 85 80 63 90 88 84 guo et al neurocomputing 187 2016 42 detector human pose estimation speciﬁcally utilizes architecture ﬁrst layer address ambiguity body part holistic way generates initial pose estimation second layer reﬁnes joint location estimation model achieved advance several challenging datasets however method suffers inaccuracy region since difﬁcult learn direct regression complex pose vector image processing method propose detect human body part individually followed graphic model incorporate spatial information instead training work using whole image chen et al 205 utilized local part patch background patch train dcnn order learn conditional probability part presence spatial relationship incorporating graphic model algorithm gained promising performance moreover jain et al 206 trained multiple smaller convnets perform independent binary part classiﬁcation followed weak spatial model remove strong outlier enforce global pose consistency similarly tompson et al 207 designed convnet architecture perform likelihood regression body part followed implicit graphic model promote joint consistency model wa extended 208 argues pooling layer cnns would limit spatial localization accuracy try recover precision loss pooling process especially improve method 207 adding carefully designed spatial dropout layer present novel network reuses tional feature improve precision spatial locality also approach suggesting combining local part appearance holistic view part accurate human pose estimation example ouyang et al 209 derived deep model deep belief net dbn attempt take advantage three information source human articulation mixture type appearance score deformation combine representation learn holistic human body articulation pattern hand fan et al 210 proposed convolutional neutral network integrate holistic partial view cnn framework take part patch body ches input combine local contextual information accurate pose estimation scheme tend design new architecture carreira et al 211 introduced model called iterative error feedback ief model encompass rich structure input output space incorporating feedback show promising result trend challenge along promising performance deep learning ha achieved research literature ha indicated several important challenge well inherent trend described next theoretical understanding although promising result addressing computer vision task achieved deep learning method lying theory not well understood no clear standing architecture perform better others difﬁcult determine structure many layer many node layer proper certain task also need speciﬁc knowledge choose sensible value learning rate strength regularizer etc design architecture ha historically determined basis chu et al 140 proposed theoretical method determining optimal number feature map however theoretical method only worked extremely small receptive ﬁelds better understand behavior cnn architecture zeiler et al 52 developed visualization technique gave insight function intermediate feature layer revealing feature interpretable pattern brought possibility better architecture design similar visualization wa also studied yu et al 141 apart visualizing feature rcnn 29 attempted discover learning pattern cnn tested performance pattern training process found convolutional layer learn general feature convey cnn representational capacity top layer addition ing cnn feature agrawal et al 122 investigated effect some commonly used strategy cnn performance provided backed intuition apply cnn model computer vision problem despite progress achieved theory deep learning signiﬁcant room better understanding evolving optimizing cnn architecture toward improving desirable property invariance class discrimination vision human vision ha remarkable proﬁciency computer vision task even simple visual representation change geometric transformation background variation occlusion vision refer either bridging semantic gap term accuracy bringing new insight study human brain integrated machine learning tectures compared traditional feature cnn mimic human brain structure build activation feature study 166 aimed evaluate much retrieval improvement achieved developing deep learning technique whether deep feature desirable key bridge semantic gap long term seen fig 18 image classiﬁcation error imagenet test set decrease 10 6 2012 163 promising improvement veriﬁes efﬁciency cnns particular result 163 ha exceeded accuracy human raters however not conclude representational performance cnn rival brain 123 example easy produce image completely unrecognizable human one cnn belief contain recognizable object conﬁdence 124 result highlight difference human vision current cnn model raise question generality cnns computer vision study 123 found like cortex recent cnns could generate similar feature space category distinct one image different category result indicates cnns may provide insight standing primate visual processing another study 125 author considered novel approach brain decoding fmri data leveraging unlabeled data temporal cnns learned multiple layer temporal ﬁlters trained powerful brain decoding model whether cnn model rely computational mechanism similar primate visual system yet determined ha potential improvement mimicking incorporating primate visual system guo et al neurocomputing 187 2016 43 training limited data larger model demonstrate potential capacity become tendency recent development however shortage training data may limit size learning ability model especially expensive obtain fully labeled data overcome need enormous amount training data train large network effectively remains addressed currently two commonly used solution obtain training data ﬁrst solution generalize training data existing data based various data augmentation scheme scaling rotating cropping top wu et al 159 adopted color casting vignetting lens distortion ques could produce much converted training example broad coverage second solution collect training data weak learning algorithm recently ha range article learning visual concept image search engine order scale computer vision recognition system zhou et al 128 proposed conceptlearner approach could automatically learn thousand visual concept detector weakly labeled image collection besides reduce laborious bounding box annotation cost object detection many supervised approach emerged sence labeling 51 nevertheless promising develop technique generating collecting comprehensive training data could make network learn better feature robust various change geometric transformation occlusion time complexity early cnns seen method required lot computational resource not candidate application one trend towards developing new tectures allow running cnn study 18 conducted series experiment constrained time cost proposed model fast application yet competitive existing cnn model addition ﬁxing time complexity also help understand impact factor depth number ﬁlters ﬁlter size etc another study 15 eliminated redundant computation forward backward propagation cnns resulted speedup 1500 time ha robust ﬂexibility various cnn model different design structure reach high efﬁciency gpu implementation ren et al 3 converted key operator deep cnns vectorized form high parallelism achieved given basic parallelized operator provided uniﬁed framework vision application powerful model deep learning related algorithm moved forward result various computer vision task large margin becomes challenging make progress top might several direction powerful model ﬁrst direction increase generalization ability increasing size network larger network could normally bring higher quality performance care taken address issue may cause overﬁtting need lot computational resource second direction combine information multiple source feature fusion ha long popular appealing fusion categorized two type 1 combine tures layer network different layer may learn different feature 29 promising could develop algorithm make feature layer plementary example deepindex 156 proposed integrate multiple cnn feature multiple inverted index including different layer one model several layer distinct model 2 combine feature different type obtain comprehensive model integrating type feature sift improve image retrieval performance deepembedding 157 used sift feature build inverted index structure extracted cnn feature local patch enhance matching strength third direction towards powerful model design speciﬁc deep network currently almost based scheme adopt shared network prediction may not distinctive enough promising direction train speciﬁc deep network focus type object interested study 27 ha veriﬁed annotation useful annotation object detection viewed kind speciﬁc deep network focus object rather whole image another possible solution train different network different category instance 158 built intuition not class equally difﬁcult distinguish true class label designed initial coarse classiﬁer cnn well several ﬁne cnns adopting classiﬁcation strategy achieves performance conclusion paper present comprehensive review deep learning develops categorization scheme analyze existing deep learning literature divide deep learning algorithm four category according basic model derived convolutional neural network restricted boltzmann machine autoencoder sparse coding approach four class discussed analyzed detail application computer vision domain paper mainly report advancement cnn based scheme extensively utilized suitable image notably some recent article reported inspiring advance showing some algorithm already exceeded accuracy human raters despite promising result reported far niﬁcant room advance example underlying theoretical foundation doe not yet explain condition perform well outperform approach determine optimal structure certain task paper describes challenge summarizes new trend designing training deep neural network along several direction may explored future acknowledgment work wa supported leiden university grant 2006002026 national university defense technology grant 61571453 nwo grant nvidia corporation grant reference 1 bordes glorot weston et al joint learning word meaning representation semantic parsing proceeding aistats 2012 2 ciresan meier schmidhuber transfer learning latin chinese character deep neural network proceeding ijcnn 2012 3 ren xu vectorization deep convolutional neural network vision task proceeding aaai guo et al neurocomputing 187 2016 44 4 mikolov sutskever chen et distributed representation word phrase compositionality proceeding nip 2013 5 ciresan meier schmidhuber deep neural network image classiﬁcation proceeding cvpr 2012 6 krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural network proceeding nip 2012 7 8 bengio learning deep architecture ai found trend mach learn 2 1 2009 9 deng tutorial survey architecture algorithm application deep learning apsipa trans signal inf process 3 2014 10 schmidhuber deep learning neural network overview neural netw 61 2015 11 bengio deep learning representation looking forward statistical language speech processing springer berlin heidelberg 2013 12 bengio courville vincent representation learning review new perspective pattern anal mach intell ieee trans 35 8 2013 13 lecun learning invariant feature hierarchy proceeding eccv workshop 2012 14 goroshin lecun saturating proceeding iclr 2013 15 li zhao wang highly efﬁcient forward backward propagation convolutional neural network pixelwise classiﬁcation arxiv preprint arxiv 2014 16 erhan bengio courville et doe unsupervised help deep learning mach learn 11 2010 17 lecun bottou bengio et learning applied document recognition proc ieee 86 11 1998 18 sun convolutional neural network constrained time cost proceeding cvpr 2015 19 zeiler hierarchical convolutional deep learning computer vision thesis new york university 2014 20 szegedy liu jia et going deeper convolution ceedings cvpr 2015 21 min lin qiang chen shuicheng yan network network proceeding iclr 2013 22 boureau ponce lecun theoretical analysis feature pooling visual recognition proceeding icml 2010 23 scherer müller behnke evaluation pooling operation volutional architecture object recognition proceeding icann 2010 24 cireşan meier masci et neural network visual object classiﬁcation proceeding ijcai 2011 25 zeiler fergus stochastic pooling regularization deep volutional neural network proceeding iclr 2013 26 zhang ren et spatial pyramid pooling deep convolutional network visual recognition proceeding eccv 2014 27 ouyang luo zeng et deformable deep convolutional neural network object detection proceeding cvpr 2015 28 gong wang guo et orderless pooling deep volutional activation feature proceeding eccv 2014 29 girshick donahue darrell et rich feature hierarchy accurate object detection semantic segmentation proceeding cvpr 2014 30 oquab bottou laptev et learning transferring image representation using convolutional neural network proceeding cvpr 2014 31 simonyan zisserman deep convolutional network image recognition proceeding iclr 2015 32 zeng ouyang wang contextual deep learning pedestrian detection proceeding iccv 2013 33 sun wang tang deep convolutional network cascade facial point detection proceeding cvpr 2013 34 miclut committee deep feedforward network trained data pattern recognition springer international publishing pp 2014 35 weston ratle mobahi et deep learning via embedding neural network trick trade springer berlin berg pp 36 simonyan vedaldi zisserman deep fisher network image classiﬁcation proceeding nip 2013 37 chen song huang et contextualizing object detection classiﬁcation proceeding cvpr 2011 38 hinton srivastava krizhevsky et improving neural network preventing feature detector arxiv preprint arxiv 2012 39 baldi sadowski understanding dropout proceeding nip 2013 40 ba frey adaptive dropout training deep neural network ceedings nip 2013 41 mcallester tutorial dropout bound arxiv preprint arxiv 2013 42 wager wang liang dropout training adaptive regularization proceeding nip 2013 43 wang manning fast dropout training proceeding icml 2013 44 srivastava hinton krizhevsky et dropout simple way prevent neural network overﬁtting mach learn 15 1 2014 45 goodfellow courville et empirical analysis dropout piecewise linear network proceeding iclr 2014 46 wan l zeiler zhang et regularization neural network using dropconnect proceeding icml 2013 47 howard some improvement deep convolutional neural network based image classiﬁcation arxiv preprint arxiv 2013 48 dosovitskiy springenberg brox unsupervised feature learning augmenting single image arxiv preprint arxiv 2013 49 hinton osindero teh fast learning algorithm deep belief net neural comput 18 7 2006 50 poultney chopra cun efﬁcient learning sparse representation model proceeding nip 2006 51 song lee jegelka et discovery visual pattern conﬁgurations proceeding nip 2014 52 zeiler fergus visualizing understanding convolutional neural network proceeding eccv 2014 53 hinton sejnowski learning relearning boltzmann machine 1 mit press cambridge 1986 54 hinton contrastive divergence learning proceeding tenth international workshop artiﬁcial intelligence statistic np society artiﬁcial intelligence statistic 2005 pp 55 hinton practical guide training restricted boltzmann machine momentum 9 1 2010 926 56 cho raiko ihler enhanced gradient adaptive learning rate training restricted boltzmann machine proceeding icml 2011 57 nair hinton rectiﬁed linear unit improve restricted boltzmann machine proceeding icml 2010 58 arel rose karnowski deep machine new frontier artiﬁcial intelligence research research frontier comput intell mag ieee 5 4 2010 59 lee ekanadham ng sparse deep belief net model visual area proceeding nip 2008 60 nair hinton object recognition deep belief net ceedings nip 2009 61 lee grosse ranganath et convolutional deep belief network scalable unsupervised learning hierarchical representation ings icml 2009 62 lee grosse ranganath et unsupervised learning hierarchical representation convolutional deep belief network commun acm 54 10 2011 63 tang eliasmith deep network robust visual recognition ceedings icml 2010 64 huang lee learning hierarchical representation face veriﬁcation convolutional deep belief network proceeding cvpr 2012 65 salakhutdinov hinton deep boltzmann machine proceeding aistats 2009 66 salakhutdinov larochelle efﬁcient learning deep boltzmann machine proceeding aistats 2010 67 salakhutdinov hinton efﬁcient learning procedure deep boltzmann machine neural comput 24 8 2012 68 hinton salakhutdinov better way pretrain deep boltzmann machine proceeding nip 2012 69 cho raiko ilin et pretraining algorithm deep boltzmann machine proceeding icann 2013 70 montavon müller deep boltzmann machine centering trick neural network trick trade springer berlin heidelberg 2012 pp 71 goodfellow courville bengio joint training deep boltzmann machine classiﬁcation arxiv preprint arxiv 2013 72 goodfellow mirza courville et deep boltzmann machine proceeding nip 2013 73 ngiam chen koh et learning deep energy model ceedings icml 2011 74 elfwing uchibe doya expected restricted boltzmann machine classiﬁcation neural netw 2014 75 liou cheng liou et autoencoder word computing 139 2014 76 hinton salakhutdinov reducing dimensionality data neural network science 313 5786 2006 77 zhang kan et network cfan face alignment proceeding eccv 2014 78 jiang zhang zhang et novel sparse deep unsupervised learning proceeding icaci 2013 79 zhou arpit nwogu et joint training better deep encoders arxiv preprint arxiv 2014 80 goodfellow lee le et measuring invariance deep network proceeding nip 2009 81 ngiam coates lahiri et optimization method deep learning proceeding icml 2011 82 zou ng yu unsupervised learning visual invariance temporal coherence proceeding nip workshop guo et al neurocomputing 187 2016 45 83 simoncelli e statistical modeling photographic image 2005 84 le building feature using large scale unsupervised learning proceeding icassp 2013 85 vincent larochelle bengio et extracting composing robust feature denoising autoencoders proceeding icml 2008 86 vincent larochelle lajoie et stacked denoising autoencoders learning useful representation deep network local denoising criterion mach learn 11 2010 87 rifai vincent muller et contractive explicit invariance feature extraction proceeding icml 2011 88 alain bengio regularized learn data generating distribution proceeding iclr 2013 89 mesnil dauphin glorot et unsupervised transfer learning challenge deep learning approach proceeding icml 2012 90 masci meier cireşan et stacked convolutional hierarchical feature extraction proceeding icann 2011 91 baccouche mamalet wolf et convolutional sparse sequence classiﬁcation proceeding bmvc 2012 92 leng guo zhang et object retrieval stacked local volutional autoencoder signal process 2014 93 memisevic konda krueger autoencoders beneﬁts feature proceeding iclr 2015 94 olshausen field sparse coding overcomplete basis set strategy employed vi 37 23 1997 95 yu zhang gong nonlinear learning using local coordinate coding proceeding nip 2009 96 raina battle lee et learning transfer learning unlabeled data proceeding icml 2007 97 wang yang yu et linear coding image classiﬁcation proceeding cvpr 2010 98 yang yu gong et linear spatial pyramid matching using sparse coding image classiﬁcation proceeding cvpr 2009 99 donoho large underdetermined system linear equation minimal solution also sparsest solution commun pure appl math 59 6 2006 100 censor parallel optimization theory algorithm application oxford university press oxford united kingdom 1997 101 rumelhart hinton williams learning representation propagating error nature 323 6088 1986 102 lee battle raina et efﬁcient sparse coding algorithm ceedings nip 2006 103 mairal bach ponce et online dictionary learning sparse coding proceeding icml 2009 104 mairal bach ponce et online learning matrix factorization sparse coding mach learn 11 2010 105 friedman hastie höﬂing et pathwise coordinate optimization ann appl stat 1 2 2007 106 gregor lecun learning fast approximation sparse coding ceedings icml 2010 107 chambolle de vore lee et nonlinear wavelet image cessing variational problem compression noise removal wavelet shrinkage image process ieee trans 7 3 1998 108 beck teboulle fast iterative algorithm application image deblurring proceeding icassp 2009 109 kavukcuoglu ranzato lecun fast inference sparse coding algorithm application object recognition arxiv preprint arxiv 2010 110 balasubramanian yu lebanon smooth sparse coding via marginal regression learning sparse representation proceeding icml 2013 111 lazebnik schmid ponce beyond bag feature spatial pyramid matching recognizing natural scene category proceeding cvpr 2006 112 coates ng importance encoding versus training sparse coding vector quantization proceeding icml 2011 113 gao tsang chia et local feature not sparse coding image classiﬁcation proceeding cvpr 2010 114 gao tsang chia laplacian sparse coding hypergraph laplacian sparse coding application pattern anal mach intell ieee trans 35 1 2013 115 yu lin lafferty learning image representation pixel level via hierarchical sparse coding proceeding cvpr 2011 116 zeiler krishnan taylor et deconvolutional network proceeding cvpr 2010 117 zeile taylor fergus adaptive deconvolutional network mid high level feature learning proceeding iccv 2011 118 zhou yu zhang et image classiﬁcation using coding local image descriptor proceeding eccv 2010 119 lin lv zhu et image classiﬁcation fast feature extraction svm training proceeding cvpr 2011 120 kavukcuoglu wang et unsupervised feature learning deep sparse coding proceeding sdm 2014 121 szegedy toshev erhan deep neural network object detection proceeding nip 2013 122 agrawal girshick malik analyzing performance multilayer neural network object recognition proceeding eccv 2014 123 cadieu hong yamins et deep neural network rival representation primate cortex core visual object recognition plo comput biol 10 12 2014 124 nguyen yosinski clune deep neural network easily fooled high conﬁdence prediction unrecognizable image proceeding cvpr 2015 125 firat aksan oztekin et learning deep temporal representation brain decoding arxiv preprint arxiv 2014 126 chen shrivastava gupta neil extracting visual knowledge web data proceeding iccv 2013 127 divvala farhadi guestrin learning everything anything visual concept learning proceeding cvpr 2014 128 zhou jagadeesh piramuthu conceptlearner discovering visual concept weakly labeled image collection proceeding cvpr 2015 129 master large scale object detection czech technical university 2014 130 csurka dance fan et visual categorization bag keypoints proceeding eccv workshop 2004 131 boser guyon vapnik training algorithm optimal margin classiﬁers proceeding fifth annual workshop computational learning theory acm 1992 132 dalal triggs histogram oriented gradient human detection proceeding cvpr 2005 133 wang han yan human detector partial occlusion handling proceeding iccv 2009 134 perronnin sánchez mensink improving ﬁsher kernel scale image classiﬁcation proceeding eccv 2010 135 jaakkola haussler exploiting generative model discriminative classiﬁers proceeding nip 1999 136 deng dong socher et imagenet hierarchical image database proceeding cvpr 2009 137 noh hong han learning deconvolution network semantic mentation proceeding iccv 2015 138 hariharan arbeláez girshick et hypercolumns object mentation localization proceeding cvpr 2015 139 mostajabi yadollahpour shakhnarovich feedforward semantic segmentation feature proceeding cvpr 2015 140 chu krzyżak analysis feature map selection supervised learning using convolutional neural network advance artiﬁcial intelligence springer international publishing 2014 pp 141 yu yang bai et visualizing comparing convolutional neural network arxiv preprint arxiv 2014 142 hoffman guadarrama tzeng et lsda large scale detection adaptation proceeding nip 2014 143 hoffman guadarrama tzeng et object classiﬁers object detector adaptation approach 2014 144 chen papandreou kokkinos et semantic image segmentation deep convolutional net fully connected crfs proceeding iclr 2015 145 sermanet eigen zhang et overfeat integrated recognition localization detection using convolutional network proceeding iclr 2014 146 long shelhamer darrell fully convolutional network semantic segmentation proceeding cvpr 2015 147 erhan szegedy toshev et scalable object detection using deep neural network proceeding cvpr 2014 148 dai sun convolutional feature masking joint object stuff segmentation proceeding cvpr 2015 149 liu guo wu et deep index accurate efﬁcient image retrieval proceeding icmr 2015 150 alexe deselaers ferrari measuring objectness image window pattern anal mach intell ieee trans 34 11 2012 151 uijlings van de sande gevers et selective search object recognition int comput vi 104 2 2013 152 endres hoiem category independent object proposal proceeding eccv 2010 153 cheng zhang lin et bing binarized normed gradient objectness estimation proceeding cvpr 2014 154 zitnick dollár edge box locating object proposal edge proceeding eccv 2014 155 hosang benenson schiele good detection proposal really proceeding bmvc 2014 156 liu guo wu lew deepindex accurate efﬁcient image retrieval proceeding icmr 2015 157 zheng wang tian seeing big picture deep embedding contextual evidence arxiv preprint arxiv 2014 158 yan jagadeesh decoste et hierarchical deep volutional neural network image classiﬁcation proceeding iccv 2015 159 wu yan et deep image scaling image recognition arxiv preprint arxiv 2015 160 ngiam chen chia et tiled convolutional neural network proceeding nip guo et al neurocomputing 187 2016 46 161 younes convergence markovian stochastic algorithm rapidly decreasing ergodicity rate stoch int probab stoch process 65 1999 162 zhang ren et delving deep rectiﬁers surpassing level performance imagenet classiﬁcation proceeding iccv 2015 163 ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift proceeding nip 2015 164 hariharan arbeláez girshick et simultaneous detection segmentation proceeding eccv 2014 165 razavian azizpour sullivan carlsson cnn feature astounding baseline recognition proceeding cvpr shop 2014 166 wan wang hoi et deep learning image retrieval comprehensive study proceeding multimedia 2014 167 yosinski clune bengio et transferable feature deep neural network proceeding nip 2014 168 eslami heess winn shape boltzmann machine strong model object shape proceeding cvpr 2012 169 kae sohn lee et augmenting crfs boltzmann machine shape prior image labeling proceeding cvpr 2013 170 dahl ranzato mohamed et phone recognition restricted boltzmann machine proceeding nip 2010 171 sun zhou li et search feature image retrieval proceeding icimcs 2014 172 babenko slesarev chigorin et neural code image retrieval proceeding eccv 2014 173 oquab bottou laptev et object localization free supervised learning convolutional neural network proceeding cvpr 2015 174 srivastava salakhutdinov multimodal learning deep boltzmann machine proceeding nip 2012 175 wang distributed optimization deeply nested system proceeding aistats 2014 176 felzenszwalb girshick mcallester et object detection discriminatively trained model pattern anal mach intell ieee trans 32 9 2010 177 girshick fast proceeding iccv 2015 178 ren girshick et faster towards object detection region proposal network proceeding nip 2015 179 redmon divvala girshick et only look uniﬁed object detection arxiv preprint arxiv 2015 180 dai hoiem learning localize detected object proceeding cvpr 2012 181 hoiem chodpathumwan dai diagnosing error object detector proceeding eccv 2012 182 dong chen yan et towards uniﬁed object detection semantic segmentation proceeding eccv 2014 183 zhu urtasun salakhutdinov et segdeepm exploiting tation context deep neural network object detection ceedings cvpr 2015 184 gidaris komodakis object detection via semantic cnn model proceeding iccv 2015 185 zhang sohn villegas et improving object detection deep convolutional network via bayesian optimization structured prediction proceeding cvpr 2015 186 ren girshick et object detection network convolutional feature map arxiv preprint arxiv 2015 187 liang liu wei et towards computational baby learning supervised approach object detection proceeding iccv 2015 188 xie tu edge detection proceeding iccv 2015 189 russakovsky deng su et imagenet large scale visual recognition challenge int comput vi 115 3 2015 190 wang zhang lin et deep joint task learning generic object extraction proceeding nip 2014 191 yoo park lee et pyramid pooling deep volutional representation proceeding cvpr workshop 2015 192 jain tompson lecun et modeep deep learning framework using motion feature human pose estimation proceeding accv 2014 193 pﬁster simonyan charles et deep convolutional neural network efﬁcient pose estimation gesture video proceeding accv 2015 194 pﬁster charles zisserman flowing convnets human pose mation video proceeding iccv 2015 195 yu guo tao et human pose recovery supervised spectral embedding neurocomputing 166 2015 196 felzenszwalb huttenlocher et pictorial structure object recognition int comput vi 99 2 2005 197 tian zitnick narasimhan exploring spatial hierarchy mixture model human pose estimation proceeding eccv 2012 198 wang li beyond physical connection tree model human pose estimation proceeding cvpr 2013 199 pishchulin andriluka gehler et poselet conditioned pictorial structure proceeding cvpr 2013 200 dantone gall leistner et human pose estimation using body part dependent joint regressors proceeding cvpr 2013 201 sapp taskar modec multimodal decomposable model human pose estimation proceeding cvpr 2013 202 johnson everingham clustered pose nonlinear appearance model human pose estimation proceeding bmvc 2010 203 eichner zisserman et articulated human pose estimation retrieval almost unconstrained still image int comput vi 99 2 2012 204 toshev szegedy deeppose human pose estimation via deep neural network proceeding cvpr 2014 205 chen yuille articulated pose estimation graphical model image dependent pairwise relation proceeding nip 2014 206 jain tompson andriluka et learning human pose estimation feature convolutional network proceeding iclr 2014 207 tompson jain lecun et joint training convolutional network graphical model human pose estimation proceeding nip 2014 208 tompson goroshin jain et efﬁcient object localization using convolutional network proceeding cvpr 2015 209 ouyang chu wang deep learning human pose estimation proceeding cvpr 2014 210 fan zheng lin et combining local appearance holistic view deep neural network human pose estimation ings cvpr 2015 211 carreira agrawal fragkiadaki et human pose estimation iterative error feedback arxiv preprint arxiv 2015 212 huang boyer ilic robust human body shape pose tracking proceeding 2013 213 lin shen reid et efﬁcient piecewise training deep structured model semantic segmentation arxiv preprint arxiv 2015 214 zheng jayasumana et conditional random ﬁelds recurrent neural network proceeding iccv 2015 215 papandreou chen murphy et learning dcnn semantic image segmentation proceeding iccv 2015 216 dai sun boxsup exploiting bounding box supervise convolutional network semantic segmentation proceeding iccv yanming guo received degree information system engineering degree operational research national university defense technology changsha china 2011 2013 respectively currently leiden institute advanced computer science liacs leiden versity current research interest include image classiﬁcation object detection image retrieval yu liu received degree degree school software technology dalian university technology dalian china 2011 2014 tively currently leiden institute advanced computer science liacs leiden university current research interest include semantic mentation image retrieval ard oerlemans received degree degree leiden university 2004 2011 respectively ha ever software engineer senior software engineer lead software engineer vdg security well computer vision engineer prime vision currently computer vision engineer vdg security current research interest include video analysis video image retrieval interest point detection visual concept detection also interested logically motivated computer vision technique optimization approach large scale image video analysis guo et al neurocomputing 187 2016 47 songyang lao received degree information system engineering degree system engineering national university defense technology changsha china 1990 1996 respectively currently professor school information system management wa ing scholar dublin city university irish 2004 current research interest include image processing video analysis computer interaction song wu received degree degree computer science southwest university chongqing china 2009 2012 respectively currently leiden institute advanced computer science liacs leiden university netherlands current research interest include image matching image retrieval classiﬁcation michael lew imagery medium research cluster liacs director liacs medium lab received doctorate university illinois became postdoctoral researcher leiden university one year later became ﬁrst leiden university fellow wa pilot program tenure track professor 2003 became tenured associate professor leiden university wa invited serve chair full professor computer science tsinghua versity mit china ha published 100 peer reviewed paper three best paper citation area computer vision retrieval machine learning currently september 2014 ha cited paper history acm transaction multimedia addition ha cited paper acm international conference multimedia information retrieval mir 2008 also acm mir ha served organizing committee dozen acm ieee conference served founding chair acm icmr steering committee served chair acm mir acm civr steering committee addition international journal multimedia information retrieval springer member acm sigmm executive board highest inﬂuential committee sigmm guo et al neurocomputing 187 2016 48