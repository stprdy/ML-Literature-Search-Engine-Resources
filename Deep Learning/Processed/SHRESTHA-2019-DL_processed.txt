receiv april accept april date public april date current version may digit object identifi review deep learn algorithm architectur ajay shrestha ausif mahmood senior member ieee depart comput scienc engin univers bridgeport bridgeport ct usa correspond author ajay shrestha shrestha abstract deep learn dl play increasingli import role live ha alreadi made huge impact area cancer diagnosi precis medicin car predict forecast speech recognit painstakingli handcraft featur extractor use tradit learn classiﬁc pattern recognit system scalabl data set mani case depend problem complex dl also overcom limit earlier shallow network prevent efﬁcient train abstract hierarch represent train data deep neural network dnn use multipl deep layer unit highli optim algorithm architectur thi paper review sever optim method improv accuraci train reduc train time delv math behind train algorithm use recent deep network describ current shortcom enhanc implement review also cover differ type deep architectur deep convolut network deep residu network recurr neural network reinforc learn variat autoencod index term machin learn algorithm optim artiﬁci intellig deep neural network architectur convolut neural network backpropag supervis unsupervis learn introduct neural network machin learn ml techniqu inspir resembl human nervou system structur brain consist process unit organ input hidden output layer node unit layer connect node adjac layer connect ha weight valu input multipli respect weight sum unit sum undergo transform base tion function case sigmoid function tan hyperbol rectiﬁ linear unit relu tion use becaus mathemat favor deriv make easier comput partial deriv error delta respect individu weight sigmoid tanh function also squash input narrow output rang option respect ment satur nonlinear output plateau rate respect threshold relu hand exhibit satur behavior f x max x output function fed input subsequ unit next layer result ﬁnal output layer use solut problem associ editor coordin review thi manuscript approv public wa jiang neural network use varieti lem includ pattern recognit classiﬁc cluster dimension reduct comput vision natur languag process nlp regress predict analysi etc exampl imag recognit figur show deep neural network call lution neural network cnn learn hierarch level represent input vector fulli identifi object red squar ﬁgure simpli gross gener pixel valu highlight section ﬁgure cnn progress extract higher represent imag layer ﬁnalli recogn imag implement neural network consist follow step acquir train test data set train network make predict test data paper organ follow section introduct machin learn background motiv classiﬁc neural network dnn architectur train algorithm ieee translat content mine permit academ research onli person use also permit requir ieee permiss see http inform volum shrestha mahmood review dl algorithm architectur figur imag recognit cnn shortcom train algorithm optim train algorithm architectur algorithm implement conclus background frank rosenblatt creat perceptron ﬁrst prototyp know neural network two layer process unit could recogn simpl pattern instead undergo research develop neural network enter dark phase histori professor mit demonstr even learn simpl xor function addit wa anoth ﬁnding particularli dampen motiv dnn univers mation theorem show singl hidden layer wa abl solv ani continu problem wa mathemat proven well question valid dnn singl hidden layer could use learn wa efﬁcient wa far cri conveni capabl afford hierarch abstract multipl hidden layer dnn know wa univers approxim theorem held back progress dnn back way train dnn either factor prolong ai winter phase histori artiﬁci intellig get much fund interest result advanc much either breakthrough dnn occur advent backpropag learn algorithm wa propos wa fulli understood appli neural network learn wa made possibl deeper understand applic backpropag algorithm tion featur extractor differenti dnn earlier gener machin learn techniqu dnn type neural network model multilay perceptron mlp train algorithm learn represent data set without ani manual design featur extractor name deep learn suggest consist higher deeper number process er contrast shallow learn model fewer layer unit shift shallow deep learn ha allow complex function map efﬁcient map shallow architectur thi improv ha complement prolifer cheaper process unit graphic process unit gpgpu larg volum data set big data train gpgpu less power cpu number parallel process core outnumb cpu core order magnitud thi make gpgpu better implement dnn tion backpropag algorithm gpu adopt advanc ml particularli deep learn attribut explos data bigdata last year ml continu impact disrupt area live educ ﬁnanc govern healthcar manufactur market motiv deep learn perhap signiﬁc develop ﬁeld comput scienc recent time impact ha felt nearli scientiﬁc ﬁeld alreadi disrupt transform busi industri race among world lead economi technolog nie advanc deep learn alreadi mani area deep learn ha exceed human level capabl perform predict movi rate decis approv loan applic time taken car deliveri etc march three deep learn pioneer yoshua bengio geoffrey hinton yann lecun award ture award also refer nobel prize comput lot ha accomplish advanc deep learn deep learn ha potenti improv human live accur diagnosi diseas like cancer discoveri new drug predict natur disast report deep learn network wa abl learn imag diseas wa abl diagnos level board certiﬁ dermatologist googl ai wa abl beat averag accuraci us board certiﬁ gener pathologist grade prostat cancer goal thi review cover vast subject deep learn present holist survey dispers tion one articl present novel work collat work lead author wide scope breadth deep learn review paper focu speciﬁc area implement without encompass full scope ﬁeld thi review cover differ type volum shrestha mahmood review dl algorithm architectur figur feedforward neural network b unrol rnn time deep learn network architectur deep learn rithm shortcom optim method latest implement applic ii classif neural network neural network classiﬁ follow differ type feedforward neural network recurr neural network rnn radial basi function neural network kohonen self organ neural network modular neural network feedforward neural network inform ﬂow one direct input output layer via hidden node ani form ani circl loopback figur show particular type implement multilay feedforward neural network valu tion comput along forward pass path z weigh sum input repres activ function f z layer w repres weight two unit adjoin layer indic subscript letter b repres bia valu unit unlik feedforward neural network process unit rnn form cycl output layer becom input next layer typic onli layer network thu output layer becom input form feedback loop thi allow network memori previou state use inﬂuenc current output one signiﬁc outcom thi differ unlik feedforward neural network rnn take sequenc input gener sequenc output valu well render veri use applic requir process sequenc time phase input data like speech recognit video classiﬁc etc figur demonstr unrol rnn time sequenc sentenc constitut input word would correspond layer thu network would unfold unrol time rnn mathemat explan diagram xt repres input time u v w learn paramet share step ot output time st repres state time comput follow f activ function relu st f uxt radial basi function neural network use cation function approxim time seri predict lem etc consist input hidden output layer hidden layer includ radial basi function implement gaussian function node repres cluster center network learn design input center output layer combin output radial basi function weight paramet perform classiﬁc infer kohonen neural network self organ network model input data use unsupervis ing consist two fulli connect layer input layer output layer output layer organ dimension grid activ function weight repres attribut posit output layer node euclidian distanc input data output layer node respect weight calcul weight closest node neighbor input data updat bring closer input data formula wi wi α x x input data time wi ith weight time neighborhood function ith jth node modular neural network break larg network smaller independ neural network modul smaller network perform speciﬁc task later combin part singl output entir network volum shrestha mahmood review dl algorithm architectur dnn implement follow popular way spars autoencod convolut neural network cnn convnet restrict boltzmann machin rbm long memori lstm autoencod neural network learn ture encod given dataset order perform dimension reduct spars autoencod variat autoencod unit output valu close zero inact ﬁre deep cnn use multipl layer unit collect interact input pixel valu case imag result desir featur extract cnn ﬁnd applic imag recognit recommend system nlp rbm use learn probabl distribut within data set network use backpropag train backpropag use gradient descent error reduct adjust weight base partial deriv error respect weight neural network model also divid low two distinct categori discrimin gener discrimin model approach data ﬂow input layer via hidden layer output layer use supervis train problem like classiﬁc regress gener model hand data ﬂow opposit direct use unsupervis probabilist distribut problem input x correspond label given discrimin model learn probabl tribut p probabl given x directli wherea gener model learn joint probabl p x p predict gener whenev label data avail discrimin approach undertaken provid effect train label data avail gener approach taken train broadli categor three type supervis unsupervis supervis learn consist label data use train network wherea unsupervis learn label data set thu learn base back unsupervis learn neural network train use gener model rbm later could use standard supervis learn rithm use test data set determin tern classiﬁc big data ha push envelop even deep learn sheer volum varieti data contrari intuit inclin clear consensu whether supervis learn better unsupervis learn merit use case refer demonstr enhanc result unsupervis learn use unstructur video sequenc camera motion estim monocular depth ﬁed neural network deep belief network dbm describ chen lin use label unlabel data supervis unsupervis learn respect improv perform develop way automat extract meaning featur label unlabel high dimension data space challeng yann lecun et al assert one way could achiev thi would util integr unsupervis vise learn complement unsupervis learn data supervis learn label data refer learn dnn train algorithm overcom two major challeng prematur converg overﬁt ture converg occur weight bia dnn settl state onli optim local level miss global minima entir dimension space overﬁt hand describ state dnn becom highli tailor given train data set ﬁne grain level becom unﬁt rigid less adapt ani test data set along differ type train algorithm architectur also differ machin learn work tabl librari made train model easier framework make complex mathemat tion train algorithm statist model avail without write provid distribut parallel process capabl nient develop deploy featur figur show graph variou deep learn librari along github star github largest host servic provid sourc code world github star indic popular project github tensorflow popular dl librari iii dnn architectur deep neural network consist sever layer node ferent architectur develop solv problem differ domain cnn use time comput vision imag recognit rnn commonli use time seri hand clear winner gener problem like classiﬁc choic architectur could depend multipl factor nonetheless evalu classiﬁ conclud parallel random forest essenti parallel implement variat decis tree perform best three common architectur deep neural network convolut neural network autoencod restrict boltzmann machin rbm long memori lstm convolut neural network cnn base human visual cortex neural network choic comput vision imag recognit volum shrestha mahmood review dl algorithm architectur figur github star deep learn librari tabl popular deep learn framework librari video recognit also use area nlp drug discoveri etc shown figur cnn consist seri convolut er follow fulli connect layer normal softmax function layer figur illustr known layer cnn architectur devis lecun et al digit recognit seri tipl convolut layer perform progress reﬁn featur extract everi layer move input output layer fulli connect layer perform tion follow convolut layer pool layer often insert convolut layer cnn take n n pixel imag input layer consist group neuron call ﬁlter nel unlik neural network neuron featur extract layer cnn connect neuron adjac layer instead onli connect spatial map ﬁxed size partial overlap ron previou layer input imag featur map thi region input call local recept ﬁeld lower number connect reduc train time chanc overﬁt neuron ﬁlter connect number neuron previou input layer featur map constrain sequenc weight bias factor speed learn reduc memori requir network thu neuron speciﬁc ﬁlter look pattern differ part input imag layer reduc size network addit along local recept ﬁeld share weight within ﬁlter effect reduc network suscept shift scale distort imag pool local averag ﬁlter use often achiev ﬁnal layer cnn respons actual classiﬁc neuron layer fulli connect deep cnn implement multipl seri convolut layer layer deep natur cnn result high qualiti represent maintain local reduc paramet invari minor variat input imag case backpropag use sole train paramet weight bias cnn brief descript algorithm cost function respect individu train exampl x hidden layer volum shrestha mahmood review dl algorithm architectur figur architectur cnn charact recognit deﬁn j w b x b x equat error term δ layer l given δ l w l δ z l δ error l th layer network whose cost function j w b x f z l repres deriv activ function l j w b x δ l j w b x δ input input layer actual input imag l input l layer error layer calcul δ l k upsampl w l k δ k f z l k k repres ﬁlter number layer sampl layer error ha cascad opposit direct mean pool use upsampl evenli distribut error previou input unit ﬁnalli gradient featur map l k j w b x x l δ k l k j w b x x b δ k b l k repres convolut error input l layer respect k ﬁlter algorithm repres descript ﬂow backpropag algorithm use cnn goe multipl epoch either maximum iter reach cost function target met addit discrimin model imag nition cnn also use gener model deconvolv imag make blurri imag sharper algorithm cnn backpropag algorithm pseudo code initi weight randomli gener valu small set learn rate small valu posit iter n begin n max iter cost function criteria met imag xi forward propag convolut pool fulli conﬂect layer deriv cost fuction valu imag error term δ l respect weight type layer note error get propag layer layer follow sequenc connect layer layer layer gradient l k l k weight l k bia respect layer gradient calcul follow sequenc layer layer connect layer weight w l ji l ji l ji bia b l j l j l j refer achiev thi leverag fourier format regular invers blur imag denois differ implement cnn ha shown continu improv accuraci comput vision improv test benchmark imagenet ensur unbias result variat implement cnn architectur alexnet cnn develop run nvidia parallel ing platform support gpu volum shrestha mahmood review dl algorithm architectur figur linear represent data input use pca incept deep cnn develop googl resnet veri deep residu network develop microsoft place ilsvrc competit imagenet dataset vgg veri deep cnn develop larg scale imag recognit dcgan deep convolut gener adversari work propos use unsupervis learn hierarchi featur represent input object autoencod autoencod neural network use unsupervis rithm learn represent input data set dimension reduct recreat origin data set learn algorithm base implement backpropag autoencod extend idea princip compon analysi pca shown figur pca form data linear represent figur demonstr input data reduc linear vector use pca autoencod hand go produc nonlinear represent pca mine set linear variabl direct largest varianc p dimension input data point repres orthogon direct constitut lower less dimension space origin data point project princip direct thu ting inform correspond orthogon direct pca focus varianc rather covari correl look linear function varianc goal determin direct figur train stage autoencod least mean squar error would least reconstruct error autoencod use encod decod block hidden layer gener pca perform dimension reduct eventu reconstruct origin data use greedi layer layer unsupervis train backpropag despit use backpropag mostli use supervis train autoencod consid unsupervis dnn becaus regener input x instead differ set target valu x hinton et al abl achiev near perfect reconstruct imag use autoencod prove far better pca perform dimension reduct autoencod come interest represent input vector hidden layer thi often attribut smaller number node hidden layer everi second layer layer block even higher number node hidden layer sparsiti constraint enforc hidden unit retain interest lower dimens represent input achiev sparsiti node restrict ﬁring output set valu close zero figur show singl layer featur detector block rbm use follow volum shrestha mahmood review dl algorithm architectur figur autoencod node unrol unrol combin stack rbm creat encod block revers encod block creat decod section ﬁnalli network tune backpropag figur illustr simpliﬁ represent autoencod reduc dimens input data learn recreat output layer wang et al success implement deep autoencod stack rbm block similar figur achiev better ele accuraci efﬁcienc proper orthogon decomposit pod method dimension reduct distribut paramet system dpss equat describ averag activ function j jth unit layer xth input activ neuron ˆ ρj xm aj x sparsiti paramet ρ introduc ρ veri close zero ˆ ρ ensur ˆ ρ ρ penalti term kl ˆ ρj introduc kl diverg term kl ˆ ρj ˆ ρ els becom larg monoton differ two valu diverg updat cost function jspars w b j w b β x kl ˆ ρj equal number unit layer β paramet control sparsiti penalti term weight restrict boltzmann machin rbm restrict boltzmann machin artiﬁci neural work appli unsupervis learn algorithm figur restrict boltzmann machin build gener model unlabel data goal train network increas function product log probabl vector visibl unit probabilist reconstruct input learn probabl distribut input shown figur rbm made network call visibl layer hidden layer unit visibl layer connect unit hidden layer connect unit layer energi e function conﬁgur visibl hidden unit v h express follow way e v h x iεvis aivi x jεhidden bjhj x j vihjwij vi hj vector state visibl unit hidden unit ai bj repres bia visibl hidden unit wij denot weight respect visibl hidden unit partit function z repres sum possibl pair visibl hidden vector z x v h v h probabl everi pair visibl hidden vector given follow p v h z v h probabl particular visibl layer vector vide follow p v z x h v h see equat abov partit function becom higher lower energi function valu thu dure train process weight bias network adjust arriv lower energi thu maxim probabl assign train vector mathemat conveni comput deriv log probabl train vector p v volum shrestha mahmood review dl algorithm architectur figur lstm block memori cell gate equat abov sent expect respect distribut thu adjust weight denot follow ϵ learn rate ϵ long memori lstm lstm implement recurr neural network wa ﬁrst propos hochreit et al unlik earlier describ feed forward network ture lstm retain knowledg earlier state train work requir memori state ness lstm partli address major limit rnn problem vanish gradient let gradient pass unalt shown illustr figur lstm consist block memori cell state signal ﬂow regul input forget output gate gate control store read written cell lstm use googl appl amazon voic recognit platform ﬁgure c x h repres cell input output valu subscript denot time step valu previou lstm block time denot current block valu symbol σ sigmoid function tanh hyperbol tangent function oper wise summat x multipl comput gate describ equat ft σ wf xt wf bf σ wixt bi ot σ woxt bo ct ft wcxt bc ht ot ct f forget input output gate vector respect w w b weight input weight recurr output bia cation respect smaller variat lstm known gate recurr unit gru gru smaller size lstm includ output gate perform better lstm onli simpler dataset lstm recurr neural network keep track term depend therefor great learn sequenc input data build model reli context earlier state cell block lstm retain pertin inform previou state input forget output gate dictat new data go cell remain cell cell valu use calcul output lstm block respect naul et al demonstr lstm gru base autoencod automat featur extract comparison dnn network tabl provid compact summari comparison differ dnn architectur exampl mentat applic dataset dl softwar work present tabl impli exhaust addit categor network tectur could implement hybrid fashion even though rbm gener model train consid unsupervis element model train ﬁnetun supervis learn tabl also provid exampl common cation use differ architectur iv train algorithm learn algorithm constitut main part deep learn number layer differenti deep neural network shallow one higher number layer deeper becom layer special detect speciﬁc aspect featur indic najafabadi et al case imag face recognit ﬁrst layer detect edg second detect higher featur variou part face ear eye third layer go complex order even learn facial shape variou person even though layer might learn detect deﬁn featur sequenc alway design especi unsupervis learn featur tor layer manual program prior develop train algorithm gradient descent classiﬁ scale lager dataset adapt variat dataset thi messag wa echo paper yann lecun et demonstr system automat learn reduc manual design heurist yield far better pattern recognit backpropag provid represent learn olog raw data fed without need manual massag classiﬁ automat ﬁnd represent need classiﬁc recognit volum shrestha mahmood review dl algorithm architectur tabl dnn network comparison tabl goal learn algorithm ﬁnd optim valu weight vector solv class problem domain train algorithm gradient descent stochast gradient descent momentum algorithm backpropag time gradient descent gradient descent gd underli idea machin learn deep learn algorithm base concept newton algorithm ﬁnding root zero valu function achiev thi randomli pick point curv slide right left along base neg posit valu tive slope function chosen point valu function f x becom zero idea use gradient descent travers descend along certain path weight space cost function keep decreas stop onc error rate ceas decreas newton method prone get stuck local minima deriv function current point zero likewis thi risk also present use gradient descent function fact impact ampliﬁ dimens sent weight variabl landscap dnn result set weight cost function volum shrestha mahmood review dl algorithm architectur figur error calcul multilay neural network one half squar differ desir output minu current output shown c backpropag methodolog use gradient descent backpropag chain rule partial deriv employ determin error delta ani chang valu weight individu weight adjust reduc cost function everi learn iter train data set result ﬁnal weight landscap weight valu process sampl train dataset befor appli updat weight thi process repeat object aka cost function reduc ani figur show error deriv relat output hidden layer weight summat error deriv relat input unit abov layer calcul partial error deriv respect wjk equal stochast gradient descent stochast gradient descent sgd common variat implement gradient descent gradient descent process sampl train dataset befor appli updat weight sgd updat appli run batch n number sampl sinc updat weight frequent sgd gd converg toward global minimum much faster momentum standard sgd learn rate use ﬁxed multipli gradient comput step size updat weight thi caus updat overshoot potenti minima gradient steep delay converg gradient noisi use concept momentum physic momentum algorithm present veloc v variabl conﬁgur exponenti decreas averag gradient thi help prevent costli descent wrong direct equat α momentum paramet ϵ learn rate veloc updat v actual updat θ v algorithm algorithm lma primarili use solv least squar problem curv ting least squar problem tri ﬁt given data point function least amount sum squar error actual data point point function lma use combin gradient descent method gradient descent employ reduc sum squar error updat paramet function direct descent method minim error assum function local quadrat ﬁnd minimum quadrat ﬁtting function denot ˆ p data point denot ti yi squar error written p xm ti ti p σyi p w p yt wy w ˆ ˆ yt w ˆ measur error ti σyi invers weight matrix wii gradient descent squar error function relat n paramet denot p w p p w p yˆ wj hgd αjt w j jacobian matrix size n use place hgd updat direct steepest gradient descent equat method updat hgn follow h jt wj hgn jt w volum shrestha mahmood review dl algorithm architectur marquardt updat hlm gener combin gradient descent method result equat h jt wj λ diag jt wj hlm jt w backpropag time backpropag time bptt standard method train recurr neural network shown figur unrol rnn time make appear like feedforward network unlik feedforward work unrol rnn ha exact set weight ue layer repres train process time domain backward pass thi time domain work calcul gradient respect speciﬁc weight layer averag updat weight differ time increment layer chang ensur valu weight layer continu stay uniform comparison deep learn algorithm tabl provid summari comparison common deep learn algorithm advantag disadvantag present along techniqu address disadvantag gradient train common type train backpropag time tion tailor recurr neural network contrast genc ﬁnd use probabilist model rbm evolutionari algorithm appli hyperparamet optim train model optim weight reinforc learn could use game theori agent system problem exploit explor need optim shortcom train algorithm sever shortcom standard use train algorithm dnn common one describ vanish explod gradient deep neural network prone vanish ing gradient due inher way gradient deriv comput layer layer ing manner layer contribut exponenti decreas increas deriv weight increas decreas base gradient reduc cost tion error veri small gradient caus network take long time train wherea larg gradient caus train overshoot diverg thi made wors activ function like sigmoid tanh function squash output small rang sinc chang weight nomin effect output ing could take much longer thi problem mitig use linear activ function like relu proper weight initi tabl deep learn algorithm comparison tabl local minima local minima alway global minima convex function make gradient descent base optim fool proof wherea nonconvex function tion base gradient descent particularli vulner issu prematur converg local minima local minima shown figur easili mistaken global absolut minima flat region like local minima ﬂat region saddl point figur also pose similar challeng gradient descent base optim nonconvex tion train algorithm could potenti mislead thi area gradient come halt thi point steep edg steep edg anoth section optim face area steep gradient could caus gradient volum shrestha mahmood review dl algorithm architectur figur gradient descent figur flat saddl point mark black dot region nonconvex function weight updat overshoot miss tial global minima train time train time import factor gaug efﬁcienc algorithm uncommon graduat student train model day week comput lab model requir exorbit amount time larg dataset train often time mani sampl dataset add valu train process case introduc nois advers affect train overfit add neuron dnn undoubtedli model network complex problem dnn lend high conform train data also high risk overﬁt outlier nois train data shown figur thi result delay train test time result lower qualiti predict actual test data classiﬁc cluster problem overﬁt creat high order polynomi output separ decis boundari train set take longer result degrad result test figur overfit classif data set one way overcom overﬁt choos number neuron hidden layer wise match problem size type algorithm use approxim appropri number neuron magic bullet best bet experi use case get optim valu vi optim train algorithm goal dnn improv accuraci model test data train algorithm aim achiev end goal reduc cost function common root caus three ﬁve shortcom mention abov primarili due fact train algorithm assum problem area convex function problem high number node sheer possibl combin weight valu weight learn train dataset addit crucial paramet refer hyperparamet directli learnt train dataset hyperparamet take rang valu add complex ﬁnding optim architectur model signiﬁc room improv standard train algorithm popular way enhanc accuraci dnn paramet initi techniqu sinc solut space huge initi paramet outsiz inﬂuenc fast slow ing converg prematur converg suboptim point initi strategi tend heurist natur refer propos normal initi weight initi follow manner w refer propos anoth techniqu call spars initi number incom weight cap certain limit caus retain high divers reduc chanc satur volum shrestha mahmood review dl algorithm architectur hyperparamet optim learn rate regular paramet constitut commonli use hyperparamet dnn learn rate determin rate weight updat purpos regular prevent overﬁt ular paramet affect degre inﬂuenc loss function cnn addit hyperparamet number ﬁlter ﬁlter shape number dropout max pool shape convolut layer number node fulli connect layer paramet veri import train model dnn come optim set paramet valu ing feat exhaust iter combin hyperparamet valu comput veri expens exampl train evalu dnn full dataset take ten minut seven hyperparamet eight potenti valu take min minut almost year exhaust train evalu network combin ramet valu hyperparamet optim ent metaheurist metaheurist natur inspir guid principl help travers search space intellig yet much faster exhaust method particl swarm optim pso anoth type metaheurist use hyperparamet tion pso model around bird ﬂy around search food dure migrat veloc locat bird particl adjust steer swarm toward better solut vast search space escalant et al use pso hyperparamet optim build competit model rank among top rel compar method genet algorithm ga metaheurist monli use solv combinatori optim problem mimic select crossov process speci reproduct contribut evolut improv speci prospect surviv figur show diagram figur illustr crossov process part respect genet sequenc merg parent form new genet sequenc children goal ﬁnd ulat member sequenc number resembl dna nucleotid meet ﬁtness requir ulat member repres potenti solut popul member select base differ method elit roulett rank tournament elit method rank popul member ﬁtness onli use high ﬁtness member crossov process mutat process make random chang number sequenc entir process continu desir ﬁtness maximum number iter reach refer propos parallel tion ga achiev better faster result parallel provid speedup better result period exchang popul member distribut parallel oper genet algorithm differ set figur genet algorithm b crossov genet algorithm popul member hybrid process mix primari algorithm ga thi case oper like local search shrestha mahmood incorpor local search method ga improv search optim solut refer postul correctli perform exchang ga breed innov result creation solut hard problem like real life collabor exchang vidual organ societi addit ga variat metaheurist also use evolv optim deep learn architectur hyperparamet propos codeepneat framework base deep neuroevolut techniqu ﬁnding optim architectur match task hand volum shrestha mahmood review dl algorithm architectur adapt learn rate learn rate huge impact train dnn speed train time help navig ﬂat surfac ter overcom pitfal function adapt learn rate allow us chang learn rate eter respons gradient momentum sever tive method propos refer describ follow algorithm adagrad rmsprop adam algorithm learn rate eter increas partial deriv respect stay sign decreas sign chang adagrad sophist prescrib invers proport scale learn rate squar root cumul squar gradient adagrad effect dnn train sinc chang learn rate function histor gradient adagrad becom suscept converg rmsprop algorithm modiﬁc adagrad rithm make effect nonconvex problem space rmsprod replac summat squar gradient adagrad exponenti decay move averag gradient effect drop impact histor ent adam denot adapt moment estim latest evolut adapt learn algorithm integr idea adagrad rmsprop tum like adagrad rmsprod adam provid individu learn rate paramet adam includ beneﬁt earlier method doe better job handl object noisi spars gradient problem adam use ﬁrst moment mean use rmsprop well second moment dient uncent varianc util exponenti move averag squar gradient figur show rel perform variou adapt learn rate mechan adam outperform rest batch normal network get train variat weight paramet distribut actual data input layer dnn chang often make larg small thu make difﬁcult train network especi activ function implement satur nonlinear sigmoid tanh function iofe szegedi propos idea batch izat ha made huge differ improv train time accuraci dnn updat input unit varianc zero mean supervis pretrain supervis pretrain constitut break complex problem smaller part train simpler figur multilay network train cost mnist dataset use differ adapt learn algorithm figur dnn without dropout model later combin solv larger model greedi algorithm commonli use supervis train dnn dropout commonli use method lower risk overﬁt dropout techniqu randomli choos unit nullifi weight output inﬂuenc forward pass backpropag figur show fulli connect dnn left dnn dropout right method includ use regular simpli enlarg train dataset use label preserv techniqu dropout work better regular reduc risk overﬁt also speed train process refer volum shrestha mahmood review dl algorithm architectur propos dropout techniqu demonstr signiﬁc improv supervis learn base dnn puter vision comput biolog speech recognit document classiﬁc problem train speed cloud gpu process train time one key perform indic machin learn cloud comput gpu lend selv veri well speed train process cloud provid massiv amount comput power major cloud vendor includ gpu power server easili provis use train dnn demand competit price cloud vendor amazon web servic aw instanc provid thousand parallel gpu core gpu instanc optim machin learn summari dl algorithm shortcom resolut techniqu tabl provid summari deep learn algorithm come resolut techniqu tabl also list caus effect shortcom vii architectur algorithm implement thi section describ differ implement neural network use varieti train method network tectur model also includ model idea incorpor machin learn gener deep residu learn abil add layer dnn ha allow us solv harder problem microsoft research asia msra appli layer deep residu network resnet dataset place ilsvrc tion dnn imagenet dataset figur demonstr simpliﬁ version microsoft win deep residu learn model despit depth network simpli ad layer dnn doe improv guarante result contrari degrad qualiti solut thi make train dnn straight forward msra team wa abl overcom degrad make hope stack layer match residu map instead desir map follow function f x h x f x residu map h x desir map recast desir map end accord msra team much easier optim residu map oddbal stochast gradient descent train data creat equal higher train error yet assum tabl dl algorithm shortcom resolut techniqu figur deep residu learn model msra microsoft thu use train exampl number time simpson argu thi assumpt invalid make case hi paper number time train exampl use proport respect train error train exampl ha higher error rate use train network higher number time volum shrestha mahmood review dl algorithm architectur train exampl simpson prove hi methodolog term oddbal stochast gradient descent train set video frame simpson ate train select probabl distribut train exampl base error valu peg frequenc use train exampl base distribut deep belief network chen lin highlight fact convent neural network easili get stuck local minima tion propos dnn architectur call larg scale deep belief network dbn use label unlabel learn featur represent dbn made layer rbm stack togeth learn probabl distribut input vector employ unsupervis supervis algorithm niqu mitig risk get trap local minima equat chang weight c momentum factor α learn rate v h visibl hidden unit respect α equat probabl distribut hidden visibl input p hj w σ x wijvi aj p vi w σ j x wijhj bi big data big data provid tremend opportun challeng deep learn big data known vs volum iti verac varieti unlik shallow network huge volum varieti data handl dnn signiﬁcantli improv train process abil ﬁt complex model ﬂip side sheer iti data gener daunt process jajafabadi et al rais similar challeng learn stream data credit card usag monitor fraud detect propos use parallel distribut process thousand cpu core addit also use cloud provid support base usag workload data repres qualiti case comput vision imag constrain sourc studio much easier recogn one unconstrain sourc like surveil camera refer propos method util multipl imag unconstrain sourc enhanc recognit process deep learn help mine extract use pattern big data build model infer predict busi decis make massiv volum structur unstructur data media ﬁle get figur learn multipl layer represent gener today make inform retriev veri leng deep learn help semant index enabl inform readili access search engin thi involv build model provid relationship document keyword contain make inform retriev effect gener top connect gener model much train usual implement approach discriminatori recognit model develop use backpropag model one take vector represent input object comput higher level featur represent subsequ layer ﬁnal discrimin recognit pattern output layer one shortcom backpropag requir label data train geoffrey hinton propos novel way overcom thi limit propos dnn use gener connect oppos connect mimic way gener visual imageri dream without actual sensori input gener connect data represent put network use gener raw vector represent origin input one layer time layer featur represent learn thi approach perfect either gener model even standard recognit model gener model figur sinc correct upstream caus event layer known parison actual caus predict made approxim infer procedur made recognit weight rij adjust increas probabl correct predict volum shrestha mahmood review dl algorithm architectur figur dbn deep boltzmann machin equat adjust recognit weight rij α hi hj x hirij unsupervis deep boltzmann machin vast major dnn train base supervis ing real life learn base supervis unsupervis learn fact learn pervis unsupervis learn relev today age big data analyt becaus raw data unlabel one way overcom limit backpropag get stuck local minima incorpor supervis unsupervis train quit evid gener unsupervis learn good gener becaus essenti adjust weight tri match recreat input data layer time thi effect unsupervis train alway label data geoffrey hinton ruslan salakhutdinov describ multipl layer rbm stack togeth train layer layer greedi unsupervis way essenti creat call deep belief network modifi stack make model symmetr weight thu creat deep boltzmann machin dbm four layer deep belief network deep boltzmann machin shown figur dbm layer train one time use unsupervis method tweak use supervis backpropag mnist norb dataset shown figur receiv favor result valid beneﬁt combin supervis unsupervis learn method equat show probabl distribut visibl two hidden unit dbm unsupervis figur pretrain stack alter rbm creat dbm figur dbm get initi determinist neural network supervis p vi σ j w ijhj p σ j w j p j σ x w x w post unsupervis dbm convert determinist neural network tune network supervis learn use label data demonstr figur approxim posterior distribut q gener input tor margin q j ad tional input network shown ﬁgure abov subsequ backpropag use network volum shrestha mahmood review dl algorithm architectur extrem learn machin elm variat learn methodolog layer allow us extract complex featur pattern problem might solv faster ter less number layer refer propos cnn term deepbox outperform larger network speed accuraci evalu object elm anoth type neural network one den layer linear model learnt dataset singl iter adjust weight den layer output wherea weight input hidden layer randomli initi ﬁxed elm obvious converg much faster agat onli appli simpler problem classiﬁc regress sinc propos elm huang et al came multilay version elm take complex problem combin unsupervis multilay encod random initi weight demonstr faster converg lower train time state art multilay perceptron train algorithm multiobject spars featur learn model gong et al develop spars featur learn model base auto encod use evolutionari algorithm optim two compet object sparsiti hidden unit reconstruct error input vendor ae fair better model sparsiti determin human intervent less optim method sinc time complex evolutionari algorithm high util tial evolut de base decomposit cut time demonstr ha better result standard ae auto encod spars respons rmb sesm spars encod symmetr machin test mnist dataset compar result implement learn procedur ousli iter evolutionari optim step stochast gradient descent optim reconstruct error step optim select optim point pareto frontier object step optim paramet θ θ stochast gradient descent follow reconstruct error function auto encod train data set l x loss function x repres input repres output reconstruct input x l x f θ x figur show pareto frontier function use achiev compromis two compet object function figur pareto frontier figur spectral cluster represent multiclass learn base kernel spectral cluster mehrkanoon et al propos multiclass learn rithm base kernel spectral cluster ksc use label unlabel data novelti propos introduct regular term ad cost function ksc allow label membership appli unlabel data exampl achiev follow way unsupervis learn base kernel spectral ing ksc use core model regular term introduc label label data ad model figur illustr data point spectral cluster represent spectral cluster sc algorithm divid data point graph use laplacian doubl deriv oper wherea ksc simpli extens sc use least squar support vector machin methodolog volum shrestha mahmood review dl algorithm architectur sinc unlabel data abundantli avail rel label data would beneﬁci make unsupervis thi case learn veri deep convolut network natur languag process deep cnn mostli use comput vision veri effect conneau et al use ﬁrst time nlp convolut layer goal analyz extract layer hierarch represent word sentenc syntact semant textual level one major setback lack earlier deep cnn nlp becaus deeper network tend caus satur degrad accuraci thi addit process overhead layer et al state degrad caus overﬁt becaus deeper system difﬁcult optim refer address thi issu shortcut connect convolut block let gradient propag freeli along abl valid beneﬁt shortcut layer respect conneau et al architectur sist seri convolut block separ pool halv resolut follow pool classiﬁc end metaheurist metaheurist use train neural network overcom limit learn implement metaheurist train algorithm weight neural network connect repres dimens solut search space problem tri solv goal come near possibl optim valu weight locat search space repres global best solut particl swarm optim pso type metaheurist inspir movement bird sky consist particl candid solut move search space reach near optim solut paper krpan jakobov ran parallel mentat use backpropag pso result demonstr parallel improv efﬁcaci algorithm parallel backpropag efﬁcient onli larg network wherea parallel pso ha wider inﬂuenc variou size problem similarli dong zhou complement pso supervis learn control modul guid search global minima optim problem supervis learn modul provid feedback back fusion bd retain divers social attractor renew overcom stagnat metaheurist provid high level guidanc inspir natur appli solv mathemat problem similar way propos porat concept intellig teacher privileg inform essenti extra inform avail dure train dure evalu test dnn train process genet algorithm genet algorithm metaheurist effect use train dnn ga mimic evolutionari process select crossov mutat popul ber repres possibl solut set weight unlik pso includ onli one oper adjust tion evolutionari algorithm like ga includ variou step select crossov mutat method lation member undergo sever iter select crossov base known strategi achiev better solut next iter gener ga ha undergon decad improv reﬁnement sinc wa ﬁrst propos sever way perform tion elit roulett rank tournament dozen way perform crossov larrañaga et al alon select methodolog repres explor solut space crossov repres exploit select solut candid goal get better tion wider explor deeper exploit addit tweak introduc mutat parallel cluster ga execut independ island member exchang island everi often addit also util local search greedi algorithm nearest neighbor algorithm improv qualiti solut lin et al demonstr success incorpor ga result better classiﬁc accuraci perform polynomi neural network standard ga oper includ select crossov mutat use paramet includ partial descript pd input ﬁrst layer bia input featur ga wa enhanc incorpor concept mitochondri dna mtdna evolut quit evid casual observ simpl reason crossov popul member much similar doe yield much varianc offspr likewis infer ga select crossov solut veri similar would result high degre explor solut space fact might run risk get pigeonhol restrict pattern divers key overcom risk get stuck local minima thi risk mitig exploit idea mtdna mtdna repres one percent human chromosom concept incorpor drial dna ga wa introduc shrestha mood describ way restrict crossov popul member solut candid base proxim mtdna valu unlik rest dna mtdna onli inherit femal thu continu marker lineag genet proxim premis behind thi offspr popul member similar genet makeup help overcom volum shrestha mahmood review dl algorithm architectur figur continent model mtdna local minima figur describ parallel tribut natur full implement along ga oper select mutat mtdna rate crossov train process enhanc implement continent model distribut server run multipl thread run instanc ga mtdna popul member exchang server ﬁxed number iter shown figur neural machin translat nmt neural machin translat turnkey solut use translat sentenc provid improv tradit statist machin translat smt scalabl larg model dataset also requir lot comput power train translat ha difﬁcult rare word reason larg tech compani like googl microsoft improv nmt implement nmt label googl neural machin translat gnmt skype translat respect gmnt shown figur sist encod decod lstm block organ layer wa present overcom shortcom nmt enhanc deep lstm neural network includ encod decod layer method break rare difﬁcult word infer mean confer machin translat gnmt receiv result par languag benchmark learn imag real life includ multipl instanc object need multipl label describ ture ofﬁc space could includ laptop comput desk cubicl person type comput zhou et al propos miml learn framework correspond mimlboost mimlsvm algorithm efﬁcient learn individu object label complex high level concept like ofﬁc space goal learn f dataset xm ym xi repres set instanc xi ni xij j ni yi repres set instanc yi li yik k li ni number instanc xi li number label yi mimlboost use decomposit dition singl instanc singl label supervis learn wherea mimlsvn util featur mation instead tri learn idea complex entiti ofﬁc space took altern rout learn lower level individu object infer higher level concept adversari train machin learn train deploy use done isol comput increas done highli interconnect commerci product environ take face recognit system network could train ﬂeet server train dataset import extern data sourc train model could deploy anoth server accept api call real time input imag peopl enter build respond match interconnect architectur expos machin learn wide attack surfac input train dataset manipul volum shrestha mahmood review dl algorithm architectur figur gnmt architectur encod neural network left decod neural network right adversari compromis output imag match network entir model respect adversari machin learn rel new ﬁeld research take new threat machin learn accord adversari email spammer exploit lack stationari data distribut ulat input actual spam email normal email refer demonstr vulner discuss applic domain featur data distribut use reduc risk impact adversari attack gaussian mixtur model gaussian mixtur model gmm statist probabilist model use repres multipl normal gaussian tion within larger distribut use em estim maxim algorithm unsupervis set gmm could use repres height distribut larg popul group two gaussian distribut male femal figur demonstr gmm three gaussian distribut within gmm ha use primarili speech recognit track object video sequenc gmm veri tive extract speech featur model abil densiti function desir level accuraci long sufﬁcient compon tion maxim make easi ﬁt model probabl densiti function gmm given follow p x xm cmn x µm cm figur gmm exampl three compon number number gaussian compon cm weight gaussian x µm repres random variabl x follow mean vector µm siames network purpos siames network determin degre similar two imag shown ﬁgure siames network consist two ident cnn network ident weight paramet two imag compar pass separ two twin cnn respect vector represent output ate use contrast diverg loss function function deﬁn follow l w dw max volum shrestha mahmood review dl algorithm architectur figur siames network dw repres euclidean distanc two output vector shown ﬁgure output contrast diverg loss function either indic imag indic imag repres margin valu greater idea siames network ha extend come triplet network includ three ident network use assess similar given imag two imag sinc softmax layer output must match number class standard cnn becom impract problem larg number class thi issu appli siames network number output softmax twin network requir match number class thi abil scale mani class classiﬁc extend use siames network beyond tradit cnn use siames network use handwritten check recognit signatur veriﬁc text similar etc variat autoencod name suggest variat autoencod vae type autoencod consist encod decod part shown ﬁgure fall gener model class neural network use unsupervis learn vae learn low dimension represent latent variabl model origin high dimension dataset gaussian distribut kl diverg method good way compar distribut therefor loss function vae combin cross entropi mean squar error minim reconstruct error kl diverg make compress latent variabl follow gaussian distribut sampl probabl distribut gener new dataset sampl tativ origin dataset ha found variou applic figur variat autoencod includ gener imag video game pictur ﬁgure x input z encod output latent variabl p x repres distribut associ p z repres distribut associ goal infer p z base p follow certain distribut mathemat deriv vae origin propos suppos want infer p base q tri minim kl diverg two dkl q xq log q p z e log q p e log q dkl kl diverg e repres expect use bay rule p p p z p x dkl q e log q p z p x e log q p p z log p x allow us easili sampl p z gener new data set p z normal distribut n q repres gaussian paramet µ x p x kl diverg q p z deriv close form dkl n µ x x x k exp x x x deep reinforc learn primari idea reinforc learn ing agent learn environ help random experiment explor deﬁn reward exploit consist ﬁnite number state si resent agent environ action ai agent probabl pa move one state anoth base action ai reward ra si associ move next state action goal balanc maxim current reward r futur reward γ max q predict best action deﬁn thi function q γ equat repres ﬁxed discount factor q repres summat volum shrestha mahmood review dl algorithm architectur current reward r futur reward γ max q shown q r γ max q reinforc learn speciﬁc suit problem consist reward game like chess go etc alphago googl program beat human go champion also use reinforc ing combin deep network architectur reinforc learn get deep reinforc learn drl extend use reinforc even complex game area robot smart grid healthcar ﬁnanc etc drl problem intract reinforc learn solv higher number hidden layer deep network reinforc learn base algorithm imiz reward action taken agent gener adversari network gan gan consist gener discrimin neural work gener network gener complet new fake data base input data unsupervis learn discrimin network attempt distinguish whether data real train set gener gener network train increas probabl deceiv discrimin network make gener data indistinguish origin gan propos goodfellow et al ha veri popular ha mani applic good bad abl success synthes realist imag text method enhanc deep learn deep learn optim differ area cuss train algorithm enhanc parallel process paramet optim variou architectur area simultan implement framework get best result speciﬁc problem train rithm ﬁnetun differ level incorpor heurist hyperparamet optim time train deep learn network model major factor gaug perform algorithm network instead train network data set select smaller repres data set full train distribut set use instanc select method mont carlo sampl effect sampl method result prevent overﬁt improv accuraci speed learn process without compromis qualiti train dataset albelwi mahmood design framework combin dataset reduct deconvolut network correl coefﬁcient updat object function method wa use optim paramet object function result compar latest known result mnist dataset thu combin optim tipl level use multipl method promis ﬁeld research lead advanc machin learn viii conclus thi tutori provid thorough overview neural network deep neural network took deeper dive train algorithm architectur highlight shortcom get stuck local minima overﬁt train time larg lem set examin sever way come challeng differ optim method investig adapt learn rate hyperparamet optim effect method improv accuraci network survey review sever recent paper studi present implement improv train process also includ tabl summar content concis manner tabl provid full view differ aspect deep learn correl deep learn still nascent stage tremend opportun exploit current explor optim method solv complex problem train rentli constrain overﬁt train time highli suscept get stuck local minima continu overcom challeng deep learn work acceler breakthrough across applic machin learn artiﬁci intellig conflict interest author declar conﬂict interest found sponsor role design studi lection analys interpret data write manuscript decis publish result orcid ajay shrestha http refer rosenblatt perceptron probabilist model inform storag organ brain psychol vol pp minski papert perceptron introduct tional geometri expand edit cambridg usa mit press cybenko approxim superposit sigmoid function math control signal vol pp hornik approxim capabl multilay feedforward work neural vol pp werbo beyond regress new tool predict analysi behavior scienc dissert harvard cambridg usa lecun bengio hinton deep learn natur vol pp may jordan mitchel machin learn trend perspect prospect scienc vol pp ng machin learn yearn technic strategi ai engin era deep learn tech metz ture award pioneer artiﬁci intellig new york ny usa new york time volum shrestha mahmood review dl algorithm architectur nagpal et develop valid deep learn algorithm improv gleason score prostat cancer corr nevo ml ﬂood forecast scale corr esteva et classiﬁc skin cancer deep neural network natur vol arulkumaran deisenroth brundag bharath deep reinforc learn brief survey ieee signal process vol pp gheisari wang bhuiyan survey deep learn big data proc ieee int conf comput sci eng cse jul pp pouyanfar survey deep learn algorithm techniqu applic acm comput vol varga mosavi ruiz deep learn review proc adv intel syst pp buhmann radial basi function cambridg cambridg univ press akinduko mirk gorban som tic initi versu princip compon inf vol pp chen deep modular neural network springer handbook comput intellig kacprzyk pedrycz ed berlin germani springer pp ng jordan discrimin gener classiﬁ comparison logist regress naiv bay proc int conf neural inf process syst cambridg usa mit press pp bishop lasserr gener discrimin get best world bayesian vol pp zhou brown snave low unsupervis learn depth video corr apr chen lin big data deep learn challeng tive ieee access vol pp lecun kavukcuoglu farabet convolut network applic vision proc ieee int symp circuit pp gousio vasilescu serebrenik zaidman lean rent github data demand proc work conf mine softw repositori hyderabad india pp top deep learn github repositori onlin avail http cernada barro amorim need hundr classiﬁ solv real world classiﬁc problem mach learn vol pp lecun bottou bengio haffner ing appli document recognit proc ieee vol pp lecun bengio convolut network imag speech time seri handbook brain theori neural network michael ed cambridg usa mit press pp taylor fergu lecun bregler convolut ing featur comput vision berlin germani springer ng jul convolut neural network ufldl onlin avail http schuler burger harmel schölkopf machin learn approach imag deconvolut proc ieee conf comput vi pattern jun pp radford metz chintala unsupervis represent learn deep convolut gener adversari network corr jolliff princip compon analysi mathemat tic ed new york ny usa springer noda multimod integr learn object manipul ior use deep neural network proc int conf intel robot pp hinton salakhutdinov reduc dimension data neural network scienc vol pp wang li chen chen deep model reduct distribut paramet system ieee tran man vol pp ng jul autoencod ufldl onlin avail http teh hinton restrict boltzmann machin face recognit proc adv neural inf process pp hinton practic guid train restrict boltzmann machin neural network trick trade montavon orr müller ed berlin germani springer pp hochreit schmidhub long memori neural vol pp metz appl bring ai revolut phone wire tech ger schmidhub cummin learn forget continu predict lstm neural vol pp chung empir evalu gate recurr neural work sequenc onlin avail http cho learn phrase represent use rnn decod statist machin onlin avail http naul bloom pérez van der walt recurr neural network classiﬁc unevenli sampl variabl star natur vol pp najafabadi villanustr khoshgoftaar seliya wald muharemag deep learn applic challeng big data analyt big data vol goodfellow bengio courvil deep learn adapt comput machin learn cambridg usa mit press gavin method nonlinear least squar problem tech glorot bengio understand difﬁculti train deep feedforward neural network proc int conf artif intel pp marten deep learn via optim proc int conf int conf mach learn haifa israel omnipress pp escalant mont sucar particl swarm model select mach learn vol pp shrestha mahmood improv genet algorithm tune crossov scale architectur j vol mar sastri goldberg kendal genet algorithm goldberg design innov lesson compet genet algorithm boston usa springer miikkulainen evolv deep neural network corr mar duchi hazan singer adapt subgradi method onlin learn stochast optim mach learn vol pp jul kingma ba adam method stochast optim corr ioff szegedi batch normal acceler deep network train reduc intern covari shift corr srivastava hinton krizhevski sutskev salakhutdinov dropout simpl way prevent neural network overﬁt mach learn vol pp aw servic jul amazon instanc zon instanc type onlin avail http http zhang ren sun deep residu learn imag recognit proc ieee conf comput vi pattern recognit cvpr jun pp simpson uniform learn deep neural network via oddbal stochast gradient descent corr han otto klare jain unconstrain face recognit identifi person interest media collect ieee tran inf forens secur vol pp letsch berri inform retriev latent semant index inf vol pp volum shrestha mahmood review dl algorithm architectur hinton learn multipl layer represent trend cognit vol pp salakhutdinov hinton deep boltzmann machin proc int conf artif intel van max ed pp kuo hariharan malik deepbox learn object convolut network corr may huang zhu siew extrem learn machin ori applic neurocomput vol pp tang deng huang extrem learn machin multilay perceptron ieee tran neural netw learn vol pp apr gong liu li cai su multiobject spars featur learn model deep neural network ieee tran neural netw learn vol pp mehrkanoon alzat mall langon suyken multiclass semisupervis learn base upon kernel spectral ing ieee tran neural netw learn vol pp apr langon mall alzat suyken kernel spectral cluster applic corr may conneau schwenk barrault lecun veri deep lution network text classiﬁc corr jun krpan jakobov parallel neural network train opencl proc int conv mipro may pp dong zhou supervis learn control method improv particl swarm optim algorithm ieee tran man cybern vol pp jul vapnik izmailov learn use privileg inform lariti control knowledg transfer mach learn vol pp sampson adapt natur artiﬁci system vol holland ed philadelphia pa usa siam pp razali geraghti genet algorithm perform differ select strategi solv tsp proc world congr pp larrañaga kuijper murga inza dizdarev genet algorithm travel salesman problem review resent oper artif intel vol pp apr whitley genet algorithm tutori statist vol pp jun lin prasad saxena improv polynomi neural network classiﬁ use genet algorithm ieee tran man vol pp guo et use next gener sequenc technolog studi effect radiat therapi mitochondri dna mutat mutat toxicol environ mutagenesi vol pp wu googl neural machin translat system bridg gap human machin translat corr zhou zhang huang li label learn artif vol pp huang joseph nelson rubinstein tygar adversari machin learn proc acm workshop secur artif chicago il usa pp yu deng automat speech recognit deep learn approach london springer hadsel chopra lecun dimension reduct ing invari map proc ieee comput soc conf comput vi pattern recognit cvpr jun pp shrestha mahmood enhanc siames network train import sampl proc int conf agent artif intel pragu czech republ scitepress pp kingma well variat onlin avail http silver et master game go deep neural network tree search natur vol henderson islam bellemar pineau introduct deep reinforc learn corr goodfellow et al gener adversari onlin avail http reed akata yan logeswaran schiel lee gener adversari text imag onlin avail http brighton mellish advanc instanc select base learn algorithm data mine knowl discoveri vol pp albelwi mahmood framework design architectur deep convolut neural network entropi vol ajay shrestha receiv degre comput engin degre puter scienc univers bridgeport ct usa respect current pursu degre comput scienc engin ha guest lectur pennsylvania state versiti also adjunct faculti school engin univers bridgeport thermo fisher scientiﬁc branford ct usa manag technic oper hi research interest includ machin learn metaheurist ha serv technic committe member intern confer system comput scienc softwar engin scss receiv academ excel award graduat research assistantship hi graduat graduat studi respect ha serv chapter vice presid ofﬁcer upsilon pi epsilon upe sinc receiv upe execut council award present upe execut council ausif mahmood sm receiv degre electr comput neer washington state univers usa current chair person puter scienc engin depart professor comput scienc neer depart electr engin depart univers bridgeport bridgeport ct usa hi research interest includ parallel distribut comput comput vision deep learn comput architectur volum