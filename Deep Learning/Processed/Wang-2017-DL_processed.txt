origin deep learning origin deep learning haohan wang haohanw bhiksha raj bhiksha language technology institute school computer science carnegie mellon university abstract paper review evolutionary history deep learning model cover genesis neural network associationism modeling brain studied model dominate last decade research deep learning like convolutional neural network deep belief network recurrent neural network addition review model paper primarily focus precedent model examining initial idea assembled construct early model preliminary model developed current form many evolutionary path last half century diversity direction example cnn built prior knowledge biological vision system dbn evolved modeling power computation complexity graphical model many nowadays model neural counterpart ancient linear model paper review evolutionary path oﬀers concise thought ﬂow model developed aim provide thorough background deep learning importantly along path paper summarizes gist behind milestone proposes many direction guide future research deep learning 1 3 mar 2017 wang raj introduction deep learning ha dramatically improved many diﬀerent artiﬁcial intelligent task like object detection speech recognition machine translation lecun et 2015 deep architecture nature grant deep learning possibility solving many complicated ai task bengio 2009 result researcher extending deep learning variety diﬀerent modern domain task additional traditional task like object detection face recognition language model example osako et al 2015 us recurrent neural network denoise speech signal gupta et al 2015 us stacked autoencoders discover clustering pattern gene expression gatys et al 2015 us neural model generate image diﬀerent style wang et al 2016 us deep learning allow sentiment analysis multiple modality simultaneously etc period era witness blooming deep learning research however fundamentally push deep learning research frontier forward one need thoroughly understand ha attempted history current model exist present form paper summarizes evolutionary history several diﬀerent deep learning model explains main idea behind model relationship ancestor understand past work not trivial deep learning ha evolved long time history showed table therefore paper aim oﬀer reader major milestone deep learning research cover milestone showed table 1 well many additional work split story diﬀerent section clearness presentation paper start discussion research human brain modeling although success deep learning nowadays not necessarily due resemblance human brain due deep architecture ambition build system simulate brain indeed thrust initial development neural network therefore next section begin connectionism naturally lead age shallow neural network matures maturity neural network paper continues brieﬂy discus cessity extending shallow neural network deeper one well promise deep neural network make challenge deep architecture introduces establishment deep neural network paper diverges three ferent popular deep learning topic speciﬁcally section 4 paper elaborates deep belief net construction component restricted boltzmann machine evolve modeling power computation load section 5 paper focus development history convolutional neural network featured prominent step along ladder imagenet competition section 6 paper discus ment recurrent neural network successor like lstm attention model success achieved paper primarily discus deep learning model optimization deep tecture inevitable topic society section 7 devoted brief summary optimization technique including advanced gradient method dropout batch tion etc paper could read complementary schmidhuber 2015 schmidhuber paper aimed assign credit contributed present state art paper focus every single incremental work along path therefore not 2 origin deep learning table 1 major milestone covered paper year contributer contribution 300 bc aristotle introduced associationism started history human attempt understand brain 1873 alexander bain introduced neural grouping earliest model neural network inspired hebbian learning rule 1943 mcculloch pitt introduced mcp model considered ancestor artiﬁcial neural model 1949 donald hebb considered father neural network introduced hebbian learning rule lay foundation modern neural network 1958 frank rosenblatt introduced ﬁrst perceptron highly resembles modern perceptron 1974 paul werbos introduced backpropagation 1980 teuvo kohonen introduced self organizing map kunihiko fukushima introduced neocogitron inspired convolutional neural network 1982 john hopﬁeld introduced hopﬁeld network 1985 hilton sejnowski introduced boltzmann machine 1986 paul smolensky introduced harmonium later known restricted boltzmann machine michael jordan deﬁned introduced recurrent neural network 1990 yann lecun introduced lenet showed possibility deep neural network practice 1997 schuster paliwal introduced bidirectional recurrent neural network hochreiter schmidhuber introduced lstm solved problem vanishing gradient recurrent neural network 2006 geoﬀrey hinton introduced deep belief network also introduced pretraining technique opened current deep learning era 2009 salakhutdinov hinton introduced deep boltzmann machine 2012 geoﬀrey hinton introduced dropout eﬃcient way training neural network 3 wang raj orate well enough hand paper aimed providing background reader understand model developed therefore phasize milestone elaborate idea help build association idea addition path classical deep learning model schmidhuber 2015 also discus recent deep learning work build classical linear model another article reader could read complementary anderson rosenfeld 2000 author conducted extensive interview scientiﬁc leader topic neural network history 4 origin deep learning aristotle modern artiﬁcial neural network study deep learning artiﬁcial neural network originates ambition build computer system simulating human brain build system requires understanding functionality cognitive system therefore paper trace way back origin attempt understand brain start discussion aristotle associationism around 300 associationism therefore accomplish act reminiscence pas certain series precursive movement arrive movement one quest habitually consequent hence hunt mental train excogitating present some similar contrary coadjacent process reminiscence take place movement case sometimes time sometimes part whole subsequent movement already half remarkable paragraph aristotle seen starting point ism burnham 1888 associationism theory state mind set conceptual element organized association element inspired plato aristotle examined process remembrance recall brought four law association boeree 2000 contiguity thing event spatial temporal proximity tend associated mind frequency number occurrence two event proportional strength association two event similarity thought one event tends trigger thought similar event contrast thought one event tends trigger thought opposite event back aristotle described implementation law mind common sense example feel smell taste apple naturally lead concept apple common sense nowadays surprising see law proposed 2000 year ago still serve fundamental assumption machine learning method example sample near deﬁned distance clustered one group explanatory variable frequently occur response variable draw attention model data usually represented embeddings latent space contemporaneously similar law also proposed zeno citium epicurus st augustine hippo theory associationism wa later strengthened variety philosopher psychologist thomas hobbes stated complex experience association simple experience association sensation also believed association exists mean coherence frequency strength 5 wang raj figure 1 illustration neural grouping bain 1873 factor meanwhile john locke introduced concept association idea still separated concept idea sensation idea reﬂection stated complex idea could derived combination two simple idea david hume later reduced aristotle four law three resemblance similarity contiguity cause eﬀect believed whatever coherence world seemed wa matter three law dugald stewart extended three law several principle among obvious one accidental coincidence sound word thomas reid believed no original quality mind wa required explain spontaneous recurrence thinking rather habit james mill emphasized law frequency key learning similar later stage research david hartley physician wa remarkably regarded one made associationism popular hartley 2013 addition existing law proposed argument memory could conceived smaller scale vibration region brain original sensory experience vibration link represent complex idea therefore act material basis stream consciousness idea potentially inspired hebbian learning rule discussed later paper lay foundation neural network bain neural grouping besides david hartley alexander bain also contributed fundamental idea hebbian learning rule wilkes wade 1997 book bain 1873 related process associative memory distribution activity neural grouping term used denote neural network back proposed constructive mode storage capable assembling wa required contrast alternative traditional mode storage prestored memory illustrate idea bain ﬁrst described computational ﬂexibility allows neural grouping function multiple association stored hypothesis bain managed describe structure highly resembled neural 6 origin deep learning network today individual cell summarizing stimulation selected linked cell within grouping showed figure joint stimulation c trigger x stimulation b c trigger stimulation c trigger original illustration b c stand simulation x outcome cell establishment associative structure neural grouping function memory bain proceeded describe construction structure followed direction associationism stated relevant impression neural grouping must made temporal contiguity period either one occasion repeated occasion bain described computational property neural grouping connection strengthened weakened experience via change intervening therefore induction circuit would selected comparatively strong weak see following section hebb postulate highly resembles bain scription although nowadays usually label postulate hebb rather bain according wilkes wade 1997 omission bain contribution may also due bain lack conﬁdence theory eventually bain wa not convinced doubted practical value neural grouping hebbian learning rule hebbian learning rule named donald hebb since wa introduced work organization behavior hebb 1949 hebb also seen father neural network work didier bigand 2011 1949 hebb stated famous rule cell ﬁre together wire together emphasized activation behavior cell speciﬁcally book stated axon cell near enough excite cell b repeatedly persistently take part ﬁring some growth process metabolic change take place one cell eﬃciency one cell ﬁring b archaic paragraph modern machine learning language following ηxiy 1 stand change synaptic weight wi neuron input signal xi denotes postsynaptic response η denotes learning rate word hebbian learning rule state connection two unit strengthened frequency two unit increase although hebbian learning rule seen laying foundation neural network seen today drawback obvious appear weight tions keep increasing weight dominant signal increase exponentially known unstableness hebbian learning rule principe et 1999 fortunately problem not inﬂuence hebb identity father neural network 7 wang raj oja rule principal component analyzer erkki oja extended hebbian learning rule avoid unstableness property also showed neuron following updating rule approximating behavior principal component analyzer pca oja 1982 long story short oja introduced normalization term rescue hebbian learning rule showed learning rule simply online update principal component analyzer present detail argument following paragraph starting equation 1 following notation oja showed wt ηxiy denotes iteration straightforward way avoid exploding weight apply normalization end iteration yielding wt ηxiy pn wt ηxiy 2 1 2 n denotes number neuron equation expanded following form wt z η yxi z wi pn j yxjwj z pn 1 2 two assumption introduced 1 η small therefore approximately 0 2 weight normalized therefore z pn 1 2 two assumption introduced back previous equation oja rule wa proposed following wt ηy xi 2 oja took step show neuron wa updated rule wa eﬀectively performing principal component analysis data show oja ﬁrst equation 2 following form two additional assumption oja 1982 wt cwt wt cwt wt c covariance matrix input proceeded show property many conclusion another work oja karhunen 1985 linked back pca fact component pca eigenvectors ﬁrst component eigenvector corresponding largest eigenvalue covariance matrix intuitively could interpret property simpler explanation eigenvectors c solution maximize rule updating function since wt eigenvectors covariance matrix x get wt pca oja learning rule concludes story learning rule neural network proceed visit idea neural model 8 origin deep learning mcp neural model donald hebb seen father neural network ﬁrst model neuron could trace back six year ahead publication hebbian learning rule neurophysiologist warren mcculloch mathematician walter pitt speculated inner working neuron modeled primitive neural network electrical circuit based ﬁndings mcculloch pitt 1943 model known mcp neural model wa linear step function upon weighted linearly interpolated data could described 1 p wixi zj 0 0 otherwise stand output xi stand input signal wi stand corresponding weight zj stand inhibitory input θ stand threshold function designed way activity any inhibitory input completely prevents excitation neuron any time despite resemblance mcp neural model modern perceptron still diﬀerent distinctly many diﬀerent aspect mcp neural model initially built electrical circuit later see study neural network ha borrowed many idea ﬁeld electrical circuit weight mcp neural model wi ﬁxed contrast adjustable weight modern perceptron weight must assigned manual calculation idea inhibitory input quite unconventional even seen today might idea worth study modern deep learning research perceptron success mcp neural model frank rosenblatt substantialized hebbian learning rule introduction perceptrons rosenblatt 1958 theorist like hebb focusing biological system natural environment rosenblatt constructed electronic device named perceptron wa showed ability learn accordance associationism rosenblatt 1958 introduced perceptron context vision system showed figure 2 introduced rule organization perceptron following stimulus impact retina sensory unit respond manner pulse amplitude frequency proportional stimulus intensity impulse transmitted projection area ai projection area optional impulse transmitted association area random connection sum impulse intensity equal greater threshold θ unit unit ﬁres 9 wang raj illustration organization perceptron rosenblatt 1958 b typical perceptron modern machine ing literature figure 2 perceptrons new ﬁgure illustration organization perceptron rosenblatt 1958 b typical perceptron nowadays ai projection area omitted response unit work fashion intermediate unit figure 2 illustrates explanation perceptron left right four unit sensory unit projection unit association unit response unit respectively projection unit receives information sensory unit pass onto association unit unit often omitted description similar model omission projection unit structure resembles structure nowadays perceptron neural network showed figure 2 b sensory unit collect data association unit linearly add data diﬀerent weight apply transform onto thresholded sum pas result response unit one distinction early stage neuron model modern perceptrons introduction activation function use sigmoid function example figure 2 b originates argument linear threshold function softened simulate biological neural network bose et 1996 well consideration feasibility computation replace step function continuous one mitchell et 1997 rosenblatt introduction perceptron widrow et al 1960 introduced model called adaline however diﬀerence rosenblatt perceptron adaline mainly algorithm aspect primary focus paper neural network model skip discussion adaline perceptron linear representation power perceptron fundamentally linear function input signal therefore limited represent linear decision boundary like logical operation like not not xor sophisticated decision boundary required limitation wa highlighted minski papert 1969 attacked limitation perception emphasizing perceptrons not solve function like xor nxor result little research wa done area 10 origin deep learning b c figure 3 linear representation power preceptron show concrete example introduce linear preceptron only two input therefore decision boundary form line space choice threshold magnitude shift line horizontally sign function pick one side line halfspace function represents halfspace showed figure 3 figure 3 b present two node b denote input well node denote situation trigger node denote situation neither trigger figure 3 b figure 3 c show clearly linear perceptron used describe operation two input however figure 3 interested xor operation operation no longer described single linear decision boundary next section show representation ability greatly enlarged put perceptrons together make neural network however keep stacking one neural network upon make deep learning model representation power not necessarily increase 11 wang raj modern neural network era deep learning section introduce some important property neural network property partially explain popularity neural network gain day also vate necessity exploring deeper architecture speciﬁc discus set universal approximation property property ha condition show although shallow neural network universal approximator deeper ture signiﬁcantly reduce requirement resource retaining representation power last also show some interesting property discovered backpropagation may inspire some related research today universal approximation property step perceptrons basic neural network only placing perceptrons together placing perceptrons side side get single neural network stacking one neural network upon get neural network often known perceptrons mlp kawaguchi 2000 one remarkable property neural network widely known universal approximation property roughly describes mlp represent any function discussed property three diﬀerent aspect boolean approximation mlp one hidden represent any boolean function exactly continuous approximation mlp one hidden layer approximate any bounded continuous function arbitrary accuracy arbitrary approximation mlp two hidden layer approximate any function arbitrary accuracy discus three property detail following paragraph suit diﬀerent reader interest ﬁrst oﬀer intuitive explanation property oﬀer proof representation any boolean function approximation property straightforward previous section shown every linear preceptron perform either according de morgan law every propositional formula converted equivalent conjunctive normal form multiple function therefore simply rewrite target boolean function multiple operation design network way input layer performs operation hidden layer simply operation formal proof not diﬀerent intuitive explanation skip simplicity paper follow widely accepted naming convention call neural network one hidden layer neural network 12 origin deep learning b c figure 4 example universal approximation any bounded continuous function approximation any bounded continuous function continuing linear representation power perceptron discussed previously want represent complex function showed figure 4 use set linear perceptrons describing halfspace one perceptrons shown figure 4 b need ﬁve perceptrons perceptrons bound target function showed figure 4 c number showed figure 4 c represent number subspace described perceptrons fall corresponding region see appropriate selection threshold θ 5 figure 4 c bound target function therefore describe any bounded continuous function only one hidden layer even shape complicated figure 4 property wa ﬁrst shown cybenko 1989 hornik et 1989 speciﬁc cybenko 1989 showed function following form f x x ωiσ wt x θ 3 f x dense subspace word arbitrary function g x subspace f x x x ϵ ϵ equation 3 σ denotes activation function squashing function back wi denotes weight input layer ωi denotes weight hidden layer conclusion wa drawn proof contradiction theorem riesz representation theorem fact closure f x not subspace f x contradicts assumption σ activation squashing function till today property ha drawn thousand citation unfortunately many later work cite property inappropriately castro et 2000 equation 3 not widely accepted form neural network doe not deliver output linear output instead ten year later property wa shown castro et al 2000 concluded story showing ﬁnal output squashed universal approximation property still hold note property wa shown context activation function ing function deﬁnition squashing function σ r 0 1 function 13 wang raj figure 5 threshold not necessary large number linear perceptrons property x 1 x many activation function recent deep learning research not fall category approximation arbitrary function move explain property need ﬁrst show major property regarding combining linear perceptrons neural network figure 5 show number linear perceptrons increase bound target function area outside polygon sum close threshold shrink following trend use large number perceptrons bound circle achieved even without knowing threshold area close threshold shrink nothing left outside circle fact area sum n 2 n number perceptrons used therefore neural network one hidden layer represent circle arbitrary diameter introduce another hidden layer used combine output many diﬀerent circle newly added hidden layer only used perform operation figure 6 show example extra hidden layer used merge circle previous layer neural network used approximate any function target function not necessarily continuous however circle requires large number neuron consequently entire function requires even property wa showed lapedes farber 1988 cybenko 1988 tively looking back property today not arduous build connection property fourier series approximation informal word state every function curve decomposed sum many simpler curve linkage show universal approximation property show any layer neural network represent one simple surface second hidden layer sum simple surface approximate arbitrary function know one hidden layer neural network simply performs thresholded sum eration therefore only step left show ﬁrst hidden layer represent simple surface understand simple surface linkage fourier transform one imagine one cycle sinusoid case bump plane case 14 origin deep learning figure 6 neural network used approximate leaf shaped function one dimension create simple surface only need two sigmoid function appropriately placed example following x h 1 x h 1 x x create simple surface height could easily generalized case need sigmoid function neuron simple surface simple surface contributes ﬁnal function one neuron added onto second hidden layer therefore despite number neuron need one never need third hidden layer approximate any function similarly gibbs phenomenon aﬀects fourier series approximation mation not guarantee exact representation universal approximation property showed great potential shallow neural work price exponentially many neuron layer one question reduce number required neuron maintaining representation power question motivates people proceed deeper neural network despite shallow neural network already inﬁnite modeling power another issue worth tion although neural network approximate any function not trivial ﬁnd set parameter explain data next two section discus two question respectively 15 wang raj necessity depth universal approximation property shallow neural network come price nentially many neuron therefore not realistic question maintain expressive power network reducing number computation unit ha asked year intuitively bengio delalleau 2011 suggested nature pursue deeper network 1 human neural system deep architecture see example section 5 human visual cortex 2 human tend resent concept one level abstraction composition concept lower level nowadays solution build deeper architecture come conclusion state representation power k layer neural network polynomial many neuron need expressed exponentially many neuron k layer structured used however theoretically conclusion still completed conclusion could trace back three decade ago yao 1985 showed limitation shallow circuit function hastad 1986 later showed property parity circuit function computable polynomial size depth k requires exponential size depth restricted k showed property mainly application demorgan law state any rewritten ands vice versa therefore simpliﬁed circuit ands appear one rewriting one layer ands therefore merge operation neighboring layer repeating procedure wa able represent function fewer layer computation moving circuit neural network delalleau bengio 2011 compared deep shallow neural network showed function could expressed n neuron network depth k required least 2 n 1 k neuron neural network bianchini scarselli 2014 extended study general neural work many major activation function including tanh sigmoid derived conclusion concept betti number used number describe representation power neural network showed shallow network resentation power only grow polynomially respect number neuron deep architecture representation grow exponentially respect number neuron also related conclusion neural network tanh bartlett maass 2003 p number parameter recently eldan shamir 2015 presented thorough proof show depth neural network exponentially valuable width neural network standard mlp any popular activation function conclusion drawn only weak assumption constrain activation function mildly increasing surable able allow shallow neural network approximate any univariate lipschitz function finally theory support fact deeper network preferred shallow one however reality many problem arise keep increasing layer among increased diﬃculty learning proper parameter probably prominent one immediately next section discus main drive searching parameter neural network backpropagation 16 origin deep learning backpropagation property proceed need clarify name backpropagation originally not referring algorithm used learn parameter neural network instead stand technique help eﬃciently compute gradient parameter gradient descent algorithm applied learn parameter 1989 however nowadays widely recognized term refer gradient descent algorithm technique compared standard gradient descent update parameter spect error backpropagation ﬁrst propagates error term output layer back layer parameter need updated us standard gradient descent update parameter respect propagated error intuitively derivation backpropagation organizing term gradient expressed chain rule derivation neat skipped paper due extensive resource available werbos 1990 mitchell et 1997 lecun et 2015 instead discus two interesting seemingly contradictory property backpropagation backpropagation find global optimal linear separable data gori tesi 1992 studied problem local minimum backpropagation estingly society belief neural network deep learning approach believed suﬀer local optimal proposed architecture global optimal guaranteed only weak assumption network needed reach global optimal including pyramidal architecture upper layer fewer neuron weight matrix full row rank number input neuron not smaller data however approach may not relevant anymore require data linearly separable condition many model applied backpropagation fails linear separable data hand brady et al 1989 studied situation backpropagation fails linearly separable data set showed could situation data linearly separable neural network learned backpropagation not ﬁnd boundary also showed example situation occurs illustrative example only hold misclassiﬁed data sample signiﬁcantly le correctly classiﬁed data sample word misclassiﬁed data sample might outlier therefore interesting property viewed today arguably desirable property backpropagation typically expect machine learning model neglect outlier therefore ﬁnding ha not attracted many attention however no matter whether data outlier not neural network able overﬁt training data given suﬃcient training iteration legitimate learning algorithm especially considering brady et al 1989 showed inferior algorithm 17 wang raj wa able overﬁt data therefore phenomenon played critical role research improving optimization technique recently studying cost surface neural network indicated existence saddle point choromanska et 2015 dauphin et 2014 pascanu et 2014 may explain ﬁndings brady et al back late backpropagation enables optimization deep neural network however still long way go optimize well later section 7 brieﬂy discus technique related optimization neural network 18 origin deep learning network memory deep belief net figure 7 trade oﬀof representation power computation complexity several model guide development better model background modern neural network set proceed visit prominent branch current deep learning family ﬁrst stop branch lead popular restricted boltzmann machine deep belief net start model understand data unsupervisedly figure 7 summarizes model covered section horizontal axis stand computation complexity model vertical axis stand representation power six milestone focused section placed ﬁgure self organizing map discussion start self organizing map som invented kohonen 1990 som powerful technique primarily used reducing dimension data usually one two dimension germano 1999 reducing dimensionality som also retains topological similarity data point also seen tool clustering imposing topology clustered representation figure 8 illustration self organizing map two dimension hidden neuron therefore learns two dimension representation data upper shaded node denote unit som used 19 wang raj figure 8 illustration map represent data lower circle denote data no connection node som position node ﬁxed representation not viewed only numerical value instead position also matter property diﬀerent some representation criterion example compare case vector som used denote color denote green set c green red purple representation use any vector 1 0 0 0 1 0 0 0 1 long specify bit green correspondingly however som only two vector possible 1 0 0 0 0 1 since som aim represent data retaining similarity red purple much similar green red green purple green not represented way split red purple one notice example only used demonstrate position unit som matter practice value som unit not restricted integer learned som usually good tool visualizing data example conduct survey happiness level richness level country feed data som trained unit represent happiest richest country one corner represent opposite country furthest corner rest two corner represent richest yet unhappiest poorest happiest country rest country positioned accordingly advantage som allows one some literature bullinaria 2004 example one may notice connection illustration model however connection only used represent neighborhood relationship node no information ﬂowing via connection paper show many model rely clear illustration information ﬂow decide save connection denote 20 origin deep learning easily tell country ranked among world simple glance learned unit guthikonda 2005 learning algorithm understanding representation power som proceed eter learning algorithm classic algorithm heuristic intuitive shown use som example j index unit w weight initialize weight unit wi j j pick vk randomly select best matching unit bmu p q arg mini j 2 select node interest neighbor bmu wi wi j wp q r update weight wi j wi j p j p q l 2 j end unit v denotes data vector k index data denotes current iteration n constrains maximum number step allowed p denotes penalty considering distance unit p q unit j l learning rate r denotes radius used select neighbor node l r typically decrease increase 2 denotes euclidean distance dist denotes distance position unit algorithm explains som used learn representation similarity retained always selects subset unit similar data sampled adjust weight unit match data sampled however algorithm relies careful selection radius neighbor selection good initialization weight otherwise although learned weight local property topological similarity loses property globally sometimes two similar cluster similar event separated another dissimilar cluster similar event simpler word unit green may actually separate unit red unit purple network not appropriately trained germano 1999 hopﬁeld network hopﬁeld network historically described form neural network ﬁrst introduced hopﬁeld 1982 recurrent context refers fact weight connecting neuron bidirectional hopﬁeld network widely recognized memory property memory property simulation spin glass theory therefore start discussion spin glass term recurrent confusing nowadays popularity recurrent neural network rnn gain 21 wang raj figure 9 illustration hopﬁeld network fully connected network six binary thresholding neural unit every unit connected data therefore unit denoted unshaded node spin glass spin glass physic term used describe magnetic phenomenon many work done detailed study related theory edward anderson 1975 ezard et 1990 paper only describe intuitively group dipole placed together any space dipole forced align ﬁeld generated dipole location however aligning change ﬁeld location leading dipole ﬂip causing ﬁeld original location change eventually change converge stable state describe stable state ﬁrst deﬁne total ﬁeld location j sj oj ct x k sk jk oj external ﬁeld ct constant depends temperature sk polarity kth dipole djk distance location j location therefore total potential energy system pe x j sjoj ctsj x k sk jk 4 magnetic system evolve potential energy minimum hopfield network hopﬁeld network fully connected neural network binary thresholding neural unit value unit either 0 unit fully connected bidirectional weight some literature may use 1 denote value unit choice value doe not aﬀect idea hopﬁled network change formulation energy function paper only discus context 0 1 value 22 origin deep learning setting energy hopﬁeld network deﬁned e x sibi x j sisjwi j 5 state unit b denotes bias w denotes bidirectional weight j index unit energy function closely connects potential energy function spin glass showed equation hopﬁeld network typically applied memorize state data weight network designed learned make sure energy minimized given state interest therefore another state presented network weight ﬁxed hopﬁeld network search state minimize energy recover state memory example face completion task some image face presented hopﬁeld network way unit network corresponds pixel one image image presented one network calculate weight minimize energy given face later one image corrupted distorted presented network network able recover original image searching conﬁguration state minimize energy starting corrupted input presented term energy may remind people physic explain hopﬁeld network work physic scenario clearer nature us hopﬁeld network memorize equilibrium position pendulum equilibrium position pendulum ha lowest gravitational potential energy therefore whenever pendulum placed converge back equilibrium position learning inference learning weight hopﬁeld network straightforward gurney 1997 weight calculated wi j x j notation equation learning procedure simple still worth mentioning essential step hopﬁeld network applied solve practical problem however ﬁnd many online tutorial omit step make worse refer inference state remove confusion paper similar term used standard machine learning society refer calculation weight model either solution numerical solution parameter learning training refer process applying existing model weight known onto solving problem inference 5 testing decode hidden state data predict label inference hopﬁeld network also intuitive state data network test inverting state one unit whether energy decrease 5 inference conventionally used way machine learning society although some statistician may disagree usage 23 wang raj network invert state proceed test next unit procedure called asynchronous update procedure obviously subject sequential order selection unit counterpart known synchronous update network ﬁrst test unit inverts simultaneously method may lead local optimal synchronous update may even result increasing energy may converge oscillation loop state capacity one distinct disadvantage hopﬁeld network not keep memory eﬃcient network n unit only store memory bit network n unit ha edge addition storing memory instance data connection ha integer value range thus number bit required store n unit 1 hopﬁeld 1982 therefore safely draw conclusion although hopﬁeld network remarkable idea enables network memorize data extremely ineﬃcient practice invention hopﬁeld network many work attempted study increase capacity original hopﬁeld network storkey 1997 liou yuan 1999 liou lin 2006 despite attempt made hopﬁeld network still gradually fade society replaced model inspired immediately following section discus popular boltzmann machine restricted mann machine study model upgraded initial idea hopﬁeld network evolve replace boltzmann machine boltzmann machine invented ackley et al 1985 stochastic version hopﬁeld network got name boltzmann distribution boltzmann distribution boltzmann distribution named ludwig boltzmann investigated extensively willard 1902 originally used describe probability distribution particle system various possible state following f kt stand state e corresponding energy k boltzmann constant thermodynamic temperature respectively naturally ratio two bution only characterized diﬀerence energy following r f f e kt known boltzmann factor 24 origin deep learning figure 10 illustration boltzmann machine introduction hidden unit shaded node model conceptually split two part visible unit hidden unit red dashed line used highlight conceptual separation distribution speciﬁed energy probability deﬁned term state divided normalizer following psi esi kt p j esj kt boltzmann machine mentioned previously boltzmann machine stochastic version hopﬁeld network figure 10 introduces idea hidden unit introduced turn hopﬁeld network boltzmann machine boltzmann machine only visible unit connected data hidden unit used assist visible unit describe distribution data therefore model conceptually split visible part hidden part still maintains fully connected network among unit stochastic introduced boltzmann machine improved hopﬁeld work regarding leaping local optimum oscillation state inspired physic method transfer state regardless current energy introduced set state state 1 mean state regardless current state following probability p 1 1 stand diﬀerence energy state oﬀ stand temperature idea inspired physic process higher temperature likely state addition probability higher energy state transferring lower energy state always greater reverse idea highly related popular optimization molecule move faster kinetic energy provided could achieved heating corresponds zeroth law thermodynamics 25 wang raj algorithm called simulated annealing khachaturyan et 1979 aarts korst 1988 back simulated annealing hardly relevant nowadays deep learning society regardless historical importance term introduces within section assume 1 constant sake simpliﬁcation energy boltzmann machine energy function boltzmann machine deﬁned equation 5 deﬁned hopﬁeld network except visible unit hidden unit noted separately following e v h x vibi x k hkbk x j vivjwij x k vihkwik x k l hkhlwk l v stand visible unit h stand hidden unit equation also connects back equation 4 except boltzmann machine split energy function according hidden unit visible unit based energy function probability joint conﬁguration visible unit hidden unit deﬁned following p v h v h p n n probability unit achieved marginalizing joint bility example marginalizing hidden unit get probability distribution visible unit p v p h v h p n n could used sample visible unit generating data boltzmann machine trained stable state called thermal librium distribution probability p v h remain constant distribution energy constant however probability visible unit hidden unit may vary energy may not minimum related thermal equilibrium deﬁned only constant factor distribution part system thermal equilibrium hard concept understand one imagine pouring cup hot water bottle pouring cup cold water onto hot water start bottle feel hot bottom feel cold top gradually bottle feel mild cold water hot water mix heat transferred however temperature bottle becomes mild stably corresponding distribution p v h doe not necessarily mean molecule cease move corresponding p v h parameter learning common way train boltzmann machine determine parameter maximize likelihood observed data gradient descent log likelihood 26 origin deep learning function usually performed determine parameter simplicity following derivation based single observation first log likelihood function visible unit l v w log p v w log x h h x n n second term rh normalizer take derivative log likelihood function w simplify v w x h p v h x n p n n v h ep n n e denotes expectation thus gradient likelihood function composed two part ﬁrst part expected gradient energy function respect conditional distribution p second part expected gradient energy function respect joint distribution variable state however calculating expectation generally infeasible any model involves summing huge number possible general approach solving problem use markov chain monte carlo mcmc approximate sum v w si sj p si sj p 6 denotes expectation equation 6 diﬀerence expectation value product state data fed visible state expectation product state no data fed ﬁrst term calculated taking average value energy function gradient visible hidden unit driven observed data sample practice ﬁrst term generally straightforward calculate calculating second term generally complicated involves running set markov chain reach current model equilibrium distribution taking average energy function gradient based sample however sampling procedure could computationally complicated motivates topic next section restricted boltzmann machine restricted boltzmann machine restricted boltzmann machine rbm originally known harmonium invented smolensky 1986 version boltzmann machine restriction no connection either visible unit hidden unit figure 11 illustration restricted boltzmann machine achieved based boltzmann machine figure 10 connection hidden unit well connection visible unit removed model becomes bipartite graph restriction introduced energy function rbm much simpler e v h x vibi x k hkbk x k vihkwik 7 27 wang raj figure 11 illustration restricted boltzmann machine restriction no connection hidden unit shaded node no connection visible unit unshaded node boltzmann machine turn restricted boltzmann machine model bipartite graph contrastive divergence rbm still trained way boltzmann machine trained since energy function rbm much simpler sampling method used infer second term equation 6 becomes easier despite relative simplicity learning procedure still requires large amount sampling step approximate model distribution emphasize diﬃculties sampling mechanism well simplify introduction equation 6 diﬀerent set notation following v w si sj si sj 8 use denote data distribution denote model distribution notation remain unchanged therefore diﬃculty mentioned method learn parameter requires potentially inﬁnitely many sampling step approximate model distribution hinton 2002 overcame issue magically introduction method named contrastive divergence empirically found one doe not perform inﬁnitely many sampling step converge model distribution ﬁnite k step sampling enough therefore equation 8 eﬀectively v w si sj si sj pk remarkably hinton 2002 showed k 1 suﬃcient learning algorithm work well practice hinton 2005 attempted justify contrastive divergence theory derivation led negative conclusion contrastive divergence 28 origin deep learning figure 12 illustration deep belief network deep belief network not stacking rbm together bottom layer layer except top one not connection only connection top biased algorithm ﬁnite k not represent model distribution however empirical result suggested ﬁnite k approximate model distribution well enough resulting small enough bias addition algorithm work well practice strengthened idea contrastive divergence reasonable modeling power fast approximation algorithm rbm quickly draw great attention becomes one fundamental building block deep neural network following two section introduce two distinguished deep neural network built based machine namely deep belief net deep boltzmann machine deep belief net deep belief network introduced hinton et al 2006 8 showed rbms stacked trained greedy manner figure 12 show structure deep belief network diﬀerent stacking rbm dbn only allows connection connection top one layer following bottom layer only connection probably better way understand dbn think generative model despite fact dbn generally described stacked rbm quite diﬀerent putting one rbm top probably appropriate think dbn rbm extended layer specially devoted generating pattern data therefore model only need sample thermal equilibrium topmost layer pas visible state top generate data paper generally seen opening nowadays deep learning era ﬁrst introduces possibility training deep neural network layerwise training 29 wang raj parameter learning parameter learning deep belief network fall two step ﬁrst step second step layerwise success deep belief network largely due troduction pretraining idea simple reason work still attracts researcher simply ﬁrst train network component component bottom treating ﬁrst two layer rbm train treat second layer third layer another rbm train parameter idea turn oﬀer critical support success later tuning process several explanation attempted explain mechanism intuitively clever way initialization put parameter value appropriate range bengio et al 2007 suggested unsupervised initializes model point parameter space lead eﬀective optimization process optimization ﬁnd lower minimum empirical cost function erhan et al 2010 empirically argued regularization explanation pervised pretraining guide learning towards basin attraction minimum support better generalization training data set addition deep belief network pretraining mechanism also inspires training many classical model including autoencoders poultney et 2006 bengio et 2007 deep boltzmann machine salakhutdinov hinton 2009 some model inspired classical model like yu et 2010 performed carried optimize work search parameter lead lower minimum deep belief network two diﬀerent ﬁne tuning strategy dependent goal network fine tuning generative model generative model achieved contrastive version algorithm hinton et 1995 algorithm intriguing reason designed interpret brain work scientist found sleeping critical process brain function seems inverse version learn awake algorithm also ha two step wake phase propagate information bottom adjust weight reconstructing layer sleep phase inverse wake phase propagate information top adjust weight reconstructing layer contrastive version algorithm add one contrastive divergence phase wake phase sleep phase wake phase only go visible layer top rbm sample top rbm contrastive divergence sleep phase start visible layer top rbm fine tuning discriminative model strategy ﬁne tuning dbn discriminative model simply apply standard backpropagation model 30 origin deep learning figure 13 illustration deep boltzmann machine deep boltzmann machine like stacking rbm together connection every two layer bidirectional since label data however still necessary spite generally good performance backpropagation deep boltzmann machine last milestone introduce family deep generative model deep boltzmann machine introduced salakhutdinov hinton 2009 figure 13 show three layer deep boltzmann machine dbm distinction tween dbm dbn mentioned previous section dbm allows bidirectional connection bottom layer therefore dbm represents idea stacking rbms much better way dbn although might clearer dbm named deep restricted boltzmann machine due nature dbm energy function deﬁned extension energy function rbm equation 7 showed following e v h x vibi n x x k hn kbn k x k viwikhk x x k l hn kwn k l dbm n hidden layer similarity energy function grant possibility training dbm constrative divergence however typically necessary deep boltzmann machine dbm deep belief network dbn acronym suggest deep boltzmann machine deep belief network many similarity especially ﬁrst glance deep neural network nates idea restricted boltzmann machine name deep belief network 31 wang raj seems indicate also partially originates bayesian network krieg 2001 also rely layerwise success parameter learning however fundamental diﬀerences two model dramatic duced connection made bottom layer directed bidirectional structure dbm grant possibility dbm learn complex pattern data also grant possibility approximate inference procedure incorporate feedback addition initial pas ing deep boltzmann machine better propagate uncertainty ambiguous input deep generative model future deep boltzmann machine last milestone discus history generative model still much work dbm even done future lake et al 2015 introduces bayesian program learning framework simulate human learning ability large scale visual concept addition performance learning classiﬁcation task model pass visual turing test term generating handwritten character world alphabet word generative performance model indistinguishable human behavior not deep neural model model outperforms several concurrent deep neural network deep neural counterpart bayesian program learning framework surely expected even better performance conditional image generation given part image also another interesting topic recently problem usually solved pixel network pixel cnn van den oord et 2016 pixel rnn oord et 2016 however given part image seems simplify generation task another contribution generative model generative adversarial network fellow et 2014 however gan still young discussed paper 32 origin deep learning convolutional neural network vision problem section start discus diﬀerent family model convolutional ral network cnn family distinct family previous section convolutional neural network family mainly evolves knowledge human visual cortex fore section ﬁrst introduce one important reason account success convolutional neural network vision problem bionic design replicate human vision system nowadays convolutional neural network probably inate design rather ancestor background brieﬂy introduce successful model make self famous imagenet challenge deng et 2009 last present some known problem vision task may guide future research direction vision task visual cortex convolutional neural network widely known inspired visual cortex however except some publication discus inspiration brieﬂy poggio serre 2013 cox dean 2014 resource present inspiration thoroughly section focus discussion basic visual cortex hubel wiesel 1959 lay ground study convolutional neural network visual cortex brain located occipital lobe located back skull part cerebral cortex play important role processing visual information visual information coming eye go series brain structure reach visual cortex part visual cortex receive sensory input known primary visual cortex also known area visual information managed extrastriate area including visual area two four also visual area paper primarily focus visual area related object recognition known ventral stream consists area inferior temporal gyrus one higher level ventral stream visual processing associated representation complex object feature global shape like face perception haxby et 2000 figure 14 illustration ventral stream visual cortex show information process procedure retina receives image information pass way inferior temporal gyrus component retina convert light energy come ray bouncing oﬀof object chemical energy chemical energy converted action potential transferred onto primary visual cortex fact several brain structure involved retina omit structure deliberately discus component connection established technology lutional neural network one interested developing powerful model encouraged investigate component 33 wang raj figure 14 brief illustration ventral stream visual cortex human vision system consists primary visual cortex visual area inferior temporal gyrus primary visual cortex mainly fulﬁlls task edge detection edge area strongest local contrast visual signal also known secondary visual cortex ﬁrst region within visual sociation area receives strong feedforward connection sends strong connection later area cell tuned extract mainly simple property visual signal orientation spatial frequency colour complex property fulﬁlls function including detecting object feature intermediate complexity like simple geometric shape addition orientation spatial frequency color also shown strong attentional modulation moran desimone 1985 also receives direct input inferior temporal gyrus ti responsible identifying object based color form object comparing processed information stored memory object identify object kolb et 2014 word performs semantic level task like face recognition many description function visual cortex revive recollection convolutional neural network reader exposed some relevant technical literature later section discus detail convolutional neural network help build explicit connection even reader barely 34 origin deep learning knowledge convolutional neural network hierarchical structure visual cortex immediately ring bell neural network besides convolutional neural network visual cortex ha inspiring work computer vision long time example li 1998 built neural model inspired primary visual cortex another granularity serre et al 2005 introduced system feature detection inspired visual cortex de ladurantaye et al 2012 published book describing model information processing visual cortex poggio serre 2013 conducted comprehensive survey relevant topic focus any particular subject detail survey section discus connection visual cortex convolutional neural network detail begin neocogitron borrows some idea visual cortex later inspires convolutional neural network neocogitron visual cortex neocogitron proposed fukushima 1980 generally seen model inspires convolutional neural network computation side neural network sists two diﬀerent kind layer feature extractor structured connection organize extracted feature consists number inspired cell primary visual cortex serf feature extractor ideally trained responsive particular feature presented receptive ﬁeld generally local feature edge particular orientation extracted lower layer global feature extracted higher layer structure highly resembles human conceive object resembles complex cell higher pathway visual cortex mainly introduced shift invariant property feature extracted parameter learning parameter learning process only parameter updated itron also trained unsupervisedly good feature extractor training process similar hebbian learning rule strengthens connection whichever show strongest response training mechanism also introduces problem hebbian learning rule introduces strength connection saturate since keep increasing solution wa also introduced fukushima 1980 wa introduced name inhibitory cell performed function normalization avoid problem convolutional neural network visual cortex proceed neocogitron convolutional neural network first troduce building component convolutional layer subsampling layer assemble component present convolutional neural network using lenet example 35 wang raj figure 15 simple illustration two dimension convolution operation convolution operation convolution operation strictly mathematical operation treated equally operation like addition multiplication not discussed particularly machine learning literature however still discus ness reader may not familiar convolution mathematical operation two function f g produce third function h integral express amount overlap one function f shifted function g described formally following h z f τ g dτ denoted h f convolutional neural network typically work convolution tion could summarized figure showed figure 15 leftmost matrix input matrix middle one usually called kernel matrix convolution applied matrix result showed rightmost matrix convolution process product followed sum showed example left upper matrix convoluted kernel result slide target 3 3 matrix one column right convoluted kernel get result keep sliding record result matrix kernel 3 3 every target matrix 3 3 thus every 3 3 matrix convoluted one digit whole 5 5 matrix shrunk 3 3 matrix 5 3 ﬁrst 3 mean size kernel matrix one realize convolution locally shift invariant mean many diﬀerent combination nine number upper 3 3 matrix placed convoluted result invariant property play critical role vision problem ideal case recognition result not changed due shift rotation feature critical property used solved elegantly lowe 1999 bay et al 2006 convolutional neural network brought performance new level connection cnn visual cortex idea two dimension convolution discus convolution useful operation simulate task performed visual cortex 36 origin deep learning identity kernel b edge detection kernel c blur kernel sharpen kernel e lighten kernel f darken kernel g random kernel 1 h random kernel 2 figure 16 convolutional kernel example diﬀerent kernel applied image result diﬀerently processed image note 1 9 divisor applied kernel convolution operation usually known kernel diﬀerent choice kernel diﬀerent operation image could achieved operation typically including identity edge detection blur sharpening etc introducing random matrix tion operator some interesting property might discovered figure 16 illustration some example kernel applied ﬁgure one see diﬀerent kernel applied fulﬁll diﬀerent task random kernel also applied transform image some interesting outcome figure 16 b show edge detection one central task primary visual cortex fulﬁlled clever choice kernel furthermore clever selection kernel lead u success replication visual cortex result learning meaningful convolutional kernel parameter learning one central task convolutional neural network applied vision task also explains 37 wang raj figure 17 illustration lenet conv stand convolutional layer pling stand subsampling layer many popular model usually perform well task only limited process kernel well trained universally applicable understanding essential role convolution operation play vision task proceed investigate some major milestone along way pioneer convolutional neural network lenet section devoted model widely recognized ﬁrst convolutional neural network lenet invented le cun et al 1990 made popular lecun et inspired neocogitron section introduce convolutional neural network via introducing lenet figure 17 show illustration architecture lenet consists two pair convolutional layer subsampling layer connected fully connected layer rbf layer classiﬁcation convolutional layer convolutional layer primarily layer performs convolution operation discussed previously clever selection convolution kernel eﬀectively simulate task visual cortex convolutional layer introduces another operation convolution assist simulation successful transform considering relu nair hinton 2010 transform deﬁned following f x max 0 x transform remove negative part input resulting clearer contrast meaningful feature opposed side product kernel produce therefore grant convolution power extracting useful feature allows simulate function visual cortex closely 38 origin deep learning subsampling layer subsampling layer performs simpler task only sample one input every region look some diﬀerent strategy sampling considered like taking maximum value input taking averaged value input even probabilistic pooling taking random one lee et 2009 sampling turn input representation smaller manageable embeddings importantly sampling make network invariant small transformation tions translation input image small distortion input not change outcome pooling since take value local neighborhood lenet two important component introduced stack together semble convolutional neural network following recipe figure 17 end famous lenet lenet known ability classify digit handle variety diﬀerent problem digit including variance position scale rotation squeezing digit even diﬀerent stroke width digit meanwhile introduction lenet lecun et al also introduces mnist database later becomes standard benchmark digit recognition ﬁeld milestone imagenet challenge success lenet convolutional neural network ha shown great tential solving vision task potential attracted large number researcher aiming solve vision task regarding object recognition cifar classiﬁcation krizhevsky hinton 2009 imagenet challenge russakovsky et 2015 along path several superstar milestone attracted great attention ha applied ﬁelds good performance section brieﬂy discus model alexnet lenet one start era convolutional neural network alexnet invented krizhevsky et al 2012 one start era cnn used imagenet classiﬁcation alexnet ﬁrst evidence cnn perform well historically diﬃcult imagenet dataset performs well lead society competition developing cnns success alexnet not only due unique design architecture also due clever mechanism training avoid computationally expensive training process alexnet ha split two stream trained two gpus also used data augmentation technique consist image translation horizontal reﬂections patch extraction recipe alexnet shown figure however rarely any lesson learned architecture alexnet despite remarkable performance even unfortunately fact particular architecture alexnet doe not grounded theoretical support push many researcher blindly burn computing resource 39 wang raj figure 18 illustration alexnet test new architecture many model introduced period only may worth mentioning future vgg blind competition exploring diﬀerent architecture simonyan zisserman 2014 showed simplicity promising direction model named vgg although vgg deeper 19 layer model around time architecture extremely simpliﬁed layer 3 3 convolutional layer 2 2 pooling layer simple usage convolutional layer simulates larger ﬁlter keeping beneﬁts smaller ﬁlter size combination two convolutional layer ha eﬀective receptive ﬁeld 5 5 convolutional layer fewer parameter spatial size input volume layer decrease result convolutional pooling layer depth volume increase increased number ﬁlters vgg number ﬁlters double pooling layer behavior reinforces idea vgg shrink spatial dimension grow depth vgg not winner imagenet competition year winner googlenet invented szegedy et al 2015 googlenet introduced several important concept like inception module concept later used girshick et 2014 girshick 2015 ren et 2015 design architecture barely contribute vgg doe society especially considering residual net following path vgg imagenet challenge unprecedented level residual net residual net resnet 152 layer network wa ten time deeper wa usually seen time wa invented et al 2015 following path vgg introduces resnet explores deeper structure simple layer however naively 40 origin deep learning figure 19 illustration residual block resnet increasing number layer only result worse result training case testing case et 2015 breakthrough resnet introduces allows resnet substantially deeper previous network called residual block idea behind residual block some input certain layer denoted x passed component two layer later either following traditional path involves convolutional layer relu transform succession denote result f x going express way directly pass x result input component two layer later f x x instead typically seen f x idea residual block illustrated figure complementary work et al 2016 validated residual block essential propagating information smoothly therefore simpliﬁes optimization also extended resnet version success cifar data set another interesting perspective resnet provided veit et 2016 showed resnet behave behaves like ensemble shallow network express way introduced allows resnet perform collection independent network network signiﬁcantly shallower integrated resnet also explains gradient passed architecture without vanished talk vanishing gradient problem discus recurrent neural network next section another work not directly relevant resnet may help understand conducted hariharan et al 2015 showed feature lower layer informative addition summarized ﬁnal layer resnet still not completely vacant clever design number layer whole network number layer residual block allows identity bypass still choice require experimental validation nonetheless some extent resnet ha shown critical reasoning help development cnn better blind 41 wang raj experimental trail addition idea residual block ha found actual visual cortex ventral stream visual cortex directly accept signal primary visual cortex although resnet not designed according ﬁrst place introduction neural model successful challenge canziani et al 2016 conducted comprehensive experimental study comparing model upon comparison showed still room improvement fully connected layer show strong ineﬃciencies smaller batch image challenge chance fundamental vision problem resnet not end story new model technique appear every day push limit cnns example zhang et al took step put residual block inside residual block zagoruyko komodakis 2016 attempted decrease depth network increasing width however incremental work kind not scope paper would like end story convolutional neural network some current challenge fundamental vision problem may not able solved naively investigation machine learning technique network property vision blindness spot convolutional neural network reached unprecedented accuracy object tion however may still far industry reliable application due some intriguing property found szegedy et al 2013 szegedy et al 2013 showed could force deep learning model misclassify image simply adding perturbation image importantly perturbation may not even observed naked human eye word two object look almost human may recognized diﬀerent object neural network example alexnet also shown property likely modeling problem contrast problem raised insuﬃcient training hand nguyen et al 2015 showed could generate pattern convey almost no information human recognized some object neural network high conﬁdence sometimes 99 since neural network cally forced make prediction not surprising see network classify meaningless patter something however high conﬁdence may indicate fundamental diﬀerences neural network human learn know world figure 20 show some example aforementioned two work construction show neural network may misclassify object easily recognized human something unusual hand neural network may also classify some weird pattern not believed object human something familiar property may restrict usage deep learning real world application reliable prediction necessary even without example one may also realize reliable prediction neural network could issue due fundamental property matrix existence null space long perturbation happens within null space matrix one may able alter image dramatically neural network still make 42 origin deep learning b c e f g h figure 20 illustration some mistake neural network szegedy et 2013 adversarial image generated based original image diﬀerences original one naked eye neural network successfully classify original one fail adversarial one e h nguyen et 2015 pattern generated neural network classify e school bus f guitar g peacock h pekinese respectively misclassiﬁcation high conﬁdence null space work like blind spot matrix change within null space never sensible corresponding matrix blind spot not discourage promising future neural network contrary make convolutional neural network resemble human vision system deeper level human vision system blind spot gregory cavanagh 2011 also exist wandell 1995 interesting work might seen linking ﬂaws human vision system defect neural network helping overcome defect future human labeling preference last present some misclassiﬁed image resnet imagenet lenge hopefully some example could inspire some new methodology invented fundamental vision problem figure 21 show some misclassiﬁed image resnet applied imagenet lenge label provided human eﬀort unexpected even many human therefore error rate resnet general human usually predicts error rate 5 probably hitting limit since labeling preference tator harder predict actual label example figure 21 b h labeled tiny part image important content expressed image hand figure 21 e annotated background image image obviously centering object 43 wang raj ﬂute b guinea pig c wig seashore e alp f screwdriver g comic book h sunglass figure 21 some failed image imagenet classiﬁcation resnet primary label associated image improve performance resnet reached one direction might modeling annotator labeling preference one assumption could annotator prefer label image make distinguishable some established work modeling human factor wilson et 2015 could helpful however important question whether worth optimizing model increase testing result imagenet dataset since remaining misclassiﬁcations may not result incompetency model problem annotation introduction data set like coco lin et 2014 flickr plummer et 2015 visualgenome krishna et 2016 may open new era vision problem competitive challenge however fundamental problem experience section introduces never forgotten 44 origin deep learning time series data recurrent network section start discus new family deep learning model attracted many attention especially task time series data sequential data recurrent neural network rnn class neural network whose connection unit form directed cycle nature grant ability work temporal data ha also discussed literature like grossberg 2013 lipton et 2015 paper continue oﬀer complementary view survey emphasis evolutionary history milestone model aim provide insight future direction coming model recurrent neural network jordan network elman network discussed previously hopﬁeld network widely recognized recurrent neural network although formalization distinctly diﬀerent recurrent neural network deﬁned nowadays therefore despite literature tend begin discussion rnn hopﬁeld network not treat member rnn family avoid unnecessary confusion modern deﬁnition recurrent initially introduced jordan 1986 network ha one cycle possible follow path unit back network referred recurrent nonrecurrent network ha no cycle model jordan 1986 later referred jordan network simple neural network one hidden layer input denoted x weight hidden layer denoted wh weight output layer denoted wy weight recurrent computation denoted wr hidden representation denoted h output denoted jordan network formulated ht σ whx σ wyht year later another rnn wa introduced elman 1990 formalized recurrent structure slightly diﬀerently later network known elman network elman network formalized following ht σ whx σ wyht only diﬀerence whether information previous time step provided previous output previous hidden layer diﬀerence illustrated figure diﬀerence illustrated respect historical contribution work one may notice no fundamental diﬀerence two structure since yt wyht therefore only diﬀerence lie choice wr originally elman only introduces network wr general case could derived nevertheless step jordan network elman network still remarkable introduces possibility passing information hidden layer signiﬁcantly improve ﬂexibility structure design later work 45 wang raj structure jordan network b structure elman network figure 22 diﬀerence recurrent structure jordan network elman network backpropagation time recurrent structure make traditional backpropagation infeasible recurrent structure not end point backpropagation stop intuitively one solution unfold recurrent structure expand ward neural network certain time step apply traditional backpropagation onto unfolded neural network solution known backpropagation time bptt independently invented several researcher including robinson fallside 1987 werbos 1988 mozer 1989 however recurrent neural network usually ha complex cost surface naive backpropagation may not work well later paper see recurrent structure introduces some critical problem example vanishing gradient problem make optimization rnn great challenge society bidirectional recurrent neural network unfold rnn get structure feedforward neural network nite depth therefore build conceptual connection rnn feedforward network inﬁnite layer since neural network history bidirectional neural network playing important role like hopﬁeld network rbm dbm question recurrent structure correspond inﬁnite layer bidirectional model answer bidirectional recurrent neural network 46 origin deep learning figure 23 unfolded structured brnn temporal order left right hidden layer 1 unfolded standard way rnn hidden layer 2 unfolded simulate reverse connection bidirectional recurrent neural network brnn wa invented schuster paliwal 1997 goal introduce structure wa unfolded bidirectional neural network therefore applied time series data not only information passed following natural temporal sequence information also reversely provide knowledge previous time step figure 23 show unfolded structure brnn hidden layer 1 unfolded standard way rnn hidden layer 2 unfolded simulate reverse connection transparency figure 23 applied emphasize unfolding rnn only concept used illustration purpose actual model handle data diﬀerent time step single model brnn formulated following ht 1 σ 1 ht 2 σ 2 σ 1 2 subscript 1 2 denote variable associated hidden layer 1 2 respectively introduction recurrent connection back future tion time no longer directly feasible solution treat model combination two rnns standard one reverse one apply bptt onto weight updated simultaneously two gradient computed long memory another breakthrough rnn family wa introduced year brnn hochreiter schmidhuber 1997 introduced new neuron rnn family named long memory lstm wa invented term lstm used refer algorithm 47 wang raj designed overcome vanishing gradient problem help special designed memory cell nowadays lstm widely used denote any recurrent network memory cell nowadays referred lstm cell lstm wa introduced overcome problem rnns not long term cies bengio et 1994 overcome issue requires specially designed memory cell illustrated figure 24 lstm consists several critical component state value used oﬀer information output data denoted x state value previous hidden layer traditional rnn denoted state value linear combination hidden state input current time step denoted σ wixxt 9 state value serve memory denoted gate value used decide information ﬂow state gate decides whether input state enters internal state denoted g gt σ wgiit 10 gate decides whether internal state forgets previous internal state denoted f ft σ wfiit 11 gate decides whether internal state pass value output hidden state next time step denoted ot σ woiit 12 finally considering gate decide information ﬂow state last two equation complete formulation lstm mt 13 ht 14 product figure 24 describes detail lstm cell work figure 24 b show input state constructed described equation figure 24 c show 48 origin deep learning lstm memory cell b input data previous hidden state form input state c calculating input gate forget gate calculating output gate e update internal state f output update hidden state figure 24 lstm cell detailed function 49 wang raj input gate forget gate computed described equation 10 equation figure 24 show output gate computed described equation figure 24 e show internal state updated described equation figure 24 f show output hidden state updated described equation weight parameter need learned training therefore theoretically lstm learn memorize long time dependency necessary learn forget past necessary making powerful model important theoretical guarantee many work attempted improve lstm example gers schmidhuber 2000 added peephole connection allows gate use information internal state cho et al 2014 introduced gated recurrent unit known gru simpliﬁed lstm merging internal state hidden state one state merging forget gate input gate simple update gate integrating lstm cell bidirectional rnn also intuitive look graf et 2013 interestingly despite novel lstm variant proposed greﬀet al 2015 conducted experiment investigating performance lstms got conclusion none variant improve upon standard lstm architecture signiﬁcantly probably improvement lstm another direction rather updating structure inside cell attention model seem direction go attention model attention model loosely based bionic design simulate behavior human vision attention mechanism human look image not scan bit bit stare whole image focus some major part gradually build context capturing gist attention mechanism ﬁrst discussed larochelle hinton 2010 denil et al 2012 attention model mostly refer model introduced bahdanau et 2014 machine translation soon applied many diﬀerent domain like chorowski et 2015 speech recognition xu et 2015 image caption generation attention model mostly used sequence output prediction instead seeing whole sequential data make one single prediction example language model model need make sequential prediction sequential input task like machine translation image caption generation therefore attention model mostly used answer question pay attention based previously predicted label hidden state output sequence may not linked input sequence input data may not even sequence therefore usually framework cho et 2015 necessary encoder used encode data representation decoder used make sequential prediction attention mechanism used locate region representation predicting label current time step figure 25 show basic attention model network structure representation encoder encodes accessible attention model attention model only selects some region pas onto lstm cell usage prediction making 50 origin deep learning figure 25 unfolded structured attention model transparency used show unfolding only conceptual representation encoder learns available decoder across time step attention module only selects some pas onto lstm cell prediction therefore magic attention model attention module figure 25 help localize informative representation formalize work use r denote encoded representation total region representation use h denote hidden state lstm cell attention module generate unscaled weight ith region encoded resentation βt f r j j attention weight computed previous time step puted current time step simple softmax function αt exp βt pm j exp βt j therefore use weight α reweight representation r prediction two way representation reweighted soft attention result simple weighted sum context vector rt x j αt jcj hard attention model forced make hard decision only localizing one region sampling one region following multinoulli distribution 51 wang raj deep input architecture b deep recurrent ture c deep output architecture figure 26 three diﬀerent formulation deep recurrent neural network one problem hard attention sampling multinoulli distribution not diﬀerentiable therefore gradient based method hardly applied variational method ba et 2014 policy gradient based method sutton et 1999 considered deep rnns future rnns last section evolutionary path rnn family visit some idea not fully explored deep recurrent neural network although recurrent neural network suﬀers many issue deep neural network ha recurrent connection current rnns still not deep model regarding tion learning compared model family pascanu et al formalizes idea constructing deep rnns extending current rnns figure 26 show three diﬀerent direction construct deep recurrent neural network increasing layer input component figure 26 recurrent component figure 26 b output component figure 26 c respectively 52 origin deep learning future rnns rnns improved variety diﬀerent way like assembling piece together conditional random field yang et 2016 together cnn component hovy 2016 addition convolutional operation directly built lstm resulting convlstm xingjian et 2015 convlstm also connected variety diﬀerent component de brabandere et 2016 kalchbrenner et 2016 one fundamental problem training rnns gradient problem introduced detail bengio et 1994 problem basically state traditional activation function gradient bounded gradient computed backpropagation following chain rule error signal decrease exponentially within time step bptt trace back dependency lost lstm relu known good solution gradient problem however solution introduce way bypass problem clever design instead solving fundamentally method work well practically fundamental problem general rnn still solved pascanu et al attempted some solution still done 53 wang raj optimization neural network primary focus paper deep learning model however optimization inevitable topic development history deep learning model section brieﬂy revisit major topic optimization neural network introduction model some algorithm discussed along model only discus remaining method not mentioned previously gradient method despite fact neural network developed ﬁfty year tion neural network still heavily rely gradient descent method within algorithm backpropagation paper doe not intend introduce classical backpropagation gradient descent method stochastic version batch version simple technique like momentum method start right topic therefore discussion following gradient method start vanilla gradient descent following θt θ gradient parameter θ η hyperparameter usually known learning rate rprop rprop wa introduced riedmiller braun 1993 unique method even studied back today doe not fully utilize information gradient only considers sign word update parameter following θt θ 0 ηi θ 0 stand indicator function unique formalization allows gradient method overcome some cost curvature may not easily solved today dominant method method may worth some study day adagrad adagrad wa introduced duchi et al 2011 follows idea introducing adaptive learning rate mechanism assigns higher learning rate parameter updated mildly assigns lower learning rate parameter updated dramatically measure degree update applied norm historical gradient st θ θ θ 2 therefore update rule following θt η st θ ϵ small term avoid η divided zero 54 origin deep learning adagrad ha showed great improvement robustness upon traditional dient method dean et 2012 however problem norm accumulates fraction η norm decay substantial small term adadelta adadelta extension adagrad aim reducing decaying rate learning rate proposed zeiler 2012 instead accumulating gradient time step adagrad adadelta previously accumulation adding current term onto previously accumulated result resulting st 2 β 2 1 θ 2 β weight update rule adagrad θt η st θ almost another famous gradient variant named adam adam stand adaptive moment estimation proposed kingma ba 2014 adam like combination momentum method adagrad method component time step formally time step θ θ 1 θ st 2 2 1 θ 2 η st θ modern gradient variant published promising claim helpful improve convergence rate previous method empirically method seem indeed helpful however many case good choice method seems only beneﬁt limited extent dropout dropout wa introduced hinton et 2012 srivastava et 2014 technique soon got inﬂuential not only good performance also simplicity implementation idea simple randomly dropping some unit training formally training case hidden unit randomly omitted network probability suggested hinton et al 2012 dropout seen eﬃcient way perform model averaging across large number diﬀerent neural network overﬁtting avoided much le cost computation seems method never get published resource trace back hinton slide slide 55 wang raj actual performance introduces dropout soon became popular upon introduction lot work ha attempted understand mechanism diﬀerent perspective including baldi sadowski 2013 cho 2013 et 2016 ha also applied train model like svm chen et 2014 batch normalization layer normalization batch normalization introduced ioﬀe szegedy 2015 another breakthrough optimization deep neural network addressed problem named internal covariate shift intuitively problem understood following two step 1 learned function barely useful input change statistic input function sometimes denoted covariates 2 layer function change parameter layer change input current layer change could dramatic may shift distribution input ioﬀe szegedy 2015 proposed batch normalization solve issue formally following step µb 1 n n x xi b 1 n n x xi 2 ˆ xi σb ϵ yi ˆ xi µl µb σb denote mean variance batch µl σl two parameter learned algorithm rescale shift output xi yi input output function respectively step performed every batch training batch normalization turned work well training empirically soon became popularly ba et al 2016 proposes technique layer normalization transpose batch normalization layer normalization computing mean variance used normalization summed input neuron layer single training case therefore technique ha nature advantage applicable recurrent neural network straightforwardly however seems transposed batch normalization not implemented simple batch normalization therefore ha not become inﬂuential batch normalization optimization optimal model architecture last section optimization technique neural network revisit some old method attempted aim learn optimal model architecture many method known constructive network approach od proposed decade ago not raise enough impact back nowadays powerful computation resource people start consider method 56 origin deep learning two remark need made proceed 1 obviously od trace back counterpart machine learning ﬁeld method not perform enough raise impact focusing discussion evolutionary path may mislead reader instead only list method reader seek inspiration 2 many method not exclusively optimization technique method usually proposed particularly designed tecture technically speaking method distributed previous section according model associated however method barely inspire modern modeling research may chance inspire modern optimization research list method section learning one earliest important work topic wa proposed fahlman lebiere 1989 introduced model well corresponding algorithm named learning idea algorithm start minimum work build towards bigger network whenever another hidden unit added parameter previous hidden unit ﬁxed algorithm only search optimal parameter hidden unit interestingly unique architecture learning grant work grow deeper wider time every newly added hidden unit take data together output previously added unit input two important question algorithm 1 ﬁx parameter current hidden unit proceed add tune newly added one 2 terminate entire algorithm two question answered similar manner algorithm add new hidden unit no signiﬁcant change existing architecture terminates overall performance satisfying training process may introduce problem overﬁtting might account fact method seen much modern deep learning research tiling algorithm ezard nadal 1989 presented idea tiling algorithm learns ters number layer well number hidden unit layer simultaneously feedforward neural network boolean function later algorithm wa extended multiple class version parekh et al 1997 algorithm work way every layer try build layer hidden unit cluster data diﬀerent cluster only one label one cluster algorithm keep increasing number hidden unit clustering pattern achieved proceed add another layer ezard nadal 1989 also oﬀered proof theoretical guarantee tiling rithm basically theorem say tiling algorithm greedily improve mance neural network 57 wang raj upstart algorithm frean 1990 proposed upstart algorithm long story short algorithm simply neural network version standard decision tree safavian landgrebe 1990 tree node replaced linear perceptron therefore tree seen neural network us core component neural network tree node result standard way building tree advertised building neural network automatically similarly bengio et al 2005 proposed boosting algorithm replace weak classiﬁer neuron evolutionary algorithm evolutionary algorithm family algorithm us mechanism inspired biological evolution search parameter space optimal solution some prominent example family genetic algorithm mitchell 1998 simulates natural selection ant colony optimization algorithm colorni et 1991 simulates cooperation ant colony explore surroundings yao 1999 oﬀered extensive survey usage evolution algorithm upon optimization neural network yao introduced several encoding scheme enable neural network architecture learned evolutionary algorithm encoding scheme basically transfer network architecture vector standard algorithm take input optimize far discussed some representative algorithm aimed learn network architecture automatically algorithm eventually fade modern deep learning research conjecture two main reason outcome 1 rithms tend overﬁt data 2 algorithm following greedy search paradigm unlikely ﬁnd optimal architecture however rapid development machine learning method computation resource last decade hope constructive network method listed still inspire reader substantial contribution modern deep learning research 58 origin deep learning conclusion paper revisited evolutionary path nowadays deep learning model revisited path three major family deep learning model deep generative model family convolutional neural network family recurrent neural network family well some topic optimization technique paper could serve two goal 1 first document major milestone science history impacted current development deep learning stone not limited development computer science ﬁelds 2 importantly revisiting evolutionary path major milestone paper able gest reader remarkable work developed among thousand contemporaneous publication brieﬂy summarize three direction many milestone pursue occam razor seems part society tends favor complex model layering one architecture onto another hoping backpropagation ﬁnd optimal parameter history say mastermind tend think simple dropout widely recognized not only performance simplicity implementation intuitive tentative reasoning hopﬁeld network restricted boltzmann machine model simpliﬁed along iteration rbm ready ambitious model proposed substantially parameter contemporaneous one must solve problem no others solve nicely remarkable lstm much complex traditional rnn bypass vanishing gradient problem nicely deep belief network famous not due fact ﬁrst one come idea putting one rbm onto another due come algorithm allow deep architecture trained eﬀectively widely read many model inspired domain knowledge outside machine learning statistic ﬁeld human visual cortex ha greatly inspired development convolutional neural network even recent popular residual network ﬁnd corresponding mechanism human visual cortex generative adversarial network also ﬁnd some connection game theory wa developed ﬁfty year ago hope direction help some reader impact current society direction also able summarized revisit milestone reader acknowledgement thanks demo quick generation example figure thanks bojian han carnegie mellon versity example figure thanks blog summary gradient method section thanks yutong zheng xupeng tong carnegie mellon university suggesting some relevant content 59 wang raj reference emile aarts jan korst simulated annealing boltzmann machine david h ackley geoﬀrey e hinton terrence j sejnowski learning algorithm boltzmann machine cognitive science 9 1 james anderson edward rosenfeld talking net oral history neural network mit press martin arjovsky soumith chintala eon bottou wasserstein gan arxiv preprint jimmy ba volodymyr mnih koray kavukcuoglu multiple object recognition visual attention arxiv preprint jimmy lei ba jamie ryan kiros geoﬀrey e hinton layer normalization arxiv preprint dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate arxiv preprint alexander bain mind body theory relation alexander bain henry king company pierre baldi peter j sadowski understanding dropout advance neural mation processing system page peter l bartlett wolfgang maass vapnik chervonenkis dimension neural net handbook brain theory neural network page herbert bay tinne tuytelaars luc van gool surf speeded robust feature european conference computer vision page springer yoshua bengio learning deep architecture ai foundation trend r machine learning 2 1 yoshua bengio olivier delalleau expressive power deep architecture international conference algorithmic learning theory page springer yoshua bengio patrice simard paolo frasconi learning dependency gradient descent diﬃcult ieee transaction neural network 5 2 yoshua bengio nicolas l roux pascal vincent olivier delalleau patrice marcotte convex neural network advance neural information processing system page yoshua bengio pascal lamblin dan popovici hugo larochelle et al greedy training deep network advance neural information processing system 2007 60 origin deep learning monica bianchini franco scarselli complexity shallow deep neural network classiﬁers esann cg boeree psychology beginning retrieved april james g booth james p hobert maximizing generalized linear mixed model likelihood automated monte carlo em algorithm journal royal statistical society series b statistical methodology 61 1 org bornschein yoshua bengio reweighted arxiv preprint nirmal k bose et al neural network fundamental graph algorithm application number bos martin l brady raghu raghavan joseph slawny back propagation fails separate perceptrons succeed ieee transaction circuit system 36 5 george w brown iterative solution game ﬁctitious play activity analysis duction allocation 13 1 john bullinaria self organizing map fundamental introduction neural yuri burda roger grosse ruslan salakhutdinov importance weighted autoencoders arxiv preprint wh burnham memory historically experimentally considered historical sketch older conception memory american journal psychology 2 1 alfredo canziani adam paszke eugenio culurciello analysis deep neural network model practical application arxiv preprint miguel geoﬀrey hinton contrastive divergence learning aistats volume 10 page citeseer juan luis castro carlos javier manta jm benıtez neural network continuous squashing function output universal approximators neural network 13 6 ning chen jun zhu jianfei chen bo zhang dropout training support vector machine arxiv preprint xi chen yan duan rein houthooft john schulman ilya sutskever pieter abbeel infogan interpretable representation learning information maximizing generative versarial net advance neural information processing system page 2016 61 wang raj kyunghyun cho understanding dropout training perceptrons iary independent stochastic neuron international conference neural information processing page springer kyunghyun cho bart van enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio learning phrase representation using rnn statistical machine translation arxiv preprint kyunghyun cho aaron courville yoshua bengio describing multimedia content using network ieee transaction multimedia 17 11 anna choromanska mikael henaﬀ michael mathieu erard ben arous yann lecun loss surface multilayer network aistats jan k chorowski dzmitry bahdanau dmitriy serdyuk kyunghyun cho yoshua gio model speech recognition advance neural information processing system page avital cnaan nm laird peter slasor tutorial biostatistics using general linear mixed model analyse unbalanced repeated measure longitudinal data stat med alberto colorni marco dorigo vittorio maniezzo et al distributed optimization ant colony proceeding ﬁrst european conference artiﬁcial life volume 142 page paris france david daniel cox thomas dean neural network computer vision current biology 24 18 g cybenko continuous valued neural network two hidden layer suﬃcient george cybenko approximation superposition sigmoidal function mathematics control signal system 2 4 zihang dai amjad almahairi bachman philip eduard hovy aaron courville brating generative adversarial network iclr submission yann n dauphin razvan pascanu caglar gulcehre kyunghyun cho surya ganguli yoshua bengio identifying attacking saddle point problem optimization advance neural information processing system page bert de brabandere xu jia tinne tuytelaars luc van gool dynamic ﬁlter network neural information processing system nip vincent de ladurantaye jacques jean rouat model information processing visual cortex citeseer 2012 62 origin deep learning jeﬀrey dean greg corrado rajat monga kai chen matthieu devin mark mao andrew senior paul tucker ke yang quoc v le et al large scale distributed deep network advance neural information processing system page olivier delalleau yoshua bengio shallow deep network advance neural information processing system page jia deng wei dong richard socher li kai li li imagenet scale hierarchical image database computer vision pattern recognition cvpr ieee conference page ieee misha denil loris bazzani hugo larochelle nando de freitas learning attend deep architecture image tracking neural computation 24 8 didier emmanuel bigand rethinking physical rehabilitation medicine new technology induce new learning strategy springer science business medium carl doersch tutorial variational autoencoders arxiv preprint john duchi elad hazan yoram singer adaptive subgradient method online learning stochastic optimization journal machine learning research 12 jul angela lee duckworth eli tsukayama henry may establishing causality using gitudinal hierarchical linear modeling illustration predicting achievement control social psychological personality science samuel frederick edward phil w anderson theory spin glass journal physic f metal physic 5 5 ronen eldan ohad shamir power depth feedforward neural network arxiv preprint jeﬀrey l elman finding structure time cognitive science 14 2 dumitru erhan yoshua bengio aaron courville manzagol pascal vincent samy bengio doe unsupervised help deep learning journal machine learning research 11 feb scott e fahlman christian lebiere learning architecture marcus frean upstart algorithm method constructing training feedforward neural network neural computation 2 2 kunihiko fukushima neocognitron neural network model nism pattern recognition unaﬀected shift position biological cybernetics 36 4 1980 63 wang raj leon gatys alexander ecker matthias bethge neural algorithm artistic style arxiv preprint tom germano self organizing map available wpi felix gers urgen schmidhuber recurrent net time count neural network ijcnn 2000 proceeding international joint conference volume 3 page ieee ross girshick fast proceeding ieee international conference puter vision page ross girshick jeﬀdonahue trevor darrell jitendra malik rich feature hierarchy accurate object detection semantic segmentation proceeding ieee conference computer vision pattern recognition page ian goodfellow nip 2016 tutorial generative adversarial network arxiv preprint ian goodfellow jean mehdi mirza bing xu david sherjil ozair aaron courville yoshua bengio generative adversarial net advance neural information processing system page marco gori alberto tesi problem local minimum backpropagation ieee transaction pattern analysis machine intelligence 14 1 eline gravelines deep learning via stacked sparse autoencoders automated wise brain parcellation based functional connectivity phd thesis university western ontario alex graf navdeep jaitly mohamed hybrid speech recognition deep bidirectional lstm automatic speech recognition understanding asru 2013 ieee workshop page ieee klaus greﬀ rupesh kumar srivastava jan ık ba r steunebrink urgen schmidhuber lstm search space odyssey arxiv preprint richard gregory patrick cavanagh blind spot scholarpedia 6 10 stephen grossberg recurrent neural network scholarpedia 8 2 aman gupta haohan wang madhavi ganapathiraju learning structure gene expression data using deep architecture application gene clustering bioinformatics biomedicine bibm 2015 ieee international conference page ieee kevin gurney introduction neural network crc press shyam guthikonda kohonen map wittenberg university 2005 64 origin deep learning bharath hariharan pablo aez ross girshick jitendra malik hypercolumns object segmentation localization proceeding ieee conference computer vision pattern recognition page david hartley observation man volume cambridge university press johan hastad almost optimal lower bound small depth circuit proceeding eighteenth annual acm symposium theory computing page acm james v haxby elizabeth hoﬀman ida gobbini distributed human neural system face perception trend cognitive science 4 6 kaiming xiangyu zhang shaoqing ren jian sun deep residual learning image recognition arxiv preprint kaiming xiangyu zhang shaoqing ren jian sun identity mapping deep residual network arxiv preprint donald olding hebb organization behavior neuropsychological theory ogy press robert theory backpropagation neural network neural network 1989 international joint conference page ieee geoﬀrey e hinton training product expert minimizing contrastive divergence neural computation 14 8 geoﬀrey e hinton peter dayan brendan j frey radford neal algorithm unsupervised neural network science 268 5214 geoﬀrey e hinton simon osindero teh fast learning algorithm deep belief net neural computation 18 7 geoﬀrey e hinton nitish srivastava alex krizhevsky ilya sutskever ruslan r salakhutdinov improving neural network preventing feature tectors arxiv preprint sepp hochreiter urgen schmidhuber long memory neural computation 9 8 john j hopﬁeld neural network physical system emergent collective tional ability proceeding national academy science 79 8 kurt hornik maxwell stinchcombe halbert white multilayer feedforward network universal approximators neural network 2 5 zhiting hu xuezhe zhengzhong liu eduard hovy eric xing harnessing deep neural network logic rule arxiv preprint david h hubel torsten n wiesel receptive ﬁelds single neurones cat striate cortex journal physiology 148 3 1959 65 wang raj sergey ioﬀe christian szegedy batch normalization accelerating deep network ing reducing internal covariate shift arxiv preprint michael jordan serial order parallel distributed processing approach advance psychology nal kalchbrenner aaron van den oord karen simonyan ivo danihelka oriol vinyals alex graf koray kavukcuoglu video pixel network arxiv preprint kiyoshi kawaguchi multithreaded software model backpropagation neural network application ag khachaturyan sv semenovskaya b vainstein proach determination structure amplitude phase sov phys crystallogr 524 diederik kingma jimmy ba adam method stochastic optimization arxiv preprint diederik p kingma max welling variational bayes arxiv preprint diederik p kingma shakir mohamed danilo jimenez rezende max welling supervised learning deep generative model advance neural information processing system page tinne hoﬀkjeldsen john von neumann conception minimax theorem journey diﬀerent mathematical context archive history exact science 56 1 teuvo kohonen map proceeding ieee 78 9 bryan kolb ian q whishaw g campbell teskey introduction brain ior volume 1273 mark l krieg tutorial bayesian belief network ranjay krishna yuke zhu oliver groth justin johnson kenji hata joshua kravitz stephanie chen yannis kalantidis li david shamma et al visual genome connecting language vision using crowdsourced dense image annotation arxiv preprint alex krizhevsky geoﬀrey hinton learning multiple layer feature tiny image alex krizhevsky ilya sutskever geoﬀrey e hinton imagenet classiﬁcation deep convolutional neural network advance neural information processing system page 2012 66 origin deep learning tejas kulkarni william f whitney pushmeet kohli josh tenenbaum deep lutional inverse graphic network advance neural information processing system page brenden lake ruslan salakhutdinov joshua b tenenbaum concept learning probabilistic program induction science 350 6266 alan lapedes robert farber neural net work neural information processing system page hugo larochelle geoﬀrey e hinton learning combine foveal glimpse order boltzmann machine advance neural information processing system page b boser le cun john denker henderson richard e howard w hubbard lawrence jackel handwritten digit recognition network advance neural information processing system citeseer yann lecun eon bottou yoshua bengio patrick haﬀner learning applied document recognition proceeding ieee 86 11 yann lecun corinna cortes christopher jc burges mnist database written digit yann lecun yoshua bengio geoﬀrey hinton deep learning nature 521 7553 honglak lee roger grosse rajesh ranganath andrew ng convolutional deep belief network scalable unsupervised learning hierarchical representation ceedings annual international conference machine learning page acm zhaoping li neural model contour integration primary visual cortex neural computation 10 4 lin michael maire serge belongie james hay pietro perona deva ramanan piotr ar c lawrence zitnick microsoft coco common object context european conference computer vision page springer liou lin finite memory loading hairy neuron natural computing 5 1 liou yuan error tolerant associative memory biological bernetics 81 4 christoph lippert jennifer listgarten ying liu carl kadie robert davidson david heckerman fast linear mixed model association study nature method 8 10 zachary c lipton john berkowitz charles elkan critical review recurrent neural network sequence learning arxiv preprint 2015 67 wang raj david g lowe object recognition local feature computer vision proceeding seventh ieee international conference volume 2 page ieee xuezhe eduard hovy sequence labeling via arxiv preprint xuezhe yingkai gao zhiting hu yaoliang yu yuntian deng eduard hovy dropout regularization arxiv preprint maschler eilon solan shmuel zamir game theory translated hebrew ziv hellman edited mike born charles e mcculloch john neuhaus generalized linear mixed model wiley online library warren mcculloch walter pitt logical calculus idea immanent nervous activity bulletin mathematical biophysics 5 4 marc ezard nadal learning feedforward layered network tiling algorithm journal physic mathematical general 22 12 marc ezard giorgio parisi virasoro spin glass theory beyond marvin l minski seymour papert perceptrons introduction computational geometry mit press cambridge melanie mitchell introduction genetic algorithm mit press tom mitchell et al machine learning wcb jeﬀrey moran robert desimone selective attention gate visual processing extrastriate cortex frontier cognitive neuroscience michael c mozer focused algorithm temporal pattern recognition complex system 3 4 kevin p murphy machine learning probabilistic perspective mit press vinod nair geoﬀrey e hinton rectiﬁed linear unit improve restricted boltzmann machine proceeding international conference machine learning page john nash game annals mathematics page john f nash et al equilibrium point game proc nat acad sci usa 36 1 anh nguyen jason yosinski jeﬀclune deep neural network easily fooled high conﬁdence prediction unrecognizable image 2015 ieee conference computer vision pattern recognition cvpr page ieee 2015 68 origin deep learning danh v nguyen damla urk raymond j carroll linear mixed eﬀects model application longitudinal data journal nonparametric tic 20 6 erkki oja simpliﬁed neuron model principal component analyzer journal matical biology 15 3 erkki oja juha karhunen stochastic approximation eigenvectors value expectation random matrix journal mathematical analysis application 106 1 aaron van den oord nal kalchbrenner koray kavukcuoglu pixel recurrent neural network arxiv preprint keiichi osako rita singh bhiksha raj complex recurrent neural network ing speech signal application signal processing audio acoustic paa 2015 ieee workshop page ieee rajesh g parekh jihoon yang vasant honavar constructive neural network learning algorithm pattern classiﬁcation razvan pascanu caglar gulcehre kyunghyun cho yoshua bengio construct deep recurrent neural network arxiv preprint razvan pascanu tomas mikolov yoshua bengio diﬃculty training recurrent neural network icml 3 razvan pascanu yann n dauphin surya ganguli yoshua bengio saddle point problem optimization arxiv preprint bryan plummer liwei wang chris cervantes juan c caicedo julia hockenmaier svetlana lazebnik entity collecting correspondence richer model proceeding ieee international conference computer vision page tomaso poggio thomas serre model visual cortex scholarpedia 8 4 christopher poultney sumit chopra yann l cun et al eﬃcient learning sparse resentations model advance neural information processing system page jose c principe neil r euliano w curt lefebvre neural adaptive system fundamental simulation john wiley son shaoqing ren kaiming ross girshick jian sun faster towards time object detection region proposal network advance neural information processing system page martin riedmiller heinrich braun direct adaptive method faster gation learning rprop algorithm neural network ieee international conference page ieee 1993 69 wang raj aj robinson frank fallside utility driven dynamic error propagation network university cambridge department engineering frank rosenblatt perceptron probabilistic model information storage nization brain psychological review 65 6 david e rumelhart geoﬀrey e hinton ronald j williams learning internal sentations error propagation technical report dtic document olga russakovsky jia deng hao su jonathan krause sanjeev satheesh sean heng huang andrej karpathy aditya khosla michael bernstein et al imagenet large scale visual recognition challenge international journal computer vision 115 3 rasoul safavian david landgrebe survey decision tree classiﬁer methodology ruslan salakhutdinov geoﬀrey e hinton deep boltzmann machine aistats volume 1 page 3 tim salimans ian goodfellow wojciech zaremba vicki cheung alec radford xi chen improved technique training gans advance neural information processing system page urgen schmidhuber deep learning neural network overview neural network mike schuster kuldip k paliwal bidirectional recurrent neural network ieee action signal processing 45 11 thomas serre lior wolf tomaso poggio object recognition feature inspired visual cortex 2005 ieee computer society conference computer vision pattern recognition cvpr 05 volume 2 page ieee noam shazeer azalia mirhoseini krzysztof maziarz andy davis quoc le geoﬀrey ton jeﬀdean outrageously large neural network expert layer arxiv preprint karen simonyan andrew zisserman deep convolutional network image recognition arxiv preprint paul smolensky information processing dynamical system foundation harmony theory technical report dtic document kihyuk sohn honglak lee xinchen yan learning structured output representation using deep conditional generative model advance neural information processing system page nitish srivastava geoﬀrey e hinton alex krizhevsky ilya sutskever ruslan dinov dropout simple way prevent neural network overﬁtting journal machine learning research 15 1 2014 70 origin deep learning amos storkey increasing capacity hopﬁeld network without sacriﬁcing functionality international conference artiﬁcial neural network page springer richard sutton david mcallester satinder p singh yishay mansour et al icy gradient method reinforcement learning function approximation nip volume 99 page christian szegedy wojciech zaremba ilya sutskever joan bruna dumitru erhan ian goodfellow rob fergus intriguing property neural network arxiv preprint christian szegedy wei liu yangqing jia pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich going deeper convolution proceeding ieee conference computer vision tern recognition page aaron van den oord nal kalchbrenner lasse espeholt oriol vinyals alex graf et al conditional image generation pixelcnn decoder advance neural information processing system page andreas veit michael j wilber serge belongie residual network behave like sembles relatively shallow network advance neural information processing system page pascal vincent hugo larochelle isabelle lajoie yoshua bengio zagol stacked denoising autoencoders learning useful representation deep network local denoising criterion journal machine learning research 11 dec 3408 martin j wainwright michael jordan et al graphical model exponential family variational inference foundation trend r machine learning 1 brian wandell foundation vision sinauer associate haohan wang jingkang yang multiple confounders correction regularized linear mixed eﬀect model application biological process haohan wang aaksha meghawat morency eric p xing learning improving generalization multimodal sentiment analysis arxiv preprint paul j werbos generalization backpropagation application recurrent gas market model neural network 1 4 paul j werbos backpropagation time doe proceeding ieee 78 10 bernard widrow et al adaptive adaline neuron using chemical 1960 71 wang raj alan l wilkes nicholas j wade bain neural network brain cognition 33 3 gibbs j willard elementary principle statistical mechanic rational foundation thermodynamics new york charles scribners son london edward arnold andrew g wilson christoph dann chris lucas eric p xing human kernel advance neural information processing system page shi xingjian zhourong chen hao wang yeung wong chun woo convolutional lstm network machine learning approach precipitation nowcasting advance neural information processing system page kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan nov richard zemel yoshua bengio show attend tell neural image caption generation visual attention arxiv preprint 2 3 zhilin yang ruslan salakhutdinov william cohen sequence tagging scratch arxiv preprint andrew yao separating hierarchy oracle annual symposium foundation computer science sfcs 1985 xin yao evolving artiﬁcial neural network proceeding ieee 87 9 dong yu li deng george dahl role dependent speech recognition proc nip workshop deep learning unsupervised feature learning sergey zagoruyko nikos komodakis wide residual network arxiv preprint matthew zeiler adadelta adaptive learning rate method arxiv preprint chiyuan zhang samy bengio moritz hardt benjamin recht oriol vinyals understanding deep learning requires rethinking generalization arxiv preprint ke zhang miao sun tony x han xingfang yuan liru guo tao liu ual network residual network multilevel residual network arxiv preprint junbo zhao michael mathieu yann lecun generative adversarial work arxiv preprint xiang zhou matthew stephen eﬃcient analysis ciation study nature genetics 44 7 2012 72