origin deep learn origin deep learn haohan wang haohanw bhiksha raj bhiksha languag technolog institut school comput scienc carnegi mellon univers abstract thi paper review evolutionari histori deep learn model cover genesi neural network association model brain studi model domin last decad research deep learn like convolut neural network deep belief network recurr neural network addit review model thi paper primarili focus preced model abov examin initi idea assembl construct earli model preliminari model develop current form mani evolutionari path last half centuri divers direct exampl cnn built prior knowledg biolog vision system dbn evolv model power comput complex graphic model mani nowaday model neural counterpart ancient linear model thi paper review evolutionari path oﬀer concis thought ﬂow model develop aim provid thorough background deep learn importantli along path thi paper summar gist behind mileston propos mani direct guid futur research deep learn mar wang raj introduct deep learn ha dramat improv mani diﬀer artiﬁci intellig task like object detect speech recognit machin translat lecun et deep architectur natur grant deep learn possibl solv mani complic ai task bengio result research extend deep learn varieti diﬀer modern domain task addit tradit task like object detect face recognit languag model exampl osako et al use recurr neural network denois speech signal gupta et al use stack autoencod discov cluster pattern gene express gati et al use neural model gener imag diﬀer style wang et al use deep learn allow sentiment analysi multipl modal simultan etc thi period era wit bloom deep learn research howev fundament push deep learn research frontier forward one need thoroughli understand ha attempt histori whi current model exist present form thi paper summar evolutionari histori sever diﬀer deep learn model explain main idea behind model relationship ancestor understand past work trivial deep learn ha evolv long time histori show tabl therefor thi paper aim oﬀer reader major mileston deep learn research cover mileston show tabl well mani addit work split stori diﬀer section clear present thi paper start discuss research human brain model although success deep learn nowaday necessarili due resembl human brain due deep architectur ambit build system simul brain inde thrust initi develop neural network therefor next section begin connection natur lead age shallow neural network matur matur neural network thi paper continu brieﬂi discuss cessiti extend shallow neural network deeper one well promis deep neural network make challeng deep architectur introduc establish deep neural network thi paper diverg three ferent popular deep learn topic speciﬁc section thi paper elabor deep belief net construct compon restrict boltzmann machin evolv model power comput load section thi paper focus develop histori convolut neural network featur promin step along ladder imagenet competit section thi paper discuss ment recurr neural network successor like lstm attent model success achiev thi paper primarili discuss deep learn model optim deep tectur inevit topic thi societi section devot brief summari optim techniqu includ advanc gradient method dropout batch tion etc thi paper could read complementari schmidhub schmidhub paper aim assign credit contribut present state art hi paper focus everi singl increment work along path therefor origin deep learn tabl major mileston cover thi paper year contribut contribut bc aristotl introduc association start histori human attempt understand brain alexand bain introduc neural group earliest model neural network inspir hebbian learn rule mcculloch pitt introduc mcp model consid ancestor artiﬁci neural model donald hebb consid father neural network introduc hebbian learn rule lay foundat modern neural network frank rosenblatt introduc ﬁrst perceptron highli resembl modern perceptron paul werbo introduc backpropag teuvo kohonen introduc self organ map kunihiko fukushima introduc neocogitron inspir convolut neural network john hopﬁeld introduc hopﬁeld network hilton sejnowski introduc boltzmann machin paul smolenski introduc harmonium later known restrict boltzmann machin michael jordan deﬁn introduc recurr neural network yann lecun introduc lenet show possibl deep neural network practic schuster paliw introduc bidirect recurr neural network hochreit schmidhub introduc lstm solv problem vanish gradient recurr neural network geoﬀrey hinton introduc deep belief network also introduc pretrain techniqu open current deep learn era salakhutdinov hinton introduc deep boltzmann machin geoﬀrey hinton introduc dropout eﬃcient way train neural network wang raj orat well enough hand paper aim provid background reader understand model develop therefor phasiz mileston elabor idea help build associ idea addit path classic deep learn model schmidhub also discuss recent deep learn work build classic linear model anoth articl reader could read complementari anderson rosenfeld author conduct extens interview scientiﬁc leader topic neural network histori origin deep learn aristotl modern artiﬁci neural network studi deep learn artiﬁci neural network origin ambit build comput system simul human brain build system requir understand function cognit system therefor thi paper trace way back origin attempt understand brain start discuss aristotl association around association therefor accomplish act reminisc pass certain seri precurs movement arriv movement one quest habitu consequ henc hunt mental train excogit present similar contrari coadjac thi process reminisc take place movement case sometim time sometim part whole subsequ movement alreadi half thi remark paragraph aristotl seen start point ism burnham association theori state mind set conceptu element organ associ element inspir plato aristotl examin process remembr recal brought four law associ boere contigu thing event spatial tempor proxim tend associ mind frequenc number occurr two event proport strength associ two event similar thought one event tend trigger thought similar event contrast thought one event tend trigger thought opposit event back aristotl describ implement law mind common sens exampl feel smell tast appl natur lead concept appl common sens nowaday surpris see law propos year ago still serv fundament assumpt machin learn method exampl sampl near deﬁn distanc cluster one group explanatori variabl frequent occur respons variabl draw attent model data usual repres embed latent space contemporan similar law also propos zeno citium epicuru st augustin hippo theori association wa later strengthen varieti philosoph psychologist thoma hobb state complex experi associ simpl experi associ sensat also believ associ exist mean coher frequenc strength wang raj figur illustr neural group bain factor meanwhil john lock introduc concept associ idea still separ concept idea sensat idea reﬂect state complex idea could deriv combin two simpl idea david hume later reduc aristotl four law three resembl similar contigu caus eﬀect believ whatev coher world seem wa matter three law dugald stewart extend three law sever principl among obviou one accident coincid sound word thoma reid believ origin qualiti mind wa requir explain spontan recurr think rather habit jame mill emphas law frequenc key learn veri similar later stage research david hartley physician wa remark regard one made association popular hartley addit exist law propos hi argument memori could conceiv smaller scale vibrat region brain origin sensori experi vibrat link repres complex idea therefor act materi basi stream conscious thi idea potenti inspir hebbian learn rule discuss later thi paper lay foundat neural network bain neural group besid david hartley alexand bain also contribut fundament idea hebbian learn rule wilk wade thi book bain relat process associ memori distribut activ neural group term use denot neural network back propos construct mode storag capabl assembl wa requir contrast altern tradit mode storag prestor memori illustr hi idea bain ﬁrst describ comput ﬂexibl allow neural group function multipl associ store hypothesi bain manag describ structur highli resembl neural origin deep learn network today individu cell summar stimul select link cell within group show figur joint stimul c trigger x stimul b c trigger stimul c trigger hi origin illustr b c stand simul x outcom cell establish thi associ structur neural group function memori bain proceed describ construct structur follow direct association state relev impress neural group must made tempor contigu period either one occas repeat occas bain describ comput properti neural group connect strengthen weaken experi via chang interven therefor induct circuit would select compar strong weak see follow section hebb postul highli resembl bain scription although nowaday usual label thi postul hebb rather bain accord wilk wade thi omiss bain contribut may also due bain lack conﬁdenc hi theori eventu bain wa convinc doubt practic valu neural group hebbian learn rule hebbian learn rule name donald hebb sinc wa introduc hi work organ behavior hebb hebb also seen father neural network becaus thi work didier bigand hebb state famou rule cell ﬁre togeth wire togeth emphas activ behavior cell speciﬁc hi book state axon cell near enough excit cell b repeatedli persist take part ﬁring growth process metabol chang take place one cell eﬃcienc one cell ﬁring b thi archaic paragraph modern machin learn languag follow ηxiy stand chang synapt weight wi neuron input signal xi denot postsynapt respons η denot learn rate word hebbian learn rule state connect two unit strengthen frequenc two unit increas although hebbian learn rule seen lay foundat neural network seen today drawback obviou appear weight tion keep increas weight domin signal increas exponenti thi known unstabl hebbian learn rule princip et fortun problem inﬂuenc hebb ident father neural network wang raj oja rule princip compon analyz erkki oja extend hebbian learn rule avoid unstabl properti also show neuron follow thi updat rule approxim behavior princip compon analyz pca oja long stori short oja introduc normal term rescu hebbian learn rule show hi learn rule simpli onlin updat princip compon analyz present detail thi argument follow paragraph start equat follow notat oja show wt ηxiy denot iter straightforward way avoid explod weight appli normal end iter yield wt ηxiy pn wt ηxiy n denot number neuron abov equat expand follow form wt z η yxi z wi pn j yxjwj z pn two assumpt introduc η small therefor approxim weight normal therefor z pn two assumpt introduc back previou equat oja rule wa propos follow wt ηy xi oja took step show neuron wa updat thi rule wa eﬀect perform princip compon analysi data show thi oja ﬁrst equat follow form two addit assumpt oja wt cwt wt cwt wt c covari matrix input proceed show thi properti mani conclus hi anoth work oja karhunen link back pca fact compon pca eigenvector ﬁrst compon eigenvector correspond largest eigenvalu covari matrix intuit could interpret thi properti simpler explan eigenvector c solut maxim rule updat function sinc wt eigenvector covari matrix x get wt pca oja learn rule conclud stori learn rule neural network proceed visit idea neural model origin deep learn mcp neural model donald hebb seen father neural network ﬁrst model neuron could trace back six year ahead public hebbian learn rule neurophysiologist warren mcculloch mathematician walter pitt specul inner work neuron model primit neural network electr circuit base ﬁnding mcculloch pitt model known mcp neural model wa linear step function upon weight linearli interpol data could describ p wixi zj otherwis stand output xi stand input signal wi stand correspond weight zj stand inhibitori input θ stand threshold function design way activ ani inhibitori input complet prevent excit neuron ani time despit resembl mcp neural model modern perceptron still diﬀer distinctli mani diﬀer aspect mcp neural model initi built electr circuit later see studi neural network ha borrow mani idea ﬁeld electr circuit weight mcp neural model wi ﬁxed contrast adjust weight modern perceptron weight must assign manual calcul idea inhibitori input quit unconvent even seen today might idea worth studi modern deep learn research perceptron success mcp neural model frank rosenblatt substanti hebbian learn rule introduct perceptron rosenblatt theorist like hebb focus biolog system natur environ rosenblatt construct electron devic name perceptron wa show abil learn accord association rosenblatt introduc perceptron context vision system show figur introduc rule organ perceptron follow stimuli impact retina sensori unit respond manner puls amplitud frequenc proport stimulu intens impuls transmit project area ai thi project area option impuls transmit associ area random connect sum impuls intens equal greater threshold θ thi unit thi unit ﬁre wang raj illustr organ perceptron rosenblatt b typic perceptron modern machin ing literatur figur perceptron new ﬁgure illustr organ perceptron rosenblatt b typic perceptron nowaday ai project area omit respons unit work fashion intermedi unit figur illustr hi explan perceptron left right four unit sensori unit project unit associ unit respons unit respect project unit receiv inform sensori unit pass onto associ unit thi unit often omit descript similar model omiss project unit structur resembl structur nowaday perceptron neural network show figur b sensori unit collect data associ unit linearli add data diﬀer weight appli transform onto threshold sum pass result respons unit one distinct earli stage neuron model modern perceptron introduct activ function use sigmoid function exampl figur b thi origin argument linear threshold function soften simul biolog neural network bose et well consider feasibl comput replac step function continu one mitchel et rosenblatt introduct perceptron widrow et al introduc model call adalin howev diﬀer rosenblatt perceptron adalin mainli algorithm aspect primari focu thi paper neural network model skip discuss adalin perceptron linear represent power perceptron fundament linear function input signal therefor limit repres linear decis boundari like logic oper like xor sophist decis boundari requir thi limit wa highlight minski papert attack limit percept emphas perceptron solv function like xor nxor result veri littl research wa done thi area origin deep learn b c figur linear represent power preceptron show concret exampl introduc linear preceptron onli two input therefor decis boundari form line space choic threshold magnitud shift line horizont sign function pick one side line halfspac function repres halfspac show figur figur b present two node b denot input well node denot situat trigger node denot situat neither trigger figur b figur c show clearli linear perceptron use describ oper two input howev figur interest xor oper oper longer describ singl linear decis boundari next section show represent abil greatli enlarg put perceptron togeth make neural network howev keep stack one neural network upon make deep learn model represent power necessarili increas wang raj modern neural network era deep learn thi section introduc import properti neural network properti partial explain popular neural network gain day also vate necess explor deeper architectur speciﬁc discuss set univers approxim properti properti ha condit show although shallow neural network univers approxim deeper ture signiﬁcantli reduc requir resourc retain represent power last also show interest properti discov backpropag may inspir relat research today univers approxim properti step perceptron basic neural network onli place perceptron togeth place perceptron side side get singl neural network stack one neural network upon get neural network often known perceptron mlp kawaguchi one remark properti neural network wide known univers approxim properti roughli describ mlp repres ani function discuss thi properti three diﬀer aspect boolean approxim mlp one hidden repres ani boolean function exactli continu approxim mlp one hidden layer approxim ani bound continu function arbitrari accuraci arbitrari approxim mlp two hidden layer approxim ani function arbitrari accuraci discuss three properti detail follow paragraph suit diﬀer reader interest ﬁrst oﬀer intuit explan properti oﬀer proof represent ani boolean function thi approxim properti veri straightforward previou section shown everi linear preceptron perform either accord de morgan law everi proposit formula convert equival conjunct normal form multipl function therefor simpli rewrit target boolean function multipl oper design network way input layer perform oper hidden layer simpli oper formal proof veri diﬀer thi intuit explan skip simplic thi paper follow wide accept name convent call neural network one hidden layer neural network origin deep learn b c figur exampl univers approxim ani bound continu function approxim ani bound continu function continu linear represent power perceptron discuss previous want repres complex function show figur use set linear perceptron describ halfspac one perceptron shown figur b need ﬁve perceptron perceptron bound target function show figur c number show figur c repres number subspac describ perceptron fall correspond region see appropri select threshold θ figur c bound target function therefor describ ani bound continu function onli one hidden layer even shape complic figur thi properti wa ﬁrst shown cybenko hornik et speciﬁc cybenko show function follow form f x x ωiσ wt x θ f x dens subspac word arbitrari function g x subspac f x x x ϵ ϵ equat σ denot activ function squash function back wi denot weight input layer ωi denot weight hidden layer thi conclus wa drawn proof contradict theorem riesz represent theorem fact closur f x subspac f x contradict assumpt σ activ squash function till today thi properti ha drawn thousand citat unfortun mani later work cite thi properti inappropri castro et becaus equat wide accept form neural network becaus doe deliv output linear output instead ten year later thi properti wa shown castro et al conclud thi stori show ﬁnal output squash thi univers approxim properti still hold note thi properti wa shown context activ function ing function deﬁnit squash function σ r function wang raj figur threshold necessari larg number linear perceptron properti x x mani activ function recent deep learn research fall thi categori approxim arbitrari function befor move explain thi properti need ﬁrst show major properti regard combin linear perceptron neural network figur show number linear perceptron increas bound target function area outsid polygon sum close threshold shrink follow thi trend use larg number perceptron bound circl thi achiev even without know threshold becaus area close threshold shrink noth left outsid circl fact area sum n n number perceptron use therefor neural network one hidden layer repres circl arbitrari diamet introduc anoth hidden layer use combin output mani diﬀer circl thi newli ad hidden layer onli use perform oper figur show exampl extra hidden layer use merg circl previou layer neural network use approxim ani function target function necessarili continu howev circl requir larg number neuron consequ entir function requir even thi properti wa show laped farber cybenko tive look back thi properti today arduou build connect thi properti fourier seri approxim inform word state everi function curv decompos sum mani simpler curv thi linkag show thi univers approxim properti show ani layer neural network repres one simpl surfac second hidden layer sum simpl surfac approxim arbitrari function know one hidden layer neural network simpli perform threshold sum erat therefor onli step left show ﬁrst hidden layer repres simpl surfac understand simpl surfac linkag fourier transform one imagin one cycl sinusoid case bump plane case origin deep learn figur neural network use approxim leaf shape function one dimens creat simpl surfac onli need two sigmoid function appropri place exampl follow x h x h x x creat simpl surfac height thi could easili gener case need sigmoid function neuron simpl surfac simpl surfac contribut ﬁnal function one neuron ad onto second hidden layer therefor despit number neuron need one never need third hidden layer approxim ani function similarli gibb phenomenon aﬀect fourier seri approxim thi mation guarante exact represent univers approxim properti show great potenti shallow neural work price exponenti mani neuron layer one question reduc number requir neuron maintain represent power thi question motiv peopl proceed deeper neural network despit shallow neural network alreadi inﬁnit model power anoth issu worth tion although neural network approxim ani function trivial ﬁnd set paramet explain data next two section discuss two question respect wang raj necess depth univers approxim properti shallow neural network come price nential mani neuron therefor realist question maintain thi express power network reduc number comput unit ha ask year intuit bengio delalleau suggest natur pursu deeper network becaus human neural system deep architectur see exampl section human visual cortex human tend resent concept one level abstract composit concept lower level nowaday solut build deeper architectur come conclus state represent power k layer neural network polynomi mani neuron need express exponenti mani neuron k layer structur use howev theoret thi conclus still complet thi conclus could trace back three decad ago yao show limit shallow circuit function hastad later show thi properti pariti circuit function comput polynomi size depth k requir exponenti size depth restrict k show thi properti mainli applic demorgan law state ani rewritten vice versa therefor simpliﬁ circuit appear one rewrit one layer therefor merg thi oper neighbor layer repeat thi procedur wa abl repres function fewer layer comput move circuit neural network delalleau bengio compar deep shallow neural network show function could express n neuron network depth k requir least n k neuron neural network bianchini scarselli extend thi studi gener neural work mani major activ function includ tanh sigmoid deriv conclus concept betti number use thi number describ represent power neural network show shallow network resent power onli grow polynomi respect number neuron deep architectur represent grow exponenti respect number neuron also relat conclus neural network tanh bartlett maass p number paramet recent eldan shamir present thorough proof show depth neural network exponenti valuabl width neural network standard mlp ani popular activ function conclus drawn onli weak assumpt constrain activ function mildli increas surabl abl allow shallow neural network approxim ani univari lipschitz function final theori support fact deeper network prefer shallow one howev realiti mani problem aris keep increas layer among increas diﬃculti learn proper paramet probabl promin one immedi next section discuss main drive search paramet neural network backpropag origin deep learn backpropag properti befor proceed need clarifi name backpropag origin refer algorithm use learn paramet neural network instead stand techniqu help eﬃcient comput gradient paramet gradient descent algorithm appli learn paramet howev nowaday wide recogn term refer gradient descent algorithm techniqu compar standard gradient descent updat paramet spect error backpropag ﬁrst propag error term output layer back layer paramet need updat use standard gradient descent updat paramet respect propag error intuit deriv backpropag organ term gradient express chain rule deriv neat skip thi paper due extens resourc avail werbo mitchel et lecun et instead discuss two interest seemingli contradictori properti backpropag backpropag find global optim linear separ data gori tesi studi problem local minima backpropag estingli societi believ neural network deep learn approach believ suﬀer local optim propos architectur global optim guarante onli weak assumpt network need reach global optim includ pyramid architectur upper layer fewer neuron weight matric full row rank number input neuron smaller data howev approach may relev anymor requir data linearli separ condit mani model appli backpropag fail linear separ data hand bradi et al studi situat backpropag fail linearli separ data set show could situat data linearli separ neural network learn backpropag ﬁnd boundari also show exampl thi situat occur hi illustr exampl onli hold misclassiﬁ data sampl signiﬁcantli less correctli classiﬁ data sampl word misclassiﬁ data sampl might outlier therefor thi interest properti view today arguabl desir properti backpropag typic expect machin learn model neglect outlier therefor thi ﬁnding ha attract mani attent howev matter whether data outlier neural network abl overﬁt train data given suﬃcient train iter legitim learn algorithm especi consid bradi et al show inferior algorithm wang raj wa abl overﬁt data therefor thi phenomenon play critic role research improv optim techniqu recent studi cost surfac neural network indic exist saddl point choromanska et dauphin et pascanu et may explain ﬁnding bradi et al back late backpropag enabl optim deep neural network howev still long way go befor optim well later section brieﬂi discuss techniqu relat optim neural network origin deep learn network memori deep belief net figur trade oﬀof represent power comput complex sever model guid develop better model background modern neural network set proceed visit promin branch current deep learn famili ﬁrst stop branch lead popular restrict boltzmann machin deep belief net start model understand data unsupervisedli figur summar model cover thi section horizont axi stand comput complex model vertic axi stand represent power six mileston focus thi section place ﬁgure self organ map discuss start self organ map som invent kohonen som power techniqu primarili use reduc dimens data usual one two dimens germano reduc dimension som also retain topolog similar data point also seen tool cluster impos topolog cluster represent figur illustr self organ map two dimens hidden neuron therefor learn two dimens represent data upper shade node denot unit som use wang raj figur illustr map repres data lower circl denot data connect node som posit node ﬁxed represent view onli numer valu instead posit also matter thi properti diﬀer represent criterion exampl compar case vector som use denot color denot green set c green red purpl represent use ani vector long specifi bit green correspondingli howev som onli two vector possibl thi becaus sinc som aim repres data retain similar red purpl much similar green red green purpl green repres way split red purpl one notic thi exampl onli use demonstr posit unit som matter practic valu som unit restrict integ learn som usual good tool visual data exampl conduct survey happi level rich level countri feed data som train unit repres happiest richest countri one corner repres opposit countri furthest corner rest two corner repres richest yet unhappiest poorest happiest countri rest countri posit accordingli advantag som allow one literatur bullinaria exampl one may notic connect illustr model howev connect onli use repres neighborhood relationship node inform ﬂow via connect thi paper show mani model reli clear illustr inform ﬂow decid save connect denot origin deep learn easili tell countri rank among world simpl glanc learn unit guthikonda learn algorithm understand represent power som proceed eter learn algorithm classic algorithm heurist intuit shown use som exampl j index unit w weight initi weight unit wi j j pick vk randomli select best match unit bmu p q arg mini j select node interest neighbor bmu wi wi j wp q r updat weight wi j wi j p j p q l j end unit v denot data vector k index data denot current iter n constrain maximum number step allow p denot penalti consid distanc unit p q unit j l learn rate r denot radiu use select neighbor node l r typic decreas increas denot euclidean distanc dist denot distanc posit unit thi algorithm explain som use learn represent similar retain alway select subset unit similar data sampl adjust weight unit match data sampl howev thi algorithm reli care select radiu neighbor select good initi weight otherwis although learn weight local properti topolog similar lose thi properti global sometim two similar cluster similar event separ anoth dissimilar cluster similar event simpler word unit green may actual separ unit red unit purpl network appropri train germano hopﬁeld network hopﬁeld network histor describ form neural network ﬁrst introduc hopﬁeld recurr thi context refer fact weight connect neuron bidirect hopﬁeld network wide recogn becaus memori properti thi memori properti simul spin glass theori therefor start discuss spin glass term recurr veri confus nowaday becaus popular recurr neural network rnn gain wang raj figur illustr hopﬁeld network fulli connect network six binari threshold neural unit everi unit connect data therefor unit denot unshad node spin glass spin glass physic term use describ magnet phenomenon mani work done detail studi relat theori edward anderson ezard et thi paper onli describ thi intuit group dipol place togeth ani space dipol forc align ﬁeld gener dipol locat howev align chang ﬁeld locat lead dipol ﬂip caus ﬁeld origin locat chang eventu chang converg stabl state describ stabl state ﬁrst deﬁn total ﬁeld locat j sj oj ct x k sk jk oj extern ﬁeld ct constant depend temperatur sk polar kth dipol djk distanc locat j locat therefor total potenti energi system pe x j sjoj ctsj x k sk jk magnet system evolv thi potenti energi minimum hopfield network hopﬁeld network fulli connect neural network binari threshold neural unit valu unit either unit fulli connect bidirect weight literatur may use denot valu unit choic valu doe aﬀect idea hopﬁl network chang formul energi function thi paper onli discuss context valu origin deep learn thi set energi hopﬁeld network deﬁn e x sibi x j sisjwi j state unit b denot bia w denot bidirect weight j index unit thi energi function close connect potenti energi function spin glass show equat hopﬁeld network typic appli memor state data weight network design learn make sure energi minim given state interest therefor anoth state present network weight ﬁxed hopﬁeld network search state minim energi recov state memori exampl face complet task imag face present hopﬁeld network way unit network correspond pixel one imag imag present one network calcul weight minim energi given face later one imag corrupt distort present thi network network abl recov origin imag search conﬁgur state minim energi start corrupt input present term energi may remind peopl physic explain hopﬁeld network work physic scenario clearer natur use hopﬁeld network memor equilibrium posit pendulum becaus equilibrium posit pendulum ha lowest gravit potenti energi therefor whenev pendulum place converg back equilibrium posit learn infer learn weight hopﬁeld network straightforward gurney weight calcul wi j x j notat equat thi learn procedur simpl still worth mention essenti step hopﬁeld network appli solv practic problem howev ﬁnd mani onlin tutori omit thi step make wors refer infer state remov confus thi paper similar term use standard machin learn societi refer calcul weight model either solut numer solut paramet learn train refer process appli exist model weight known onto solv problem infer test decod hidden state data predict label infer hopﬁeld network also intuit state data network test invert state one unit whether energi decreas infer convent use way machin learn societi although statistician may disagre thi usag wang raj network invert state proceed test next unit thi procedur call asynchron updat thi procedur obvious subject sequenti order select unit counterpart known synchron updat network ﬁrst test unit invert simultan method may lead local optim synchron updat may even result increas energi may converg oscil loop state capac one distinct disadvantag hopﬁeld network keep memori veri eﬃcient becaus network n unit onli store memori bit network n unit ha edg addit store memori instanc data connect ha integ valu rang thu number bit requir store n unit hopﬁeld therefor safe draw conclus although hopﬁeld network remark idea enabl network memor data extrem ineﬃci practic invent hopﬁeld network mani work attempt studi increas capac origin hopﬁeld network storkey liou yuan liou lin despit attempt made hopﬁeld network still gradual fade societi replac model inspir immedi follow thi section discuss popular boltzmann machin restrict mann machin studi model upgrad initi idea hopﬁeld network evolv replac boltzmann machin boltzmann machin invent ackley et al stochast version hopﬁeld network got name boltzmann distribut boltzmann distribut boltzmann distribut name ludwig boltzmann investig extens willard origin use describ probabl distribut particl system variou possibl state follow f kt stand state es correspond energi k boltzmann constant thermodynam temperatur respect natur ratio two bution onli character diﬀer energi follow r f f e kt known boltzmann factor origin deep learn figur illustr boltzmann machin introduct hidden unit shade node model conceptu split two part visibl unit hidden unit red dash line use highlight conceptu separ distribut speciﬁ energi probabl deﬁn term state divid normal follow psi esi kt p j esj kt boltzmann machin mention previous boltzmann machin stochast version hopﬁeld network figur introduc idea hidden unit introduc turn hopﬁeld network boltzmann machin boltzmann machin onli visibl unit connect data hidden unit use assist visibl unit describ distribut data therefor model conceptu split visibl part hidden part still maintain fulli connect network among unit stochast introduc boltzmann machin improv hopﬁeld work regard leap local optimum oscil state inspir physic method transfer state regardless current energi introduc set state state mean state regardless current state follow probabl p stand diﬀer energi state oﬀ stand temperatur idea inspir physic process higher temperatur like state addit probabl higher energi state transfer lower energi state alway greater revers thi idea highli relat veri popular optim molecul move faster kinet energi provid could achiev heat thi correspond zeroth law thermodynam wang raj algorithm call simul anneal khachaturyan et aart korst back simul anneal hardli relev nowaday deep learn societi regardless histor import term introduc within thi section assum constant sake simpliﬁc energi boltzmann machin energi function boltzmann machin deﬁn equat deﬁn hopﬁeld network except visibl unit hidden unit note separ follow e v h x vibi x k hkbk x j vivjwij x k vihkwik x k l hkhlwk l v stand visibl unit h stand hidden unit thi equat also connect back equat except boltzmann machin split energi function accord hidden unit visibl unit base thi energi function probabl joint conﬁgur visibl unit hidden unit deﬁn follow p v h v h p n n probabl unit achiev margin thi joint biliti exampl margin hidden unit get probabl distribut visibl unit p v p h v h p n n could use sampl visibl unit gener data boltzmann machin train stabl state call thermal librium distribut probabl p v h remain constant becaus distribut energi constant howev probabl visibl unit hidden unit may vari energi may minimum thi relat thermal equilibrium deﬁn onli constant factor distribut part system thermal equilibrium hard concept understand one imagin pour cup hot water bottl pour cup cold water onto hot water start bottl feel hot bottom feel cold top gradual bottl feel mild cold water hot water mix heat transfer howev temperatur bottl becom mild stabli correspond distribut p v h doe necessarili mean molecul ceas move correspond p v h paramet learn common way train boltzmann machin determin paramet maxim likelihood observ data gradient descent log likelihood origin deep learn function usual perform determin paramet simplic follow deriv base singl observ first log likelihood function visibl unit l v w log p v w log x h h x n n second term rh normal take deriv log likelihood function w simplifi v w x h p v h x n p n n v h ep n n e denot expect thu gradient likelihood function compos two part ﬁrst part expect gradient energi function respect condit distribut p second part expect gradient energi function respect joint distribut variabl state howev calcul expect gener infeas ani model involv sum huge number possibl gener approach solv thi problem use markov chain mont carlo mcmc approxim sum v w si sj p si sj p denot expect equat diﬀer expect valu product state data fed visibl state expect product state data fed ﬁrst term calcul take averag valu energi function gradient visibl hidden unit driven observ data sampl practic thi ﬁrst term gener straightforward calcul calcul second term gener complic involv run set markov chain reach current model equilibrium distribut take averag energi function gradient base sampl howev thi sampl procedur could veri comput complic motiv topic next section restrict boltzmann machin restrict boltzmann machin restrict boltzmann machin rbm origin known harmonium invent smolenski version boltzmann machin restrict connect either visibl unit hidden unit figur illustr restrict boltzmann machin achiev base boltzmann machin figur connect hidden unit well connect visibl unit remov model becom bipartit graph thi restrict introduc energi function rbm much simpler e v h x vibi x k hkbk x k vihkwik wang raj figur illustr restrict boltzmann machin restrict connect hidden unit shade node connect visibl unit unshad node boltzmann machin turn restrict boltzmann machin model bipartit graph contrast diverg rbm still train way boltzmann machin train sinc energi function rbm much simpler sampl method use infer second term equat becom easier despit thi rel simplic thi learn procedur still requir larg amount sampl step approxim model distribut emphas diﬃculti sampl mechan well simplifi introduct equat diﬀer set notat follow v w si sj si sj use denot data distribut denot model distribut notat remain unchang therefor diﬃculti mention method learn paramet requir potenti inﬁnit mani sampl step approxim model distribut hinton overcam thi issu magic introduct method name contrast diverg empir found one doe perform inﬁnit mani sampl step converg model distribut ﬁnite k step sampl enough therefor equat eﬀect v w si sj si sj pk remark hinton show k suﬃcient learn algorithm work well practic hinton attempt justifi contrast diverg theori deriv led neg conclus contrast diverg origin deep learn figur illustr deep belief network deep belief network stack rbm togeth bottom layer layer except top one connect onli connect top bias algorithm ﬁnite k repres model distribut howev empir result suggest ﬁnite k approxim model distribut well enough result small enough bia addit algorithm work well practic strengthen idea contrast diverg reason model power fast approxim algorithm rbm quickli draw great attent becom one fundament build block deep neural network follow two section introduc two distinguish deep neural network built base machin name deep belief net deep boltzmann machin deep belief net deep belief network introduc hinton et al show rbm stack train greedi manner figur show structur deep belief network diﬀer stack rbm dbn onli allow connect connect top one layer follow bottom layer onli connect probabl better way understand dbn think gener model despit fact dbn gener describ stack rbm quit diﬀer put one rbm top probabl appropri think dbn rbm extend layer special devot gener pattern data therefor model onli need sampl thermal equilibrium topmost layer pass visibl state top gener data thi paper gener seen open nowaday deep learn era ﬁrst introduc possibl train deep neural network layerwis train wang raj paramet learn paramet learn deep belief network fall two step ﬁrst step second step layerwis success deep belief network larg due troduct pretrain idea simpl reason whi work still attract research simpli ﬁrst train network compon compon bottom treat ﬁrst two layer rbm train treat second layer third layer anoth rbm train paramet idea turn oﬀer critic support success later tune process sever explan attempt explain mechan intuit clever way initi put paramet valu appropri rang bengio et al suggest unsupervis initi model point paramet space lead eﬀect optim process optim ﬁnd lower minimum empir cost function erhan et al empir argu regular explan pervis pretrain guid learn toward basin attract minima support better gener train data set addit deep belief network thi pretrain mechan also inspir train mani classic model includ autoencod poultney et bengio et deep boltzmann machin salakhutdinov hinton model inspir classic model like yu et perform carri optim work search paramet lead lower minimum deep belief network two diﬀer ﬁne tune strategi depend goal network fine tune gener model gener model achiev contrast version algorithm hinton et thi algorithm intrigu reason design interpret brain work scientist found sleep critic process brain function seem invers version learn awak algorithm also ha two step wake phase propag inform bottom adjust weight reconstruct layer sleep phase invers wake phase propag inform top adjust weight reconstruct layer abov contrast version thi algorithm add one contrast diverg phase wake phase sleep phase wake phase onli goe visibl layer top rbm sampl top rbm contrast diverg sleep phase start visibl layer top rbm fine tune discrimin model strategi ﬁne tune dbn discrimin model simpli appli standard backpropag model origin deep learn figur illustr deep boltzmann machin deep boltzmann machin like stack rbm togeth connect everi two layer bidirect sinc label data howev still necessari spite gener good perform backpropag deep boltzmann machin last mileston introduc famili deep gener model deep boltzmann machin introduc salakhutdinov hinton figur show three layer deep boltzmann machin dbm distinct tween dbm dbn mention previou section dbm allow bidirect connect bottom layer therefor dbm repres idea stack rbm much better way dbn although might clearer dbm name deep restrict boltzmann machin due natur dbm energi function deﬁn extens energi function rbm equat show follow e v h x vibi n x x k hn kbn k x k viwikhk x x k l hn kwn k l dbm n hidden layer thi similar energi function grant possibl train dbm constr diverg howev typic necessari deep boltzmann machin dbm deep belief network dbn acronym suggest deep boltzmann machin deep belief network mani similar especi ﬁrst glanc deep neural network nate idea restrict boltzmann machin name deep belief network wang raj seem indic also partial origin bayesian network krieg also reli layerwis success paramet learn howev fundament diﬀer two model dramat duce connect made bottom layer direct bidirect structur dbm grant possibl dbm learn complex pattern data also grant possibl approxim infer procedur incorpor feedback addit initi pass ing deep boltzmann machin better propag uncertainti ambigu input deep gener model futur deep boltzmann machin last mileston discuss histori gener model still much work dbm even done futur lake et al introduc bayesian program learn framework simul human learn abil larg scale visual concept addit perform learn classiﬁc task model pass visual ture test term gener handwritten charact world alphabet word gener perform model indistinguish human behavior deep neural model model outperform sever concurr deep neural network deep neural counterpart bayesian program learn framework sure expect even better perform condit imag gener given part imag also anoth interest topic recent problem usual solv pixel network pixel cnn van den oord et pixel rnn oord et howev given part imag seem simplifi gener task anoth contribut gener model gener adversari network fellow et howev gan still young discuss thi paper origin deep learn convolut neural network vision problem thi section start discuss diﬀer famili model convolut ral network cnn famili distinct famili previou section convolut neural network famili mainli evolv knowledg human visual cortex fore thi section ﬁrst introduc one import reason account success convolut neural network vision problem bionic design replic human vision system nowaday convolut neural network probabl inat design rather ancestor background brieﬂi introduc success model make selv famou imagenet challeng deng et last present known problem vision task may guid futur research direct vision task visual cortex convolut neural network wide known inspir visual cortex howev except public discuss thi inspir brieﬂi poggio serr cox dean resourc present thi inspir thoroughli thi section focu discuss basic visual cortex hubel wiesel lay ground studi convolut neural network visual cortex brain locat occipit lobe locat back skull part cerebr cortex play import role process visual inform visual inform come eye goe seri brain structur reach visual cortex part visual cortex receiv sensori input known primari visual cortex also known area visual inform manag extrastri area includ visual area two four also visual area thi paper primarili focu visual area relat object recognit known ventral stream consist area inferior tempor gyru one higher level ventral stream visual process associ represent complex object featur global shape like face percept haxbi et figur illustr ventral stream visual cortex show inform process procedur retina receiv imag inform pass way inferior tempor gyru compon retina convert light energi come ray bounc oﬀof object chemic energi thi chemic energi convert action potenti transfer onto primari visual cortex fact sever brain structur involv retina omit structur deliber discuss compon connect establish technolog lution neural network one interest develop power model encourag investig compon wang raj figur brief illustr ventral stream visual cortex human vision system consist primari visual cortex visual area inferior tempor gyru primari visual cortex mainli fulﬁll task edg detect edg area strongest local contrast visual signal also known secondari visual cortex ﬁrst region within visual sociat area receiv strong feedforward connect send strong connect later area cell tune extract mainli simpl properti visual signal orient spatial frequenc colour complex properti fulﬁll function includ detect object featur intermedi complex like simpl geometr shape addit orient spatial frequenc color also shown strong attent modul moran desimon also receiv direct input inferior tempor gyru ti respons identifi object base color form object compar process inform store memori object identifi object kolb et word perform semant level task like face recognit mani descript function visual cortex reviv recollect convolut neural network reader expos relev technic literatur later thi section discuss detail convolut neural network help build explicit connect even reader bare origin deep learn knowledg convolut neural network thi hierarch structur visual cortex immedi ring bell neural network besid convolut neural network visual cortex ha inspir work comput vision long time exampl li built neural model inspir primari visual cortex anoth granular serr et al introduc system featur detect inspir visual cortex de ladurantay et al publish book describ model inform process visual cortex poggio serr conduct comprehens survey relev topic focu ani particular subject detail survey thi section discuss connect visual cortex convolut neural network detail begin neocogitron borrow idea visual cortex later inspir convolut neural network neocogitron visual cortex neocogitron propos fukushima gener seen model inspir convolut neural network comput side neural network sist two diﬀer kind layer featur extractor structur connect organ extract featur consist number inspir cell primari visual cortex serv featur extractor ideal train respons particular featur present recept ﬁeld gener local featur edg particular orient extract lower layer global featur extract higher layer thi structur highli resembl human conceiv object resembl complex cell higher pathway visual cortex mainli introduc shift invari properti featur extract paramet learn dure paramet learn process onli paramet updat itron also train unsupervisedli good featur extractor train process veri similar hebbian learn rule strengthen connect whichev show strongest respons thi train mechan also introduc problem hebbian learn rule introduc strength connect satur sinc keep increas solut wa also introduc fukushima wa introduc name inhibitori cell perform function normal avoid problem convolut neural network visual cortex proceed neocogitron convolut neural network first troduc build compon convolut layer subsampl layer assembl compon present convolut neural network use lenet exampl wang raj figur simpl illustr two dimens convolut oper convolut oper convolut oper strictli mathemat oper treat equal oper like addit multipl discuss particularli machin learn literatur howev still discuss ness reader may familiar convolut mathemat oper two function f g produc third function h integr express amount overlap one function f shift function g describ formal follow h z f τ g dτ denot h f convolut neural network typic work convolut tion could summar figur show figur leftmost matrix input matrix middl one usual call kernel matrix convolut appli matric result show rightmost matrix convolut process product follow sum show exampl left upper matrix convolut kernel result slide target matrix one column right convolut kernel get result keep slide record result matrix becaus kernel everi target matrix thu everi matrix convolut one digit whole matrix shrunk matrix becaus ﬁrst mean size kernel matrix one realiz convolut local shift invari mean mani diﬀer combin nine number upper matrix place convolut result thi invari properti play critic role vision problem becaus ideal case recognit result chang due shift rotat featur thi critic properti use solv elegantli low bay et al convolut neural network brought perform new level connect cnn visual cortex idea two dimens convolut discuss convolut use oper simul task perform visual cortex origin deep learn ident kernel b edg detect kernel c blur kernel sharpen kernel e lighten kernel f darken kernel g random kernel h random kernel figur convolut kernel exampl diﬀer kernel appli imag result diﬀer process imag note divisor appli kernel convolut oper usual known kernel diﬀer choic kernel diﬀer oper imag could achiev oper typic includ ident edg detect blur sharpen etc introduc random matric tion oper interest properti might discov figur illustr exampl kernel appli ﬁgure one see diﬀer kernel appli fulﬁll diﬀer task random kernel also appli transform imag interest outcom figur b show edg detect one central task primari visual cortex fulﬁll clever choic kernel furthermor clever select kernel lead us success replic visual cortex result learn meaning convolut kernel paramet learn one central task convolut neural network appli vision task thi also explain whi wang raj figur illustr lenet conv stand convolut layer pling stand subsampl layer mani popular model usual perform well task onli limit process kernel well train univers applic understand essenti role convolut oper play vision task proceed investig major mileston along way pioneer convolut neural network lenet thi section devot model wide recogn ﬁrst convolut neural network lenet invent le cun et al made popular lecun et inspir neocogitron thi section introduc convolut neural network via introduc lenet figur show illustr architectur lenet consist two pair convolut layer subsampl layer connect fulli connect layer rbf layer classiﬁc convolut layer convolut layer primarili layer perform convolut oper discuss previous clever select convolut kernel eﬀect simul task visual cortex convolut layer introduc anoth oper convolut assist simul success transform consid relu nair hinton transform deﬁn follow f x max x transform remov neg part input result clearer contrast meaning featur oppos side product kernel produc therefor thi grant convolut power extract use featur allow simul function visual cortex close origin deep learn subsampl layer subsampl layer perform simpler task onli sampl one input everi region look diﬀer strategi sampl consid like take maximum valu input take averag valu input even probabilist pool take random one lee et sampl turn input represent smaller manag embed importantli sampl make network invari small transform tion translat input imag small distort input chang outcom pool sinc take valu local neighborhood lenet two import compon introduc stack togeth sembl convolut neural network follow recip figur end famou lenet lenet known abil classifi digit handl varieti diﬀer problem digit includ varianc posit scale rotat squeez digit even diﬀer stroke width digit meanwhil introduct lenet lecun et al also introduc mnist databas later becom standard benchmark digit recognit ﬁeld mileston imagenet challeng success lenet convolut neural network ha shown great tential solv vision task potenti attract larg number research aim solv vision task regard object recognit cifar classiﬁc krizhevski hinton imagenet challeng russakovski et along thi path sever superstar mileston attract great attent ha appli ﬁeld good perform thi section brieﬂi discuss model alexnet lenet one start era convolut neural network alexnet invent krizhevski et al one start era cnn use imagenet classiﬁc alexnet ﬁrst evid cnn perform well thi histor diﬃcult imagenet dataset perform well lead societi competit develop cnn success alexnet onli due thi uniqu design architectur also due clever mechan train avoid comput expens train process alexnet ha split two stream train two gpu also use data augment techniqu consist imag translat horizont reﬂect patch extract recip alexnet shown figur howev rare ani lesson learn architectur alexnet despit remark perform even unfortun fact thi particular architectur alexnet doe ground theoret support push mani research blindli burn comput resourc wang raj figur illustr alexnet test new architectur mani model introduc dure thi period onli may worth mention futur vgg blind competit explor diﬀer architectur simonyan zisserman show simplic promis direct model name vgg although vgg deeper layer model around time architectur extrem simpliﬁ layer convolut layer pool layer thi simpl usag convolut layer simul larger ﬁlter keep beneﬁt smaller ﬁlter size becaus combin two convolut layer ha eﬀect recept ﬁeld convolut layer fewer paramet spatial size input volum layer decreas result convolut pool layer depth volum increas becaus increas number ﬁlter vgg number ﬁlter doubl pool layer thi behavior reinforc idea vgg shrink spatial dimens grow depth vgg winner imagenet competit year winner googlenet invent szegedi et al googlenet introduc sever import concept like incept modul concept later use girshick et girshick ren et design architectur bare contribut vgg doe societi especi consid residu net follow path vgg imagenet challeng unpreced level residu net residu net resnet layer network wa ten time deeper wa usual seen dure time wa invent et al follow path vgg introduc resnet explor deeper structur simpl layer howev naiv origin deep learn figur illustr residu block resnet increas number layer onli result wors result train case test case et breakthrough resnet introduc allow resnet substanti deeper previou network call residu block idea behind residu block input certain layer denot x pass compon two layer later either follow tradit path involv convolut layer relu transform success denot result f x go express way directli pass x result input compon two layer later f x x instead typic seen f x idea residu block illustr figur complementari work et al valid residu block essenti propag inform smoothli therefor simpliﬁ optim also extend resnet version success cifar data set anoth interest perspect resnet provid veit et show resnet behav behav like ensembl shallow network express way introduc allow resnet perform collect independ network network signiﬁcantli shallow integr resnet thi also explain whi gradient pass architectur without vanish talk vanish gradient problem discuss recurr neural network next section anoth work directli relev resnet may help understand conduct hariharan et al show featur lower layer inform addit summar ﬁnal layer resnet still complet vacant clever design number layer whole network number layer residu block allow ident bypass still choic requir experiment valid nonetheless extent resnet ha shown critic reason help develop cnn better blind wang raj experiment trail addit idea residu block ha found actual visual cortex ventral stream visual cortex directli accept signal primari visual cortex although resnet design accord thi ﬁrst place introduct neural model success challeng canziani et al conduct comprehens experiment studi compar model upon comparison show still room improv fulli connect layer show strong ineﬃci smaller batch imag challeng chanc fundament vision problem resnet end stori new model techniqu appear everi day push limit cnn exampl zhang et al took step put residu block insid residu block zagoruyko komodaki attempt decreas depth network increas width howev increment work thi kind scope thi paper would like end stori convolut neural network current challeng fundament vision problem may abl solv naiv investig machin learn techniqu network properti vision blind spot convolut neural network reach unpreced accuraci object tion howev may still far industri reliabl applic due intrigu properti found szegedi et al szegedi et al show could forc deep learn model misclassifi imag simpli ad perturb imag importantli perturb may even observ nake human eye word two object look almost human may recogn diﬀer object neural network exampl alexnet also shown thi properti like model problem contrast problem rais insuﬃci train hand nguyen et al show could gener pattern convey almost inform human recogn object neural network high conﬁdenc sometim sinc neural network calli forc make predict surpris see network classifi meaningless patter someth howev thi high conﬁdenc may indic fundament diﬀer neural network human learn know thi world figur show exampl aforement two work construct show neural network may misclassifi object easili recogn human someth unusu hand neural network may also classifi weird pattern believ object human someth familiar properti may restrict usag deep learn real world applic reliabl predict necessari even without exampl one may also realiz reliabl predict neural network could issu due fundament properti matrix exist null space long perturb happen within null space matrix one may abl alter imag dramat neural network still make origin deep learn b c e f g h figur illustr mistak neural network szegedi et adversari imag gener base origin imag diﬀer origin one nake eye neural network success classifi origin one fail adversari one e h nguyen et pattern gener neural network classifi e school bu f guitar g peacock h pekines respect misclassiﬁc high conﬁdenc null space work like blind spot matrix chang within null space never sensibl correspond matrix thi blind spot discourag promis futur neural network contrari make convolut neural network resembl human vision system deeper level human vision system blind spot gregori cavanagh also exist wandel interest work might seen link ﬂaw human vision system defect neural network help overcom defect futur human label prefer veri last present misclassiﬁ imag resnet imagenet leng hope exampl could inspir new methodolog invent fundament vision problem figur show misclassiﬁ imag resnet appli imagenet leng label provid human eﬀort veri unexpect even mani human therefor error rate resnet gener human usual predict error rate probabl hit limit sinc label prefer tator harder predict actual label exampl figur b h label tini part imag import content express imag hand figur e annot background imag imag obvious center object wang raj ﬂute b guinea pig c wig seashor e alp f screwdriv g comic book h sunglass figur fail imag imagenet classiﬁc resnet primari label associ imag improv perform resnet reach one direct might model annot label prefer one assumpt could annot prefer label imag make distinguish establish work model human factor wilson et could help howev import question whether worth optim model increas test result imagenet dataset sinc remain misclassiﬁc may result incompet model problem annot introduct data set like coco lin et flickr plummer et visualgenom krishna et may open new era vision problem competit challeng howev fundament problem experi thi section introduc never forgotten origin deep learn time seri data recurr network thi section start discuss new famili deep learn model attract mani attent especi task time seri data sequenti data recurr neural network rnn class neural network whose connect unit form direct cycl thi natur grant abil work tempor data ha also discuss literatur like grossberg lipton et thi paper continu oﬀer complementari view survey emphasi evolutionari histori mileston model aim provid insight futur direct come model recurr neural network jordan network elman network discuss previous hopﬁeld network wide recogn recurr neural network although formal distinctli diﬀer recurr neural network deﬁn nowaday therefor despit literatur tend begin discuss rnn hopﬁeld network treat member rnn famili avoid unnecessari confus modern deﬁnit recurr initi introduc jordan network ha one cycl possibl follow path unit back network refer recurr nonrecurr network ha cycl hi model jordan later refer jordan network simpl neural network one hidden layer input denot x weight hidden layer denot wh weight output layer denot wy weight recurr comput denot wr hidden represent denot h output denot jordan network formul ht σ whx σ wyht year later anoth rnn wa introduc elman formal recurr structur slightli diﬀer later hi network known elman network elman network formal follow ht σ whx σ wyht onli diﬀer whether inform previou time step provid previou output previou hidden layer thi diﬀer illustr figur diﬀer illustr respect histor contribut work one may notic fundament diﬀer two structur sinc yt wyht therefor onli diﬀer lie choic wr origin elman onli introduc hi network wr gener case could deriv nevertheless step jordan network elman network still remark introduc possibl pass inform hidden layer signiﬁcantli improv ﬂexibl structur design later work wang raj structur jordan network b structur elman network figur diﬀer recurr structur jordan network elman network backpropag time recurr structur make tradit backpropag infeas becaus recurr structur end point backpropag stop intuit one solut unfold recurr structur expand ward neural network certain time step appli tradit backpropag onto thi unfold neural network thi solut known backpropag time bptt independ invent sever research includ robinson fallsid werbo mozer howev recurr neural network usual ha complex cost surfac naiv backpropag may work well later thi paper see recurr structur introduc critic problem exampl vanish gradient problem make optim rnn great challeng societi bidirect recurr neural network unfold rnn get structur feedforward neural network nite depth therefor build conceptu connect rnn feedforward network inﬁnit layer sinc neural network histori bidirect neural network play import role like hopﬁeld network rbm dbm question recurr structur correspond inﬁnit layer bidirect model answer bidirect recurr neural network origin deep learn figur unfold structur brnn tempor order left right hidden layer unfold standard way rnn hidden layer unfold simul revers connect bidirect recurr neural network brnn wa invent schuster paliw goal introduc structur wa unfold bidirect neural network therefor appli time seri data onli inform pass follow natur tempor sequenc inform also revers provid knowledg previou time step figur show unfold structur brnn hidden layer unfold standard way rnn hidden layer unfold simul revers connect transpar figur appli emphas unfold rnn onli concept use illustr purpos actual model handl data diﬀer time step singl model brnn formul follow ht σ ht σ σ subscript denot variabl associ hidden layer respect introduct recurr connect back futur tion time longer directli feasibl solut treat thi model combin two rnn standard one revers one appli bptt onto weight updat simultan onc two gradient comput long memori anoth breakthrough rnn famili wa introduc year brnn hochreit schmidhub introduc new neuron rnn famili name long memori lstm wa invent term lstm use refer algorithm wang raj design overcom vanish gradient problem help special design memori cell nowaday lstm wide use denot ani recurr network memori cell nowaday refer lstm cell lstm wa introduc overcom problem rnn long term cie bengio et overcom thi issu requir special design memori cell illustr figur lstm consist sever critic compon state valu use oﬀer inform output data denot x state valu previou hidden layer thi tradit rnn denot state valu linear combin hidden state input current time step denot σ wixxt state valu serv memori denot gate valu use decid inform ﬂow state gate decid whether input state enter intern state denot g gt σ wgiit gate decid whether intern state forget previou intern state denot f ft σ wfiit gate decid whether intern state pass valu output hidden state next time step denot ot σ woiit final consid gate decid inform ﬂow state last two equat complet formul lstm mt ht product figur describ detail lstm cell work figur b show input state construct describ equat figur c show origin deep learn lstm memori cell b input data previou hidden state form input state c calcul input gate forget gate calcul output gate e updat intern state f output updat hidden state figur lstm cell detail function wang raj input gate forget gate comput describ equat equat figur show output gate comput describ equat figur e show intern state updat describ equat figur f show output hidden state updat describ equat weight paramet need learn dure train therefor theoret lstm learn memor long time depend necessari learn forget past necessari make power model thi import theoret guarante mani work attempt improv lstm exampl ger schmidhub ad peephol connect allow gate use inform intern state cho et al introduc gate recurr unit known gru simpliﬁ lstm merg intern state hidden state one state merg forget gate input gate simpl updat gate integr lstm cell bidirect rnn also intuit look grave et interestingli despit novel lstm variant propos greﬀet al conduct experi investig perform lstm got conclus none variant improv upon standard lstm architectur signiﬁcantli probabl improv lstm anoth direct rather updat structur insid cell attent model seem direct go attent model attent model loos base bionic design simul behavior human vision attent mechan human look imag scan bit bit stare whole imag focu major part gradual build context captur gist attent mechan ﬁrst discuss larochel hinton denil et al attent model mostli refer model introduc bahdanau et machin translat soon appli mani diﬀer domain like chorowski et speech recognit xu et imag caption gener attent model mostli use sequenc output predict instead see whole sequenti data make one singl predict exampl languag model model need make sequenti predict sequenti input task like machin translat imag caption gener therefor attent model mostli use answer question pay attent base previous predict label hidden state output sequenc may link input sequenc input data may even sequenc therefor usual framework cho et necessari encod use encod data represent decod use make sequenti predict attent mechan use locat region represent predict label current time step figur show basic attent model network structur represent encod encod access attent model attent model onli select region pass onto lstm cell usag predict make origin deep learn figur unfold structur attent model transpar use show unfold onli conceptu represent encod learn avail decod across time step attent modul onli select pass onto lstm cell predict therefor magic attent model thi attent modul figur help local inform represent formal work use r denot encod represent total region represent use h denot hidden state lstm cell attent modul gener unscal weight ith region encod resent βt f r j j attent weight comput previou time step pute current time step simpl softmax function αt exp βt pm j exp βt j therefor use weight α reweight represent r predict two way represent reweight soft attent result simpl weight sum context vector rt x j αt jcj hard attent model forc make hard decis onli local one region sampl one region follow multinoulli distribut wang raj deep input architectur b deep recurr ture c deep output architectur figur three diﬀer formul deep recurr neural network one problem hard attent sampl multinoulli distribut diﬀerenti therefor gradient base method hardli appli variat method ba et polici gradient base method sutton et consid deep rnn futur rnn thi veri last section evolutionari path rnn famili visit idea fulli explor deep recurr neural network although recurr neural network suﬀer mani issu deep neural network ha becaus recurr connect current rnn still deep model regard tion learn compar model famili pascanu et al formal idea construct deep rnn extend current rnn figur show three diﬀer direct construct deep recurr neural network increas layer input compon figur recurr compon figur b output compon figur c respect origin deep learn futur rnn rnn improv varieti diﬀer way like assembl piec togeth condit random field yang et togeth cnn compon hovi addit convolut oper directli built lstm result convlstm xingjian et thi convlstm also connect varieti diﬀer compon de brabander et kalchbrenn et one fundament problem train rnn gradient problem introduc detail bengio et problem basic state tradit activ function gradient bound gradient comput backpropag follow chain rule error signal decreas exponenti within time step bptt trace back depend lost lstm relu known good solut gradient problem howev solut introduc way bypass thi problem clever design instead solv fundament method work well practic fundament problem gener rnn still solv pascanu et al attempt solut still done wang raj optim neural network primari focu thi paper deep learn model howev optim inevit topic develop histori deep learn model thi section brieﬂi revisit major topic optim neural network dure introduct model algorithm discuss along model onli discuss remain method mention previous gradient method despit fact neural network develop ﬁfti year tion neural network still heavili reli gradient descent method within algorithm backpropag thi paper doe intend introduc classic backpropag gradient descent method stochast version batch version simpl techniqu like momentum method start right topic therefor discuss follow gradient method start vanilla gradient descent follow θt θ gradient paramet θ η hyperparamet usual known learn rate rprop rprop wa introduc riedmil braun uniqu method even studi back today doe fulli util inform gradient onli consid sign word updat paramet follow θt θ ηi θ stand indic function thi uniqu formal allow gradient method overcom cost curvatur may easili solv today domin method thi method may worth studi day adagrad adagrad wa introduc duchi et al follow idea introduc adapt learn rate mechan assign higher learn rate paramet updat mildli assign lower learn rate paramet updat dramat measur degre updat appli norm histor gradient st θ θ θ therefor updat rule follow θt η st θ ϵ small term avoid η divid zero origin deep learn adagrad ha show great improv robust upon tradit dient method dean et howev problem norm accumul fraction η norm decay substanti small term adadelta adadelta extens adagrad aim reduc decay rate learn rate propos zeiler instead accumul gradient time step adagrad adadelta previous accumul befor ad current term onto previous accumul result result st β θ β weight updat rule adagrad θt η st θ almost anoth famou gradient variant name adam adam stand adapt moment estim propos kingma ba adam like combin momentum method adagrad method compon time step formal time step θ θ θ st θ η st θ modern gradient variant publish promis claim help improv converg rate previou method empir method seem inde help howev mani case good choic method seem onli beneﬁt limit extent dropout dropout wa introduc hinton et srivastava et techniqu soon got inﬂuenti onli becaus good perform also becaus simplic implement idea veri simpl randomli drop unit train formal train case hidden unit randomli omit network probabl suggest hinton et al dropout seen eﬃcient way perform model averag across larg number diﬀer neural network overﬁt avoid much less cost comput seem thi method never get publish resourc trace back hinton slide http slide wang raj becaus actual perform introduc dropout soon becam veri popular upon introduct lot work ha attempt understand mechan diﬀer perspect includ baldi sadowski cho et ha also appli train model like svm chen et batch normal layer normal batch normal introduc ioﬀ szegedi anoth breakthrough optim deep neural network address problem name intern covari shift intuit problem understood follow two step learn function bare use input chang statist input function sometim denot covari layer function chang paramet layer chang input current layer thi chang could dramat may shift distribut input ioﬀ szegedi propos batch normal solv thi issu formal follow step µb n n x xi b n n x xi ˆ xi σb ϵ yi ˆ xi µl µb σb denot mean varianc batch µl σl two paramet learn algorithm rescal shift output xi yi input output function respect step perform everi batch dure train batch normal turn work veri well train empir soon becam popularli ba et al propos techniqu layer normal transpos batch normal layer normal comput mean varianc use normal sum input neuron layer singl train case therefor thi techniqu ha natur advantag applic recurr neural network straightforwardli howev seem thi transpos batch normal implement simpl batch normal therefor ha becom inﬂuenti batch normal optim optim model architectur veri last section optim techniqu neural network revisit old method attempt aim learn optim model architectur mani method known construct network approach od propos decad ago rais enough impact back nowaday power comput resourc peopl start consid method origin deep learn two remark need made befor proceed obvious od trace back counterpart machin learn ﬁeld becaus method perform enough rais impact focus discuss evolutionari path may mislead reader instead onli list method reader seek inspir mani method exclus optim techniqu becaus method usual propos particularli design tectur technic speak method distribut previou section accord model associ howev becaus method bare inspir modern model research may chanc inspir modern optim research list method thi section learn one earliest import work thi topic wa propos fahlman lebier introduc model well correspond algorithm name learn idea algorithm start minimum work build toward bigger network whenev anoth hidden unit ad paramet previou hidden unit ﬁxed algorithm onli search optim paramet hidden unit interestingli uniqu architectur learn grant work grow deeper wider time becaus everi newli ad hidden unit take data togeth output previous ad unit input two import question thi algorithm ﬁx paramet current hidden unit proceed add tune newli ad one termin entir algorithm two question answer similar manner algorithm add new hidden unit signiﬁc chang exist architectur termin overal perform satisfi thi train process may introduc problem overﬁt might account fact thi method seen much modern deep learn research tile algorithm ezard nadal present idea tile algorithm learn ter number layer well number hidden unit layer simultan feedforward neural network boolean function later thi algorithm wa extend multipl class version parekh et al algorithm work way everi layer tri build layer hidden unit cluster data diﬀer cluster onli one label one cluster algorithm keep increas number hidden unit cluster pattern achiev proceed add anoth layer ezard nadal also oﬀer proof theoret guarante tile rithm basic theorem say tile algorithm greedili improv manc neural network wang raj upstart algorithm frean propos upstart algorithm long stori short thi algorithm simpli neural network version standard decis tree safavian landgreb tree node replac linear perceptron therefor tree seen neural network becaus use core compon neural network tree node result standard way build tree advertis build neural network automat similarli bengio et al propos boost algorithm replac weak classiﬁ neuron evolutionari algorithm evolutionari algorithm famili algorithm use mechan inspir biolog evolut search paramet space optim solut promin exampl thi famili genet algorithm mitchel simul natur select ant coloni optim algorithm colorni et simul cooper ant coloni explor surround yao oﬀer extens survey usag evolut algorithm upon optim neural network yao introduc sever encod scheme enabl neural network architectur learn evolutionari algorithm encod scheme basic transfer network architectur vector standard algorithm take input optim far discuss repres algorithm aim learn network architectur automat algorithm eventu fade modern deep learn research conjectur two main reason thi outcom rithm tend overﬁt data algorithm follow greedi search paradigm unlik ﬁnd optim architectur howev rapid develop machin learn method comput resourc last decad hope construct network method list still inspir reader substanti contribut modern deep learn research origin deep learn conclus thi paper revisit evolutionari path nowaday deep learn model revisit path three major famili deep learn model deep gener model famili convolut neural network famili recurr neural network famili well topic optim techniqu thi paper could serv two goal first document major mileston scienc histori impact current develop deep learn stone limit develop comput scienc ﬁeld importantli revisit evolutionari path major mileston thi paper abl gest reader remark work develop among thousand contemporan public brieﬂi summar three direct mani mileston pursu occam razor seem part societi tend favor complex model layer one architectur onto anoth hope backpropag ﬁnd optim paramet histori say mastermind tend think simpl dropout wide recogn onli becaus perform becaus simplic implement intuit tent reason hopﬁeld network restrict boltzmann machin model simpliﬁ along iter rbm readi ambiti model propos substanti paramet contemporan one must solv problem solv nice remark lstm much complex tradit rnn bypass vanish gradient problem nice deep belief network famou due fact ﬁrst one come idea put one rbm onto anoth due come algorithm allow deep architectur train eﬀect wide read mani model inspir domain knowledg outsid machin learn statist ﬁeld human visual cortex ha greatli inspir develop convolut neural network even recent popular residu network ﬁnd correspond mechan human visual cortex gener adversari network also ﬁnd connect game theori wa develop ﬁfti year ago hope direct help reader impact current societi direct also abl summar revisit mileston reader acknowledg thank demo http quick gener exampl figur thank bojian han carnegi mellon versiti exampl figur thank blog http summari gradient method section thank yutong zheng xupeng tong carnegi mellon univers suggest relev content wang raj refer emil aart jan korst simul anneal boltzmann machin david h ackley geoﬀrey e hinton terrenc j sejnowski learn algorithm boltzmann machin cognit scienc jame anderson edward rosenfeld talk net oral histori neural network mit press martin arjovski soumith chintala eon bottou wasserstein gan arxiv preprint jimmi ba volodymyr mnih koray kavukcuoglu multipl object recognit visual attent arxiv preprint jimmi lei ba jami ryan kiro geoﬀrey e hinton layer normal arxiv preprint dzmitri bahdanau kyunghyun cho yoshua bengio neural machin translat jointli learn align translat arxiv preprint alexand bain mind bodi theori relat alexand bain henri king compani pierr baldi peter j sadowski understand dropout advanc neural mation process system page peter l bartlett wolfgang maass vapnik chervonenki dimens neural net handbook brain theori neural network page herbert bay tinn tuytelaar luc van gool surf speed robust featur european confer comput vision page springer yoshua bengio learn deep architectur ai foundat trend r machin learn yoshua bengio olivi delalleau express power deep architectur intern confer algorithm learn theori page springer yoshua bengio patric simard paolo frasconi learn depend gradient descent diﬃcult ieee transact neural network yoshua bengio nicola l roux pascal vincent olivi delalleau patric marcott convex neural network advanc neural inform process system page yoshua bengio pascal lamblin dan popovici hugo larochel et al greedi train deep network advanc neural inform process system origin deep learn monica bianchini franco scarselli complex shallow deep neural network classiﬁ esann cg boere psycholog begin retriev april jame g booth jame p hobert maxim gener linear mix model likelihood autom mont carlo em algorithm journal royal statist societi seri b statist methodolog org bornschein yoshua bengio reweight arxiv preprint nirmal k bose et al neural network fundament graph algorithm applic number bo martin l bradi raghu raghavan joseph slawni back propag fail separ perceptron succeed ieee transact circuit system georg w brown iter solut game ﬁctitiou play activ analysi duction alloc john bullinaria self organ map fundament introduct neural yuri burda roger gross ruslan salakhutdinov import weight autoencod arxiv preprint wh burnham memori histor experiment consid histor sketch older concept memori american journal psycholog alfredo canziani adam paszk eugenio culurciello analysi deep neural network model practic applic arxiv preprint miguel geoﬀrey hinton contrast diverg learn aistat volum page cites juan lui castro carlo javier manta jm benıtez neural network continu squash function output univers approxim neural network ning chen jun zhu jianfei chen bo zhang dropout train support vector machin arxiv preprint xi chen yan duan rein houthooft john schulman ilya sutskev pieter abbeel infogan interpret represent learn inform maxim gener versari net advanc neural inform process system page wang raj kyunghyun cho understand dropout train perceptron iari independ stochast neuron intern confer neural inform process page springer kyunghyun cho bart van enboer caglar gulcehr dzmitri bahdanau fethi bougar holger schwenk yoshua bengio learn phrase represent use rnn statist machin translat arxiv preprint kyunghyun cho aaron courvil yoshua bengio describ multimedia content use network ieee transact multimedia anna choromanska mikael henaﬀ michael mathieu erard ben arou yann lecun loss surfac multilay network aistat jan k chorowski dzmitri bahdanau dmitriy serdyuk kyunghyun cho yoshua gio model speech recognit advanc neural inform process system page avit cnaan nm laird peter slasor tutori biostatist use gener linear mix model analys unbalanc repeat measur longitudin data stat med alberto colorni marco dorigo vittorio maniezzo et al distribut optim ant coloni proceed ﬁrst european confer artiﬁci life volum page pari franc david daniel cox thoma dean neural network comput vision current biolog g cybenko continu valu neural network two hidden layer suﬃcient georg cybenko approxim superposit sigmoid function mathemat control signal system zihang dai amjad almahairi bachman philip eduard hovi aaron courvil brate gener adversari network iclr submiss yann n dauphin razvan pascanu caglar gulcehr kyunghyun cho surya ganguli yoshua bengio identifi attack saddl point problem optim advanc neural inform process system page bert de brabander xu jia tinn tuytelaar luc van gool dynam ﬁlter network neural inform process system nip vincent de ladurantay jacqu jean rouat model inform process visual cortex cites origin deep learn jeﬀrey dean greg corrado rajat monga kai chen matthieu devin mark mao andrew senior paul tucker ke yang quoc v le et al larg scale distribut deep network advanc neural inform process system page olivi delalleau yoshua bengio shallow deep network advanc neural inform process system page jia deng wei dong richard socher li kai li li imagenet scale hierarch imag databas comput vision pattern recognit cvpr ieee confer page ieee misha denil lori bazzani hugo larochel nando de freita learn attend deep architectur imag track neural comput didier emmanuel bigand rethink physic rehabilit medicin new technolog induc new learn strategi springer scienc busi media carl doersch tutori variat autoencod arxiv preprint john duchi elad hazan yoram singer adapt subgradi method onlin learn stochast optim journal machin learn research jul angela lee duckworth eli tsukayama henri may establish causal use gitudin hierarch linear model illustr predict achiev control social psycholog person scienc samuel frederick edward phil w anderson theori spin glass journal physic f metal physic ronen eldan ohad shamir power depth feedforward neural network arxiv preprint jeﬀrey l elman find structur time cognit scienc dumitru erhan yoshua bengio aaron courvil manzagol pascal vincent sami bengio whi doe unsupervis help deep learn journal machin learn research feb scott e fahlman christian lebier learn architectur marcu frean upstart algorithm method construct train feedforward neural network neural comput kunihiko fukushima neocognitron neural network model nism pattern recognit unaﬀect shift posit biolog cybernet wang raj leon gati alexand ecker matthia bethg neural algorithm artist style arxiv preprint tom germano self organ map avail http wpi felix ger urgen schmidhub recurr net time count neural network ijcnn proceed intern joint confer volum page ieee ross girshick fast proceed ieee intern confer puter vision page ross girshick jeﬀdonahu trevor darrel jitendra malik rich featur hierarchi accur object detect semant segment proceed ieee confer comput vision pattern recognit page ian goodfellow nip tutori gener adversari network arxiv preprint ian goodfellow jean mehdi mirza bing xu david sherjil ozair aaron courvil yoshua bengio gener adversari net advanc neural inform process system page marco gori alberto tesi problem local minima backpropag ieee transact pattern analysi machin intellig elin gravelin deep learn via stack spars autoencod autom wise brain parcel base function connect phd thesi univers western ontario alex grave navdeep jaitli moham hybrid speech recognit deep bidirect lstm automat speech recognit understand asru ieee workshop page ieee klau greﬀ rupesh kumar srivastava jan ık ba r steunebrink urgen schmidhub lstm search space odyssey arxiv preprint richard gregori patrick cavanagh blind spot scholarpedia stephen grossberg recurr neural network scholarpedia aman gupta haohan wang madhavi ganapathiraju learn structur gene express data use deep architectur applic gene cluster bioinformat biomedicin bibm ieee intern confer page ieee kevin gurney introduct neural network crc press shyam guthikonda kohonen map wittenberg univers origin deep learn bharath hariharan pablo aez ross girshick jitendra malik hypercolumn object segment local proceed ieee confer comput vision pattern recognit page david hartley observ man volum cambridg univers press johan hastad almost optim lower bound small depth circuit proceed eighteenth annual acm symposium theori comput page acm jame v haxbi elizabeth hoﬀman ida gobbini distribut human neural system face percept trend cognit scienc kaim xiangyu zhang shaoq ren jian sun deep residu learn imag recognit arxiv preprint kaim xiangyu zhang shaoq ren jian sun ident map deep residu network arxiv preprint donald old hebb organ behavior neuropsycholog theori ogi press robert theori backpropag neural network neural network intern joint confer page ieee geoﬀrey e hinton train product expert minim contrast diverg neural comput geoﬀrey e hinton peter dayan brendan j frey radford neal algorithm unsupervis neural network scienc geoﬀrey e hinton simon osindero teh fast learn algorithm deep belief net neural comput geoﬀrey e hinton nitish srivastava alex krizhevski ilya sutskev ruslan r salakhutdinov improv neural network prevent featur tector arxiv preprint sepp hochreit urgen schmidhub long memori neural comput john j hopﬁeld neural network physic system emerg collect tional abil proceed nation academi scienc kurt hornik maxwel stinchcomb halbert white multilay feedforward network univers approxim neural network zhite hu xuezh zhengzhong liu eduard hovi eric xing har deep neural network logic rule arxiv preprint david h hubel torsten n wiesel recept ﬁeld singl neuron cat striat cortex journal physiolog wang raj sergey ioﬀ christian szegedi batch normal acceler deep network ing reduc intern covari shift arxiv preprint michael jordan serial order parallel distribut process approach advanc psycholog nal kalchbrenn aaron van den oord karen simonyan ivo danihelka oriol vinyal alex grave koray kavukcuoglu video pixel network arxiv preprint kiyoshi kawaguchi multithread softwar model backpropag neural network applic ag khachaturyan sv semenovskaya b vainstein proach determin structur amplitud phase sov phi crystallogr diederik kingma jimmi ba adam method stochast optim arxiv preprint diederik p kingma max well variat bay arxiv preprint diederik p kingma shakir moham danilo jimenez rezend max well supervis learn deep gener model advanc neural inform process system page tinn hoﬀkjeldsen john von neumann concept minimax theorem journey diﬀer mathemat context archiv histori exact scienc teuvo kohonen map proceed ieee bryan kolb ian q whishaw g campbel teskey introduct brain ior volum mark l krieg tutori bayesian belief network ranjay krishna yuke zhu oliv groth justin johnson kenji hata joshua kravitz stephani chen yanni kalantidi li david shamma et al visual genom connect languag vision use crowdsourc dens imag annot arxiv preprint alex krizhevski geoﬀrey hinton learn multipl layer featur tini imag alex krizhevski ilya sutskev geoﬀrey e hinton imagenet classiﬁc deep convolut neural network advanc neural inform process system page origin deep learn teja kulkarni william f whitney pushmeet kohli josh tenenbaum deep lution invers graphic network advanc neural inform process system page brenden lake ruslan salakhutdinov joshua b tenenbaum concept learn probabilist program induct scienc alan laped robert farber neural net work neural inform process system page hugo larochel geoﬀrey e hinton learn combin foveal glimps order boltzmann machin advanc neural inform process system page b boser le cun john denker henderson richard e howard w hubbard lawrenc jackel handwritten digit recognit network advanc neural inform process system cites yann lecun eon bottou yoshua bengio patrick haﬀner learn appli document recognit proceed ieee yann lecun corinna cort christoph jc burg mnist databas written digit yann lecun yoshua bengio geoﬀrey hinton deep learn natur honglak lee roger gross rajesh ranganath andrew ng convolut deep belief network scalabl unsupervis learn hierarch represent ceed annual intern confer machin learn page acm zhaop li neural model contour integr primari visual cortex neural comput lin michael mair serg belongi jame hay pietro perona deva ramanan piotr ar c lawrenc zitnick microsoft coco common object context european confer comput vision page springer liou lin finit memori load hairi neuron natur comput liou yuan error toler associ memori biolog bernet christoph lippert jennif listgarten ying liu carl kadi robert davidson david heckerman fast linear mix model associ studi natur method zachari c lipton john berkowitz charl elkan critic review recurr neural network sequenc learn arxiv preprint wang raj david g low object recognit local featur comput vision proceed seventh ieee intern confer volum page ieee xuezh eduard hovi sequenc label via arxiv preprint xuezh yingkai gao zhite hu yaoliang yu yuntian deng eduard hovi dropout regular arxiv preprint maschler eilon solan shmuel zamir game theori translat hebrew ziv hellman edit mike born charl e mcculloch john neuhau gener linear mix model wiley onlin librari warren mcculloch walter pitt logic calculu idea imman nervou activ bulletin mathemat biophys marc ezard nadal learn feedforward layer network tile algorithm journal physic mathemat gener marc ezard giorgio parisi virasoro spin glass theori beyond marvin l minski seymour papert perceptron introduct comput geometri mit press cambridg melani mitchel introduct genet algorithm mit press tom mitchel et al machin learn wcb jeﬀrey moran robert desimon select attent gate visual process extrastri cortex frontier cognit neurosci michael c mozer focus algorithm tempor pattern recognit complex system kevin p murphi machin learn probabilist perspect mit press vinod nair geoﬀrey e hinton rectiﬁ linear unit improv restrict boltzmann machin proceed intern confer machin learn page john nash game annal mathemat page john f nash et al equilibrium point game proc nat acad sci usa anh nguyen jason yosinski jeﬀclun deep neural network easili fool high conﬁdenc predict unrecogniz imag ieee confer comput vision pattern recognit cvpr page ieee origin deep learn danh v nguyen damla urk raymond j carrol linear mix eﬀect model applic longitudin data journal nonparametr tic erkki oja simpliﬁ neuron model princip compon analyz journal matic biolog erkki oja juha karhunen stochast approxim eigenvector valu expect random matrix journal mathemat analysi applic aaron van den oord nal kalchbrenn koray kavukcuoglu pixel recurr neural network arxiv preprint keiichi osako rita singh bhiksha raj complex recurr neural network ing speech signal applic signal process audio acoust paa ieee workshop page ieee rajesh g parekh jihoon yang vasant honavar construct neural network learn algorithm pattern classiﬁc razvan pascanu caglar gulcehr kyunghyun cho yoshua bengio construct deep recurr neural network arxiv preprint razvan pascanu toma mikolov yoshua bengio diﬃculti train recurr neural network icml razvan pascanu yann n dauphin surya ganguli yoshua bengio saddl point problem optim arxiv preprint bryan plummer liwei wang chri cervant juan c caicedo julia hockenmai svetlana lazebnik entiti collect correspond richer model proceed ieee intern confer comput vision page tomaso poggio thoma serr model visual cortex scholarpedia christoph poultney sumit chopra yann l cun et al eﬃcient learn spars resent model advanc neural inform process system page jose c princip neil r euliano w curt lefebvr neural adapt system fundament simul john wiley son shaoq ren kaim ross girshick jian sun faster toward time object detect region propos network advanc neural inform process system page martin riedmil heinrich braun direct adapt method faster gation learn rprop algorithm neural network ieee intern confer page ieee wang raj aj robinson frank fallsid util driven dynam error propag network univers cambridg depart engin frank rosenblatt perceptron probabilist model inform storag nizat brain psycholog review david e rumelhart geoﬀrey e hinton ronald j william learn intern sentat error propag technic report dtic document olga russakovski jia deng hao su jonathan kraus sanjeev satheesh sean heng huang andrej karpathi aditya khosla michael bernstein et al imagenet larg scale visual recognit challeng intern journal comput vision rasoul safavian david landgreb survey decis tree classiﬁ methodolog ruslan salakhutdinov geoﬀrey e hinton deep boltzmann machin aistat volum page tim saliman ian goodfellow wojciech zaremba vicki cheung alec radford xi chen improv techniqu train gan advanc neural inform process system page urgen schmidhub deep learn neural network overview neural network mike schuster kuldip k paliw bidirect recurr neural network ieee action signal process thoma serr lior wolf tomaso poggio object recognit featur inspir visual cortex ieee comput societi confer comput vision pattern recognit cvpr volum page ieee noam shazeer azalia mirhoseini krzysztof maziarz andi davi quoc le geoﬀrey ton jeﬀdean outrag larg neural network expert layer arxiv preprint karen simonyan andrew zisserman veri deep convolut network imag recognit arxiv preprint paul smolenski inform process dynam system foundat harmoni theori technic report dtic document kihyuk sohn honglak lee xinchen yan learn structur output represent use deep condit gener model advanc neural inform process system page nitish srivastava geoﬀrey e hinton alex krizhevski ilya sutskev ruslan dinov dropout simpl way prevent neural network overﬁt journal machin learn research origin deep learn amo storkey increas capac hopﬁeld network without sacriﬁc function intern confer artiﬁci neural network page springer richard sutton david mcallest satind p singh yishay mansour et al ici gradient method reinforc learn function approxim nip volum page christian szegedi wojciech zaremba ilya sutskev joan bruna dumitru erhan ian goodfellow rob fergu intrigu properti neural network arxiv preprint christian szegedi wei liu yangq jia pierr sermanet scott reed dragomir anguelov dumitru erhan vincent vanhouck andrew rabinovich go deeper convolut proceed ieee confer comput vision tern recognit page aaron van den oord nal kalchbrenn lass espeholt oriol vinyal alex grave et al condit imag gener pixelcnn decod advanc neural inform process system page andrea veit michael j wilber serg belongi residu network behav like sembl rel shallow network advanc neural inform process system page pascal vincent hugo larochel isabel lajoi yoshua bengio zagol stack denois autoencod learn use represent deep network local denois criterion journal machin learn research dec martin j wainwright michael jordan et al graphic model exponenti famili variat infer foundat trend r machin learn brian wandel foundat vision sinauer associ haohan wang jingkang yang multipl confound correct regular linear mix eﬀect model applic biolog process haohan wang aaksha meghawat morenc eric p xing learn improv gener multimod sentiment analysi arxiv preprint paul j werbo gener backpropag applic recurr ga market model neural network paul j werbo backpropag time doe proceed ieee bernard widrow et al adapt adalin neuron use chemic wang raj alan l wilk nichola j wade bain neural network brain cognit gibb j willard elementari principl statist mechan ration foundat thermodynam new york charl scribner son london edward arnold andrew g wilson christoph dann chri luca eric p xing human kernel advanc neural inform process system page shi xingjian zhourong chen hao wang yeung wong chun woo convolut lstm network machin learn approach precipit nowcast advanc neural inform process system page kelvin xu jimmi ba ryan kiro kyunghyun cho aaron courvil ruslan nov richard zemel yoshua bengio show attend tell neural imag caption gener visual attent arxiv preprint zhilin yang ruslan salakhutdinov william cohen sequenc tag scratch arxiv preprint andrew yao separ hierarchi oracl annual symposium foundat comput scienc sfc xin yao evolv artiﬁci neural network proceed ieee dong yu li deng georg dahl role depend speech recognit proc nip workshop deep learn unsupervis featur learn sergey zagoruyko niko komodaki wide residu network arxiv preprint matthew zeiler adadelta adapt learn rate method arxiv preprint chiyuan zhang sami bengio moritz hardt benjamin recht oriol vinyal understand deep learn requir rethink gener arxiv preprint ke zhang miao sun toni x han xingfang yuan liru guo tao liu ual network residu network multilevel residu network arxiv preprint junbo zhao michael mathieu yann lecun gener adversari work arxiv preprint xiang zhou matthew stephen eﬃcient analysi ciation studi natur genet